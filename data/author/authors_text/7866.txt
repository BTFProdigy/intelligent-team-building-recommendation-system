Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 779?786, Vancouver, October 2005. c?2005 Association for Computational Linguistics
The Hiero Machine Translation System:
Extensions, Evaluation, and Analysis
David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin
Institute for Advanced Computer Studies (UMIACS)
University of Maryland, College Park, MD 20742, USA
{dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu
Abstract
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has been largely absent from the best
performing machine translation systems in recent
community-wide evaluations. In this paper, we dis-
cuss a new hierarchical phrase-based statistical ma-
chine translation system (Chiang, 2005), present-
ing recent extensions to the original proposal, new
evaluation results in a community-wide evaluation,
and a novel technique for fine-grained comparative
analysis of MT systems.
1 Introduction
Hierarchical organization is a well known prop-
erty of language, and yet the notion of hierarchi-
cal structure has, for the last several years, been
absent from the best performing machine transla-
tion systems in community-wide evaluations. Statis-
tical phrase-based models (e.g. (Och and Ney, 2004;
Koehn et al, 2003; Marcu andWong, 2002)) charac-
terize a source sentence f as a flat partition of non-
overlapping subsequences, or ?phrases?, f?1 ? ? ? f?J ,
and the process of translation involves selecting tar-
get phrases e?i corresponding to the f? j and modify-
ing their sequential order. The need for some way
to model aspects of syntactic behavior, such as the
tendency of constituents to move together as a unit,
is widely recognized?the role of syntactic units is
well attested in recent systematic studies of trans-
lation (Fox, 2002; Hwa et al, 2002; Koehn and
Knight, 2003), and their absence in phrase-based
models is quite evident when looking at MT system
output. Nonetheless, attempts to incorporate richer
linguistic features have generally met with little suc-
cess (Och et al, 2004a).
Chiang (2005) introduces Hiero, a hierarchical
phrase-based model for statistical machine transla-
tion. Hiero extends the standard, non-hierarchical
notion of ?phrases? to include nonterminal sym-
bols, which permits it to capture both word-level and
phrase-level reorderings within the same framework.
The model has the formal structure of a synchronous
CFG, but it does not make any commitment to a
linguistically relevant analysis, and it does not re-
quire syntactically annotated training data. Chiang
(2005) reported significant performance improve-
ments in Chinese-English translation as compared
with Pharaoh, a state-of-the-art phrase-based system
(Koehn, 2004).
In Section 2, we review the essential elements
of Hiero. In Section 3 we describe extensions to
this system, including new features involving named
entities and numbers and support for a fourfold
scale-up in training set size. Section 4 presents new
evaluation results for Chinese-English as well as
Arabic-English translation, obtained in the context
of the 2005 NISTMT Eval exercise. In Section 5, we
introduce a novel technique for fine-grained com-
parative analysis of MT systems, which we em-
ploy in analyzing differences between Hiero?s and
Pharaoh?s translations.
2 Hiero
Hiero is a stochastic synchronous CFG, whose pro-
ductions are extracted automatically from unanno-
tated parallel texts, and whose rule probabilities
form a log-linear model learned by minimum-error-
rate training; together with a modified CKY beam-
search decoder (similar to that of Wu (1996)). We
describe these components in brief below.
779
S ? ?S 1 X 2 ,S 1 X 2 ?
S ? ?X 1 ,X 1 ?
X ? ?yu X 1 you X 2 , have X 2 with X 1 ?
X ? ?X 1 de X 2 , the X 2 that X 1 ?
X ? ?X 1 zhiyi, one of X 1 ?
X ? ?Aozhou,Australia?
X ? ?shi, is?
X ? ?shaoshu guojia, few countries?
X ? ?bangjiao, diplomatic relations?
X ? ?Bei Han,North Korea?
Figure 1: Example synchronous CFG
2.1 Grammar
A synchronous CFG or syntax-directed transduction
grammar (Lewis and Stearns, 1968) consists of pairs
of CFG rules with aligned nonterminal symbols. We
denote this alignment by coindexation with boxed
numbers (Figure 1). A derivation starts with a pair
of aligned start symbols, and proceeds by rewrit-
ing pairs of aligned nonterminal symbols using the
paired rules (Figure 2).
Training begins with phrase pairs, obtained as by
Och, Koehn, and others: GIZA++ (Och and Ney,
2000) is used to obtain one-to-many word align-
ments in both directions, which are combined into a
single set of refined alignments using the ?final-and?
method of Koehn et al (2003); then those pairs of
substrings that are exclusively aligned to each other
are extracted as phrase pairs.
Then, synchronous CFG rules are constructed
out of the initial phrase pairs by subtraction: ev-
ery phrase pair ? f? , e?? becomes a rule X ? ? f? , e??,
and a phrase pair ? f? , e?? can be subtracted from a
rule X ? ??1 f??2, ?1e??2? to form a new rule X ?
??1X i ?2, ?1X i ?2?, where i is an index not already
used. Various filters are also applied to reduce the
number of extracted rules. Since one of these filters
restricts the number of nonterminal symbols to two,
our extracted grammar is equivalent to an inversion
transduction grammar (Wu, 1997).
2.2 Model
The model is a log-linear model (Och and Ney,
2002) over synchronous CFG derivations. The
weight of a derivation is PLM(e)?LM , the weighted
language model probability, multiplied by the prod-
uct of the weights of the rules used in the derivation.
The weight of each rule is, in turn:
(1) w(X ? ??, ??) =
?
i
?i(X ? ??, ??)?i
where the ?i are features defined on rules. The ba-
sic model uses the following features, analogous to
Pharaoh?s default feature set:
? P(? | ?) and P(? | ?)
? the lexical weights Pw(? | ?) and Pw(? | ?)
(Koehn et al, 2003);1
? a phrase penalty exp(1);
? a word penalty exp(l), where l is the number of
terminals in ?.
The exceptions to the above are the two ?glue?
rules, which are the rules with left-hand side S in
Figure 1. The second has weight one, and the first
has weight w(S ? ?S 1 X 2 ,S 1 X 2 ?) = exp(??g),
the idea being that parameter ?g controls the model?s
preference for hierarchical phrases over serial com-
bination of phrases.
Phrase translation probabilities are estimated by
relative-frequency estimation. Since the extraction
process does not generate a unique derivation for
each training sentence pair, a distribution over pos-
sible derivations is hypothesized, which gives uni-
form weight to all initial phrases extracted from a
sentence pair and uniform weight to all rules formed
out of an initial phrase. This distribution is then used
to estimate the phrase translation probabilities.
The lexical-weighting features are estimated us-
ing a method similar to that of Koehn et al (2003).
The language model is a trigram model with mod-
ified Kneser-Ney smoothing (Chen and Goodman,
1998), trained using the SRI-LM toolkit (Stolcke,
2002).
1This feature uses word alignment information, which is dis-
carded in the final grammar. If a rule occurs in training with
more than one possible word alignment, Koehn et al take the
maximum lexical weight; Hiero uses a weighted average.
780
?S 1 ,S 1 ? ? ?S 2 X 3 ,S 2 X 3 ?
? ?S 4 X 5 X 3 ,S 4 X 5 X 3 ?
? ?X 6 X 5 X 3 ,X 6 X 5 X 3 ?
? ?Aozhou X 5 X 3 ,Australia X 5 X 3 ?
? ?Aozhou shi X 3 ,Australia is X 3 ?
? ?Aozhou shi X 7 zhiyi,Australia is one of X 7 ?
? ?Aozhou shi X 8 de X 9 zhiyi,Australia is one of the X 9 that X 8 ?
? ?Aozhou shi yu X 1 you X 2 de X 9 zhiyi,Australia is one of the X 9 that have X 2 with X 1 ?
Figure 2: Example partial derivation of a synchronous CFG.
The feature weights are learned by maximizing
the BLEU score (Papineni et al, 2002) on held-out
data, using minimum-error-rate training (Och, 2003)
as implemented by Koehn. The implementation was
slightly modified to ensure that the BLEU scoring
matches NIST?s definition and that hypotheses in
the n-best lists are merged when they have the same
translation and the same feature vector.
3 Extensions
In this section we describe our extensions to the base
Hiero system that improve its performance signif-
icantly. First, we describe the addition of two new
features to the Chinese model, in a manner similar
to that of Och et al (2004b); then we describe how
we scaled the system up to a much larger training
set.
3.1 New features
The LDC Chinese-English named entity lists (900k
entries) are a potentially valuable resource, but
previous experiments have suggested that simply
adding them to the training data does not help
(Vogel et al, 2003). Instead, we placed them in
a supplementary phrase-translation table, giving
greater weight to phrases that occurred less fre-
quently in the primary training data. For each en-
try ? f , {e1, . . . , en}?, we counted the number of times
c( f ) that f appeared in the primary training data,
and assigned the entry the weight 1c( f )+1 , which
was then distributed evenly among the supplemen-
tary phrase pairs {? f , ei?}. We then created a new
model feature for named entities. When one of these
supplementary phrase pairs was used in transla-
tion, its feature value for the named-entity feature
was the weight defined above, and its value in the
other phrase-translation and lexical-weighting fea-
tures was zero. Since these scores belonged to a sep-
arate feature from the primary translation probabili-
ties, they could be reweighted independently during
minimum-error-rate training.
Similarly, to process Chinese numbers and dates,
we wrote a rule-based Chinese number/date transla-
tor, and created a new model feature for it. Again,
the weight given to this module was optimized
during minimum-error-rate training. In some cases
we wrote the rules to provide multiple uniformly-
weighted English translations for a Chinese phrase
(for example,k? (bari) could become ?the 8th? or
?on the 8th?), allowing the language model to decide
between the options.
3.2 Scaling up training
Chiang (2005) reports on experiments in Chinese-
English translation using a model trained on
7.2M+9.2M words of parallel data.2 For the NIST
MT Eval 2005 large training condition, consider-
ably more data than this is allowable. We chose
to use only newswire data, plus data from Sino-
rama, a Taiwanese news magazine.3 This amounts
to almost 30M+30M words. Scaling to this set re-
quired reducing the initial limit on phrase lengths,
previously fixed at 10, to avoid explosive growth of
2Here and below, the notation ?X + Y words? denotes X
words of foreign text and Y words of English text.
3From Sinorama, only data from 1991 and later were used,
as articles prior to that were translated quite loosely.
781
the extracted grammar. However, since longer initial
phrases can be beneficial for translation accuracy,
we adopted a variable length limit: 10 for the FBIS
corpus and other mainland newswire sources, and 7
for the HK News corpus and Sinorama. (During de-
coding, limits of up to 15 were sometimes used; in
principle these limits should all be the same, but in
practice it is preferable to tune them separately.)
For Arabic-English translation, we used the ba-
sic Hiero model, without special features for named
entities or numbers/dates. We again used only the
newswire portions of the allowable training data; we
also excluded the Ummah data, as the translations
were found to be quite loose. Since this amounted
to only about 1.5M+1.5M words, we used a higher
initial phrase limit of 15 during both training and de-
coding.
4 Evaluation
Figure 1 shows the performance of several systems
on NIST MT Eval 2003 Chinese test data: Pharaoh
(2004 version), trained only on the FBIS data; Hi-
ero, with various combinations of the new features
and the larger training data.4 This table also shows
Hiero?s performance on the NIST 2005 MT evalua-
tion task.5 The metric here is case-sensitive BLEU.6
Figure 2 shows the performance of two systems
on Arabic in the NIST 2005 MT Evaluation task:
DC, a phrase-based decoder for a model trained by
Pharaoh, and Hiero.
5 Analysis
Over the last few years, several automatic metrics
for machine translation evaluation have been intro-
duced, largely to reduce the human cost of itera-
tive system evaluation during the development cy-
cle (Lin and Och, 2004; Melamed et al, 2003; Pap-
ineni et al, 2002). All are predicated on the concept
4The third line, corresponding to the model without new fea-
tures trained on the larger data, may be slightly depressed be-
cause the feature weights from the fourth line were used instead
of doing minimum-error-rate training specially for this model.
5Full results are available at http://www.nist.gov/
speech/tests/summaries/2005/mt05.htm. For this test, a
phrase length limit of 15 was used during decoding.
6For this task, the translation output was uppercased using
the SRI-LM toolkit: essentially, it was decoded again using
an HMM whose states and transitions are a trigram language
model of cased English, and whose emission probabilities are
reversed, i.e., probability of cased word given lowercased word.
System Features Train MT03 MT05
Pharaoh standard FBIS 0.268
Hiero standard FBIS 0.288
Hiero standard full 0.329
Hiero +nums, names full 0.339 0.300
Table 1: Chinese results. (BLEU-4; MT03 case-
insensitive, MT05 case-sensitive)
System Train MT05
DC full 0.399
Hiero full 0.450
Table 2: Arabic results. (BLEU-4; MT03 case-
insensitive, MT05 scores case-sensitive.
of n-gram matching between the sentence hypothe-
sized by the translation system and one or more ref-
erence translations?that is, human translations for
the test sentence. Although the motivations and for-
mulae underlying these metrics are all different, ul-
timately they all produce a single number represent-
ing the ?goodness? of the MT system output over a
set of reference documents. This facility is valuable
in determining whether a given system modification
has a positive impact on overall translation perfor-
mance. However, the metrics are all holistic. They
provide no insight into the specific competencies or
weaknesses of one system relative to another.
Ideally, we would like to use automatic methods
to provide immediate diagnostic information about
the translation output?what the system does well,
and what it does poorly. At the most general level,
we want to know how our system performs on the
two most basic problems in translation?word trans-
lation and reordering. Unigram precision and recall
statistics tell us something about the performance of
an MT system?s internal translation dictionaries, but
nothing about reordering. It is thought that higher or-
der n-grams correlate with the reordering accuracy
of MT systems, but this is again a holistic metric.
What we would really like to know is howwell the
system is able to capture systematic reordering pat-
terns in the input, which ones it is successful with,
and which ones it has difficulty with. Word n-grams
are little help here: they are too many, too sparse, and
it is difficult to discern general patterns from them.
782
5.1 A New Analysis Method
In developing a new analysis method, we are moti-
vated in part by recent studies suggesting that word
reorderings follow general patterns with respect to
syntax, although there remains a high degree of flex-
ibility (Fox, 2002; Hwa et al, 2002). This suggests
that in a comparative analysis of two MT systems, it
may be useful to look for syntactic patterns that one
system captures well in the target language and the
other does not, using a syntax based metric.
We propose to summarize reordering patterns us-
ing part-of-speech sequences. Unfortunately, recent
work has shown that applying statistical parsers to
ungrammatical MT output is unreliable at best, with
the parser often assigning unreasonable probabili-
ties and incongruent structure (Yamada and Knight,
2002; Och et al, 2004a). Anticipating that this
would be equally problematic for part-of-speech
tagging, we make the conservative choice to apply
annotation only to the reference corpus. Word n-
gram correspondences with a reference translation
are used to infer the part-of-speech tags for words in
the system output.
First, we tagged the reference corpus with parts
of speech. We used MXPOST (Ratnaparkhi, 1996),
and in order to discover more general patterns, we
map the tag set down after tagging, e.g. NN, NNP,
NNPS and NNS all map to NN. Second, we com-
puted the frequency freq(ti . . . t j) of every possible
tag sequence ti . . . t j in the reference corpus. Third,
we computed the correspondence between each hy-
pothesis sentence and each of its corresponding ref-
erence sentences using an approximation to max-
imum matching (Melamed et al, 2003). This al-
gorithm provides a list of runs or contiguous se-
quences of words ei . . . e j in the reference that are
also present in the hypothesis. (Note that runs are
order-sensitive.) Fourth, for each recalled n-gram
ei . . . e j, we looked up the associated tag sequence
ti . . . t j and incremented a counter recalled(ti . . . t j).
Finally, we computed the recall of tag patterns,
R(ti . . . t j) = recalled(ti . . . t j)/freq(ti . . . t j), for all
patterns in the corpus.
By examining examples of these tag sequences in
the reference corpus and their hypothesized trans-
lations, we expect to gain some insight into the
comparative strengths and weaknesses of the MT
systems? reordering models. (An interactive plat-
form for this analysis is demonstrated by Lopez and
Resnik (2005).)
5.2 Chinese
We performed tag sequence analysis on the Hiero
and Pharaoh systems trained on the FBIS data only.
Table 3 shows those n-grams for which Hiero and
Pharaoh?s recall differed significantly (p < 0.01).
The numbers shown are the ratio of Hiero?s recall
to Pharaoh?s. Note that the n-grams on which Hi-
ero had better recall are dominated by fragments of
prepositional phrases (in the Penn Treebank tagset,
prepositions are tagged IN or TO).
Our hypothesis is that Hiero produces English PPs
better because many of them are translated from
Chinese phrases which have an NP modifying an NP
to its right, often connected with the particle? (de).
These are often translated into English as PPs, which
modify the NP to the left. A correct translation, then,
would have to reorder the two NPs. Notice in the ta-
ble that Hiero recalls proportionally more n-grams
as n increases, corroborating the intuition that Hiero
should be better at longer-distance reorderings.
Investigating this hypothesis qualitatively, we in-
spected the first five occurrences of the n-grams of
the first type on the list (JJ NN IN DT NN). Of
these, we omit one example because both systems
recalled the n-gram correctly, and one because they
differed only in lexical choice (Hiero matched the
5-gram with one reference sentence, Pharaoh with
zero). The other three examples are shown below (H
= Hiero, P = Pharaoh):
(2) T?
UN
?h
security
?Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28?36,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalizing local translation models
Michael Subotin
Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland
College Park, MD 20742
msubotin@umiacs.umd.edu
Abstract
We investigate translation modeling based on
exponential estimates which generalize essen-
tial components of standard translation mod-
els. In application to a hierarchical phrase-
based system the simplest generalization al-
lows its models of lexical selection and re-
ordering to be conditioned on arbitrary at-
tributes of the source sentence and its anno-
tation. Viewing these estimates as approxi-
mations of sentence-level probabilities moti-
vates further elaborations that seek to exploit
general syntactic and morphological patterns.
Dimensionality control with `1 regularizers
makes it possible to negotiate the tradeoff be-
tween translation quality and decoding speed.
Putting together and extending several recent
advances in phrase-based translation we ar-
rive at a flexible modeling framework that al-
lows efficient leveraging of monolingual re-
sources and tools. Experiments with features
derived from the output of Chinese and Arabic
parsers and an Arabic lemmatizer show signif-
icant improvements over a strong baseline.
1 Introduction
Effective handling of large and diverse inventories
of feature functions is one of the most pressing
open problems in machine translation. While min-
imum error training (Och, 2003) has by now be-
come a standard tool for interpolating a small num-
ber of aggregate scores, it is not well suited for
learning in high-dimensional feature spaces. At the
same time, although recent years have seen consid-
erable progress in development of general methods
for large-scale prediction of complex outputs (Bak?r
et al, 2007), their application to language transla-
tion has presented considerable challenges. Sev-
eral studies have shown that large-margin methods
can be adapted to the special complexities of the
task (Liang et al, 2006; Tillmann and Zhang, 2006;
Cowan et al, 2006) . However, the capacity of these
algorithms to improve over state-of-the-art baselines
is currently limited by their lack of robust dimen-
sionality reduction. Performance gains are closely
tied to the number and variety of candidate features
that enter into the model, and increasing the size of
the feature space not only slows down training in
terms of the number of iterations required for con-
vergence, but can also considerably reduce decod-
ing speed, leading to run-time costs that may be un-
acceptable in industrial settings. Vector space re-
gression has shown impressive performance in other
tasks involving string-to-string mappings (Cortes et
al., 2007), but its application to language transla-
tion presents a different set of open problems (Wang
et al, 2007). Other promising formalisms, which
have not yet produced end-to-end systems compet-
itive with standard baselines, include the approach
due to Turian et al(2006), the hidden-state syn-
chronous grammar-based exponential model studied
by Blunsom et al(2008), and a similar model in-
corporating target-side n-gram features proposed in
Subotin (2008).
Taken together the results of these studies point
to a striking overarching conclusion: the humble
relative frequency estimate of phrase-based mod-
els makes for a surprisingly strong baseline. The
present paper investigates a family of models that
28
capitalize on this practical insight to allow efficient
optimization of weights for a virtually unlimited
number of features. We take as a point of depar-
ture the observation that the essential translation
model scores comprising standard decoding deci-
sion rules can be recovered as special cases of a
more general family of models. As we discuss be-
low, they are equal to maximum likelihood solu-
tions for locally normalized ?piecewise? approxi-
mations to sentence-level probabilities, where word
alignment is used to determine the subset of fea-
tures observed in each training example. The cases
for which such solutions have a closed form corre-
spond to particular restrictions placed on the feature
space. Thus, relative frequency phrase models can
be obtained by limiting the feature space to indica-
tor functions for the phrase pairs consistent with an
alignment. By removing unnecessary restrictions we
restore the full flexibility of local exponential mod-
els, including their ability to use features depending
on arbitrary aspects of the source sentence and its
annotation. The availability of robust algorithms for
dimensionality reduction with `1 regularizers (Ng,
2004) means that we can start with a virtually un-
limited number of candidate features and negotiate
the tradeoff between translation quality and decod-
ing speed in a way appropriate for a given setting.
A further attractive property of locally normalized
models is the modest computational cost of their
training and ease of its parallelization. This is par-
ticularly so for the models we concentrate on in this
paper, defined so that parameter estimation decom-
poses into a large number of small optimization sub-
problems which can be solved independently.
Several variants of these models beyond rela-
tive frequencies have appeared in the literature be-
fore. Maximum entropy estimation for transla-
tion of individual words dates back to Berger et
al (1996), and the idea of using multi-class classi-
fiers to sharpen predictions normally made through
relative frequency estimates has been recently rein-
troduced under the rubric of word sense disambigua-
tion and generalized to substrings (Chan et al2007;
Carpuat and Wu 2007a; Carpuat and Wu 2007b).
Maximum entropy models for non-lexicalized re-
ordering rules for a phrase-based system with CKY
decoding has been described by Xiong et al(2006).
Some of our experiments, where exponential models
conditioned on the source sentence and its parse an-
notation are associated with all rewrite rules in a hi-
erarchical phrase-based system (Chiang, 2007) and
all word-level probabilities in standard lexical mod-
els, may be seen as a synthesis of these ideas.
The broader perspective of viewing the product of
such local probabilities as a particular approxima-
tion of sentence-level likelihood points the way be-
yond multi-class classification, and this type of gen-
eralization is the main original contribution of the
present work. Training a classifier to predict the tar-
get phrase for every source phrase is equivalent to
conjoining all contextual features of the model with
an indicator function for the surface form of some
rule in the grammar. We can also use features based
on less specific representation of a rule. Of par-
ticular importance for machine translations are rep-
resentations which generalize reordering informa-
tion beyond identity of individual words ? a type of
generalization that presents a challenge in hierarchi-
cal phrase-based translation. With generalized local
models this can be accomplished by adding features
tracking only ordering patterns of rules. We exper-
iment with a case of such models which allows us
to preserve decomposition of parameter estimation
into independent subproblems.
Besides varying the structure of the feature space,
we can also extend the range of normalization for
the exponential models beyond target phrases co-
occurring with a given source phrase in the phrase
table. This choice is especially natural for richly in-
flected languages, since it enables us to model mul-
tiple levels of morphological representation at once
and estimate probabilities for rules whose surface
forms have not been observed in training. We apply
a simple variant of this approach to Arabic-English
lexical models.
Experimental results across eight test sets in two
language pairs support the intuition that features
conjoined with indicator functions for surface forms
of rules yield higher gains for test sets with better
coverage in training data, while features based on
less specific representations become more useful for
test sets with lower baselines.
The types of features explored in this paper rep-
resent only a small portion of available options, and
much practical experimentation remains to be done,
particularly in order to find the most effective ex-
29
tensions of the feature space beyond multiclass clas-
sification. However, the results reported here show
considerable promise and we believe that the flexi-
bility of these models combined with their computa-
tional efficiency makes them potentially valuable as
an extension for a variety of systems using transla-
tion models with local conditional probabilities and
as a feature selection method for globally trained
models.
2 Hierarchical phrase-based translation
We take as our starting point David Chiang?s Hiero
system, which generalizes phrase-based translation
to substrings with gaps (Chiang, 2007). Consider
for instance the following set of context-free rules
with a single non-terminal symbol:
?A , A ? ? ?A1 A2 , A1 A2 ?
?A , A ? ? ? d? A1 ide?esA2 , A1 A2 ideas ?
?A , A ? ? ? incolores , colorless ?
?A , A ? ? ? vertes , green ?
?A , A ? ? ? dormentA , sleepA ?
?A , A ? ? ? furieusement , furiously ?
It is one of many rule sets that would suffice to
generate the English translation 1b for the French
sentence 1a.
1a. d? incolores ide?es vertes dorment furieusement
1b. colorless green ideas sleep furiously
As shown by Chiang (2007), a weighted gram-
mar of this form can be collected and scored by
simple extensions of standard methods for phrase-
based translation and efficiently combined with a
language model in a CKY decoder to achieve large
improvements over a state-of-the-art phrase-based
system. The translation is chosen to be the target-
side yield of the highest-scoring synchronous parse
consistent with the source sentence. Although a va-
riety of scores interpolated into the decision rule for
phrase-based systems have been investigated over
the years, only a handful have been discovered to be
consistently useful, as is in our experience also the
case for the hierarchical variant. Setting aside spe-
cialized components such as number translators, we
concentrate on the essential sub-models1 comprising
1To avoid confusion with features of the exponential models
described below we shall use the term ?model? for the terms
the translation model: the phrase models and lexical
models.
3 Local exponential translation models
3.1 Relative frequency solutions
Standard phrase models associate conditional proba-
bilities with subparts of translation hypotheses, usu-
ally computed as relative frequencies of counts of
extracted phrases.2 Let ry be the target side of a rule
and rx its source side. The weight of the rule in the
?reverse? phrase model would then be computed as
p(ry|rx) =
count(?rx, ry?)
?
ry? count(?r
x, ry??)
(1)
When used to score a translation hypothesis cor-
responding to some synchronous parse tree T , the
phrase model may be conceived as an approxima-
tion of the probability of a target sentence Y given a
source sentence X
p(Y |X) ?
?
r?T
p(ry|rx) (2)
Although there is nothing in current learning the-
ory that would prompt one to expect that expressions
of this form should be effective, their surprisingly
strong performance in machine translation in an em-
pirical observation borne out by many studies. In
order to build on this practical insight it is useful to
gain a clearer understanding of their formal proper-
ties.
We start by writing out an expression for the like-
lihood of training data which would give rise to max-
imum likelihood solutions like those in eq. 1. Con-
sider a feature vector whose components are indi-
cator functions for rules in the grammar, and let
us define an exponential model for a sentence pair
(Xm, Ym) of the form
p (Ym|Xm) ?
?
r?(Xm,Ym)
p(ry|rx) (3)
=
?
r?(Xm,Ym)
exp{w ? fr(Xm, Ym)}
?
r?:rx=r?x exp{w ? fr?(Xm, Ym)}
(4)
interpolated using MERT.
2Chiang (2007) uses a heuristic estimate of fractional counts
in these computations. For completeness we report both vari-
ants in the experiments.
30
where fr(Xm, Ym) is a restriction of the feature
vector such that all of its entries except for the one
corresponding to the rule r are zero and the summa-
tion is over all rules in the grammar with the same
source side. As can be verified by writing out the
likelihood for the training set and setting its gradi-
ent to zero, maximum likelihood estimation based
on eq. 4 yields estimates equal to relative frequency
solutions. In fact, because its normalization fac-
tors have non-zero parameters in common only for
rules which share the same source phrase, param-
eter estimation decomposes into independent opti-
mization subproblems, one for each source phrase
in the grammar. However, recovering relative fre-
quencies of the needed form requires further atten-
tion to the relationship between the definition of fea-
ture functions and phrase extraction. Computation
of phrase models in machine translation crucially re-
lies on a form of feature selection not widely known
in other contexts. A rule is considered to be ob-
served in a sentence pair only if it is consistent with
predictions of a word alignment model according to
heuristics for alignment combination and phrase ex-
traction. The standard recipes in translation model-
ing can thus be seen to include a feature selection
procedure that applies individually to each training
example.
3.2 Classifier solutions
We can now generalize these relative frequency
estimates by relaxing the restrictions they implic-
itly place on the form of permissible feature func-
tions. The simplest elaboration involves allow-
ing indicator functions for rules to be conjoined
with indicator functions for arbitrary attributes of
the source sentence or its annotation. This pre-
serves a decomposition of parameter estimation of
optimization subproblems associated with individ-
ual source phrase, but effectively replaces proba-
bilities p(ry|rx) in eqs. 2 and 3 with probabili-
ties conditioned on the source phrase together with
some of its source-side context. We may, for ex-
ample, conjoin an indicator function for the rule
?A , A ? ? ? d? A1 ide?esA2 , A1 A2 ideas ? with a
function telling us whether a part-of-speech tagger
has identified the word at the left edge of the source-
side gap A2 as an adjective, which would provide
additional evidence for the target side of this rule.
Combining a grammar-based formalism with con-
textual features raises a subtle question of whether
rules which have gaps at the edges and can match at
multiple positions of a training example should be
counted as having occurred together with their re-
spective contextual features once for each possible
match. To avoid favoring monotone rules, which
tend to match at many positions, over reordering
rules, which tend to match at a single span, we ran-
domly sample only one of such multiple matches for
training.
Unlike conventional phrase models, contextually-
conditioned probabilities cannot be stored in a pre-
computed phrase table. Instead, we store informa-
tion about features and their weights and compute
the normalization factors at run-time at the point
when they are first needed by the decoder.
At the expense of more complicated decoding
procedures we could also apply the same line of
reasoning to generalize the ?noisy channel? phrase
model p(rx|ry) to be conditioned on local target-
side context in a translation hypothesis, possibly
combining target-side annotation of the training set
with surface form of rules. We do not pursue this
elaboration in part because we are skeptical about its
potential for success. The current state of machine
translation rarely permits constructing well-formed
translations, so that most of the contextual features
on the target side would be rarely if at all observed
in the training data, resulting in sparse and noisy es-
timates. Furthermore, we have yet to find a case
where relative frequency estimates p(rx|ry) make a
useful contribution to the system when contextually-
conditioned ?reverse? probabilities are used, sug-
gesting that viewing translation modeling as approx-
imating sentence-level probabilities p(Y |X) may be
a more fruitful avenue in the long term.
For translation with phrases without gaps classi-
fier solutions of eq. 4 are equivalent to a maximum
entropy variant of the phrase sense disambigua-
tion approach studied by Carpuat & Wu (2007b).
These solutions are also closely related to the ap-
proximation known as piecewise training in graph-
ical model literature (Sutton and McCallum, 2005;
Sutton and Minka, 2006) and independently stated
in a more general form by Pe?rez-Cruz et al(2007).
Aside from formal differences between feature tem-
plates defined by graphical models and grammars,
31
which are beyond the scope of our discussion, there
are several further contrasts between these studies
and standard practice in machine translation in how
the learned parameters are used to make predic-
tions. Unlike inference in piecewise-trained graphi-
cal models, where all parameters for a given output
are added together without normalization, features
that enter into the score for a translation hypothe-
sis are restricted to be consistent with a single syn-
chronous parse and the local probabilities are nor-
malized in decoding as in training.
3.3 Lexical models
The use of conditional probabilities in standard lex-
ical models also gives us a straightforward way to
generalize them in the same way as phrase models.
Consider the lexical model pw(ry|rx), defined fol-
lowing Koehn et al(2003), with a denoting the most
frequent word alignment observed for the rule in the
training set.
pw(r
y|rx) =
n?
i=1
1
|j|(i, j) ? a|
?
(i,j)?a
p(wyi |w
x
j )
(5)
We replace p(wyi |w
x
j ) with context-conditioned
probabilities, computed similarly to eq. 4, but at the
level of individual words. Our experience suggests
that, unlike the analogous phrase model, the stan-
dard lexical model pw(rx|ry) is not made redundant
by this elaboration, and we use its baseline variant
in all our experiments. While this approach seeks to
make the most of practical insights underlying state-
of-the-art baselines, it is of course not the only way
to combine rule-based and word-based features. See
for example Sutton & Minka (2006) for a discussion
of alternatives that are closer in spirit to the idea of
approximating global probabilities.
3.4 Further generalizations
An immediate practical benefit of interpreting rela-
tive frequency and classifier estimates of translation
models as special cases is the possibility of gener-
alizing them further by introducing additional fea-
tures based on less specific representations of rules
and words.
Among the least specific and most potentially use-
ful representations of hierarchical phrases are those
limited to the patterns formed by gaps and words,
allowing the model to generalize reordering infor-
mation beyond individual tokens. We study two
types of ordering patterns. For rules with two gaps
we form features by conjoining contextual indicator
functions with functions indicating whether the gap
pattern is monotone or inverting. We also use an-
other type of ordering features, representing the pat-
tern formed by gaps and contiguous subsequences
of words. For example, the rule with the right-hand
side ? d? A1 ide?esA2 , A1 A2 ideas ?might be asso-
ciated with the pattern ? aA1 aA2 , A1 A2 a ?. Be-
cause some source-side patterns of this type apply
to many different rules it is no longer possible to de-
compose parameter estimation into small indepen-
dent optimization subproblems. For practical conve-
nience we enforce decomposition in the experiments
reported below in the following way. We define indi-
cator functions for sequences of closed-class words
and the most frequent part-of-speech tag for open-
class words on the source side. For the rule above
and a simple tag-set the pattern tracked by such an
indicator function would be d? A1 N A2 . We require
all reordering features to be conjoined with an indi-
cator function of this type, ensuring that each cor-
responds to a separate optimization subproblem. We
further split larger optimization subproblems, so that
parameters for identical reordering features are in
some cases estimated separately for different subsets
of rules.
Morphological inflection provides motivation for
another class of features not bound to surface repre-
sentations. In this paper we explore a particularly
simple example of this approach, adding features
conjoined with indicator functions for Arabic lem-
mas to the lexical models in Arabic-English trans-
lation. This preserves decomposition of parameter
estimation, with subproblems now associated with
individual lemmas rather than words. Lemma-based
features suggest another extension of the modeling
framework. Instead of computing the sums in nor-
malization factors over all English words aligned to
a given Arabic token in the training data, we let the
sum range over all English words aligned to Arabic
words sharing its lemma. This also defines probabil-
ities for Arabic words whose surface forms have not
been observed in training, although we do not take
advantage of estimates for out-of-vocabulary words
32
in the experiments below.
3.5 Regularization
We apply `1 regularization (Ng, 2004; Gao et al,
2007) to make learning more robust to noise and
control the effective dimensionality of the feature
space by subtracting a weighted sum of absolute val-
ues of parameter weights from the log-likelihood of
the training data
w? = argmax
w
LL(w) ?
?
i
Ci|wi| (6)
We optimize the objective using a variant of the
orthant-wise limited-memory quasi-Newton algo-
rithm proposed by Andrew & Gao (2007).3 All val-
ues Ci are set to 1 in most of the experiments below,
although we apply stronger regularization (Ci = 3)
to reordering features. Tuning regularization trade-
offs individually for different feature types is an at-
tractive option, but our experiments suggest that us-
ing cross-entropy on a held-out portion of training
data for that purpose does not help performance.
We leave investigation of the alternatives for future
work.
4 Experiments
4.1 Data and methods
We apply the models to Arabic-English and
Chinese-English translation, with training sets con-
sisting of 108,268 and 1,017,930 sentence pairs, re-
spectively.4 All conditions use word alignments
produced by sequential iterations of IBM model 1,
HMM, and IBM model 4 in GIZA++ , followed
3Our implementation of the algo-
rithm as a SciPy routine is available at
http://www.umiacs.umd.edu/?msubotin/owlqn.py
4The Arabic-English data came from Arabic News Transla-
tion Text Part 1 (LDC2004T17), Arabic English Parallel News
Text (LDC2004T18), and Arabic Treebank English Translation
(LDC2005E46). Chinese-English data came from Xinhua Chi-
nese English Parallel News Text Version 1 beta (LDC2002E18),
Chinese Treebank English Parallel Corpus (LDC2003E07),
Chinese English News Magazine Parallel Text (LDC2005T10),
FBIS Multilanguage Texts (LDC2003E14), Chinese News
Translation Text Part 1 (LDC2005T06), and the HKNews por-
tion of Hong Kong Parallel Text (LDC2004T08). Some sen-
tence pairs were not included in the training sets due to large
length discrepancies.
by ?diag-and? symmetrization (Koehn et al, 2003).
Thresholds for phrase extraction and decoder prun-
ing were set to values typical for the baseline sys-
tem (Chiang, 2007). Unaligned words at the outer
edges of rules or gaps were disallowed. A trigram
language model with modified interpolated Kneser-
Ney smoothing (Chen and Goodman, 1998) was
trained by the SRILM toolkit on the Xinhua por-
tion of the Gigaword corpus and the English side of
the parallel training set. Evaluation was based on
the BLEU score with 95% bootstrap confidence in-
tervals for the score and difference between scores,
calculated by scripts in version 11a of the NIST dis-
tribution. The 2002 NIST MT evaluation sets was
used for development. The 2003, 2004, 2005, and
2006 sets were used for testing.
The decision rule was based on the standard log-
linear interpolation of several models, with weights
tuned byMERT on the development set (Och, 2003).
The baseline consisted of the language model, two
phrase translation models, two lexical models, and
a brevity penalty. In the runs where generalized ex-
ponential models were used they replaced both of
the baseline phrase translation models. The feature
set used for exponential phrase models in the exper-
iments included all the rules in the grammar and all
aligned word pairs for lexical models. Elementary
contextual features were based on Viterbi parses ob-
tained from the Stanford parser. Word features in-
cluded identities of word unigrams and bigrams ad-
jacent to a given rule, possibly including rule words.
Part-of-speech features included similar ngrams up
to the length of 3 and the tags for rule tokens. These
features were collected for training by a straightfor-
ward extension of rule extraction algorithms imple-
mented in the baseline system for each possible lo-
cation of ngrams with respect to the rule: namely, at
the outer edges of the rule and at the edges of any
gaps that it has. Our models also include a subset
of contextual features formed by pairwise combina-
tions of these elementary features. A final type of
contextual features in these experiments was the se-
quence of the highest nodes in the parse tree that fill
the span of the rule and the sequences that fill its
gaps. We used an in-house Arabic tokenizer based
on a Java implementation of Buckwalter?s morpho-
logical analyzer and incorporating simple statistics
from the Penn Arabic treebank, also extending it to
33
perform lemmatization.
The total number of candidate features thus de-
fined is very large, and we use a number of sim-
ple heuristics to reduce it prior to training. They
are not essential to the estimates and were chosen
so that the models could be trained in a few hours
on a small cluster. With the exception of discarding
all except the 10 most frequent target phrases ob-
served with each source phrase,5 which benefits per-
formance, we expect that relaxing these restrictions
would improve the score. These limitations included
count-based thresholds on the frequency of contex-
tual features included into the model, the frequency
of rules and reordering patterns conjoined with other
features, and the size of optimization subproblems to
which contextual features are added. We don?t con-
join contextual features to rules whose source phrase
terminals are all punctuation symbols. For subprob-
lems of size exceeding a certain threshold, we train
on a subsample of available training instances. For
the Chinese-English task we do not add reorder-
ing features to problems with low-entropy distribu-
tions of inversion and reordering patterns and dis-
card rules with two non-terminals altogether if the
entropy of their reordering patterns falls under a
threshold. None of these restrictions were applied to
the baselines. Finally, we solve only those optimiza-
tion subproblems which include parameters needed
in the development and training sets. This leads to a
reduction of costs that is similar to phrase table fil-
tering and likewise does not affect the solution. At
decoding time all features for the translation models
and their weights are accessed from a disk-mapped
trie.
4.2 Results and discussion
The results are shown in tables 1 and 2. For both lan-
guage pairs we had a choice between using a base-
line that is computed in the same way as the other ex-
ponential models, with the exception of its use of rel-
ative frequency estimates and a baseline that incor-
porates averaged fractional counts for phrase mod-
els and lexical models, as used by Chiang (2007).
For the sake of completeness we report both (though
without performing statistical comparisons between
5This has prompted us to add an additional target-side token
to lexical models, which subsumes the discarded items under a
single category.
Condition MT03 MT04 MT05 MT06
Rel. freq. 48.24 43.92 47.53 37.94
Frac. 48.34 45.68 47.95 39.41
Context 49.47* 45.65 48.76 39.49
+lex 50.42* 46.07* 49.66* 39.32
+lex+lemma 49.86* 47.02* 49.29* 40.81*
Table 1: Arabic-English translation, BLEU scores on
testing. Conditions include two baselines: simple rela-
tive frequency (rel. freq.) and fractional estimates (frac.).
Experimental conditions: contextual features in phrase
models (context); same and contextual features in lexi-
cal models (+lex); same and lemma based features in lex-
ical models (+lex+lemma). Stars mark statistically sig-
nificant improvements over the fractional baseline which
produced a higher score on the dev-test MT02 set than
the other baseline (59.75 vs. 59.66).
Condition MT03 MT04 MT05 MT06
Rel. freq. 32.62 27.53 30.50 22.78
Frac. 32.56 27.98 30.42 23.16
Context 33.16* 28.35* 31.52* 23.67*
+lex 33.50* 28.14* 31.98* 23.05
+lex+reord 33.12* 28.27* 31.73* 23.45*
Table 2: Chinese-English translation, BLEU scores on
testing. Conditions include two baselines: simple rela-
tive frequency (rel. freq.) and fractional estimates (frac.).
Experimental conditions: contextual features in phrase
models (context); same and contextual features in lexi-
cal models (+lex); same and reordering features in phrase
models (+lex+reord). Stars mark statistically significant
improvements over the simple relative frequency baseline
which produced a higher score on the dev-test MT02 set
than the other baseline (33.62 vs. 33.53).
34
them). Statistical tests for experimental conditions
were performed in comparison to the baseline which
achieved higher score on the test-dev MT02 set: the
fractional count baseline for Arabic-English and the
simple relative count baseline for Chinese-English.
We test models with classifier solutions for phrase
models alone and for phrase models together with
lexical models in both language pairs. For Arabic-
English translation we also experiment with adding
features based on lemmas to lexical models, while
for Chinese-English we add ?reordering? features ?
features based on the ordering pattern of gaps for
rules with two gaps and features based on ordering
of gaps and words for rules with a single gap.
For both language pairs the results show con-
sistent distinctions in behavior of different mod-
els between the test sets giving rise to generally
higher scores (MT03 and MT05) and generally
lower scores (MT04 and MT06). The fractional
counts seem to be consistently more helpful for
test sets with poorer coverage, although the rea-
son for this is not immediately clear. For exponen-
tial models the two type of sets present two pos-
sible sources of difference. The lower-performing
sets have poorer coverage in the training data, and
they also may suffer from lower-quality annotation,
since the training sets for both the translation mod-
els and the annotation tools are dominated by text
in the same, newswire domain. Overall, the use of
features based on surface forms is more beneficial
for MT03 and MT05. Indeed, using lexical models
with contextual features in addition to phrase models
hurts performance on MT06 for Arabic-English and
on both MT04 and MT06 for Chinese-English. In
contrast, using features based on less specific repre-
sentations is more beneficial on test sets with poorer
coverage, while hurting performance on MT03 and
MT05. This agrees with our intuitions and also sug-
gests that the differences in coverage of training data
for the translation models may be playing a more
important role in these trends than coverage for an-
notation tools.
5 Conclusion
We have outlined a framework for translation
modeling that synthesizes several recent advances
in phrase-based machine translation and suggests
many other ways to leverage sub-token representa-
tions of words as well as syntactic and morpholog-
ical annotation tools, of which the experiments re-
ported here explore only a small fraction. Indeed,
the range and practicality of the available options is
perhaps its most attractive feature. The inital results
are promising and we are optimistic that continued
exploration of this class of models will uncover even
more effective uses.
Acknowledgments
I would like to thank David Chiang, Chris Dyer, Lise
Getoor, Kevin Gimpel, Adam Lopez, Nitin Mad-
nani, Smaranda Muresan, Noah Smith, Amy Wein-
berg and especially Philip Resnik for discussions re-
lating to this work. I am also grateful to David Chi-
ang for sharing source code of the Hiero translation
system and to the two anonymous reviewers for their
constructive comments.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L1-regularized log-linear models. In Proc.
ICML 2007
Go?khan H. Bak?r, Thomas Hofmann, Bernhard
Scho?lkopf, Alexander J. Smola, Ben Taskar and
S. V. N. Vishwanathan, eds. 2007. Predicting
Structured Data. MIT Press.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing Computational Linguistics,
22(1).
Phil Blunsom, Trevor Cohn and Miles Osborne. 2008.
Discriminative Synchronous Transduction for Statisti-
cal Machine Translation In proc. ACL 2008.
Marine Carpuat and Dekai Wu. 2007a. Improving Sta-
tistical Machine Translation using Word Sense Disam-
biguation In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007)..
Marine Carpuat and Dekai Wu. 2007b. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation (TMI 2007).
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. ACL.
Stanley F. Chen and Joshua T. Goodman. 1998. An Em-
pirical Study of Smoothing Techniques for Language
35
Modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Corinna Cortes, Mehryar Mohri, and Jason Weston.
2007. A General Regression Framework for Learning
String-to-String Mappings. In Predicting Structured
Data. MIT Press.
Brooke Cowan, Ivona Kucerova, and Michael Collins.
2006. A Discriminative Model for Tree-to-Tree Trans-
lation. In proceedings of EMNLP 2006.
J. Gao, G. Andrew, M. Johnson and K. Toutanova 2007.
A Comparative Study of Parameter Estimation Meth-
ods for Statistical Natural Language Processing. In
Proc. ACL 2007.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. Proceed-
ings of the Human Language Technology Conference
(HLT-NAACL 2003).
P. Liang, Alexandre Bouchard-Cote, D. Klein and B.
Taskar. 2006. An End-to-End Discriminative Ap-
proach to Machine Translation. In Association for
Computational Linguistics (ACL06).
A. Y. Ng. 2004. Feature selection, L1 vs. L2 regular-
ization, and rotational invariance In Proceedings of
the Twenty-first International Conference on Machine
Learning
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In ACL 2003:
Proc. of the 41st Annual Meeting of the Association for
Computational Linguistics.
F. Pe?rez-Cruz, Z. Ghahramani and M. Pontil. 2007. Ker-
nel Conditional Graphical Models In Predicting Struc-
tured Data. MIT Press.
Michael Subotin. 2008. Exponential models for machine
translation. Generals paper, Department of Linguis-
tics, University of Maryland.
Charles Sutton and Andrew McCallum. 2005. Piecewise
training for undirected models. In Conference on Un-
certainty in Artificial Intelligence (UAI).
Charles Sutton and Tom Minka. 2006. Local Training
and Belief Propagation. Microsoft Research Technical
Report TR-2006-121..
Christoph Tillmann and Tong Zhang 2006. A Dis-
criminative Global Training Algorithm for Statistical
MT. In Association for Computational Linguistics
(ACL06).
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed 2006. Scalable Discriminative Learning for
Natural Language Parsing and Translation In Proceed-
ings of the 20th Annual Conference on Neural Infor-
mation Processing Systems (NIPS).
ZhuoranWang, John Shawe-Taylor, and Sandor Szedmak
2007. Kernel Regression Based Machine Translation.
In Proceedings of NAACL HLT.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy
based phrase reordering model for statistical machine
translation. In Proceedings of the 21st international
Conference on Computational Linguistics and the 44th
Annual Meeting of the ACL.
36
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 230?238,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An exponential translation model for target language morphology
Michael Subotin
Paxfire, Inc.
Department of Linguistics & UMIACS, University of Maryland
msubotin@gmail.com
Abstract
This paper presents an exponential model
for translation into highly inflected languages
which can be scaled to very large datasets. As
in other recent proposals, it predicts target-
side phrases and can be conditioned on source-
side context. However, crucially for the task
of modeling morphological generalizations, it
estimates feature parameters from the entire
training set rather than as a collection of sepa-
rate classifiers. We apply it to English-Czech
translation, using a variety of features captur-
ing potential predictors for case, number, and
gender, and one of the largest publicly avail-
able parallel data sets. We also describe gen-
eration and modeling of inflected forms un-
observed in training data and decoding proce-
dures for a model with non-local target-side
feature dependencies.
1 Introduction
Translation into languages with rich morphology
presents special challenges for phrase-based meth-
ods. Thus, Birch et al(2008) find that transla-
tion quality achieved by a popular phrase-based sys-
tem correlates significantly with a measure of target-
side, but not source-side morphological complexity.
Recently, several studies (Bojar, 2007; Avramidis
and Koehn, 2009; Ramanathan et al, 2009; Yen-
iterzi and Oflazer, 2010) proposed modeling target-
side morphology in a phrase-based factored mod-
els framework (Koehn and Hoang, 2007). Under
this approach linguistic annotation of source sen-
tences is analyzed using heuristics to identify rel-
evant structural phenomena, whose occurrences are
in turn used to compute additional relative frequency
(maximum likelihood) estimates predicting target-
side inflections. This approach makes it difficult
to handle the complex interplay between different
predictors for inflections. For example, the ac-
cusative case is usually preserved in translation, so
that nouns appearing in the direct object position of
English clauses tend to be translated to words with
accusative case markings in languages with richer
morphology, and vice versa. However, there are
exceptions. For example, some verbs that place
their object in the accusative case in Czech may be
rendered as prepositional constructions in English
(Naughton, 2005):
David was looking for Jana
David hledal Janu
David searched Jana-ACC
Conversely, direct objects of some English verbs
can be translated by nouns with genitive case
markings in Czech:
David asked Jana where Karel was
David zeptal se Jany kde je Karel
David asked SELF Jana-GEN where is Karel
Furthermore, English noun modifiers are often
rendered by Czech possessive adjectives and a ver-
bal complement in one language is commonly trans-
lated by a nominalizing complement in another lan-
guage, so that the part of speech (POS) of its head
need not be preserved. These complications make it
difficult to model morphological phenomena using
230
closed-form estimates. This paper presents an alter-
native approach based on exponential phrase mod-
els, which can straightforwardly handle feature sets
with arbitrarily elaborate source-side dependencies.
2 Hierarchical phrase-based translation
We take as our starting point David Chiang?s Hiero
system, which generalizes phrase-based translation
to substrings with gaps (Chiang, 2007). Consider
for instance the following set of context-free rules
with a single non-terminal symbol:
?A , A ? ? ?A1A2 , A1A2 ?
?A , A ? ? ? d?A1 ide?esA2 , A1A2 ideas ?
?A , A ? ? ? incolores , colorless ?
?A , A ? ? ? vertes , green ?
?A , A ? ? ? dormentA , sleepA ?
?A , A ? ? ? furieusement , furiously ?
It is one of many rule sets that would suffice to
generate the English translation 1b for the French
sentence 1a.
1a. d? incolores ide?es vertes dorment furieusement
1b. colorless green ideas sleep furiously
As shown by Chiang (2007), a weighted gram-
mar of this form can be collected and scored by sim-
ple extensions of standard methods for phrase-based
translation and efficiently combined with a language
model in a CKY decoder to achieve large improve-
ments over a state-of-the-art phrase-based system.
The translation is chosen to be the target-side yield
of the highest-scoring synchronous parse consistent
with the source sentence. Although a variety of
scores interpolated into the decision rule for phrase-
based systems have been investigated over the years,
only a handful have been discovered to be consis-
tently useful. In this work we concentrate on ex-
tending the target-given-source phrase model1.
3 Exponential phrase models with shared
features
The model used in this work is based on the familiar
equation for conditional exponential models:
1To avoid confusion with features of the exponential mod-
els described below we shall use the term ?model? rather than
?feature? for the terms interpolated using MERT.
p(Y |X) =
e~w?~f(X,Y )
?
Y ??GEN(X) e
~w?~f(X,Y ?)
where ~f(X,Y ) is a vector of feature functions,
~w is a corresponding weight vector, so that ~w ?
~f(X,Y ) =
?
iwifi(X,Y ), and GEN(X) is a
set of values corresponding to Y . For a target-
given-source phrase model the predicted outcomes
are target-side phrases ry, the model is conditioned
on a source-side phrase rx together with some con-
text, and each GEN(X) consists of target phrases
ry co-occurring with a given source phrase rx in the
grammar.
Maximum likelihood estimation for exponential
model finds the values of weights that maximize the
likelihood of the training data, or, equivalently, its
logarithm:
LL(~w) = log
M?
m=1
p(Ym|Xm) =
M?
m=1
log p(Ym|Xm)
where the expressions range over all training in-
stances {m}. In this work we extend the objective
using an `2 regularizer (Ng, 2004; Gao et al, 2007).
We obtain the counts of instances and features from
the standard heuristics used to extract the grammar
from a word-aligned parallel corpus.
Exponential models and other classifiers have
been used in several recent studies to condition
phrase model probabilities on source-side context
(Chan et al2007; Carpuat and Wu 2007a; Carpuat
and Wu 2007b). However, this has been gener-
ally accomplished by training independent classi-
fiers associated with different source phrases. This
approach is not well suited to modeling target-
language inflections, since parameters for the fea-
tures associated with morphological markings and
their predictors would be estimated separately from
many, generally very small training sets, thereby
preventing the model from making precisely the
kind of generalization beyond specific phrases that
we seek to obtain. Instead we continue the approach
proposed in Subotin (2008), where a single model
defined by the equations above is trained on all of the
data, so that parameters for features that are shared
by rule sets with difference source sides reflect cu-
mulative feature counts, while the standard relative
231
frequency model can be obtained as a special case
of maximum likelihood estimation for a model con-
taining only the features for rules.2 Recently, Jeong
et al(2010) independently proposed an exponential
model with shared features for target-side morphol-
ogy in application to lexical scores in a treelet-based
system.
4 Features
The feature space for target-side inflection models
used in this work consists of features tracking the
source phrase and the corresponding target phrase
together with its complete morphological tag, which
will be referred to as rule features for brevity. The
feature space also includes features tracking the
source phrase together with the lemmatized repre-
sentation of the target phrase, called lemma features
below. Since there is little ambiguity in lemmati-
zation for Czech, the lemma representations were
for simplicity based on the most frequent lemma
for each token. Finally, we include features associ-
ating aspects of source-side annotation with inflec-
tions of aligned target words. The models include
features for three general classes of morphological
types: number, case, and gender. We add inflec-
tion features for all words aligned to at least one En-
glish verb, adjective, noun, pronoun, or determiner,
excepting definite and indefinite articles. A sepa-
rate feature type marks cases where an intended in-
flection category is not applicable to a target word
falling under these criteria due to a POS mismatch
between aligned words.
4.1 Number
The inflection for number is particularly easy to
model in translating from English, since it is gen-
erally marked on the source side, and POS taggers
based on the Penn treebank tag set attempt to infer
it in cases where it is not. For word pairs whose
source-side word is a verb, we add a feature marking
the number of its subject, with separate features for
noun and pronoun subjects. For word pairs whose
source side is an adjective, we add a feature marking
the number of the head of the smallest noun phrase
that contains it.
2Note that this model is estimated from the full parallel cor-
pus, rather than a held-out development set.
4.2 Case
Among the inflection types of Czech nouns, the only
type that is not generally observed in English and
does not belong to derivational morphology is in-
flection for case. Czech marks seven cases: nomi-
nal, genitive, dative, accusative, vocative, locative,
and instrumental. Not all of these forms are overtly
distinguished for all lexical items, and some words
that function syntactically as nouns do not inflect at
all. Czech adjectives also inflect for case and their
case has to match the case of their governing noun.
However, since the source sentence and its anno-
tation contain a variety of predictors for case, we
model it using only source-dependent features. The
following feature types for case were included:
? The structural role of the aligned source word
or the head of the smallest noun phrase con-
taining the aligned source word. Features were
included for the roles of subject, direct object,
and nominal predicate.
? The preposition governing the smallest noun
phrase containing the aligned source word, if
it is governed by a preposition.
? An indicator for the presence of a possessive
marker modifying the aligned source word or
the head of the smallest noun phrase containing
the aligned source word.
? An indicator for the presence of a numeral
modifying the aligned source word or the head
of the smallest noun phrase containing the
aligned source word.
? An indication that aligned source word modi-
fied by quantifiers many, most, such, or half.
These features would be more properly defined
based on the identity of the target word aligned
to these quantifiers, but little ambiguity seems
to arise from this substitution in practice.
? The lemma of the verb governing the aligned
source word or the head of the smallest noun
phrase containing the aligned source word.
This is the only lexicalized feature type used in
the model and we include only those features
which occur over 1,000 times in the training
data.
232
wx
1
w
x
2
w
x
3
w
y
1
w
y
2
w
y
3
w
x
4
r
1
r
2
observed dependency: w
x
2 
? w
x
3
assumed dependency: w
y
1 
? w
y
3
Figure 1: Inferring syntactic dependencies.
Features corresponding to aspects of the source
word itself and features corresponding to aspects of
the head of a noun phrase containing it were treated
as separate types.
4.3 Gender
Czech nouns belong to one of three cases: feminine,
masculine, and neuter. Verbs and adjectives have to
agree with nouns for gender, although this agree-
ment is not marked in some forms of the verb. In
contrast to number and case, Czech gender generally
cannot be predicted from any aspect of the English
source sentence, which necessitates the use of fea-
tures that depend on another target-side word. This,
in turn, requires a more elaborate decoding proce-
dure, described in the next section. For verbs we
add a feature associating the gender of the verb with
the gender of its subject. For adjectives, we add a
feature tracking the gender of the governing nouns.
These dependencies are inferred from source-side
annotation via word alignments, as depicted in fig-
ure 1, without any use of target-side dependency
parses.
5 Decoding with target-side model
dependencies
The procedure for decoding with non-local target-
side feature dependencies is similar in its general
outlines to the standard method of decoding with a
language model, as described in Chiang (2007). The
search space is organized into arrays called charts,
each containing a set of items whose scores can be
compared with one another for the purposes of prun-
ing. Each rule that has matched the source sen-
tence belongs to a rule chart associated with its
location-anchored sequence of non-terminal and ter-
minal source-side symbols and any of its aspects
which may affect the score of a translation hypothe-
sis when it is combined with another rule. In the case
of the language model these aspects include any of
its target-side words that are part of still incomplete
n-grams. In the case of non-local target-side depen-
dencies this includes any information about features
needed for this rule?s estimate and tracking some
target-side inflection beyond it or features tracking
target-side inflections within this rule and needed for
computation of another rule?s estimate. We shall re-
fer to both these types of information as messages,
alluding to the fact that it will need to be conveyed to
another point in the derivation to finish the compu-
tation. Thus, a rule chart for a rule with one non-
terminal can be denoted as as
?
xi1i+1Ax
j
j1+1, ?
?
,
where we have introduced the symbol ? to represent
the set of messages associated with a given item in
the chart. Each item in the chart is associated with
a score s, based on any submodels and heuristic es-
timates that can already be computed for that item
and used to arrange the chart items into a priority
queue. Combinations of one or more rules that span
a substring of terminals are arranged into a differ-
ent type of chart which we shall call span charts. A
span chart has the form [i1, j1;?1], where ?1 is a set
of messages, and its items are likewise prioritized by
a partial score s1.
The decoding procedure used in this work is based
on the cube pruning method, fully described in Chi-
ang (2007). Informally, whenever a rule chart is
combined with one or more span charts correspond-
ing to its non-terminals, we select best-scoring items
from each chart and update derivation scores by per-
forming any model computations that become pos-
sible once we combine the corresponding items.
Crucially, whenever an item in one of the charts
crosses a pruning threshold, we discard the rest of
that chart?s items, even though one of them could
generate a better-scoring partial derivation in com-
233
bination with an item from another chart. It is there-
fore important to estimate incomplete model scores
as well as we can. We estimate these scores by com-
puting exponential models using all features without
non-local dependencies.
Schematically, our decoding procedure can be il-
lustrated by three elementary cases. We take the
example of computing an estimate for a rule whose
only terminal on both sides is a verb and which re-
quires a feature tracking the target-side gender in-
flection of the subject. We make use of a cache
storing all computed numerators and denominators
of the exponential model, which makes it easy to
recompute an estimate given an additional feature
and use the difference between it and the incomplete
estimate to update the score of the partial deriva-
tion. In the simplest case, illustrated in figure 2, the
non-local feature depends on the position within the
span of the rule?s non-terminal symbol, so that its
model estimate can be computed when its rule chart
is combined with the span chart for its non-terminal
symbol. This is accomplished using a feature mes-
sage, which indicates the gender inflection for the
subject and is denoted as mf (i), where the index
i refers to the position of its ?recipient?. Figure 3
illustrates the case where the non-local feature lies
outside the rule?s span, but the estimated rule lies in-
side a non-terminal of the rule which contains the
feature dependency. This requires sending a rule
message mr(i), which includes information about
the estimated rule (which also serves as a pointer to
the score cache) and its feature dependency. The fi-
nal example, shown in figure 4, illustrates the case
where both types of messages need to be propagated
until we reach a rule chart that spans both ends of
the dependency. In this case, the full estimate for a
rule is computed while combining charts neither of
which corresponds directly to that rule.
A somewhat more formal account of the decod-
ing procedure is given in figure 5, which shows a
partial set of inference rules, generally following the
formalism used in Chiang (2007), but simplifying
it in several ways for brevity. Aside from the no-
tation introduced above, we also make use of two
updating functions. The message-updating function
um(?) takes a set of messages and outputs another
set that includes those messages mr(k) and mf (k)
whose destination k lies outside the span i, j of the
A
Sb
A
V
1 2
m
f
(2)
Score
cache
Figure 2: Non-local dependency, case A.
A
Sb
A
V
1 2
m
r
(1)
Score
cache
Figure 3: Non-local dependency, case B.
A
Sb
A
V
1 2
Score
cache
m
r
(1)
Adv
A
3
m
f
(3)
Figure 4: Non-local dependency, case C.
234
Figure 5: Simplified set of inference rules for decoding
with target-side model dependencies.
chart. The score-updating function us(?) computes
those model estimates which can be completed us-
ing a message in the set ? and returns the difference
between the new and old scores.
6 Modeling unobserved target inflections
As a consequence of translating into a morphologi-
cally rich language, some inflected forms of target
words are unobserved in training data and cannot
be generated by the decoder under standard phrase-
based approaches. Exponential models with shared
features provide a straightforward way to estimate
probabilities of unobserved inflections. This is ac-
complished by extending the sets of target phrases
GEN(X) over which the model is normalized by
including some phrases which have not been ob-
served in the original sets. When additional rule
features with these unobserved target phrases are in-
cluded in the model, their weights will be estimated
even though they never appear in the training exam-
ples (i.e, in the numerator of their likelihoods).
We generate unobserved morphological variants
for target phrases starting from a generation proce-
dure for target words. Morphological variants for
words were generated using the U?FAL MORPHO
tool (Kolovratn??k and Pr?ikryl, 2008). The forms pro-
duced by the tool from the lemma of an observed in-
flected word form were subjected to several restric-
tions:
? For nouns, generated forms had to match the
original form for number.
? For verbs, generated forms had to match the
original form for tense and negation.
? For adjectives, generated forms had to match
the original form for degree of comparison and
negation.
? For pronouns, excepting relative and interrog-
ative pronouns, generated forms had to match
the original form for number, case, and gender.
? Non-standard inflection forms for all POS were
excluded.
The following criteria were used to select rules for
which expanded inflection sets were generated:
? The target phrase had to contain exactly one
word for which inflected forms could be gen-
erated according to the criteria given above.
? If the target phrase contained prepositions or
numerals, they had to be in a position not ad-
jacent to the inflected word. The rationale for
this criterion was the tendency of prepositions
and numerals to determine the inflection of ad-
jacent words.
? The lemmatized form of the phrase had to ac-
count for at least 25% of target phrases ex-
tracted for a given source phrase.
The standard relative frequency estimates for the
p(X|Y ) phrase model and the lexical models do not
provide reasonable values for the decoder scores for
unobserved rules and words. In contrast, exponen-
tial models with surface and lemma features can be
straightforwardly trained for all of them. For the ex-
periments described below we trained an exponen-
tial model for the p(Y |X) lexical model. For greater
speed we estimate the probabilities for the other
two models using interpolated Kneser-Ney smooth-
ing (Chen and Goodman, 1998), where the surface
form of a rule or an aligned word pair plays to role
of a trigram, the pairing of the source surface form
with the lemmatized target form plays the role of a
bigram, and the source surface form alone plays the
role of a unigram.
235
7 Corpora and baselines
We investigate the models using the 2009 edition
of the parallel treebank from U?FAL (Bojar and
Z?abokrtsky?, 2009), containing 8,029,801 sentence
pairs from various genres. The corpus comes with
automatically generated annotation and a random-
ized split into training, development, and testing
sets. Thus, the annotation for the development and
testing sets provides a realistic reflection of what
could be obtained for arbitrary source text. The
English-side annotation follows the standards of the
Penn Treebank and includes dependency parses and
structural role labels such as subject and object. The
Czech side is labeled with several layers of annota-
tion, of which only the morphological tags and lem-
mas are used in this study. The Czech tags follow
the standards of the Prague Dependency Treebank
2.0.
The impact of the models on translation accuracy
was investigated for two experimental conditions:
? Small data set: trained on the news portion of
the data, containing 140,191 sentences; devel-
opment and testing sets containing 1500 sen-
tences of news text each.
? Large data set: trained on all the training data;
developing and testing sets each containing
1500 sentences of EU, news, and fiction data in
equal portions. The other genres were excluded
from the development and testing sets because
manual inspection showed them to contain a
considerable proportion of non-parallel sen-
tences pairs.
All conditions use word alignments produced by
sequential iterations of IBM model 1, HMM, and
IBM model 4 in GIZA++, followed by ?diag-and?
symmetrization (Koehn et al, 2003). Thresholds
for phrase extraction and decoder pruning were set
to values typical for the baseline system (Chiang,
2007). Unaligned words at the outer edges of rules
or gaps were disallowed. A 5-gram language model
with modified interpolated Kneser-Ney smoothing
(Chen and Goodman, 1998) was trained by the
SRILM toolkit (Stolcke, 2002) on a set of 208 mil-
lion running words of text obtained by combining
the monolingual Czech text distributed by the 2010
ACL MT workshop with the Czech portion of the
training data. The decision rule was based on the
standard log-linear interpolation of several models,
with weights tuned by MERT on the development
set (Och, 2003). The baselines consisted of the lan-
guage model, two phrase translation models, two
lexical models, and a brevity penalty.
The proposed exponential phrase model contains
several modifications relative to a standard phrase
model (called baseline A below) with potential to
improve translation accuracy, including smoothed
estimates and estimates incorporating target-side
tags. To gain better insight into the role played by
different elements of the model, we also tested a sec-
ond baseline phrase model (baseline B), which at-
tempted to isolate the exponential model itself from
auxiliary modifications. Baseline B was different
from the experimental condition in using a gram-
mar limited to observed inflections and in replac-
ing the exponential p(Y |X) phrase model by a rel-
ative frequency phrase model. It was different from
baseline A in computing the frequencies for the
p(Y |X) phrase model based on counts of tagged
target phrases and in using the same smoothed es-
timates in the other models as were used in the ex-
perimental condition.
8 Parameter estimation
Parameter estimation was performed using a modi-
fied version of the maximum entropy module from
SciPy (Jones et al, 2001) and the LBFGS-B algo-
rithm (Byrd et al, 1995). The objective included
an `2 regularizer with the regularization trade-off
set to 1. The amount of training data presented a
practical challenge for parameter estimation. Sev-
eral strategies were pursued to reduce the computa-
tional expenses. Following the approach of Mann
et al(2009), the training set was split into many
approximately equal portions, for which parameters
were estimated separately and then averaged for fea-
tures observed in multiple portions. The sets of tar-
get phrases for each source phrase prior to genera-
tion of additional inflected variants were truncated
by discarding extracted rules which were observed
with frequency less than the 200-th most frequent
target phrase for that source phrase.
Additional computational challenges remained
236
due to an important difference between models with
shared features and usual phrase models. Features
appearing with source phrases found in development
and testing data share their weights with features ap-
pearing with other source phrases, so that filtering
the training set for development and testing data af-
fects the solution. Although there seems to be no
reason why this would positively affect translation
accuracy, to be methodologically strict we estimate
parameters for rule and lemma features without in-
flection features for larger models, and then com-
bine them with weights for inflection feature esti-
mated from a smaller portion of training data. This
should affect model performance negatively, since it
precludes learning trade-offs between evidence pro-
vided by the different kinds of features, and there-
fore it gives a conservative assessment of the re-
sults that could be obtained at greater computational
costs. The large data model used parameters for the
inflection features estimated from the small data set.
In the runs where exponential models were used they
replaced the corresponding baseline phrase transla-
tion model.
9 Results and discussion
Table 1 shows the results. Aside from the two base-
lines described in section 7 and the full exponen-
tial model, the table also reports results for an ex-
ponential model that excluded gender-based features
(and hence non-local target-side dependencies). The
highest scores were achieved by the full exponential
model, although baseline B produced surprisingly
disparate effects for the two data sets. This sug-
gests a complex interplay of the various aspects of
the model and training data whose exploration could
further improve the scores. Inclusion of gender-
based features produced small but consistent im-
provements. Table 2 shows a summary of the gram-
mars.
We further illustrate general properties of these
models using toy examples and the actual param-
eters estimated from the large data set. Table 3
shows representative rules with two different source
sides. The column marked ?no infl.? shows model
estimates computed without inflection features. One
can see that for both rule sets the estimated probabil-
ities for rules observed a single time is only slightly
Condition Small set Large set
Baseline A 0.1964 0.2562
Baseline B 0.2067 0.2522
Expon-gender 0.2114 0.2598
Expon+gender 0.2128 0.2615
Table 1: BLUE scores on testing. See section 7 for a
description of the baselines.
Condition Total rules Observed rules
Small set 17,089,850 3,983,820
Large set 39,349,268 23,679,101
Table 2: Grammar sizes after and before generation of
unobserved inflections (all filtered for dev/test sets).
higher than probabilities for generated unobserved
rules. However, rules with relatively high counts
in the second set receive proportionally higher es-
timates, while the difference between the singleton
rule and the most frequent rule in the second set,
which was observed 3 times, is smoothed away to
an even greater extent. The last two columns show
model estimates when various inflection features are
included. There is a grammatical match between
nominative case for the target word and subject po-
sition for the aligned source word and between ac-
cusative case for the target word and direct object
role for the aligned source word. The other pair-
ings represent grammatical mismatches. One can
see that the probabilities for rules leading to correct
case matches are considerably higher than the alter-
natives with incorrect case matches.
rx Count Case No infl. Sb Obj
1 1 Dat 0.085 0.037 0.035
1 3 Acc 0.086 0.092 0.204
1 0 Nom 0.063 0.416 0.063
2 1 Instr 0.007 0.002 0.003
2 31 Nom 0.212 0.624 0.169
2 0 Acc 0.005 0.002 0.009
Table 3: The effect of inflection features on estimated
probabilities.
237
10 Conclusion
This paper has introduced a scalable exponential
phrase model for target languages with complex
morphology that can be trained on the full parallel
corpus. We have showed how it can provide esti-
mates for inflected forms unobserved in the training
data and described decoding procedures for features
with non-local target-side dependencies. The results
suggest that the model should be especially useful
for languages with sparser resources, but that per-
formance improvements can be obtained even for a
very large parallel corpus.
Acknowledgments
I would like to thank Philip Resnik, Amy Weinberg,
Hal Daume? III, Chris Dyer, and the anonymous re-
viewers for helpful comments relating to this work.
References
E. Avramidis and P. Koehn. 2008. Enriching Morpholog-
ically Poor Languages for Statistical Machine Transla-
tion. In Proc. ACL 2008.
A. Birch, M. Osborne and P. Koehn. 2008. Predicting
Success in Machine Translation. The Conference on
Empirical Methods in Natural Language Processing
(EMNLP), 2008.
O. Bojar. 2007. English-to-Czech Factored Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation.
O. Bojar and Z. Z?abokrtsky?. 2009. Large Parallel
Treebank with Rich Annotation. Charles University,
Prague. http://ufal.mff.cuni.cz/czeng/czeng09/, 2009.
R. H. Byrd, P. Lu and J. Nocedal. 1995. A Limited Mem-
ory Algorithm for Bound Constrained Optimization.
SIAM Journal on Scientific and Statistical Computing,
16(5), pp. 1190-1208.
M. Carpuat and D. Wu. 2007a. Improving Statistical
Machine Translation using Word Sense Disambigua-
tion. Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007).
M. Carpuat and D. Wu. 2007b. How Phrase Sense Dis-
ambiguation outperforms Word Sense Disambiguation
for Statistical Machine Translation. 11th Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI 2007)
Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proc. ACL 2007.
S.F. Chen and J.T. Goodman. 1998. An Empirical
Study of Smoothing Techniques for Language Mod-
eling. Technical Report TR-10-98, Computer Science
Group, Harvard University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201-228.
J. Gao, G. Andrew, M. Johnson and K. Toutanova. 2007.
A Comparative Study of Parameter Estimation Meth-
ods for Statistical Natural Language Processing. In
Proc. ACL 2007.
M. Jeong, K. Toutanova, H. Suzuki, and C. Quirk. 2010.
A Discriminative Lexicon Model for Complex Mor-
phology. The Ninth Conference of the Association for
Machine Translation in the Americas (AMTA-2010).
E. Jones, T. Oliphant, P. Peterson and others.
SciPy: Open source scientific tools for Python.
http://www.scipy.org/
P. Koehn and H. Hoang. 2007. Factored translation mod-
els. The Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2007.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of the Hu-
man Language Technology Conference (HLT-NAACL
2003).
D. Kolovratn??k and L. Pr?ikryl. 2008. Pro-
grama?torska? dokumentace k projektu Morfo.
http://ufal.mff.cuni.cz/morfo/, 2008.
G. Mann, R. McDonald, M. Mohri, N. Silberman, D.
Walker. 2009. Efficient Large-Scale Distributed
Training of Conditional Maximum Entropy Models.
Advances in Neural Information Processing Systems
(NIPS), 2009.
J. Naughton. 2005. Czech. An Essential Grammar. Rout-
ledge, 2005.
A.Y. Ng. 2004. Feature selection, L1 vs. L2 regular-
ization, and rotational invariance. In Proceedings of
the Twenty-first International Conference on Machine
Learning.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. ACL 2003.
A. Ramanathan, H. Choudhary, A. Ghosh, P. Bhat-
tacharyya. 2009. Case markers and Morphology: Ad-
dressing the crux of the fluency problem in English-
Hindi SMT. In Proc. ACL 2009.
A. Stolcke. 2002. SRILM ? An Extensible Language
Modeling Toolkit. International Conference on Spo-
ken Language Processing, 2002.
M. Subotin. 2008. Generalizing Local Translation Mod-
els. Proceedings of SSST-2, Second Workshop on
Syntax and Structure in Statistical Translation.
R. Yeniterzi and K. Oflazer. 2010. Syntax-to-
Morphology Mapping in Factored Phrase-Based Sta-
tistical Machine Translation from English to Turkish.
In Proc. ACL 2010.
238
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 59?67,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
A System for Predicting ICD-10-PCS Codes
from Electronic Health Records
Michael Subotin
3M Health Information Systems
Silver Spring, MD
msubotin@mmm.com
Anthony R. Davis
3M Health Information Systems
Silver Spring, MD
adavis4@mmm.com
Abstract
Medical coding is a process of classify-
ing health records according to standard
code sets representing procedures and di-
agnoses. It is an integral part of health
care in the U.S., and the high costs it
incurs have prompted adoption of natu-
ral language processing techniques for au-
tomatic generation of these codes from
the clinical narrative contained in elec-
tronic health records. The need for effec-
tive auto-coding methods becomes even
greater with the impending adoption of
ICD-10, a code inventory of greater com-
plexity than the currently used code sets.
This paper presents a system that predicts
ICD-10 procedure codes from the clinical
narrative using several levels of abstrac-
tion. First, partial hierarchical classifica-
tion is used to identify potentially rele-
vant concepts and codes. Then, for each
of these concepts we estimate the confi-
dence that it appears in a procedure code
for that document. Finally, confidence val-
ues for the candidate codes are estimated
using features derived from concept confi-
dence scores. The concept models can be
trained on data with ICD-9 codes to sup-
plement sparse ICD-10 training resources.
Evaluation on held-out data shows promis-
ing results.
1 Introduction
In many countries reimbursement rules for health
care services stipulate that the patient encounter
must be assigned codes representing diagnoses
that were made for and procedures that were per-
formed on the patient. These codes may be as-
signed by general health care personnel or by spe-
cially trained medical coders. The billing codes
used in the U.S. include International Statisti-
cal Classification of Diseases and Related Health
Problems (ICD) codes, whose version 9 is cur-
rently in use and whose version 10 was scheduled
for adoption in October 2014
1
, as well as Current
Procedural Terminology (CPT) codes. The same
codes are also used for research, internal book-
keeping, and other purposes.
Assigning codes to clinical documentation of-
ten requires extensive technical training and in-
volves substantial labor costs. This, together with
increasing prominence of electronic health records
(EHRs), has prompted development and adoption
of NLP algorithms that support the coding work-
flow by automatically inferring appropriate codes
from the clinical narrative and other information
contained in the EHR (Chute et al., 1994; Heinze
et al., 2001; Resnik et al., 2006; Pakhomov et al.,
2006; Benson, 2006). The need for effective auto-
coding methods becomes especially acute with the
introduction of ICD-10 and the associated increase
of training and labor costs for manual coding.
The novelty and complexity of ICD-10 presents
unprecedented challenges for developers of rule-
based auto-coding software. Thus, while ICD-9
contains 3882 codes for procedures, the number
of codes defined by the ICD-10 Procedure Cod-
ing System (PCS) is greater than 70,000. Further-
more, the organization of ICD-10-PCS is funda-
mentally different from ICD-9, which means that
the investment of time and money that had gone
into writing auto-coding rules for ICD-9 proce-
dure codes cannot be easily leveraged in the tran-
sition to ICD-10.
In turn, statistical auto-coding methods are con-
strained by the scarcity of available training data
with manually assigned ICD-10 codes. While this
problem will be attenuated over the years as ICD-
10-coded data are accumulated, the health care
1
The deadline was delayed by at least a year while this
paper was in review.
59
industry needs effective technology for ICD-10
computer-assisted coding in advance of the imple-
mentation deadline. Thus, for developers of statis-
tical auto-coding algorithms two desiderata come
to the fore: these algorithms should take advantage
of all available training data, including documents
supplied only with ICD-9 codes, and they should
possess high capacity for statistical generalization
in order to maximize the benefits of training mate-
rial with ICD-10 codes.
The auto-coding system described here seeks
to meet both these requirements. Rather than
predicting codes directly from the clinical narra-
tive, a set of classifiers is first applied to identify
coding-related concepts that appear in the EHR.
We use General Equivalence Mappings (GEMs)
between ICD-9 and ICD-10 codes (CMS, 2014)
to train these models not only on data with human-
assigned ICD-10 codes, but also on ICD-9-coded
data. We then use the predicted concepts to de-
rive features for a model that estimates probabil-
ity of ICD-10 codes. Besides the intermediate ab-
straction to concepts, the code confidence model
itself is also designed so as to counteract sparsity
of the training data. Rather than train a separate
classifier for each code, we use a single model
whose features can generalize beyond individual
codes. Partial hierarchical classification is used for
greater run-time efficiency. To our knowledge, this
is the first research publication describing an auto-
coding system for ICD-10-PCS. It is currently de-
ployed, in tandem with other auto-coding mod-
ules, to support computer-assisted coding in the
3M
TM
360 Encompass
TM
System.
The rest of the paper is organized as follows.
Section 2 reviews the overall organization of ICD-
10-PCS. Section 4.1 outlines the run-time process-
ing flow of the system to show how its components
fit together. Section 4.2 describes the concept con-
fidence models, including the hierarchical classi-
fication components. Section 4.3 discusses how
data with manually assigned ICD-9 codes is used
to train some of the concept confidence models.
Section 4.4 describes the code confidence model.
Finally, Section 5 reports experimental results.
2 ICD-10 Procedure Coding System
ICD-10-PCS is a set of codes for medical proce-
dures, developed by 3M Health Information Sys-
tems under contract to the Center for Medicare and
Medicaid Services of the U.S. government. ICD-
10-PCS has been designed systematically; each
code consists of seven characters, and the charac-
ter in each of these positions signifies one partic-
ular aspect of the code. The first character des-
ignates the ?section? of ICD-10-PCS: 0 for Med-
ical and Surgical, 1 for Obstetrics, 2 for Place-
ment, and so on. Within each section, the seven
components, or axes of classification, are intended
to have a consistent meaning; for example in the
Medical and Surgical section, the second charac-
ter designates the body system involved, the third
the root operation, and so on (see Table 1 for a
list). All procedures in this section are thus clas-
sified along these axes. For instance, in a code
such as 0DBJ3ZZ, the D in the second position in-
dicates that the body system involved is the gas-
trointestinal system, B in the third position always
indicates that the root operation is an excision of a
body part, the J in the fourth position indicates that
the appendix is the body part involved, and the 3 in
the fifth position indicates that the approach is per-
cutaneous. The value Z in the last two axes means
than neither a device nor a qualifier are specified.
Character Meaning
1st Section
2nd Body System
3rd Root Operation
4th Body Part
5th Approach
6th Device
7th Qualifier
Table 1: Character Specification of the Medical
and Surgical Section of ICD-10-PCS
Several consequences of the compositional
structure of ICD-10-PCS are especially relevant
for statistical auto-coding methods.
On the one hand, it defines over 70,000 codes,
many of which are logically possible, but very rare
in practice. Thus, attempts to predict the codes as
unitary entities are bound to suffer from data spar-
sity problems even with a large training corpus.
Furthermore, some of the axis values are formu-
lated in ways that are different from how the cor-
responding concepts would normally be expressed
in a clinical narrative. For example, ICD-10-PCS
uses multiple axes (root opreration, body part, and,
in a sense, the first two axes as well) to encode
what many traditional procedure terms (such as
those ending in -tomy and -plasty) express by a
60
single word, while the device axis uses generic
categories where a clinical narrative would refer
only to specific brand names. This drastically lim-
its how much can be accomplished by matching
code descriptions or indexes derived from them
against the text of EHRs.
On the other hand, the systematic conceptual
structure of PCS codes and of the codeset as a
whole can be exploited to compensate for data
sparsity and idiosyncracies of axis definitions by
introducing abstraction into the model.
3 Related work
There exists a large literature on automatic clas-
sification of clinical text (Stanfill et al., 2010). A
sizeable portion of it is devoted to detecting cate-
gories corresponding to billing codes, but most of
these studies are limited to one or a handful of cat-
egories. This is in part because the use of patient
records is subject to strict regulation. Thus, the
corpus used for most auto-coding research up to
date consists of about two thousand documents an-
notated with 45 ICD-9 codes (Pestian et al., 2007).
It was used in a shared task at the 2007 BioNLP
workshop and gave rise to papers studying a va-
riety of rule-based and statistical methods, which
are too numerous to list here.
We limit our attention to a smaller set of re-
search publications describing identification of an
entire set of billing codes, or a significant por-
tion thereof, which better reflects the role of auto-
coding in real-life applications. Mayo Clinic was
among the earliest adopters of auto-coding (Chute
et al., 1994), where it was deployed to assign
codes from a customized and greatly expanded
version of ICD-8, consisting of almost 30K diag-
nostic codes. A recently reported version of their
system (Pakhomov et al., 2006) leverages a com-
bination of example-based techniques and Na??ve
Bayes classification over a database of over 20M
EHRs. The phrases representing the diagnoses
have to be itemized as a list beforehand. In an-
other pioneering study, Larkey & Croft (1995) in-
vestigated k-Nearest Neighbor, Na??ve Bayes, and
relevance feedback on a set of 12K discharge
summaries, predicting ICD-9 codes. Heinze et
al (2000) and Ribeiro-Neto et al (2001) describe
systems centered on symbolic computation. Jiang
et al (2006) discuss confidence assessment for
ICD-9 and CPT codes, performed separately from
code generation. Medori & Fairon (2010) com-
bine information extraction with a Na??ve Bayes
classifier, working with a corpus of about 20K dis-
charge summaries in French. In a recent paper,
Perotte et al (2014) study standard and hierarchi-
cal classification using support vector machines on
a corpus of about 20K EHRs with ICD-9 codes.
We are not aware of any previous publications
on auto-coding for ICD-10-PCS, and the results
of these studies cannot be directly compared with
those reported below due to the unique nature of
this code set. Our original contributions also in-
clude explicit modeling of concepts and the ca-
pability to assign previously unobserved codes
within a machine learning framework.
4 Methods
4.1 Run-time processing flow
We first describe the basic run-time processing
flow of the system, shown in Figure 1.
Figure 1: Run-time processing flow
In a na??ve approach, one could generate all
codes from the ICD-10-PCS inventory for each
EHR
2
and estimate their probability in turn, but
this would be too computationally expensive. In-
stead, the hypothesis space is restricted by two-
2
We use the term EHR generically in this paper. The sys-
tem can be applied at the level of individual clinical docu-
ments or entire patient encounters, whichever is appropriate
for the given application.
61
level hierarchical classification with beam search.
First, a set of classifiers estimates the confidence
of all PCS sections (one-character prefixes of the
codes), one per section. The sections whose con-
fidence exceeds a threshold are used to generate
candidate body systems (two-character code pre-
fixes), whose confidence is estimated by another
set of classifiers. Then, body systems whose con-
fidence exceeds a threshold are used to generate
a set of candidate codes and the set of concepts
expressed by these codes. The probability of ob-
serving each of the candidate concepts in the EHR
is estimated by a separate classifier. Finally, these
concept confidence scores are used to derive fea-
tures for a model that estimates the probability of
observing each of the candidate codes, and the
highest-scoring codes are chosen according to a
thresholding decision rule.
The choice of two hierarchical layers is partially
determined by the amount of training data with
ICD-10 codes available for this study, since many
three-character code prefixes are too infrequent to
train reliable classifiers. Given more training data,
additional hierarchical classification layers could
be used, which would trade a higher risk of recall
errors against greater processing speed. The same
trade-off can be negotiated by adjusting the beam
search threshold.
4.2 Concept confidence models
Estimation of concept confidence ? including the
confidence of code prefixes in the two hierarchi-
cal classification layers ? is performed by a set of
classifiers, one per concept, which are trained on
EHRs supplied with ICD-10 and ICD-9 procedure
codes.
The basis for training the concept models is
provided by a mapping between codes and con-
cepts expressed by the codes. For example, the
code 0GB24ZZ (Excision of Left Adrenal Gland,
Percutaneous Endoscopic Approach) expresses,
among other concepts, the concept adrenal gland
and the more specific concept left adrenal gland.
It also expresses the concept of adrenalectomy
(surgical removal of one or both of the adrenal
glands), which corresponds to the regular expres-
sion 0G[BT][234]..Z over ICD-10-PCS codes.
We used the code-to-concept mapping described
in Mills (2013), supplemented by some additional
categories that do not correspond to traditional
clinical concepts. For example, our set of concepts
included entries for the categories of no device
and no qualifer, which are widely used in ICD-10-
PCS. We also added entries that specified the de-
vice axis or the qualifier axis together with the first
three axes, where they were absent in the original
concept map, reasoning that the language used to
express the choice of the device or qualifier can be
specific to particular procedures and body parts.
For data with ICD-10-PCS codes, the logic used
to generate training instances is straightforward.
Whenever a manually assigned code expresses a
given concept, a positive training instance for the
corresponding classifier is generated. Negative
training instances are sub-sampled from the con-
cepts generated by hierarchical classification lay-
ers for that EHR. As can be seen from this logic,
the precise question that the concept models seek
to answer is as follows: given that this particular
concept has been generated by the upstream hier-
archical layers, how likely is it that it will be ex-
pressed by one of the ICD-10 procedure codes as-
signed to that EHR?
In estimating concept confidence we do not at-
tempt to localize where in the clinical narrative
the given concept is expressed. Our baseline
feature set is simply a bag of tokens. We also
experimented with other feature types, including
frequency-based weighting schemes for token fea-
ture values and features based on string matches of
Unified Medical Language System (UMLS) con-
cept dictionaries. For the concepts of left and right
we define an additional feature type, indicating
whether the token left or right appears more fre-
quently in the EHR. While still rudimentary, this
feature type is more apt to infer laterality than a
bag of tokens.
A number of statistical methods can be used
to estimate concept confidence. We use the
Mallet (McCallum, 2002) implementation of `
1
-
regularized logistic regression, which has shown
good performance for NLP tasks in terms of ac-
curacy as well as scalability at training and run-
time (Gao et al., 2007).
4.3 Training on ICD-9 data
In training concept confidence models on data
with ICD-9 codes we make use of the General
Equivalence Mappings (GEMs), a publicly avail-
able resource establishing relationships between
ICD-9 and ICD-10 codes (CMS, 2014). Most cor-
respondences between ICD-9 and ICD-10 proce-
62
dure codes are one-to-many, although other map-
ping patterns are also found. Furthermore, a code
in one set can correspond to a combination of
codes from the other set. For example, the ICD-
9 code for combined heart-lung transplantation
maps to a set of pairs of ICD-10 codes, the first
code in the pair representing one of three possible
types of heart transplantation, and the other rep-
resenting one of three possible types of bilateral
lung transplantation.
A complete description of the rules underlying
GEMs and our logic for processing them is beyond
the scope of this paper, and we limit our discussion
to the principles underlying our approach. We first
distribute a unit probability mass over the ICD-
10 codes or code combinations mapped to each
ICD-9 code, using logic that reflects the struc-
ture of GEMs and distributing probability mass
uniformly among comparable alternatives. From
these probabilities we compute a cumulative prob-
ability mass for each concept appearing in the
ICD-10 codes. For example, if an ICD-9 code
maps to four ICD-10 codes over which we dis-
tribute a uniform probability distibution, and a
given concept appears in two of them, we assign
the probability of 0.5 to that concept. For a given
EHR, we assign to each concept the highest prob-
ability it receives from any of the codes observed
for the EHR. Finally, we use the resulting concept
probabilities to weight positive training instances.
Negative instances still have unit weights, since
they correspond to concepts that can be unequivo-
cably ruled out based on the GEMs.
4.4 Code confidence model
The code confidence model produces a confidence
score for candidate codes generated by the hierar-
chical classification layers, using features derived
from the output of the code confidence models
described above. The code confidence model is
trained on data with ICD-10 codes. Whenever a
candidate code matches a code assigned by hu-
man annotators, a positive training instance is gen-
erated. Otherwise, a negative instance is gener-
ated, with sub-sampling. We report experiments
using logistic regression with `
1
and `
2
regulariza-
tion (Gao et al., 2007).
The definition of features used in the model re-
quires careful attention, because it is in the form of
the feature space that the proposed model differs
from a standard one-vs-all approach. To elucidate
the contrast we may start with a form of the feature
space that would correspond to one-vs-all classi-
fication. This can be achieved by specifying the
identity of a particular code in all feature names.
Then, the objective function for logistic regression
would decompose into independent learning sub-
problems, one for each code, producing a collec-
tion of one-vs-all classifiers. There are clear draw-
backs to this approach. If all parameters are re-
stricted to a specific code, the training data would
be fragmented along the same lines. Thus, even
if features derived from concepts may seem to en-
able generalization, in reality they would in each
case be estimated only from training instances cor-
responding to a single code, causing unnecessary
data sparsity.
This shortcoming can be overcome in logistic
regression simply by introducing generalized fea-
tures, without changing the rest of the model (Sub-
otin, 2011). Thus, in deriving features from scores
of concept confidence models we include only
those concepts which are expressed by the given
code, but we do not specify the identity of the code
in the feature names. In this way the weights for
these features are estimated at once from training
instances for all codes in which these concepts ap-
pear. We combine these generalized features with
the code-bound features described earlier. The lat-
ter should help us learn more specific predictors
for particular procedures, when such predictors
exist in the feature space.
While the scores of concept confidence mod-
els provide the basis for the feature space of the
code confidence model, there are multiple ways in
which features can be derived from these scores.
The simplest way is to take concept identity (op-
tionally specified by code identity) as the fea-
ture name and the confidence score as the feature
value. We supplement these features with features
based on score quantization. That is, we thresh-
old each concept confidence score at several points
and define binary features indicating whether the
score exceeds each of the thresholds. For both
these feature types, we generate separate features
for predictions of concept models trained on ICD-
9 data and concept models trained on ICD-10 data
in order to allow the code confidence model to
learn how useful predictions of concept confidence
models are, depending on the type of their training
data.
Both the concept confidence models and the
63
code confidence model can be trained on data with
ICD-10 codes. We are thus faced with the ques-
tion of how best to use this limited resource. The
simplest approach would be to train both types of
models on all available training data, but there is a
concern that predictions of the concept models on
their own training data would not reflect their out-
of-sample performance, and this would mislead
the code confidence model into relying on them
too much. An alternative approach, often called
stacked generalization (Wolpert, 1992), would be
to generate training data for the code confidence
model by running concept confidence models on
out-of-sample data. We compare the performance
of these approaches below.
5 Evaluation
5.1 Methodology
We evaluated the proposed model using a cor-
pus of 28,536 EHRs (individual clinical records),
compiled to represent a wide variety of clinical
contexts and supplied with ICD-10-PCS codes by
trained medical coders. The corpus was annotated
under the auspices of 3M Health Information Sys-
tems for the express purpose of developing auto-
coding technology for ICD-10. There was a total
of 51,082 PCS codes and 5,650 unique PCS codes
in the corpus, only 76 of which appeared in more
than 100 EHRs, and 2,609 of which appeared just
once. Multiple coders worked on some of the doc-
uments, but they were allowed to collaborate, pro-
ducing what was effectively a single set of codes
for each EHR. We held out about a thousand EHRs
for development testing and evaluation, each, us-
ing the rest for training. The same corpus, as well
as 175,798 outpatient surgery EHRs with ICD-9
procedure codes submitted for billing by a health
provider were also used to train hierarchical and
concept confidence models.
We evaluated auto-coding performance by a
modified version of mean reciprocal rank (MRR).
MRR is a common evaluation metric for systems
with ranked outputs. For a set of Q correct out-
puts with ranks rank
i
among all outputs, standard
MRR is computed as:
MRR =
1
Q
Q
?
i=1
1
rank
i
For example, a MRR value of 0.25 means that
that the correct answer has rank 4 on average. This
metric is designed for tasks where only one of the
outputs can be correct. When applied directly to
tasks where more than one output can be correct,
MRR unfairly penalizes cases with multiple cor-
rect outputs, increasing the rank of some correct
outputs on account of other, higher-ranked outputs
that are also correct. We modify MRR for our task
by ignoring correct outputs in the rank computa-
tions. In other words, the rank of a correct output
is computed as the number of higher-ranked incor-
rect outputs, plus one. This metric has the advan-
tage of summarizing the accuracy of an auto-coder
without reference to a particular choice of thresh-
old, which may be determined by business rules or
research considerations, as would be the case for
precision and recall.
One advantage of regularized logistic regres-
sion is that the value of 1 is often a near-optimal
setting for the regularization trade-off parameter.
This can save considerable computation time that
would be required for tuning this parameter for
each experimental condition. We have previously
observed that the value of 1 consistently produced
near-optimal results for the `
1
regularizer in con-
cept confidence models and for the `
2
regularizer
in the code confidence models, and we have used
this setting for all the experiments reported here.
For the code confidence model with `
1
-regularized
logistic regression we saw a slight improvement
with weaker regularization, and we report the best
result we obtained for this model below.
5.2 Results
The results are shown in Table 2. The top MMR
score of 0.572 corresponds to a micro-averaged F-
score of 0.485 (0.490 precision, 0.480 recall) when
the threshold is chosen to obtain approximately
equal values for recall and precision
3
. The best
result was obtained when:
? the concept models used bag-of-tokens fea-
tures (with the additional laterality features
described in Section 4.2);
? both concept models trained on ICD-9 data
and those trained on ICD-10 data were used;
? the code confidence model was trained on
data with predictions of concept models
trained on all of ICD-10 data (i.e., no
3
To put these numbers into perspective, note that the aver-
age accuracy of trained medical coders for ICD-10 has been
estimated to be 63% (HIMSS/WEDI, 2013).
64
data splitting for stacked generalization was
used);
? the code confidence model used all of the fea-
ture types described in Section 4.4;
? the code confidence model used logistic re-
gression with `
2
regularization.
We examine the impact of all these choices on
system performance in turn.
Model MRR
All data, all features, `
2
reg. 0.572
Concept model training:
Trained on ICD-10 only 0.558
Trained on ICD-9 only 0.341
Code model features:
One-vs-all 0.519
No code-bound features 0.553
No quantization features 0.560
Stacked generalization:
half & half data split 0.501
5-fold cross-validation 0.539
Code model algorithm:
`
1
regularization 0.528
Table 2: Evaluation results. Each row after the
first correponds to varying one aspect of the model
shown in the first row. See Section 5.3 for details
of the experimental conditions.
5.3 Discussion
Despite its apparent primitive nature, the bag-of-
token feature space for the concept confidence
models has turned out to provide a remarkably
strong baseline. Our experiments with frequency-
based weighting schemes for the feature values
and with features derived from text matches from
the UMLS concept dictionaries did not yield sub-
stantial improvements in the results. Thus, the use
of UMLS-based features, obtained using Apache
ConceptMapper, yielded a relative improvement
of 0.6% (i.e., 0.003 in absolute terms), but at the
cost of nearly doubling run-time processing time.
Nonetheless, we remain optimistic that more so-
phisticated features can benefit performance of the
concept models while maintaining their scalabil-
ity.
As can be seen from the table, both concept
models trained on ICD-9 data and those trained on
ICD-10 data contributed to the overall effective-
ness of the system. However, the contribution of
the latter is markedly stronger. This suggests that
further research is needed in finding the best ways
of exploiting ICD-9-coded data for ICD-10 auto-
coding. Given that data with ICD-9 codes is likely
to be more readily available than ICD-10 training
data in the foreseeable future, this line of investi-
gation holds potential for significant gains in auto-
coding performance.
For the choice of features used in the code con-
fidence model, the most prominent contribution is
made by the feature that generalize beyond spe-
cific codes, as discussed in Section 4.4. Adding
these features yields a 10% relative improvement
over the set of features equivalent to a one-vs-
all model. In fact, using the generalized features
alone (see the row marked ?no code-bound fea-
tures? in Table 2) gives a score only 0.02 lower
than the best result. As would be expected, gener-
alized features are particularly important for codes
with limited training data. Thus, if we restrict
our attention to codes with fewer than 25 training
instances (which account for 95% of the unique
codes in our ICD-10 training data), we find that
generalized features yielded a 25% relative im-
provement over the one-vs-all model (0.247 to
0.309). In contrast, for codes with over 100 train-
ing instances (which account for 1% of the unique
codes, but 36% of the total code volume in our
corpus) the relative improvement from generalized
features is less than 4% (0.843 to 0.876). These
numbers afford two further observations. First,
the model can be improved dramatically by adding
a few dozen EHRs per code to the training cor-
pus. Secondly, there is still much room for re-
search in mitigating the effects of data sparsity
and improving prediction accuracy for less com-
mon codes. Elsewhere in Table 2 we see that
quantization-based features contribute a modest
predictive value.
Perhaps the most surprising result of the series
came from investigating the options for using the
available ICD-10 training data, which act as train-
ing material both for concept confidence models
and the code confidence model. The danger of
training both type of models on the same corpus
is intuitively apparent. If the training instances
for the code model are generated by concept mod-
els whose training data included the same EHRs,
the accuracy of these concept predictions may not
65
reflect out-of-sample performance of the concept
models, causing the code model to rely on them
excessively.
The simplest implementation of Wolpert?s
stacked generalization proposal, which is intended
to guard against this risk, is to use one part of the
corpus to train one predictive layer and use its pre-
dictions on the another part of the corpus to train
the other layer. The result in Table 2 (see the
row marked ?half & half data split?) shows that
the resulting increase in sparsity of the training
data for both models leads to a major degradation
of the system?s performance, even though at run-
time concept models trained on all available data
are used. We also investigated a cross-validation
version of stacked generalization designed to mit-
igate against this fragmentation of training data.
We trained a separate set of concept models on the
training portion of each cross-validation fold, and
ran them on the held-out portion. The training set
for the code confidence model was then obtained
by combining these held-out portions. At run-
time, concept models trained on all of the avail-
able data were used. However, as intuitively com-
pelling as the arguments motivating this procedure
may be, the results were not competitive with the
baseline approach of using all available training
data for all the models.
Finally, we found that an `
2
regularizer per-
formed clearly better than an `
1
regularizer for the
code confidence model, even though we set the `
2
trade-off constant to 1 and tuned the `
1
trade-off
constant on the development test set. This is in
contrast to concept confidence models, where we
observed slightly better results with `
1
regulariza-
tion than with `
2
regularization.
6 Conclusion
We have described a system for predicting ICD-
10-PCS codes from the clinical narrative con-
tained in EHRs. The proposed approach seeks to
mitigate the sparsity of training data with manu-
ally assigned ICD-10-PCS codes in three ways:
through an intermediate abstraction to clinical
concepts, through the use of data with ICD-9
codes to train concept confidence models, and
through the use of a code confidence model
whose parameters can generalize beyond individ-
ual codes. Our experiments show promising re-
sults and point out directions for further research.
Acknowledgments
We would like to thank Ron Mills for provid-
ing the crosswalk between ICD-10-PCS codes and
clinical concepts; Guoli Wang, Michael Nossal,
Kavita Ganesan, Joel Bradley, Edward Johnson,
Lyle Schofield, Michael Connor, Jean Stoner and
Roxana Safari for helpful discussions relating to
this work; and the anonymous reviewers for their
constructive criticism.
References
Sean Benson. 2006. Computer-assisted Coding Soft-
ware Improves Documentation, Coding, Compli-
ance, and Revenue. Perspectives in Health Infor-
mation Management, CAC Proceedings, Fall 2006.
Centers for Medicare & Medicaid Services. 2014.
General Equivalence Mappings. Documentation
for Technical Users. Electronically published at
cms.gov.
Chute CG, Yang Y, Buntrock J. 1994. An evalua-
tion of computer assisted clinical classification algo-
rithms. Proc Annu Symp Comput Appl Med Care.,
1994:162?6.
Jianfeng Gao, Galen Andrew, Mark Johnson, Kristina
Toutanova. 2007. A Comparative Study of Param-
eter Estimation Methods for Statistical Natural Lan-
guage Processing. ACL 2007.
Daniel T. Heinze, Mark L. Morsch, Ronald E. Shef-
fer, Jr., Michelle A. Jimmink, Mark A. Jennings,
William C. Morris, and Amy E. W. Morsch. 2000.
LifeCode
TM
? A Natural Language Processing Sys-
tem for Medical Coding and Data Mining. AAAI
Proceedings.
Daniel T. Heinze, Mark Morsch, Ronald Sheffer,
Michelle Jimmink, Mark Jennings, William Mor-
ris, and Amy Morsch. 2001. LifeCode: A De-
ployed Application for Automated Medical Coding.
AI Magazine, Vol 22, No 2.
HIMSS/WEDI. 2013. ICD-10 National Pilot Pro-
gram Outcomes Report. Electronically published at
himss.org.
Yuankai Jiang, Michael Nossal, and Philip Resnik.
2006. How Does the System Know It?s Right?
Automated Confidence Assessment for Compliant
Coding. Perspectives in Health Information Man-
agement, Computer Assisted Coding Conference
Proceedings, Fall 2006.
Leah Larkey and W. Bruce Croft. 1995. Automatic As-
signment of ICD9 Codes To Discharge Summaries.
Technical report, Center for Intelligent Information
Retrieval at University of Massachusetts.
66
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu
Medori, Julia and Fairon, C?edrick. 2010. Machine
Learning and Features Selection for Semi-automatic
ICD-9-CM Encoding. Proceedings of the NAACL
HLT 2010 Second Louhi Workshop on Text and Data
Mining of Health Documents, 2010: 84?89.
Ronald E. Mills. 2013. Methods using multi-
dimensional representations of medical codes. US
Patent Application US20130006653.
S.V. Pakhomov, J.D. Buntrock, and C.G. Chute. 2006.
Automating the assignment of diagnosis codes to
patient encounters using example-based and ma-
chine learning techniques. J Am Med Inform Assoc,
13(5):516?25.
Adler Perotte, Rimma Pivovarov, Karthik Natarajan,
Nicole Weiskopf, Frank Wood, No?emie Elhadad .
2014. Diagnosis code assignment: models and eval-
uation metrics. J Am Med Inform Assoc, 21(2):231?
7.
Pestian, JP, Brew C, Matykiewicz P, Hovermale DJ,
Johnson N, Bretonnel Cohen K, and Duch W. 2007.
A shared task involving multi-label classification
of clinical free text. Proceedings ACL: BioNLP,
2007:97?104.
Philip Resnik, Michael Niv, Michael Nossal, Gregory
Schnitzer, Jean Stoner, Andrew Kapit, and Richard
Toren. 2006. Using intrinsic and extrinsic metrics
to evaluate accuracy and facilitation in computer-
assisted coding.. Perspectives in Health Informa-
tion Management, Computer Assisted Coding Con-
ference Proceedings, Fall 2006.
Berthier Ribeiro-Neto, Alberto H.F. Laender and Lu-
ciano R.S. de Lima. 2001. An experimental study
in automatically categorizing medical documents.
Journal of the American Society for Information Sci-
ence and Technology, 52(5): 391?401.
Mary H. Stanfill, Margaret Williams, Susan H. Fenton,
Robert A. Jenders, and William R. Hersh. 2010.
A systematic literature review of automated clinical
coding and classification systems. J Am Med Inform
Assoc., 17(6): 646?651.
Michael Subotin. 2011. An exponential translation
model for target language morphology. ACL 2011.
David H. Wolpert. 1992. Stacked Generalization.
Neural Networks, 5:241?259.
67
