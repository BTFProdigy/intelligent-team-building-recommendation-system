Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 323?330, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Practically Unsupervised Learning Method to Identify Single-Snippet
Answers to Definition Questions on the Web
Ion Androutsopoulos and Dimitrios Galanis
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34, Athens, Greece
Abstract
We present a practically unsupervised
learning method to produce single-snippet
answers to definition questions in ques-
tion answering systems that supplement
Web search engines. The method exploits
on-line encyclopedias and dictionaries to
generate automatically an arbitrarily large
number of positive and negative definition
examples, which are then used to train an
SVM to separate the two classes. We show
experimentally that the proposed method
is viable, that it outperforms the alterna-
tive of training the system on questions
and news articles from TREC, and that it
helps the search engine handle definition
questions significantly better.
1 Introduction
Question answering (QA) systems for document col-
lections typically aim to identify in the collections
text snippets (e.g., 50 or 250 characters long) or ex-
act answers (e.g., names, dates) that answer natu-
ral language questions submitted by their users. Al-
though they are commonly evaluated on newspaper
archives, as in the TREC QA track, QA systems can
also supplement Web search engines, to help them
return snippets, as opposed to Web pages, that pro-
vide more directly the information users require.
Most current QA systems first classify the input
question into one of several categories (e.g., ques-
tions asking for locations, persons, etc.), producing
expectations for types of named entities that must
be present in the answer (locations, person names,
etc.). Using the question?s terms as a query, an infor-
mation retrieval (IR) system identifies relevant doc-
uments. Snippets of these documents are then se-
lected and ranked, using criteria such as whether or
not they contain the expected types of named enti-
ties, the percentage of the question?s terms they con-
tain, etc. The system then outputs the most highly-
ranked snippets, or named entities therein.
The approach highlighted above performs poorly
with definition questions (e.g., ?What is gasohol??,
?Who was Duke Ellington??), because definition
questions do not generate expectations for particular
types of named entities, and they typically contain
only a single term. Definition questions are particu-
larly common; in the QA track of TREC-2001, where
the distribution of question types reflected that of
real user logs, 27% of the questions were requests
for definitions. Of course, answers to many defini-
tion questions can be found in on-line encyclopedias
and dictionaries.1 There are always, however, new
or less widely used terms that are not included in
such resources, and this is also true for many names
of persons and products. Hence, techniques to dis-
cover definitions in ordinary Web pages and other
document collections are valuable. Definitions of
this kind are often ?hidden? in oblique contexts (e.g.,
?He said that gasohol, a mixture of gasoline and
ethanol, has been great for his business.?).
In recent work, Miliaraki and Androutsopoulos
(2004), hereafter M&A, proposed a method we call
1See, for example, Wikipedia (http://www.wikipedia.org/).
WordNet?s glosses are another on-line source of definitions.
323
DEFQA, which handles definition questions. The
method assumes that a question preprocessor sep-
arates definition from other types of questions, and
that in definition questions this module also identi-
fies the term to be defined, called the target term.2
The input to DEFQA is a (possibly multi-word) tar-
get term, along with the r most highly ranked docu-
ments that an IR system returned for that term. The
output is a list of k 250-character snippets from the
r documents, at least one of which must contain an
acceptable short definition of the target term, much
as in the QA track of TREC-2000 and TREC-2001.3
We note that since 2003, TREC requires defini-
tion questions to be answered by lists of comple-
mentary snippets, jointly providing a range of in-
formation nuggets about the target term (Voorhees,
2003). In contrast, here we focus on locating single-
snippet definitions. We believe this task is still in-
teresting and of practical use. For example, a list
of single-snippet definitions accompanied by their
source URLs is a good starting point for users of
search engines wishing to obtain definitions. Single-
snippet definitions can also be useful in information
extraction, where the templates to be filled in often
require short entity descriptions. We also note that
the post-2003 TREC task has encountered evaluation
problems, because it is difficult to agree on which
nuggets should be included in the multi-snippet def-
initions (Hildebrandt et al, 2004). In contrast, our
experimental results of Section 4 indicate strong
inter-assessor agreement for single-snippet answers,
suggesting that it is easier to agree upon what con-
stitutes an acceptable single-snippet definition.
DEFQA relies on an SVM, which is trained to clas-
sify 250-character snippets that have the target term
at their centre, hereafter called windows, as accept-
able definitions or non-definitions.4 To train the
SVM, a collection of q training target terms is used;
M&A used the target terms of definition questions
from TREC-2000 and TREC-2001. The terms are
submitted to an IR system, which returns the r most
2Alternatively, the user can be asked to specify explicitly the
question type and target term via a form-based interface.
3Definition questions were not considered in TREC-2002.
4See, for example, Scholkopf and Smola (2002) for in-
formation on SVMs. Following M&A, we use a linear SVM,
as implemented by Weka?s SMO class (http://www.cs.waikato.
ac.nz/ml/weka/). The windows may be shorter than 250 charac-
ters, when the surrounding text is limited.
highly ranked documents per target term. The win-
dows of the q ? r resulting documents are tagged
as acceptable definitions or non-definitions, and be-
come the training instances of the SVM. At run time,
when a definition question is submitted, the r top-
ranked documents are obtained, their windows are
collected, and for each window the SVM returns a
score indicating how confident it is that the window
is a definition. The k windows with the highest con-
fidence scores are then reported to the user.
The SVM actually operates on vector representa-
tions of the windows, that comprise the verdicts or
attributes of previous methods by Joho and Sander-
son (2000) and Prager et al (2002), as well as
attributes corresponding to automatically acquired
lexical patterns. On TREC-2000 and TREC-2001
data, M&A found that DEFQA clearly outperformed
the original methods of Joho and Sanderson and
Prager et al Their best configuration answered cor-
rectly 73% of 160 definition questions in a cross-
validation experiment with k = 5, r = 50, q = 160.
A limitation of DEFQA is that it cannot be trained
easily on new document collections, because it re-
quires the training windows to be tagged as defini-
tions or non-definitions. In the experiments of M&A,
there were 18,473 training windows. Tagging them
was easy, because the windows were obtained from
TREC questions and documents, and the TREC or-
ganizers provide Perl patterns that can be used to
judge whether a snippet from TREC?s documents is
among the acceptable answers of a TREC question.5
For non-TREC questions and document collections,
however, where such patterns are unavailable, sep-
arating thousands of training windows into the two
categories by hand is a laborious task.
In this paper, we consider the case where DE-
FQA is used as an add-on to a Web search engine.
There are three training alternatives in this setting:
(i) train DEFQA on TREC questions and documents;
(ii) train DEFQA on a large collection of manually
tagged training windows obtained from Web pages
that the search engine returned for training target
terms; or (iii) devise techniques to tag automatically
the training windows of (ii). We have developed a
technique along alternative (iii), which exploits on-
5The patterns? judgements are not always perfect, which in-
troduces some noise in the training examples.
324
line encyclopedias and dictionaries. This allows us
to generate and tag automatically an arbitrarily large
number of training windows, in effect converting
DEFQA to an unsupervised method. We show ex-
perimentally that the new unsupervised method is
viable, that it outperforms alternative (i), and that it
helps the search engine handle definition questions
significantly better than on its own.
2 Attributes of DEFQA
DEFQA represents each window as a vector compris-
ing the values of the following attributes:6
SN: The ordinal number of the window in the
document, in our case Web page, it originates from.
The intuition is that windows that mention the target
term first in a document are more likely to define it.
WC: What percentage of the 20 words that are
most frequent across all the windows of the tar-
get term are present in the particular window rep-
resented by the vector. A stop-list and a stemmer are
applied first when computing WC .7 In effect, the 20
most frequent words form a simplistic centroid of all
the candidate answers, and WC measures how close
the vector?s window is to this centroid.
RK: The ranking of the Web page the window
originates from, as returned by the search engine.
Manual patterns: 13 binary attributes, each sig-
nalling whether or not the window matches a differ-
ent manually constructed lexical pattern (e.g., ?tar-
get, a/an/the?, as in ?Tony Blair, the British prime
minister?). The patterns are those used by Joho and
Sanderson, and four more added by M&A. They are
intended to perform well across text genres.
Automatic patterns: A collection of m binary at-
tributes, each showing if the window matches a dif-
ferent automatically acquired lexical pattern. The
patterns are sequences of n tokens (n ? {1, 2, 3})
that must occur either directly before or directly af-
ter the target term (e.g., ?target, which is?). These
patterns are acquired as follows. First, all the n-
grams that occur directly before or after the target
terms in the training windows are collected. The n-
grams that have been encountered at least 10 times
are considered candidate patterns. From those, the
6SN and WC originate from Joho and Sanderson (2000).
7We use the 100 most frequent words of the British National
Corpus as the stop-list, and Porter?s stemmer.
m patterns with the highest precision scores are re-
tained, where precision is the number of training
definition windows the pattern matches divided by
the total number of training windows it matches. We
set m to 200, the value that led to the best results
in the experiments of M&A. The automatically ac-
quired patterns allow the system to detect definition
contexts that are not captured by the manual pat-
terns, including genre-specific contexts.
M&A also explored an additional attribute, which
carried the verdict of Prager et al?s WordNet-based
method (2002). However, they found the additional
attribute to lead to no significant improvements, and,
hence, we do not use it. We have made no attempt to
extend the attribute set of M&A; for example, with
attributes showing if the window contains the target
term in italics, if the window is part of a list that
looks like a glossary, or if the window derives from
an authority Web page. We leave such extensions
for future work. Our contribution is the automatic
generation of training examples.
3 Generating training examples
When training DEFQA on windows from Web pages,
a mechanism to tag the training windows as defi-
nitions or non-definitions is required. Rather than
tagging them manually, we use a measure of how
similar the wording of each training window is to
the wording of definitions of the same target term
obtained from on-line encyclopedias and dictionar-
ies. This is possible because we pick training target
terms for which there are several definitions in dif-
ferent on-line encyclopedias and dictionaries; here-
after we call these encyclopedia definitions.8 Train-
ing windows whose wording is very similar to that
of the corresponding encyclopedia definitions are
tagged as definition windows (positive examples),
while windows whose wording differs significantly
from the encyclopedia definitions are tagged as non-
definitions (negative examples). Training windows
for which the similarity score does not indicate great
similarity or dissimilarity to the wording of the en-
cyclopedia definitions are excluded from DEFQA?s
8We use randomly selected entries from the index of
http://www.encyclopedia.com/ as training terms, and Google?s
?define:? feature, that returns definitions from on-line encyclo-
pedias and dictionaries, to obtain the encyclopedia definitions.
325
training, as they cannot be tagged as positive or neg-
ative examples with sufficiently high confidence.
Note that encyclopedia definitions are used only
to tag training windows. Once the system has been
trained, it can be used to discover on ordinary Web
pages definitions of terms for which there are no en-
cyclopedia definitions, and indeed this is the main
purpose of the system. Note also that we train DE-
FQA on windows obtained from Web pages returned
by the search engine for training terms. This allows
it to learn characteristics of the particular search en-
gine being used; for example, what weight to as-
sign to RK , depending on how much the search
engine succeeds in ranking pages containing defi-
nitions higher. More importantly, it allows DEFQA
to select lexical patterns that are indicative of def-
initions in Web pages, as opposed to patterns that
are indicative of definitions in electronic encyclope-
dias and dictionaries. The latter explains why we
do not train DEFQA directly on encyclopedia defi-
nitions; another reason is that DEFQA requires both
positive and negative examples, while encyclopedia
definitions provide only positive ones.
We now explain how we compute the similar-
ity between a training window and the collection
C of encyclopedia definitions for the window?s tar-
get term. We first remove stop-words, punctua-
tion, other non-alphanumeric characters and the tar-
get term from the training window, and apply a stem-
mer, leading to a new form W of the training win-
dow. We then compute the similarity of W to C as:
sim(W,C) = 1/|W | ? ?|W |i=1sim(wi, C)
where |W | is the number of distinct words in W , and
sim(wi, C) is the similarity of the i-th distinct word
of W to C, defined as follows:
sim(wi, C) = fdef (wi, C) ? idf (wi)
fdef (wi, C) is the percentage of definitions in C that
contain wi, and idf (wi) is the inverse document fre-
quency of wi in the British National Corpus (BNC):
idf (wi) = 1 + log
N
df(wi)
N is the number of documents in BNC, and df (wi)
the number of BNC documents where wi occurs; if
wi does not occur in BNC, we use the lowest df score
of BNC. sim(wi, C) is highest for words that oc-
cur in all the encyclopedia definitions and are used
rarely in English. A training window with a large
proportion of such words most probably defines the
target term. More formally, given two thresholds t+
and t? with t? ? t+, we tag W as a positive ex-
ample if sim(W,C) ? t+, as a negative example if
sim(W,C) ? t?, and we exclude it from the train-
ing of DEFQA if t? < sim(W,C) < t+. Hereafter,
we refer to this method of generating training exam-
ples as the similarity method.
To select reasonable values for t+ and t?, we con-
ducted a preliminary experiment for t? = t+ = t;
i.e., both thresholds were set to the same value t
and no training windows were excluded. We used
q = 130 training target terms from TREC definition
questions, for which we had multiple encyclopedia
definitions. For each term, we collected the r = 10
most highly ranked Web pages.9 To alleviate the
class imbalance problem, whereby the positive ex-
amples (definitions) are much fewer than the nega-
tive ones (non-definitions), we kept only the first 5
windows from each Web page (SN ? 5), based on
the observation that windows with great SN scores
are almost certainly non-definitions; we do the same
in the training stage of all the experiments of this pa-
per, and at run-time, when looking for windows to
report, we ignore windows with SN > 5. From the
resulting collection of training windows, we selected
randomly 400 windows, and tagged them both man-
ually and via the similarity method, with t ranging
from 0 to 1. Figures 1 and 2 show the precision and
recall of the similarity method on positive and neg-
ative training windows, respectively, for varying t.
Here, positive precision is the percentage of training
windows the similarity method tagged as positive
examples (definitions) that were indeed positive; the
true classes of the training windows were taken to be
those assigned by the human annotators. Positive re-
call is the percentage of truly positive examples that
the similarity method tagged as positive. Negative
precision and negative recall are defined similarly.
Figures 1 and 2 indicate that there is no single
threshold t that achieves both high positive preci-
sion and high negative precision. To be confident
9In all our experiments, we used the Altavista search engine.
326
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1similarity threshold
p
r
e
c
i
s
i
o
n
-
r
e
c
a
l
l
precision of positive examplesrecall of positive examples
Figure 1: Positive precision and recall
that the training windows the similarity method will
tag as positive examples are indeed positive (high
positive precision), one has to set t close to 1; and to
be confident that the training windows the similar-
ity method will tag as negative examples are indeed
negative (high negative precision), t has to be set
close to 0. This is why we use two separate thresh-
olds and discard the training windows whose simi-
larity score is between t? and t+. Figures 1 and 2
also indicate that in both positive and negative ex-
amples the similarity method achieves perfect pre-
cision only at the cost of very low recall; i.e., if we
insist that all the resulting training examples must
have been tagged correctly (perfect positive and neg-
ative precision), the resulting examples will be very
few (low positive and negative recall). There is also
another consideration when selecting t? and t+: the
ratio of positive to negative examples that the sim-
ilarity method generates must be approximately the
same as the true ratio before discarding any training
windows, in order to avoid introducing an artificial
bias in the training of DEFQA?s SVM; the true ratio
among the 400 training windows before discarding
any windows was approximately 0.37 : 1.
Based on the considerations above, in the remain-
ing experiments of this paper we set t+ to 0.5. In
Figure 1, this leads to a positive precision of 0.72
(and positive recall 0.49), which does not improve
much by adopting a larger t+, unless one is willing
to set t+ at almost 1 at the price of very low posi-
tive recall. In the case of t?, setting it to any value
less than 0.34 leads to a negative precision above
0.9, though negative recall drops sharply as t? ap-
proaches 0 (Figure 2). For example, setting t? to
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1similarity threshold
p
r
e
c
i
s
i
o
n
-
r
e
c
a
l
l
precision of negative examples
recall of negative examples
Figure 2: Negative precision and recall
0.32, leads to 0.92 negative precision, 0.75 negative
recall, and approximately the same positive to nega-
tive ratio (0.31 : 1) as the true observed ratio. In the
experiments of Section 4, we keep t+ fixed to 0.5,
and set t? to the value in the range (0, 0.34) that
leads to the positive to negative ratio that is closest
to the true ratio we observed in the 400 windows.
The high negative precision we achieve (> 0.9)
suggests that the resulting negative examples are al-
most always truly negative. In contrast, the lower
positive precision (0.72) indicates that almost one in
every four resulting positive examples is in reality a
non-definition. This is a point where our similarity
method needs to be improved; we return to this point
in Section 6. Our experiments, however, show that
despite this noise, the similarity method already out-
performs the alternative of training DEFQA on TREC
data. Note also that once the thresholds have been
selected, we can generate automatically an arbitrar-
ily large set of training examples, by starting with a
sufficiently large number q of training terms to com-
pensate for discarded training examples.
4 Evaluation
We tested two different forms of DEFQA. The first
one, dubbed DEFQAt , was trained on the q = 160
definition questions of TREC-2000 and TREC-2001
and the corresponding TREC documents, resulting in
3,800 training windows.10 The second form of DE-
10For each question, the TREC organizers provide the 50 most
highly ranked documents that an IR engine returned from the
TREC document collection. We keep the top r = 10 of these
documents, while M&A kept all 50. Furthermore, as discussed
in Section 3, we retain up to the first 5 windows from each doc-
327
FQA, dubbed DEFQAs , was trained via the similarity
method, with q = 480 training target terms, leading
to 7,200 training windows; as discussed in Section 3,
one of the advantages of the similarity method is that
one can generate an arbitrarily large set of training
windows. As in the preliminary experiment of Sec-
tion 3, r (Web pages per target term) was set to 10
in both systems. To simplify the evaluation and test
DEFQA in a more demanding scenario, we set k to
1, i.e., the systems were allowed to return only one
snippet per question, as opposed to the more lenient
k = 5 in the experiments of M&A.
We also wanted a measure of how well DEFQAt
and DEFQAs perform compared to a search engine
on its own. For this purpose, we compared the per-
formance of the two systems to that of a baseline,
dubbed BASE1 , which always returns the first win-
dow of the Web page the search engine ranked first.
In a search engine that highlights question terms
in the returned documents, the snippet returned by
BASE1 is presumably the first snippet a user would
read hoping to find an acceptable definition. To
study how much DEFQAt and DEFQAs improve upon
random behaviour, we also compared them to a sec-
ond baseline, BASEr , which returns a randomly se-
lected window among the first five windows of all r
Web pages returned by the search engine.
All four systems were evaluated on 81 unseen tar-
get terms. Their responses were judged indepen-
dently by two human assessors, who had to mark
each response as containing an acceptable short def-
inition or not. As already pointed out, DEFQAt and
DEFQAs consult encyclopedia definitions only dur-
ing training, and at run time the systems are in-
tended to be used with terms for which no ency-
clopedia definitions are available. During this eval-
uation, however, we deliberately chose the 81 test
terms from the index of an on-line encyclopedia.
This allowed us to give the encyclopedia?s defini-
tions to the assessors, to help them judge the accept-
ability of the single-snippet definitions the systems
located on Web pages; many terms where related to,
for example, medicine or biology, and without the
encyclopedia?s definitions the assessors would not
be aware of their meanings. The following is a snip-
pet returned correctly by DEFQAs for ?genome?:
ument. This is why we have fewer training windows than M&A.
discipline comparative genomics functional genomics bioinfor-
matics the emergence of genomics as a discipline in 1920 , the
term genome was proposed to denote the totality of all genes on
all chromosomes in the nucleus of a cell . biology has. . .
while what follows is a non-definition snippet re-
turned wrongly by BASE1 :
what is a genome national center for biotechnology information
about ncbi ncbi at a glance a science primer databases. . .
The examples illustrate the nature of the snippets
that the systems and assessors had to consider. The
snippets often contain phrases that acted as links in
the original pages, or even pieces of programming
scripts that our rudimental preprocessing failed to
remove. (We remove only HTML tags, and apply
a simplistic tokenizer.) Nevertheless, in most cases
the assessors had no trouble agreeing whether or
not the resulting snippets contained acceptable short
definitions. KCo was 0.80, 0.81, 0.90, 0.89, and
0.86 in the assessment of the responses of DEFQAs ,
DEFQAt , BASEr , BASE1 , and all responses, respec-
tively, indicating strong inter-assessor agreement.11
The agreement was slightly lower in DEFQAs and
DEFQAt , because there were a few marginally ac-
ceptable or truncated definitions the assessors were
uncertain about. There were also 4 DEFQAs answers
and 3 BASE1 answers that defined secondary mean-
ings of the target terms; e.g., apart from a kind of
lizard, ?gecko? is also the name of a graphics engine,
and ?Exodus? is also a programme for ex-offenders.
Such answers were counted as wrong, though this
may be too strict. With a larger k, there would be
space to return both the main and secondary mean-
ings, and the evaluation could require this.
Table 1 shows that DEFQAs answered correctly
approximately 6 out of 10 definition questions. This
is lower than the score reported by M&A (73%),
but remarkably high given that in our evaluation
the systems were allowed to return only one snip-
pet per question; i.e., the task was much harder than
in M&A?s experiments. DEFQAs answered correctly
more than twice as many questions as DEFQAt , de-
spite the fact that its training data contained a lot of
noise. (Single-tailed difference-of-proportions tests
show that all the differences of Table 1 are statisti-
11We follow the notation of Di Eugenio and Glass (2004).
The KS&C figures were identical. The 2 ? P (A) ? 1 figures
were 0.80, 0.85, 0.95, 0.95, and 0.89 respectively.
328
assessor 1 assessor 2 average
BASEr 14.81 (12) 14.81 (12) 14.81 (12)
BASE1 14.81 (12) 12.35 (10) 13.58 (11)
DEFQAt 25.93 (21) 25.93 (21) 25.93 (21)
DEFQAs 55.56 (45) 60.49 (49) 58.02 (47)
Table 1: Percentage of questions answered correctly
cally significant at ? = 0.001.) The superiority of
DEFQAs appears to be mostly due to its automati-
cally acquired patterns. DEFQAt too was able to ac-
quire several good patterns (e.g., ?by target?, ?known
as target?, ?target, which is?, ?target is used in?), but
its pattern set alo comprises a large number of irrel-
evant n-grams; this had also been observed by M&A.
In contrast, the acquired pattern set of DEFQAs is
much cleaner, with much fewer irrelevant n-grams,
which is probably due to the largest, almost double,
number of training windows. Furthermore, the pat-
tern set of DEFQAs contains many n-grams that are
indicative of definitions on the Web. For example,
many Web pages that define terms contain text of the
form ?What is a target? A target is. . . ?, and DEFQAs
has discovered patterns of the form ?what is a/an/the
target?, ?? A/an/the target?, etc. It has also discov-
ered patterns like ?FAQ target?, ?home page target?,
?target page? etc., that seem to be good indications
of Web windows containing definitions.
Overall, DEFQA?s process of acquiring lexical pat-
terns worked better in DEFQAs than in DEFQAt , and
we believe that the performance of DEFQAs could be
improved further by acquiring more than 200 pat-
terns; we hope to investigate this in future work,
along with an investigation of how the performance
of DEFQAs relates to q, the number of training target
terms. Finally, note that the scores of both baselines
are very poor, indicating that DEFQAs performs sig-
nificantly better than picking the first, or a random
snippet among those returned by the search engine.
5 Related work
Definition questions have recently attracted several
QA researchers. Many of the proposed approaches,
however, rely on manually crafted patterns or heuris-
tics to identify definitions, and do not employ learn-
ing algorithms (Liu et al, 2003; Fujii and Ishikawa,
2004; Hildebrandt et al, 2004; Xu et al, 2004).
Ng et al (2001) use machine learning (C5 with
boosting) to classify and rank candidate answers in
a general QA system, but they do not treat defi-
nition questions in any special way; consequently,
their worst results are for ?What. . . ?? questions,
that presumably include definition questions. Itty-
cheriah and Roukos (2002) employ a maximum en-
tropy model to rank candidate answers in a general-
purpose QA system. Their maximum entropy model
uses a very rich set of attributes, that includes 8,500
n-gram patterns. Unlike our work, their n-grams are
five or more words long, they are coupled to two-
word question prefixes, and, in the case of definition
questions, they do not need to be anchored at the tar-
get term. The authors, however, do not provide sep-
arate performance figures for definition questions.
Blair-Goldensohn et al (2003) focus on defini-
tion questions, but aim at producing coherent multi-
snippet definitions, rather than single-snippet defi-
nitions. The heart of their approach is a compo-
nent that uses machine learning (Ripper) to identify
sentences that can be included in the multi-sentence
definition. This component plays a role similar to
that of our SVM, but it is intended to admit a larger
range of sentences, and appears to employ only at-
tributes conveying the ordinal number of the sen-
tence in its document and the frequency of the target
term in the sentence?s context.
Since TREC-2003, several researchers have pro-
posed ways to generate multi-snippet definitions
(Cui et al, 2004; Fujii and Ishikawa, 2004; Hilde-
brandt et al, 2004; Xu et al, 2004). The typical
approach is to locate definition snippets, much as
in our work, and then report definition snippets that
are sufficiently different; most of the proposals use
some form of clustering to avoid reporting redun-
dant snippets. Such methods could also be applied
to DEFQA, to extend it to the post-2003 TREC task.
On-line encyclopedias and dictionaries have been
used to handle definition questions in the past, but
not as in our work. Hildebrandt et al (2004) look up
target terms in encyclopedias and dictionaries, and
then, knowing the answers, try to find supporting
evidence for them in the TREC document collection.
Xu et al (2004) collect from on-line encyclopedias
and dictionaries words that co-occur with the tar-
get term; these words and their frequencies are then
used as a centroid of the target term, and candidate
329
answers are ranked by computing their similarity to
the centroid. This is similar to our WC attribute.
Cui et al (2004) also employ a form of centroid,
comprising words that co-occur with the target term.
The similarity to the centroid is taken into consider-
ation when ranking candidate answers, but it is also
used to generate training examples for a learning
component that produces soft patterns, in the same
way that we use the similarity method to produce
training examples for the SVM. As in our work, the
training examples that the centroid generates may
be noisy, but the component that produces soft pat-
terns manages to generalize over the noise. To the
best of our knowledge, this is the only other unsu-
pervised learning approach for definition questions
that has been proposed. We hope to compare the
two approaches experimentally in future work. For
the moment, we can only point out that Cui et al?s
centroid approach generates only positive examples,
while our similarity method generates both positive
and negative ones; this allows us to use a principled
SVM learner, as opposed to Cui et al?s more ad hoc
soft patterns that incorporate only positive examples.
6 Conclusions and future work
We presented an unsupervised method to learn to lo-
cate single-snippet answers to definition questions
in QA systems that supplement Web search en-
gines. The method exploits on-line encyclopedias
and dictionaries to generate automatically an arbi-
trarily large number of positive and negative defini-
tion examples, which are then used to train an SVM
to separate the two classes. We have shown experi-
mentally that the proposed method is viable, that it
outperforms training the QA system on TREC data,
and that it helps the search engine handle definition
questions significantly better than on its own.
We have already pointed out the need to improve
the positive precision of the training examples. One
way may be to combine our similarity method with
Cui et al?s centroids. We also plan to study the ef-
fect of including more automatically acquired pat-
terns and using more training target terms. Finally,
our method can be improved by including attributes
for the layout and authority of Web pages.
References
S. Blair-Goldensohn, K.R. McKeown, and A.H. Schlaik-
jer. 2003. A hybrid approach for answering defi-
nitional questions. Technical Report CUCS-006-03,
Columbia University.
H. Cui, M.-Y. Kan, and T.-S. Chua. 2004. Unsupervised
learning of soft patterns for generating definitions from
online news. In Proceedings of WWW-2004, pages
90?99, New York, NY.
B. Di Eugenio and M. Glass. 2004. The kappa statistic:
A second look. Comput. Linguistics, 30(1):95?101.
A. Fujii and T. Ishikawa. 2004. Summarizing encyclo-
pedic term descriptions on the Web. In Proceedings of
COLING-2004, pages 645?651, Geneva, Switzerland.
W. Hildebrandt, B. Katz, and J. Lin. 2004. An-
swering definition questions using multiple knowledge
sources. In Proceedings of HLT-NAACL 2004, pages
49?56, Boston, MA.
A. Ittycheriah and S. Roukos. 2002. IBM?s statistical
question answering system ? TREC-11. In Proceed-
ings of TREC-2002.
H. Joho and M. Sanderson. 2000. Retrieving descriptive
phrases from large amounts of free text. In Proc. of the
9th ACM Conference on Information and Knowledge
Management, pages 180?186, McLean, VA.
B. Liu, C.W. Chin, and H.T. Ng. 2003. Mining topic-
specific concepts and definitions on the Web. In Pro-
ceedings of WWW-2003, Budapest, Hungary.
S. Miliaraki and I. Androutsopoulos. 2004. Learning to
identify single-snippet answers to definition questions.
In Proceedings of COLING-2004, pages 1360?1366,
Geneva, Switzerland.
H.T. Ng, J.L.P. Kwan, and Y. Xia. 2001. Question an-
swering using a large text database: A machine learn-
ing approach. In Proceedings of EMNLP-2001, pages
67?73, Pittsburgh, PA.
J. Prager, J. Chu-Carroll, and K. Czuba. 2002. Use of
WordNet hypernyms for answering what-is questions.
In Proceedings of TREC-2001.
B. Scholkopf and A. Smola. 2002. Learning with ker-
nels. MIT Press.
E.M. Voorhees. 2003. Evaluating answers to definition
questions. In Proceedings of HLT-NAACL 2003, pages
109?111, Edmonton, Canada.
J. Xu, R. Weischedel, and A. Licuanan. 2004. Eval-
uation of an extraction-based approach to answering
definitional questions. In Proceedings of SIGIR-2004,
pages 418?424, Sheffield, U.K.
330
Proceedings of the EACL 2009 Demonstrations Session, pages 17?20,
Athens, Greece, 3 April 2009. c?2009 Association for Computational Linguistics
An Open-Source Natural Language Generator for OWL Ontologies and
its Use in Prote?ge? and Second Life
Dimitrios Galanis?, George Karakatsiotis?, Gerasimos Lampouras?, Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Athens, Greece
+Digital Curation Unit, Research Centre ?Athena?, Athens, Greece
Abstract
We demonstrate an open-source natural
language generation engine that produces
descriptions of entities and classes in En-
glish and Greek from OWL ontologies that
have been annotated with linguistic and
user modeling information expressed in
RDF. We also demonstrate an accompany-
ing plug-in for the Prote?ge? ontology editor,
which can be used to create the ontology?s
annotations and generate previews of the
resulting texts by invoking the generation
engine. The engine has been embedded in
robots acting as museum tour guides in the
physical world and in Second Life; here
we demonstrate the latter application.
1 Introduction
NaturalOWL (Galanis and Androutsopoulos, 2007;
Androutsopoulos and Galanis, 2008) is a natu-
ral language generation engine that produces de-
scriptions of entitities (e.g., items for sale, mu-
seum exhibits) and classes (e.g., types of exhibits)
in English and Greek from OWL DL ontologies;
the ontologies must have been annotated with lin-
guistic and user modeling annotations expressed
in RDF.1 An accompanying plug-in for the well
known Prote?ge? ontology editor is available, which
can be used to create the linguistic and user model-
ing annotations while editing an ontology, as well
as to generate previews of the resulting texts by
invoking the generation engine.2
NaturalOWL is based on ideas from ILEX
(O?Donnell et al, 2001) and M-PIRO (Isard et al,
2003; Androutsopoulos et al, 2007), but it uses
1See http://www.w3.org/TR/owl-features/
for information on OWL and its versions. For information
on RDF, consult http://www.w3.org/RDF/.
2M-PIRO?s authoring tool (Androutsopoulos et al, 2007),
now called ELEON (Bilidas et al, 2007), can also be used; see
http://www.iit.demokritos.gr/skel/.
Figure 1: Generating texts in Second Life.
templates instead of systemic grammars, it is pub-
licly available as open-source software, it is writ-
ten entirely in Java, and it provides native support
for OWL ontologies, making it particularly useful
for Semantic Web applications (Antoniou and van
Harmelen, 2004).3 Well known advantages of nat-
ural language generation (Reiter and Dale, 2000)
include the ability to generate texts in multiple lan-
guages from the same ontology; and the ability to
tailor the semantic content and language expres-
sions of the texts to the user type (e.g., child vs.
adult) and the interaction history (e.g., by avoiding
repetitions, or by comparing to previous objects).
In project XENIOS (Vogiatzis et al, 2008), Nat-
uralOWL was embedded in a mobile robot acting
as a museum guide, and in project INDIGO it is
being integrated in a more advanced robotic guide
that includes a multimodal dialogue manager, fa-
cial animation, and mechanisms to recognize and
express emotions (Konstantopoulos et al, 2009).
Here, we demonstrate a similar application, where
NaturalOWL is embedded in a robotic avatar acting
3NaturalOWL comes with a GNU General Public Li-
cense (GPL). The software can be downloaded from
http://nlp.cs.aueb.gr/.
17
as a museum guide in Second Life (Oberlander et
al., 2008), as shown in figure 1. We also demon-
strate how the underlying ontology of the museum
and its linguistic and user modeling annotations
can be edited in Prote?ge?.
2 NaturalOWL?s architecture
NaturalOWL adopts a typical natural language
generation pipeline (Reiter and Dale, 2000). It
produces texts in three stages: document planning,
microplanning, and surface realization.
In document planning, the system first selects
from the ontology the logical facts (OWL triples)
that will be conveyed to the user, taking into ac-
count interest scores manually assigned to the
facts via the annotations of the ontology, as well
as a dynamcally updated user model that shows
what information has already been conveyed to the
user. Logical facts that report similarities or differ-
ences to previously encountered entities may also
be included in the output of content selection, giv-
ing rise to comparisons like the one in figure 1.
The selected facts are then ordered using a man-
ually specified partial order, which is also part of
the ontology?s annotations.
In micro-planning, the system turns each se-
lected fact into a sentence by using micro-plans, in
effect patterns that leave referring expressions un-
derspecified. Figure 2 shows a micro-plan being
edited with NaturalOWL?s Prote?ge? plug-in. The
micro-plan specifies that to express a fact that in-
volves the made-of property, the system should
concatenate an automatically generated referring
expression (e.g., name, pronoun, definite noun
phrase) in nominative case for the owner of the
fact (semantic subject of the triple), the verb form
?is made? (or ?are made?, if the subject is in plu-
ral), the preposition ?of?, and then another au-
tomatically generated referring expression in ac-
cusative case for the filler of the property (seman-
tic object). The referring expressions are gener-
ated by taking into account the context of each
sentence, attempting to avoid repetitions without
introducing ambiguities. Domain-independent ag-
gregation rules are then employed to combine the
resulting sentences into longer ones.
In surface realization, the final form of the text
is produced; it can be marked up automatically
with tags that indicate punctuation symbols, gram-
matical categories, the logical facts expressed by
the sentences, the interest (Int) of each sen-
tence?s information, the degree (Assim) to which
the information is taken to be assimilated by the
user etc., as shown below. In INDIGO, compar-
isons are also marked up with angles that guide
the robot to turn to the object(s) it compares to.
<Period>
<Sentence Property=".../#type"
Int="3" Assim="0">
<Demonstrative ref=".../#exhibit1"
role="owner">
This</Demonstrative>
<Verb>is</Verb>
<NP ref=".../#Amphora" role="filler">
an amphora</NP>
</Sentence>
<Punct>,</Punct>
<Sentence Property=".../#subtype
Int="3" Assim="1">
<EmptyRef ref=".../#Amphora"
role="owner"/>
<NP ref=".../#Vessel" role="filler">
a type of vessel</NP>
</Sentence>
<Punct>;</Punct>
<Sentence Property=".../#paintedBy"
Int="2" Assim="0">
<Pronoun ref=".../#exhibit1"
role="owner">
it</Pronoun>
<Verb>was painted</Verb>
<Preposition>by</Preposition>
<Name ref=".../#pKleo" role="filler">
the painter of Kleophrades</Name>
</Sentence>
<Punct>.</Punct>
</Period>
2.1 Using NaturalOWL?s Prote?ge? plug-in
NaturalOWL?s plug-in for Prote?ge? can be used to
specify all the linguistic and user modeling an-
notations of the ontologies that NaturalOWL re-
quires. The annotations in effect establish a
domain-dependent lexicon, whose entries are as-
sociated with classes or entities of the ontology;
micro-plans, which are associated with proper-
ties of the ontology; a partial order of proper-
ties, which is used in document planning; interest
scores, indicating how interesting the various facts
of the ontology are to each user type; parameters
that control, for example, the desired length of the
generated texts. The plug-in can also be used to
generate previews of the resulting texts, for differ-
ent types of users, with or without comparisons,
etc., as illustrated in figure 3. The resulting anno-
tations are then saved in RDF.
2.2 Using NaturalOWL in Second Life
In Second Life, each user controls an avatar, which
can, among other actions, move in the virtual
world, touch objects, or communicate with other
18
Figure 2: Specifying a micro-plan with NaturalOWL?s Prote?ge? plug-in.
Figure 3: Generating a text preview with NaturalOWL?s Prote?ge? plug-in.
19
avatars; in the latter case, the user types text on the
keyboard. In the Second Life application that we
demonstrate, the robot is an avatar that is not con-
trolled by a human, but by our own Second Life
client software.4 The client software includes a
navigation component, which controls the robot?s
movement, and it allows the robot to ?utter? texts
generated by NaturalOWL, instead of expecting
keyboard input. Whenever a visitor near the robot
touches an exhibit, an appropriate event is sent to
the robot, which then goes near the exhibit and
starts describing it.5
3 Conclusions and further work
The demonstration presents an open-source nat-
ural language generation engine for OWL ontolo-
gies, which generates descriptions of entities and
classes in English and Greek. The engine is ac-
companied by a Prote?ge? plug-in, which can be
used to annotate the ontologies with linguistic and
user modeling information required by the gener-
ation engine. The demonstration includes an ap-
plication in Second Life, where the generation en-
gine is embedded in a robotic avatar acting as a
museum guide. We are currently extending Natu-
ralOWL to handle follow up questions about enti-
ties or classes mentioned in the generated texts.
Acknowledgments
NaturalOWL was developed in project XENIOS,
which was funded by the Greek General Secre-
tariat of Research and Technology and the Euro-
pean Union.6 NaturalOWL is now being extended
in project INDIGO, which is funded by the Euro-
pean Union; our work in INDIGO is also supported
by additional funding from the Greek General Sec-
retariat of Research and Technology.7
References
I. Androutsopoulos and D. Galanis. 2008. Generating
natural language descriptions fromOWL ontologies:
experience from the NaturalOWL system. Technical
report, Department of Informatics, Athens Univer-
sity of Economics and Business, Greece.
4Our client was built using the libsecondlife li-
brary; see http://www.libsecondlife.org/. More
precisly, the robot is an object controlled by an invisible
robotic avatar, which is in turn controlled by our client.
5A video showing the robotic avatar in action is available
at http://www.vimeo.com/801099.
6See http://www.ics.forth.gr/xenios/.
7See http://www.ics.forth.gr/indigo/.
I. Androutsopoulos, J. Oberlander, and V. Karkaletsis.
2007. Source authoring for multilingual generation
of personalised object descriptions. Natural Lan-
guage Engineering, 13(3):191?233.
G. Antoniou and F. van Harmelen. 2004. A Semantic
Web primer. MIT Press.
D. Bilidas, M. Theologou, and V. Karkaletsis. 2007.
Enriching OWL ontologies with linguistic and user-
related annotations: the ELEON system. In Proceed-
ings of the 19th IEEE International Conference on
Tools with Artificial Intelligence, Patras, Greece.
D. Galanis and I. Androutsopoulos. 2007. Generat-
ing multilingual descriptions from linguistically an-
notated OWL ontologies: the NATURALOWL sys-
tem. In Proceedings of the 11th European Workshop
on Natural Language Generation, pages 143?146,
Schloss Dagstuhl, Germany.
A. Isard, J. Oberlander, I. Androutsopoulos, and
C. Matheson. 2003. Speaking the users? languages.
IEEE Intelligent Systems, 18(1):40?45.
S. Konstantopoulos, A. Tegos, D. Bilidas, I. Androut-
sopoulos, G. Lampouras, P. Malakasiotis, C. Math-
eson, and O. Deroo. 2009. Adaptive natural-
language interaction. In Proceedings of 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics (system demonstra-
tions), Athens, Greece.
J. Oberlander, G. Karakatsiotis, A. Isard, and I. An-
droutsopoulos. 2008. Building an adaptive museum
gallery in Second Life. In Proceedings of Museums
and the Web, Montreal, Quebec, Canada.
M. O?Donnell, C. Mellish, J. Oberlander, and A. Knott.
2001. ILEX: an architecture for a dynamic hypertext
generation system. Natural Language Engineering,
7(3):225?250.
E. Reiter and R. Dale. 2000. Building natural lan-
guage generation systems. Cambridge University
Press.
D. Vogiatzis, D. Galanis, V. Karkaletsis, I. Androut-
sopoulos, and C.D. Spyropoulos. 2008. A conver-
sant robotic guide to art collections. In Proceedings
of the 2nd Workshop on Language Technology for
Cultural Heritage Data, Language Resources and
Evaluation Conference, Marrakech, Morocco.
20
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 885?893,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An extractive supervised two-stage method for sentence compression
Dimitrios Galanis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Centre ?Athena?, Greece
Abstract
We present a new method that compresses
sentences by removing words. In a first stage,
it generates candidate compressions by re-
moving branches from the source sentence?s
dependency tree using a Maximum Entropy
classifier. In a second stage, it chooses the
best among the candidate compressions using
a Support Vector Machine Regression model.
Experimental results show that our method
achieves state-of-the-art performance without
requiring any manually written rules.
1 Introduction
Sentence compression is the task of producing a
shorter form of a single given sentence, so that the
new form is grammatical and retains the most im-
portant information of the original one (Jing, 2000).
Sentence compression is valuable in many applica-
tions, for example when displaying texts on small
screens (Corston-Oliver, 2001), in subtitle genera-
tion (Vandeghinste and Pan, 2004), and in text sum-
marization (Madnani et al, 2007).
People use various methods to shorten sentences,
including word or phrase removal, using shorter
paraphrases, and common sense knowledge. How-
ever, reasonable machine-generated sentence com-
pressions can often be obtained by only remov-
ing words. We use the term extractive to refer to
methods that compress sentences by only removing
words, as opposed to abstractive methods, where
more elaborate transformations are also allowed.
Most of the existing compression methods are ex-
tractive (Jing, 2000; Knight and Marcu, 2002; Mc-
Donald, 2006; Clarke and Lapata, 2008; Cohn and
Lapata, 2009). Although abstractive methods have
also been proposed (Cohn and Lapata, 2008), and
they may shed more light on how people compress
sentences, they do not always manage to outperform
extractive methods (Nomoto, 2009). Hence, from an
engineering perspective, it is still important to inves-
tigate how extractive methods can be improved.
In this paper, we present a new extractive sentence
compression method that relies on supervised ma-
chine learning.1 In a first stage, the method gener-
ates candidate compressions by removing branches
from the source sentence?s dependency tree using a
Maximum Entropy classifier (Berger et al, 2006). In
a second stage, it chooses the best among the candi-
date compressions using a Support Vector Machine
Regression (SVR) model (Chang and Lin, 2001). We
show experimentally that our method compares fa-
vorably to a state-of-the-art extractive compression
method (Cohn and Lapata, 2007; Cohn and Lapata,
2009), without requiring any manually written rules,
unlike other recent work (Clarke and Lapata, 2008;
Nomoto, 2009). In essence, our method is a two-
tier over-generate and select (or rerank) approach to
sentence compression; similar approaches have been
adopted in natural language generation and parsing
(Paiva and Evans, 2005; Collins and Koo, 2005).
2 Related work
Knight and Marcu (2002) presented a noisy channel
sentence compression method that uses a language
model P (y) and a channel model P (x|y), where x
1An implementation of our method will be freely available
from http://nlp.cs.aueb.gr/software.html
885
is the source sentence and y the compressed one.
P (x|y) is calculated as the product of the proba-
bilities of the parse tree tranformations required to
expand y to x. The best compression of x is the
one that maximizes P (x|y) ? P (y), and it is found
using a noisy channel decoder. In a second, alter-
native method Knight and Marcu (2002) use a tree-
to-tree transformation algorithm that tries to rewrite
directly x to the best y. This second method uses
C4.5 (Quinlan, 1993) to learn when to perform tree
rewriting actions (e.g., dropping subtrees, combin-
ing subtrees) that transform larger trees to smaller
trees. Both methods were trained and tested on
data from the Ziff-Davis corpus (Knight and Marcu,
2002), and they achieved very similar grammatical-
ity and meaning preservation scores, with no statis-
tically significant difference. However, their com-
pression rates (counted in words) were very dif-
ferent: 70.37% for the noisy-channel method and
57.19% for the C4.5-based one.
McDonald (2006) ranks each candidate compres-
sion using a function based on the dot product of a
vector of weights with a vector of features extracted
from the candidate?s n-grams, POS tags, and depen-
dency tree. The weights were learnt from the Ziff-
Davis corpus. The best compression is found us-
ing a Viterbi-like algorithm that looks for the best
sequence of source words that maximizes the scor-
ing function. The method outperformed Knight and
Marcu?s (2002) tree-to-tree method in grammatical-
ity and meaning preservation on data from the Ziff-
Davis corpus, with a similar compression rate.
Clarke and Lapata (2008) presented an unsuper-
vised method that finds the best compression using
Integer Linear Programming (ILP). The ILP obejc-
tive function takes into account a language model
that indicates which n-grams are more likely to be
deleted, and a significance model that shows which
words of the input sentence are important. Man-
ually defined constraints (in effect, rules) that op-
erate on dependency trees indicate which syntactic
constituents can be deleted. This method outper-
formed McDonald?s (2006) in grammaticality and
meaning preservation on test sentences from Edin-
burgh?s ?written? and ?spoken? corpora.2 However,
the compression rates of the two systems were dif-
2See http://homepages.inf.ed.ac.uk/s0460084/data/.
ferent (72.0% vs. 63.7% for McDonald?s method,
both on the written corpus).
We compare our method against Cohn and Lap-
ata?s T3 system (Cohn and Lapata, 2007; Cohn and
Lapata, 2009), a state-of-the-art extractive sentence
compression system that learns parse tree transduc-
tion operators from a parallel extractive corpus of
source-compressed trees. T3 uses a chart-based de-
coding algorithm and a Structured Support Vector
Machine (Tsochantaridis et al, 2005) to learn to
select the best compression among those licensed
by the operators learnt.3 T3 outperformed McDon-
ald?s (2006) system in grammaticality and meaning
preservation on Edinburgh?s ?written? and ?spoken?
corpora, achieving comparable compression rates
(Cohn and Lapata, 2009). Cohn and Lapata (2008)
have also developed an abstractive version of T3,
which was reported to outperform the original, ex-
tractive T3 in meaning preservation; there was no
statistically significant difference in grammaticality.
Finally, Nomoto (2009) presented a two-stage ex-
tractive method. In the first stage, candidate com-
pressions are generated by chopping the source sen-
tence?s dependency tree. Many ungrammatical com-
pressions are avoided using hand-crafted drop-me-
not rules for dependency subtrees. The candidate
compressions are then ranked using a function that
takes into account the inverse document frequen-
cies of the words, and their depths in the source
dependency tree. Nomoto?s extractive method was
reported to outperform Cohn and Lapata?s abstrac-
tive version of T3 on a corpus collected via RSS
feeds. Our method is similar to Nomoto?s, in that
it uses two stages, one that chops the source depen-
dency tree generating candidate compressions, and
one that ranks the candidates. However, we experi-
mented with more elaborate ranking models, and our
method does not employ any manually crafted rules.
3 Our method
As already mentioned, our method first generates
candidate compressions, which are then ranked. The
candidate compressions generator operates by re-
moving branches from the dependency tree of the
3T3 appears to be the only previous sentence compres-
sion method whose implementation is publicly available; see
http://www.dcs.shef.ac.uk/?tcohn/t3/.
886
input sentence (figure 1); this stage is discussed in
section 3.1 below. We experimented with different
ranking functions, discussed in section 3.2, which
use features extracted from the source sentence s
and the candidate compressions c1, . . . , ck.
3.1 Generating candidate compressions
Our method requires a parallel training corpus con-
sisting of sentence-compression pairs ?s, g?. The
compressed sentences g must have been formed by
only deleting words from the corresponding source
sentences s. The ?s, g? training pairs are used to es-
timate the propability that a dependency edge e of a
dependency tree Ts of an input sentence s is retained
or not in the dependency tree Tg of the compressed
sentence g. More specifically, we want to estimate
the probabilities P (Xi|context(ei)) for every edge
ei of Ts, where Xi is a variable that can take one
of the following three values: not del, for not delet-
ing ei; del u for deleting ei along with its head; and
del l for deleting e along with its modifier. The head
(respectively, modifier) of ei is the node ei originates
from (points to) in the dependency tree. context(ei)
is a set of features that represents ei?s local context
in Ts, as well as the local context of the head and
modifier of ei in s.
The propabilities above can be estimated using
the Maximum Entropy (ME) framework (Berger et
al., 2006), a method for learning the distribution
P (X|V ) from training data, where X is discrete-
valued variable and V = ?V1, . . . , Vn? is a real or
discrete-valued vector. Here, V = context(ei) and
X = Xi. We use the following features in V :
? The label of the dependency edge ei, as well as
the POS tag of the head and modifier of ei.
? The entire head-label-modifier triple of ei. This
feature overlaps with the previous two features,
but it is common in ME models to use feature
combinations as additional features, since they
may indicate a category more strongly than the
individual initial features.4
? The POS tag of the father of ei?s head, and the
label of the dependency that links the father to
ei?s head.
4http://nlp.stanford.edu/pubs/maxent-tutorial-slides.pdf.
? The POS tag of each one of the three previous
and the three following words of ei?s head and
modifier in s (12 features).
? The POS tag bi-grams of the previous two and
the following two words of ei?s head and mod-
ifier in s (4 features).
? Binary features that show which of the possible
labels occur (or not) among the labels of the
edges that have the same head as ei in Ts (one
feature for each possible dependency label).
? Two binary features that show if the subtree
rooted at the modifier of ei or ei?s uptree (the
rest of the tree, when ei?s subtree is removed)
contain an important word. A word is consid-
ered important if it appears in the document s
was drawn from significantly more often than
in a background corpus. In summarization,
such words are called signature terms and are
thought to be descriptive of the input; they can
be identified using the log-likelihood ratio ? of
each word (Lin and Hovy, 2000; Gupta et al,
2007).
For each dependency edge ei of a source training
sentence s, we create a training vector V with the
above features. If ei is retained in the dependency
tree of the corresponding compressed sentence g in
the corpus, V is assigned the category not del. If
ei is not retained, it is assigned the category del l
or del u, depending on whether the head (as in the
ccomp of ?said? in Figure 1) or the modifier (as in
the dobj of ?attend?) of ei has also been removed.
When the modifier of an edge is removed, the entire
subtree rooted at the modifier is removed, and simi-
larly for the uptree, when the head is removed. We
do not create training vectors for the edges of the
removed subtree of a modifier or the edges of the
removed uptree of a head.
Given an input sentence s and its dependency tree
Ts, the candidate compressions generator produces
the candidate compressed sentences c1, . . . , cn by
deleting branches of Ts and putting the remaining
words of the dependency tree in the same order as in
s. The candidates c1, . . . , cn correspond to possible
assignments of values to theXi variables (recall that
Xi = not del|del l|del u) of the edges ei of Ts.
887
source: gold:
said
ccomp

nsubj
K
%%K
K
attend
nsubj

aux
KK
%%K
K
attend
nusbj

aux
JJ
%%J
J dobj
*j
*j
***j
*j
prep
+k+k
+k+k
+k
+++k
+k+k
+k+k
+k
he Mother
num

num
JJ
%%J
J
J amod
TT
TT
**TT
TT
will
Mother
num

num
JJ
$$J
J
J amod
TT
TT
**TT
TT
will hearing
det
##G
G
on
pobj
F
""
Catherine 82 superior
measure

Catherine 82 superior
measure

the Friday mother
det
mother
det
the
the
Figure 1: Dependency trees of a source sentence and its compression by a human (taken from Edinburgh?s ?written?
corpus). The source sentence is: ?Mother Catherine, 82, the mother superior, will attend the hearing on Friday, he
said.? The compressed one is: ?Mother Catherine, 82, the mother superior, will attend.? Deleted edges and words are
shown curled and underlined, respectively.
Hence, there are at most 3m?1 candidate compres-
sions, where m is the number of words in s. This
is a large number of candidates, even for modestly
long input sentences. In practice, the candidates are
fewer, because del l removes an entire subtree and
del u an entire uptree, and we do not need to make
decisions Xi about the edges of the deleted subtrees
and uptrees. To reduce the number of candidates
further, we ignore possible assignments that contain
decisions Xi = x to which the ME model assigns
probabilities below a threshold t; i.e., the ME model
is used to prune the space of possible assignments.
When generating the possible assignments to the
Xi variables, we examine the edges ei of Ts in a
top-down breadth-first manner. In the source tree of
Figure 1, for example, we first consider the edges
of ?said?; the left-to-right order is random, but let
us assume that we consider first the ccomp edge.
There are three possible actions: retain the edge
(not del), remove it along with the head ?said?
(del u), or remove it along with the modifier ?at-
tend? and its subtree (del l). If the ME model assigns
a low probability to one of the three actions, that ac-
tion is ignored. For each one of the (remaining) ac-
tions, we obtain a new form of Ts, and we continue
to consider its (other) edges. We process the edges
in a top-down fashion, because the ME model allows
del l actions much more often than del u actions,
and when del l actions are performed near the root
of Ts, they prune large parts of the space of possible
assignments to the Xi variables. Some of the candi-
date compressions that were generated for an input
sentence by setting t = 0.2 are shown in Table 1,
along with the gold (human-authored) compression.
3.2 Ranking candidate compressions
Given that we now have a method that generates
candidate compressions c1, . . . , ck for a sentence s,
we need a function F (ci|s) that will rank the candi-
date compressions. Many of them are ungrammat-
ical and/or do not convey the most important infor-
mation of s. F (ci|s) should help us select a short
candidate that is grammatical and retains the most
important information of s.
3.2.1 Grammaticality and importance rate
A simple way to rank the candidate compressions
is to assign to each one a score intended to measure
its grammaticality and importance rate. By gram-
maticality, Gramm(ci), we mean how grammati-
cally well-formed candidate ci is. A common way
to obtain such a measure is to use an n-gram lan-
888
s: Then last week a second note, in the same handwriting, informed Mrs Allan that the search was
on the wrong side of the bridge.
g: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.
c1: Last week a second note informed Mrs Allan that the search was on the side.
c2: Last week a second note informed Mrs Allan that the search was.
c3: Last week a second note informed Mrs Allan the search was on the wrong side of the bridge.
c4: Last week in the same handwriting informed Mrs Allan the search was on the wrong side of the bridge.
Table 1: A source sentence s, its gold (human authored) compression g, and candidate compressions c1, . . . , c4.
guage model trained on a large background corpus.
However, language models tend to assign smaller
probabilities to longer sentences; therefore they fa-
vor short sentences, but not necessarily the most ap-
propriate compressions. To overcome this problem,
we follow Cordeiro et al (2009) and normalize the
score of a trigram language model as shown below,
where w1, . . . , wm are the words of candidate ci.
Gramm(ci) = logPLM (ci)
1/m =
(1/m) ? log(
m?
j=1
P (wj |wj?1, wj?2)) (1)
The importance rate ImpRate(ci|s), defined be-
low, estimates how much information of the original
sentence s is retained in candidate ci. tf(wi) is the
term frequency of wi in the document that contained
? (? = ci, s), and idf(wi) is the inverse document
frequency of wi in a background corpus. We actu-
ally compute idf(wi) only for nouns and verbs, and
set idf(wi) = 0 for other words.
ImpRate(ci|s) = Imp(ci)/Imp(s) (2)
Imp(?) =
?
wi??
tf(wi) ? idf(wi) (3)
The ranking F (c|s) is then defined as a linear
combination of grammaticality and importance rate:
F (ci|s) = ? ?Gramm(ci) + (1? ?) ?
? ImpRate(ci|s)? ? ? CR(ci|s) (4)
A compression rate penalty factor CR(ci|s) =
|c|/|s| is included, to bias our method towards gen-
erating shorter or longer compressions; | ? | denotes
word length in words (punctuation is ignored). We
explain how the weigths ?, ? are tuned in following
sections. We call LM-IMP the configuration of our
method that uses the ranking function of equation 4.
3.2.2 Support Vector Regression
A more sophisticated way to select the best com-
pression is to train a Support Vector Machines Re-
gression (SVR) model to assign scores to feature vec-
tors, with each vector representing a candidate com-
pression. SVR models (Chang and Lin, 2001) are
trained using l training vectors (x1, y1), . . . , (xl, yl),
where xi ? Rn and yi ? R, and learn a function
f : Rn ? R that generalizes the training data. In
our case, xi is a feature vector representing a candi-
date compression ci, and yi is a score indicating how
good a compression ci is. We use 98 features:
? Gramm(ci) and ImpRate(ci|s), as above.
? 2 features indicating the ratio of important and
unimportant words of s, identified as in section
3.1, that were deleted.
? 2 features that indicate the average depth of
the deleted and not deleted words in the depen-
dency tree of s.
? 92 features that indicate which POS tags appear
in s and how many of them were deleted in ci.
For every POS tag label, we use two features,
one that shows how many POS tags of that la-
bel are contained in s and one that shows how
many of these POS tags were deleted in ci.
To assign a regression score yi to each training
vector xi, we experimented with the following func-
tions that measure how similar ci is to the gold com-
pression g, and how grammatical ci is.
? Grammatical relations overlap: In this case, yi
is theF1-score of the dependencies of ci against
those of the gold compression g. This measure
has been shown to correlate well with human
judgements (Clarke and Lapata, 2006). As in
889
the ranking function of section 3.2.1, we add a
compression rate penalty factor.
yi = F1(d(ci)), d(g))? ? ? CR(ci|s) (5)
d(?) denotes the set of dependencies. We call
SVR-F1 the configuration of our system that
uses equation 5 to rank the candidates.
? Tokens accuracy and grammaticality: Tokens
accuracy, TokAcc(ci|s, g), is the percentage of
tokens of s that were correctly retained or re-
moved in ci; a token was correctly retained
or removed, if it was also retained (or re-
moved) in the gold compression g. To cal-
culate TokAcc(ci|s, g), we need the word-to-
word alignment of s to g, and s to ci. These
alignments were obtained as a by-product of
computing the corresponding (word) edit dis-
tances. We also want the regression model to
favor grammatical compressions. Hence, we
use a linear combination of tokens accuracy
and grammaticality of ci:
yi = ? ? TokAcc(ci|s, g) +
(1? ?) ?Gramm(ci)? ? ? CR(ci|s) (6)
Again, we add a compression rate penalty, to
be able to generate shorter or longer compres-
sions. We call SVR-TOKACC-LM the config-
uration of our system that uses equation 6.
4 Baseline and T3
As a baseline, we use a simple algorithm based on
the ME classifier of section 3.1. The baseline pro-
duces a single compression c for every source sen-
tence s by considering sequentially the edges ei of
s?s dependency tree in a random order, and perform-
ing at each ei the single action (not del, del u, or
del l) that the ME model considers more probable;
the words of the chopped dependency tree are then
put in the same order as in s. We call this system
Greedy-Baseline.
We compare our method against the extractive
version of T3 (Cohn and Lapata, 2007; Cohn and
Lapata, 2009), a state-of-the-art sentence compres-
sion system that applies sequences of transduction
operators to the syntax trees of the source sentences.
The available tranduction operators are learnt from
the syntax trees of a set of source-gold pairs. Ev-
ery operator transforms a subtree ? to a subtree ?,
rooted at symbols X and Y , respectively.
To find the best sequence of transduction opera-
tors that can be applied to a source syntax tree, a
chart-based dynamic programming decoder is used,
which finds the best scoring sequence q?:
q? = arg max
q
score(q;w) (7)
where score(q;w) is the dot product ??(q), w?.
?(q) is a vector-valued feature function, and w is a
vector of weights learnt using a Structured Support
Vector Machine (Tsochantaridis et al, 2005).
?(q) consists of: (i) the log-probability of the re-
sulting candidate, as returned by a tri-gram language
model; and (ii) features that describe how the opera-
tors of q are applied, for example the number of the
terminals in each operator?s ? and ? subtrees, the
POS tags of the X and Y roots of ? and ? etc.
5 Experiments
We used Stanford?s parser (de Marneffe et al, 2006)
and ME classifier (Manning et al, 2003).5 For
the (trigram) language model, we used SRILM with
modified Kneser-Ney smoothing (Stolcke, 2002).6
The language model was trained on approximately
4.5 million sentences of the TIPSTER corpus. To
obtain idf(wi) values, we used approximately 19.5
million verbs and nouns from the TIPSTER corpus.
T3 requires the syntax trees of the source-gold
pairs in Penn Treebank format, as well as a trigram
language model. We obtained T3?s trees using Stan-
ford?s parser, as in our system, unlike Cohn and La-
pata (2009) that use Bikel?s (2002) parser. The lan-
guage models in T3 and our system are trained on
the same data and with the same options used by
Cohn and Lapata (2009). T3 also needs a word-to-
word alignment of the source-gold pairs, which was
obtained by computing the edit distance, as in Cohn
and Lapata (2009) and SVR-TOKACC-LM.
We used Edinburgh?s ?written? sentence com-
pression corpus (section 2), which consists of
source-gold pairs (one gold compression per source
5Both available from http://nlp.stanford.edu/.
6See http://www.speech.sri.com/projects/srilm/.
890
sentence). The gold compressions were created by
deleting words. We split the corpus in 3 parts: 1024
training, 324 development, and 291 testing pairs.
5.1 Best configuration of our method
We first evaluated experimentally the three configu-
rations of our method (LM-IMP, SVR-F1, SVR-
TOKACC-LM), using the F1-measure of the de-
pendencies of the machine-generated compressions
against those of the gold compressions as an auto-
matic evaluation measure. This measure has been
shown to correlate well with human judgements
(Clarke and Lapata, 2006).
In all three configurations, we trained the ME
model of section 3.1 on the dependency trees of the
source-gold pairs of the training part of the corpus.
We then used the trained ME classifier to generate
the candidate compressions of each source sentence
of the training part. We set t = 0.2, which led to
at most 10,000 candidates for almost every source
sentence. We kept up to 1.000 candidates for each
source sentence, and we selected randonly approx-
imately 10% of them, obtaining 18,385 candidates,
which were used to train the two SVR configurations;
LM-IMP requires no training.
To tune the ? parameters of LM-IMP and SVR-
TOKACC-LM in equations 4 and 6, we initially set
? = 0 and we experimented with different val-
ues of ?. For each one of the two configurations
and for every different ? value, we computed the
average compression rate of the machine-generated
compressions on the development set. In the rest
of the experiments, we set ? to the value that gave
an average compression rate approximatelly equal to
that of the gold compressions of the training part.
We then experimented with different values of ?
in all three configurations, in equations 4?6, to pro-
duce smaller or longer compression rates. The ? pa-
rameter provides a uniform mechanism to fine-tune
the compression rate in all three configurations, even
in SVR-F1 that has no ?. The results on the de-
velopment part are shown in Figure 2, along with
the baseline?s results. The baseline has no param-
eters to tune; hence, its results are shown as a sin-
gle point. Both SVR models outperform LM-IMP,
which in turn outperforms the baseline. Also, SVR-
TOKACC-LM performs better or as well as SVR-
F1 for all compression rates. Note, also, that the
perfomance of the two SVR configurations might be
improved further by using more training examples,
whereas LM-IMP contains no learning component.
Figure 2: Evaluation results on the development set.
5.2 Our method against T3
We then evaluated the best configuration of our
method (SVR-TOKACC-LM) against T3, both au-
tomatically (F1-measure) and with human judges.
We trained both systems on the training set of the
corpus. In our system, we used the same ? value that
we had obtained from the experiments of the previ-
ous section. We then varied the values of our sys-
tem?s ? parameter to obtain approximately the same
compression rate as T3.
For the evaluation with the human judges, we se-
lected randomly 80 sentences from the test part. For
each source sentence s, we formed three pairs, con-
taining s, the gold compression, the compression
of SVR-TOKACC-LM, and the compression of T3,
repsectively, 240 pairs in total. Four judges (grad-
uate students) were used. Each judge was given 60
pairs in a random sequence; they did not know how
the compressed sentenes were obtained and no judge
saw more than one compression of the same source
sentence. The judges were told to rate (in a scale
from 1 to 5) the compressed sentences in terms of
grammaticality, meaning preservation, and overall
quality. Their average judgements are shown in Ta-
ble 2, where the F1-scores are also included. Cohn
and Lapata (2009) have reported very similar scores
891
for T3 on a different split of the corpus (F1: 49.48%,
CR: 61.09%).
system G M Ov F1 (%) CR (%)
T3 3.83 3.28 3.23 47.34 59.16
SVR 4.20 3.43 3.57 52.09 59.85
gold 4.73 4.27 4.43 100.00 78.80
Table 2: Results on 80 test sentences. G: grammaticality,
M: meaning preservation, Ov: overall score, CR: com-
pression rate, SVR: SVR-TOKACC-LM.
Our system outperforms T3 in all evaluation mea-
sures. We used Analysis of Variance (ANOVA) fol-
lowed by post-hoc Tukey tests to check whether the
judge ratings differ significantly (p < 0.1); all judge
ratings of gold compressions are significantly differ-
ent from T3?s and those of our system; also, our sys-
tem differs significantly from T3 in grammaticality,
but not in meaning preservation and overall score.
We also performed Wilcoxon tests, which showed
that the difference in the F1 scores of the two sys-
tems is statistically significant (p < 0.1) on the 80
test sentences. Table 3 shows the F1 scores and the
average compression rates for all 291 test sentences.
Both systems have comparable compression rates,
but again our system outperforms T3 in F1, with a
statistically significant difference (p < 0.001).
system F1 CR
SVR-TOKACC-LM 53.75 63.72
T3 47.52 64.16
Table 3: F1 scores on the entire test set.
Finally, we computed the Pearson correlation r of
the overall (Ov) scores that the judges assigned to
the machine-generated compressions with the corre-
sponding F1 scores. The two measures were found
to corellate reliably (r = 0.526). Similar results
have been reported (Clarke and Lapata, 2006) for
Edinburgh?s ?spoken? corpus (r = 0.532) and the
Ziff-Davis corpus (r = 0.575).
6 Conclusions and future work
We presented a new two-stage extractive method
for sentence compression. The first stage gener-
ates candidate compressions by removing or not
edges from the source sentence?s dependency tree;
an ME model is used to prune unlikely edge deletion
or non-deletions. The second stage ranks the can-
didate compressions; we experimented with three
ranking models, achieving the best results with an
SVR model trained with an objective function that
combines token accuracy and a language model.
We showed experimentally, via automatic evalua-
tion and with human judges, that our method com-
pares favorably to a state-of-the-art extractive sys-
tem. Unlike other recent approaches, our system
uses no hand-crafted rules. In future work, we plan
to support more complex tranformations, instead of
only removing words and experiment with different
sizes of training data.
The work reported in this paper was carried out in
the context of project INDIGO, where an autonomous
robotic guide for museum collections is being devel-
oped. The guide engages the museum?s visitors in
spoken dialogues, and it describes the exhibits that
the visitors select by generating textual descriptions,
which are passed on to a speech synthesizer. The
texts are generated from logical facts stored in an on-
tology (Galanis et al, 2009) and from canned texts;
the latter are used when the corresponding informa-
tion is difficult to encode in symbolic form (e.g., to
store short stories about the exhibits). The descrip-
tions of the exhibits are tailored depending on the
type of the visitor (e.g., child vs. adult), and an im-
portant tailoring aspect is the generation of shorter
or longer descriptions. The parts of the descrip-
tions that are generated from logical facts can be
easily made shorter or longer, by conveying fewer
or more facts. The methods of this paper are used
to automatically shorten the parts of the descrip-
tions that are canned texts, instead of requiring mul-
tiple (shorter and longer) hand-written versions of
the canned texts.
Acknowledgements
This work was carried out in INDIGO, an FP6 IST
project funded by the European Union, with addi-
tional funding provided by the Greek General Sec-
retariat of Research and Technology.7
7Consult http://www.ics.forth.gr/indigo/.
892
References
A.L. Berger, S.A. Della Pietra, and V.J. Della Pietra.
2006. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
D. Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Proceedings
of the 2nd International Conference on Human Lan-
guage Technology Research, pages 24?27.
C.C Chang and C.J Lin. 2001. LIBSVM: a library for
Support Vector Machines. Technical report. Software
available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
J. Clarke and M. Lapata. 2006. Models for sentence
compression: A comparison across domains, training
requirements and evaluation measures. In Proceedings
of COLING, pages 377?384.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Artificial Intelligence Research, 1(31):399?
429.
T. Cohn and M. Lapata. 2007. Large margin syn-
chronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoNLL,
pages 73?82.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING,
pages 137?144.
T. Cohn and M. Lapata. 2009. Sentence compression
as tree to tree tranduction. Artificial Intelligence Re-
search, 34:637?674.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?69.
J. Cordeiro, G. Dias, and P. Brazdil. 2009. Unsupervised
induction of sentence compression rules. In Proceed-
ings of the ACL Workshop on Language Generation
and Summarisation, pages 391?399.
S. Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the NAACL
Workshop on Automatic Summarization, pages 89?98.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC,
pages 449?454.
Dimitrios Galanis, George Karakatsiotis, Gerasimos
Lampouras, and Ion Androutsopoulos. 2009. An
open-source natural language generator for OWL on-
tologies and its use in protege and second life. In Pro-
ceedings of the Demonstrations Session at EACL 2009,
pages 17?20, Athens, Greece, April. Association for
Computational Linguistics.
S. Gupta, A. Nenkova, and D. Jurafsky. 2007. Measur-
ing importance and query relevance in topic-focused
multi-document summarization. In Proceedings of
ACL, pages 193?196.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP, pages 310?
315.
K. Knight and D. Marcu. 2002. Summarization beyond
sentence extraction: A probalistic approach to sen-
tence compression. Artificial Intelligence, 139(1):91?
107.
C.W. Lin and E. Hovy. 2000. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of ACL, pages 495?501.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions
for automatic text summarization. In Proceedings of
DUC.
C. D. Manning, D. Klein, and C. Manning. 2003. Op-
timization, maxent models, and conditional estimation
without magic. In tutorial notes of HLT-NAACL 2003
and ACL 2003.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL, pages 297?304.
T. Nomoto. 2009. A comparison of model free versus
model intensive approaches to sentence compression.
In Proceedings of EMNLP, pages 391?399.
D. Paiva and R. Evans. 2005. Empirically-based con-
trol of natural language generation. In Proceedings of
ACL.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, pages 901?
904.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for indepen-
dent and structured output spaces. Machine Learning
Research, 6:1453?1484.
V. Vandeghinste and Y. Pan. 2004. Sentence compres-
sion for automated subtitling: A hybrid approach. In
Proceedings of the ACL Workshop ?Text Summariza-
tion Branches Out?, pages 89?95.
893
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 1?11,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
A New Sentence Compression Dataset and Its Use in an Abstractive
Generate-and-Rank Sentence Compressor
Dimitrios Galanis? and Ion Androutsopoulos?+
?Department of Informatics, Athens University of Economics and Business, Greece
+Digital Curation Unit ? IMIS, Research Center ?Athena?, Greece
Abstract
Sentence compression has attracted much in-
terest in recent years, but most sentence com-
pressors are extractive, i.e., they only delete
words. There is a lack of appropriate datasets
to train and evaluate abstractive sentence com-
pressors, i.e., methods that apart from delet-
ing words can also rephrase expressions. We
present a new dataset that contains candi-
date extractive and abstractive compressions
of source sentences. The candidate compres-
sions are annotated with human judgements
for grammaticality and meaning preservation.
We discuss how the dataset was created, and
how it can be used in generate-and-rank ab-
stractive sentence compressors. We also re-
port experimental results with a novel abstrac-
tive sentence compressor that uses the dataset.
1 Introduction
Sentence compression is the task of producing a
shorter form of a grammatical source (input) sen-
tence, so that the new form will still be grammati-
cal and it will retain the most important information
of the source (Jing, 2000). Sentence compression is
useful in many applications, such as text summariza-
tion (Madnani et al, 2007) and subtitle generation
(Corston-Oliver, 2001). Methods for sentence com-
pression can be divided in two categories: extrac-
tive methods produce compressions by only remov-
ing words, whereas abstractive methods may addi-
tionally rephrase expressions of the source sentence.
Extractive methods are generally simpler and have
dominated the sentence compression literature (Jing,
2000; Knight and Marcu, 2002; McDonald, 2006;
Cohn and Lapata, 2007; Clarke and Lapata, 2008;
Cohn and Lapata, 2009; Nomoto, 2009; Galanis
and Androutsopoulos, 2010; Yamangil and Shieber,
2010). Abstractive methods, however, can in prin-
ciple produce shorter compressions that convey the
same information as longer extractive ones. Further-
more, humans produce mostly abstractive compres-
sions (Cohn and Lapata, 2008); hence, abstractive
compressors may generate more natural outputs.
When evaluating extractive methods, it suffices
to have a single human gold extractive compres-
sion per source sentence, because it has been shown
that measuring the similarity (as F1-measure of de-
pendencies) between the dependency tree of the
gold compression and that of a machine-generated
compression correlates well with human judgements
(Riezler et al, 2003; Clarke and Lapata, 2006a).
With abstractive methods, however, there is a much
wider range of acceptable abstractive compressions
of each source sentence, to the extent that a single
gold compression per source is insufficient. Indeed,
to the best of our knowledge no measure to com-
pare a machine-generated abstractive compression
to a single human gold compression has been shown
to correlate well with human judgements.
One might attempt to provide multiple human
gold abstractive compressions per source sentence
and employ measures from machine translation, for
example BLEU (Papineni et al, 2002), to compare
each machine-generated compression to all the cor-
responding gold ones. However, a large number of
gold compressions would be necessary to capture all
(or at least most) of the acceptable shorter rephras-
1
ings of the source sentences, and it is questionable
if human judges could provide (or even think of) all
the acceptable rephrasings. In machine translation,
n-gram-based evaluation measures like BLEU have
been criticized exactly because they cannot cope
sufficiently well with paraphrases (Callison-Burch
et al, 2006), which play a central role in abstractive
sentence compression (Zhao et al, 2009a).1
Although it is difficult to construct datasets for
end-to-end automatic evaluation of abstractive sen-
tence compression methods, it is possible to con-
struct datasets to evaluate the ranking components
of generate-and-rank abstractive sentence compres-
sors, i.e., compressors that first generate a large set
of candidate abstractive (and possibly also extrac-
tive) compressions of the source and then rank them
to select the best one. In previous work (Galanis and
Androutsopoulos, 2010), we presented a generate-
and-rank extractive sentence compressor, hereafter
called GA-EXTR, which achieved state-of-the art re-
sults. We aim to construct a similar abstractive
generate-and-rank sentence compressor. As part of
this endeavour, we needed a dataset to automatically
test (and train) several alternative ranking compo-
nents. In this paper, we introduce a dataset of this
kind, which we also make publicly available.2
The dataset consists of pairs of source sentences
and candidate extractive or abstractive compres-
sions. The candidate compressions were generated
by first using GA-EXTR and then applying exist-
ing paraphrasing rules (Zhao et al, 2009b) to the
best extractive compressions of GA-EXTR. Each pair
(source and candidate compression) was then scored
by a human judge for grammaticality and meaning
preservation. We discuss how the dataset was con-
structed and how we established upper and lower
performance boundaries for ranking components of
compressors that may use it. We also present the
1Ways to extend n-gram measures to account for para-
phrases have been proposed (Zhou et al, 2006; Kauchak and
Barzilay, 2006; Pado? et al, 2009), but they require accu-
rate paraphrase recognizers (Androutsopoulos and Malakasio-
tis, 2010), which are not yet available; or they assume that
the same paraphrase generation resources (Madnani and Dorr,
2010), for example paraphrasing rules, that some abstractive
sentence compressors (including ours) use always produce ac-
ceptable paraphrases, which is not the case as discussed below.
2The new dataset and GA-EXTR are freely available from
http://nlp.cs.aueb.gr/software.html.
current version of our abstractive sentence compres-
sor, and we discuss how its ranking component was
improved by performing experiments on the dataset.
Section 2 below summarizes prior work on ab-
stractive sentence compression. Section 3 discusses
the dataset we constructed. Section 4 describes our
abstractive sentence compressor. Section 5 presents
our experimental results, and Section 6 concludes.
2 Prior work on abstractive compression
The first abstractive compression method was pro-
posed by Cohn and Lapata (2008). It learns a set of
parse tree transduction rules from a training dataset
of pairs, each pair consisting of a source sentence
and a single human-authored gold abstractive com-
pression. The set of transduction rules is then aug-
mented by applying a pivoting approach to a par-
allel bilingual corpus; we discuss similar pivoting
mechanisms below. To compress a new sentence, a
chart-based decoder and a Structured Support Vec-
tor Machine (Tsochantaridis et al, 2005) are used to
select the best abstractive compression among those
licensed by the rules learnt.
The dataset that Cohn and Lapata (2008) used
to learn transduction rules consists of 570 pairs of
source sentences and abstractive compressions. The
compressions were produced by humans who were
allowed to use any transformation they wished. We
used a sample of 50 pairs from that dataset to con-
firm that humans produce mostly abstractive com-
pressions. Indeed, 42 (84%) of the compressions
were abstractive, and only 7 (14%) were simply ex-
tractive.3 We could not use that dataset, however,
for automatic evaluation purposes, since it only pro-
vides a single human gold abstract compression per
source, which is insufficient as already discussed.
More recently, Zhao et al (2009a) presented a
sentence paraphrasing method that can be config-
ured for different tasks, including a form of sentence
compression. For each source sentence, Zhao et al?s
method uses a decoder to produce the best possible
paraphrase, much as in phrase-based statistical ma-
chine translation (Koehn, 2009), but with phrase ta-
bles corresponding to paraphrasing rules (e.g., ?X
3Cohn and Lapata?s dataset is available from http://
staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/#
Corpus. One pair (2%) of our sample had a ?compression?
that was identical to the input.
2
is the author of Y ? ? ?X wrote Y ?) obtained from
parallel and comparable corpora (Zhao et al, 2008).
The decoder uses a log-linear objective function, the
weights of which are estimated with a minimum er-
ror rate training approach (Och, 2003). The objec-
tive function combines a language model, a para-
phrase model (combining the quality scores of the
paraphrasing rules that turn the source into the can-
didate paraphrase), and a task-specific model; in the
case of sentence compression, the latter model re-
wards shorter candidate paraphrases.
We note that Zhao et al?s method (2009a) is in-
tended to produce paraphrases, even when config-
ured to prefer shorter paraphrases, i.e., the compres-
sions are still intended to convey the same informa-
tion as the source sentences. By contrast, most sen-
tence compression methods (both extractive and ab-
stractive, including ours) are expected to retain only
the most important information of the source sen-
tence, in order to achieve better compression rates.
Hence, Zhao et al?s sentence compression task is not
the same as the task we are concerned with, and the
compressions we aim for are significantly shorter.
3 The new dataset
To construct the new dataset, we used source sen-
tences from the 570 pairs of Cohn and Lapata (Sec-
tion 2). This way a human gold abstractive com-
pression is also available for each source sentence,
though we do not currently use the gold compres-
sions in our experiments. We actually used only 346
of the 570 source sentences of Cohn and Lapata, re-
serving the remaining 224 for further experiments.4
To obtain candidate compressions, we first ap-
plied GA-EXTR to the 346 source sentences, and we
then applied the paraphrasing rules of Zhao et al
(2009b) to the resulting extractive compressions; we
provide more information about GA-EXTR and the
paraphrasing rules below. We decided to apply para-
phrasing rules to extractive compressions, because
we noticed that most of the 42 human abstractive
compressions of the 50 sample pairs from Cohn and
Lapata?s dataset that we initially considered (Sec-
tion 2) could be produced from the corresponding
source sentences by first deleting words and then us-
4The 346 sources are from 19 randomly selected articles
among the 30 that Cohn and Lapata drew source sentences from.
ing shorter paraphrases, as in the following example.
source: Constraints on recruiting are constraints on
safety and have to be removed.
extractive: Constraints on recruiting have to be re-
moved.
abstractive: Recruiting constraints must be removed.
3.1 Extractive candidate compressions
GA-EXTR, which we first applied to the dataset?s
source sentences, generates extractive candidate
compressions by pruning branches of each source?s
dependency tree; a Maximum Entropy classifier is
used to guide the pruning. Subsequently, GA-EXTR
ranks the extractive candidates using a Support Vec-
tor Regression (SVR) model, which assigns a score
F (eij |si) to each candidate extractive compression
eij of a source sentence si by examining features
of si and eij ; consult our previous work (Galanis
and Androutsopoulos, 2010) for details.5 For each
source si, we kept the (at most) kmax = 10 extrac-
tive candidates eij with the highest F (eij |si) scores.
3.2 Abstractive candidate compressions
We then applied Zhao et al?s (2009b) paraphrasing
rules to each one of the extractive compressions eij .
The rules are of the form left ? right, with left and
right being sequences of words and slots; the slots
are part-of-speech tagged and they can be filled in
with words of the corresponding categories. Exam-
ples of rules are shown below.
? get rid of NNS1? remove NNS1
? get into NNP1? enter NNP1
? NNP1 was written by NNP2? NNP2 wrote NNP1
Roughly speaking, the rules were extracted from
a parallel English-Chinese corpus, based on the as-
sumption that two English phrases ?1 and ?2 that
are often aligned to the same Chinese phrase ? are
5We trained GA-EXTR on approximately 1,050 pairs of
source sentences and gold human extractive compressions,
obtained from Edinburgh?s ?written? extractive dataset; see
http://jamesclarke.net/research/resources.
The source sentences of that dataset are from 82 documents.
The 1,050 pairs that we used had source sentences from 52 out
of the 82 documents. We did not use source sentences from
the other 30 documents, because they were used by Cohn and
Lapata (2008) to build their abstractive dataset (Section 2),
from which we drew source sentences for our dataset.
3
si
 '' ,,ei1
 '' ,,
ei2
,, -- ..
? ? ? eik
-- ..ai1.1
 && ,,
ai1.2
,,
? ? ? ai1.mi1 ai2.1 ai2.2 ? ? ? ai2.mi2 ? ? ? aik.1 ? ? ? aik.mik
ai1.1.1

ai1.1.2 ? ? ? ai1.1.mi1.1 ai1.2.1 ? ? ?
? ? ?
Figure 1: Generating candidate extractive (eij) and abstractive (aij...) compressions from a source sentence (si).
likely to be paraphrases and, hence, can be treated
as a paraphrasing rule ?1 ? ?2. This pivoting was
used, for example, by Bannard and Callison-Burch
(2005), and it underlies several other paraphrase
extraction methods (Riezler et al, 2007; Callison-
Burch, 2008; Kok and Brockett, 2010). Zhao et
al. (2009b) provide approximately one million rules,
but we use only approximately half of them, because
we use only rules that can shorten a sentence, and
only in the direction that shortens the sentence.
From each extractive candidate eij , we pro-
duced abstractive candidates aij.1, aij.2, . . . , aij.mij
(Figure 1) by applying a single (each time
different) applicable paraphrasing rule to eij .
From each of the resulting abstractive candidates
aij.l, we produced further abstractive candidates
aij.l.1, aij.l.2, . . . , aij.l.mij.l by applying again a sin-
gle (each time different) rule. We repeated this pro-
cess in a breadth-first manner, allowing up to at most
rulemax = 5 rule applications to an extractive candi-
date eij , i.e., up to depth six in Figure 1, and up to
a total of abstrmax = 50 abstractive candidates per
eij . Zhao et al (2009b) associate each paraphrasing
rule with a score, intended to indicate its quality.6
Whenever multiple paraphrasing rules could be ap-
plied, we applied the rule with the highest score first.
3.3 Human judgement annotations
For each one of the 346 sources si, we placed its
extractive (at most kmax = 10) and abstractive (at
most abstrmax = 50) candidate compressions into
a single pool (extractive and abstractive together),
and we selected from the pool the (at most) 10 can-
didate compressions cij with the highest language
6Each rule is actually associated with three scores. We use
the ?Model 1? score; see Zhao et al (2009b) for details.
model scores, computed using a 3-gram language
model.7 For each cij , we formed a pair ?si, cij?,
where si is a source sentence and cij a candidate
(extractive or abstractive) compression. This led to
3,072 ?si, cij? pairs. Each pair was given to a human
judge, who scored it for grammaticality (how gram-
matical cij was) and meaning preservation (to what
extent cij preserved the most important information
of si). Both scores were provided on a 1?5 scale (1
for rubbish, 5 for perfect). The dataset that we use
in the following sections and that we make publicly
available comprises the 3,072 pairs and their gram-
maticality and meaning preservation scores.
We define the GM score of an ?si, cij? pair to be
the sum of its grammaticality and meaning preser-
vation scores. Table 1 shows the distribution of
GM scores in the 3,072 pairs. Low GM scores (2?
5) are less frequent than higher scores (6?10), but
this is not surprising given that we selected pairs
whose cij had high language model scores, that
we used the kmax extractive compressions of each
si that GA-EXTR considered best, and that we as-
signed higher preference to applying paraphrasing
rules with higher scores. We note, however, that ap-
plying a paraphrasing rule does not necessarily pre-
serve neither grammaticality nor meaning, even if
the rule has a high score. Szpektor et al (2008) point
out that, for example, a rule like ?X acquire Y ??
?X buy Y ? may work well in many contexts, but
not in ?Children acquire language quickly?. Sim-
ilarly, ?X charged Y with? ? ?X accused Y of?
should not be applied to sentences about batteries.
Many (but not all) inappropriate rule applications
7We used SRILM with modified Kneser-Ney smoothing
(Stolcke, 2002). We trained the language model on approxi-
mately 4.5 million sentences from the TIPSTER corpus.
4
Training part Test part
GM extractive abstractive total extractive abstractive total
score candidates candidates candidates candidates candidates candidates
2 13 (1.3%) 10 (1.3%) 23 (1.3%) 19 (1.9%) 2 (0.4%) 21 (1.5%)
3 26 (2.7%) 28 (3.6%) 54 (3.1%) 10 (1.0%) 0 (0%) 10 (0.7%)
4 55 (5.8%) 29 (5.1%) 94 (5.5%) 51 (5.3%) 26 (6.2%) 77 (5.5%)
5 52 (5.5%) 65 (8.5%) 117 (6.9%) 77 (8.0%) 42 (10.0%) 119 (8.6%)
6 102 (10.9%) 74 (9.7%) 176 (10.3%) 125 (13.0%) 83 (19.8%) 208 (15.1%)
7 129 (13.8%) 128 (16.8%) 257 (15.1%) 151 (15.7%) 53 (12.6%) 204 (14.8%)
8 157 (16.8%) 175 (23.0%) 332 (19.5%) 138 (14.3%) 85 (20.3%) 223 (16.1%)
9 177 (18.9%) 132 (17.3%) 309 (18.2%) 183 (19.0%) 84 (20.1%) 267 (19.3%)
10 223 (23.8%) 110 (14.4%) 333 (19.6%) 205 (21.3%) 43 (10.2%) 248 (18.0%)
total 934 (55.1%) 761 (44.9%) 1,695 (100%) 959 (69.6%) 418 (30.4%) 1,377 (100%)
Table 1: Distribution of GM scores (grammaticality plus meaning preservation) in our dataset.
lead to low language model scores, which is partly
why there are more extractive than abstractive can-
didate compressions in the dataset; another reason is
that few or no paraphrasing rules apply to some of
the extractive candidates.
We use 1,695 (from 188 source sentences) of the
3,072 pairs to train different versions of our abstrac-
tive compressor?s ranking component, discussed be-
low, and 1,377 pairs (from 158 sources) as a test set.
3.4 Inter-annotator agreement
Although we used a total of 16 judges (computer sci-
ence graduate students), each one of the 3,072 pairs
was scored by a single judge, because a prelimi-
nary study indicated reasonably high inter-annotator
agreement.8 More specifically, before the dataset
was constructed, we created 161 ?si, cij? pairs (from
22 source sentences) in the same way, and we gave
them to 3 of the 16 judges. Each pair was scored by
all three judges. The average (over pairs of judges)
Pearson correlation of the grammaticality, meaning
preservation, and GM scores, was 0.63, 0.60, and
0.69, respectively.9 We conjecture that the higher
correlation of GM scores, compared to grammati-
cality and meaning preservation, is due to the fact
that when a candidate compression looks bad the
judges sometimes do not agree if they should re-
duce the grammaticality or the meaning preservation
8The judges were fluent, but not native, English speakers.
9The Pearson correlation ranges in [?1,+1] and measures
the linear relationship of two variables. A correlation of +1 in-
dicates perfect positive relationship, while ?1 indicates perfect
negative relationship; a correlation of 0 signals no relationship.
candidate average Pearson
compressions correlation
Extractive 112 0.71
Abstractive 49 0.64
All 161 0.69
Table 2: Inter-annotator agreement on GM scores.
score, but the difference does not show up in the GM
score (the sum). Table 2 shows the average corre-
lation of the GM scores of the three judges on the
161 pairs, and separately for pairs that involved ex-
tractive or abstractive candidate compressions. The
judges agreed more on extractive candidates, since
the paraphrasing stage that is involved in the abstrac-
tive candidates makes the task more subjective.10
3.5 Performance boundaries
When presented with two pairs ?si, cij? and?
si, cij?
?
with the same si and equally long cij and
cij? , an ideal ranking component should prefer the
pair with the highest GM score. More generally, to
consider the possibly different lengths of cij and cij? ,
we first define the compression rate CR(cij |si) of a
candidate compression cij as follows, where |?| is
length in characters; lower values of CR are better.
CR(cij |si) =
|cij |
|si|
The GMC? score of a candidate compression, which
also considers the compression rate by assigning it a
10The correlation that we measured on extractive candidates
(0.71) is very close to the corresponding figure (0.746) that has
been reported by Clarke and Lapata (2006b).
5
Figure 2: Results of three SVR-based ranking components on our dataset, along with performance boundaries obtained
using an oracle and a random baseline. The right diagram shows how the performance of our best SVR-based ranking
component is affected when using only 33% and 63% of the training examples.
weight ?, is then defined as follows.
GMC?(cij |si) = GM(cij |si)? ? ? CR(cij |si)
For a given ?, when presented with ?si, cij? and?
si, cij?
?
, an ideal ranking component should prefer
the pair with the highest GMC? score.
The upper curve of the left diagram of Figure 2
shows the performance of an ideal ranking com-
ponent, an oracle, on the test part of the dataset.
For every source si, the oracle selects the ?si, cij?
pair (among the at most 10 pairs of si) for which
GMC?(cij |si) is maximum; if two pairs have iden-
tical GMC? scores, it prefers the one with the low-
est CR(cij |si). The vertical axis shows the average
GM(cij |si) score of the selected pairs, for all the si
sources, and the horizontal axis shows the average
CR(cij |si). Different points of the curve are obtained
by using different ? values. As the selected candi-
dates get shorter (lower compression rate), the aver-
age GM score decreases, as one would expect.11
11The discontinuity in the oracle?s curve for average com-
The other curves of Figure 2 correspond to al-
ternative ranking components that we tested, dis-
cussed below, which do not consult the judges? GM
scores. For each si, these ranking components at-
tempt to guess the GM scores of the ?si, cij? pairs
that are available for si, and they then rank the pairs
by GMC? using the guessed GM scores. The lower
points of the left diagram were obtained with a base-
line ranking component that assigns a random GM
score to each pair. The oracle and the baseline can
be seen as establishing upper and lower performance
boundaries of ranking components on our dataset.
4 Our abstractive compressor
Our abstractive sentence compressor operates in two
stages. Given a source sentence si, extractive and
pression rates above 0.7, i.e., when long compressions are only
mildly penalized, is caused by the fact that many long candi-
date compressions have high and almost equal GM scores, but
still very different compression rates; hence, a slight modifica-
tion of ? leads the oracle to select candidates with the same GM
scores, but very different compression rates.
6
abstractive candidate compressions are first gener-
ated as in Sections 3.1 and 3.2. In a second stage, a
ranking component is used to select the best candi-
date. Below we discuss the three SVR-based ranking
components that we experimented with.
4.1 Ranking candidates with an SVR
An SVR is very similar to a Support Vector Machine
(Vapnik, 1998; Cristianini and Shawe-Taylor, 2000;
Joachims, 2002), but it is trained on examples of the
form ?xl, y(xl)?, where each xl ? Rn is a vector of n
features, and y(xl) ? R. The SVR learns a function
f : Rn ? R intended to return f(x) values as close
as possible to the correct y(x) values.12 In our case,
each vector xij contains features providing informa-
tion about an ?si, cij? pair of a source sentence si
and a candidate compression cij . For pairs that have
been scored by human judges, the f(xij) returned by
the SVR should ideally be y(xij) = GMC?(cij |si);
once trained, however, the SVR may be presented
with xij vectors of unseen ?si, cij? pairs.
For an unseen source si, our abstractive compres-
sor first generates extractive and abstractive candi-
dates cij , it then forms the vectors xij of all the
pairs ?si, cij?, and it returns the cij for which the
SVR?s f(xij) is maximum. On a test set (like the
test part of our dataset), if the f(xij) values the
SVR returns are very close to the corresponding
y(xij) = GMC?(cij |si) scores, the ranking compo-
nent will tend to select the same cij for each si as the
oracle, i.e., it will achieve optimum performance.
4.2 Base form of our SVR ranking component
The simplest form of our SVR-based ranking compo-
nent, called SVR-BASE, uses vectors xij that include
the following features of ?si, cij?. Hereafter, if cij is
an extractive candidate, then e(cij) = cij ; otherwise
e(cij) is the extractive candidate that cij was derived
from by applying paraphrasing rules.13
? The language model score of si and cij (2 fea-
12We use LIBSVM (http://www.csie.ntu.edu.tw/
?cjlin/libsvm) with an RBF kernel, which permits the
SVR to learn non-linear functions. We also experimented with a
ranking SVM, but the results were slightly inferior.
13All the feature values are normalized in [0, 1]; this also ap-
plies to the GMC? scores when they are used by the SVR. The
e(cij) of each cij and the paraphrasing rules that were applied
to e(cij) to produce cij are also included in the dataset.
tures), computed as in Section 3.3.
? The F (e(cij)|si) score that GA-EXTR returned.
? The compression rate CR(e(cij)|si).
? The number (possibly zero) of paraphrasing
rules that were applied to e(cij) to produce cij .
4.3 Additional PMI-based features
For two words w1, w2, their PMI score is:
PMI(w1, w2) = log
P (w1, w2)
P (w1) ? P (w2)
where P (w1, w2) is the probability of w1, w2 co-
occurring; we require them to co-occur in the same
sentence at a maximum distance of 10 tokens.14
If w1, w2 are completely independent, then their
PMI score is zero. If they always co-occur, their
PMI score is maximum, equal to ? logP (w1) =
? logP (w2).15 We use PMI to assess if the words
of a candidate compression co-occur as frequently
as those of the source sentence; if not, this may indi-
cate an inappropriate application of a paraphrasing
rule (e.g., having replaced ?charged Y with? by ?X
accused Y of? in a sentence about batteries).
More specifically, we define the PMI(?) score of
a sentence ? to be the average PMI(wi, wj) of ev-
ery two content words wi, wj that co-occur in ? at
a maximum distance of 10 tokens; below N is the
number of such pairs.
PMI(?) =
1
N
?
?
i,j
PMI(wi, wj)
In our second SVR-based ranking component, SVR-
PMI, we compute PMI(si), PMI(e), and PMI(cij),
and we include them as three additional features;
otherwise SVR-PMI is identical to SVR-BASE.
14We used texts from TIPSTER and AQUAINT, a total of 953
million tokens, to estimate PMI(w1, w2).
15A problem with PMI is that two frequent and completely de-
pendent words receive lower scores than two other, less frequent
completely dependent words (Manning and Schutze, 2000).
Pecina (2005), however, found PMI to be the best collocation
extraction measure; and Newman et al (2010) found it to be the
best measure of ?topical coherence? for sets of words.
7
4.4 Additional LDA-based features
Our third SVR-based ranking component includes
features from a Latent Dirichlet Allocation (LDA)
model (Blei et al, 2003). Roughly speaking, LDA
models assume that each document d of |d| words
w1, . . . , w|d| is generated by iteratively (for r =
1, . . . , |d|) selecting a topic tr from a document-
specific multinomial distribution P (t|d) overK top-
ics, and then (for each r) selecting a word wr from a
topic-specific multinomial distribution P (w|t) over
the vocabulary.16 The probability, then, of encoun-
tering a word w in a document d is the following.
P (w|d) =
?
t
P (w|t) ? P (t|d) (1)
An LDA model can be trained on a corpus to estimate
the parameters of the distributions it involves; and
given a trained model, there are methods to infer the
topic distribution P (t|d?) of a new document d?.17
In our case, we treat each source sentence as
a new document d?, and we use an LDA model
trained on a generic corpus to infer the topic distri-
bution P (t|d?) of the source sentence.18 We assume
that a good candidate compression should contain
words with high P (w|d?), computed as in Equation
1 with P (t|d) = P (t|d?) and using the P (w|t) that
was learnt during training, because words with high
P (w|d?) are more likely to express (high P (w|t))
prominent topics (high P (t|d?)) of the source.
Consequently, we can assess how good a can-
didate compression is by computing the average
P (w|d?) of its words; we actually compute the
average logP (w|d?). More specifically, for a
given source si and another sentence ?, we define
LDA(?|si) as follows (d? = si), where w1, . . . , w|?|
are now the words of ?, ignoring stop-words.
LDA(?|si) =
1
|?|
?
|?|?
r=1
logP (wr|si)
16The document-specific parameters of the first multinomial
distribution are drawn from a Dirichlet distribution.
17We use MALLET (http://mallet.cs.umass.edu),
with Gibbs sampling (Griffiths and Steyvers, 2004). We
set K = 800, having first experimented with K =
200, 400, 600, 800, 1000.
18We trained the LDA model on approximately 106,000 arti-
cles from the TIPSTER and AQUAINT corpora.
In our third SVR-based ranking component, SVR-
PMI-LDA, the feature vector xij of each ?si, cij?
pair includes LDA(cij |si), LDA(e(cij)|si), and
LDA(si|si) as additional features; otherwise, SVR-
PMI-LDA is identical to SVR-PMI. The third feature
allows the SVR to check how far LDA(cij |si) and
LDA(e(cij)|si) are from LDA(si|si).
5 Experiments
To assess the performance of SVR-BASE, SVR-PMI,
and SVR-PMI-LDA, we trained the three SVR-based
ranking components on the training part of our
dataset, and we evaluated them on the test part. We
repeated the experiments for 81 different ? values to
obtain average GM scores at different average com-
pression rates (Section 3.5). The resulting curves
of the three SVR-based ranking components are in-
cluded in Figure 2 (left diagram). Overall, SVR-
PMI-LDA performed better than SVR-PMI and SVR-
BASE, since it achieved the best average GM scores
throughout the range of average compression rates.
In general, SVR-PMI also performed better than
SVR-BASE, though the average GM score of SVR-
BASE was sometimes higher. All three SVR-based
ranking components performed better than the ran-
dom baseline, but worse than the oracle; hence, there
is scope for further improvements in the ranking
components, which is also why we believe other re-
searchers may wish to experiment with our dataset.
The oracle selected abstractive (as opposed to
simply extractive) candidates for 20 (13%) to 30
(19%, depending on ?) of the 158 source sentences
of the test part; the same applies to the SVR-based
ranking components. Hence, good abstractive can-
didates (or at least better than the corresponding ex-
tractive ones) are present in the dataset. Humans,
however, produce mostly abstractive compressions,
as already discussed; the fact that the oracle (which
uses human judgements) does not select abstrac-
tive candidates more frequently may be an indica-
tion that more or better abstractive candidates are
needed. We plan to investigate alternative methods
to produce more abstractive candidates. For exam-
ple, one could translate each source to multiple pivot
languages and back to the original language by using
multiple commercial machine translation engines in-
stead of, or in addition to applying paraphrasing
8
source generated
Gillette was considered a leading financial analyst on the beverage in-
dustry - one who also had an expert palate for wine tasting.
Gillette was seen as a leading financial analyst on the beverage industry
- one who also had an expert palate.
Nearly 200,000 lawsuits were brought by women who said they suf-
fered injuries ranging from minor inflammation to infertility and in
some cases, death.
Lawsuits were made by women who said they suffered injuries ranging
from inflammation to infertility in some cases, death.
Marcello Mastroianni, the witty, affable and darkly handsome Italian
actor who sprang on international consciousness in Federico Fellini?s
1960 classic ?La Dolce Vita,? died Wednesday at his Paris home.
Marcello Mastroianni died Wednesday at his home.
A pioneer in laparoscopy, he held over 30 patents for medical instru-
ments used in abdominal surgery such as tubal ligations.
He held over 30 patents for the medical tools used in abdominal surgery.
LOS ANGELES - James Arnold Doolittle, a Los Angeles dance im-
presario who brought names such as Joffrey and Baryshnikov to local
dance stages and ensured that a high-profile ?Nutcracker Suite? was
presented here every Christmas, has died.
James Arnold Doolittle, a Los Angeles dance impresario is dead.
After working as a cashier for a British filmmaker in Rome, he joined
an amateur theatrical group at the University of Rome, where he was
taking some classes.
After working as a cashier for a British filmmaker in Rome, he joined
an amateur group at the University of Rome, where he was using some
classes.
He was a 1953 graduate of the Johns Hopkins Medical School and after
completing his residency in gynecology and surgery, traveled to Den-
mark where he joined the staff of the National Cancer Center there.
He was a graduate of the Johns Hopkins Medical School and traveled
to Denmark where he joined a member of the National Cancer Center
there.
Mastroianni, a comic but also suave and romantic leading man in some
120 motion pictures, had suffered from pancreatic cancer.
Mastroianni, a leading man in some 120 motion pictures, had subjected
to cancer.
Table 3: Examples of good (upper five) and bad (lower three) compressions generated by our abstractive compressor.
rules. An approach of this kind has been proposed
for sentence paraphrasing (Zhao et al, 2010).
The right diagram of Figure 2 shows how the per-
formance of SVR-PMI-LDA is affected when using
33% or 63% of the training ?si, ci? pairs. As more
examples are used, the performance improves, sug-
gesting that better results could be obtained by using
more training data. Finally, Table 3 shows examples
of good and bad compressions the abstractive com-
pressor produced with SVR-PMI-LDA.
6 Conclusions and future work
We presented a new dataset that can be used to train
and evaluate the ranking components of generate-
and-rank abstractive sentence compressors. The
dataset contains pairs of source sentences and can-
didate extractive or abstractive compressions. The
candidate compressions were obtained by first ap-
plying a state-of-the-art extractive compressor to the
source sentences, and then applying existing para-
phrasing rules, obtained from parallel corpora. The
dataset?s pairs have been scored by human judges
for grammaticality and meaning preservation. We
discussed how performance boundaries for ranking
components that use the dataset can be established
by using an oracle and a random baseline, and by
considering different compression rates. We also
discussed the current version of an abstractive sen-
tence compressor that we are developing, and how
the dataset was used to train and evaluate three dif-
ferent SVR-based ranking components of the com-
pressor with gradually more elaborate features sets.
The feature set of the best ranking component that
we tested includes language model scores, the con-
fidence and compression rate of the underlying ex-
tractive compressor, the number of paraphrasing
rules that have been applied, word co-occurrence
features, as well as features based on an LDA model.
In future work, we plan to improve our abstractive
sentence compressor, possibly by including more
features in the ranking component. We also plan
to investigate alternative ways to produce candidate
compressions, such as sentence paraphrasing meth-
ods that exploit multiple commercial machine trans-
lation engines to translate the source sentences to
multiple pivot languages and back to the original
language (Zhao et al, 2010). Using methods of this
kind, it may be possible to produce a second, alterna-
tive dataset with more and possibly better abstractive
candidates. We also plan to make the final version of
our abstractive compressor publicly available.
Acknowledgments
This work was partly carried out during INDIGO, an
FP6 IST project funded by the European Union, with
additional funding from the Greek General Secre-
9
tariat of Research and Technology.19
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of
ACL, pages 597?604, Ann Arbor, MI.
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
allocation. In Journal of Machine Learning Research.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation
research. In Proceedings of EACL, pages 249?256,
Trento, Italy.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of EMNLP, pages 196?205, Honolulu, HI.
J. Clarke and M. Lapata. 2006a. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of ACL-COLING.
J. Clarke and M. Lapata. 2006b. Models for sentence
compression: A comparison across domains, training
requirements and evaluation measures. In Proceedings
of ACL-COLING.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear programming
approach. Journal of Artificial Intelligence Research,
1(31):399?429.
T. Cohn and M. Lapata. 2007. Large margin syn-
chronous generation and its application to sentence
compression. In Proceedings of EMNLP-CONLL.
T. Cohn and M. Lapata. 2008. Sentence compression
beyond word deletion. In Proceedings of COLING.
T. Cohn and M. Lapata. 2009. Sentence compression as
tree to tree tranduction. Journal of Artificial Intelli-
gence Research, 34:637?674.
S. Corston-Oliver. 2001. Text compaction for display
on very small screens. In Proceedings of the NAACL
Workshop on Automatic Summarization.
N. Cristianini and J. Shawe-Taylor. 2000. An In-
troduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Univer-
sity Press.
D. Galanis and I. Androutsopoulos. 2010. An extrac-
tive supervised two-stage method for sentence com-
pression. In Proceedings of HLT-NAACL.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topics. In Proceedings of the National Academy of Sci-
ences.
19Consult http://www.ics.forth.gr/indigo/.
H. Jing. 2000. Sentence reduction for automatic text
summarization. In Proceedings of ANLP.
T. Joachims. 2002. Learning to Classify Text Using Sup-
port Vector Machines: Methods, Theory, Algorithms.
Kluwer.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the HLT-
NAACL, pages 455?462, New York, NY.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probalistic approach to
sentence compression. Artificial Intelligence, 139(1).
P. Koehn. 2009. Statistical Machine Translation. Cam-
bridge University Press.
S. Kok and C. Brockett. 2010. Hitting the right para-
phrases in good time. In Proceedings of HLT-NAACL,
pages 145?153, Los Angeles, CA.
N. Madnani and B.J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
N. Madnani, D. Zajic, B. Dorr, N. F. Ayan, and J. Lin.
2007. Multiple alternative sentence compressions
for automatic text summarization. In Proceedings of
DUC.
C.D. Manning and H. Schutze. 2000. Foundations of
Statistical Natural Language Processing. MIT Press.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
EACL.
D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. 2010.
Automatic evaluation of topic coherence. In Proceed-
ings of HLT-NAACL.
T. Nomoto. 2009. A comparison of model free versus
model intensive approaches to sentence compression.
In Proceedings of EMNLP.
J. F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Robust machine translation evaluation with en-
tailment features. In Proceedings of ACL-IJCNLP,
pages 297?305, Singapore.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318,
Philadelphia, PA.
P. Pecina. 2005. An extensive empirical study of colloca-
tion extraction methods. In Proceedings of the Student
Research Workshop of ACL.
S. Riezler, T.H. King, R. Crouch, and A. Zaenen.
2003. Statistical sentence condensation using ambigu-
ity packing and stochastic disambiguation methods for
lexical-functional grammar. In Proceedings of HLT-
NAACL.
10
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proceedings of ACL,
pages 464?471, Prague, Czech Republic.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of the International Con-
ference on Spoken Language Processing, pages 901?
904.
I. Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.
2008. Contextual preferences. In Proceedings of
ACL-HLT, pages 683?691, Columbus, OH.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2005. Support vector machine learning for indepen-
dent and structured output spaces. Machine Learning
Research, 6:1453?1484.
V. Vapnik. 1998. Statistical Learning Theory. John Wi-
ley.
E. Yamangil and S. M. Shieber. 2010. Bayesian syn-
chronous tree-substitution grammar induction and its
application to sentence compression. In Proceedings
of ACL.
S. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008. Com-
bining multiple resources to improve SMT-based para-
phrasing model. In Proceedings of ACL-HLT, pages
1021?1029, Columbus, OH.
S. Zhao, X. Lan, T. Liu, and S. Li. 2009a. Application-
driven statistical paraphrase generation. In Proceed-
ings of ACL.
S. Zhao, H. Wang, T. Liu, and S. Li. 2009b. Extract-
ing paraphrase patterns from bilingual parallel cor-
pora. Natural Language Engineering, 15(4):503?526.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010. Leverag-
ing multiple MT engines for paraphrase generation. In
Proceedings of COLING.
L. Zhou, C.-Y. Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of EMNLP, pages 77?84.
11
