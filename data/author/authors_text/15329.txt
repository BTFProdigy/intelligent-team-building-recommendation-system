A Class-based Probabil ist ic approach to Structural 
Disambiguat ion 
Stephen Clark and David Weir 
School of Cognit ive and Comput ing  Sciences 
University of Sussex 
Brighton, BN1 9IIQ, UK 
{ st ephec:l_, david~r}Ocogs, usx. ac. uk 
Abstract  
Knowledge of which words are able to fill p~rtic- 
ular argum.ent slots of a predicate can be used 
tbr structural disambiguation. This paper de- 
scribes a proposal :for acquiring such knowledge, 
and in line with much of the recent work in this 
area, a probabilistic approach is taken. We de- 
velop a novel way of using a semantic hierar- 
chy to estimate the probabilities, and demon- 
strate the general approach using a preposi- 
tional phrase atta.chment experiment. 
1 Introduction 
Knowledge of which words are able to fill 
particular a.rgument slots of a. l?redlca.te ca,n 
be used tbr structural disa.mbiguation. In 
the following example (Charnial~, 1993), the 
fact that dog, rather than prize, is often 
the su.1)ject of r'lm, can t)e used to decide 
on the attachment site of the relative clause: 
Fred awarded a prize for the dog that ran the fastest 
We describe a proposal for acquiring such 
knowledge, and as in other recent work in this 
area (Resnik, 1993; l,i and Abe, t998), a prob- 
abilistic approach is taken. Using probabilities 
accords with the intuition that there are no ab- 
solute constraints on the arguments of predi- 
cates, bu.t rather that constraints are satisfied 
to a certain degree (Resnik, 1993). Unfortu- 
nately, defining probabilities in terms of words 
leads to a model with a vast number of param- 
eters, resulting in a sparse data problem. To 
overcome this, we propose to define a probabil- 
ity model in terms of senses from a semantic hi- 
erarchy, exploiting the fact that senses of nouns 
can be grouped together into semantically sim- 
ilar classes. 
We use the semantic hierarchy of noun senses 
in WordNet (Fellbamn, 1.998), which consists of 
qexicalised concepts' related by the qs-a-kind- 
of' relation. If c' is a kind of c, then c is a hy- 
pcrnym of c', and c' a hyponym of c. Counts are 
passed u.p the hierarchy fl'om the senses of nouns 
appearing in the data. Thus if cat chicl~cn a.p- 
pears in the data, th.e count for this item passes 
u\]) to (meat}, (good}, and all the other hyper- 
nyms of that sense of chicken. 1 In. order to es- 
timate the probability that a sense of chichcn 
~tppea.rs as the object of the verb cat, we repre- 
sent (chicken} using a. suitable hypern3qn , such 
as (:eood), and base our probability estimate on 
that instead. The level at which (chicken) is 
represented is cruciah it should be high enough 
for adequate counts to have accumulated, but 
not too high so that the hypernym is no longer 
representat ive  of (chicken}.  An exanlp le  of a 
hypernym whidl would be too high is (e r t t i ty ) ,  
as not all entities are semantically similar with 
respect o the object position ot7 cat. 
The problem of choosing an appropria.te l vel 
in the h.ierarchy at which to represent a par- 
ticular noun sense (given a predicate and argu- 
ment position) has been investigated by Resnik 
(1993), Li and Abe (1998) and ll,iba,s (1995). 
The learning mechanism presented \]lore is a 
novel approach based on tinding semantically 
similar sets of concepts in a hierarchy. We 
demonstrate the effectiveness of our approach 
using a PP-attachment experiment. 
2 The Input Data  and Semant ic  
Hierarchy 
The data used to estimate the probal)ilities is 
a multiset of 'co-occurrence triples': a noun 
IWe use italics when referring to words, ~Lnd angled 
bra.ckets for concepts. This notation does not alwa.ys 
pick out a concept uniquely, but the context should make 
cle~n: the concept being referred to. 
194 
\]enltna., verb len, ina, and argunien/, i)osition. 2 
l,et the li;ilivc.rso of verbs~ argll l i lent posi- 
tions aud ,lOtlBS tha.t call appear in the in- 
put data. be denoted "l) = { "Vl~... ~ 'vkv }~ "\]~., : 
{ , , . , , . . . ,  ,.,,~ } a,d N = { ' ,~, , . . . ,  ",J,H }, ~'esi,e,- 
tiw:ly. Sucll data c~ui I)e obtMned fro,,, a. tree- 
ban,s, or from a. shallow pa:rser. Note tha.t we 
do l ie, distinguish I)etwee, i a.lternative seiises of 
ve,'\])s~ a,lld assUIfle tha.t each \]llsta.llCe O\[ a. no,i l l  
i,I t i le data, refel?s to exactly olle conc, ept .  
The sei-i-la.ii{ic hiel:a.rchy used is the tie,Ill hy- 
\[)efltylIl ta.xo:nonly of \/Vo,'dNet (vel'SiOll \]..6). :3 
l,e~ (7,' = { e l , . . . ,  Chc } be tile sot; of concepts 
in WordNet (lq: ,-~ 66,000). A concel,t is rel)re- 
sented in \?ordNet 1,y a synset: a sel o1' syilolly- 
,nous words which cat, I)<7: used to (lenotc lha.1 
c.oncei)t. I!kil7 exa.nll)iO ~ the COliC;el)l, ~co('.a.iile~ 
a.s iii the (\[rtlg~ iS represented l)y l.he following 
synset: {cocaine, co(-ai't~, coD(;, .,no'w, C}. l,et 
syn(c) C ;V l,e the syllscl; I'or the (:olicel)t c, 
a.d  let c,,(,,.) - { c I"" m sy,,(+:) } I,e the. set of 
concepts that  (;a, ii be denoted by the llO,lli 17.. 
The \]liera.rc\] U has the stl'llCtlll'e O\[ a directed 
acyciic gi'a.l,\]i : althougli the nunll)er of nodes in 
the gra.ph wil;h lllO,'e 1,ha.l/ olle \])al'elll. is only 
a,rOtllid cite i)er(tent o f  the  tota l .  The  edges ill 
the graph \['orni what we call the dii'ecl.-isa rela-- 
Lion (dire('.t-is;,. C C' x {'). l,et isa = dii'ect-isa. X 
be ,lie tI'a, llSitivc~ re\[lexive (;\]OStlre Of (lirect-isa, 
so t\]iat (c/, c) ~ \]sa :=> c is a \] iy\])ernynl o;\[' (/; a.nd 
let ?~ = { c' \[(c' ,c) misa. } I)e the set consisting 
of the concept c and all of its hyponynis. Thus, 
the set (*ood) conta.ins all the concel)ts wtiich 
:q,re kinds of food, inclllditig (food). 
Note tha.t words in the data can al)pear in 
SyllSCts a, liy\vh01'0 ill the  hiel;archy. Even  COll- 
cel)ts Sllch a.s (entity)~ which a,l)pe.ar 1,ear t\]ie 
l:OOt el" the hierarchy, have synsets containing 
words which may a.pl)ear hi the da.ta. The 
synset for (entity) is {c'ntitg, something}, and 
the words cntit;q a.nd something (';/,ll apl)ear in 
the a.rgulnent positions of verbs in the data. 
3 Probab i l i ty  Es t imat ion  
The problem being a.ddressed in this section is 
to est imate p(civ, r), for c C C, v < P, and 
2Only verbs a.re considered here, but this work applies 
to other predicates which take a.rgunmnts that can bc 
orga.nised into a semantic hierarchy. 
3When wc refer (.o concepts ill \'VoMNct, wc nlea.n 
concepts ill WordNet's nomi ta, xonomy.  
r C 'R.. The I~roba.bility p(eiv , r )  is the 1)rob- 
ability tha.t some lie\ill in syn(c)~ when dellOf 
itlg coneet)t c, appears in position 'r of" verl) 'v 
(given r a.nd v). Using the relative clause ex- 
anlpie fl'Otil tile hi,reduction> the p,'obal)ilities 
p((dog}lru,z,subj ) a, nd p((prize)lrv.',z,sub.i ) ca.n 
be toni_pared to decide on the attachluent site 
f l  i,i I red awa'rded a p?'izc Jot iitc dog t/tat ran 
~l,; f,.~:~;.,.:. We expe~t 1'((dog) l""', ~m,.i) to 1,e 
grea.ter than  p((pr?ze) l'l'zt'~z,subj). Al thougt ,  the 
\['OCllS iS O11 7,(c1~,,,), the  tochniq l ies  descr ibed  
here can be used to estimat, e other lm)babilities, 
such a.s p(c, f ly  ). (in fa.ct, the latter prol)alfii- 
il, y is used hi the Pl>-a?ta.clinient CXl)erhnents 
de.qcril)ed in Section 5.) 
Using n, axinlun/ likelihood to cstinia.tc 
\])(C\['V, '/')iS i iot  via.hie 1)eca.use of the l iugc nui l l -  
t)er of I)al'al,l or.ors i\]i vo\] red. ~/lal ly COl IIt,i II a.tiolls 
O\[ C, 'l) a.lld 7' will / lot OCCIII" in the data .  '1'o re- 
duce  the ntii,il)ei' of i)a.ranletcrs which need to 
\])e estima.ted~ we utilise tile fa.ct tha.t COllCepts 
Call be grouped ini;o cla.sses, a.nd \ ] 'epresent  C IlS- 
ing a class (/, for some hypernynl c' of c. Ilow- 
ever, p(c'lv , r)ca.nnot be used as a.n estiniate of 
v(( l , , ,  ,'), as V((:I"', "') is give.  l,y the foliowi,,g: 
E . (  c'l,,, .,.) = v( ( : ' l . ,  ,) 
<:"C~ 
The probal)ility ~)(("l'~") i,,c,'eases as c' 
moves up the hiera.rchy. For example,  
s)((eooa)l,;aZ,oi,.i ) is ,,or a good estiu,a,te of 
1,((chicken)leat,obj ). What can be done 
though, is to condition on sets of concepts, and 
use the probabil ity p(v\[c', r). If it ca:n be shown 
tha.t p(v\[c', r), for some hypernym c' of c, is a. 
reasoliable esti lnate o1' v(vlc, v), then we have a. 
wa.y of estiniati,ig p(clv, r). To get \])(vie; , r ) f rom 
l ,(dv, ,') i~ayes ,',ie is ,,sed: 
p(4*,,,') p(vl~, "'v(cl'') = 7 ) ~  
The prol)abilities p(clr ) and p(v\[r) cm~ be esti- 
mated using maximum likelihood esti,n~tes, a.s 
the conditioning event is likely to occur often 
enough for sp;tl'se data not to be a problem. 
(Alternatively ()tie could ha.ok-off to p(c) and 
ply) respectively, or use a. linear combhia.tion 
of p(d' , )a. ,d \],(c), ,.,d P0,1v)a,d V('0, ,'espoc- 
tively.) The formula.e for these est imates will 
I,e give,, shortly. This only leaves plY\[c, r). The 
195 
proposM is to estilnate P(eatl(~h?cken>, oh j) us- 
ing  p(eat\](food), oh j), or something similar. The 
following proposition shows that if p(vlc" , r) is 
I 
the same for each c" in c ' ,  where c' is some 
hypernym of c, then p(v\]c', r) will be equal to 
p(v\[c, r): 
- -  I 
\],(~1c",7.) :/~ for all c" ~ c' ~ \],(vlc',7.)= 
The proo f  is as fo l lows:  
,\],(~17") \],(vl7,7") = \ ] , (71~, , )~ 
- ~'(L 17") ~ p(c"lv,7,) 
z,(c'l,') - clinG 1 
_ z,(~l~') V" \],(~ d', 7.~ \]'(c'17) 
z,(c~lT,) ~ ' ' p(~lT) 
_ 2 a, ~ p(c"l,0 
p(c'lT,) _ 
Cl ing I 
= /~ 
So in order to estimate p(v\[c,r), we need a 
way of searching for a set c', where c' is a hy- 
pernym of c, which consists of concepts c" which 
have similar p(v\]c", r). Of conrse we cannot ex- 
pect to find a set consisting of concepts which 
have identical p(vlc", r), which the proposition 
strictly requires, but if the p(vlc" , 7") are simila.r, 
then we can expect p(vld , r) to be a. reasonable 
estimate of p(vlc , 7"). We refer to the set c' as 
the %imilarity-class' of c, and the suitable hy- 
pernym, c l, as top(c, v, r). The next section ex- 
plains how we determine similarity classes. The 
maxim.urn likelihood estimates for the relevant 
probabilities m:e given in Ta.ble 1.4 
4 F ind ing  S imi la r i ty -c lasses  
First we explain how we determine if a set of 
concepts has similar p(vlc", r) for each concept 
c" in the set. Then we explain how we determine 
top(c, v, r). 
4Since we are a.ssuming the data. is not sense dis- 
a.mbiguated, f,:eq(c, v, r) cannot be obtained by sim- 
ply counting senses. The standard approach, which is 
adopted here, is to estimate fl'eq(c, v, r) by distributing 
the count tor each noun n in syn(c) evenly among all 
senses of the noun. Yarowsky (1992) and \]{esnik (1993) 
explain how the noise introduced by this technique tends 
to dissipate as counts are passed up the hierarchy. 
Table 1: Maximum Likelihood Esti:lnates 
freq(c, v, r) is the number of (n, v, r) triples in 
the data in which n is being used to denote c. 
fl 'eq(c,r) Ev'EV freq(c,v',r) 
P(CI? ' )  : " frcq(r)  - -  Ev 'EVEdcc f rcq(c ' ,v ' , r )  
freq(v,r) Ec'Ec freq(ct,v,r) 
/ ) (VlT")-  freq(r) : Zv ,EVZc ,  ccfreq(c',v',r) 
\]}(vie w, 7") -- freq(c-i"v'r) Z~"c~77freq(c't'v'r) 
rreq(d,,-) = Ev,evE~,,~Tf,-eq(~",'~,',,-) 
Tile method used for comparing the p(vlc" , r) 
for c" in some set c', is based on the technique 
ill Clark and Weir (1999) used for tinding homo- 
geneous ets of concepts in the WordNet noun 
hierarchy. Rather than directly compare esti- 
mates ofp(vlc" , r), which are likely to be unreli- 
able, we consider the children of c', and use esti- 
mates based on counts which have accumulated 
I , /  , /  at the children. If c' has children Q,%, . . . ,  c,,,,, 
I 
we compare e'(~l<, ") for each i. Th~s is an 
I 
a.pproximation, but if the p(vlc}, r) arc similar, 
I 
then we assume that the p(vlc" ,r)  for c" in c' 
are similar too. 
To deterlnine whether the children of some 
? . , ./is the hyperny,~ c' have simila,' \]'('~'14) where c~ 
ith child, we apply a X 2 test to a contingency 
tM)le of frequency counts. Table 2 shows some 
e?a.mple frequencies for c' equM to (nutriment), 
in the ol)ject position of cat. The figures in 
brackets are the expected values, based on the 
marginal totMs in the table. The null hypoth- 
esis of the test is that p(vl@ r ) i s  the same for 
each i. libr TM)1e 2 the null hypothesis is tlmt 
,I tbr every child, ci, of (nutr?ment}, the probabil- 
ity p(catlc~, obj) is the same. 
The log-likelihood X 2 statistic corresponding 
to TM)le 2 is 4.8. The log-likelihood X 2 statistic 
is used rather than the Pearson's X 2 statistic 
because it is thought to be more appropriate 
when the counts in the contingency table are 
low (\])unning, 1993). This tends to occur when 
the test is being applied to a set of concepts 
near the foot of the hierarchy, s We compared 
5Fisher,s exa.ct test could be used for tables with low 
counts, but we do not do so because tables dolninated 
by low counts are likely to have a. high percentage of 
noise, due to the way counts for a noun are split ~unong 
196 
ridable 2: Contingency tal)le for children of (nutriment) 
ci 
milk) 
<meal) 
(course) 
(d?s~) 
(del?cacy) 
f r \ [N(~,  cat, oh.i) 
o.o (o.(~) 
J.a (l.r) 
s.a (s.r) 
o.a (~ .s) 
\] 5.d 
r,.~,q(~, oh.i)- 
l't'(x I(~, cal, oh j) 
9.0 (s..,~) 
rs.o (so..o) 
24.r (24.a) 
s2.a (s~ ..0) 
2r.4 (~s.9) 
221.4 
r,4q(~, oh.i) = 
E, ,~v  r,.~,q(W, . ,, oh.i) 
9.0 
86.5 
26.0 
87.6 
27.7 
the l)erformance of log-likelihood X 2 and Pear- 
son's X ~2 using the l>P-~tttaehment experhnent 
described in Section 5. It was found that the 
log-likelihood ~2 test; did perform slightly bet- 
t('r. \]"or a signitic~nce l w;I ot' 0.05 (which is the 
level used in the exl)eriments), with 4 degrees 
of freedom, the critical wdue is 1,1.86 (llowell, 
;1!197). Thus in this ca.se, tlle null hyl~othesis 
would not be rejected. 
In order to determine top(c, v, r), we conlpare 
l,(vl~7, v) re,: the children of the hypernyms of 
c. hlitially top(c, 'v, r) ix assigned to I)e the con- 
eet)t c itself. Then, l>y worldng Ull the hierarrclly, 
top((:, 'V, r) is reassigned to I)(' successive hyl)er- 
nyms of c until the siblings of tol)(C , ~7+ 7')have 
siglfifi(:a.ntly different prol)abilities. In cases 
where a. concept has more than one I)a.J'ent, the 
parent is chosen which results in tile lowest :\~2 
wflue as this indicates the p(v\[U,r) are more 
simila.r. The set top(c ,v , r )  is the sinfi\]a.rity- 
cla.ss of c t'or verb v and position r. 
Th(; next section provides evidence that tile 
technique for choosing lOl)(C , v, r), which we call 
the 'simihu'ity-class' technique, does select an 
appropriate level of generalisation. 
5 Exper iments  us ing  PP -a t tachment  
ambigu i ty  
The l>P-atta.chme:nt problem we address con- 
siders 4-tuples of the form v,:,t,,pr, n2, and 
the l)robleln is to decide wllether tile prel)o- 
sitional phrase pr n2 attaches to the verl> v 
or the 71oun nl. For exatnl)le, in the fol- 
lowing cas(; tim l)rol)lent is to decide whether 
a l ternat ive  senses.  YVe rely on the  log- l ikel ihood X ,2 test  
re turn ing  a, non-s ign i f i cant  result  in these cases. 
J)'om minister attaches to awaii or approvah 
a.wait apt)7'owd from minister 
We chose the l~P-attachn~ent l)roblenl beca.use 
P l>-attaehment is a perw,.sive form of ambiguity, 
and there exist sta.ndard training and text da.ta~ 
which ma.kes for easy comparisons with other 
a.pproache~s. This p7'oblenl has been tackled by a 
nu nlber of resea.rehers, lh'ill and Resnik (1994), 
Ratnal)arkhi et al (\]994), Collins (1995), Za- 
w:el and l)aelemans (\] 997) all report results be- 
tween 81% and 85%, with Stetina. and Nagao 
(\] 997) tel)erring a result of 88%, which matches 
lhe hunm,t+ l>erf'ornlan(;e on this task rel)orted by 
Ratnal>arkhi (% al. (199.'1). 
Althougll th(' l)l)-attachnwnt l)roblem has 
chara('teristics that n,a.ke it suita.ble for ('valua.- 
t;ion, it; I)resents a inuch bigger sparse data. t)\]:ol)- 
le, m tlla.n would 1)e exl)ected in other l)roblems 
such as relative (:lausc atSadlment. The reason 
for this is that we need 1;(7 cot,sider how ~l C()l~ - 
Cel)t is associated with combi~zations of predi- 
cates and prel)ositions. T\]le al)proach described 
11(;7"(; uses prolml)ilities of the Ibrnl p(c, prlv ) 
,u,d ~,,(c.z,,l,,.,), who,;o ,~ ~ ~,l(,+~). Th is  . lea, is  
that for many predicate/prel)osition combina- 
tions which occur infl'equently in the d~ta., there 
are few examples of n2 which ca.n be used lot 
populating Wo7'dNet in these cases. Despite 
this, we were still able to carry out an ewl.lu- 
ation by considering subsets of the test (ta.ta for 
which the relewmt predicate~preposition com- 
I)inations did occur frequently in tit(; training 
d at a,. 
We deckle on tile a.tta('hnmnt site by compar- 
197 
ing p(c~, pr\[v) and p(c,~,, p,'\],q), where 
= a rg n ax l,(c,p,'lv) 
c,z 1 = arg max p(c, prlTq ) 
The sense of n2 is chosen which maximises 
the relevant probability in each potential at- 
tachment case. If p(c,,,p,jv)is greater than 
1)(%, :m'l~l), the attachment is made to v, oth- 
erwise to nl. If n2 is not in WordNet we com- 
pare p(prlv ) and p(prl~t~). Probabilities of the 
form p(c, prlv ) and p(c, prl~tl ) are used rather 
than p(clv,pr ) and p(cl~l,p,j, because the as- 
sociation between the preposition and v and ~q 
contains useful information. In fact, for a lot 
of cases this intbrmation alone can be used to 
decide on the correct attachment site_ The orig- 
inal corpus-based method of \]Jindle and ll.ooth 
(1993) used exactly this information. Thus the 
method described here can be thought of as Hin- 
dle and Rooth's method with additional class- 
based information about n2. 
In order to estimate p(c,, ,pr lv)(and 
p(C,~l,ln'l,,,,)) we apply the same procedure 
as described in Section 3, first rewriting the 
probability using Bayes' rule: 
p,,.)p(c,,, p,.) p (c , , , j , , l v ) - -  p(vlcv, v(v) 
p,.) !'(P'q c,, ) 
: p(dc,,, l,(v) 
The probabilities p(c.~) and p(v) can be es- 
timated using maximum likelihood estimates, 
a.nd p(vlcv, p,' ) and j,(p,'lc,) can be esti- 
m.ated using maximum likelihood estimates of 
p(vltop(c~ ,v,p,'),pr) and p(prltop(%,pr)) re- 
spectively. 6 
We used the training and test data described 
in l/.atn.aparkhi et al (1994:), which, was taken 
Doln the Penn %:eebank and has now become 
the standard data set for this task. The data 
set consists of tuples of the form (v, ~zl, p~', n2), 
together with the attachment site for each tu- 
ple. There is also a development set to prevent 
implicit training on the test set during develop- 
ment. \~e extracted (v, pr, '~2) and (hi, pr, ,z2) 
~ln Section 4 we only gave the procedure for deter- 
mining top(c~, v, pr), but top(c~, pr) can be determined 
in an analogous fashion. 
triples from the training set, and in order to in- 
crease the number of training triples, we also 
extracted triples Kern unambiguous cases of at- 
tachlnent in the Penn %'eebank. We prepro- 
cessed the training and test data by \]emmatising 
the words, replacing numerical amounts with 
the words ~definite_quantity', replacing mone- 
tary amounts with the words 'sum_olLmoney' 
etc. We then ignored those triples in the re- 
sulting training set (but not test set) for which 
7z2 was not in WordNet, which left a total of 
66,881 triples of training data.. The test set 
contains 3,097 examples. 
Table 3 gives seine examples of the ex- 
tent to which the similarity-class technique 
is generalising, using the training data just 
described, and a significance level of 0.05. 
The chosen hypernym is shown in Ul)per 
case. Note that the WordNet hierarchy con- 
sists of nine separate sub-hierarchies, headed 
by such concepts  as (ent i ty>,  (abst rac t ion) ,  
(psycho log ica l~eature) ,  bnt  we assume the ex- 
istence of a single root which dominates each 
of the sub-hierarchies, which is referred to as 
(root>. In cases where WordNet is very sparsely 
populated, it is preferable to go to (root), 
rather than stay at the root of one of the sub- 
hierarchies where the data may be noisy or too 
sparse to be o\[' any use. The table shows that 
with the amount of data ava.ilable from the Tree- 
bank, the similarity-class technique is selecting 
a. level at or close to (root> in many cases. 
We compared the similarity-class technique 
with fixing the level of generalisation. Two tixed 
levels were used: the root of the entire hieraJ'- 
chy ((root>), and the set consisting of the roots 
of each of the 9 sul>hierarchies. The procedure 
which always selects (root} ignores any informa- 
tion about ~z2, and is equivalent o comparing 
p(prlv ) and p(prl ,h),  which is the ltindle and 
Rooth approach. The results on the 3,097 test 
cases are shown in Table 4. We used a. signifi- 
cance level a of 0.05 tbr the X 2 test. r
As the table shows, the disambiguation ac- 
curacy is below the state of the art. However, 
the results are comparable with those of l,i and 
rSimilar results were obtained using alternative l vels 
of signifiea.nce. Rather than simply selecting a value for 
a, such as 0.05, a' can be tree,ted as a parameter of the 
model, whose optimum value caJl be obtained by running 
the disambiguation method on some held-out supervised 
data. 
198 
'l'al)le 3: Ilow the simila.rity-cla.ss technique chooses top(c, v, pr)a.lld top(c, nq, pr) 
(?Zl, \])?', C) I Iypernyms of c 
( bid,for,(company) ) 
( ~i~io,,,,i,,,,<c~sh> ) 
(v, l", c) 
('l,ol, i,J!q, O J; <t tans act ion>) 
( clo.5",8, (t\[,, <def i n i t  e_quant  ity>) 
( , ,~ ,  wiU,,,<oeeioia~> ) 
< company> <establ ishment> (or ganisat  ion)<social _group> (GROUP) <root> 
(risk) <venture> (task)(+york> (activity)<act> (ROOT> 
( cash>( curt ency)(monetary mystem) (asset)(POSSESS I ON)(root ) 
<transact i on)<group_act ion) <act >(ROOT) 
<D RF II~ I TE_QUAN= TY> <mea~ur e> <abst tact ion> <root> 
<o~i~ial><adjudicator><perso~><li~e~orm><CAUSA~,~G~,NT><e~tity><root> 
'l'able <1: ( ,o~) le te  test set :~()97 test cases 
(~eneralisation technique % co:red. 
SiJnila.rity-cla.ss 80.3 
Select root of sub-hiera.rchy 77.9 
Alwa,ys select (root> 79.0 
Table 5: (root> 1)eing selected for 1)oth a.ttach- 
nlent 1)oints \] 713 test cases 
(hmeralisa+tion technique 
Simila.rity-cla.ss 
Select root of sub-hierarchy 
Ahva.ys select (root> 
% tort'cot 
90.3 
811.4 
79.6 
Abe (119!)8) who a.dol)t a similar a.l>proa(:h us- 
i:ng \VorclNet, but with a, differ<rot raining and 
test set. I,i a.nd Abe iml>rOVed on the l\[\]n- 
die and Rooth techni(lue l)y 1.5%, whh;h is i, 
line with our results. As a.n evahla.tion of the 
simibu'ity-class tec\]lnique, the result is incon- 
clusive. The rca.son for this is tha.t when the 
technique wa,s being used to estima.te \])( vlc,,, \])r ) 
a.Hd P(?~.:I \[c.,zl, I)?'), in many cases t i le root  o1" 1lie 
hiera.rchy wa.s being chosen as the apl>rOl)riat;e 
level of genera.lisa.tion, due to a. sparsely popu- 
la.ted WordNet in tha.t insta.nce. Recall that this 
is la.rgely due to tit<', fa.ct that we a.rc a.ttemltt- 
ing to popula.te WordNet fbr comltina.tions of 
predic~tes ~md prepositions. In such cases tile 
sinlil~u'ity-elass technique is not helping because 
there is very little or no informa.tion a.1)otlt ~,2. s
aln an effort to obtahl more do.to, we a, pplicd the ex- 
traction heuristic of lla.tna.parkhi (1998) to \?all Street 
Journa.l text, which increased the nuntl)er of training 
triples by ~L factor of 111. '\['his only a.chievcd comparable 
results, however, presumably boca.use the high volume 
of noise in the dat~ outweighs the benefit of the increase 
in da.ta size. \]{.atnaparkhi reports only 69% a.ccuracy tot 
Table 6: (root> being select(,(I for at most one 
o1' ill(, a.tl.a('hnmnt points 1032 i.cst ('asc.~ 
(l(,,eralisaCioll techniqu(,~ % ('ol:re('t 
Sitnilaril.y-cla.ss 88. I 
Select root of sul)-hierar(:hy 85.5 
Alwa.ys select ( root )  8,5.6 
In order to eva.lua.te the similarity-class tech- 
nique further, we took those test cases for which 
tile root wa, s not being selected when estima.ting 
bet:t, J,(,,I,,~. J,') .+,,d \])('/,.1 I~,,. pv). n:\],is .pplied 
to 113 c~ses. The results ~u;e given in Table 5. 
We a.lso took those test cases for which the root 
was I)eing selected when estimating +~t most one 
of p(v\[c+,,pr) a.nd p(,q \[c,~, pr). This a.pplie, d to 
\]032 test ca.sos. The results a.re shown in %> 
ble 6. 
the extraction heuristic when applied to the \]%nn Tree- 
ba.nk (excluding cases where the ln:eposition is of). 
199 
6 Conc lus ions  
We have shown that when instances of Word- 
Net are well populated with examples of 
n2, the method described here for solving 
P1)-attachment ambiguities is highly accurate. 
When WordNet is sparsely populated, the 
method automatically resorts to comparing just 
the preposition and each of the potential attach- 
ment sites, as the similarity-class technique will 
select {root} as the appropriate l vel of general- 
\]sat\]on for n2 in such cases. We have also shown 
the similarity-class technique to be superior to 
using a fixed level of general\]sat\]on in WordNet. 
Further work will look at how to integrate 
probabilities uch as p(clv, r) into a model of 
dependency structure, similar to that of Collins 
(1996) and Collins (1997), which can be used 
\['or parse selection. However, knowledge of se- 
\]ectional preferences cannot by itself solve the 
problem of structural disambiguation, and this 
further work will also look at using additional 
knowledge, such a.s subcategorisation informa- 
tion. 
Re ferences  
Eric Brill a.nd Philip Resnik. 1994. A rule-based 
approach to prel)ositional phrase a.tta.chment 
disanfl)iguation, in 1)~vcccdi'ngs of the .\[iJ- 
Icc~th International Co~@rcncc on C'ompu- 
rational Linguistics. 
Eugene (~harnia.k. 1993. ,5'tali.slical Language 
Lcarni'ng. The MIT Press. 
Stephen Clark and l)avid Weir. 1999. An it- 
erative approach to estimating frequencies 
over a semantic hierarchy. In P'lvcccdinqs of 
the Joint ,57GDA 2' ConJ~rcncc on Empirical 
Methods in Natural Language Proccssi'ng and 
Very Large Co17~ora , \])ages 258 265. 
Michael Collins. 11995. Prepositional phrase 
attachment through a backed-off model. In 
Proceedings of the Third l?orhshop on Very 
Large Cou)ora , pages 27-38, Cambridge, 
Massachusetts. 
Michael Collins. 1996. A new statistical parser 
based on bigram lexical dependencies. In 
P~vcecdings of the 3/tth Annual Meeting of the 
A CL, pages 184-1911. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
cccdings of the 351h A~mual Mccti'ng of the 
Association for Computational Linguistics, 
pages 16-23. 
Ted \])unning. 1993. Accurate luethods Ibr the 
statistics of surprise and coincidence. Com- 
putational Linguistics, 19(1):61-74. 
Christiane Fellbaum, editor. 1998. WordNct 
An l'2lcctronic Lcxical Database. The MIT 
Press. 
Donald ltindle and Mats Rooth. 1993. Struc- 
tural ambiguity and lexical relations. Com- 
putational Linguistics, \] 9(1): 103-120. 
l)avid Howell. 11997. Statistical Methods for 
Psychology: ~th cd. Duxbury Press. 
Hang Li and Naoki Abe. 11998. Genera.liz- 
ing case frames using a thesaurus and the 
MI)L principle. Computational Linguislics, 
24(2): 17-244. 
Adwait l{a.t:na.parldli, Jeff Reynar, and Saliln 
Roukos. 1994. A maximum entropy model 
fbr prepositional phrase attachment. In P,v- 
cccdings of l, hc A RI)A Human Language "l~ch- 
nology Workshop, pages 250-255. 
Adwait \]/.atnaparkhi. 1998. Unsupervised sta- 
tistical models tbr prepositional phrase at- 
tachment. In P~vcccdings of thc ,5'cvcnlccnth 
hzicrnalionol ConJE'rencc on Computational 
Linguistics, Montreal, Canada, Aug. 
Pllilip Rcsnik. 71993. ,5'clcction a~zd hdbrma- 
lion: A Class-Based Approach to l, czical Re- 
lationships. Ph.l). thesis, University of Penn- 
sylvania. 
Francesc l{ibas. 1995. On learning more appro- 
priate selectional restrictions. In Procccdings 
of the ,5'cvcnth ConJ~rcncc of the IJuropcan 
Chapter of the Association for Computational 
I,i~tguistics, l) U blin, Irelal, d. 
;liri Stetina and Makoto Na.gao. 1997. Corpus 
based PP attachment ambiguity resolution 
with a semantic dictionary. In Proceedings of 
thc Fiflh I?orteshop on Very Large Corpora, 
pages 66-80, Beijing and ltong Kong. 
David Yarowsky. 11992. Word-sense disam- 
biguation using statistical models of Roger's 
categories trained on large corpora. \]n P,v- 
cccdin.qs of COLING-92, pages 454-460. 
Jakub Zaw:el and Walter l)aelemans. 1997. 
Melnory-based learning: Using similarity for 
smoothing. In Proceedings of A CL/EACL- 
97, Madrid, Spain. 
200 
Characterising Measures of Lexical Distributional Similarity
Julie Weeds, David Weir and Diana McCarthy
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{juliewe, davidw,dianam}@sussex.ac.uk
Abstract
This work investigates the variation in a word?s dis-
tributionally nearest neighbours with respect to the
similarity measure used. We identify one type of
variation as being the relative frequency of the neigh-
bour words with respect to the frequency of the tar-
get word. We then demonstrate a three-way connec-
tion between relative frequency of similar words, a
concept of distributional gnerality and the seman-
tic relation of hyponymy. Finally, we consider the
impact that this has on one application of distribu-
tional similarity methods (judging the composition-
ality of collocations).
1 Introduction
Over recent years, many Natural Language Pro-
cessing (NLP) techniques have been developed
that might benefit from knowledge of distribu-
tionally similar words, i.e., words that occur in
similar contexts. For example, the sparse data
problem can make it difficult to construct lan-
guage models which predict combinations of lex-
ical events. Similarity-based smoothing (Brown
et al, 1992; Dagan et al, 1999) is an intuitively
appealing approach to this problem where prob-
abilities of unseen co-occurrences are estimated
from probabilities of seen co-occurrences of dis-
tributionally similar events.
Other potential applications apply the hy-
pothesised relationship (Harris, 1968) between
distributional similarity and semantic similar-
ity; i.e., similarity in the meaning of words can
be predicted from their distributional similarity.
One advantage of automatically generated the-
sauruses (Grefenstette, 1994; Lin, 1998; Curran
and Moens, 2002) over large-scale manually cre-
ated thesauruses such as WordNet (Fellbaum,
1998) is that they might be tailored to a partic-
ular genre or domain.
However, due to the lack of a tight defini-
tion for the concept of distributional similarity
and the broad range of potential applications, a
large number of measures of distributional sim-
ilarity have been proposed or adopted (see Sec-
tion 2). Previous work on the evaluation of dis-
tributional similarity methods tends to either
compare sets of distributionally similar words
to a manually created semantic resource (Lin,
1998; Curran and Moens, 2002) or be oriented
towards a particular task such as language mod-
elling (Dagan et al, 1999; Lee, 1999). The first
approach is not ideal since it assumes that the
goal of distributional similarity methods is to
predict semantic similarity and that the seman-
tic resource used is a valid gold standard. Fur-
ther, the second approach is clearly advanta-
geous when one wishes to apply distributional
similarity methods in a particular application
area. However, it is not at all obvious that one
universally best measure exists for all applica-
tions (Weeds and Weir, 2003). Thus, applying a
distributional similarity technique to a new ap-
plication necessitates evaluating a large number
of distributional similarity measures in addition
to evaluating the new model or algorithm.
We propose a shift in focus from attempting
to discover the overall best distributional sim-
ilarity measure to analysing the statistical and
linguistic properties of sets of distributionally
similar words returned by different measures.
This will make it possible to predict in advance
of any experimental evaluation which distribu-
tional similarity measures might be most appro-
priate for a particular application.
Further, we explore a problem faced by
the automatic thesaurus generation community,
which is that distributional similarity methods
do not seem to offer any obvious way to dis-
tinguish between the semantic relations of syn-
onymy, antonymy and hyponymy. Previous
work on this problem (Caraballo, 1999; Lin et
al., 2003) involves identifying specific phrasal
patterns within text e.g., ?Xs and other Ys? is
used as evidence that X is a hyponym of Y. Our
work explores the connection between relative
frequency, distributional generality and seman-
tic generality with promising results.
The rest of this paper is organised as follows.
In Section 2, we present ten distributional simi-
larity measures that have been proposed for use
in NLP. In Section 3, we analyse the variation in
neighbour sets returned by these measures. In
Section 4, we take one fundamental statistical
property (word frequency) and analyse correla-
tion between this and the nearest neighbour sets
generated. In Section 5, we relate relative fre-
quency to a concept of distributional generality
and the semantic relation of hyponymy. In Sec-
tion 6, we consider the effects that this has on a
potential application of distributional similarity
techniques, which is judging compositionality of
collocations.
2 Distributional similarity measures
In this section, we introduce some basic con-
cepts and then discuss the ten distributional
similarity measures used in this study.
The co-occurrence types of a target word are
the contexts, c, in which it occurs and these
have associated frequencies which may be used
to form probability estimates. In our work, the
co-occurrence types are always grammatical de-
pendency relations. For example, in Sections 3
to 5, similarity between nouns is derived from
their co-occurrences with verbs in the direct-
object position. In Section 6, similarity between
verbs is derived from their subjects and objects.
The k nearest neighbours of a target word w
are the k words for which similarity with w is
greatest. Our use of the term similarity measure
encompasses measures which should strictly be
referred to as distance, divergence or dissimilar-
ity measures. An increase in distance correlates
with a decrease in similarity. However, either
type of measure can be used to find the k near-
est neighbours of a target word.
Table 1 lists ten distributional similarity mea-
sures. The cosine measure (Salton and McGill,
1983) returns the cosine of the angle between
two vectors.
The Jensen-Shannon (JS) divergence measure
(Rao, 1983) and the ?-skew divergence measure
(Lee, 1999) are based on the Kullback-Leibler
(KL) divergence measure. The KL divergence,
or relative entropy, D(p||q), between two prob-
ability distribution functions p and q is defined
(Cover and Thomas, 1991) as the ?inefficiency
of assuming that the distribution is q when the
true distribution is p?: D(p||q) =
?
c p log
p
q .
However, D(p||q) = ? if there are any con-
texts c for which p(c) > 0 and q(c) = 0. Thus,
this measure cannot be used directly on maxi-
mum likelihood estimate (MLE) probabilities.
One possible solution is to use the JS diver-
gence measure, which measures the cost of using
the average distribution in place of each individ-
ual distribution. Another is the ?-skew diver-
gence measure, which uses the p distribution to
smooth the q distribution. The value of the pa-
rameter ? controls the extent to which the KL
divergence is approximated. We use ? = 0.99
since this provides a close approximation to the
KL divergence and has been shown to provide
good results in previous research (Lee, 2001).
The confusion probability (Sugawara et al,
1985) is an estimate of the probability that one
word can be substituted for another. Words
w1 and w2 are completely confusable if we are
equally as likely to see w2 in a given context as
we are to see w1 in that context.
Jaccard?s coefficient (Salton and McGill,
1983) calculates the proportion of features be-
longing to either word that are shared by both
words. In the simplest case, the features of a
word are defined as the contexts in which it has
been seen to occur. simja+mi is a variant (Lin,
1998) in which the features of a word are those
contexts for which the pointwise mutual infor-
mation (MI) between the word and the context
is positive, where MI can be calculated using
I(c, w) = log P (c|w)P (c) . The related Dice Coeffi-
cient (Frakes and Baeza-Yates, 1992) is omitted
here since it has been shown (van Rijsbergen,
1979) that Dice and Jaccard?s Coefficients are
monotonic in each other.
Lin?s Measure (Lin, 1998) is based on his
information-theoretic similarity theorem, which
states, ?the similarity between A and B is mea-
sured by the ratio between the amount of in-
formation needed to state the commonality of
A and B and the information needed to fully
describe what A and B are.?
The final three measures are settings in
the additive MI-based Co-occurrence Retrieval
Model (AMCRM) (Weeds and Weir, 2003;
Weeds, 2003). We can measure the precision
and the recall of a potential neighbour?s re-
trieval of the co-occurrences of the target word,
where the sets of required and retrieved co-
occurrences (F (w1) and F (w2) respectively) are
those co-occurrences for which MI is positive.
Neighbours with both high precision and high
recall retrieval can be obtained by computing
Measure Function
cosine simcm(w2, w1) =
?
c
P (c|w1).P (c|w2)
??
c
P (c|w1)2
?
c
P (c|w2)2
Jens.-Shan. distjs(w2, w1) = 12
(
D
(
p||p+q2
)
+D
(
q||p+q2
))
where p = P (c|w1) and q = P (c|w2)
?-skew dist?(w2, w1) = D (p||(?.q + (1? ?).p)) where p = P (c|w1) and q = P (c|w2)
conf. prob. simcp(w2|w1) =
?
c
P (w1|c).P (w2|c).P (c)
P (w1)
Jaccard?s simja(w2, w1) =
|F (w1)?F (w2)|
|F (w1)?F (w2)|
where F (w) = {c : P (c|v) > 0}
Jacc.+MI simja+mi(w2,W1) =
|F (w1)?F (w2)|
|F (w1)?F (w2)|
where F (w) = {c : I(c, w) > 0}
Lin?s simlin(w2, w1) =
?
F (w1)?F (w2)
(I(c,w1)+I(c,w2))
?
F (w1)
I(c,w1)+
?
F (w2)
I(c,w2)
where F (w) = {c : I(c, w) > 0}
precision simP(w2, w1) =
?
F (w1)?F (w2)
I(c,w2)
?
F (w2)
I(c,w2)
where F (w) = {c : I(c, w) > 0}
recall simR(w2, w1) =
?
F (w1)?F (w2)
I(c,w1)
?
F (w1)
I(c,w1)
where F (w) = {c : I(c, w) > 0}
harm. mean simhm(w2, w1) =
2.simP (w2,w1).simR(w2,w1)
simP (w2,w1)+simR(w2,w1)
where F (w) = {c : I(c, w) > 0}
Table 1: Ten distributional similarity measures
their harmonic mean (or F-score).
3 Overlap of neighbour sets
We have described a number of ways of calcu-
lating distributional similarity. We now con-
sider whether there is substantial variation in
a word?s distributionally nearest neighbours ac-
cording to the chosen measure. We do this by
calculating the overlap between neighbour sets
for 2000 nouns generated using different mea-
sures from direct-object data extracted from the
British National Corpus (BNC).
3.1 Experimental set-up
The data from which sets of nearest neighbours
are derived is direct-object data for 2000 nouns
extracted from the BNC using a robust accurate
statistical parser (RASP) (Briscoe and Carroll,
2002). For reasons of computational efficiency,
we limit ourselves to 2000 nouns and direct-
object relation data. Given the goal of compar-
ing neighbour sets generated by different mea-
sures, we would not expect these restrictions to
affect our findings. The complete set of 2000
nouns (WScomp) is the union of two sets WShigh
and WSlow for which nouns were selected on the
basis of frequency: WShigh contains the 1000
most frequently occurring nouns (frequency >
500), and WSlow contains the nouns ranked
3001-4000 (frequency ? 100). By excluding
mid-frequency nouns, we obtain a clear sepa-
ration between high and low frequency nouns.
The complete data-set consists of 1,596,798 co-
occurrence tokens distributed over 331,079 co-
occurrence types. From this data, we computed
the similarity between every pair of nouns ac-
cording to each distributional similarity mea-
sure. We then generated ranked sets of nearest
neighbours (of size k = 200 and where a word
is excluded from being a neighbour of itself) for
each word and each measure.
For a given word, we compute the overlap be-
tween neighbour sets using a comparison tech-
nique adapted from Lin (1998). Given a word
w, each word w? in WScomp is assigned a rank
score of k ? rank if it is one of the k near-
est neighbours of w using measure m and zero
otherwise. If NS(w,m) is the vector of such
scores for word w and measure m, then the
overlap, C(NS(w,m1),NS(w,m2)), of two neigh-
bour sets is the cosine between the two vectors:
C(NS(w,m1),NS(w,m2)) =
?
w? rm1(w
?, w)? rm2(w
?, w)
?k
i=1 i2
The overlap score indicates the extent to which
sets share members and the extent to which
they are in the same order. To achieve an over-
lap score of 1, the sets must contain exactly
the same items in exactly the same order. An
overlap score of 0 is obtained if the sets do not
contain any common items. If two sets share
roughly half their items and these shared items
are dispersed throughout the sets in a roughly
similar order, we would expect the overlap be-
tween sets to be around 0.5.
cm js ? cp ja ja+mi lin
cm 1.0(0.0) 0.69(0.12) 0.53(0.15) 0.33(0.09) 0.26(0.12) 0.28(0.15) 0.32(0.15)
js 0.69(0.12) 1.0(0.0) 0.81(0.10) 0.46(0.31) 0.48(0.18) 0.49(0.20) 0.55(0.16)
? 0.53(0.15) 0.81(0.10) 1.0(0.0) 0.61(0.08) 0.4(0.27) 0.39(0.25) 0.48(0.19)
cp 0.33(0.09) 0.46(0.31) 0.61(0.08) 1.0(0.0) 0.24(0.24) 0.20(0.18) 0.29(0.15)
ja 0.26(0.12) 0.48(0.18) 0.4(0.27) 0.24(0.24) 1.0(0.0) 0.81(0.08) 0.69(0.09)
ja+mi 0.28(0.15) 0.49(0.20) 0.39(0.25) 0.20(0.18) 0.81(0.08) 1.0(0.0) 0.81(0.10)
lin 0.32(0.15) 0.55(0.16) 0.48(0.19) 0.29(0.15) 0.69(0.09) 0.81(0.10) 1.0(0.0)
Table 2: Cross-comparison of first seven similarity measures in terms of mean overlap of neighbour
sets and corresponding standard deviations.
P R hm
cm 0.18(0.10) 0.31(0.13) 0.30(0.14)
js 0.19(0.12) 0.55(0.18) 0.51(0.18)
? 0.08(0.08) 0.74(0.14) 0.41(0.23)
cp 0.03(0.04) 0.57(0.10) 0.25(0.18)
ja 0.36(0.30) 0.38(0.30) 0.74(0.14)
ja+mi 0.42(0.30) 0.40(0.31) 0.86(0.07)
lin 0.46(0.25) 0.52(0.22) 0.95(0.039)
Table 3: Mean overlap scores for seven simi-
larity measures with precision, recall and the
harmonic mean in the AMCRM.
3.2 Results
Table 2 shows the mean overlap score between
every pair of the first seven measures in Table 1
calculated over WScomp. Table 3 shows the mean
overlap score between each of these measures
and precision, recall and the harmonic mean in
the AMCRM. In both tables, standard devia-
tions are given in brackets and boldface denotes
the highest levels of overlap for each measure.
For compactness, each measure is denoted by
its subscript from Table 1.
Although overlap between most pairs of
measures is greater than expected if sets of
200 neighbours were generated randomly from
WScomp (in this case, average overlap would be
0.08 and only the overlap between the pairs
(?,P) and (cp,P) is not significantly greater
than this at the 1% level), there are substan-
tial differences between the neighbour sets gen-
erated by different measures. For example, for
many pairs, neighbour sets do not appear to
have even half their members in common.
4 Frequency analysis
We have seen that there is a large variation in
neighbours selected by different similarity mea-
sures. In this section, we analyse how neighbour
sets vary with respect to one fundamental statis-
tical property ? word frequency. To do this, we
measure the bias in neighbour sets towards high
frequency nouns and consider how this varies
depending on whether the target noun is itself
a high frequency noun or low frequency noun.
4.1 Measuring bias
If a measure is biased towards selecting high fre-
quency words as neighbours, then we would ex-
pect that neighbour sets for this measure would
be made up mainly of words from WShigh. Fur-
ther, the more biased the measure is, the more
highly ranked these high frequency words will
tend to be. In other words, there will be high
overlap between neighbour sets generated con-
sidering all 2000 nouns as potential neighbours
and neighbour sets generated considering just
the nouns in WShigh as potential neighbours. In
the extreme case, where all of a noun?s k nearest
neighbours are high frequency nouns, the over-
lap with the high frequency noun neighbour set
will be 1 and the overlap with the low frequency
noun neighbour set will be 0. The inverse is, of
course, true if a measure is biased towards se-
lecting low frequency words as neighbours.
If NSwordset is the vector of neighbours (and
associated rank scores) for a given word, w, and
similarity measure, m, and generated consider-
ing just the words in wordset as potential neigh-
bours, then the overlap between two neighbour
sets can be computed using a cosine (as be-
fore). If Chigh = C(NScomp,NShigh) and Clow =
C(NScomp,NSlow), then we compute the bias to-
wards high frequency neighbours for word w us-
ing measure m as: biashighm(w) =
Chigh
Chigh+Clow
The value of this normalised score lies in the
range [0,1] where 1 indicates a neighbour set
completely made up of high frequency words, 0
indicates a neighbour set completely made up of
low frequency words and 0.5 indicates a neigh-
bour set with no biases towards high or low fre-
quency words. This score is more informative
than simply calculating the proportion of high
high freq. low freq.
target nouns target nouns
cm 0.90 0.87
js 0.94 0.70
? 0.98 0.90
cp 1.00 0.99
ja 0.99 0.21
ja+mi 0.95 0.14
lin 0.85 0.38
P 0.12 0.04
R 0.99 0.98
hm 0.92 0.28
Table 4: Mean value of biashigh according to
measure and frequency of target noun.
and low frequency words in each neighbour set
because it weights the importance of neighbours
by their rank in the set. Thus, a large number
of high frequency words in the positions clos-
est to the target word is considered more biased
than a large number of high frequency words
distributed throughout the neighbour set.
4.2 Results
Table 4 shows the mean value of the biashigh
score for every measure calculated over the set
of high frequency nouns and over the set of low
frequency nouns. The standard deviations (not
shown) all lie in the range [0,0.2]. Any deviation
from 0.5 of greater than 0.0234 is significant at
the 1% level.
For all measures and both sets of target
nouns, there appear to be strong tendencies to
select neighbours of particular frequencies. Fur-
ther, there appears to be three classes of mea-
sures: those that select high frequency nouns
as neighbours regardless of the frequency of the
target noun (cm, js, ?, cp andR); those that se-
lect low frequency nouns as neighbours regard-
less of the frequency of the target noun (P); and
those that select nouns of a similar frequency to
the target noun (ja, ja+mi, lin and hm).
This can also be considered in terms of distri-
butional generality. By definition, recall prefers
words that have occurred in more of the con-
texts that the target noun has, regardless of
whether it occurs in other contexts as well i.e.,
it prefers distributionally more general words.
The probability of this being the case increases
as the frequency of the potential neighbour in-
creases and so, recall tends to select high fre-
quency words. In contrast, precision prefers
words that have occurred in very few contexts
that the target word has not i.e., it prefers dis-
tributionally more specific words. The prob-
ability of this being the case increases as the
frequency of the potential neighbour decreases
and so, precision tends to select low frequency
words. The harmonic mean of precision and re-
call prefers words that have both high precision
and high recall. The probability of this being
the case is highest when the words are of sim-
ilar frequency and so, the harmonic mean will
tend to select words of a similar frequency.
5 Relative frequency and hyponymy
In this section, we consider the observed fre-
quency effects from a semantic perspective.
The concept of distributional generality in-
troduced in the previous section has parallels
with the linguistic relation of hyponymy, where
a hypernym is a semantically more general term
and a hyponym is a semantically more specific
term. For example, animal is an (indirect1) hy-
pernym of dog and conversely dog is an (indi-
rect) hyponym of animal. Although one can
obviously think of counter-examples, we would
generally expect that the more specific term dog
can only be used in contexts where animal can
be used and that the more general term animal
might be used in all of the contexts where dog
is used and possibly others. Thus, we might ex-
pect that distributional generality is correlated
with semantic generality ? a word has high
recall/low precision retrieval of its hyponyms?
co-occurrences and high precision/low recall re-
trieval of its hypernyms? co-occurrences.
Thus, if n1 and n2 are related and P(n2, n1) >
R(n2, n1), we might expect that n2 is a hy-
ponym of n1 and vice versa. However, having
discussed a connection between frequency and
distributional generality, we might also expect
to find that the frequency of the hypernymic
term is greater than that of the hyponymic
term. In order to test these hypotheses, we ex-
tracted all of the possible hyponym-hypernym
pairs (20, 415 pairs in total) from our list of 2000
nouns (using WordNet 1.6). We then calculated
the proportion for which the direction of the hy-
ponymy relation could be accurately predicted
by the relative values of precision and recall and
the proportion for which the direction of the hy-
ponymy relation could be accurately predicted
by relative frequency. We found that the direc-
tion of the hyponymy relation is correlated in
the predicted direction with the precision-recall
1There may be other concepts in the hypernym chain
between dog and animal e.g. carnivore and mammal.
values in 71% of cases and correlated in the pre-
dicted direction with relative frequency in 70%
of cases. This supports the idea of a three-way
linking between distributional generality, rela-
tive frequency and semantic generality. We now
consider the impact that this has on a potential
application of distributional similarity methods.
6 Compositionality of collocations
In its most general sense, a collocation is a ha-
bitual or lexicalised word combination. How-
ever, some collocations such as strong tea are
compositional, i.e., their meaning can be de-
termined from their constituents, whereas oth-
ers such as hot dog are not. Both types are
important in language generation since a sys-
tem must choose between alternatives but only
non-compositional ones are of interest in lan-
guage understanding since only these colloca-
tions need to be listed in the dictionary.
Baldwin et al (2003) explore empirical
models of compositionality for noun-noun com-
pounds and verb-particle constructions. Based
on the observation (Haspelmath, 2002) that
compositional collocations tend to be hyponyms
of their head constituent, they propose a model
which considers the semantic similarity between
a collocation and its constituent words.
McCarthy et al (2003) also investigate sev-
eral tests for compositionality including one
(simplexscore) based on the observation that
compositional collocations tend to be similar in
meaning to their constituent parts. They ex-
tract co-occurrence data for 111 phrasal verbs
(e.g. rip off ) and their simplex constituents
(e.g. rip) from the BNC using RASP and cal-
culate the value of simlin between each phrasal
verb and its simplex constituent. The test
simplexscore is used to rank the phrasal verbs
according to their similarity with their simplex
constituent. This ranking is correlated with hu-
man judgements of the compositionality of the
phrasal verbs using Spearman?s rank correlation
coefficient. The value obtained (0.0525) is dis-
appointing since it is not statistically significant
(the probability of this value under the null hy-
pothesis of ?no correlation? is 0.3).2
However, Haspelmath (2002) notes that a
compositional collocation is not just similar to
one of its constituents ? it can be considered to
be a hyponym of its head constituent. For ex-
ample, ?strong tea? is a type of ?tea? and ?to
2Other tests for compositionality investigated by Mc-
Carthy et al (2003) do much better.
Measure rs P (rs) under H0
simlin 0.0525 0.2946
precision -0.160 0.0475
recall 0.219 0.0110
harmonic mean 0.011 0.4562
Table 5: Correlation with compositionality for
different similarity measures
rip up? is a way of ?ripping?.
Thus, we hypothesised that a distributional
measure which tends to select more general
terms as neighbours of the phrasal verb (e.g. re-
call) would do better than measures that tend
to select more specific terms (e.g. precision) or
measures that tend to select terms of a similar
specificity (e.g simlin or the harmonic mean of
precision and recall).
Table 5 shows the results of using different
similarity measures with the simplexscore test
and data of McCarthy et al (2003). We now see
significant correlation between compositionality
judgements and distributional similarity of the
phrasal verb and its head constituent. The cor-
relation using the recall measure is significant
at the 5% level; thus we can conclude that if
the simplex verb has high recall retrieval of the
phrasal verb?s co-occurrences, then the phrasal
is likely to be compositional. The correlation
score using the precision measure is negative
since we would not expect the simplex verb to
be a hyponym of the phrasal verb and thus, if
the simplex verb does have high precision re-
trieval of the phrasal verb?s co-occurrences, it is
less likely to be compositional.
Finally, we obtained a very similar result
(0.217) by ranking phrasals according to their
inverse relative frequency with their simplex
constituent (i.e., freq(simplex)freq(phrasal) ). Thus, it would
seem that the three-way connection between
distributional generality, hyponymy and rela-
tive frequency exists for verbs as well as nouns.
7 Conclusions and further work
We have presented an analysis of a set of dis-
tributional similarity measures. We have seen
that there is a large amount of variation in the
neighbours selected by different measures and
therefore the choice of measure in a given appli-
cation is likely to be important.
We also identified one of the major axes of
variation in neighbour sets as being the fre-
quency of the neighbours selected relative to the
frequency of the target word. There are three
major classes of distributional similarity mea-
sures which can be characterised as 1) higher
frequency selecting or high recall measures; 2)
lower frequency selecting or high precision mea-
sures; and 3) similar frequency selecting or high
precision and recall measures.
A word tends to have high recall similarity
with its hyponyms and high precision similarity
with its hypernyms. Further, in the majority of
cases, it tends to be more frequent than its hy-
ponyms and less frequent than its hypernyms.
Thus, there would seem to a three way corre-
lation between word frequency, distributional
generality and semantic generality.
We have considered the impact of these ob-
servations on a technique which uses a distribu-
tional similarity measure to determine composi-
tionality of collocations. We saw that in this ap-
plication we achieve significantly better results
using a measure that tends to select higher fre-
quency words as neighbours rather than a mea-
sure that tends to select neighbours of a similar
frequency to the target word.
There are a variety of ways in which this work
might be extended. First, we could use the ob-
servations about distributional generality and
relative frequency to aid the process of organ-
ising distributionally similar words into hierar-
chies. Second, we could consider the impact of
frequency characteristics in other applications.
Third, for the general application of distribu-
tional similarity measures, it would be useful
to find other characteristics by which distribu-
tional similarity measures might be classified.
Acknowledgements
This work was funded by a UK EPSRC stu-
dentship to the first author, UK EPSRC project
GR/S26408/01 (NatHab) and UK EPSRC
project GR/N36494/01 (RASP). We would like
to thank Adam Kilgarriff and Bill Keller for use-
ful discussions.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka,
and Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL-2003 Workshop on Multiword
Expressions, pages 89?96, Sapporo, Japan.
Edward Briscoe and John Carroll. 2002. Robust ac-
curate statistical annotation of general text. In
Proceedings of LREC-2002, pages 1499?1504.
P.F. Brown, V.J. DellaPietra, P.V deSouza, J.C. Lai,
and R.L. Mercer. 1992. Class-based n-gram mod-
els of natural language. Computational Linguis-
tics, 18(4):467?479.
Sharon Caraballo. 1999. Automatic construction of
a hypernym-labelled noun hierarchy from text. In
Proceedings of ACL-99, pages 120?126.
T.M. Cover and J.A. Thomas. 1991. Elements of
Information Theory. Wiley, New York.
James R. Curran and Marc Moens. 2002. Im-
provements in automatic thesaurus extraction. In
ACL-SIGLEX Workshop on Unsupervised Lexical
Acquisition, Philadelphia.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1999.
Similarity-based models of word cooccurrence
probabilities. Machine Learning Journal, 34.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structures and Algo-
rithms. Prentice Hall.
Gregory Grefenstette. 1994. Corpus-derived first-,
second- and third-order word affinities. In Pro-
ceedings of Euralex, pages 279?290, Amsterdam.
Zelig S. Harris. 1968. Mathematical Structures of
Language. Wiley, New York.
Martin Haspelmath. 2002. Understanding Morphol-
ogy. Arnold Publishers.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of ACL-1999, pages 23?32.
Lillian Lee. 2001. On the effectiveness of the skew
divergence for statistical language analysis. Arti-
ficial Intelligence and Statistics, pages 65?72.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among dis-
tributionally similar words. In Proceedings of
IJCAI-03, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL ?98, pages 768?774, Montreal.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL-2003
Workshop on Multiword Expressions, pages 73?
80, Sapporo, Japan.
C. Radhakrishna Rao. 1983. Diversity: Its measure-
ment, decomposition, apportionment and analy-
sis. Sankyha: The Indian Journal of Statistics,
44(A):1?22.
G. Salton and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
K.M. Sugawara, K. Nishimura, K. Toshioka,
M. Okachi, and T. Kaneko. 1985. Isolated word
recognition using hidden markov models. In Pro-
ceedings of the ICASSP-1985, pages 1?4.
C.J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, second edition.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings
of EMNLP-2003, pages 81?88, Sapporo, Japan.
Julie Weeds. 2003. Measures and Applications of
Lexical Distributional Similarity. Ph.D. thesis,
Department of Informatics, University of Sussex.
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 291?299,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Parsing Mildly Non-projective Dependency Structures?
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
David Weir and John Carroll
Department of Informatics
University of Sussex, United Kingdom
{davidw,johnca}@sussex.ac.uk
Abstract
We present parsing algorithms for vari-
ous mildly non-projective dependency for-
malisms. In particular, algorithms are pre-
sented for: all well-nested structures of
gap degree at most 1, with the same com-
plexity as the best existing parsers for con-
stituency formalisms of equivalent genera-
tive power; all well-nested structures with
gap degree bounded by any constant k;
and a new class of structures with gap de-
gree up to k that includes some ill-nested
structures. The third case includes all the
gap degree k structures in a number of de-
pendency treebanks.
1 Introduction
Dependency parsers analyse a sentence in terms
of a set of directed links (dependencies) express-
ing the head-modifier and head-complement rela-
tionships which form the basis of predicate argu-
ment structure. We take dependency structures to
be directed trees, where each node corresponds to
a word and the root of the tree marks the syn-
tactic head of the sentence. For reasons of effi-
ciency, many practical implementations of depen-
dency parsing are restricted to projective struc-
tures, in which the subtree rooted at each word
covers a contiguous substring of the sentence.
However, while free word order languages such
as Czech do not satisfy this constraint, parsing
without the projectivity constraint is computation-
ally complex. Although it is possible to parse
non-projective structures in quadratic time under a
model in which each dependency decision is inde-
pendent of all the others (McDonald et al, 2005),
?Partially supported by MEC and FEDER (HUM2007-
66607-C04) and Xunta de Galicia (PGIDIT07SIN005206PR,
INCITE08E1R104022ES, INCITE08ENA305025ES, IN-
CITE08PXIB302179PR, Rede Galega de Proc. da Linguaxe
e RI, Bolsas para Estad??as INCITE ? FSE cofinanced).
the problem is intractable in the absence of this as-
sumption (McDonald and Satta, 2007).
Nivre and Nilsson (2005) observe that most
non-projective dependency structures appearing
in practice are ?close? to being projective, since
they contain only a small proportion of non-
projective arcs. This has led to the study of
classes of dependency structures that lie be-
tween projective and unrestricted non-projective
structures (Kuhlmann and Nivre, 2006; Havelka,
2007). Kuhlmann (2007) investigates several such
classes, based on well-nestedness and gap degree
constraints (Bodirsky et al, 2005), relating them
to lexicalised constituency grammar formalisms.
Specifically, he shows that: linear context-free
rewriting systems (LCFRS) with fan-out k (Vijay-
Shanker et al, 1987; Satta, 1992) induce the set
of dependency structures with gap degree at most
k ? 1; coupled context-free grammars in which
the maximal rank of a nonterminal is k (Hotz and
Pitsch, 1996) induce the set of well-nested depen-
dency structures with gap degree at most k ? 1;
and LTAGs (Joshi and Schabes, 1997) induce the
set of well-nested dependency structures with gap
degree at most 1.
These results establish that there must be
polynomial-time dependency parsing algorithms
for well-nested structures with bounded gap de-
gree, since such parsers exist for their correspond-
ing lexicalised constituency-based formalisms.
However, since most of the non-projective struc-
tures in treebanks are well-nested and have a small
gap degree (Kuhlmann and Nivre, 2006), devel-
oping efficient dependency parsing strategies for
these sets of structures has considerable practical
interest, since we would be able to parse directly
with dependencies in a data-driven manner, rather
than indirectly by constructing intermediate con-
stituency grammars and extracting dependencies
from constituency parses.
We address this problem with the following
contributions: (1) we define a parsing algorithm
291
for well-nested dependency structures of gap de-
gree 1, and prove its correctness. The parser runs
in time O(n7), the same complexity as the best
existing algorithms for LTAG (Eisner and Satta,
2000), and can be optimised to O(n6) in the non-
lexicalised case; (2) we generalise the previous al-
gorithm to any well-nested dependency structure
with gap degree at most k in time O(n5+2k); (3)
we generalise the previous parsers to be able to
analyse not only well-nested structures, but also
ill-nested structures with gap degree at most k sat-
isfying certain constraints1, in time O(n4+3k); and
(4) we characterise the set of structures covered by
this parser, which we call mildly ill-nested struc-
tures, and show that it includes all the trees present
in a number of dependency treebanks.
2 Preliminaries
A dependency graph for a string w1 . . . wn is a
graph G = (V,E), where V = {w1, . . . , wn}
and E ? V ? V . We write the edge (wi, wj)
as wi ? wj , meaning that the word wi is a syn-
tactic dependent (or a child) of wj or, conversely,
that wj is the governor (parent) of wi. We write
wi ?? wj to denote that there exists a (possi-
bly empty) path from wi to wj . The projection
of a node wi, denoted bwic, is the set of reflexive-
transitive dependents of wi, that is: bwic = {wj ?
V | wj ?? wi}. An interval (with endpoints i and
j) is a set of the form [i, j] = {wk | i ? k ? j}.
A dependency graph is said to be a tree if it is:
(1) acyclic: wj ? bwic implies wi ? wj 6? E; and
(2) each node has exactly one parent, except for
one node which we call the root or head. A graph
verifying these conditions and having a vertex set
V ? {w1, . . . , wn} is a partial dependency tree.
Given a dependency tree T = (V,E) and a node
u ? V , the subtree induced by the node u is the
graph Tu = (buc, Eu) where Eu = {wi ? wj ?
E | wj ? buc}.
2.1 Properties of dependency trees
We now define the concepts of gap degree and
well-nestedness (Kuhlmann and Nivre, 2006). Let
T be a (possibly partial) dependency tree for
w1 . . . wn: We say that T is projective if bwic is
an interval for every word wi. Thus every node
in the dependency structure must dominate a con-
tiguous substring in the sentence. The gap degree
1Parsing unrestricted ill-nested structures, even when the
gap degree is bounded, is NP-complete: these structures are
equivalent to LCFRS for which the recognition problem is
NP-complete (Satta, 1992).
of a particular node wk in T is the minimum g ? N
such that bwkc can be written as the union of g+1
intervals; that is, the number of discontinuities in
bwkc. The gap degree of the dependency tree T is
the maximum among the gap degrees of its nodes.
Note that T has gap degree 0 if and only if T is
projective. The subtrees induced by nodes wp and
wq are interleaved if bwpc ? bwqc = ? and there
are nodes wi, wj ? bwpc and wk, wl ? bwqc such
that i < k < j < l. A dependency tree T is
well-nested if it does not contain two interleaved
subtrees. A tree that is not well-nested is said to
be ill-nested. Note that projective trees are always
well-nested, but well-nested trees are not always
projective.
2.2 Dependency parsing schemata
The framework of parsing schemata (Sikkel,
1997) provides a uniform way to describe, anal-
yse and compare parsing algorithms. Parsing
schemata were initially defined for constituency-
based grammatical formalisms, but Go?mez-
Rodr??guez et al (2008a) define a variant of the
framework for dependency-based parsers. We
use these dependency parsing schemata to de-
fine parsers and prove their correctness. Due to
space constraints, we only provide brief outlines
of the main concepts behind dependency parsing
schemata.
The parsing schema approach considers pars-
ing as deduction, generating intermediate results
called items. An initial set of items is obtained
from the input sentence, and the parsing process
involves deduction steps which produce new items
from existing ones. Each item contains informa-
tion about the sentence?s structure, and a success-
ful parsing process produces at least one final item
providing a full dependency analysis for the sen-
tence or guaranteeing its existence. In a depen-
dency parsing schema, items are defined as sets of
partial dependency trees2. To define a parser by
means of a schema, we must define an item set
and provide a set of deduction steps that operate
on it. Given an item set I, the set of final items
for strings of length n is the set of items in I that
contain a full dependency tree for some arbitrary
string of length n. A final item containing a de-
pendency tree for a particular string w1 . . . wn is
said to be a correct final item for that string. These
2The formalism allows items to contain forests, and the
dependency structures inside items are defined in a notation
with terminal and preterminal nodes, but these are not needed
here.
292
concepts can be used to prove the correctness of
a parser: for each input string, a parsing schema?s
deduction steps allow us to infer a set of items,
called valid items for that string. A schema is said
to be sound if all valid final items it produces for
any arbitrary string are correct for that string. A
schema is said to be complete if all correct final
items are valid. A correct parsing schema is one
which is both sound and complete.
In constituency-based parsing schemata, deduc-
tion steps usually have grammar rules as side con-
ditions. In the case of dependency parsers it is
also possible to use grammars (Eisner and Satta,
1999), but many algorithms use a data-driven ap-
proach instead, making individual decisions about
which dependencies to create by using probabilis-
tic models (Eisner, 1996) or classifiers (Yamada
and Matsumoto, 2003). To represent these algo-
rithms as deduction systems, we use the notion
of D-rules (Covington, 1990). D-rules take the
form a ? b, which says that word b can have a
as a dependent. Deduction steps in non-grammar-
based parsers can be tied to the D-rules associated
with the links they create. In this way, we ob-
tain a representation of the underlying logic of the
parser while abstracting away from control struc-
tures (the particular model used to create the de-
cisions associated with D-rules). Furthermore, the
choice points in the parsing process and the infor-
mation we can use to make decisions are made ex-
plicit in the steps linked to D-rules.
3 The WG1 parser
3.1 Parsing schema for WG1
We define WG1, a parser for well-nested depen-
dency structures of gap degree ? 1, as follows:
The item set is IWG1 = I1 ? I2, with
I1 = {[i, j, h, , ] | i, j, h ? N, 1 ? h ? n,
1 ? i ? j ? n, h 6= j, h 6= i? 1},
where each item of the form [i, j, h, , ] repre-
sents the set of all well-nested partial dependency
trees3 with gap degree at most 1, rooted at wh, and
such that bwhc = {wh} ? [i, j], and
I2 = {[i, j, h, l, r] | i, j, h, l, r ? N, 1 ? h ? n,
1 ? i < l ? r < j ? n, h 6= j, h 6= i? 1,
h 6= l ? 1, h 6= r}
3In this and subsequent schemata, we use D-rules to ex-
press parsing decisions, so partial dependency trees are as-
sumed to be taken from the set of trees licensed by a set of
D-rules.
where each item of the form [i, j, h, l, r] represents
the set of all well-nested partial dependency trees
rooted at wh such that bwhc = {wh} ? ([i, j] \
[l, r]), and all the nodes (except possibly h) have
gap degree at most 1. We call items of this form
gapped items, and the interval [l, r] the gap of
the item. Note that the constraints h 6= j, h 6=
i + 1, h 6= l ? 1, h 6= r are added to items to
avoid redundancy in the item set. Since the result
of the expression {wh} ? ([i, j] \ [l, r]) for a given
head can be the same for different sets of values of
i, j, l, r, we restrict these values so that we cannot
get two different items representing the same de-
pendency structures. Items ? violating these con-
straints always have an alternative representation
that does not violate them, that we can express
with a normalising function nm(?) as follows:
nm([i, j, j, l, r]) = [i, j ? 1, j, l, r] (if r ? j ? 1 or r = ),
or [i, l ? 1, j, , ] (if r = j ? 1).
nm([i, j, l ? 1, l, r]) = [i, j, l ? 1, l ? 1, r](if l > i + 1),
or [r + 1, j, l ? 1, , ] (if l = i + 1).
nm([i, j, i ? 1, l, r]) = [i ? 1, j, i ? 1, l, r].
nm([i, j, r, l, r]) = [i, j, r, l, r ? 1] (if l < r),
or [i, j, r, , ] (if l = r).
nm([i, j, h, l, r]) = [i, j, h, l, r] for all other items.
When defining the deduction steps for this and
other parsers, we assume that they always produce
normalised items. For clarity, we do not explicitly
write this in the deduction steps, writing ? instead
of nm(?) as antecedents and consequents of steps.
The set of initial items is defined as the set
H = {[h, h, h, , ] | h ? N, 1 ? h ? n},
where each item [h, h, h, , ] represents the set
containing the trivial partial dependency tree con-
sisting of a single node wh and no links. This
same set of hypotheses can be used for all the
parsers, so we do not make it explicit for subse-
quent schemata. Note that initial items are sepa-
rate from the item set IWG1 and not subject to its
constraints, so they do not require normalisation.
The set of final items for strings of length n in
WG1 is defined as the set
F = {[1, n, h, , ] | h ? N, 1 ? h ? n},
which is the set of items in IWG1 containing de-
pendency trees for the complete input string (from
position 1 to n), with their head at any word wh.
The deduction steps of the parser can be seen in
Figure 1A.
The WG1 parser proceeds bottom-up, by build-
ing dependency subtrees and joining them to form
larger subtrees, until it finds a complete depen-
dency tree for the input sentence. The logic of
293
A. WG1 parser:
Link Ungapped:
[h1, h1, h1, , ]
[i2, j2, h2, , ]
[i2, j2, h1, , ] wh2 ? wh1
such that wh2 ? [i2, j2] ? wh1 /? [i2, j2],
Link Gapped:
[h1, h1, h1, , ]
[i2, j2, h2, l2, r2]
[i2, j2, h1, l2, r2] wh2 ? wh1
such that wh2 ? [i2, j2] \ [l2, r2] ? wh1 /? [i2, j2] \ [l2, r2],
Combine Ungapped:
[i, j, h, , ] [j + 1, k, h, , ]
[i, k, h, , ]
Combine Opening Gap:
[i, j, h, , ] [k, l, h, , ]
[i, l, h, j + 1, k ? 1]
such that j < k ? 1,
Combine Keeping Gap Left:
[i, j, h, l, r] [j + 1, k, h, , ]
[i, k, h, l, r]
Combine Keeping Gap Right:
[i, j, h, , ] [j + 1, k, h, l, r]
[i, k, h, l, r]
Combine Closing Gap:
[i, j, h, l, r] [l, r, h, , ]
[i, j, h, , ]
Combine Shrinking Gap Left:
[i, j, h, l, r] [l, k, h, , ]
[i, j, h, k + 1, r]
Combine Shrinking Gap Right:
[i, j, h, l, r] [k, r, h, , ]
[i, j, h, l, k ? 1]
Combine Shrinking Gap Centre:
[i, j, h, l, r] [l, r, h, l2, r2]
[i, j, h, l2, r2]
B. WGK parser:
Link:
[h1, h1, h1, []]
[i2, j2, h2, [(l1, r1), . . . , (lg, rg)]]
[i2, j2, h1, [(l1, r1), . . . , (lg, rg)]]
wh2 ? wh1
such that wh2 ? [i2, j2] \
?g
p=1[lp, rp]
?wh1 /? [i2, j2] \
?g
p=1[lp, rp].
Combine Shrinking Gap Right:
[i, j, h, [(l1, r1), . . . , (lq?1, rq?1), (lq, r?), (ls, rs), . . . , (lg, rg)]]
[rq + 1, r?, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
Combine Opening Gap:
[i, lq ? 1, h, [(l1, r1), . . . , (lq?1, rq?1)]]
[rq + 1, m, h, [(lq+1, rq+1), . . . , (lg, rg)]]
[i, m, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k and lq ? rq ,
Combine Shrinking Gap Left:
[i, j, h, [(l1, r1), . . . , (lq, rq), (l?, rs), (ls+1, rs+1), . . . , (lg, rg)]]
[l?, ls ? 1, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
Combine Keeping Gaps:
[i, j, h, [(l1, r1), . . . , (lq, rq)]]
[j + 1, m, h, [(lq+1, rq+1), . . . , (lg, rg)]]
[i, m, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k,
Combine Shrinking Gap Centre:
[i, j, h, [(l1, r1), . . . , (lq, rq), (l?, r?), (ls, rs), . . . , (lg, rg)]]
[l?, r?, h, [(lq+1, rq+1), . . . , (ls?1, rs?1)]]
[i, j, h, [(l1, r1), . . . , (lg, rg)]]
such that g ? k
C. Additional steps to turn WG1 into MG1:
Combine Interleaving:
[i, j, h, l, r] [l, k, h, r + 1, j]
[i, k, h, , ]
Combine Interleaving Gap C:
[i, j, h, l, r] [l, k, h, m, j]
[i, k, h, m, r]
such that m < r + 1,
Combine Interleaving Gap L:
[i, j, h, l, r]
[l, k, h, r + 1, u]
[i, k, h, j + 1, u]
such that u > j,
Combine Interleaving Gap R:
[i, j, h, l, r]
[k, m, h, r + 1, j]
[i, m, h, l, k ? 1]
such that k > l.
D. General form of the MGk Combine step:
[ia1 , iap+1 ? 1, h, [(ia1+1, ia2 ? 1), . . . , (iap?1+1, iap ? 1)]]
[ib1 , ibq+1 ? 1, h, [(ib1+1, ib2 ? 1), . . . , (ibq?1+1, ibq ? 1)]]
[imin(a1,b1), imax(ap+1,bq+1) ? 1, h, [(ig1 , ig1+1 ? 1), . . . , (igr , igr+1 ? 1)]]
for each string of length n with a?s located at positions a1 . . . ap(1 ? a1 < . . . < ap ? n), b?s at positions b1 . . . bq(1 ? b1 <
. . . < bq ? n), and g?s at positions g1 . . . gr(2 ? g1 < . . . < gr ? n ? 1), such that 1 ? p ? k, 1 ? q ? k, 0 ? r ? k ? 1,
p + q + r = n, and the string does not contain more than one consecutive appearance of the same symbol.
Figure 1: Deduction steps for the parsers defined in the paper.
the parser can be understood by considering how
it infers the item corresponding to the subtree in-
duced by a particular node, given the items for the
subtrees induced by the direct dependents of that
node. Suppose that, in a complete dependency
analysis for a sentence w1 . . . wn, the word wh
has wd1 . . . wdp as direct dependents (i.e. we have
dependency links wd1 ? wh, . . . , wdp ? wh).
Then, the item corresponding to the subtree in-
duced by wh is obtained from the ones correspond-
ing to the subtrees induced by wd1 . . . wdp by: (1)
applying the Link Ungapped or Link Gapped step
to each of the items corresponding to the subtrees
induced by the direct dependents, and to the hy-
pothesis [h, h, h, , ]. This allows us to infer p
items representing the result of linking each of the
dependent subtrees to the new head wh; (2) ap-
plying the various Combine steps to join all of the
294
items obtained in the previous step into a single
item. The Combine steps perform a union oper-
ation between subtrees. Therefore, the result is a
dependency tree containing all the dependent sub-
trees, and with all of them linked to h: this is
the subtree induced by wh. This process is ap-
plied repeatedly to build larger subtrees, until, if
the parsing process is successful, a final item is
found containing a dependency tree for the com-
plete sentence.
3.2 Proving correctness
The parsing schemata formalism can be used to
prove the correctness of a parsing schema. To
prove that WG1 is correct, we need to prove
its soundness and completeness.4 Soundness is
proven by checking that valid items always con-
tain well-nested trees. Completeness is proven by
induction, taking initial items as the base case and
showing that an item containing a correct subtree
for a string can always be obtained from items
corresponding to smaller subtrees. In order to
prove this induction step, we use the concept of
order annotations (Kuhlmann, 2007; Kuhlmann
and Mo?hl, 2007), which are strings that lexicalise
the precedence relation between the nodes of a de-
pendency tree. Given a correct subtree, we divide
the proof into cases according to the order annota-
tion of its head and we find that, for every possible
form of this order annotation, we can find a se-
quence of Combine steps to infer the relevant item
from smaller correct items.
3.3 Computational complexity
The time complexity of WG1 is O(n7), as the
step Combine Shrinking Gap Centre works with 7
free string positions. This complexity with respect
to the length of the input is as expected for this
set of structures, since Kuhlmann (2007) shows
that they are equivalent to LTAG, and the best ex-
isting parsers for this formalism also perform in
O(n7) (Eisner and Satta, 2000). Note that the
Combine step which is the bottleneck only uses the
7 indexes, and not any other entities like D-rules,
so its O(n7) complexity does not have any addi-
tional factors due to grammar size or other vari-
ables. The space complexity of WG1 is O(n5)
for recognition, due to the 5 indexes in items, and
O(n7) for full parsing.
4Due to space constraints, correctness proofs for the
parsers are not given here. Full proofs are provided in the
extended version of this paper, see (Go?mez-Rodr??guez et al,
2008b).
It is possible to build a variant of this parser
with time complexity O(n6), as with parsers for
unlexicalised TAG, if we work with unlexicalised
D-rules specifying the possibility of dependencies
between pairs of categories instead of pairs of
words. In order to do this, we expand the item set
with unlexicalised items of the form [i, j, C, l, r],
where C is a category, apart from the existing
items [i, j, h, l, r]. Steps in the parser are dupli-
cated, to work both with lexicalised and unlex-
icalised items, except for the Link steps, which
always work with a lexicalised item and an un-
lexicalised hypothesis to produce an unlexicalised
item, and the Combine Shrinking Gap steps, which
can work only with unlexicalised items. Steps are
added to obtain lexicalised items from their unlex-
icalised equivalents by binding the head to partic-
ular string positions. Finally, we need certain vari-
ants of the Combine Shrinking Gap steps that take
2 unlexicalised antecedents and produce a lexi-
calised consequent; an example is the following:
Combine Shrinking Gap Centre L:
[i, j, C, l, r]
[l + 1, r, C, l2, r2]
[i, j, l, l2, r2]
such that cat(wl)=C
Although this version of the algorithm reduces
time complexity with respect to the length of the
input to O(n6), it also adds a factor related to the
number of categories, as well as constant factors
due to using more kinds of items and steps than
the original WG1 algorithm. This, together with
the advantages of lexicalised dependency parsing,
may mean that the original WG1 algorithm is more
practical than this version.
4 The WGk parser
The WG1 parsing schema can be generalised to
obtain a parser for all well-nested dependency
structures with gap degree bounded by a constant
k(k ? 1), which we call WGk parser. In order to
do this, we extend the item set so that it can contain
items with up to k gaps, and modify the deduction
steps to work with these multi-gapped items.
4.1 Parsing schema for WGk
The item set IWGk is the set of all
[i, j, h, [(l1, r1), . . . , (lg, rg)]] where i, j, h, g ? N
, 0 ? g ? k, 1 ? h ? n, 1 ? i ? j ? n , h 6= j,
h 6= i? 1; and for each p ? {1, 2, . . . , g}:
lp, rp ? N, i < lp ? rp < j, rp < lp+1 ? 1,
h 6= lp ? 1, h 6= rp.
An item [i, j, h, [(l1, r1), . . . , (lg, rg)]] repre-
sents the set of all well-nested partial dependency
295
trees rooted at wh such that bwhc = {wh}?([i, j]\
?g
p=1[lp, rp]), where each interval [lp, rp] is called
a gap. The constraints h 6= j, h 6= i + 1, h 6=
lp ? 1, h 6= rp are added to avoid redundancy, and
normalisation is defined as in WG1. The set of fi-
nal items is defined as the set F = {[1, n, h, []] |
h ? N, 1 ? h ? n}. Note that this set is the same
as in WG1, as these are the items that we denoted
[1, n, h, , ] in the previous parser.
The deduction steps can be seen in Figure 1B.
As expected, the WG1 parser corresponds to WGk
when we make k = 1. WGk works in the same
way as WG1, except for the fact that Combine
steps can create items with more than one gap5.
The correctness proof is also analogous to that of
WG1, but we must take into account that the set of
possible order annotations is larger when k > 1,
so more cases arise in the completeness proof.
4.2 Computational complexity
The WGk parser runs in time O(n5+2k): as in
the case of WG1, the deduction step with most
free variables is Combine Shrinking Gap Cen-
tre, and in this case it has 5 + 2k free indexes.
Again, this complexity result is in line with what
could be expected from previous research in con-
stituency parsing: Kuhlmann (2007) shows that
the set of well-nested dependency structures with
gap degree at most k is closely related to cou-
pled context-free grammars in which the maxi-
mal rank of a nonterminal is k + 1; and the con-
stituency parser defined by Hotz and Pitsch (1996)
for these grammars also adds an n2 factor for each
unit increment of k. Note that a small value of
k should be enough to cover the vast majority of
the non-projective sentences found in natural lan-
guage treebanks. For example, the Prague Depen-
dency Treebank contains no structures with gap
degree greater than 4. Therefore, a WG4 parser
would be able to analyse all the well-nested struc-
tures in this treebank, which represent 99.89% of
the total. Increasing k beyond 4 would not pro-
duce further improvements in coverage.
5 Parsing ill-nested structures
The WGk parser analyses dependency structures
with bounded gap degree as long as they are
well-nested. This covers the vast majority of
5In all the parsers in this paper, Combine steps may be
applied in different orders to produce the same result, causing
spurious ambiguity. In WG1 and WGk, this can be avoided
when implementing the schemata, by adding flags to items
so as to impose a particular order.
the structures that occur in natural-language tree-
banks (Kuhlmann and Nivre, 2006), but there is
still a significant minority of sentences that con-
tain ill-nested structures. Unfortunately, the gen-
eral problem of parsing ill-nested structures is NP-
complete, even when the gap degree is bounded:
this set of structures is closely related to LCFRS
with bounded fan-out and unbounded production
length, and parsing in this formalism has been
proven to be NP-complete (Satta, 1992). The
reason for this high complexity is the problem
of unrestricted crossing configurations, appearing
when dependency subtrees are allowed to inter-
leave in every possible way. However, just as
it has been noted that most non-projective struc-
tures appearing in practice are only ?slightly? non-
projective (Nivre and Nilsson, 2005), we charac-
terise a sense in which the structures appearing in
treebanks can be viewed as being only ?slightly?
ill-nested. In this section, we generalise the algo-
rithms WG1 and WGk to parse a proper superset
of the set of well-nested structures in polynomial
time; and give a characterisation of this new set
of structures, which includes all the structures in
several dependency treebanks.
5.1 The MG1 and MGk parsers
The WGk parser presented previously is based on
a bottom-up process, where Link steps are used to
link completed subtrees to a head, and Combine
steps are used to join subtrees governed by a com-
mon head to obtain a larger structure. As WGk is a
parser for well-nested structures of gap degree up
to k, its Combine steps correspond to all the ways
in which we can join two sets of sibling subtrees
meeting these constraints, and having a common
head, into another. Thus, this parser does not use
Combine steps that produce interleaved subtrees,
since these would generate items corresponding to
ill-nested structures.
We obtain a polynomial parser for a wider set of
structures of gap degree at most k, including some
ill-nested ones, by having Combine steps repre-
senting every way in which two sets of sibling sub-
trees of gap degree at most k with a common head
can be joined into another, including those produc-
ing interleaved subtrees, like the steps for gap de-
gree 1 shown in Figure 1C. Note that this does not
mean that we can build every possible ill-nested
structure: some structures with complex crossed
configurations have gap degree k, but cannot be
built by combining two structures of that gap de-
gree. More specifically, our algorithm will be able
296
to parse a dependency structure (well-nested or
not) if there exists a binarisation of that structure
that has gap degree at most k. The parser im-
plicitly works by finding such a binarisation, since
Combine steps are always applied to two items and
no intermediate item generated by them can ex-
ceed gap degree k (not counting the position of
the head in the projection).
More formally, let T be a dependency structure
for the string w1 . . . wn. A binarisation of T is
a dependency tree T ? over a set of nodes, each of
which may be unlabelled or labelled with a word
in {w1 . . . wn}, such that the following conditions
hold: (1) each node has at most two children, and
(2) wi ? wj in T if and only if wi ?? wj in
T ?. A dependency structure is mildly ill-nested
for gap degree k if it has at least one binarisation
of gap degree ? k. Otherwise, we say that it is
strongly ill-nested for gap degree k. It is easy
to prove that the set of mildly ill-nested structures
for gap degree k includes all well-nested structures
with gap degree up to k.
We define MG1, a parser for mildly ill-nested
structures for gap degree 1, as follows: (1) the
item set is the same as that of WG1, except that
items can now contain any mildly ill-nested struc-
tures for gap degree 1, instead of being restricted
to well-nested structures; and (2) deduction steps
are the same as in WG1, plus the additional steps
shown in Figure 1C. These extra Combine steps
allow the parser to combine interleaved subtrees
with simple crossing configurations. The MG1
parser still runs in O(n7), as these new steps do
not use more than 7 string positions.
The proof of correctness for this parser is sim-
ilar to that of WG1. Again, we use the concept
of order annotations. The set of mildly ill-nested
structures for gap degree k can be defined as those
that only contain annotations meeting certain con-
straints. The soundness proof involves showing
that Combine steps always generate items contain-
ing trees with such annotations. Completeness is
proven by induction, by showing that if a subtree
is mildly ill-nested for gap degree k, an item for
it can be obtained from items for smaller subtrees
by applying Combine and Link steps. In the cases
where Combine steps have to be applied, the order
in which they may be used to produce a subtree
can be obtained from its head?s order annotation.
To generalise this algorithm to mildly ill-nested
structures for gap degree k, we need to add a Com-
bine step for every possible way of joining two
structures of gap degree at most k into another.
This can be done systematically by considering a
set of strings over an alphabet of three symbols:
a and b to represent intervals of words in the pro-
jection of each of the structures, and g to repre-
sent intervals that are not in the projection of ei-
ther structure, and will correspond to gaps in the
joined structure. The legal combinations of struc-
tures for gap degree k will correspond to strings
where symbols a and b each appear at most k + 1
times, g appears at most k times and is not the first
or last symbol, and there is no more than one con-
secutive appearance of any symbol. Given a string
of this form, the corresponding Combine step is
given by the expression in Figure 1D. As a particu-
lar example, the Combine Interleaving Gap C step
in Figure 1C is obtained from the string abgab.
Thus, we define the parsing schema for MGk, a
parser for mildly ill-nested structures for gap de-
gree k, as the schema where (1) the item set is
like that of WGk, except that items can now con-
tain any mildly ill-nested structures for gap degree
k, instead of being restricted to well-nested struc-
tures; and (2) the set of deduction steps consists of
a Link step as the one in WGk, plus a set of Com-
bine steps obtained as expressed in Figure 1D.
As the string used to generate a Combine step
can have length at most 3k + 2, and the result-
ing step contains an index for each symbol of the
string plus two extra indexes, the MGk parser has
complexity O(n3k+4). Note that the item and de-
duction step sets of an MGk parser are always su-
persets of those of WGk. In particular, the steps
for WGk are those obtained from strings that do
not contain abab or baba as a scattered substring.
5.2 Mildly ill-nested dependency structures
The MGk algorithm defined in the previous sec-
tion can parse any mildly ill-nested structure for a
given gap degree k in polynomial time. We have
characterised the set of mildly ill-nested structures
for gap degree k as those having a binarisation of
gap degree ? k. Since a binarisation of a depen-
dency structure cannot have lower gap degree than
the original structure, this set only contains struc-
tures with gap degree at most k. Furthermore, by
the relation between MGk and WGk, we know that
it contains all the well-nested structures with gap
degree up to k.
Figure 2 shows an example of a structure that
has gap degree 1, but is strongly ill-nested for gap
degree 1. This is one of the smallest possible such
structures: by generating all the possible trees up
to 10 nodes (without counting a dummy root node
297
Language
Structures
Total
Nonprojective
Total
By gap degree By nestedness
Gap
degree 1
Gap
degree 2
Gap
degree 3
Gap
deg. > 3
Well-
Nested
Mildly
Ill-Nested
Strongly
Ill-Nested
Arabic 2995 205 189 13 2 1 204 1 0
Czech 87889 20353 19989 359 4 1 20257 96 0
Danish 5430 864 854 10 0 0 856 8 0
Dutch 13349 4865 4425 427 13 0 4850 15 0
Latin 3473 1743 1543 188 10 2 1552 191 0
Portuguese 9071 1718 1302 351 51 14 1711 7 0
Slovene 1998 555 443 81 21 10 550 5 0
Swedish 11042 1079 1048 19 7 5 1008 71 0
Turkish 5583 685 656 29 0 0 665 20 0
Table 1: Counts of dependency trees classified by gap degree, and mild and strong ill-nestedness (for their gap degree); appear-
ing in treebanks for Arabic (Hajic? et al, 2004), Czech (Hajic? et al, 2006), Danish (Kromann, 2003), Dutch (van der Beek et al,
2002), Latin (Bamman and Crane, 2006), Portuguese (Afonso et al, 2002), Slovene (Dz?eroski et al, 2006), Swedish (Nilsson
et al, 2005) and Turkish (Oflazer et al, 2003; Atalay et al, 2003).
Figure 2: One of the smallest strongly ill-nested structures.
This dependency structure has gap degree 1, but is only
mildly ill-nested for gap degree ? 2.
located at position 0), it can be shown that all the
structures of any gap degree k with length smaller
than 10 are well-nested or only mildly ill-nested
for that gap degree k.
Even if a structure T is strongly ill-nested for
a given gap degree, there is always some m ? N
such that T is mildly ill-nested for m (since every
dependency structure can be binarised, and binari-
sations have finite gap degree). For example, the
structure in Figure 2 is mildly ill-nested for gap de-
gree 2. Therefore, MGk parsers have the property
of being able to parse any possible dependency
structure as long as we make k large enough.
In practice, structures like the one in Figure 2
do not seem to appear in dependency treebanks.
We have analysed treebanks for nine different lan-
guages, obtaining the data presented in Table 1.
None of these treebanks contain structures that are
strongly ill-nested for their gap degree. There-
fore, in any of these treebanks, the MGk parser can
parse every sentence with gap degree at most k.
6 Conclusions and future work
We have defined a parsing algorithm for well-
nested dependency structures with bounded gap
degree. In terms of computational complexity,
this algorithm is comparable to the best parsers
for related constituency-based formalisms: when
the gap degree is at most 1, it runs in O(n7),
like the fastest known parsers for LTAG, and can
be made O(n6) if we use unlexicalised depen-
dencies. When the gap degree is greater than 1,
the time complexity goes up by a factor of n2
for each extra unit of gap degree, as in parsers
for coupled context-free grammars. Most of the
non-projective sentences appearing in treebanks
are well-nested and have a small gap degree, so
this algorithm directly parses the vast majority of
the non-projective constructions present in natural
languages, without requiring the construction of a
constituency grammar as an intermediate step.
Additionally, we have defined a set of struc-
tures for any gap degree k which we call mildly
ill-nested. This set includes ill-nested structures
verifying certain conditions, and can be parsed in
O(n3k+4) with a variant of the parser for well-
nested structures. The practical interest of mildly
ill-nested structures can be seen in the data ob-
tained from several dependency treebanks, show-
ing that all of the ill-nested structures in them are
mildly ill-nested for their corresponding gap de-
gree. Therefore, our O(n3k+4) parser can analyse
all the gap degree k structures in these treebanks.
The set of mildly ill-nested structures for gap
degree k is defined as the set of structures that have
a binarisation of gap degree at most k. This defini-
tion is directly related to the way the MGk parser
works, since it implicitly finds such a binarisation.
An interesting line of future work would be to find
an equivalent characterisation of mildly ill-nested
structures which is more grammar-oriented and
would provide a more linguistic insight into these
structures. Another research direction, which we
are currently working on, is exploring how vari-
ants of the MGk parser?s strategy can be applied
to the problem of binarising LCFRS (Go?mez-
Rodr??guez et al, 2009).
298
References
Susana Afonso, Eckhard Bick, Renato Haber, and Di-
ana Santos. 2002. ?Floresta sinta?(c)tica?: a tree-
bank for Portuguese. In Proc. of LREC 2002, pages
1968?1703, Las Palmas, Spain.
Nart B. Atalay, Kemal Oflazer, and Bilge Say. 2002.
The annotation process in the Turkish treebank. In
Proc. of EACL Workshop on Linguistically Inter-
preted Corpora - LINC, Budapest, Hungary.
David Bamman and Gregory Crane. 2006. The design
and use of a Latin dependency treebank. In Proc. of
5th Workshop on Treebanks and Linguistic Theories
(TLT2006), pages 67?78.
Manuel Bodirsky, Marco Kuhlmann, and Mathias
Mo?hl. 2005. Well-nested drawings as models
of syntactic structure. Technical Report, Saar-
land University. Electronic version available at:
http://www.ps.uni-sb.de/Papers/.
Michael A. Covington. 1990. A dependency parser
for variable-word-order languages. Technical Re-
port AI-1990-01, Athens, GA.
Sas?o Dz?eroski, Tomaz? Erjavec, Nina Ledinek, Petr Pa-
jas, Zdene?k ?Zabokrtsky?, and Andreja ?Zele. 2006.
Towards a Slovene dependency treebank. In Proc.
of LREC 2006, pages 1388?1391, Genoa, Italy.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of ACL-99, pages 457?
464, Morristown, NJ. ACL.
Jason Eisner and Giorgio Satta. 2000. A faster parsing
algorithm for lexicalized tree-adjoining grammars.
In Proc. of 5th Workshop on Tree-Adjoining Gram-
mars and Related Formalisms (TAG+5), pages 14?
19, Paris.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING-96, pages 340?345, Copenhagen.
Carlos Go?mez-Rodr??guez, John Carroll, and David
Weir. 2008a. A deductive approach to dependency
parsing. In Proc. of ACL?08:HLT, pages 968?976,
Columbus, Ohio. ACL.
Carlos Go?mez-Rodr??guez, David Weir, and John Car-
roll. 2008b. Parsing mildly non-projective depen-
dency structures. Technical Report CSRP 600, De-
partment of Informatics, University of Sussex.
Carlos Go?mez-Rodr??guez, Marco Kuhlmann, Giorgio
Satta, and David Weir. 2009. Optimal reduction of
rule length in linear context-free rewriting systems.
In Proc. of NAACL?09:HLT (to appear).
Jan Hajic?, Otakar Smrz?, Petr Zema?nek, Jan ?Snaidauf,
and Emanuel Bes?ka. 2004. Prague Arabic depen-
dency treebank: Development in data and tools. In
Proc. of NEMLAR International Conference on Ara-
bic Language Resources and Tools, pages 110?117.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Jarmila
Panevova?, Petr Sgall, Petr Pajas, Jan ?Ste?pa?nek, Jir???
Havelka, and Marie Mikulova?. 2006. Prague depen-
dency treebank 2.0. CDROM CAT: LDC2006T01,
ISBN 1-58563-370-4.
Jir??? Havelka. 2007. Beyond projectivity: Multilin-
gual evaluation of constraints and measures on non-
projective structures. In Proc. of ACL 2007, Prague,
Czech Republic. ACL.
Gu?nter Hotz and Gisela Pitsch. 1996. On pars-
ing coupled-context-free languages. Theor. Comput.
Sci., 161(1-2):205?233. Elsevier, Essex, UK.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In Handbook of for-
mal languages, pages 69?124. Springer-Verlag,
Berlin/Heidelberg/NY.
Matthias T. Kromann. 2003. The Danish dependency
treebank and the underlying linguistic theory. In
Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT2003).
Marco Kuhlmann and Mathias Mo?hl. 2007. Mildly
context-sensitive dependency languages. In Proc. of
ACL 2007, Prague, Czech Republic. ACL.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In Proc.
of COLING/ACL main conference poster sessions,
pages 507?514, Morristown, NJ, USA. ACL.
Marco Kuhlmann. 2007. Dependency Structures and
Lexicalized Grammars. Doctoral dissertation, Saar-
land University, Saarbru?cken, Germany.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In IWPT 2007: Proc. of the 10th Confer-
ence on Parsing Technologies. ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of
HLT/EMNLP 2005, pages 523?530, Morristown,
NJ, USA. ACL.
Jens Nilsson, Johan Hall, and Joakim Nivre. 2005.
MAMBA meets TIGER: Reconstructing a Swedish
treebank from antiquity. In Proc. of NODALIDA
2005 Special Session on Treebanks, pages 119?132.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proc. of ACL?05,
pages 99?106, Morristown, NJ, USA. ACL.
Kemal Oflazer, Bilge Say, Dilek Zeynep Hakkani-Tu?r
and Go?khan Tu?r. 2003. Building a Turkish tree-
bank. In A. Abeille, ed., Building and Exploit-
ing Syntactically-annotated Corpora. Kluwer, Dor-
drecht.
Giorgio Satta. 1992. Recognition of linear context-
free rewriting systems. In Proc. of ACL-92, pages
89?95, Morristown, NJ. ACL.
Klaas Sikkel. 1997. Parsing Schemata ? A Frame-
work for Specification and Analysis of Parsing Al-
gorithms. Springer-Verlag, Berlin/Heidelberg/NY.
L. van der Beek, G. Bouma, R. Malouf, and G. van
Noord. 2002. The Alpino dependency treebank.
In Computational Linguistics in the Netherlands
(CLIN), Twente University.
K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi.
1987. Characterizing structural descriptions pro-
duced by various grammatical formalisms. In Proc.
of ACL-87, pages 104?111, Morristown, NJ. ACL.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of 8th International Workshop on
Parsing Technologies (IWPT 2003), pages 195?206.
299
D-Tree Substitution Grammars 
Owen Rambow* 
AT&T Labs-Research 
Dav id  Weir~ 
University of Sussex 
K. V i jay -Shanker t  
University of Delaware 
There is considerable interest among computational linguists in lexicalized grammatical frame- 
works; lexicalized tree adjoining rammar (LTAG) is one widely studied example. In this paper, 
we investigate how derivations in LTAG can be viewed not as manipulations of trees but as 
manipulations of tree descriptions. Changing the way the lexicalized formalism is viewed raises 
questions as to the desirability of certain aspects of the formalism. We present anew formalism, 
d-tree substitution grammar (DSG). Derivations in DSG involve the composition of d-trees, 
special kinds of tree descriptions. Trees are read off rom derived -trees. We show how the DSG 
formalism, which is designed to inherit many of the characterestics of LTAG, can be used to express 
a variety of linguistic analyses not available in LTAG. 
1. Introduction 
There is considerable interest among computational linguists in lexicalized grammati- 
cal frameworks. From a theoretical perspective, this interest is motivated by the widely 
held assumption that grammatical structure is projected from the lexicon. From a prac- 
tical perspective, the interest stems from the growing importance of word-based cor- 
pora in natural anguage processing. Schabes (1990) defines a lexicalized grammar as 
a grammar in which every elementary structure (rules, trees, etc.) is associated with a 
lexical item and every lexical item is associated with a finite set of elementary struc- 
tures of the grammar. Lexicalized tree adjoining grammar (LTAG) (Joshi and Schabes 
1991) is a widely studied example of a lexicalized grammatical formalism. 1 
In LTAG, the elementary structures of the grammar are phrase structure trees. 
Because of the extended omain of locality of a tree (as compared to a context-free 
string rewriting rule), the elementary trees of an LTAG can provide possible syntactic 
contexts for the lexical item or items that anchor the tree, i.e., from which the syntactic 
structure in the tree is projected. LTAG provides two operations for combining trees: 
substitution and adjunction. The substitution operation appends one tree at a frontier 
node of another tree. The adjunction operation is more powerful: it can be used to 
insert one tree within another. This property of adjoining has been widely used in the 
LTAG literature to provide an account for long-distance dependencies. For example, 
* ATT Labs-Research, B233 180 Park Ave, PO Box 971, Florham Park, NJ 07932-0971, USA. E-mail: 
rambow@research.att.com 
t Department ofComputer and Information Science University of Delaware Newark, Delaware 19716. 
E-mail: vijay@udel.edu 
School of Cognitive and Computing Sciences University of Sussex Brighton, BN1 6QH E. Sussex UK. 
E-mail: david.weir@cogs.susx.ac.uk 
1 Other examples of lexicalized grammar formalisms include different varieties of categorial grammars 
and dependency grammars. Neither HPSG nor LFG are lexicalized in the sense of Schabes (1990). 
Computational Linguistics Volume 27, Number 1 
NP S 
Peter NP VP 
John V NP 
I I 
saw e 
fl: S 
NP VP 
you V S 
I 
thought 
NP S 
Peter NP VP 
you V S 
thought NP VP 
I /N  
John V NP 
I I 
saw e 
Figure 1 
Example of adjunction. 
Figure I shows a typical analysis of topicalization. 2 The related nodes for the filler and 
the gap in the elementary tree c~ are moved further apart when the tree 7 is obtained 
by adjoining the auxiliary tree fl within ~. This shows that adjunction changes the 
structural relationship between some of the nodes in the tree into which adjunction 
occurs. 
In LTAG, the lexicalized elementary objects are defined in such a way that the 
structural relationships between the anchor and each of its dependents change during 
the course of a derivation through the operation of adjunction, as just illustrated. This 
approach is not the only possibility. An alternative would be to define the relationships 
between the nodes of the elementary objects in such a way that these relationships 
hold throughout the derivation, regardless of how the derivation proceeds. 
This perspective on the LTAG formalism was explored in Vijay-Shanker (1992) 
where, following the principles of d-theory parsing (Marcus, Hindle, and Fleck 1983), 
LTAG was seen as a system manipulating descriptions of trees rather than as a tree 
2 The same analysis holds for wh-movement, but we use topicalization as an example in order to avoid 
the superficial complication of the auxiliary needed in English questions. Sometimes, topicalized 
sentences sound somewhat less natural than the corresponding wh-questions, which are always 
structurally equivalent. 
88 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
fl': 
NP S 
I 
Peter 
I 
I 
S 
NP VP 
I ' 
John 
I 
t 
VP 
V NP 
I I 
saw e 
S O d : 
NP VP 
I ' 
I 
you VP 
V S 
I 
thought 
Figure 2 
Adjunction example revisited. 
NP S 
Peter NP VP 
I ' ! 
you VP 
V S 
thought NP VP 
I ' 
I 
John VP 
V NP 
I I 
saw e 
rewriting formalism. Elementary objects are descriptions of possible syntactic ontexts 
for the anchor, formalized in a logic for describing nodes and the relationships (dom- 
inance, immediate dominance, linear precedence) that hold between them. 
From this perspective, instead of positing the elementary tree ~ in Figure 1, we can 
describe the projection of syntactic structure from the transitive verb. This description is
presented pictorially as c~  in Figure 2. The solid lines indicate immediate domination, 
whereas the dashed lines indicate a domination of arbitrary length. The description 
a ~ not only partially describes the tree c~ (by taking the dominations to be those of 
length 0) but also any tree (such as "~) that can be derived by using the operations 
of adjunction and substitution starting from c~. In fact, (~t describes exactly what is 
common among these trees. 
By expressing elementary objects in terms of tree descriptions, we can describe 
syntactic structure projected from a lexical item in a way that is independent of the 
derivations in which it is used. This is achieved by employing composition operations 
that produce descriptions that are compatible with the descriptions being combined. 
For instance, adjoining, seen from this perspective, serves to further specify the un- 
derspecified ominations. In Figure 2, the description -y~ is obtained by additionally 
stating that the domination between the two nodes labeled S in c~  is now given by 
the domination relation between the two nodes labeled S in fl~. 
As we will explore in this paper, changing the way the lexicalized formalism is 
viewed, from tree rewriting to tree descriptions, raises questions as to the desirability 
89 
Computational Linguistics Volume 27, Number 1 
S 
NP~ S NP VP 
' I ' ! ! 
many of us VP John VP 
to meet NPi V VP 
I I 
e hopes 
Figure 3 
A problem for LTAG. 
of certain aspects of the formalism. Specifically, we claim that the following two aspects 
of LTAG appear unnecessarily restrictive from the perspective of tree description: 
. 
. 
In LTAG, the root and foot of auxiliary trees must be labeled by the same 
nonterminal symbol. This is not a minor issue since it derives from one 
of the most fundamental principles of LTAG, factoring of recursion. This 
principle states that auxiliary trees express factored out recursion, which 
can be reintroduced via the adjunction operation. It has had a profound 
influence on the way that the formalism has been applied linguistically. 3 
An example of how this can create problems is shown in Figure 3. In this 
case, the "adjoined" tree has a root labeled S and a foot labeled VP, 
something that is not permissible in LTAG. Note that without his 
constraint, he combination would appear to be exactly like adjoining. 
We consider this aspect in more detail in Section 4.1. 
The adjunction operation embeds all of the adjoined tree within that part 
of the tree at which adjunction occurs. This is illustrated in -y' (Figure 2) 
where both parts (separated by domination) of fl~ appear within one 
underspecified domination relationship in c~'. 
The foot node of tree fl in Figure 1 corresponds to a required argument 
of the lexical anchor, thought. The adjunction operation accomplishes the 
role of expanding this argument node. Unlike the substitution operation, 
where an entire tree is inserted below the argument node, with 
adjunction, only a subtree of ~ appears below the argument node; the 
remainder appears in its entirety above the root node of ft. However, if 
we view the trees as descriptions, as in Figure 2, and if we take the 
expansion of the foot node as the main goal served by adjunction, it is 
not clear why the composition should have anything to say about the 
domination relationship between the other parts of the two objects being 
combined. In the description approach, in order to obtain 3/we (in a 
3 Note that in feature-based LTAG there is no restriction that the two feature structures be the same, or 
even that they be compatible. 
90 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
PP S , 
I 
i 
To some of us VP 
V PP 
I I 
appears e 
Figure 4 
Another problem for LTAG. 
VP 
S 
NP VP 
I ' I i 
John VP 
to be happy 
sense to be made precise later) substitute the second component of ~ 
(rooted in S) at the foot node of fl'. This operation does not itself entail 
any further domination constraints between the components of ~P and fl~ 
that are not directly involved in the substitution, specifically, the top 
components of o /and fl'. In the trees described it is possible for either 
one to dominate the other. 4However, adjunction further stipulates that 
the rest of ~' will appear above all of fl'. This additional constraint 
makes certain analyses unavailable for the LTAG formalism (as is well 
known). For instance, given the two lexical projections in Figure 4, the 
subtrees must be interleaved in a fashion not available with adjoining to 
produce the desired result. This aspect of adjoining is the focus of the 
discussion in Section 4.2. 
In this paper, we describe a formalism based on tree descriptions called d-tree 
substitution grammars (DSG). 5 The elementary tree descriptions in DSG can be used 
to describe lexical items and the grammatical structure they project. Each elementary 
tree description can be seen as describing two aspects of the tree structure: one part of 
the description specifies phrase structure rules for lexical projections, and a second part 
of the description states domination relationships between pairs of nodes. DSG inherits 
from LTAG the extended omain of locality of its elementary structures, and, in DSG as 
in LTAG, this extended omain of locality allows us to develop a lexicalized grammar 
in which lexical items project grammatical structure, including positions for arguments. 
But DSG departs from LTAG in that it does not include factoring of recursion as 
a constraint on the makeup of the grammatical projections. Furthermore, in DSG, 
arguments are added to their head by a single operation that we call generalized 
substitution, whereas in LTAG two operations are used: adjunction and substitution. 
DSG is intended to be a simple framework with which it is possible to provide 
analyses for those cases described with LTAG as well as for various cases in which 
extensions of LTAG have been needed, such as different versions of multicomponent 
4 Of course, the node labels further estrict possible dominance in this case. 
5 This paper is based on Rarnbow, Vijay-Shanker, and Weir (1995), where DSG was called DTG (d-tree 
grammar). 
91 
Computational Linguistics Volume 27, Number 1 
Z% {Xl ~ X2, {Ul ~ U2, Xl /k X3, ~3 U3 ~1 /k U3~ X2 -~ 3:3~ U2 "~ ~3~ 
X3 '/~ Y l ,  I I 
yl /k y2, I I Zl /~ Z2, 
Yl /X Y3' O~ O~ z l  /X za' y2 -~ Y3} z2 -~ z3} 
Y3 z3 
Figure 5 
A pair of tree descriptions (which are also d-trees). 
LTAG. Furthermore, because the elementary objects are expressed in terms of logical 
descriptions, it has been possible to investigate the characteristics of the underspecifi- 
cation that is used in these descriptions (Vijay-Shanker and Weir 1999). 
In Section 2, we give some formal definitions and in Section 3 discuss some of 
the formal properties of DSG. In Section 4, we present analyses in DSG for various 
linguistic constructions in several languages, and compare them to the corresponding 
LTAG analyses. In Section 5, we discuss the particular problem of modeling syntactic 
dependency. We conclude with a discussion of some related work and summary. 
2. Def in i t ion of DSG 
D-trees are the primitive elements of a DSG. D-trees are descriptions of trees, in partic- 
ular, certain types of expressions in a tree description language such as that of Rogers 
and Vijay-Shanker (1992). In this section we define tree descriptions and substitution 
of tree descriptions (Section 2.1) and d-trees (Section 2.2) together with some associ- 
ated terminology and the graphical representation (Section 2.3). We then define d-tree 
substitution grammars, along with derivations of d-tree substitution grammars (Sec- 
tion 2.4) and languages generated by these grammars (Section 2.5), and close with an 
informal discussion of path constraints (Section 2.6). 
2.1 Tree Descr ipt ions and Subst i tut ion 
In the following, we are interested in a tree description language that provides at least 
the following binary predicate symbols: A, /~, and -~. These three predicate sym- 
bols are intended to be interpreted as the immediate domination, domination, and 
precedence r lations, respectively. That is, in a tree model, the literal x/~ y would be 
interpreted as node (referred to by the variable) x immediately dominates node (re- 
ferred to by) y, the literal x/~ y would be interpreted such that x dominates y, and 
x -~ y indicates that x is to the left of y. In addition to these predicate symbols, we 
assume there is a finite set of unary function symbols, such as label, which are to be 
used to describe node labeling. Finally, we assume the language includes the equality 
symbol. 
We will now introduce the notion of tree description. 
Def in i t ion 
A tree descr ipt ion is a finite set (conjunction) of positive literals in a tree description 
language. 
In order to make the presentation more readable, tree descriptions are usually pre- 
sented graphically rather than as logical expressions. Figure 5 gives two tree descrip- 
tions, each presented both graphically and in terms of tree descriptions. We introduce 
92 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
x x3 
,p 
/ 
Z3 
Y5 
Figure 6 
A tree description (which is also a d-tree) with three components. 
the conventions used in the graphical representations in more detail in Section 2.3. 
Note that with a functor for each feature, feature structure labels can be specified as 
required. Although feature structures will be used in the linguistic examples presented 
in Section 4, for the remainder of this section we will assume that each node is labeled 
with a symbol by the function label. Furthermore, we assume that these symbols come 
from two pairwise distinct sets of symbols, the terminal and nonterminal labels. (Note 
that the examples in this section do not show labels for nodes, but rather their names, 
while the examples in subsequent sections how the labels.) 
In the following, we consider a tree description to be satisfiable if it is satisfied 
by a finite tree model. For our current purposes, we assume that a tree model will be 
defined as a finite universe (the set of nodes) and will interpret the predicate symbols: 
G, ,~,, and -~ as the immediate domination, domination, and precedence relations, 
respectively. For more details on the notion of satisfiability and the definition of tree 
models, see Backofen, Rogers, and Vijay-Shanker (1995), where the axiomatization of
their theory is also discussed. 6 
We use d ~ d ~ to indicate that the description d~ logically follows from d, in other 
words, that d ~ is known in d. 7 Given a tree description d, we say x dominates y in d if 
d ~ x ,~ y (similarly for the immediate domination and precedes relations). 
We use vars(d) to denote the set of variables involved in the description d. For 
convenience, we will also call the variables in vars(d) the nodes of description d. For a 
tree description d, a node x E vars(d) is a frontier node of d if for all y E vars(d) such 
that x # y, it is not the case that d =~ x ~ y. Only frontier nodes of the tree description 
can be labeled with terminals. A frontier node labeled with a nonterminal is called a 
subst i tut ion node. 
A useful notion for tree description is the notion of components. Given a tree 
description d, consider the binary relation on vars(d) corresponding to the immediate 
domination relations pecified in d; i.e., the relation {(x, Y/ I x, Y E vars(d), d =~ x A y}. 
The transitive, symmetric, reflexive closure of this relation partitions vars(d) into equiv- 
alence classes that we call components. For example, the nodes in the tree descrip- 
tion in Figure 6 fall into the three components: { Xl, x2, x3, x4, x5 }, { yl, y2, y3, y4, Y5 }, 
and { zl,z2, Zg, Z4,Z5 }- In particular, note that y4 and Zl (likewise x3 and z2) are not 
in the same components despite the fact that y4 dominates zl is known in that de- 
6 Note that the symbol /~ in this paper replaces the symbol/~* used in Backofen, Rogers, and 
Vijay-Shanker (1995). 
7 In other words, d =~ d ~ iff d A ~d ~ is not satisfied by any tree model. 
93 
Computational Linguistics Volume 27, Number 1 
0 3 : ~  
{Xl  t 2:2~ 
x l  /k x3, OFX2 ~ X 3 
3:2 "~ X3~ I 
x3/h Yl, I 
Yl /k y2, 
y2 "< Ul, 
Ul \]~ U2~ I?1 
Ul t u3,  ~U 2 0153 
I/,2 "~ u3 
'//'2 \]~' Zl ~ ! I 
Zl \]~ Z2 ~ I 
Zl t z3, Z~Z 1 
z2 "~ Z3} 
Z3 
Figure 7 
Result of substitution by tree description root. 
scription. This is because the reflexive, symmetric, and transitive closure of the im- 
mediate domination relation known in the description will not include these pairs 
of nodes. 
We say that x is the root of a component  if it dominates every node in its com- 
ponent, and we say that x is on the frontier of a component if the only node in its 
component that it dominates i  itself. Note that x can be on the frontier of a component 
of d without being a frontier node of a tree description. For example, in Figure 6, x3 
is a frontier of a component but not a frontier of the tree description. In contrast, z3 is 
both a frontier of a component as well as a frontier of the tree description. We say that 
x is the root of a tree descr ipt ion if it dominates every node in the tree description. 
Note that it need not be the case that every tree description has a root. For example, 
according to the definition of tree descriptions, the description in Figure 6 is a tree de- 
scription and does not have a root. Although we know that either xl or Yl dominates 
all nodes in a tree model of the tree description, we don't know which. 
We can now define the subst i tut ion operat ion on tree descr ipt ions that will be 
used in DSG. We use dl \[y/x\] to denote the description obtained from dl by replacing 
all instances in dl of the variable x by y. 
Def in i t ion 
Let dl and d2 be two tree descriptions. Without loss of generality, we assume that 
vars(dl) N vars(d2) = ?. Let x E vars(dl) be a root of a component of dl and 
y E vars(d2) be a substitution ode in the frontier of d2. Let d be the description 
dl kJ d2\[x/y\]. We say that d is obtained from dl and d2 by subst i tut ing x at y. 
Note that in addition, we may place restrictions on the values of the labeling 
functions for x and y in the above definition. Typically, for a node labeling function such 
as label we require label(x) = label(y), and for functions that return feature structures 
we require unifiability (with the unification being the new value of the feature function 
for y). 
Figure 7 shows the result of substituting the root Ul of the tree description on 
the right of Figure 5 at the substitution ode Y3 of the tree description on the left of 
Figure 5. 
Figure 8 shows the result of substituting a node that is not the root of the tree 
94 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Xl \]~ .T2~ 
Xl /'x X3, 
x3 ~ y~, ~X2 N~ x3 c~t  ,
Yl A y~, . / 'c 4 Yl fk Zl, I q) U 2 O '//3 
Y2 "~ Zl, I I 
Ul \]k U2~ ~ Y l  i 
Ul ~ U3~ 
u2 -~ ua, (5 Y2 ~Y3:  zl  
U2 /~ Zl~ 
Zl /k Z2, (9 Z2 (9 Z3 
zl /k z3, 
z2 ~ z3} 
Figure 8 
Result of substitution by component root. 
description but the root Zl of a component of the tree description on the right of Figure 5 
at the substitution ode y3 of the tree description on the left of Figure 5. 
2.2 D-Trees 
D-trees are certain types of tree descriptions: not all tree descriptions are d-trees. In 
describing syntactic structure, we are interested in two kinds of primitive tree de- 
scriptions. The first kind of primitive tree description, which we call parent-child 
descriptions, involves n + 1 (n _> 1) variables, say x, xl,. ?., Xn, and in addition to spec- 
ifying categorial information associated with these variables, specifies tree structure of 
the form 
{X & I1 . . . . .  X A Xn, Xl "g X2 . . . . .  Xn-1 "?, Xn} 
A parent-child escription corresponds to a phrase structure rule in a context-free 
grammar, and by extension, to a phrase structure rule in X-bar theory, to the instanti- 
ation of a rule schema in HPSG, or to a c-structure rule in LFG. As in a context-free 
grammar, in DSG we assume the siblings Xl , . . . ,  Xn are totally ordered by precedence, s 
The second kind of primitive description, which we call a dominat ion description, 
has the form {x & y}, where x and y are variables. In projecting from a lexical item to 
obtain the elementary objects of a grammar, this underspecified domination statement 
allows for structures projected from other lexical items to be interspersed during the 
derivation process. 
Definit ion 
A d-tree is a satisfiable description in the smallest class of tree descriptions obtained 
by closing the primitive tree descriptions under the substitution operation. 
For example, Figure 9 shows how the d-tree in Figure 6 is produced by using 
six parent-child escriptions and two domination descriptions. The ovals show cases 
of substitution; the circle represents a case of two successive substitutions. Figure 10 
shows a tree description that is not a d-tree: it is not a parent-child escription, nor 
8 One could, of course, relax this constraint and assume that they are only partially ordered. However, 
for now, we do not consider such an extension. See Section 4.4 for a discussion. 
95 
Computational Linguistics Volume 27, Number 1 
~g4 
X5 I 
X3 
Y2 
Y5 
,s 
J 
J 
2:4 
| f 
Z3 
Figure 9 
Derivation of an elementary d-tree. 
{x & y, o.x 
6y "o z 
y -~ z} 
Figure 10 
A description that is not a d-tree. 
Y~ 
can it be derived from two domination descriptions by substitution, since substitution 
can only occur at the frontier nodes. 
A d-tree d is complete if it does not contain any substitution odes, i.e., all the 
frontier nodes of the description d are labeled by terminals. Given a d-tree d, we say 
that a pair of nodes, x and y (variables in vars(d)), are related by an i-edge if d ::~ xAy .  
We say that x is an i-parent and y is an i-child. Given a d-tree d, we say that a pair of 
nodes, x and y, are related by a d-edge if it is known from d that x dominates y, it is 
not known from d that x immediately dominates y, and there is no other variable in d 
that is known to be between them. That is, a pair of nodes x and y, x ~ y, are related 
byad-edge i fd  ~ x~y,d  ~ xGy,  and for a l l z  E vars(d), i fd ~ (x&zAz~y)  
then z = x or z = y. If x and y are related by a d-edge, then we say that they are 
d-parent and d-child, respectively. Note that a node in a d-tree (unlike a node in a 
tree description) cannot be both an i-parent and a d-parent at the same time. 
2.3 Graphical Presentation of a D-Tree 
We usually find it more convenient to present d-trees graphically. When presenting a 
d-tree graphically, i-edges are represented with a solid line, while d-edges are repre- 
sented with a broken line. All immediate dominance relations are always represented 
graphically, but only the domination relations corresponding to d-edges are shown 
explicitly in graphical presentations. 
By definition of d-trees, each component of a d-tree is fully specified with respect 
to immediate domination. Thus, all immediate domination relations between nodes 
in a component are indicated by i-edges. Also, by definition, components must be 
fully specified with respect o precedence. That is, for any two nodes u and v within 
96 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
a component we must know whether u precedes v or vice versa. In fact, all prece- 
dence information derives from precedence among siblings (two nodes immediately 
dominated by a common node). This means that all the precedence in a description 
can be expressed graphically simply by using the normal left-to-right ordering among 
siblings. 
Another important restriction on d-trees has to do with how components are re- 
lated to one another. As we said above, a frontier node of a component of a d-tree 
can be a d-parent but not an i-parent, and only frontier nodes of a component can 
serve as d-parents. However, by definition, a frontier node of a d-tree can neither be a 
d-parent nor an i-parent. Graphically, this restriction can be characterized as follows: 
edges specifying domination (d-edges) must connect a node on the frontier of a com- 
ponent with a node of another component. Furthermore, nodes on the frontier of a 
component can have at most one d-child. 
Recall that not every set of positive literals involving A, /~, and -~ is a legal d-tree. 
In particular, we can show that a description is a d-tree if and only if it is logically 
equivalent to descriptions that, when written graphically, would have the appearance 
described above. 
2.4 D-Tree Substitution Grammars 
We can now define d-tree substitution grammars. 
Definition 
A d-tree substitution grammar (DSG) G is a 4-tuple (VT, VN, T, ds), where VT and 
VN are pairwise distinct terminal and nonterminal alphabets, respectively, T is a 
finite set of elementary d-trees such that the functor label assigns each node in 
each d-tree in T a label in VT U VN and such that only d-tree frontier nodes take 
labels in VT, and ds is a characterization f the labels that can appear at the root 
of a derived tree. 
Derivations in DSG are defined as follows. Let G = (VT, VN, T, ds) be a DSG. 
Furthermore: 
? Let T0(G) --- T. 
? Let Ti+I = Ti U {d\[d is satisfiable and d results from combining a pair of 
d-trees in Ti by substitution at a node x such that label(x) c VN}. 
The d-tree language T(G) generated by G is defined as follows. 
T (G)={dcT i l i>0 ,  d i scomplete}  
In a lexicalized DSG, there is at least one terminal node on the frontier of every 
d-tree; this terminal is (these terminals are) designated the anchor(s) of the d-tree. 
The remaining frontier nodes (of the description) and all internal nodes are labeled by 
nonterminals. Nonterminal nodes on the frontier of a description are called substitution 
nodes because these are nodes at which a substitution must occur (see below). Finally, 
we say that a d-tree d is sentential if d has a single component and the label of the 
root of d is compatible with ds. 
2.5 Reading D-Trees 
A description d is a tree if and only if it has a single component (i.e., it does not have 
any d-edges). Therefore, the process of reading off trees from d-trees can be viewed as 
97 
Computational Linguistics Volume 27, Number 1 
a nondeterministic process that involves repeatedly removing d-edges until a d-tree 
with a single component results. 
In defining the process of removing a d-edge, we require first, that no i-edges 
be added which are not already specified in the components, and second, that those 
i-edges that are distinct prior to the process of reading off remain distinct after the 
removal of the d-edges. This means that each removal of a d-edge results in equating 
exactly one pair of nodes. These requirements are motivated by the observation that 
the i-edges represent linguistically determined structures embodied in the elementary 
d-trees that cannot be created or reduced uring a derivation. 
We now define the d-edge removal algorithm. A d-edge represents a domination 
relation of length zero or more. Given the above requirements, at the end of the 
composition process, we can, when possible, get a minimal reading of a d-edge to 
be a domination relation of length zero. Thus, we obtain the following procedure for 
removing d-edges: Consider ad-edge with a node x as the d-parent and with a d-child 
y. By definition of d-trees, x is on the frontier of a component. The d-child y can either 
be a root of a component or not. Let us first consider the case in which y is a root of 
a component. To remove this d-edge, we equate x with y.9 This gives us the minimal 
reading that meets the above requirement ( hat no i-edges are added which are not 
already specified in the components, and that those i-edges that are distinct prior to 
the process of reading off remain distinct after the removal of the d-edge). Now we 
consider the alternate case in which the d-child is not the root of its component. Let z 
be the root of the component containing y. Now both z and x are known to dominate 
y and hence in any model of the description, either z will dominate x or vice versa. 
Equating x with y (the two nodes in the d-edge under consideration) has the potential 
of requiring the collapsing of i-edges (e.g., i-edges between x and its parent and y and 
its parent in the component including z). As a consequence of our requirement, the 
only way to remove the d-edge is by equating the nodes x and z. If we equated x
with any other node dominated by z (such as y), we would also be collapsing i-edges 
from two distinct components and equating more than one pair of nodes, contrary to 
our requirement. The removal of the d-edge by equating x and z can also be viewed 
as adding a d-edge from x to z (which, as mentioned, is compatible with the given 
description and does not have the potential for collapsing i-edges). Now since this d- 
edge is between a frontier of a component and the root of another, it can be removed 
by equating the two nodes. 
Definition 
A tree t can be read off from a d-tree d iff t is obtained from d by removing the d-edges 
of d in any order using the d-edge removal algorithm. 
By selecting d-edges for removal in different orders, different rees can be pro- 
duced. Thus, in general, we can read off several trees from each d-tree in T(G). For 
example, the d-tree in Figure 6 can produce two trees: one rooted in xl (if we choose 
to collapse the edge between y4 and zl first) and one rooted in yl (if we choose to col- 
lapse the edge between x3 and z2 first). The fact that a d-tree can have several minimal 
readings can be exploited to underspecify different word orderings (see Section 4.4). 
9 This additional equality to obtain the minimal  readings is similar to unification of the so-called top and 
bottom feature structures associated with a node in tree adjoining grammars,  which happens at the end 
of a derivation. In DSG, if the labeling specifications on x and y are incompatible, then the addit ional 
equality statement does not lead to any minimal  tree model, just as in TAG, a derivation cannot 
terminate if the top and bottom feature structures associated with a node do not unify. 
98 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Thus, while a single d-tree may describe several trees, only some of these trees 
will be read off in this way. This is because of our assumptions about what is being 
implicitly stated in a d-tree--for example, our requirement that i-edges can be neither 
destroyed nor created in a derivation. Assumptions uch as these about the implicit 
content of d-trees constitute a theory of how to read off from d-trees. Variants of the 
DSG formalism can be defined, which differ with respect o this theory. 
We now define the tree and string languages of a DSG. 
Definition 
Let G be a DSG. The tree language T(G) generated by G is the set of trees that can be 
read off from sentential d-trees in T(G). 
Definition 
The string language generated by G is the set of terminal strings on the frontier of 
trees in T(G). 
2.6 DSG with Path Constraints 
In DSG, domination statements are used to express domination paths of arbitrary 
length. There is no requirement placed on the nodes that appear on such paths. In this 
section, we informally define an extension to DSG that allows for additional statements 
constraining the paths. 
Path constraints can be associated with domination statements o constrain which 
nodes, in terms of their labels, can or cannot appear within a path instantiating a
d-edge. 1? Path constraints do not directly constrain the length of the domination path, 
which still remains underspecified. Path constraints are specified in DSG by associat- 
ing with domination statements a set of labels that defines which nodes cannot appear 
within this path. u Suppose we have a statement x A y with an associated path con- 
straint set, P, then logically this pair can be understood as x A y A Vz(z ~ x A z y~ 
y AxAzAzA y) ~ label(z) ~P.  
Note that during the process of derivation involving substitution, the domination 
statements in the two descriptions being composed continue to exist and do not play 
any role in the composition operation itself. The domination statements only affect 
the reading off process. For this reason, we can capture the effect of path constraints 
by merely defining how they affect the reading off process. Recall that the reading off 
process is essentially the elimination of d-edges to arrive at a single component d-tree. 
If there is a d-edge between x and y, we consider two situations: is the d-child y the 
root of a component, or not? When y is the root of a component, then x and y are 
collapsed. Clearly any path constraint on this d-edge has no effect. However, when y 
is not the root of a component, and z is the root of the component containing y, then 
the tree we obtain from the reading off process is one where x dominates z and not 
where z properly dominates x. That is, in this case, we replace the d-edge between 
x and y with a d-edge between x and z, which we then eliminate in the reading off 
process by equating x and z. But in order to replace the d-edge between x and y with 
a d-edge between x and z, we need to make sure that the path between z and y does 
not violate the path constraint associated with the d-edge between x and y. 
10 In Rambow, Vijay-Shanker, and Weir (1995), path constraints are called "subsertion-insertion 
constraints." 
11 Rambow (1996) uses regular expressions to specify path constraints. 
99 
Computational Linguistics Volume 27, Number 1 
A A 
a A a A 
I I 
I I 
, o 
B B 
b B b B 
I I 
! I 
i i 
C C 
e 6' 
Figure 11 
Counting to three: A derivation. 
A 
a B 
I 
I 
i 
B 
/N  
b C 
i 
I 
i 
C 
C 
3. Properties of the Languages of DSG 
It is clear that any context-free language can be generated by DSG (a context-free 
grammar can simply be reinterpreted as a DSG). It is also easy to show that the weak 
generative capacity of DSG exceeds that of context-free grammars. Figure 11 shows 
three d-trees (including two copies of the same d-tree) that generate the non-context- 
free language { anb'c" In > 1 }. Figure 12 shows the result of performing the first of 
two substitutions indicated by the arrows (top) and the result of performing both 
substitutions (bottom). Note that although there are various ways that the domination 
edges can be collapsed when reading off trees from this d-tree, the order in which 
we collapse domination edges is constrained by the need to consistently label nodes 
being equated. This is what gives us the correct order of terminals. 
Figure 13 shows a grammar for the language 
{ w E { a, b, c }* \] w has an equal nonzero number of a's, b's and c's }, 
which we call Mix. This grammar is very similar to the previous one. The only differ- 
ence is that node labels are no longer used to constrain word order. Thus the domi- 
nation edges can be collapsed in any order. 
Both of the previous two examples can be extended to give a grammar for strings 
containing an equal number of any number of symbols imply by including additional 
components in the elementary d-trees for each symbol to be counted. Hence, DSG 
generates not only non-context-free languages but also non-tree adjoining languages, 
since LTAG cannot generate the language { anbncndnen i n _~ 1 } (Vijay-Shanker 1987). 
However, it appears that DSG cannot generate all of the tree adjoining languages, 
and we conjecture that the classes are therefore incomparable (we offer no proof of 
this claim in this paper). It does not appear to be possible for DSG to generate the 
copy language { ww \[ w c { a, b }* }. Intuitively, this claim can be motivated by the 
observation that nonterminal labels can be used to control the ordering of a bounded 
number of terminals (as in Figure 12), but this cannot be done in an unbounded way, 
as would be required for the copy language (since the label alphabet is finite). 
100 
Rainbow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
A A 
a A a A 
t I 
I I 
o n 
B B 
b B b B 
! / 
o / 
c 6' 
c 6' 
A A A 
a A a A a B 
I I ! 
I I I 
i n i 
B B B 
b B b B b C 
| d / / t 
I / . , . I  
I / 
C / ., / t 
/ 1 1 / 
f / 
s j 
i 
i 
e C 
I 
C 
Figure 12 
Counting to three: After substituting one tree (above) and the derived d-tree (below). 
DSG is closely related (and weakly equivalent) to two equivalent string rewriting 
systems, UVG-DL and {}-LIG (Rainbow 1994a, 1994b). In UVG-DL, several context- 
free rewrite rules are grouped into a set, and dominance links may hold between 
101 
Computational Linguistics Volume 27, Number 1 
S S S S 
a S b S c S a S 
S < 
S S 
b S c S 
:" S 
I 
? 
S 
a S 
S S S S S 
b S c S a S b S c S 
S 
I 
Figure 13 
A grammar for Mix. 
right-hand-side nonterminals and left-hand-side nonterminals of different rules from 
the same set. In a derivation, the context-free rules are applied as usual, except hat all 
rules from an instance of a set must be used in the derivation, and at the end of the 
derivation, the dominance links must correspond to dominance relations in the deriva- 
tion tree. {}-LIG is a multisebvalued variant of Linear Index Grammar (Gazdar 1988). 
UVG-DL and {}-LIG, when lexicalized, generate only context-sensitive languages. 
Finally, Vijay-Shanker, Weir, and Rainbow (1995), using techniques developed for 
UVG-DL (Rainbow 1994a; Becker and Rambow 1995), show that the languages gen- 
erated by lexicalized DSG can be recognized in polynomial time. This can be shown 
with a straightforward extension to the usual bottom-up dynamic programming algo- 
rithm for context-free grammars. In the DSG case, the nonterminals in the chart are 
paired with multisets. The nonterminals are used to verify that the immediate dom- 
inance relations (i.e., the parent-child escriptions) hold, just as in the case of CFG. 
The multisets record the domination descriptions whose lower (dominated) node has 
been found but whose upper (dominating) node still needs to be found in order for 
the parse to find a valid derivation of the input string (so-called open domination 
descriptions). The key to the complexity result is that the size of the multisets is lin- 
early bounded by the length of the input string if the grammar is lexicalized, and the 
number of multisets of size n is polynomial in n. Furthermore, if the number of open 
domination descriptions in any chart entry is bounded by some constant independent 
102 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
of the length of the input string (as is plausible for many natural anguages including 
English), the parser performs in cubic time. 
4. Some Linguistic Analyses with DSG 
In Section 1, we saw that the extended omain of locality of the elementary structures 
of DSG--which DSG shares with LTAG--allows us to develop lexicalized grammars 
in which the elementary structures contain lexical items and the syntactic structure 
they project. There has been considerable research in the context of LTAG on the is- 
sue of how to use the formalism for modeling natural anguage syntax--we mention 
as salient examples XTAG-Group (1999), a wide-coverage grammar for English, and 
Frank (1992, forthcoming), an extensive investigation from the point of view of theo- 
retical syntax. Since DSG shares the same extended omain of locality as LTAG, much 
of this research carries over to DSG. In this section, we will be presenting linguis- 
tic analyses in DSG that follow some of the elementary principles developed in the 
context of LTAG. We will call these conventions the standard LTAG practices and 
summarize them here for convenience. 
? Each elementary structure contains a lexical item (which can be 
multiword) and the syntactic structure it projects. 
? Each elementary structure for a syntactic head contains yntactic 
positions for its arguments. (In LTAG, this means substitution or foot 
nodes; in DSG, this means substitution odes.) 
? When combining two elementary structures, a syntactic relation between 
their lexical heads is established. For example, when substituting the 
elementary structure for lexical item ll into an argument position of the 
elementary structure for lexical item 12, then ll is in fact an argument 
of 12. 
In Section 1 we also saw that the adjoining operation of LTAG has two properties 
that appear arbitrary from a tree description perspective. The first property is the 
recursion requirement, which states that the root and foot of an auxiliary tree must 
be identically labeled. This requirement embodies the principle that auxiliary trees 
are seen as factoring recursion. The second property, which we will refer to as the 
nesting property of adjunction, follows from the fact that the adjoining operation 
is not symmetrical. All the structural components projected from one lexical item 
(corresponding to the auxiliary tree used in an adjoining step) are included entirely 
between two components in the other projected structure. That is, components of only 
one of the lexically projected structures can get separated in an adjoining step. 
In this section, we examine some of the ramifications of these two constraints 
by giving a number of linguistic examples for which they appear to preclude the 
formulation of an attractive analysis. We show that the additional flexibility inherent in 
the generalized substitution operation is useful in overcoming the problems that arise. 
4.1 Factoring of Recursion 
We begin by explaining why, in LTAG, the availability of analyses for long-distance 
dependencies is limited by the recursion requirement. Normally, substitution is used 
in LTAG to associate a complement to its head, and adjunction is used to associate 
a modifier. However, adjunction rather than substitution must be used with com- 
plements involving long-distance dependencies, e.g., in wh-dependencies and raising 
103 
Computational Linguistics Volume 27, Number 1 
S 
NP~ S NP VP 
I I 
many of us S John VP 
NP VP V S 
PRO VP hopes 
to meet NPi 
I 
e 
Figure 14 
S-analysis for extraction from infinitival complements. 
constructions. Such auxiliary trees are called predicative auxiliary trees. 12 In a pred- 
icative auxiliary tree, the foot node should be one of the nonterminal nodes on the 
frontier that is included ue to argument requirements of the lexical anchor of the tree 
(as determined by its active valency). However, the recursion requirement means that 
all frontier nonterminal nodes that do not have the same label as the root node must 
be designated as substitution nodes, which may mean that no well-formed auxiliary 
tree can be formed. 
Let us consider again the topicalized sentence used as an example in Section 1, 
repeated here for convenience: 
(1) Many of us, John hopes to meet 
A possible analysis is shown in Figure 3 in Section 1. We will refer to this anal- 
ysis as the VP-complement analysis. Note that the individual pieces of the structures 
projected from lexical items follow standard LTAG practices. Because of the recursion 
requirement, the tree on the right is not (a description of) an auxiliary tree. To obtain 
an auxiliary tree in order to give a usual TAG-style account of long-distance depen- 
dencies, the complement of the equi-verb (control verb) hopes must be given an S label, 
which in turn imposes a linguistic analysis using an empty (PRO) subject as shown 
in Figure 14 (or, at any rate, an analysis in which the infinitival to meet projects to S). 
The VP-complement analysis has been proposed within different frameworks, and 
has been adopted as the standard analysis in HPSG (Pollard and Sag 1994). However, 
because this would require an auxiliary tree rooted in S with a VP foot node, the 
recursion requirement precludes the adoption of such an analysis in LTAG. We are 
12 This term is from Schabes and Shieber (1994). Kroch (1987) calls such trees complement auxiliary trees. 
104 
Rainbow, Vijay-Shanker, and Weir D-Tree Substitution Grammars  
NPi S 
/ / x , ,x  ' I I 
i i 
John PP VP 
P NPi V NP PP 
I I 
to e gave the book 
Figure 15 
HPSG analysis of give expressed as trees. 
S 
NP VP 
I 
Peter 
not suggesting that one linguistic analysis is better than another, but instead we point 
out that the formal mechanism ofLTAG precludes the adoption of certain linguistically 
motivated analyses. Furthermore, this mechanism akes it difficult to express entire 
grammars originally formulated in other formalisms in LTAG; for example, when 
compiling a fragment of HPSG into TAG (Kasper et al 1995). In fact, the compilation 
produces tructures just like those (described) in Figure 3. Kasper et al (1995) consider 
the tree on the right of Figure 3 to be an auxiliary tree with the VP sibling of the anchor 
determined tobe the foot node. Technically, the tree on the right of Figure 3 caimot be 
an auxiliary tree. Kasper et al (1995) overcome the problem by making the node label 
a feature (with all nodes having a default label of no significance). This determination 
of the foot node is independent of the node labels of the frontier nodes. Instead, the 
foot node is chosen because it shares certain crucial features (other than label!) with 
the root node. These shared features are extracted from the HPSG rule schema nd 
are used to define the localization of dependencies in the compiled TAG grammar. See 
Kasper et al (1995) for details. 
A similar example involves analyses for sentences such as (2), which involve ex- 
traction from argument PPs. 
(2) John, Peter gave the book to 
Figure 15 shows the structures obtained by using the method of Kasper et al (1995) 
for compiling an HPSG fragment to TAG-like structures. In contrast to traditional TAG 
analyses (in which the elementary tree contains the preposition and its PP, with the 
NP complement of the preposition as a substitution ode), the PP argument of the 
ditransitive verb is not expanded. ~3Instead the PP tree anchored by the preposition 
is substituted. However, because of the extraction, DSG's notion of substitution rather 
than LTAG substitution would need to be used. 
These examples uggest hat the method for compiling an HPSG fragment into 
TAG-like structures discussed in Kasper et al (1995) can be simplified by compiling 
HPSG to a DSG-like framework. 
13 Recall that we are not, in this section, advocating one analysis over another; rather, we are discussing 
the range of options available to the syntactician working in the TAG framework. 
105 
Computational Linguistics Volume 27, Number 1 
NPi S / , , ,  , 
I 
i 
This painting NP 
DET KI 
a N PP 
copy P 
I 
ol 
Figure 16 
Extraction from picture-NPs. 
S 
NP VP 
I ' I i 
John VP 
V NP 
I 
bought 
NP~ 
I 
e 
We have shown a number of examples where some, but not all, of the possible 
linguistic analyses can be expressed in LTAG. It could be claimed that a formal frame- 
work limiting the range of possible analyses constitutes a methodological dvantage 
rather than a disadvantage. However, as is well known, there are several other exam- 
ples in English syntax where the factoring of recursion requirement in fact eliminates 
all plausible LTAG analyses. The only constraint assumed here is that extraction is 
localized in elementary trees. One such example in English is extraction out of a "pic- 
ture-NP" (a noun which takes a prepositional complement from which extraction i to 
the main sentence is possible), as illustrated in the following example: 
(3) This painting, John bought a copy of 
Following the standard LTAG practices, we would obtain the structures described 
in Figure 16. As these descriptions show, the recursion constraint means that adjoining 
cannot be used to provide this analysis of extraction out of NPs. See Kroch (1989) for 
various examples of such constructions in English and their treatment using an exten- 
sion of TAG called multicomponent tree adjoining rammars. (We return to analyses 
using multicomponent TAG in Section 4.2.) 
However, we now show that all of these cases can be captured uniformly with 
generalized substitution (see Figure 17). The node labeled X in fl arises due to the 
argument requirements of the anchor (the verb) and when X = S, fl is a predica- 
tive auxiliary tree in LTAG. The required derived phrase structure in these cases is 
described by 7. To obtain these trees, it would suffice to simply substitute the compo- 
nent rooted in X of c~ at the node labeled X in ft. While in general, such a substitution 
would not constrain the placement of the upper component of t ,  because of the labels 
of the relevant nodes, this substitution will always result in % The use of substitution 
at argument nodes not only captures the situations where adjoining or multicompo- 
106 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
g t :  S v g 
YPi S NP VP YPi S 
! I 
i i 
X VP NP VP 
V X V X 
J 
e 
e 
Figure 17 
General case of extraction. 
nent adjoining is used for these examples, it also allows the DSG treatment to be 
uniform, and is applicable ven in cases where there is no extraction (e.g., the upper 
component of a is not present). 
We end this discussion of the nature of foot nodes by addressing the question 
of how the choice of foot nodes limits illicit extractions. In the TAG approach, the 
designation of a foot node specifically rules out extraction from any structure that 
gets attached to any other frontier node (other arguments), or from structures that 
are adjoined in (adjuncts). However, as has been pointed out before (Abeill6 1991), the 
choice of foot nodes is not always determined by node labels alone, for example in the 
presence of sentential subjects or verbs such as ddduire, which can be argued to have 
two sentential objects. In these cases some additional linguistic riteria are needed in 
order to designate the foot node. These same linguistic riteria can be used to designate 
frontier nodes from which extraction is possible; extraction can be regulated through 
the use of features. We also note that in moving to a multicomponent TAG analysis, an 
additional regulatory mechanism becomes necessary in any case to avoid extractions 
out of subjects (and, to a lesser degree, out of adjuncts). We refer the interested reader 
to Rainbow, Vijay-Shanker, and Weir (1995) and Rambow and Vijay-Shanker (1998) for 
a fuller discussion. 
4.2 Interspersing of Components 
We now consider how the nesting constraint of LTAG limits the TAG formalism as a 
descriptive device for natural language syntax. We contrast this with the case of DSG, 
which, through the use of domination in describing elementary structures projected 
from a lexical item, allows for the interleaving of components projected from lexical 
items during a derivation. 
Consider the raising example introduced in Section 1 repeated here as (4a), along 
with its nontopicalized version (4b), which indicates a possible original position for 
107 
Computational Linguistics Volume 27, Number 1 
g 
PPi S /, , , ,  , 
I 
i 
To many of us VP 
S 
I ' I t
John VP 
V PP VP to be happy 
I I 
appears e 
Figure 18 
Topicalization out of the clause of a raising verb. 
the topicalized phrase) 4
(4) a. To many of us, John appears to be happy 
b. John appears to many of us to be happy 
Following standard LTAG practices of localizing argument structure (even in the 
presence of topicalization) and the standard LTAG analysis for the raising verb appear, 
the descriptions shown in Figure 18 could be proposed. Because of the nesting property 
of adjunction, the interleaving required to obtain the relevant phrase structure for the 
sentence (4a) cannot be realized using LTAG with the assumed lexical projections (or 
any other reasonable structures where the topicalized PP and the verb appear are in 
the same projected structure). In contrast, with these projections, using generalized 
substitution in DSG (i.e., equating the VP argument node of the verb and the root of 
the infinitival VP), the only possible derived tree is the desired one. 
We will now consider an example that does not involve a wh-type dependency: 
(5) Didn't John seem to like the gift? 
Following the principles laid out in Frank (1992) for constructing the elementary 
trees of TAG, we would obtain the projections described in Figure 19 (except for the 
node labels). Note in particular the inclusion of the auxiliary node with the cliticized 
negation marker in the projection of the raising verb seem. Clearly the TAG opera- 
tions could never yield the necessary phrase structure given this localization. Once 
again, the use of generalized substitution in DSG would result in the desired phrase 
structure. 
An alternative to the treatment in Frank (1992) is implemented in the XTAG gram- 
mar for English (XTAG-Group 1999) developed at the University of Pennsylvania. The 
XTAG grammar does not presuppose the inclusion of the auxiliary in the projection 
of the main verb. Rather, the auxiliary gets included by separately adjoining a tree 
14 Throughout this section, we underline the embedded clause with all of its arguments, such as here, the 
raised subject. 
108 
Rainbow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
I 
Didn't 
V VP 
I 
seem 
Figure 19 
Raising verb with a fronted auxiliary. 
S S 
I I I i i 
VP John VP 
to like the gift 
projected from the auxiliary verb. The adjunction of the auxiliary is forced through 
a linguistically motivated system of features. A treatment such as this is needed to 
avoid using multicomponent adjoining. In our example, the auxiliary, along with the 
negation marker, is adjoined into the tree projected by the embedded verb l ike, which 
may be considered undesirable since semantically, it is the matrix verb seem that is 
negated. We take this example to show once more that TAG imposes restrictions on 
the linguistic analyses that can be expressed in it. Specifically, there are constructions 
(which do not involve long-distance phenomena) for which one of the most widely 
developed and comprehensive theories for determining the nature of localization in 
elementary trees--that of Frank (1992)---calmot be used because of the nature of the 
TAG operation of adjunction. In contrast, the operations of DSG allow this theory of 
elementary lexical projections to be used. 
In English, the finite verb appears before the subject only in questions (and in 
some other contexts uch as neg-inversion), but in other languages, this word order 
is routine, leading to similar problems for an LTAG analysis. In V1 languages uch 
as Welsh, the subject appears in second position after the finite verb in the standard 
declarative sentence. The raised subject behaves in the same manner as the matrix 
subject, as observed in Harley and Kulick (1998) and illustrated in (6), from Hen- 
drick (1988): 
(6) a. Mae 
Is 
John 
b. Mae 
Is 
John 
Si6n yn gweld Mair 
John seeing Mary 
is seeing Mary 
Si6n yn digwydd bod yn gweld Mair 
John happening be seeing Mary 
happens to be seeing Mary 
In German, a V2 language, the finite verb appears in second position in matrix 
clauses. The first position may be occupied by any constituent (not necessarily the 
subject). When the subject is not in initial position, it follows the finite verb, both in 
109 
Computational Linguistics Volume 27, Number 1 
S 
NP VP 
I ' 
John 
NPi S 
I 
i 
, Which bridge VP 
u 
I VP PP 
v 
I P NPi 
sleep I I 
Figure 20 
Licit extraction from an adjunct in English. 
under e 
simplex sentences and in raising constructions: 
(7) a. Leider wird es standig regnen 
unfortunately will itNOM continually rain 
Unfortunately, it will rain continually 
b. Oft schien e_~s uns st~indig zu regnen 
often seemed itNo M USDA w continually to rain 
Often it seemed to us to rain continually 
In the German example, a separate adjunction of the tensed verb (as in the XTAG 
analysis of the English auxiliary) is not a viable analysis at all, since the tensed verb 
is not an auxiliary but the main (raising) verb of the matrix clause. 
We now return to examples that do not include raising, but only wh-dependencies. 
(8) a. John slept under the bridge 
b. Which bridge did John sleep under? 
Most LTAG analyses would treat he prepositional phrase in (8a) as an adjunct and 
use an intransitive frame for the verb. However, the related sentence (8b) cannot be 
analyzed with TAG operations in the same way, because the projected structures from 
the verb and the preposition would have to be as shown in Figure 20. The interspersing 
of components from these projections to obtain the desired tree cannot be obtained 
using adjoining. Clearly, with the appropriate generalized substitutions in DSG, this 
tree alone will be derived with these lexical projections. 
Related problems arise in languages in which a wh-moved element does not in- 
variably appear in sentence-initial position, as it does in English. For example, in 
110 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Kashmiri, the wh-element ends up in second position in the presence of a topic. This 
is the case even if the wh-element comes from the embedded clause and the topic from 
the matrix clause. (The data is from Bhatt, \[1994\].) 
(9) a. rameshan kyaa dyutnay tse 
RameShERc whatNoM gave yOUDA T 
What did you give Ramesh? 
b. rameshan kyaai chu baasaan ki me kor ti 
RameShERc what is belieVeNPERF that IERC do 
What does Ramesh believe that I did? 
Another example comes from Rumanian. Rumanian differs from English in that it 
allows multiple fronted wh-elements in the same clause. Leahu (1998) illustrates this 
point with the examples in (10) (her (8a) and (11a)); (10a) shows multiple wh-movement 
in the same clause, while (10b) shows multiple wh-words in one clause that originate 
from different clauses, resulting again in an interspersed order. 
(10) a. Cinei cuij ti promite o masina tj? 
who to whom promises a car 
Who promises a car to whom? 
b. Cinei pe cinej a zis ti ca a vazut tj? 
who whom has said that has seen 
Who has said he has seen whom? 
The examples discussed in this section show a range of syntactic phenomena in
English and in other languages that cannot be analyzed using the operations of TAG. 
We conclude that complex interspersing is a fairly common phenomenon i natu- 
ral language. As in the case of factoring of recursion, sometimes we find that the 
definition of adjunction precludes certain linguistically plausible analyses but allows 
others; in other cases, TAG does not seem to allow any linguistically plausible anal- 
ysis at all. However, in each case, we can use standard LTAG practices for projecting 
structures from lexical items and combine the resulting structures using the general- 
ized substitution operation of DSG to obtain the desired analyses, thus bringing out 
the underlying similarity of related constructions both within languages and cross- 
linguistically. 
4.3 Linguistic Use of Path Constraints 
In the examples discussed so far, we have not had the need to use path constraints. 
The d-edges een so far express any domination path. Recall that path constraints can 
be associated with a d-edge to express certain constraints on what nodes, in terms of 
their labels, cannot appear within a path instantiating a d-edge. 
111 
Computational Linguistics Volume 27, Number 1 
S 
NP VP 
I ' 
I 
I 
wood I 
I 
I 
I 
I 
I 
I 
I 
" ' " ' " ' . . .  ok 
? ' ' ? ' . o  ? o 
I path cgnstr,aint: I ', 
no ~ nocle 
%. 
S 
! 
i 
I 
....." ~ ' " ' . . .ok .  
V S 
' l Seem8 
S 
? ,~ .  I 
I "~"  
I 
VP 
V Vp 
' T 
appears 
VP 
NP 
I 
it 
~tc ??  
S 
VP 
to float 
Figure 21 
Path constraints are needed to rule out ungrammatical super-raising. 
As an example of the use of path constraints, let us consider the well-known case 
of "super-raising": 
(11) a. It seems wood appears to float 
b. *Wood seems it appears to float 
c. Wood seems to appear to float 
In (11a), the subject of float, wood, has raised to the appears clause, while the raising 
verb seem does not trigger raising and has an expletive it as its subject? In (11b), 
wood has raised further, and appear now has an expletive subject; (11b) is completely 
ungrammatical. If we make the intermediate raising verb appear nonfinite (and hence 
without a subject), as in (11c), the sentence is again grammatical. 
Now consider the DSG analysis for (11a) shown in Figure 21. The d-tree for seem 
has an S substitution node, since seems takes a finite complement with a subject? Appear, 
112 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
since it is finite, projects to S, but takes a VP complement since its complement, he 
float clause, is nonfinite and has no overt subject, is We furthermore assume that the 
raising verbs seem and appear do not select for subjects, but that the expletive subject 
it is freely available for inclusion in their d-trees, since expletive it is semantically 
vacuous and merely fulfills syntactic requirements ( uch as subject-verb agreement), 
not semantic ones. We substitute the float d-tree into the appear d-tree, and the result 
into the seem d-tree, as indicated by the solid arrows in Figure 21. Given the reading 
off process, this derived d-tree can be seen to express two possibilities, depending 
on where the wood component and the expletive it end up. These two possibilities 
correspond to (11a) and (11b). 
To exclude the ungrammatical result, we use the path constraints discussed in 
Section 2.6. Let us make the uncontroversial ssumption that as we project from a 
verb, we will project to a VP before projecting to an S. But we will interpret his 
notion of projection as also applying to the d-edges between nodes labeled VP: we 
annotate the d-edge between the VP nodes in the float tree (and in fact in all trees, of 
course) as having a path constraint that does not allow an S node on this path. This 
is, after all, what we would expect in claiming that the float tree represents a structure 
lexically projected from the verb float. 16 Given this additional grammatical expression, 
after the substitution at the S node of the seems tree, it is no longer possible to read off 
from the d-tree in Figure 21 a tree whose yield is the ungrammatical (11b). The only 
possible way of reading off from the derived d-tree yields (11a). 
What is striking is that this particular path constraint disallowing S nodes between 
VP nodes in structures projected from a verb can be used in other cases as well. In fact, 
this same path constraint on its own, when applied to the English examples considered 
so far, predicts the correct arrangement of all components among the two d-trees 
being combined, regardless of whether the nesting constraint of adjoining must be met 
(extraction out of clausal or VP complements, extraction from NP or PP complements), 
or not (extraction from the clause of a raising verb, raising verb with fronted auxiliary, 
or extraction from an adjunct). For example, in Figure 18, after substituting the to be 
happy component at the VP node of the appears d-tree, a path constraint on the d-edge 
between the two VP nodes of the to be happy tree makes it impossible for the to any 
of us component to intervene, thus leaving the interspersed tree as the only possible 
result of the reading off process, even if we relaxed the requirement on label equality 
for the removal of d-edges during the reading off process. 
Note that while the same path constraints apply in all cases, in LTAG, as we have 
seen, the nesting constraint of adjoining precludes deriving the correct order in some 
cases, and the use of extensions such as mult icomponent adjoining has been suggested. 
In fact, because there are both situations in which the arrangement of components of 
the lexically projected structures corresponds to adjoining and situations in which 
this arrangement is inappropriate, Vijay-Shanker (1992) raises the question of whether 
the definition of the formalism should limit the arrangement of components of the 
lexically projected structures, or whether the possible arrangements should be derived 
from the linguistic theory and from intuitions about the nature of the elementary 
objects of a grammar. This subsection partially addresses this question and shows 
15 The point we are making in this section relies on there being some distinction between the labels of the 
roots of the appear and float clauses, a linguistically uncontroversial assumption. Here, we use the 
categorial distinction between Sand VP for convenience only; we could also have assumed a difference 
in feature content. 
16 Bleam (2000) uses informal path constraints inmuch the same way in order to restrict Spanish clitic 
climbing in an LTAG analysis. 
113 
Computational Linguistics Volume 27, Number 1 
how the path constraint expressing the nature of projection from a lexical item can be 
used to derive the arrangements of components corresponding to adjoining in some 
cases as well as predict when the nesting condition of adjoining is too limiting in the 
others. 
4.4 Underspecification of Linear Precedence 
In our proposed tree description language, we provide for underspecified dominance 
but not for underspecified linear precedence. As a consequence, in the graphical repre- 
sentations of d-trees, we assume that sister nodes are always ordered as shown. This 
may seem arbitrary at first glance, especially since in many linguistic frameworks 
and theories it is common to specify linear precedence (LP) separately from syntactic 
structure (GPSG, HPSG, LFG, ID/LP-TAG \[Joshi 1987\] and FO-TAG \[Becker, Joshi, 
and Rambow 1991\], various dependency-based formalisms, and so on). This separate 
specification of LP rules allows for underspecified LP rules, which is useful in cases 
in which word order is not fully fixed. 
In principle, an underspecification f LP could easily be added to DSG without 
profoundly changing its character or formal properties. The reason we have not done 
so is that in all cases, the same effect can be achieved using underspecified dominance 
alone, though at the cost of forcing a linguistic analysis that uses binary branching 
phrase structure trees rather than n-ary branching ones. We will illustrate the point 
using examples from German, which allows for scrambling of the arguments. 
Consider the following German examples. 17
(12) a. dat~ die Kinder dem Lehrer das Buch geben 
that \[the children\]NOM \[the teacher\]DAT \[the book\]Acc give 
that the children give the teacher the book 
b. dat~ dem Lehrer die Kinder das Buch geben 
c. dat~ dem Lehrer das Buch die Kinder geben 
All orders of the three arguments are possible, resulting in six possible sentences 
(three of which are shown in (12)). In DSG, we can express this by giving the lexical 
entry for geben shown in Figure 22. TM The arguments of the verb have no dominance 
specified among them, so that when using this d-tree (which is of course not yet a 
tree) in a derivation, we can choose whichever dominance relations we want when 
we read off a tree at the end of the derivation. As a result, we obtain any ordering of 
the arguments. 
As mentioned previously, while we can derive any ordering, we cannot, in DSG, 
obtain a flat VP structure. However, our analysis has an advantage when we consider 
"long scrambling," in which arguments from two lexical verbs intersperse. (In German, 
only certain matrix verbs allow long scrambling.) If we have the subject-control verb 
versuchen 'to try', the nominative argument is the overt subject of the matrix clause, 
while the dative and accusative arguments are arguments of the embedded clause. 
Nonetheless, the same six word orders are possible (we again underline the embedded 
17 We give embedded clauses tarting with the complementizer in order to avoid the problem of V2. For 
a discussion of V2 in a framework like DSG, see Rambow (1994a) and Rambow and Santorini (1995). 
18 We label all projections from the verb (except the immediate preterminal) VP. We assume that relevant 
levels of projection are distinguished by the feature content of the nodes. This choice has mainly been 
made in order to allow us to derive verb-second matrix clause order using the same d-trees, which is 
also why the verb is in a component of its own. 
114 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
VP VP VP 
NPNoM VP NPAcc VP NPDAT VP VP 
VP 
I 
V 
I 
e 
Figure 22 
D-tree for German verb geben 'to give'. 
VP 
geben 
SUBJ XCOMP 
VP VP 
NP VP VP VP 
VP 
I 
e 
Figure 23 
D-tree for German verb versuchen 'to try'. 
VP 
VP V 
versuchen 
clause material): 
(13) a. daf~ die Kinder dem Lehrer das Buch zu geben 
that \[the children\]NOM \[the teacher\]DAT \[the book\]ncc to give 
that the children try to give the teacher the book 
b. daf~ dem Lehrer die Kinder das Buch zu geben versuchen 
c. daf~ dem Lehrer das Buch die Kinder zu geben versuchen 
versuchen 
try 
We can represent the matrix verb as shown in Figure 23, and a derivat ion as shown 
in Figure 24. It is clear that we  can still obtain all possible word  orders, and that this 
115 
Computational Linguistics Volume 27, Number 1 
OBJ INDOBJ XGOMP 
VP VP VP VP 
NP VP NP VP VP VP VP V 
SUBJ 
VP 
NP VP 
- -  - , i x ,  ' ' I " 
\ I .#  
VP V , ~versuchen.  - 
I 
VP zu geben VP 
I I 
e e 
Figure 24 
DSG derivation for a complex sentence. 
wou ld  be imposs ib le  us ing  s imple  LP rules that order  s ister nodes.  19 (It wou ld  also 
be imposs ib le  in LTAG, but  see Joshi, Becker, and  Rambow \[2000\] for an a l ternate 
d iscuss ion of long sc rambl ing  in LTAG.) 
5. Modeling Syntactic Dependency 
In the prev ious  sections, we  have presented  DSG and have  shown how it can be used  to 
prov ide  ana lyses  for a range of l inguist ic  phenomena.  In this section, we  conc lude  our  
in t roduct ion  of DSG by  d iscuss ing  the re lat ionsh ip  between der ivat ions  in DSG and 
syntactic dependency. Recently, syntact ic  dependency  has emerged as an impor tant  
factor for app l icat ions  in natura l  language process ing.  
In lex ica l ized fo rmal i sms such as LTAG, the operat ions  of the fo rmal i sm (i.e., 
in the case of LTAG, subst i tut ion and  adjunct ion)  relate structures assoc iated w i th  
two lexical i tems. It is therefore natura l  to interpret  hese operat ions  as estab l ish ing 
a direct syntact ic re lat ion between the two lexical i tems,  i.e., a re lat ion of syntact ic  
dependency.  There are at least two  types  of syntact ic  dependency :  a re lat ion of com- 
p lementat ion  (pred icate -argument  relat ion) and  a re lat ion of modi f i cat ion  (predicate-  
ad junct  relat ion).  2? Syntact ic dependency  represents  an impor tant  l inguist ic  intui t ion,  
p rov ides  a un i fo rm interface to semant ics ,  and  is, as Schabes and  Shieber (1994) argue,  
impor tant  in order  to suppor t  stat ist ical  parameters  in stochast ic f rameworks .  In fact, 
19 This kind of construction has been extensively analyzed in the Germanic syntax literature. Following 
the descriptive notion of "coherent construction" proposed by Bech (1955), Evers (1975) proposes that 
in German (and in Dutch, but not in English) a biclausal structure undergoes a special process to 
produce a monoclausal structure, in which the argument lists of the two verbs are merged and the 
verbs form a morphological unit. This analysis has been widely adopted (in one form or another) in 
the formal and computational syntax literature by introducing special mechanisms into the underlying 
formal system. If the special mechanism produces a single (flat) VP for the new argument list, then LP 
rules for the simplex case can also apply to the complex case. However, the DSG analysis has the 
advantage that it does not involve a special mechanism, and the difference between German and 
English complex clauses is related simply to the difference in word orders allowable in the simplex 
case (i.e., German but not English allows scrambling). Furthermore, the DSG analysis correctly predicts 
some "interleaved" word orders to be grammatical. See Rambow (1995) for details. 
20 In addition, we may want to identify the relation between a function word and its lexical headword 
(e.g., between a determiner and a noun) as a third type of relation. 
116 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
adore claim 
he seem 
Mary / OBJ \ seems I COMP 
I 
hotdog claim adore 
I SUB J SUBJ~~~ BJ 
he Mary hotdog 
Figure 25 
LTAG derivation tree for (14) (left); dependency tree for (14) (right). 
recent advances in parsing technology are due to the explicit stochastic modeling of 
dependency information (Collins 1997). 
Purely CFG-based approaches do not represent syntactic dependency, but other 
frameworks do, e.g. the f-structure (functional structure) of LFG (Kaplan and Bresnan 
1982), and dependency grammars (see, for example, Mel'~uk \[1988\]), for which syn- 
tactic dependency is the sole basis for representation. As observed by Rambow and 
Joshi (1997), for LTAG, we can see the derivation structure as a dependency structure, 
since in it lexemes are related directly. 
However, as we have pointed out in Section 4.1, the LTAG composition operations 
are not used uniformly: while substitution is used only to add a (nominal) complement, 
adjunction is used both for modification and (clausal) complementation. 21 Furthermore, 
there is an inconsistency in the directionality of the substitution operation and those 
uses of adjunction for clausal complementation: i  LTAG, nominal complements are 
substituted into their governing verb's tree, while the governing verb's tree is ad- 
joined into its own clausal complement. The fact that adjunction and substitution are 
used in a linguistically heterogeneous manner means that (standard) LTAG derivation 
trees do not provide a direct representation f the dependencies between the words 
of the sentence, i.e., of the predicate-argument a d modification structure. In DSG, 
this problem is overcome straightforwardly, since DSG uses generalized substitution 
for all complementation (be it nominal or clausal), while still allowing long-distance 
effects. =
There is a second, more serious problem with modeling syntactic dependency in
LTAG, as can be seen from the following example: 
(14) Hot dogs he claims Mary seems to adore 
The problem is that in the standard LTAG derivation, we adjoin both the trees 
for claim and seem into the tree for adore (Figure 25, left), while in the (commonly 
assumed) dependency structure, seem depends on claim, and adore depends on seem 
(Figure 25, right). The problem is in fact related to the interleaving problem discussed 
in Section 4.2, and can easily be solved in DSG by proposing a structure such as that 
21 Clausal complementation ca not be handled nniformly by substitution because ofthe existence of
syntactic phenomena such as long-distance wh-movement i  English. 
22 Modification can be handled by some other operation, such as sister adjunction (Rainbow, 
Vijay-Shanker, and Weir 1995), and is thus distinguished from complementation. We do not discuss 
modification i  this paper. 
117 
Computational Linguistics Volume 27, Number 1 
S 
I 
I 
i 
VP 
V VP 
I 
seems 
Figure 26 
Elementary d-tree for finite seems. 
in Figure 26 for seems, which we have already seen in Figure 21. (This structure can 
be justified on linguistic grounds independently from the dependency considerations, 
by assuming that all finite verbs--whether raising or not--project to at least S \[= IP\]. 
Raising verbs simply lack a subject of their own, but the S node is justified by the 
finiteness of the verb, not by the presence or absence of a subject.) Thus, DSG can be 
used to develop grammars in which the derivation faithfully and straightforwardly 
reflects yntactic dependency. 23 
6. Related Work 
In this section, we mention some related theoretical work and some application- 
oriented work that is based on DSG. 
On the theoretical side, Kallmeyer (1996, 1999) presents an independently con- 
ceived formalism called tree description grammar (TDG). TDG is similar to DSG: 
in both formalisms, descriptions of trees are composed uring derivations through 
conjunction and equation of nodes. Furthermore, like DSG, TDG does not allow the 
conflation of immediate dominance structure specified in elementary structures. How- 
ever, TDG allows for more than one node to be equated in a derivation step: nodes 
are "marked" and all marked nodes are required to be equated with other nodes in a 
derivation step. (Equating more than one pair of nodes in each derivation step shifts 
some of the work done in reading off in DSG to the derivation in TDG.) In DSG, 
we have designed a simple generative system based on tree descriptions involving 
dominance, using an operation that directly correspond to the linguistic notion of 
complementation. Additional mechanisms, uch as the marking of nodes and their 
simultaneous involvement in a derivation step, are not available in DSG. 
Hepple (1998) relates DSG to a system he has previously proposed in which de- 
ductions in implicational linear logic are recast as deductions involving only first-order 
formulas (Hepple 1996). He shows how this relation can be exploited to give deriva- 
tions in DSG a functional semantics. 
There is an ongoing effort to evaluate the theoretical proposals presented in this 
paper through the development of a wide-coverage DSG-based parsing system that 
provides analysis in a broadly HPSG style (Carroll et al 2000). One aspect of this work 
involves exploiting the extended omain of locality that DSG shares with TAG in order 
23 Candito and Kahane (1998) propose to use derivations in DSG to model semantic (rather than 
syntactic) dependency. 
118 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
to maximize localization of syntactic dependencies within elementary tree descriptions, 
thereby avoiding the need for unification during parsing (Carroll et al 1999). 
Nicolov and Mellish (2000) use DSG as the formalism in a generation application. 
The principal motivation for using DSG is that DSG is a lexicalized formalism which 
can provide derivations that correspond to the traditional notion of (deep) syntactic 
dependency (see Section 5), which is often considered to be the input to the syntactic 
component of a generation system. 
7. Conc lus ions  
We have introduced the grammar formalism of d-tree substitution grammars by show- 
ing how it emerges from a tree-description-theoretic analysis of tree adjoining ram- 
mars. Derivations in DSG involve the composition of d-trees, special kinds of tree 
descriptions. Trees are read off from derived d-trees. 
We have shown that the DSG formalism can be used to express a variety of lin- 
guistic analyses, including styles of analysis that do not appear to be available with 
the LTAG approach, and analyses for constructions that appear to be beyond the de- 
scriptive capacity of LTAG. Furthermore, linguistic analyses of syntactic phenomena 
are uniform, both language-internally nd cross-linguistically. Finally, DSG allows for 
a consistent modeling of syntactic dependency. 
References 
AbeillG Anne. 1991. Une grammaire l xicalisde 
d'arbres adjoints pour le fran?ais. Ph.D. 
thesis, Universit~ Paris 7. 
Backofen, Roll James Rogers, and K. 
Vijay-Shanker. 1995. A first-order 
axiomatization f the theory of finite 
trees. Journal of Language, Logic, and 
Information, 4(1):5-39. 
Bech, Gunnar. 1955. Studien fiber das deutsche 
Verbum infinitum. Det Kongelige Danske 
videnskabernes selskab. 
Historisk-Filosofiske M ddelelser, bd. 35, 
nr. 2 (1955) and bd. 36, nr. 6 (1957). 
Munksgaard, Kopenhagen. Second 
unrevised edition published 1983 by Max 
Niemeyer Verlag, Tiibingen (Linguistische 
Arbeiten 139). 
Becker, Tilman, Aravind Joshi, and Owen 
Rainbow. 1991. Long distance scrambling 
and tree adjoining rammars. In Fifth 
Conference ofthe European Chapter of the 
Association for Computational Linguistics 
(EACL'91), pages 21-26. 
Becker, Tilman and Owen Rainbow. 1995. 
Parsing non-immediate dominance 
relations. In Proceedings ofthe Fourth 
International Workshop on Parsing 
Technologies, pages 26-33, Prague. 
Bhatt, Rakesh. 1994. Word Order and Case in 
Kashmiri. Ph.D. thesis, University of 
Illinois, Urbana-Champaign. 
Bleam, Tonia. 2000. Clitic climbing and the 
power of tree adjoining rammar. In 
Anne AbeilM and Owen Rambow, editors, 
Tree Adjoining Grammars: Formalisms, 
Linguistic Analyses and Processing. CSLI 
Publications, pages 193-220. Paper 
initially presented in 1995. 
Candito, Marie-He'l~ne and Sylvain Kahane. 
1998. Defining DTG derivations to get 
semantic graphs. In Proceedings ofthe 
Fourth International Workshop on Tree 
Adjoining Grammars and Related Frameworks 
(TAG+4), IRCS Report 98-12, pages 25-28. 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Carroll, John, Nicolas Nicolov, Olga 
Shaumyan, Martine Smets, and David 
Weir. 1999. Parsing with an extended 
domain of locality. In Ninth Conference of
the European Chapter of the Association for 
Computational Linguistics (EACL'99), 
pages 217-224. 
Carroll, John, Nicolas Nicolov, Olga 
Shaumyan, Martine Smets, and David 
Weir. 2000. Engineering a wide-coverage 
lexicalized grammar. In Proceedings ofthe 
Fifth International Workshop on Tree 
Adjoining Grammars and Related 
Frameworks, pages 55-60. 
Collins, Michael. 1997. Three generative, 
lexicalised models for statistical parsing. 
In Proceedings ofthe 35th Annual Meeting, 
Madrid, Spain, July. Association for 
Computational Linguistics. 
Evers, Arnold. 1975. The Transformational 
Cycle in Dutch and German. Ph.D. thesis, 
University of Utrecht. Distributed by the 
Indiana University Linguistics Club. 
119 
Computational Linguistics Volume 27, Number 1 
Frank, Robert. 1992. Syntactic Locality and 
Tree Adjoining Grammar: Grammatical, 
Acquisition and Processing Perspectives. 
Ph.D. thesis, Department of Computer 
and Information Science, University of 
Pennsylvania. 
Frank, Robert. Forthcoming. Phrase Structure 
Composition and Syntactic Dependencies. 
MIT Press, Cambridge. 
Gazdar, G. 1988. Applicability of indexed 
grammars to natural anguages. In U. 
Reyle and C. Rohrer, editors, Natural 
Language Parsing and Linguistic Theories. D. 
Reidel, Dordrecht, pages 69-94. 
Harley, Heidi and Seth Kulick. 1998. TAG 
and raising in VSO languages. In 
Proceedings ofthe Fourth International 
Workshop on Tree Adjoining Grammars and 
Related Frameworks (TAG+4), IRCS Report 
98-12, pages 62-65. Institute for Research 
in Cognitive Science, University of 
Pennsylvania. 
Hendrick, R. 1988. Anaphora in Celtic and 
Universal Grammar. Kluwer Academic 
Publishers, Dordrecht. 
Hepple, Mark. 1996. A compilation-chart 
method for linear categorical deduction. 
In Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 537-542. 
Hepple, Mark. 1998. On same similarities 
between D-Tree Grammars and 
type-logical grammars. In Proceedings of
the Fourth International Workshop on Tree 
Adjoining Grammars and Related Frameworks 
(TAG+4), IRCS Report 98-12, pages 66-69. 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Joshi, Aravind K. 1987. Word-order 
variation in natural anguage generation. 
Technical Report, Department of
Computer and Information Science, 
University of Pennsylvania. 
Joshi, Aravind K., Tilman Becker, and Owen 
Rambow. 2000. A new twist on the 
competence/performance distinction. In 
Anne Abeill4 and Owen Rambow, editors, 
Tree Adjoining Grammars: Formalisms, 
Linguistic Analysis, and Processing. CSLI 
Publications, pages 167-182. 
Joshi, Aravind K. and Yves Schabes. 1991. 
Tree-adjoining grammars and lexicalized 
grammars. In Maurice Nivat and Andreas 
Podelski, editors, Definability and 
Recognizability of Sets of Trees. Elsevier. 
Kallmeyer, Laura. 1996. Tree description 
grammars. In D. Gibbon, editor, Natural 
Language Processing and Speech Technology. 
Results of the 3rd KONVENS Conference, 
pages 332-341, Berlin. Mouton de 
Gruyter. 
KaUmeyer, Laura. 1999. Tree Description 
Grammars and Underspecified 
Representations. Ph D. thesis, University of 
Tiibingen. Available as Technical Report 
No. 99-08 from the Institute for Research 
in Cognitive Science at the University of 
Pennsylvania. 
Kaplan, Ronald M. and Joan W. Bresnan. 
1982. Lexical-functional grammar: A
formal system for grammatical 
representation. I  J. W. Bresnan, editor, 
The Mental Representation of Grammatical 
Relations. MIT Press, Cambridge, MA. 
Kasper, Robert, Bernd Kiefer, Klaus Netter, 
and K. Vijay-Shanker. 1995. Compilation 
of HPSG and TAG. In Proceedings ofthe 
Annual Meeting, pages 92-99. Association 
for Computational Linguistics. 
Kroch, Anthony. 1987. Subjacency in a tree 
adjoining grammar. In Alexis 
Manaster-Ramer, ditor, Mathematics of
Language. John Benjamins, Amsterdam, 
pages 143-172. 
Kroch, Anthony. 1989. Asymmetries in long 
distance xtraction i  a Tree Adjoining 
Grammar. In Mark Baltin and Anthony 
Kroch, editors, Alternative Conceptions of
Phrase Structure. University of Chicago 
Press, Chicago, pages 66-98. 
Leahu, Manuela. 1998. Wh-dependencies n 
Romanian and TAG. In Proceedings ofthe 
Fourth International Workshop on Tree 
Adjoining Grammars and Related Frameworks 
(TAG+4), pages 92-95, IRCS Report 98-12, 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Marcus, Mitchell, Donald Hindle, and 
Margaret Fleck. 1983. D-theory: Talking 
about talking about rees. In Proceedings of
the 21st Annual Meeting, Cambridge, MA. 
Association for Computational 
Linguistics. 
Mel'~uk, Igor A. 1988. Dependency S ntax: 
Theory and Practice. State University of 
New York Press, New York. 
Nicolov, Nicolas and Christopher Mellish. 
2000. Protector: Efficient generation with 
lexicalized grammars. In Ruslan Mitkov 
and Nicolas Nicolov, editors, Recent 
Advances in Natural Language Processing 
(RANLP vol. II). John Benjamins, 
Amsterdam and Philadelphia, 
pages 221-243. 
Pollard, Carl and Ivan Sag. 1994. 
Head-Driven Phrase Structure Grammar. 
University of Chicago Press, Chicago. 
Rambow, Owen. 1994a. Formal and 
Computational Aspects of Natural Language 
Syntax. Ph.D. thesis, Department of
Computer and Information Science, 
University of Pennsylvania, Philadelphia. 
120 
Rambow, Vijay-Shanker, and Weir D-Tree Substitution Grammars 
Available as Tectmical Report 94-08 from 
the Institute for Research in Cognitive 
Science (IRCS) and also at ftp://ftp.cis. 
upenn.edu/pub/rambow/thesis.ps.Z. 
Rambow, Owen. 1994b. Multiset-valued 
linear index grammars. In Proceedings of
the 32nd Annual Meeting, pages 263-270. 
Association for Computational 
Linguistics. 
Rambow, Owen. 1995. Coherent 
constructions in German: Lexicon or 
syntax? In Glyn Morrill and Richard 
Oehrle, editors, Formal Grammar: 
Proceedings ofthe Conference ofthe European 
Summer School in Logic, Language, and 
Information, pages 213-226, Barcelona. 
Rambow, Owen. 1996. Word order, clause 
union, and the formal machinery of 
syntax. In Miriam Butt and Tracy 
Holloway King, editors, Proceedings ofthe 
First LFG Conference. On-line version at 
http://www-csli.stanford.edu/ 
publications/LFG/lfgl.html. 
Rainbow, Owen and Aravind Joshi. 1997. A 
formal ook at dependency gran~nars and 
phrase-structure grammars, with special 
consideration of word-order phenomena. 
In Leo Wanner, editor, Recent Trends in 
Meaning-Text Theory. John Benjamins, 
Amsterdam and Philadelphia. 
Rainbow, Owen and Beatrice Santorini. 
1995. Incremental phrase structure 
generation and a universal theory of V2. 
In J. N. Beckman, editor, Proceedings of
NELS 25, pages 373-387, Amherst, MA. 
GSLA. 
Rambow, Owen and K. Vijay-Shanker. 1998. 
Wh-islands in TAG and related 
formalisms. In Proceedings ofthe Fourth 
International Workshop on Tree Adjoining 
Grammars and Related Frameworks (TAG+4), 
pages 147-150, IRCS Report, 98-12. 
Institute for Research in Cognitive 
Science, University of Pennsylvania. 
Rambow, Owen, K. Vijay-Shanker, and 
David Weir. 1995. D-Tree Grammars. In 
Proceedings ofthe 33rd Annual Meeting, 
pages 151-158. Association for 
Computational Linguistics. 
Rogers, James and K. Vijay-Shanker. 1992. 
Reasoning with descriptions of trees. In 
Proceedings ofthe 30th Annual Meeting, 
pages 72-80. Association for 
Computational Linguistics. 
Schabes, Yves. 1990. Mathematical nd 
Computational Aspects of Lexicalized 
Grammars. Ph.D. thesis, Department of
Computer and Information Science, 
University of Pennsylvania. 
Schabes, Yves and Stuart Shieber. 1994. An 
alternative conception of tree-adjoining 
derivation. Computational Linguistics, 
20(1):91-124. 
Vijay-Shanker, K. 1987. A Study of Tree 
Adjoining Grammars. Ph.D. thesis, 
Department of Computer and Information 
Science, University of Pennsylvania, 
Philadelphia, PA, December. 
Vijay-Shanker, K. 1992. Using descriptions 
of trees in a Tree Adjoining Grammar. 
Computational Linguistics, 18(4):481-518. 
Vijay-Shanker, K. and David Weir. 1999. 
Exploring the underspecified world of 
Lexicalized Tree Adjoining Grammars. In 
Proceedings ofthe Sixth Meeting on 
Mathematics of Language. 
Vijay-Shanker, K., David Weir, and Owen 
Rainbow. 1995. Parsing D-Tree Grammars. 
In Proceedings ofthe Fourth International 
Workshop on Parsing Technologies, 
pages 252-259. ACL/SIGPARSE. 
XTAG-Group, The. 1999. A lexicalized Tree 
Adjoining Grammar for English. 
Technical Report. The Institute for 
Research in Cognitive Science, University 
of Pennsylvania. Available at: 
http://www.cis.upenn.edu/~xtag/tech- 
report/tech-report.html. 
121 

c? 2002 Association for Computational Linguistics
Class-Based Probability Estimation Using
a Semantic Hierarchy
Stephen Clark? David Weir?
University of Edinburgh University of Sussex
This article concerns the estimation of a particular kind of probability, namely, the probability
of a noun sense appearing as a particular argument of a predicate. In order to overcome the
accompanying sparse-data problem, the proposal here is to define the probabilities in terms of
senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes
consisting of semantically similar senses. There is a particular focus on the problem of how
to determine a suitable class for a given sense, or, alternatively, how to determine a suitable
level of generalization in the hierarchy. A procedure is developed that uses a chi-square test to
determine a suitable level of generalization. In order to test the performance of the estimation
method, a pseudo-disambiguation task is used, together with two alternative estimation methods.
Each method uses a different generalization procedure; the first alternative uses the minimum
description length principle, and the second uses Resnik?s measure of selectional preference. In
addition, the performance of our method is investigated using both the standard Pearson chi-
square statistic and the log-likelihood chi-square statistic.
1. Introduction
This article concerns the problem of how to estimate the probabilities of noun senses
appearing as particular arguments of predicates. Such probabilities can be useful for a
variety of natural language processing (NLP) tasks, such as structural disambiguation
and statistical parsing, word sense disambiguation, anaphora resolution, and language
modeling. To see how such knowledge can be used to resolve structural ambiguities,
consider the following prepositional phrase attachment ambiguity:
Example 1
Fred ate strawberries with a spoon.
The ambiguity arises because the prepositional phrase with a spoon can attach to either
strawberries or ate. The ambiguity can be resolved by noting that the correct sense of
spoon is more likely to be an argument of ?ate-with? than ?strawberries-with? (Li and
Abe 1998; Clark and Weir 2000).
The problem with estimating a probability model defined over a large vocabulary
of predicates and noun senses is that this involves a huge number of parameters,
which results in a sparse-data problem. In order to reduce the number of parameters,
we propose to define a probability model over senses in a semantic hierarchy and
? Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail:
stephenc@cogsci.ed.ac.uk.
? School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK. E-mail:
david.weir@cogs.susx.ac.uk.
188
Computational Linguistics Volume 28, Number 2
to exploit the fact that senses can be grouped into classes consisting of semantically
similar senses. The assumption underlying this approach is that the probability of a
particular noun sense can be approximated by a probability based on a suitably chosen
class. For example, it seems reasonable to suppose that the probability of (the food
sense of) chicken appearing as an object of the verb eat can be approximated in some
way by a probability based on a class such as FOOD.
There are two elements involved in the problem of using a class to estimate the
probability of a noun sense. First, given a suitably chosen class, how can that class
be used to estimate the probability of the sense? And second, given a particular noun
sense, how can a suitable class be determined? This article offers novel solutions to
both problems, and there is a particular focus on the second question, which can be
thought of as how to find a suitable level of generalization in the hierarchy.1
The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum
1998), version 1.6. Previous work has considered how to estimate probabilities us-
ing classes from WordNet in the context of acquiring selectional preferences (Resnik
1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also
addressed the question of how to determine a suitable level of generalization in the
hierarchy. Li and Abe use the minimum description length principle to obtain a level
of generalization, and Resnik uses a simple technique based on a statistical measure
of selectional preference. (The work by Ribas builds on that by Resnik, and the work
by McCarthy builds on that by Li and Abe.) We compare our estimation method with
those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method
outperforms these alternatives on the pseudo-disambiguation task, and an analysis of
the results shows that the generalization methods of Resnik and Li and Abe appear
to be overgeneralizing, at least for this task.
Note that the problem being addressed here is the engineering problem of es-
timating predicate argument probabilities, with the aim of producing estimates that
will be useful for NLP applications. In particular, we are not addressing the problem
of acquiring selectional restrictions in the way this is usually construed (Resnik 1993;
Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000). The purpose of using a
semantic hierarchy for generalization is to overcome the sparse data problem, rather
than find a level of abstraction that best represents the selectional restrictions of some
predicate. This point is considered further in Section 5.
The next section describes the noun hierarchy from WordNet and gives a more
precise description of the probabilities to be estimated. Section 3 shows how a class
from WordNet can be used to estimate the probability of a noun sense. Section 4 shows
how a chi-square test is used as part of the generalization procedure, and Section 5
describes the generalization procedure. Section 6 describes the alternative class-based
estimation methods used in the pseudo-disambiguation experiments, and Section 7
presents those experiments.
2. The Semantic Hierarchy
The noun hierarchy of WordNet consists of senses, or what Miller (1998) calls lexicalized
concepts, organized according to the ?is-a-kind-of? relation. Note that we are using
concept to refer to a lexicalized concept or sense and not to a set of senses; we use class to
refer to a set of senses. There are around 66,000 different concepts in the noun hierarchy
1 A third element of the problem, namely, how to obtain arguments of predicates as training data, is not
considered here. We assume the existence of such data, obtained from a treebank or shallow parser.
189
Clark and Weir Class-Based Probability Estimation
of WordNet version 1.6. A concept in WordNet is represented by a ?synset,? which is
the set of synonymous words that can be used to denote that concept. For example,
the synset for the concept ?cocaine?2 is { cocaine, cocain, coke, snow, C }. Let syn(c) be the
synset for concept c, and let cn(n) = { c |n ? syn(c) } be the set of concepts that can be
denoted by noun n.
The hierarchy has the structure of a directed acyclic graph (although only around
1% of the nodes have more than one parent), where the edges of the graph constitute
what we call the ?direct?isa? relation. Let isa be the transitive, reflexive closure of
direct?isa; then c? isa c implies c? is a kind of c. If c? isa c, then c is a hypernym of c? and
c? is a hyponym of c. In fact, the hierarchy is not a single hierarchy but instead consists of
nine separate subhierarchies, each headed by the most general kind of concept, such as
?entity?, ?abstraction?, ?event?, and ?psychological feature?. For the purposes of this work
we add a common root dominating the nine subhierarchies, which we denote ?root?.
There are some important points that need to be clarified regarding the hierarchy.
First, every concept in the hierarchy has a nonempty synset (except the notional con-
cept ?root?). Even the most general concepts, such as ?entity?, can be denoted by some
noun; the synset for ?entity? is { entity, something }. Second, there is an important distinc-
tion between an individual concept and a set of concepts. For example, the individual
concept ?entity? should not be confused with the set or class consisting of concepts
denoting kinds of entities. To make this distinction clear, we use c = { c? | c? isa c }
to denote the set of concepts dominated by concept c, including c itself. For exam-
ple, ?animal? is the set consisting of those concepts corresponding to kinds of animals
(including ?animal? itself).
The probability of a concept appearing as an argument of a predicate is written p(c |
v, r), where c is a concept in WordNet, v is a predicate, and r is an argument position.3
The focus in this article is on the arguments of verbs, but the techniques discussed
can be applied to any predicate that takes nominal arguments, such as adjectives. The
probability p(c | v, r) is to be interpreted as follows: This is the probability that some
noun n in syn(c), when denoting concept c, appears in position r of verb v (given
v and r). The example used throughout the article is p(?dog? | run, subj), which is
the conditional probability that some noun in the synset of ?dog?, when denoting the
concept ?dog?, appears in the subject position of the verb run. Note that, in practice,
no distinction is made between the different senses of a verb (although the techniques
do allow such a distinction) and that each use of a noun is assumed to correspond to
exactly one concept.4
3. Class-Based Probability Estimation
This section explains how a set of concepts, or class, from WordNet can be used to
estimate the probability of an individual concept. More specifically, we explain how
a set of concepts c?, where c? is some hypernym of concept c, can be used to estimate
p(c | v, r). (Recall that c? denotes the set of concepts dominated by c?, including c? itself.)
One possible approach would be simply to substitute c? for the individual concept c.
This is a poor solution, however, since p(c? | v, r) is the conditional probability that
2 Angled brackets are used to denote concepts in the hierarchy.
3 The term predicate is used loosely here, in that the predicate does not have to be a semantic object but
can simply be a word form.
4 A recent paper that extends the acquisition of selectional preferences to sense-sense relationships is
Agirre and Martinez (2001).
190
Computational Linguistics Volume 28, Number 2
some noun denoting a concept in c? appears in position r of verb v. For example,
p(?animal? | run, subj) is the probability that some noun denoting a kind of animal
appears in the subject position of the verb run. Probabilities of sets of concepts are
obtained by summing over the concepts in the set:
p(c? | v, r) =
?
c???c?
p(c?? | v, r) (1)
This means that p(?animal? | run, subj) is likely to be much greater than p(?dog? |
run, subj) and thus is not a good approximation of p(?dog? | run, subj).
What can be done, though, is to condition on sets of concepts. If it can be shown
that p(v | c?, r), for some hypernym c? of c, is a reasonable approximation of p(v | c, r),
then we have a way of estimating p(c | v, r). The probability p(v | c, r) can be obtained
from p(c | v, r) using Bayes? theorem:
p(c | v, r) = p(v | c, r) p(c | r)
p(v | r) (2)
Since p(c | r) and p(v | r) are conditioned on the argument slot only, we assume
these can be estimated satisfactorily using relative frequency estimates. Alternatively,
a standard smoothing technique such as Good-Turing could be used.5 This leaves p(v |
c, r). Continuing with the ?dog? example, the proposal is to estimate p(run | ?dog?, subj)
using a relative-frequency estimate of p(run | ?animal?, subj) or an estimate based on a
similar, suitably chosen class. Thus, assuming this choice of class, p(?dog? | run, subj)
would be approximated as follows:
p(?dog? | run, subj) ? p(run | ?animal?, subj)p(?dog? | subj)
p(run | subj) (3)
The following derivation shows that if p(v | c?i , r) = k for each child c?i of c?, and
p(v | c?, r) = k, then p(v | c?, r) is also equal to k:
p(v | c?, r) = p(c? | v, r) p(v | r)
p(c? | r)
(4)
=
p(v | r)
p(c? | r)
(
?
i
p(c?i | v, r) + p(c
? | v, r)
)
(5)
=
p(v | r)
p(c? | r)
(
?
i
p(v | c?i , r)
p(c?i | r)
p(v | r) + p(v | c
?, r)
p(c? | r)
p(v | r)
)
(6)
=
1
p(c? | r)
(
?
i
k p(c?i | r) + k p(c
? | r)
)
(7)
=
k
p(c? | r)
(
?
i
p(c?i | r) + p(c
? | r)
)
(8)
= k (9)
5 Unsmoothed estimates were used in this work.
191
Clark and Weir Class-Based Probability Estimation
Note that the proof applies only to a tree, since the proof assumes that c? is partitioned
by c? and the sets of concepts dominated by each of the daughters of c?, which is not
necessarily true for a directed acyclic graph (DAG). WordNet is a DAG but is a close
approximation to a tree, and so we assume this will not be a problem in practice.6
The derivation in (4)?(9) shows how probabilities conditioned on sets of concepts
can remain constant when moving up the hierarchy, and this suggests a way of finding
a suitable set, c?, as a generalization for concept c: Initially set c? equal to c and move
up the hierarchy, changing the value of c?, until there is a significant change in p(v |
c?, r). Estimates of p(v | c?i , r), for each child c?i of c?, can be compared to see whether
p(v | c?, r) has significantly changed. (We ignore the probability p(v | c?, r) and consider
the probabilities p(v | c?i , r) only.) Note that this procedure rests on the assumption that
p(v | c, r) is close to p(v | c, r). (In fact, p(v | c, r) is equal to p(v | c, r) when c is a leaf
node.) So when finding a suitable level for the estimation of p(?sandwich? | eat, obj),
for example, we first assume that p(eat | ?sandwich?, obj) is a good approximation of
p(eat | ?sandwich?, obj) and then apply the procedure to p(eat | ?sandwich?, obj).
A feature of the proposed generalization procedure is that comparing probabilities
of the form p(v | C, r), where C is a class, is closely related to comparing ratios of
probabilities of the form p(C | v, r)/p(C | r) (for a given verb and argument position):
p(v | C, r) = p(C | v, r)
p(C | r) p(v | r) (10)
Note that, for a given verb and argument position, p(v | r) is constant across classes.
Equation (10) is of interest because the ratio p(C | v, r)/p(C | r) can be interpreted as a
measure of association between the verb v and class C. This ratio is similar to point-
wise mutual information (Church and Hanks 1990) and also forms part of Resnik?s
association score, which will be introduced in Section 6. Thus the generalization pro-
cedure can be thought of as one that finds ?homogeneous? areas of the hierarchy,
that is, areas consisting of classes that are associated to a similar degree with the verb
(Clark and Weir 1999).
Finally, we note that the proposed estimation method does not guarantee that the
estimates form a probability distribution over the concepts in the hierarchy, and so a
normalization factor is required:
psc(c | v, r) =
p?(v | [c, v, r], r) p?(c|r)p?(v|r)
?
c??C p?(v | [c?, v, r], r)
p?(c?|r)
p?(v|r)
(11)
We use psc to denote an estimate obtained using our method (since the technique
finds sets of semantically similar senses, or ?similarity classes?) and [c, v, r] to denote
the class chosen for concept c in position r of verb v; p? denotes a relative frequency
estimate, and C denotes the set of concepts in the hierarchy.
Before providing the details of the generalization procedure, we give the relative-
frequency estimates of the relevant probabilities and deal with the problem of am-
6 Li and Abe (1998) also develop a theoretical framework that applies only to a tree and turn WordNet
into a tree by copying each subgraph with multiple parents. One way to extend the experiments in
Section 7 would be to investigate whether this transformation has an impact on the results of those
experiments.
192
Computational Linguistics Volume 28, Number 2
biguous data. The relative-frequency estimates are as follows:
p?(c | r) = f (c,r)f (r) =
?
v??V f (c, v
?, r)
?
v??V
?
c??C f (c
?, v?, r)
(12)
p?(v | r) = f (v,r)f (r) =
?
c??C f (c
?, v, r)
?
v??V
?
c??C f (c
?, v?, r)
(13)
p?(v | c?, r) = f (c?,v,r)
f (c?,r)
=
?
c???c? f (c
??, v, r)
?
v??V
?
c???c? f (c
??, v?, r)
(14)
where f (c, v, r) is the number of (n, v, r) triples in the data in which n is being used to
denote c, and V is the set of verbs in the data. The problem is that the estimates are
defined in terms of frequencies of senses, whereas the data are assumed to be in the
form of (n, v, r) triples: a noun, verb, and argument position. All the data used in this
work have been obtained from the British National Corpus (BNC), using the system
of Briscoe and Carroll (1997), which consists of a shallow-parsing component that is
able to identify verbal arguments.
We take a simple approach to the problem of estimating the frequencies of senses,
by distributing the count for each noun in the data evenly among all senses of the
noun:
f? (c, v, r) =
?
n?syn(c)
f (n, v, r)
|cn(n)| (15)
where f? (c, v, r) is an estimate of the number of times that concept c appears in position
r of verb v, and |cn(n)| is the cardinality of cn(n). This is the approach taken by
Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how
this apparently crude technique works surprisingly well. Alternative approaches are
described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999),
and Ciaramita and Johnson (2000).
4. Using a Chi-Square Test to Compare Probabilities
In this section we show how to test whether p(v | c?, r) changes significantly when
considering a node higher in the hierarchy. Consider the problem of deciding whether
p(run | ?canine?, subj) is a good approximation of p(run | ?dog?, subj). (?canine? is the
parent of ?dog? in WordNet.) To do this, the probabilities p(run | c?i , subj) are compared
using a chi-square test, where the c?i are the children of ?canine?. In this case, the null
hypothesis of the test is that the probabilities p(run | ci, subj) are the same for each
child ci. By judging the strength of the evidence against the null hypothesis, how
similar the true probabilities are likely to be can be determined. If the test indicates
that the probabilities are sufficiently unlikely to be the same, then the null hypothesis
is rejected, and the conclusion is that p(run | ?canine?, subj) is not a good approximation
of p(run | ?dog?, subj).
An example contingency table, based on counts obtained from a subset of the BNC
using the system of Briscoe and Carroll, is given in Table 1. (Recall that the frequencies
are estimated by distributing the count for a noun equally among the noun?s senses;
this explains the fractional counts.) One column contains estimates of counts arising
7 Resnik takes a similar approach but divides the count evenly among the noun?s senses and all the
hypernyms of those senses.
193
Clark and Weir Class-Based Probability Estimation
Table 1
Contingency table for the children of ?canine? in the subject position of run.
ci f?(ci, run, subj) f?(ci, subj) ? f?(ci, run, subj) f?(ci, subj) =
?
v?V f?(ci, v, subj)
?bitch? 0.3 (0.5) 26.7 (26.6) 27.0
?dog? 12.8 (10.5) 620.4 (622.7) 633.2
?wolf? 0.3 (0.6) 38.7 (38.4) 39.0
?jackal? 0.0 (0.3) 20.0 (19.7) 20.0
?wild dog? 0.0 (0.0) 3.0 (3.0) 3.0
?hyena? 0.0 (0.2) 10.0 (9.8) 10.0
?fox? 0.0 (1.2) 72.3 (71.1) 72.3
13.4 791.1 804.5
from concepts in ci appearing in the subject position of the verb run: f? (ci, run, subj). A
second column presents estimates of counts arising from concepts in ci appearing in
the subject position of a verb other than run. The figures in brackets are the expected
values if the null hypothesis is true.
There is a choice of which statistic to use in conjunction with the chi-square test.
The usual statistic encountered in textbooks is the Pearson chi-square statistic, de-
noted X2:
X2 =
?
i,j
(oij ? eij)2
eij
(16)
where oij is the observed value for the cell in row i and column j, and eij is the
corresponding expected value. An alternative statistic is the log-likelihood chi-square
statistic, denoted G2:8
G2 = 2
?
i,j
oij loge
oij
eij
(17)
The two statistics have similar values when the counts in the contingency table are
large (Agresti 1996). The statistics behave differently, however, when the table contains
low counts, and, since corpus data are likely to lead to some low counts, the question
of which statistic to use is an important one. Dunning (1993) argues for the use of G2
rather than X2, based on an analysis of the sampling distributions of G2 and X2, and
results obtained when using the statistics to acquire highly associated bigrams. We
consider Dunning?s analysis at the end of this section, and the question of whether to
use G2 or X2 will be discussed further there. For now, we continue with the discussion
of how the chi-square test is used in the generalization procedure.
For Table 1, the value of G2 is 3.8, and the value of X2 is 2.5. Assuming a level of
significance of ? = 0.05, the critical value is 12.6 (for six degrees of freedom). Thus,
for this ? value, the null hypothesis would not be rejected for either statistic, and the
conclusion would be that there is no reason to suppose that p(run | ?canine?, subj) is
not a reasonable approximation of p(run | ?dog?, subj).
8 An alternative formula for G2 is given in Dunning (1993), but the two are equivalent.
194
Computational Linguistics Volume 28, Number 2
Table 2
Contingency table for the children of ?liquid? in the object position of drink.
ci f?(ci, drink, obj) f?(ci, obj) ? f?(ci, drink, obj) f?(ci, obj) =
?
v?V f?(ci, v, obj)
?beverage? 261.0 (238.7) 2,367.7 (2,390.0) 2,628.7
?supernatant? 0.0 (0.1) 1.0 (0.9) 1.0
?alcohol? 11.5 (9.4) 92.0 (94.1) 103.5
?ammonia? 0.0 (0.8) 8.5 (7.7) 8.5
?antifreeze? 0.0 (0.1) 1.0 (0.9) 1.0
?distillate? 0.0 (0.5) 6.0 (5.5) 6.0
?water? 12.0 (31.6) 335.7 (316.1) 347.7
?ink? 0.0 (2.9) 32.0 (29.1) 32.0
?liquor? 0.7 (1.1) 11.6 (11.2) 12.3
285.2 2,855.5 3,140.7
As a further example, Table 2 gives counts for the children of ?liquid? in the object
position of drink. Again, the counts have been obtained from a subset of the BNC
using the system of Briscoe and Carroll. Not all the sets dominated by the children of
?liquid? are shown, as some, such as ?sheep dip?, never appear in the object position
of a verb in the data. This example is designed to show a case in which the null
hypothesis is rejected. The value of G2 for this table is 29.0, and the value of X2 is
21.2. So for G2, even if an ? value as low as 0.0005 were being used (for which the
critical value is 27.9 for eight degrees of freedom), the null hypothesis would still be
rejected. For X2, the null hypothesis is rejected for ? values greater than 0.005. This
seems reasonable, since the probabilities associated with the children of ?liquid? and
the object position of drink would be expected to show a lot of variation across the
children.
A key question is how to select the appropriate value for ?. One solution is to
treat ? as a parameter and set it empirically by taking a held-out test set and choosing
the value of ? that maximizes performance on the relevant task. For example, Clark
and Weir (2000) describes a prepositional phrase attachment algorithm that employs
probability estimates obtained using the WordNet method described here. To set the
value of ?, the performance of the algorithm on a development set could be com-
pared across different values of ?, and the value that leads to the best performance
could be chosen. Note that this approach sets no constraints on the value of ?: The
value could be as high as 0.995 or as low as 0.0005, depending on the particular
application.
There may be cases in which the conditions for the appropriate application of a chi-
square test are not met. One condition that is likely to be violated is the requirement
that expected values in the contingency table not be too small. (A rule of thumb
often found in textbooks is that the expected values should be greater than five.) One
response to this problem is to apply some kind of thresholding and either ignore
counts below the threshold, or apply the test only to tables that do not contain low
counts. Ribas (1995), Li and Abe (1998), McCarthy (2000), and Wagner (2000) all use
some kind of thresholding when dealing with counts in the hierarchy (although not in
the context of a chi-square test). Another approach would be to use Fisher?s exact test
(Agresti 1996; Pedersen 1996), which can be applied to tables regardless of the size of
195
Clark and Weir Class-Based Probability Estimation
the counts they contain. The main problem with this test is that it is computationally
expensive, especially for large contingency tables.
What we have found in practice is that applying the chi-square test to tables dom-
inated by low counts tends to produce an insignificant result, and the null hypothesis
is not rejected. The consequences of this for the generalization procedure are that
low-count tables tend to result in the procedure moving up to the next node in the
hierarchy. But given that the purpose of the generalization is to overcome the sparse-
data problem, moving up a node is desirable, and therefore we do not modify the test
for tables with low counts.
The final issue to consider is which chi-square statistic to use. Dunning (1993)
argues for the use of G2 rather than X2, based on the claim that the sampling distri-
bution of G2 approaches the true chi-square distribution quicker than the sampling
distribution of X2. However, Agresti (1996, page 34) makes the opposite claim: ?The
sampling distributions of X2 and G2 get closer to chi-squared as the sample size n
increases. . . . The convergence is quicker for X2 than G2.?
In addition, Pedersen (2001) questions whether one statistic should be preferred
over the other for the bigram acquisition task and cites Cressie and Read (1984), who
argue that there are some cases where the Pearson statistic is more reliable than the
log-likelihood statistic. Finally, the results of the pseudo-disambiguation experiments
presented in Section 7 are at least as good, if not better, when using X2 rather than G2,
and so we conclude that the question of which statistic to use should be answered on
a per application basis.
5. The Generalization Procedure
The procedure for finding a suitable class, c?, to generalize concept c in position r
of verb v works as follows. (We refer to c? as the ?similarity class? of c with respect
to v and r and the hypernym c? as top(c, v, r), since the chosen hypernym sits at the
?top? of the similarity class.) Initially, concept c is assigned to a variable top. Then,
by working up the hierarchy, successive hypernyms of c are assigned to top, and this
process continues until the probabilities associated with the sets of concepts dominated
by top and the siblings of top are significantly different. Once a node is reached that
results in a significant result for the chi-square test, the procedure stops, and top is
returned as top(c, v, r). In cases where a concept has more than one parent, the parent
is chosen that results in the lowest value of the chi-square statistic, as this indicates
the probabilities are the most similar. The set top(c, v, r) is the similarity class of c for
verb v and position r. Figure 1 gives an algorithm for determining top(c, v, r).
Figure 2 gives an example of the procedure at work. Here, top(?soup?, stir, obj) is
being determined. The example is based on data from a subset of the BNC, with 303
cases of an argument in the object position of stir. The G2 statistic is used, together with
an ? value of 0.05. Initially, top is set to ?soup?, and the probabilities corresponding
to the children of ?dish? are compared: p(stir | ?soup?, obj), p(stir | ?lasagne?, obj), p(stir |
?haggis?, obj), and so on for the rest of the children. The chi-square test results in a G2
value of 14.5, compared to a critical value of 55.8. Since G2 is less than the critical value,
the procedure moves up to the next node. This process continues until a significant
result is obtained, which first occurs at ?substance? when comparing the children of
?object?. Thus ?substance? is the chosen level of generalization.
Now we show how the chosen level of generalization varies with ? and how it
varies with the size of the data set. A note of clarification is required before presenting
the results. In related work on acquiring selectional preferences (Ribas 1995; McCarthy
196
Computational Linguistics Volume 28, Number 2
Algorithm top(c, v, r):
top ? c
sig result ? false
comment parentmin gives lowest G2 value, G2min
while not sig result & top = ?root? do
G2min ? ?
for all parents of top do
calculate G2 for sets dominated by children of parent
if G2 < G2min
then G2min ? G2
parentmin ? parent
end
if chi-square test for parentmin is significant
then sig result ? true
else move up to next node: top ? parentmin
end
return top
Figure 1
An algorithm for determining top(c, v, r).
haggislasagne
dish
nourishment
food
fare beverage
coursemeal
substance
object
uid poison
artifactground
entity
soup
G
2
: 14:5, critical value: 55:8
G
2
: 5:4, crit val: 16:9
G
2
: 5:5, crit val: 16:9
G
2
: 29:9, crit val: 58:1
G
2
: 141:1, crit val: 37:7
Figure 2
An example generalization: Determining top(?soup?, stir, obj).
197
Clark and Weir Class-Based Probability Estimation
1997; Li and Abe 1998; Wagner 2000), the level of generalization is often determined for
a small number of hand-picked verbs and the result compared with the researcher?s
intuition about the most appropriate level for representing a selectional preference.
According to this approach, if ?sandwich? were chosen to represent ?hotdog? in the
object position of eat, this might be considered an undergeneralization, since ?food?
might be considered more appropriate. For this work we argue that such an evaluation
is not appropriate; since the purpose of this work is probability estimation, the most
appropriate level is the one that leads to the most accurate estimate, and this may or
may not agree with intuition. Furthermore, we show in Section 7 that to generalize
unnecessarily can be harmful for some tasks: If we already have lots of data regarding
?sandwich?, why generalize any higher? Thus the purpose of this section is not to show
that the acquired levels are ?correct,? but simply to show how the levels vary with ?
and the sample size.
To show how the level of generalization varies with changes in ?, top(c, v, obj)
was determined for a number of hand-picked (c, v, obj) triples over a range of values
for ?. The triples were chosen to give a range of strongly and weakly selecting verbs
and a range of verb frequencies. The data were again extracted from a subset of the
BNC using the system of Briscoe and Carroll (1997), and the G2 statistic was used in
the chi-square test. The results are shown in Table 3. The number of times the verb
occurred with some object is also given in the table.
The results suggest that the generalization level becomes more specific as ? in-
creases. This is to be expected, since, given a contingency table chosen at random, a
higher value of ? is more likely to lead to a significant result than a lower value of ?.
We also see that, for some cases, the value of ? has little effect on the level. We would
expect there to be less change in the level of generalization for strongly selecting verbs,
such as drink and eat, and a greater range of levels for weakly selecting verbs such
as see. This is because any significant difference in probabilities is likely to be more
marked for a strongly selecting verb, and likely to be significant over a wider range
of ? values. The table only provides anecdotal evidence, but provides some support
to this argument.
To investigate more generally how the level of generalization varies with changes
in ?, and also with changes in sample size, we took 6, 000 (c, v, obj) triples and calcu-
lated the difference in depth between c and top(c, v, r) for each triple. The 6, 000 triples
were taken from the first experimental test set described in Section 7, and the train-
ing data from this experiment were used to provide the counts. (The test set contains
nouns, rather than noun senses, and so the sense of the noun that is most probable
given the verb and object slot was used.) An average difference in depth was then
calculated. To give an example of how the difference in depth was calculated, sup-
pose ?dog? generalized to ?placental mammal? via ?canine? and ?carnivore?; in this case
the difference would be three.
The results for various levels of ? and different sample sizes are shown in Table 4.
The figures in each column arise from using the contingency tables based on the
complete training data, but with each count in the table multiplied by the percentage
at the head of the column. Thus the 50% column is based on contingency tables in
which each original count is multiplied by 50%, which is equivalent to using a sample
one-half the size of the original training set. Reading across a row shows how the
generalization varies with sample size, and reading down a column shows how it
varies with ?. The results show clearly that the extent of generalization decreases
with an increase in the value of ?, supporting the trend observed in Table 3. The
results also show that the extent of generalization increases with a decrease in sample
198
Computational Linguistics Volume 28, Number 2
Table 3
Example levels of generalization for different values of ?.
(c, v, r), f(v, r) ?
(?coffee?, drink, obj) 0.0005 ?coffee??BEVERAGE??food? . . . ?object??entity?
0.05 ?coffee??BEVERAGE??food? . . . ?object??entity?
f (drink, obj) = 849 0.5 ?coffee??BEVERAGE??food? . . . ?object??entity?
0.995 ?coffee??BEVERAGE??food? . . . ?object??entity?
(?hotdog?, eat, obj) 0.0005 ?hotdog??sandwich??snack food??DISH? . . . ?food? . . . ?entity?
0.05 ?hotdog??sandwich??snack food??DISH? . . . ?food? . . . ?entity?
f (eat, obj) = 1,703 0.5 ?hotdog??sandwich??snack food??DISH? . . . ?food? . . . ?entity?
0.995 ?hotdog??SANDWICH??snack food??dish? . . . ?food? . . . ?entity?
(?Socrates?, kiss, obj) 0.0005 ?Socrates? . . . ?person??life form??CAUSAL AGENT??entity?
0.05 ?Socrates? . . . ?person??life form??CAUSAL AGENT??entity?
f (kiss, obj) = 345 0.5 ?Socrates? . . . ?person??life form??CAUSAL AGENT??entity?
0.995 ?Socrates? . . . ?PERSON??life form??causal agent??entity?
(?dream?, remember, obj) 0.0005 ?dream? . . . ?preoccupation??cognitive state??STATE?
0.05 ?dream? . . . ?preoccupation??cognitive state??STATE?
f (remember, obj) = 1,982 0.5 ?dream? . . . ?preoccupation??COGNITIVE STATE??state?
0.995 ?dream? . . . ?PREOCCUPATION??cognitive state??state?
(?man?, see, obj) 0.0005 ?man? . . . ?mammal? . . . ?ANIMAL??life form??entity?
0.05 ?man? . . . ?MAMMAL? . . . ?animal??life form??entity?
f (see, obj) = 16,757 0.5 ?man? . . . ?MAMMAL? . . . ?animal??life form??entity?
0.995 ?MAN? . . . ?mammal? . . . ?animal??life form??entity?
(?belief?, abandon, obj) 0.0005 ?belief??mental object??cognition??PSYCHOLOGICAL FEATURE?
0.05 ?belief??MENTAL OBJECT??cognition??psychological feature?
f (abandon, obj) = 673 0.5 ?BELIEF??mental object??cognition??psychological feature?
0.995 ?BELIEF??mental object??cognition??psychological feature?
(?nightmare?, have, obj) 0.0005 ?nightmare??dreaming??IMAGINATION? . . . ?psychological feature?
0.05 ?nightmare??dreaming??IMAGINATION? . . . ?psychological feature?
f (have, obj) = 93,683 0.5 ?nightmare??DREAMING??imagination? . . . ?psychological feature?
0.995 ?nightmare??DREAMING??imagination? . . . ?psychological feature?
Note: The selected level is shown in upper case.
Table 4
Extent of generalization for different values of ? and sample sizes.
? 100% 50% 10% 1%
0.0005 3.3 3.9 5.0 5.6
0.05 2.8 3.5 4.6 5.6
0.5 2.1 2.9 4.1 5.4
0.995 1.2 1.5 2.6 3.9
size. Again, this is to be expected, since any difference in probability estimates is less
likely to be significant for tables with low counts.
6. Alternative Class-Based Estimation Methods
The approaches used for comparison are that of Resnik (1993, 1998), subsequently
developed by Ribas (1995), and that of Li and Abe (1998), which has been adopted by
McCarthy (2000). These have been chosen because they directly address the question
of how to find a suitable level of generalization in WordNet.
199
Clark and Weir Class-Based Probability Estimation
The first alternative uses the ?association score,? which is a measure of how well
a set of concepts, C, satisfies the selectional preferences of a verb, v, for an argument
position, r:9
A(C, v, r) = p(C | v, r) log2
p(C | v, r)
p(C | r) (18)
An estimate of the association score, A?(C, v, r), can be obtained using relative frequency
estimates of the probabilities. The key question is how to determine a suitable level of
generalization for concept c, or, alternatively, how to find a suitable class to represent
concept c (assuming the choice is from those classes that contain all concepts dom-
inated by some hypernym of c). Resnik?s solution to this problem (which he neatly
refers to as the ?vertical-ambiguity? problem) is to choose the class that maximizes
the association score.
It is not clear that the class with the highest association score is always the most
appropriate level of generalization. For example, this approach does not always gen-
eralize appropriately for arguments that are negatively associated with some verb. To
see why, consider the problem of deciding how well the concept ?location? satisfies the
preferences of the verb eat for its object. Since locations are not the kinds of things that
are typically eaten, a suitable level of generalization would correspond to a class that
has a low association score with respect to eat. However, ?location? is a kind of ?entity?
in WordNet,10 and choosing the class with the highest association score is likely to
produce ?entity? as the chosen class. This is a problem, because the association score
of ?entity? with respect to eat may be too high to reflect the fact that ?location? is a very
unlikely object of the verb.
Note that the solution to the vertical-ambiguity problem presented in the previous
sections is able to generalize appropriately in such cases. Continuing with the eat
?location? example, our generalization procedure is unlikely to get as high as ?entity?
(assuming a reasonable number of examples of eat in the training data), since the
probabilities corresponding to the daughters of ?entity? are likely to be very different
with respect to the object position of eat.
The second alternative uses the minimum description length (MDL) principle.
Li and Abe use MDL to select a set of classes from a hierarchy, together with their
associated probabilities, to represent the selectional preferences of a particular verb.
The preferences and class-based probabilities are then used to estimate probabilities
of the form p(n | v, r), where n is a noun, v is a verb, and r is an argument slot.
Li and Abe?s application of MDL requires the hierarchy to be in the form of a
thesaurus, in which each leaf node represents a noun and internal nodes represent the
class of nouns that the node dominates. The hierarchy is also assumed to be in the
form of a tree. The class-based models consist of a partition of the set of nouns (leaf
nodes) and a probability associated with each class in the partition. The probabilities
are the conditional probabilities of each class, given the relevant verb and argument
position. Li and Abe refer to such a partition as a ?cut? and the cut together with the
probabilities as a ?tree cut model.? The probabilities of the classes in a cut, ?, satisfy
the following constraint:
?
C??
p(C | v, r) = 1 (19)
9 The definition used here is that given by Ribas (1995).
10 For example, the hypernyms of the concept ?Dallas? are as follows: ?city?, ?municipality?,
?urban area?, ?geographical area?, ?region?, ?location?, ?object?, ?entity?.
200
Computational Linguistics Volume 28, Number 2
<abstraction>
<life_form>
<plant>
<object>
<entity>
<substance>
<set>
<root>
<mushroom>
<artifact>
<rope>
<food>
<pizza><lobster>
<fluid><solid>
<animal>
<lobster>
<time><space>
Figure 3
Possible cut returned by MDL.
In order to determine the probability of a noun, the probability of a class is assumed
to be distributed uniformly among the members of that class:
p(n | v, r) = 1|C| p(C | v, r) for all n ? C (20)
Since WordNet is a hierarchy with noun senses, rather than nouns, at the nodes,
Li and Abe deal with the issue of word sense ambiguity using the method described
in Section 3, by dividing the count for a noun equally among the concepts whose
synsets contain the noun. Also, since WordNet is a DAG, Li and Abe turn WordNet
into a tree by copying each subgraph with multiple parents. And so that each noun
in the data appears (in a synset) at a leaf node, Li and Abe remove those parts of the
hierarchy dominated by a noun in the data (but only for that instance of WordNet
corresponding to the relevant verb).
An example cut showing part of the WordNet hierarchy is shown in Figure 3 (based
on an example from Li and Abe [1998]; the dashed lines indicate parts of the hierarchy
that are not shown in the diagram). This is a possible cut for the object position of the
verb eat, and the cut consists of the following classes: ?life form?, ?solid?, ?fluid?, ?food?,
?artifact?, ?space?, ?time?, ?set?. (The particular choice of classes for the cut in this example
is not too important; the example is designed to show how probabilities of senses are
estimated from class probabilities.) Since the class in the cut containing ?pizza? is ?food?,
the probability p(?pizza? | eat, obj) would be estimated as p(?food? | eat, obj)/|?food?|.
Similarly, since the class in the cut containing ?mushroom? is ?life form?, the probability
p(?mushroom? | eat, obj) would be estimated as p(?life form? | eat, obj)/|?life form?|.
The uniform-distribution assumption (20) means that cuts close to the root of the
hierarchy result in a greater smoothing of the probability estimates than cuts near the
leaves. Thus there is a trade-off between choosing a model that has a cut near the
leaves, which is likely to overfit the data, and a more general (simple) model near the
root, which is likely to underfit the data. MDL looks ideally suited to the task of model
selection, since it is designed to deal with precisely this trade-off. The simplicity of a
model is measured using the model description length, which is an information-theoretic
201
Clark and Weir Class-Based Probability Estimation
term and denotes the number of bits required to encode the model. The fit to the data
is measured using the data description length, which is the number of bits required to
encode the data (relative to the model). The overall description length is the sum of
the model description length and the data description length, and the MDL principle
is to select the model with the shortest description length.
We used McCarthy?s (2000) implementation of MDL. So that every noun is repre-
sented at a leaf node, McCarthy does not remove parts of the hierarchy, as Li and Abe
do, but instead creates new leaf nodes for each synset at an internal node. McCarthy
also does not transform WordNet into a tree, which is strictly required for Li and
Abe?s application of MDL. This did create a problem with overgeneralization: Many
of the cuts returned by MDL were overgeneralizing at the ?entity? node. The reason
is that ?person?, which is close to ?entity? and dominated by ?entity?, has two parents:
?life form? and ?causal agent?. This DAG-like property was responsible for the over-
generalization, and so we removed the link between ?person? and ?causal agent?. This
appeared to solve the problem, and the results presented later for the average degree
of generalization do not show an overgeneralization compared with those given in Li
and Abe (1998).
7. Pseudo-Disambiguation Experiments
The task we used to compare the class-based estimation techniques is a decision task
previously used by Pereira, Tishby, and Lee (1993) and Rooth et al (1999). The task is
to decide which of two verbs, v and v?, is more likely to take a given noun, n, as an
object. The test and training data were obtained as follows. A number of verb?direct
object pairs were extracted from a subset of the BNC, using the system of Briscoe and
Carroll. All those pairs containing a noun not in WordNet were removed, and each
verb and argument was lemmatized. This resulted in a data set of around 1.3 million
(v, n) pairs.
To form a test set, 3,000 of these pairs were randomly selected such that each
selected pair contained a fairly frequent verb. (Following Pereira, Tishby, and Lee, only
those verbs that occurred between 500 and 5,000 times in the data were considered.)
Each instance of a selected pair was then deleted from the data to ensure that the test
data were unseen. The remaining pairs formed the training data. To complete the test
set, a further fairly frequent verb, v?, was randomly chosen for each (v, n) pair. The
random choice was made according to the verb?s frequency in the original data set,
subject to the condition that the pair (v?, n) did not occur in the training data. Given
the set of (v, n, v?) triples, the task is to decide whether (v, n) or (v?, n) is the correct
pair.11
We acknowledge that the task is somewhat artificial, but pseudo-disambiguation
tasks of this kind are becoming popular in statistical NLP because of the ease with
which training and test data can be created. We also feel that the pseudo-disambig-
uation task is useful for evaluating the different estimation methods, since it directly
addresses the question of how likely a particular predicate is to take a given noun as
an argument. An evaluation using a PP attachment task was attempted in Clark and
Weir (2000), but the evaluation was limited by the relatively small size of the Penn
Treebank.
11 We note that this procedure does not guarantee that the correct pair is more likely than the incorrect
pair, because of noise in the data from the parser and also because a highly plausible incorrect pair
could be generated by chance.
202
Computational Linguistics Volume 28, Number 2
Table 5
Results for the pseudo-disambiguation task.
Generalization technique % correct av.gen. sd.gen.
Similarity class
? = 0.0005 73.8 3.3 2.0
? = 0.05 73.4 2.8 1.9
? = 0.3 73.0 2.4 1.8
? = 0.75 73.9 1.9 1.6
? = 0.995 73.8 1.2 1.2
Low class 73.6 0.9 1.0
MDL 68.3 4.1 1.9
Assoc 63.9 4.2 2.1
Note: av.gen. is the average number of generalized levels;
sd.gen. is the standard deviation.
Using our approach, the disambiguation decision for each (v, n, v?) triple was made
according to the following procedure:
if max
c?cn(n)
psc(c | v, obj) > max
c?cn(n)
psc(c | v?, obj)
then choose (v, n)
else if max
c?cn(n)
psc(c | v?, obj) > max
c?cn(n)
psc(c | v, obj)
then choose (v?, n)
else choose at random
If n has more than one sense, the sense is chosen that maximizes the relevant prob-
ability estimate; this explains the maximization over cn(n). The probability estimates
were obtained using our class-based method, and the G2 statistic was used for the
chi-square test. This procedure was also used for the MDL alternative, but using the
MDL method to estimate the probabilities.
Using the association score for each test triple, the decision was made according
to the following procedure:
if max
c?cn(n)
max
c??h(c)
A?(c?, v, obj) > max
c?cn(n)
max
c??h(c)
A?(c?, v?, obj)
then choose (v, n)
else if max
c?cn(n)
max
c??h(c)
A?(c?, v?, obj) > max
c?cn(n)
max
c??h(c)
A?(c?, v, obj)
then choose (v?, n)
else choose at random
We use h(c) to denote the set consisting of the hypernyms of c. The inner maximization
is over h(c), assuming c is the chosen sense of n, which corresponds to Resnik?s method
of choosing a set to represent c. The outer maximization is over the senses of n, cn(n),
which determines the sense of n by choosing the sense that maximizes the association
score.
The first set of results is given in Table 5. Our technique is referred to as the
?similarity class? technique, and the approach using the association score is referred
203
Clark and Weir Class-Based Probability Estimation
Table 6
Results for the pseudo-disambiguation task with one-fifth training data.
Generalization technique % correct av.gen. sd.gen.
Similarity class
? = 0.0005 66.7 4.5 1.9
? = 0.05 68.4 4.1 1.9
? = 0.3 70.2 3.7 1.9
? = 0.75 72.3 3.0 1.9
? = 0.995 72.4 1.9 1.6
Low class 71.9 1.1 1.1
MDL 62.9 4.7 1.9
Assoc 62.6 4.1 2.0
Note: av.gen. is the average number of generalized levels;
sd.gen. is the standard deviation.
to as ?Assoc.? The results are given for a range of ? values and demonstrate clearly that
the performance of similarity class varies little with changes in ? and that similarity
class outperforms both MDL and Assoc.12
We also give a score for our approach using a simple generalization procedure,
which we call ?low class.? The procedure is to select the first class that has a count
greater than zero (relative to the verb and argument position), which is likely to return
a low level of generalization, on the whole. The results show that our generalization
technique only narrowly outperforms the simple alternative. Note that, although low
class is based on a very simple generalization method, the estimation method is still
using our class-based technique, by applying Bayes? theorem and conditioning on a
class, as described in Section 3; the difference is in how the class is chosen.
To investigate the results, we calculated the average number of generalized levels
for each approach. The number of generalized levels for a concept c (relative to a
verb v and argument position r) is the difference in depth between c and top(c, v, r),
as explained in Section 5. For each test case, the number of generalized levels for
both verbs, v and v?, was calculated, but only for the chosen sense of n. The results
are given in the third column of Table 5 and demonstrate clearly that both MDL and
Assoc are generalizing to a greater extent than similarity class. (The fourth column
gives a standard deviation figure.) These results suggest that MDL and Assoc are
overgeneralizing, at least for the purposes of this task.
To investigate why the value for ? had no impact on the results, we repeated the
experiment, but with one fifth of the data. A new data set was created by taking every
fifth pair of the original 1.3 million pairs. A test set of 3,000 triples was created from
this new data set, as before, but this time only verbs that occurred between 100 and
1,000 times were considered. The results using these test and training data are given
in Table 6.
These results show a variation in performance across values for ?, with an opti-
mal performance when ? is around 0.75. (Of course, in practice, the value for ? would
need to be optimized on a held-out set.) But even with this variation, similarity class is
still outperforming MDL and Assoc across the whole range of ? values. Note that the
12 The results given for similarity class are different from those given in Clark and Weir (2001) because
the probability estimates used in Clark and Weir (2001) were not normalized.
204
Computational Linguistics Volume 28, Number 2
Table 7
Disambiguation results for G2 and X2.
? value % correct (G2) % correct (X2)
0.0005 73.8 (3.3) 74.1 (3.0)
0.05 73.4 (2.8) 73.8 (2.5)
0.3 73.0 (2.4) 74.1 (2.2)
0.75 73.9 (1.9) 74.3 (1.8)
0.995 73.8 (1.2) 73.3 (1.2)
? values corresponding to the lowest scores lead to a significant amount of general-
ization, which provides additional evidence that MDL and Assoc are overgeneralizing
for this task. The low-class method scores highly for this data set alo, but given that
the task is one that apparently favors a low level of generalization, the high score is
not too surprising.
As a final experiment, we compared the task performance using the X2, rather than
G2, statistic in the chi-square test. The results are given in Table 7 for the complete
data set.13 The figures in brackets give the average number of generalized levels.
The X2 statistic is performing at least as well as G2, and the results show that the
average level of generalization is slightly higher for G2 than X2. This suggests a possible
explanation for the results presented here and those in Dunning (1993): that the X2
statistic provides a less conservative test when counts in the contingency table are
low. (By a conservative test we mean one in which the null hypothesis is not easily
rejected.) A less conservative test is better suited to the pseudo-disambiguation task,
since it results in a lower level of generalization, on the whole, which is good for this
task. In contrast, the task that Dunning considers, the discovery of bigrams, is better
served by a more conservative test.
8. Conclusion
We have presented a class-based estimation method that incorporates a procedure for
finding a suitable level of generalization in WordNet. This method has been shown to
provide superior performance on a pseudo-disambiguation task, compared with two
alternative approaches. An analysis of the results has shown that the other approaches
appear to be overgeneralizing, at least for this task. One of the features of the gener-
alization procedure is the way that ?, the level of significance in the chi-square test,
is treated as a parameter. This allows some control over the extent of generalization,
which can be tailored to particular tasks. We have also shown that the task perfor-
mance is at least as good when using the Pearson chi-square statistic as when using
the log-likelihood chi-square statistic.
There are a number of ways in which this work could be extended. One possibility
would be to use all the classes dominated by the hypernyms of a concept, rather than
just one, to estimate the probability of the concept. An estimate would be obtained for
each hypernym, and the estimates combined in a linear interpolation. An approach
similar to this is taken by Bikel (2000), in the context of statistical parsing.
There is still room for investigation of the hidden-data problem when data are used
that have not been sense disambiguated. In this article, a very simple approach is taken,
13 ?2 performed slightly better than G2 using the smaller data set alo.
205
Clark and Weir Class-Based Probability Estimation
which is to split the count for a noun evenly among the noun?s senses. Abney and Light
(1999) have tried a more motivated approach, using the expectation maximization
algorithm, but with little success. The approach described in Clark and Weir (1999) is
shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but
only with certain values of the ? parameter, and ultimately does not improve on the
best performance.
Finally, an issue that has not been much addressed in the literature (except by
Li and Abe [1996]) is how the accuracy of class-based estimation techniques compare
when automatically acquired classes, as opposed to the manually created classes from
WordNet, are used. The pseudo-disambiguation task described here has also been used
to evaluate clustering algorithms (Pereira, Tishby, and Lee, 1993; Rooth et al, 1999),
but with different data, and so it is difficult to compare the results. A related issue
is how the structure of WordNet affects the accuracy of the probability estimates. We
have taken the structure of the hierarchy for granted, without any analysis, but it may
be that an alternative design could be more conducive to probability estimation.
Acknowledgments
This article is an extended and updated
version of a paper that appeared in the
proceedings of NAACL 2001. The work on
which it is based was carried out while the
first author was a D.Phil. student at the
University of Sussex and was supported by
an EPSRC studentship. We would like to
thank Diana McCarthy for suggesting the
pseudo-disambiguation task and providing
the MDL software, John Carroll for
supplying the data, and Ted Briscoe, Geoff
Sampson, Gerald Gazdar, Bill Keller, Ted
Pedersen, and the anonymous reviewers for
their helpful comments. We would also like
to thank Ted Briscoe for presenting an
earlier version of this article on our behalf
at NAACL 2001.
References
Abney, Steven P. and Marc Light. 1999.
Hiding a semantic hierarchy in a Markov
model. In Proceedings of the ACL Workshop
on Unsupervised Learning in Natural
Language Processing, University of
Maryland, College Park, pages 1?8.
Agirre, Eneko and David Martinez. 2001.
Learning class-to-class selectional
preferences. In Proceedings of the Fifth ACL
Workshop on Computational Language
Learning, Toulouse, France, pages 15?22.
Agresti, Alan. 1996. An Introduction to
Categorical Data Analysis. Wiley.
Bikel, Daniel M. 2000. A statistical model
for parsing and word-sense
disambiguation. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large
Corpora, pages 155?163, Hong Kong.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the Fifth
ACL Conference on Applied Natural Language
Processing, pages 356?363, Washington,
DC.
Church, Kenneth W. and Patrick Hanks.
1990. Word association norms, mutual
information, and lexicography.
Computational Linguistics, 16(1):22?29.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference with
Bayesian networks. In Proceedings of the
18th International Conference on
Computational Linguistics, pages 187?193,
Saarbrucken, Germany.
Clark, Stephen. 2001. Class-Based Statistical
Models for Lexical Knowledge Acquisition.
Ph.D. dissertation, University of Sussex.
Clark, Stephen and David Weir. 1999. An
iterative approach to estimating
frequencies over a semantic hierarchy. In
Proceedings of the Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages
258?265, University of Maryland, College
Park.
Clark, Stephen and David Weir. 2000. A
class-based probabilistic approach to
structural disambiguation. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 194?200,
Saarbrucken, Germany.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
Second Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh.
Cressie, Noel A. C. and Timothy R. C. Read.
1984. Multinomial goodness of fit tests.
206
Computational Linguistics Volume 28, Number 2
Journal of the Royal Statistics Society Series B,
46:440?464.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press.
Li, Hang and Naoki Abe. 1996. Clustering
words with the MDL principle. In
Proceedings of the 16th International
Conference on Computational Linguistics,
pages 4?9, Copenhagen, Denmark.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
McCarthy, Diana. 1997. Word sense
disambiguation for acquisition of
selectional preferences. In Proceedings of
the ACL/EACL Workshop on Automatic
Information Extraction and Building of Lexical
Semantic Resources for NLP Applications,
pages 52?61, Madrid.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching. In
Proceedings of the First Conference of the
North American Chapter of the Association for
Computational Linguistics, pages 256?263,
Seattle.
Miller, George A. 1998. Nouns in WordNet.
In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database. MIT Press,
pages 23?46.
Pedersen, Ted. 1996. Fishing for exactness.
In Proceedings of the South-Central SAS Users
Group Conference, Austin, pages 188?200.
Pedersen, Ted. 2001. A decision tree of
bigrams is an accurate predictor of word
sense. In Proceedings of the Second Meeting
of the North American Chapter of the
Association for Computational Linguistics,
pages 79?86, Pittsburgh.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional clustering
of English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach to
Lexical Relationships. Ph.D. dissertation,
University of Pennsylvania.
Resnik, Philip. 1998. WordNet and
class-based probabilities. In Christiane
Fellbaum, editor, WordNet: An Electronic
Lexical Database. MIT Press, pages 239?263.
Ribas, Francesc. 1995. On learning more
appropriate selectional restrictions. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 112?118,
Dublin.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
University of Maryland, College Park.
Wagner, Andreas. 2000. Enriching a lexical
semantic net with selectional preferences
by means of statistical corpus analysis. In
Proceedings of the ECAI-2000 Workshop on
Ontology Learning, Berlin, pages 37?42.
Co-occurrence Retrieval:
A Flexible Framework for Lexical
Distributional Similarity
Julie Weeds and David Weir?
University of Sussex
Techniques that exploit knowledge of distributional similarity between words have been proposed
in many areas of Natural Language Processing. For example, in language modeling, the sparse
data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events
from the probabilities of seen co-occurrences of similar events. In other applications, distribu-
tional similarity is taken to be an approximation to semantic similarity. However, due to the wide
range of potential applications and the lack of a strict definition of the concept of distributional
similarity, many methods of calculating distributional similarity have been proposed or adopted.
In this work, a flexible, parameterized framework for calculating distributional similarity is
proposed. Within this framework, the problem of finding distributionally similar words is cast
as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy
with the way they are measured in document retrieval. As will be shown, a number of popular
existing measures of distributional similarity are simulated with parameter settings within the
CR framework. In this article, the CR framework is then used to systematically investigate three
fundamental questions concerning distributional similarity. First, is the relationship of lexical
similarity necessarily symmetric, or are there advantages to be gained from considering it as an
asymmetric relationship? Second, are some co-occurrences inherently more salient than others
in the calculation of distributional similarity? Third, is it necessary to consider the difference in
the extent to which each word occurs in each co-occurrence type?
Two application-based tasks are used for evaluation: automatic thesaurus generation and
pseudo-disambiguation. It is possible to achieve significantly better results on both these tasks by
varying the parameters within the CR framework rather than using other existing distributional
similarity measures; it will also be shown that any single unparameterized measure is unlikely to
be able to do better on both tasks. This is due to an inherent asymmetry in lexical substitutability
and therefore also in lexical distributional similarity.
1. Introduction
Over recent years, approaches to a broad range of natural language processing (NLP)
applications have been proposed that require knowledge about the similarity of words.
The application areas in which these approaches have been proposed range from speech
recognition and parse selection to information retrieval (IR) and natural language
? Department of Informatics, University of Sussex, Falmer, Brighton, BN1 9QH, UK.
Submission received: 4 May 2004; revised submission received: 16 November 2004; accepted for
publication: 16 April 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 31, Number 4
generation. For example, language models that incorporate substantial lexical knowl-
edge play a key role in many statistical NLP techniques (e.g., in speech recognition
and probabilistic parse selection). However, they are difficult to acquire, since many
plausible combinations of events are not seen in corpus data. Brown et al (1992) report
that one can expect 14.7% of the word triples in any new English text to be unseen in a
training corpus of 366 million English words. In our own experiments with grammatical
relation data extracted by a Robust Accurate Statistical Parser (RASP) (Briscoe and
Carroll 1995; Carroll and Briscoe 1996) from the British National Corpus (BNC), we
found that 14% of noun-verb direct-object co-occurrence tokens and 49% of noun-verb
direct-object co-occurrence types in one half of the data set were not seen in the other
half. A statistical technique using a language model that assigns a zero probability
to these previously unseen events will rule the correct parse or interpretation of the
utterance impossible.
Similarity-based smoothing (Hindle 1990; Brown et al 1992; Dagan, Marcus, and
Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999) provides
an intuitively appealing approach to language modeling. In order to estimate the prob-
ability of an unseen co-occurrence of events, estimates based on seen occurrences of
similar events can be combined. For example, in a speech recognition task, we might
predict that cat is a more likely subject of growl than the word cap, even though neither
co-occurrence has been seen before, based on the fact that cat is ?similar? to words that
do occur as the subject of growl (e.g., dog and tiger), whereas cap is not.
However, what is meant when we say that cat is ?similar? to dog? Are we referring to
their semantic similarity, e.g., the components of meaning they share by virtue of both
being carnivorous four-legged mammals? Or are we referring to their distributional
similarity, e.g., in keeping with the Firthian tradition,1 the fact that these words tend to
occur as the arguments of the same verbs (e.g., eat, feed, sleep) and tend to be modified
by the same adjectives (e.g., hungry and playful).
In some applications, the knowledge required is clearly semantic. In IR, documents
might be usefully retrieved that use synonymous terms or terms subsuming those speci-
fied in a user?s query (Xu and Croft 1996). In natural language generation (including text
simplification), possible words for a concept should be similar in meaning rather than
just in syntactic or distributional behavior. In these application areas, distributional sim-
ilarity can be taken to be an approximation to semantic similarity. The underlying idea is
based largely on the central claim of the distributional hypothesis (Harris 1968), that is:
The meaning of entities, and the meaning of grammatical relations among them, is
related to the restriction of combinations of these entities relative to other entities.
This hypothesized relationship between distributional similarity and semantic sim-
ilarity has given rise to a large body of work on automatic thesaurus generation (Hindle
1990; Grefenstette 1994; Lin 1998a; Curran and Moens 2002; Kilgarriff 2003). There
are inherent problems in evaluating automatic thesaurus extraction techniques, and
much research assumes a gold standard that does not exist (see Kilgarriff [2003] and
Weeds [2003] for more discussion of this). A further problem for distributional similarity
methods for automatic thesaurus generation is that they do not offer any obvious way to
distinguish between linguistic relations such as synonymy, antonymy, and hyponymy
(see Caraballo [1999] and Lin et al [2003] for work on this). Thus, one may question
1 ?You shall know a word by the company it keeps.?(Firth 1957)
440
Weeds and Weir Co-occurrence Retrieval
the benefit of automatically generating a thesaurus if one has access to large-scale
manually constructed thesauri (e.g., WordNet [Fellbaum 1998], Roget?s [Roget 1911], the
Macquarie [Bernard 1990] and Moby2). Automatic techniques give us the opportunity
to model language change over time or across domains and genres. McCarthy et al
(2004) investigate using distributional similarity methods to find predominant word
senses within a corpus, making it possible to tailor an existing resource (WordNet) to
specific domains. For example, in the computing domain, the word worm is more likely
to be used in its ?malicious computer program? sense than in its ?earthworm? sense.
This domain knowledge will be reflected in a thesaurus automatically generated from
a computing-specific corpus, which will show increased similarity between worm and
virus and reduced similarity between worm and caterpillar.
In other application areas, however, the requirement for ?similar? words to be
semantically related as well as distributionally related is less clear. For example, in
prepositional phrase attachment ambiguity resolution, it is necessary to decide whether
the prepositional phrase attaches to the verb or the noun as in the examples (1) and (2).
1. Mary ((visited their cottage) with her brother).
2. Mary (visited (their cottage with a thatched roof)).
Hindle and Rooth (1993) note that the correct decision depends on all four lexical
events (the verb, the object, the preposition, and the prepositional object). However, a
statistical model built on the basis of four lexical events must cope with extremely sparse
data. One approach (Resnik 1993; Li and Abe 1998; Clark and Weir 2000) is to induce
probability distributions over semantic classes rather than lexical items. For example, a
cottage is a type of building and a brother is a type of person, and so the co-occurrence of
any type of building and any type of person might increase the probability that the PP
in example (1) attaches to the verb.
However, it is unclear whether the classes over which probability distributions
are induced need to be semantic or whether they could be purely distributional. If
we know that two words tend to behave the same way with respect to prepositional
phrase attachment, does it matter whether they mean similar things? Other arguments
for using semantic classes over distributional classes can similarly be disputed (Weeds
2003). For example, it is not necessary for a class of objects to have a name or symbolic
label for us to know that the objects are similar and to exploit that information. Distri-
butional classes do conflate word senses, but in a task such as PP-attachment ambiguity
resolution, we are unlikely to be working with sense-tagged examples and therefore it
is for word forms that we will wish to estimate probabilities of different attachments.
Finally, distributional classes may be over-fitted to a specific corpus, but this may be
beneficial to the extent that the over-fitting reflects a specific domain or dialect.
Further, recent empirical evidence suggests that techniques based on distributional
similarity may perform as well on this task as those based on semantic similarity.
Li (2002) shows that using a fairly small corpus (126,084 sentences from the Wall
Street Journal) and a distributional similarity technique, it is possible to outperform a
state-of-the-art, WordNet-based technique in terms of accuracy, although not in terms
of coverage. Pantel and Lin (2000) report performance of 84.3% using an unsuper-
vised approach to prepositional phrase attachment based on distributional similarity
2 The Moby Thesaurus is a product of the Moby Project, which was released into the public domain by
Grady Ward in 1996.
441
Computational Linguistics Volume 31, Number 4
techniques. This significantly outperforms previous unsupervised techniques and is
drawing close to the state-of-the-art supervised techniques (88.2%).
Having discussed why distributional similarity is important, we now turn to how
to formulate it. As we have said, two words are distributionally similar if they appear in
similar contexts. We therefore need to consider what is meant by context. For example,
two words could be considered to appear in the same context if they appear in the
same sentence, the same document, or the same grammatical dependency relation. The
effect of the type of context used is discussed by Kilgarriff and Yallop (2000). They show
that the use of sentence-level and document-level context leads to ?looser? thesauri
more akin to Roget?s, whereas the use of grammatical dependency relation level context
leads to ?tighter? thesauri more akin to WordNet. The use of grammatical dependency
relations as context gives us a tighter thesaurus because it restricts distributionally
similar words to those that are plausibly inter-substitutable (Church et al 1994), giving
us the following definition of distributional similarity:
The distributional similarity of two words is the extent to which they can be
inter-substituted without changing the plausibility3 of the sentence.
This concept of lexical substitutability highlights the relationship between distri-
butional similarity and semantic similarity, since semantic similarity can be thought of
as the degree of synonymy that exists between two words, where synonymy is defined
(Church et al 1994) as follows:
Two words are absolute synonyms if they can be inter-substituted in all possible
contexts without changing the meaning.
In our empirical work, we focus on finding semantic relationships between words
such as synonymy, antonymy and hyponymy that might be found in a tighter thesaurus
such as WordNet. Hence, the proposed framework is based on the concept of substi-
tutability, and we use grammatical dependency relations as context. However, since the
framework is based on features, there is no reason why someone wishing to find topical
relationships between words, as might be found in Roget?s, could not use the framework.
We simply do not repeat the earlier work of Kilgarriff and Yallop (2000).
However, a number of questions still remain, which this work does investigate:
1. Is lexical substitutability and therefore distributional similarity
symmetric? The concept of substitution is inherently asymmetric. It is
possible to measure the appropriateness of substituting word A for word B
without measuring the appropriateness of substituting word B for word A.
Similarity has been defined in terms of inter-substitutability; but we ask
whether there is something in the inherent asymmetry of substitution that
can be exploited by an asymmetric measure of distributional similarity.
2. Are all contexts equally important? For example, some verbs, e.g., have and
get, are selectionally weak in the constraints they place on their arguments
(Resnik 1993). Should such contexts be considered on equal terms with
selectionally strong contexts in the calculation of distributional similarity?
3 We use ?plausible sentence? to refer to a sentence that might be observed in naturally occurring
language data.
442
Weeds and Weir Co-occurrence Retrieval
3. Is it necessary to consider the difference in extent to which each word
appears in each context? Is it enough to know that both words can occur in
each context, or do similar words occur in similar contexts with similar
probabilities?
In order to answer these questions, we take a pragmatic, application-oriented ap-
proach to evaluation that is based on the assumption that we want to know which
words are distributionally similar because particular applications can make use of this
information.
However, high performance in one application area is not necessarily correlated
with high performance in another application area (Weeds and Weir 2003a). Thus, it
is not clear that the same characteristics that make a distributional similarity measure
useful in one application will make it useful in another. For example, with regard to the
question about symmetry, in some applications we may prefer a word A that can be
substituted for word B in all of the contexts in which B occurs. In other applications, we
may prefer a word A that can be substituted for word B in all of the contexts in which A
occurs. For example, asked for a semantically related word to dog, we might say animal,
since animal can generally be used in place of dog, whereas we might be less likely to
say dog for animal, since dog cannot generally be used in place of animal. This preference
in the direction of the relationship between the two words is not necessarily maintained
when one considers language modeling in the face of sparse data. If we want to learn
what other contexts animal can occur in, we might look at the co-occurrences of words
such as dog, since we know that dog can generally be replaced by animal. If we want to
learn what other contexts dog can occur in, we are less likely to look at the co-occurrences
of animal, since we know that animal can occur in contexts in which dog cannot.
Rather than attempt to find a single universally optimal distributional similarity
measure, or propose using a radically different distributional similarity measure in each
possible application, we propose a flexible, parameterized framework for calculating
distributional similarity (Section 2). Within this framework, we cast the problem of
finding distributionally similar words as one of co-occurrence retrieval (CR), for which
we can measure precision and recall by analogy with the way that they are measured
in document retrieval. Different models within this framework allow us to investigate
how frequency information is incorporated into the distributional similarity measure.
Different parameter settings within each model allow us to investigate asymmetry in
similarity. In Section 3 we discuss the data and the neighbor set comparison tech-
nique used throughout our empirical work. In Section 4 we discuss a number of
existing distributional similarity measures and discuss the extent to which these can
be simulated by settings within the CR framework. In Section 5 we evaluate the CR
framework on a semantic task (WordNet prediction) and on a language modeling task
(pseudo-disambiguation).
2. Co-occurrence Retrieval
In this section, we present a flexible framework for distributional similarity. This frame-
work directly defines a similarity function, does not require smoothing of the base
language model, and allows us to systematically explore the questions about similarity
raised in Section 1. In our approach, similarity between words is viewed as a measure
of how appropriate it is to use one word (or its distribution) in place of the other. Like
relative entropy (Cover and Thomas 1991), it is inherently asymmetric, since we can
443
Computational Linguistics Volume 31, Number 4
measure how appropriate it is to use word A instead of word B separately from how
appropriate it is to use word B instead of word A.
The framework presented here is general to the extent that it can be used to compute
similarities for any set of objects where each object has an associated set of features
or co-occurrence types and these co-occurrence types have associated frequencies
that may be used to form probability estimates. Throughout our discussion, the word
for which we are finding neighbors will be referred to as the target word. If we are
computing the similarity between the target word and another word, then the second
word is a potential neighbor of the target word. A target word?s nearest neighbors are
the potential neighbors that have the highest similarity with the target word.
2.1 Basic Concepts
Let us imagine that we have formed descriptions of each word in terms of the other
words with which they co-occur in various specified grammatical relations in some
corpus. For example, the noun cat might have the co-occurrence types ?dobj-of, feed? and
?ncmod-by, hungry?. Now let us imagine that we have lost (or accidentally deleted) the
description for word w2, but before this happened we had noticed that the description
of word w2 was very similar to that of word w1. For example, the noun dog might also
have the co-occurrence types ?dobj-of, feed? and ?ncmod-by, hungry?. Hence, we decide
that we can use the description of word w1 instead of the description of word w2 and
are hopeful that nobody will notice. How well we do will depend on the validity of
substituting w1 for w2, or, in other words, the similarity between w1 and w2.
The task we have set ourselves can be seen as co-occurrence retrieval (CR). By
analogy with information retrieval, where there is a set of documents that we would
like to retrieve and a set of documents that we do retrieve, we have a scenario where
there is a set of co-occurrences that we would like to retrieve, the co-occurrences of w2,
and a set of co-occurrences that we have retrieved, the co-occurrences of w1. Continuing
the analogy, we can measure how well we have done in terms of precision and recall,
where precision tells us how much of what was retrieved was correct and recall tells us
how much of what we wanted to retrieve was retrieved.
Our flexible framework for distributional similarity is based on this notion of co-
occurrence retrieval. As the distribution of word B moves away from being identical
to that of word A, its ?similarity? with A can decrease along one or both of two
dimensions. When B occurs in contexts that word A does not, the result is a loss of
precision, but B may remain a high-recall neighbor. For example, we might expect the
noun animal to be a high-recall neighbor of the noun dog. When B does not occur in
contexts that A does occur in, the result is a loss of recall but B may remain a high-
precision neighbor. For example, we might expect the noun dog to be a high-precision
neighbor of the noun animal. We can explore the merits of symmetry and asymmetry in
a similarity measure by varying the relative importance attached to precision and recall.
This was the first question posed about distributional similarity in Section 1.
The remainder of this section is devoted to defining two types of co-occurrence
retrieval model (CRM). Additive models are based on the Boolean concept of two
objects either sharing or not sharing a particular feature (where objects are words and
features are co-occurrence types). Difference-weighted models incorporate the differ-
ence in extent to which each word has each feature. Exploring the two types of models,
both defined on the same concepts of precision and recall, allows us to investigate the
third question posed in Section 1: Is a shared context worth the same, regardless of the
difference in the extent to which each word appears in that context?
444
Weeds and Weir Co-occurrence Retrieval
We also use the CR framework to investigate the second question posed about
distributional similarity, ?Should all contexts be treated equally?,? by using differ-
ent weight functions within each type of model. Weight functions decide which co-
occurrence types are features of a word and determine the relative importance of
features. In previous work (Weeds and Weir 2003b), we experimented with weight
functions based on combinatorial, probabilistic, and mutual information (MI). These
allow us to define type-based, token-based, and MI-based CRMs, respectively. This
work extends the previous work by also considering weighted mutual information
(WMI) (Fung and McKeown 1997), the t-test (Manning and Schu?tze 1999), the z-test
(Fontenelle et al 1994), and an approximation to the log-likelihood ratio (Manning and
Schu?tze 1999) as weight functions.
2.2 Additive Models
Having considered the intuition behind calculating precision and recall for co-
occurrence retrieval, we now formulate this formally in terms of an additive model.
We first need to consider for each word w which co-occurrence types will be re-
trieved, or predicted, by it and, conversely, required in a description of it. We will refer
to these co-occurrence types as the features of w, F(w):
F(w) = {c : D(w, c) > 0} (1)
where D(w, c) is the weight associated with word w and co-occurrence type c. Possible
weight functions will be described in Section 2.3.
The shared features of word w1 and word w2 are referred to as the set of True
Positives, TP(w1, w2), which will be abbreviated to TP in the rest of this article:
TP(w1, w2) = F(w1) ? F(w2) (2)
The precision of w1?s retrieval of w2?s features is the proportion of w1?s features that
are shared by both words, where each feature is weighted by its relative importance
according to w1:
Padd(w1, w2) =
?
TP D(w1, c)
?
F(w1 ) D(w1, c)
(3)
The recall of w1?s retrieval of w2?s features is the proportion of w2?s features that
are shared by both words, where each feature is weighted by its relative importance
according to w2:
Radd(w1, w2) =
?
TP D(w2, c)
?
F(w2 ) D(w2, c)
(4)
445
Computational Linguistics Volume 31, Number 4
Table 1
Weight functions.
Dtype(w, c) =
{
1 if P(c|w) > 0
0 otherwise
Dtok(w, c) = P(c|w)
Dmi(w, c) = I(w, c) = log
(
P(c, w)
P(c)P(w)
)
Dwmi(w, c) = P(c, w) ? log
(
P(c, w)
P(c)P(w)
)
Dt(w, c) =
P(c, w)?P(c)?P(w)
?
P(c, w)
N
Dz(w, c) =
P(c, w)?P(c)?P(w)
?
P(c).P(w)
N
Dallr(w, c) = ?2 ?
(
log L
(
F(w, c), F(w), F(c)N
)
? log L
(
F(w, c), F(w), F(w, c)F(w)
))
Precision and recall both lie in the range [0,1] and are both equal to one when
each word has exactly the same features. It should also be noted that the recall of
w1?s retrieval of w2 is equal to the precision of w2?s retrieval of w1, i.e., Radd(w1, w2) =
Padd(w2, w1).
2.3 Weight Functions
The weight function plays two important roles. First, it determines which co-
occurrences of w1 and w2 are important enough to be considered part of their descrip-
tion, or by analogy with document retrieval, which co-occurrences we want to retrieve
for w2 and which co-occurrences we have retrieved using the description of w1. It is
then used to weight contexts by their importance. In the latter case, D(w1, c) tells us the
retrieval process?s perceived relevance of co-occurrence type c, and D(w2, c) tells us the
actual relevance of co-occurrence type c. The weight functions we have considered so
far are summarized in Table 1. Each weight function can be used to define its own CRM,
which we will now discuss in more detail.
Additive type-based CRM (Dtype). In this CRM, the precision of w1?s retrieval of w2 is the
proportion of co-occurrence types occurring with w1 that also occur with w2, and the
recall of w1?s retrieval of w2 is the proportion of verb co-occurrence types (or distinct
verbs) occurring with w2 that also occur with w1. In this case, the summed values of D
are always 1, and hence the expressions for precision and recall can be simplified:
Paddtype(w1, w2) =
?
TP Dtype(w1, c)
?
F(w1 ) Dtype(w1, c)
=
|TP|
|F(w1)|
(5)
446
Weeds and Weir Co-occurrence Retrieval
Raddtype(w1, w2) =
?
TP Dtype(w2, c)
?
F(w2 ) Dtype(w2, c)
=
|TP|
|F(w2)|
(6)
Additive token-based CRM (Dtok). In this CRM, the precision of w1?s retrieval of w2 is the
proportion of co-occurrence tokens occurring with w1 that also occur with w2, and the
recall of w1?s retrieval of w2 is the proportion of co-occurrence tokens occurring with w2
that also occur with w1. Hence, words have the same features as in the type-based CRM,
but each feature is given a weight based on its probability of occurrence. Since F(w) =
{c : D(w, c) > 0} = {c : P(c|w) > 0}, it follows that
?
F(w) Dtok(w, c) = 1, and therefore
the expressions for precision and recall can be simplified:
Paddtok (w1, w2) =
?
TP Dtok(w1, c)
?
F(w1 ) Dtok(w1, c)
=
?
TP
P(c, w1) (7)
Raddtok (w1, w2) =
?
TP Dtok(w2, c)
?
F(w2) Dtok(w2, c)
=
?
TP
P(c, w2) (8)
Additive MI-based CRM (Dmi). Using pointwise mutual information (MI) (Church and
Hanks 1989) as the weight function means that a co-occurrence c is considered a feature
of word n if the probability of their co-occurrence is greater than would be expected if
words occurred independently. In addition, more informative co-occurrences contribute
more to the sums in the calculation of precision and recall and hence have more weight.
Additive WMI-based CRM (Dwmi). Weighted mutual information (WMI) (Fung and
McKeown 1997) has been proposed as an alternative to MI, particularly when MI might
lead to the over-association of low-frequency events. In this function, the pointwise
MI is multiplied by the probability of the co-occurrence; hence, reducing the weight
assigned to low-probability events.
Additive t-test based CRM (Dt). The t-test (Manning and Schu?tze 1999) is a standard
statistical test that has been proposed for collocation analysis. It measures the (signed)
difference between the observed probability of co-occurrence and the expected prob-
ability of co-occurrence, as would be observed if words occurred independently. The
difference is divided by the standard deviation in the observed distribution. Similarly
to MI, this score obviously gives more weight to co-occurrences that occur more than
would be expected, and its use as the weight function results in any co-occurrences that
occur less than would be expected being ignored.
Additive z-test based CRM (Dz). The z-test (Fontenelle et al 1994) is almost identical
to the t-test. However, using the z-test, the (signed) difference between the observed
probability of co-occurrence and the expected probability of co-occurrence is divided
by the standard deviation in the expected distribution.
Additive log-likelihood ratio based CRM (Dallr). The log-likelihood ratio (Manning and
Schu?tze 1999) considers the difference (as a log ratio) in probability of the observed
frequencies of co-occurrences and individual words occurring under the null hypoth-
447
Computational Linguistics Volume 31, Number 4
esis, that words occur independently, and under the alternative hypothesis, that they
do not.
H0 : P(c|w) = p = P(c|?w) (9)
H1 : P(c|w) = p1 = p2 = P(c|?n) (10)
If f (w, c) is the frequency of w and c occurring together, f (w) is the total frequency
of w occurring in any context, f (c) is the total frequency of c occurring with any
word, and N is the grand total of co-occurrences, then the log-likelihood ratio can
be written:
Log?(w, c) = ?2. log L(H0)
L(H1)
(11)
= ?2.
?
?
?
?
?
?
?
?
?
log L
(
f (w, c), f (w), f (c)N
)
+ log L
(
f (c) ? f (w, c), N ? f (w), f (c)N
)
? log L
(
f (w, c), f (w), f (w, c)f (w)
)
? log L
(
f (c) ? f (w, c), N ? f (w), f (c)? f (w, c)N? f (w)
)
?
?
?
?
?
?
?
?
?
(12)
where L(k, n, x) = xk(1 ? x)n?k (13)
In our implementation (see Table 1), an approximation to this formula is used,
which we term the ALLR weight function. We use an approximation because the terms
that represent the probabilities of the other contexts (i.e., seeing f (c) ? f (w, c) under each
hypothesis) tend towards ?? as N increases (since the probabilities tend towards zero).
Since N is very large in our experiments (approximately 2,000,000), we found that using
the full formula led to many weights being undefined. Further, since in this case the
probability of seeing other contexts will be approximately equal under each hypothesis,
it is a reasonable approximation to make.
Another potential problem with using the log-likelihood ratio as the weight func-
tion is that it is always positive, since the observed distribution is always more
probable than the hypothesized distribution. All of the other weight functions assign
a zero or negative weight to co-occurrence types that do not occur with a given
word and thus these zero frequency co-occurrence types are never selected as features.
This is advantageous in the computation of similarity, since computing the sums
over all co-occurrence types rather than just those co-occurring with at least one of
the words is (1) very computationally expensive and (2) due to their vast number, the
effect of these zero frequency co-occurrence types tends to outweigh the effect of
those co-occurrence types that have actually occurred. Giving such weight to these
shared non-occurrences seems unintuitive and has been shown by Lee (1999) to be
undesirable in the calculation of distributional similarity. Hence, when using the
448
Weeds and Weir Co-occurrence Retrieval
ALLR as the weight function, we use the additional restriction that P(c, w) > 0 when
selecting features.
2.4 Difference-Weighted Models
In additive models, no distinction is made between features that have occurred to the
same extent with each word and features that have occurred to different extents with
each word. For example, if two words have the same features, they are considered
identical, regardless of whether the feature occurs with the same probability with each
word or not. Here, we define a type of model that allows us to capture the difference in
the extent to which each word has each feature.
We do this by defining the similarity of two words with respect to an individual
feature, using the same principles that we use to define the similarity of two words
with respect to all their features. First, we define an extent function, E(w, c), which is the
extent to which w1 goes with c and which may be, but is not necessarily, the same as
the weight function D(n, w). Possible extent functions will be discussed in Section 2.5.
Having defined this function, we can measure the precision and recall of individual
features. The precision of an individual feature c retrieved by w1 is the extent to which
both words go with c divided by the extent to which w1 goes with c. The recall of the
retrieval of c by w1 is the extent to which both words go with c divided by the extent to
which w2 goes with c.
P (w1, w2, c) =
min(E(w1, c), E(w2, c))
E(w1, c)
(14)
R(w2, w1, c) =
min(E(w1, c), E(w2, c))
E(w2, c)
(15)
Precision and recall of an individual feature, like precision and recall of a distrib-
ution, lie in the range [0,1]. We can now redefine precision and recall of a distribution
as follows:
Pdw(w1, w2) =
?
TP D(w1, c) ? P (w1, w2, c)
?
F(w1) D(w1, c)
(16)
Rdw(w1, w2) =
?
TP D(w2, c) ? R(w1, w2, c)
?
F(w2 ) D(w2, c)
(17)
Using precision and recall of individual features as weights in the definitions
of precision and recall of a distribution captures the intuition that retrieval of a
co-occurrence type is not a black-and-white matter. Features that are shared to a
similar extent are considered more important in the calculation of distributional
similarity.
449
Computational Linguistics Volume 31, Number 4
Table 2
Extent functions.
Etype(w, c) = P(c|w)
Etok(w, c) = P(c|w)
Emi(w, c) = I(w, c) = log
(
P(c, w)
P(c)P(w)
)
Ewmi(w, c) = P(c, w). log
(
P(c, w)
P(c)P(w)
)
Et(w, c) =
P(c, w)?P(c).P(w)
?
P(c, w)
N
Ez(w, c) =
P(c, w)?P(c).P(w)
?
P(c).P(w)
N
Eallr(w, c) = ?2.
(
log L
(
f (w, c), f (w), f (c)N
)
? log L
(
f (w, c), f (w), f (w, c)f (w)
))
2.5 Extent Functions
The extent functions we have considered so far are summarized in Table 2. Note that
in general, the extent function is the same as the weight function, which leads to a
standard simplification of the expressions for precision and recall in the difference-
weighted CRMs. For example, in the difference-weighted MI-based model we get the
expressions:
Pdwmi (w1, w2) =
?
TP I(w1, c) ?
min(I(w1, c),I(w2, c))
I(w1, c)
?
F(w1 ) I(w1, c)
=
?
TP min(I(w1, c), I(w2, c))
?
F(w1 ) I(w1, c)
(18)
Rdwmi (w1, w2) =
?
TP I(w2, c) ?
min(I(w2, c),I(w1, c))
I(w2, c)
?
F(w2 ) I(w2, c)
=
?
TP min(I(w2, c), I(w1, c))
?
F(w2 ) I(w2, c)
(19)
Similar expressions can be derived for the WMI-based CRM, the t-test based CRM, the
z-test based CRM, and the ALLR-based CRM. An interesting special case is the difference-
weighted token-based CRM. In this case, since
?
F(w) P(c|w) = 1, we derive the following
expressions for precision and recall:
Pdwtok (w1, w2) =
?
TP P(c|w1) ?
min(P(c|w1), P(c|w2))
P(c|w1)
?
F(w1 ) P(c|w1)
=
?
TP
min(P(c|w1), P(c|w2)) (20)
450
Weeds and Weir Co-occurrence Retrieval
Rdwtok(w1, w2) =
?
TP P(c|w2) ?
min(P(c|w2), P(c|w1))
P(c|w2)
?
F(w2 ) P(c|w2)
=
?
TP
min(P(c|w2), P(c|w1))
= Pdwtok (w1, w2) (21)
Note that although we have defined separate precision and recall functions, we
have arrived at the same expression for both in this model. As a result, this model is
symmetric.
The only CRM in which we use a different extent and weight function is the
difference-weighted type-based CRM. This is because there is no difference between types
and tokens for an individual feature; i.e., their retrieval is equivalent. In this case, the
following expressions for precision and recall are derived:
Pdwtype(w1, w2) =
?
TP Dtype(w1, c) ? Ptype(w1, w2, c)
?
F(w1 ) Dtype(w1, c)
=
?
TP
min(P(c|w1), P(c|w2))
P(c|w1)
|F(w1)|
(22)
Rdwtype(w1, w2) =
?
TP Dtype(w2, c) ? Rtype(w1, w2, c)
?
F(w2 ) Dtype(w2, c)
=
?
TP
min(P(c|w2), P(c|w1))
P(c|w2)
|F(w2)|
(23)
Note that this is different from the additive token-based model because, although
every token is effectively considered in this model, tokens are not weighted equally.
In this model, tokens are treated differently according to which type they belong.
The importance of the retrieval (or non-retrieval) of a single token depends on the
proportion of the tokens for its particular type that it constitutes.
2.6 Combining Precision and Recall
We have, so far, been concerned with defining a pair of numbers that represents the
similarity between two words. However, in applications, it is normally necessary to
compute a single number in order to determine neighborhood or cluster membership.
The classic way to combine precision and recall in IR is to compute the F-score; that is,
the harmonic mean of precision and recall:
F = mh(P ,R) = 2 ? P ? RP +R (24)
However, we do not wish to assume that a good substitute requires both high
precision and high recall of the target distribution. It may be that, in some situations,
the best word to use in place of another word is one that only retrieves correct co-
occurrences (i.e., it is a high-precision neighbor) or it may be one that retrieves all of
the required co-occurrences (i.e., it is a high-recall neighbor). The other factor in each
case may play only a secondary role or no role at all.
We can retain generality and investigate whether high precision or high recall or
high precision and high recall are required for high similarity by computing a weighted
451
Computational Linguistics Volume 31, Number 4
Table 3
Table of special values of ? and ?.
? ? Special Case
- 1 harmonic mean of precision and recall (F-score)
? 0 weighted arithmetic mean of precision and recall
1 0 precision
0 0 recall
0.5 0 unweighted arithmetic mean
arithmetic mean of the harmonic mean and the weighted arithmetic mean of precision
and recall:4
mh(P (w1, w2),R(w1, w2)) =
2.P (w1, w2).R(w1, w2)
P (w1, w2) +R(w1, w2)
(25)
ma(P (w1, w2),R(w1, w2)) = ?.P (w1, w2) + (1 ? ?).R(w1, w2) (26)
sim(w1, w2) = ?.mh(P (w1, w2),R(w1, w2))
+ (1 ? ?).ma(P (w1, w2),R(w1, w2)) (27)
where both ? and ? lie in the range [0,1]. The resulting similarity, sim(w1, w2), will also
lie in the range [0,1] where 0 is low and 1 is high. This formula can be used in combi-
nation with any of the models for precision and recall outlined earlier. Precision and
recall can be computed once for every pair of words (and every model) whereas sim-
ilarity depends on the values of ? and ?. The flexibility allows us to investigate em-
pirically the relative significance of the different terms and thus whether one (or more)
might be omitted in future work. Table 3 summarizes some special parameter settings.
2.7 Discussion
We have developed a framework based on the concept of co-occurrence retrieval
(CR). Within this framework we have defined a number of models (CRMs) that
allow us to systematically explore three questions about similarity. First, is similarity
between words necessarily a symmetric relationship, or can we gain an advantage by
considering it as an asymmetric relationship? Second, are some features inherently
more salient than others? Third, does the difference in extent to which each word takes
each feature matter?
The CRMs and the parameter settings therein correspond to alternative possibil-
ities. First, a high-precision neighbor is not necessarily a high-recall neighbor (and,
conversely, a high-recall neighbor is not necessarily a high-precision neighbor) and
therefore we are not constrained to a symmetric relationship of similarity between
4 This is as opposed to using a standard weighted F-score (Manning and Schu?tze 1999), which uses just one
parameter ?: F? = PR?R+(1??)P . We did not use this weighting because we wished to investigate the
differences between using an arithmetic mean and a harmonic mean.
452
Weeds and Weir Co-occurrence Retrieval
words. Second, the use of different weight functions varies the relative importance
attached to features. Finally, difference-weighted models contrast with additive models
in considering the difference in extent to which each word takes each feature.
3. Data and Experimental Techniques
The rest of this paper is concerned with evaluation of the proposed framework; first, by
comparing it to existing distributional similarity measures, and second, by evaluating
performance on two tasks. Throughout our empirical work, we use one data-set and one
neighbor set comparison technique, which we now discuss in advance of presenting any
of our actual experiments.
3.1 Data
The data used for all our experimental work was noun-verb direct-object data extracted
from the BNC by a Robust Accurate Statistical Parser (RASP) (Briscoe and Carroll 1995;
Carroll and Briscoe 1996). We constructed a list of nouns that occur in both our data set
and WordNet ordered by their frequency in our corpus data. Since we are interested
in the effects of word frequency on word similarity, we selected 1,000 high-frequency
nouns and 1,000 low-frequency nouns. The 1,000 high-frequency nouns were selected
as the nouns with frequency ranks of 1?1,000; this corresponds to a frequency range of
[586,20871]. The low-frequency nouns were selected as the nouns with frequency ranks
of 3,001?4,000; this corresponds to a frequency range of [72,121].
For each target noun, 80% of the available data was randomly selected as training
data and the other 20% was set aside as test data.5 The training data was used to
compute similarity scores between all possible pairwise combinations of the 2,000 nouns
and to provide (MLE) estimates of noun-verb co-occurrence probabilities in the pseudo-
disambiguation task. The test data provides unseen co-occurrences for the pseudo-
disambiguation task.
Although we only consider similarity between nouns based on co-occurrences
with verbs in the direct-object position, the generality of the techniques proposed
is not so restricted. Any of the techniques can be applied to other parts of speech,
other grammatical relations, and other types of context. We restricted the scope of our
experimental work solely for computational and evaluation reasons. However, we
could have chosen to look at the similarity between verbs or between adjectives.6 We
chose nouns as a starting point since nouns tend to allow less sense extensions than
verbs and adjectives (Pustejovsky 1995). Further, the noun hyponymy hierarchy in
WordNet, which will be used as a pseudo-gold standard for comparison, is widely
recognized in this area of research.
Some previous work on distributional similarity between nouns has used only a
single grammatical relation (e.g., Lee 1999), whereas other work has considered multiple
grammatical relations (e.g., Lin 1998a). We consider only a single grammatical relation
because we believe that it is important to evaluate the usefulness of each grammatical
relation in calculating similarity before deciding how to combine information from
5 This results in a single 80:20 split of the complete data set, in which we are guaranteed that the original
relative frequencies of the target nouns are maintained.
6 The use of grammatical relations to model context precludes finding similarities between words of
different parts of speech. Since we are looking at similarity in terms of substitutability, we would not
expect to find a word of one part of speech substitutable for a word of another part of speech.
453
Computational Linguistics Volume 31, Number 4
different relations. In previous work (Weeds 2003), we found that considering the
subject relation as well as the direct-object relation did not improve performance on
a pseudo-disambiguation task.
Our last restriction was to only consider 2,000 of the approximately 35,000 nouns
occurring in the corpus. This restriction was for computational efficiency and to avoid
computing similarities based on the potentially unreliable descriptions of very low-
frequency words. However, since our evaluation is comparative, we do not expect our
results to be affected by this or any of the other restrictions.
3.2 Neighbor Set Comparison Technique
In several of our experiments, we measure the overlap between two different similarity
measures. We use a neighbor set comparison technique adapted from Lin (1997).
In order to compare two neighbor sets of size k, we transform each neighbor set
so that each neighbor is given a rank score of k ? rank. Potential neighbors not within
a given rank distance k of the noun score zero. This transformation is required since
scores computed on different scales are to be compared and because we wish to only
consider neighbors up to a certain rank distance. The similarity between two neighbor
sets S and S? is computed as the cosine of the rank score vectors:
C(S, S?) =
?
w?S?S? s(w) ? s?(w)
?k
i=1 i
2
(28)
where s(w) and s?(w) are the rank scores of the words within each neighbor set S and S?
respectively.
In previous work (Weeds and Weir 2003b), having computed the similarity between
neighbor sets for each noun according to each pair of measures under consideration, we
computed the mean similarity across all high-frequency nouns and all low-frequency
nouns. However, since the use of the CR framework requires parameter optimization,
here, we randomly select 60% of the nouns to form a development set and use the
remaining 40% as a test set. Thus, any parameters are optimized over the development
set nouns and performance measured at these settings over the test set.
4. Alternative Distributional Similarity Measures
In this section, we consider related work on distributional similarity measures and the
extent to which some of these measures can be simulated within the CR framework.
However, there is a large body of work on distributional similarity measures; for a
more extensive review, see Weeds (2003). Here, we concentrate on a number of more
popular measures: the Dice Coefficient, Jaccard?s Coefficient, the L1 Norm, the ?-skew
divergence measure, Hindle?s measure, and Lin?s MI-based measure.
4.1 The Dice Coefficient
The Dice Coefficient (Frakes and Baeza-Yates 1992) is a popular combinatorial similarity
measure adopted from the field of Information Retrieval for use as a measure of lexical
454
Weeds and Weir Co-occurrence Retrieval
distributional similarity. It is computed as twice the ratio between the size of the inter-
section of the two feature sets and the sum of the sizes of the individual feature sets:
simdice(w1, w2) =
2 ? |F(w1) ? F(w2)|
|F(w1)|+ |F(w2)|
where F(w) = {c : P(c|w) > 0} (29)
According to this measure, the similarity between words with no shared features
is zero and the similarity between words with identical feature sets is 1. However, as
shown below, this formula is equivalent to a special case in the CR framework: the
harmonic mean of precision and recall (or F-score) using the additive type-based CRM.
mh(Paddtype(w1, w2),Raddtype(w1, w2))=
2 ? Paddtype(w1, w2) ? Raddtype(w1, w2)
Paddtype(w1, w2) +Raddtype(w1, w2)
=
2 ? |TP||F(w1 )| ?
|TP|
|F(w2 )|
|TP|
|F(w1)| +
|TP|
|F(w2 )|
=
2 ? |TP| ? |TP|
|TP| ? |F(w1)|+ |TP| ? |F(w2)|
=
2 ? |TP|
|F(w1)|+ |F(w2)|
=
2 ? |F(w1) ? F(w2)|
|F(w1)|+ |F(w2)|
= simdice(w1, w2) (30)
Thus, when ? is set to 1 in the additive type-based CRM, the Dice Coefficient is
exactly replicated.
4.2 Jaccard?s Coefficient
Jaccard?s Coefficient (Salton and McGill 1983), also known as the Tanimoto Coefficient
(Resnik 1993), is another popular combinatorial similarity measure. It can be defined as
the proportion of features belonging to either word that are shared by both words; that
is, the ratio between the size of the intersection of the feature sets and the size of the
union of feature sets:
simjacc(w1, w2) =
|F(w1) ? F(w2)|
|F(w1) ? F(w2)|
(31)
As with the Dice Coefficient, the similarity between words with no shared co-
occurrences is zero and the similarity between words with identical features is 1. Fur-
ther, as shown by van Rijsbergen (1979), the Dice Coefficient and Jaccard?s Coefficient
are monotonic in one another. Thus, although in general the scores computed by each
will be different, the orderings or rankings of objects will be the same. In other words,
for all k and w, the k nearest neighbors of word w according to Jaccard?s Coefficient will
be identical to the k nearest neighbors of word w according to the Dice Coefficient and
the harmonic mean of precision and recall in the additive type-based CRM.
455
Computational Linguistics Volume 31, Number 4
4.3 The L1 Norm
The L1 Norm (Kaufman and Rousseeuw 1990) is a member of a family of measures
known as the Minkowski Distance, for measuring the distance7 between two points
in space. The L1 Norm is also known as the Manhattan Distance, the taxi-cab distance,
the city-block distance, and the absolute value distance, since it represents the distance
traveled between the two points if you can only travel in orthogonal directions. When
used to calculate lexical distributional similarity, the dimensions of the vector space are
co-occurrence types and the values of the vector components are the probabilities of
the co-occurrence types given the word. Thus the L1 distance between two words, w1
and w2, can be written as:
distL1 (w1, w2) =
?
c
|P(c|w1) ? P(c|w2)| (32)
However, noting the algebraic equivalence A + B ? |A ? B| ? 2 ? min(A, B) and us-
ing basic probability theory, we can rewrite the L1 Norm as follows:
distL1 (w1, w2) =
?
c
|P(c|w1) ? P(c|w2)|
=
?
c
P(c|w1) + P(c|w2) ? 2 ? min(P(c|w1), P(c|w2))
=
?
c
P(c|w1) +
?
c
P(c|w2) ? 2 ?
?
c
min(P(c|w1), P(c|w2))
= 2 ? 2 ?
?
c
min(P(c|w1), P(c|w2)) (33)
However, min(P(c|w1), P(c|w2)) > 0 if and only if c ? TP. Hence:
distL1 (w1, w2) = 2 ? 2 ?
?
TP
min(P(c|w1), P(c|w2) = 2 ? 2 ? simdwtok(w1, w2) (34)
In other words, the L1 Norm is directly related to the difference-weighted token-
based CRM. The constant and multiplying factors are required, since the CRM defines
a similarity in the range [0,1], whereas the L1 Norm defines a distance in the range [0,2]
(where 0 distance is equivalent to 1 on the similarity scale).
4.4 The ?-skew Divergence Measure
The ?-skew divergence measure (Lee 1999, 2001) is a popular approximation to the
Kullback-Leibler divergence measure8 (Kullback and Leibler 1951; Cover and Thomas
1991). It is an approximation developed to be used when unreliable MLE probabilities
7 Distance measures, also referred to as divergence and dissimilarity measures, can be viewed as the
inverse of similarity measures; that is, an increase in distance correlates with a decrease in similarity.
8 The Kullback-Leibler divergence measure is also often referred to as ?relative entropy.?
456
Weeds and Weir Co-occurrence Retrieval
would result in the actual Kullback-Leibler divergence measure being equal to ?. It is
defined (Lee 1999) as:
dist?(q, r) = D(r||?.q + (1 ? ?).r) (35)
for 0 ? ? ? 1, and where:
D(p||q) =
?
x
p(x) log
p(x)
q(x)
(36)
In effect, the q distribution is smoothed with the r distribution, which results in it
always being non-zero when the r distribution is non-zero. The parameter ? controls the
extent to which the measure approximates the Kullback-Leibler divergence measure.
When ? is close to 1, the approximation is close while avoiding the problem with
zero probabilities associated with using the Kullback-Leibler divergence measure. This
theoretical justification for using a very high value of ? (e.g., 0.99) is also borne out by
empirical evidence (Lee 2001).
The ?-skew divergence measure retains the asymmetry of the Kullback-Leibler
divergence, and Weeds (2003) discusses the significance in the direction in which it
is calculated. For the purposes of this paper, we will find the neighbors of w2 by
optimizing:9
dist?(P(c|w1), P(c|w2)) (37)
Due to the form of the ?-skew divergence measure, we do not expect any of the
CRMs to exactly simulate it. However, this measure does take into account the differ-
ences between the probabilities of co-occurrences in each distribution (as a log ratio)
and therefore we might expect that it will be fairly closely simulated by the difference-
weighted token-based CRM. Further, the ?-skew divergence measure is asymmetric.
dist?(w1, w2) measures the cost of using the distribution of w1 instead of w2 and is
calculated over the verbs that occur with w2. As such, we might expect that dist? will be
a high-recall measure, since recall is calculated over the co-occurrences of w2.
In order to determine how close any approximation is in practice, we compared
the 200 nearest neighbors according to dist? and different parameter settings within
the CR framework for 1,000 high-frequency nouns and for 1,000 low-frequency nouns,
using the data and the neighbor set comparison technique described in Section 3. Table 4
shows the optimal parameters in each CRM for simulating dist?, computed over the
development set, and the mean similarity at these settings over both the development
set and the test set. From these results, we can make the following observations.
First, the differences in mean similarities over the development set and the test
set are minimal. Thus, performance of the models with respect to different parameter
settings appears stable across different words.
Second, the differences between the models are fairly small. The difference-
weighted token-based CRM achieves a fairly close approximation to dist?, but the
9 This is what Weeds (2003) refers to as dist?1.
457
Computational Linguistics Volume 31, Number 4
Table 4
Optimized similarities between CRMs and dist? and corresponding parameter settings.
Target Noun Frequency
high low
Optimal Devel. Test Optimal Devel. Test
Parameters Sim. Sim. Parameters Sim. Sim.
CRM ? ? ? ?
simaddtype 0.25 0.4 0.74 0.75 0.5 0.3 0.66 0.66
simaddtok 0.5 0.0 0.77 0.78 0.5 0.0 0.67 0.66
simaddmi 0.0 0.0 0.76 0.77 0.25 0.1 0.72 0.73
simaddwmi 0.25 0.0 0.71 0.72 0.25 0.1 0.79 0.79
simaddt 0.5 0.0 0.84 0.85 0.5 0.2 0.71 0.71
simaddz 0.5 0.0 0.79 0.80 0.5 0.1 0.63 0.63
simaddallr 0.25 0.0 0.70 0.71 0.5 0.0 0.64 0.63
simdwtype 0.0 0.25 0.70 0.71 0.0 0.0 0.52 0.53
simdwtok ? ? 0.79 0.80 ? ? 0.58 0.58
simdwmi 0.0 0.0 0.66 0.68 0.0 0.0 0.60 0.60
simdwwmi 0.0 0.1 0.66 0.67 0.0 0.1 0.58 0.58
simdwt 0.5 0.1 0.82 0.83 0.5 0.3 0.69 0.69
simdwz 0.5 0.0 0.78 0.80 0.5 0.1 0.64 0.64
simdwallr 0.75 0.0 0.53 0.54 0.75 0.0 0.48 0.48
overall best approximation is achieved by the additive t-test based CRM. Although
none of the CRMs are able to simulate dist? exactly, the closeness of approximation
achieved in the best cases (greater than 0.7) is substantially higher than the degree
of overlap observed between other measures of distributional similarity. Weeds, Weir,
and McCarthy (2004) report an average overlap of 0.4 between neighbor sets produced
using dist? and Jaccard?s Measure and an average overlap of 0.48 between neighbor sets
produced using dist? and Lin?s similarity measure.
A third observation is that all of the asymmetric models get closest at high
levels of recall for both high- and low-frequency nouns. For example, Figure 1 illustrates
the variation in mean similarity between neighbor sets with the parameters ? and
? for the additive t-test based model. As can be seen, similarity between neighbor
sets is significantly higher at high recall settings (low ?) within the model than at high-
precision settings (high ?), which suggests that dist? has high-recall CR characteristics.
4.5 Hindle?s Measure
Hindle (1990) proposed an MI-based measure, which he used to show that nouns could
be reliably clustered based on their verb co-occurrences. We consider the variant of
458
Weeds and Weir Co-occurrence Retrieval
Figure 1
Variation (with parameters ? and ?) in development set mean similarity between neighbor sets
of the additive t-test based CRM and of dist?.
Hindle?s Measure proposed by (Lin 1998a), which overcomes the problem associated
with calculating MI for word-feature combinations that do not occur:
simhind(w1, w2) =
?
T(w1)?T(w2)
min(I(c, w1), I(c, w2)) (38)
where T(w1) = {c : I(c, n) > 0}. This expression is the same as the numerator in the
expressions for precision and recall in the difference-weighted MI-based CRM:
Pdwmi (w1, w2) =
?
TP I(w1, c) ?
min(I(w1, c),I(w2, c))
I(w1, c)
?
F(w1) I(w1, c)
=
?
TP min(I(w1, c), I(w2, c))
?
F(w1 ) I(w1, c)
(39)
Rdwmi (w1, w2) =
?
TP I(w2, c) ?
min(I(w2, c),I(w1, c))
I(w2, c)
?
F(w2 ) I(w2, c)
=
?
TP min(I(w2, c), I(w1, c))
?
F(w2 ) I(w2, c)
(40)
since TP = T(w1) ? T(w2). However, we also note that the denominator in the expres-
sion for recall depends only on w2, and therefore, for a given w2, is a constant. Since w2 is
the target word, it will remain the same as we calculate each neighbor set. Accordingly,
the value of recall for each potential neighbor w1 of w2 will be the value of simhind
divided by a constant. Hence, neighbor sets derived using simhind are identical to those
obtained using recall (? = 0,? = 0) in the difference-weighted MI-based CRM.
4.6 Lin?s Measure
Lin (1998a) proposed a measure of lexical distributional similarity based on his
information-theoretic similarity theorem (Lin 1997, 1998b):
The similarity between A and B is measured by the ratio between the amount of
information needed to state the commonality of A and B and the information needed to
fully describe what A and B are.
459
Computational Linguistics Volume 31, Number 4
If the features of a word are grammatical relation contexts, the similarity between
two words w1 and w2 can be written according to Lin?s measure as:
simlin(w1, w2) =
?
T(w1)?T(w2)(I(w1, c) + I(w2, c))
?
T(w1) I(w1, c) +
?
T(w2 ) I(w2, c)
(41)
where T(w) = {c : I(w, c) > 0}. There are parallels between simlin and simdice in that both
measures compute a ratio between what is shared by the descriptions of both nouns and
the sum of the descriptions of each noun. The major difference appears to be the use of
MI, and hence we predicted that there would be a close relationship between simlin and
the harmonic mean in the additive MI-based CRM. This relationship is shown below:
mh(Paddmi (w1, w2),Raddmi (w1, w2)) =
2 ? Paddmi (w1, w2) ? Raddmi (w1, w2)
Paddmi (w1, w2) +Raddmi (w1, w2)
(42)
=
2 ?
?
TP I(w1, c)
?
F(w1)
I(w1, c)
?
?
TP I(w2, c)
?
F(w2)
I(w2, c)
?
TP I(w1, c)
?
F(w1)
I(w1, c)
+
?
TP I(w2, c)
?
F(w2)
I(w2, c)
(43)
=
2 ?
?
TP I(w1, c) ?
?
TP I(w2, c)
?
TP I(w2, c) ?
?
F(w1 ) I(w1, c) +
?
TP I(w1, c) ?
?
F(w2 ) I(w2, c)
(44)
Now if
?
TP I(w1, c) =
?
TP I(w2, c), it follows:
mh(Paddmi (w1, w2),Raddmi (w1, w2)) =
2 ?
?
TP I(w1, c)
?
F(w1 ) I(w1, c) +
?
F(w2) I(w2, c)
(45)
=
?
TP I(w1, c) + I(w2, c)
?
F(w1 ) I(w1, c) +
?
F(w2 ) I(w2, c)
(46)
=
?
T(w1)?T(w2 )(I(w1, c) + I(w2, c))
?
T(w1 ) I(w1, c) +
?
T(w2) I(w2, c)
since T(w) = F(w) (47)
= simlin(w1, w2) (48)
Thus, when the additive MI-based model is used, ? = 1 and the condition
?
TP I(w1, c) =
?
TP I(w2, c) holds, the CR framework reduces to simlin. However, this
last necessary condition for equivalence is not one we can expect to hold for many (if
any) pairs of words. In order to investigate how good an approximation the harmonic
mean is to simlin in practice, we compared neighbor sets according to each measure
using the neighbor set comparison technique outlined earlier.
Figure 2 illustrates the variation in mean similarity between neighbor sets with the
parameters ? and ?. At ? = 1, the average similarity between neighbor rankings was
0.967 for high-frequency nouns and 0.923 for low-frequency nouns. This is significantly
higher than similarities between other standard similarity measures. However, the
optimal approximation of simlin was found using ? = 0.75 and ? = 0.5 in the additive
MI-based CRM. With these settings, the development set similarity was 0.987 for high-
460
Weeds and Weir Co-occurrence Retrieval
frequency nouns and 0.977 for low-frequency nouns. This suggests that simlin allows
more compensation for lack of recall by precision and vice versa than the harmonic
mean.
4.7 Discussion
We have seen that five of the existing lexical distributional similarity measures are (ap-
proximately) equivalent to settings within the CR framework and for one other, a weak
approximation can be made. The CR framework, however, more than simulates existing
measures of distributional similarity. It defines a space of distributional similarity mea-
sures that is already populated with a few named measures. By exploring the space, we
can discover the desirable characteristics of distributional similarity measures. It may be
that the most useful measure within this space has already been discovered, or it may
be that a new optimal combination of characteristics is discovered. The primary goal,
however, is to understand how different characteristics relate to high performance in
different applications and thus explain why one measure performs better than another.
With this goal in mind, we now turn to the applications of distributional similarity.
In the next section, we consider what characteristics of distributional similarity mea-
sures are desirable in two different application areas: (1) automatic thesaurus generation
and (2) language modeling.
5. Application-Based Evaluation
As discussed by Weeds (2003), evaluation is a major problem in this area of research.
In some areas of natural language research, evaluation can be performed against a
gold standard or against human plausibility judgments. The first of these approaches
is taken by Curran and Moens (2002), who evaluate a number of different distributional
similarity measures and weight functions against a gold standard thesaurus compiled
from Roget?s, the Macquarie thesaurus, and the Moby thesaurus. However, we argue that
this approach can only be considered when distributional similarity is required as an
approximation to semantic similarity and that, in any case, it is not ideal since it is not
Figure 2
Variation (with parameters ? and ?) in development set mean similarity between neighbor sets
of the additive MI-based CRM and of simlin.
461
Computational Linguistics Volume 31, Number 4
clear that there is a single ?right answer? as to which words are most distributionally
similar. The best measure of distributional similarity will be the one that returns the
most useful neighbors in the context of a particular application and thus leads to the
best performance in that application. This section investigates whether the desirable
characteristics of a lexical distributional similarity measure in an automatic thesaurus
generation task (WordNet prediction) are the same as those in a language modeling task
(pseudo-disambiguation).
5.1 WordNet Prediction Task
In this section, we evaluate the ability of distributional similarity measures to predict
semantic similarity by making comparisons with WordNet. An underlying assumption
of this approach is that WordNet is a gold standard for semantic similarity, which, as
is discussed by Weeds (2003), is unrealistic. However, it seems reasonable to suppose
that a distributional similarity measure that more closely predicts a semantic measure
based on WordNet is more likely to be a good predictor of semantic similarity. We chose
WordNet as our gold standard for semantic similarity since, as discussed by Kilgarriff
and Yallop (2000), distributional similarity scores calculated over grammatical relation
level context tend to be more similar to tighter thesauri, such as WordNet, than looser
thesauri such as Roget?s.
5.1.1 Experimental Set-Up. There are a number of ways to measure the distance be-
tween two nouns in the WordNet noun hierarchy (see Budanitsky [1999] for a review).
In previous work (Weeds and Weir 2003b), we used the WordNet-based similarity
measure first proposed in Lin (1997) and used in Lin (1998a):
wn simlin(w1, w2) =
max
c1?S(w1 )?c2?S(w2 )
(
max
c?sup(c1 )?sup(c2 )
2 log P(c)
log (P(c1)) + log (P(c2))
)
(49)
where S(w) is the set of senses of the word w in WordNet, sup(c) is the set of possibly in-
direct super-classes of concept c in WordNet, and P(c) is the probability that a randomly
selected word refers to an instance of concept c (estimated over some corpus such as
SemCor [Miller et al 1994]).
However, in other research (Budanitsky and Hirst 2001; Patwardhan, Banerjee, and
Pedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distance
measure of Jiang and Conrath (1997) (referred to herein as the ?JC measure?) is a
superior WordNet-based semantic similarity measure:
wn distJC(w1, w2) =
max
c1?S(w1 )?c2?S(w2 )
(
max
c?sup(c1 )?sup(c2 )
2 log(c) ? log P(c1) ? log P(c2)
)
(50)
In our work, we make an empirical comparison of neighbors derived using a
WordNet-based measure and each of the distributional similarity measures using the
technique discussed in Section 3. We have carried out the same experiments using both
the Lin measure and the JC measure. Correlation between distributional similarity mea-
sures and the WordNet measure tends to be slightly higher when using the JC measure
462
Weeds and Weir Co-occurrence Retrieval
Table 5
Optimized similarities between distributional neighbor sets and WordNet derived neighbor sets.
Noun Frequency
high low
Optimal Devel Test Optimal Devel Test
Parameters Corr. Corr. Parameters Corr. Corr.
Measure ? ? (C) (C) ? ? (C) (C)
simaddtype 0.25 0.5 0.323 0.327 0.5 0.25 0.281 0.275
simaddtok 0.25 0.3 0.302 0.310 0.25 0.0 0.266 0.263
simaddmi 0.25 0.2 0.334 0.342 0.25 0.2 0.290 0.283
simaddwmi 0.25 0.2 0.282 0.293 0.25 0.0 0.274 0.266
simaddt 0.5 0.2 0.330 0.338 0.5 0.2 0.292 0.286
simaddz 0.5 0.1 0.324 0.332 0.5 0.1 0.280 0.276
simaddallr 0.25 0.2 0.298 0.304 0.25 0.1 0.272 0.267
simdwtype 0.0 0.4 0.306 0.310 0.0 0.0 0.221 0.219
simdwtok ? ? 0.285 0.294 ? ? 0.212 0.211
simdwmi 0.0 0.2 0.324 0.333 0.0 0.0 0.266 0.261
simdwwmi 0.0 0.1 0.273 0.281 0.0 0.1 0.223 0.220
simdwt 0.5 0.2 0.328 0.333 0.5 0.3 0.289 0.282
simdwz 0.5 0.1 0.324 0.329 0.5 0.2 0.280 0.276
simdwallr 0.75 0.2 0.263 0.265 0.75 0.0 0.226 0.225
simdice 0.295 0.299 0.123 0.123
simjacc 0.295 0.299 0.123 0.123
distL1 0.285 0.294 0.212 0.211
dist? 0.310 0.317 0.289 0.281
simhind 0.320 0.326 0.267 0.261
simlin 0.313 0.323 0.192 0.186
wnsimlin 0.907 0.907 0.884 0.883
(percentage increase in similarity of approximately 10%), but the relative differences
between distributional similarity measures remain approximately the same. Here, for
brevity, we present results just using the JC measure.
5.1.2 Results. As before, we present the results separately for the 1,000 high-frequency
target nouns and for the 1,000 low-frequency target nouns. Table 5 shows the optimal
parameter settings for each CRM (computed over the development set) and the mean
similarities with the JC measure at these settings in both the development set and the
test set. It also shows the mean similarities over the development set and the test set
for each of the existing similarity measures discussed in Section 4. For reference, we
also present the mean similarity for the WordNet-based measure wn simlin. For ease
463
Computational Linguistics Volume 31, Number 4
Figure 3
Bar chart illustrating test set similarity with WordNet for each distributional similarity measure.
of comparison, the test set correlation values for each distributional measure are also
illustrated in Figure 3.
We would expect a mean overlap score of 0.08 by chance. Standard deviations in
the observed test set mean similarities were all less than 0.1, and thus any difference
between mean scores of greater than 0.016 is significant at the 99% level, and differences
greater than 0.007 are significant at the 90% level. Thus, from the results in Table 5 we
can make the following observations.
First, the best-performing distributional similarity measures, in terms of WordNet
prediction, for both high- and low-frequency nouns, are the MI-based and the t-test
based CRMs. The additive MI-based CRM performs the best for high-frequency nouns
and the additive t-test based CRM performs the best for low-frequency nouns. How-
ever, the differences between these models are not statistically significant. These CRMs
perform substantially better than all of the unparameterized distributional similarity
measures, of which the best performing are simhind and simlin for high-frequency nouns
and dist?1 for low-frequency nouns. Second, the difference-weighted versions of each
model generally perform slightly worse than their additive counterparts. Thus, the
difference in extent to which each word occurs in each context does not appear to be
a factor in determining semantic similarity. Third, all of the measures perform signifi-
cantly better for high-frequency nouns than for low-frequency nouns. However, some of
the measures (simlin, simjacc and simdice) perform considerably worse for low-frequency
nouns.
We now consider the effects of ? and ? in the CRMs on performance. The pattern
of variation across the CRMs was very similar. This pattern is illustrated using one of
the best-performing CRMs (simaddmi ) in Figure 4. With reference to this figure and to the
results for the other models (not shown), we make the following observations.
464
Weeds and Weir Co-occurrence Retrieval
Figure 4
Variation in similarity with WordNet with respect to ? and ? for the additive MI-based CRM.
First, for high- and low-frequency nouns, similarity with WordNet is higher for
low values of ? than for high values of ?. In other words, neighbors according to
the WordNet based measure tend to have high-recall retrieval of the target noun?s
co-occurrences. Second, a high value of ? leads to high performance for high-frequency
nouns but poor performance for low-frequency nouns. This suggests that WordNet-
derived neighbors of high-frequency target nouns also have high-precision retrieval
of the target noun?s co-occurrences, whereas the WordNet-derived neighbors of low-
frequency target nouns do not. This also explains why particular existing measures
(Jaccard?s / the Dice Coefficient and Lin?s Measure), which are very similar to a ? = 1
setting in the CR framework, perform well for high-frequency nouns but poorly for
low-frequency nouns.
5.1.3 Discussion. Our results in this section are comparable to those of Curran and
Moens (2002), who showed that combining the t-test with Jaccard?s coefficient outper-
formed combining MI with Jaccard?s coefficient by approximately 10% in a comparison
against a gold-standard thesaurus. However, we do not find a significant difference
between using the t-test and MI in similarity calculation. Further, we found that using
a combination of precision and recall weighted towards recall performs substantially
better than using the harmonic mean (which is equivalent to Jaccard?s measure). In our
experiments, the development-set similarity using the harmonic mean in the additive
MI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns,
and the development-set similarity using the harmonic mean in the additive t-test based
CRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns.
5.2 Pseudo-Disambiguation Task
Pseudo-disambiguation tasks have become a standard evaluation technique (Gale,
Church, and Yarowsky 1992; Schu?tze 1992; Pereira, Tishby, and Lee 1993; Schu?tze 1998;
Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al 1999; Even-
Zohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, we
may use a noun?s neighbors to decide which of two co-occurrences is the most likely.
Although pseudo-disambiguation is an artificial task, it has relevance in at least two
application areas. First, by replacing occurrences of a particular word in a test suite with
465
Computational Linguistics Volume 31, Number 4
a pair of words from which a technique must choose, we recreate a simplified version
of the word sense disambiguation task; that is, we choose between a fixed number
of homonyms based on local context. The second is in language modeling where we
wish to estimate the probabilities of co-occurrences of events but, due to the sparse
data problem, it is often the case that a possible co-occurrence has not been seen in the
training data.
5.2.1 Experimental Set-up. A typical approach to performing pseudo-disambiguation
is as follows. A large set of noun-verb direct-object pairs is extracted from a corpus,
of which a portion is used as test data and another portion is used as training data.
The training data can be used to construct a language model and/or determine the
distributionally nearest neighbors of each noun. Noun-verb pairs (n, v1) in the test data
are replaced with noun-verb-verb triples (n, v1, v2) and the task is to decide which of the
two verbs is the most likely to take the noun as its direct object. Performance is usually
measured as error rate. We will now discuss the details of our own experimental set-up.
As already discussed (Section 3), 80% of the noun-verb direct-object data extracted
from the BNC for each of 2,000 nouns was used to compute the similarity between
nouns and is also used as the language model in the pseudo-disambiguation task, and
20% of the data was set aside as test data, providing unseen co-occurrences for this
pseudo-disambiguation task.
In order to construct the test set from the test data, we took all10 of the test data set
aside for each target noun and modified it as follows. We converted each noun-verb pair
(n, v1) in the test data into a noun-verb-verb triple (n, v1, v2). v2 was randomly selected
from the verbs that have the same frequency, calculated over all the training data, as
v1 plus or minus 1. If there are no other verbs within this frequency range, then the
test instance is discarded. This method ensures that there is no systematic bias towards
v2 being of a higher or lower frequency than v1. We also ensured that (n, v2) has not
been seen in the test or training data. Ten test instances11 were then selected for each
target noun in a two-step process of (1) while more than ten triples remained, discarding
duplicate triples and (2) randomly selecting ten triples from those remaining after
step 1. At this point, we have 10,000 test instances pertaining to high-frequency nouns
and 10,000 test instances pertaining to low-frequency nouns, and there are no biases
towards the higher-frequency or lower-frequency nouns within these sets. Each of these
sets was split into five disjoint subsets, each containing two instances for each target
noun. We use these five subsets in two ways. First, we perform five-fold cross validation.
In five-fold cross validation, we compute the optimal parameter settings in four of the
subsets and the error rate at this optimal parameter setting in the remaining subset. This
is repeated five times with a different subset held out each time. We then compute an
average optimal error rate. We cannot, however, compute an average optimal parameter
setting, since this would assume a convex relationship between parameter settings and
error rate. In order to study the relationship between parameter settings and error
rate, we combine three of the sets to form a development set and two of the sets to
form a test set. The development set is used to optimize parameters and the test set
10 Unlike Lee (1999), we do not delete instances from the test data that occur in the training data. This
is discussed in detail in (Weeds 2003), but our main justification for this approach is that a single
co-occurrence of (n, v1) compared to zero co-occurrences of (n, v2) is not necessarily sufficient evidence
to conclude that the population probability of (n, v1) is greater than that of (n, v2).
11 Ten being less than the minimum number (14) of (possibly) indistinct co-occurrences for any target noun
in the original test data.
466
Weeds and Weir Co-occurrence Retrieval
to determine error rates at the optimal settings. In graphs showing the relationship
between error rate and parameter settings, it is the error rate in this development set
that is shown. In the case of the CRMs, the parameters that are optimized are ?, ?, and
k (the number of nearest neighbors).12 For the existing measures, the only parameter
to be optimized is k.
Having constructed the test sets, the task is to take each test instance (n, v1, v2) and
use the nearest neighbors of noun n (as computed from the training data) to decide
which of (n, v1) and (n, v2) was the original co-occurrence. Each of n?s neighbors, m, is
given a vote that is equal to the difference in frequencies of the co-occurrences (m, v1)
and (m, v2) and that it casts to the verb with which it occurs most frequently. Thus,
we distinguish between cases where a neighbor occurs with each verb approximately
the same number of times and where a neighbor occurs with one verb significantly
more often than the other. The votes for each verb are summed over all of the k nearest
neighbors of n, and the verb with the most votes wins. Performance is measured as
error rate.
Error rate = 1T
(
# of incorrect choices + # of ties2
)
(51)
where T is the number of test instances and a tie results when the neighbors cannot
decide between the two alternatives.
5.2.2 Results. In this section, we present results on the pseudo-disambiguation task for
all of the CRMs described in Section 2. We also compare the results with the six existing
distributional similarity measures (Section 4) and the two WordNet-based measures
(Section 5.1).
A baseline for these experiments is the performance obtained by a technique that
backs-off to the unigram probabilities of the verbs being disambiguated. By construction
of the test set, this should be approximately 0.5. The actual empirical figures are 0.553
for the high-frequency noun test set and 0.586 for the low-frequency noun test set. The
deviation from 0.5 is due to the unigram probabilities of the verbs not being exactly
equal and to their being calculated over a larger data set than just the training data
for the 2,000 target nouns. These baseline error-rates are also different from what is
observed when all 1,999 potential neighbors are considered. In this case, we obtain
an error rate of 0.6885 for the high-frequency noun test set and 0.6178 for the low-
frequency noun test set. These differences are due to the fact that the correct choice
verb, but not the incorrect choice verb, has occurred, possibly many times, with the
target noun in the training data, but a noun is not considered as a potential neighbor of
itself.
The results are summarized in Table 6. The table gives the average optimal error
rates for each measure, and for high- and low-frequency nouns, calculated using five-
fold cross validation. For ease of comparison, the cross-validated average optimal error
rates are illustrated in Figure 5. Standard deviation in the mean optimal error rate across
the five folds was always less than 0.15 and thus differences greater than 0.028 are
significant at the 99% level and differences greater than 0.012 are significant at the 90%
level. From the results, we make the following observations.
12 We also experimented with optimizing a similarity threshold t, but found that optimizing k gave better
results (Weeds 2003).
467
Computational Linguistics Volume 31, Number 4
Table 6
Mean optimal error rates using five-fold cross-validation (when optimizing k, ? and ?).
Noun Frequency Measure Noun Frequency
Measure high low high low
simaddtype 0.196 0.197 sim
dw
type 0.214 0.185
simaddtok 0.219 0.241 sim
dw
tok 0.234 0.202
simaddmi 0.178 0.169 sim
dw
mi 0.187 0.176
simaddwmi 0.173 0.192 sim
dw
wmi 0.171 0.192
simaddt 0.154 0.172 sim
dw
t 0.163 0.186
simaddz 0.164 0.183 sim
dw
z 0.167 0.193
simaddallr 0.170 0.211 sim
dw
allr 0.171 0.215
simdice 0.215 0.204 simjacc 0.215 0.204
distL1 0.234 0.202 dist?1 0.230 0.192
simhind 0.201 0.18 simlin 0.193 0.181
wn simlin 0.295 0.294 wn distJC 0.302 0.295
baseline 0.553 0.586
First, the best measure appears to be the additive t-test based CRM. This signifi-
cantly outperforms all but one (the z-test based CRM) of the other measures for high-
frequency nouns. For low-frequency nouns, slightly higher performance is obtained
using the additive MI-based CRM. This difference, however, is not statistically signifi-
cant. Second, all of the distributional similarity measures perform considerably better
than the WordNet-based measures13 at this task for high- and low-frequency nouns.
Third, for many measures, performance over high-frequency nouns is not significantly
higher (and is in some cases lower) than over low-frequency nouns. This suggests that
distributional similarity can be used in language modeling even when there is relatively
little corpus data over which to calculate distributional similarity.
We now consider the effects of the different parameters on performance. Since we
use the development set to determine the optimal parameters, we consider performance
on the development set as each parameter is varied. Table 7 shows the optimized pa-
rameter settings in the development set, error rate at these settings in the development
set, and error rate at these settings in the test set. For the CRMs, we considered how the
performance varies with each parameter when the other parameters are held constant
at their optimum values. Figure 6 shows how performance varies with ?, and Figure 7
shows how performance varies with ? for the additive and difference-weighted t-test
based and MI-based CRMs. For reference, the optimal error rates for the best performing
existing distributional similarity measure (simlin) is also shown as a straight line on each
graph.
We do not show the variation with respect to k for any of the measures, but this was
fairly similar for all measures and is as would be expected. To begin with, considering
13 However, for this task, in contrast to earlier work, wn simlin gives slightly, although insignificantly, better
performance than wn distJC.
468
Weeds and Weir Co-occurrence Retrieval
Figure 5
Bar chart illustrating cross-validated optimal error rates for each measure when k is optimised.
more neighbors increases performance, since more neighbors allow decisions to be
made in a greater number of cases. However, when k increases beyond an optimal
value, a greater number of these decisions will be in the wrong direction, since these
words are not very similar to the target word, leading to a decrease in performance. In a
small number of cases (when using the ALLR-based CRMs or the WMI-based CRMs for
high frequency nouns), performance peaks at k = 1. This suggests that these measures
may be very good at finding a few very close neighbors.
The majority of models, including the additive t-test based and additive MI-based
CRMs, perform significantly better at low values of ? (0.25-0.5) and high values of ?
(around 0.8). This indicates that a potential neighbor with high-precision retrieval of
informative features is more useful than one with high-recall retrieval. In other words,
it seems that it is better to sacrifice being able to make decisions on every test instance
with a small number of neighbors in favor of not having neighbors that predict incorrect
verb co-occurrences. This also suggests why we saw fairly low performance by the ?-
skew divergence measure on this task, since it is closest to a high-recall setting in the
additive t-test based model. The low values of ? indicate that a combination of precision
and recall that is closer to a weighted arithmetic mean is generally better than one that
is closer to an unweighted harmonic mean. However, this does not hold for the t-test
based CRMs for low-frequency nouns. Here a higher value of ? is optimal, indicating
that, in this case, requiring both recall and precision results in high performance.
6. Conclusions and Future Directions
Our main contribution is the development of a framework, first presented in a prelim-
inary form in Weeds and Weir (2003b), that is based on the concept of lexical substi-
469
Computational Linguistics Volume 31, Number 4
Table 7
Summary of results on pseudo-disambiguation task when optimizing ?, ? and k.
Noun Frequency
high low
Optimal Devel. Test Optimal Devel. Test
Parameters Error Error Parameters Error Error
Measure ? ? k ? ? k
simaddtype 0.25 0.8 150 0.193 0.193 0.25 0.75 100 0.192 0.200
simaddtok 0 0.8 250 0.211 0.224 0.5 0.1 130 0.234 0.233
simaddmi 0.25 0.8 170 0.175 0.186 0.5 0.8 120 0.169 0.178
simaddwmi 0.0 1.0 1 0.175 0.169 0.75 0.0 100 0.183 0.182
simaddt 0.25 0.8 190 0.153 0.155 0.5 0.7 110 0.165 0.176
simaddz 0.25 0.7 40 0.165 0.163 0.5 1.0 250 0.174 0.188
simaddallr 0.0 0.9 1 0.170 0.169 0.25 0.6 90 0.204 0.210
simdwtype 0 0.6 50 0.208 0.215 0.25 0.3 190 0.177 0.188
simdwtok n/a n/a 60 0.227 0.234 n/a n/a 50 0.194 0.206
simdwmi 0.25 0.8 100 0.181 0.193 0.5 0.7 160 0.172 0.173
simdwwmi 0.0 0.0 1 0.172 0.170 0.25 0.1 450 0.183 0.190
simdwt 0.5 0.8 120 0.156 0.165 0.75 0.6 250 0.179 0.187
simdwz 0.5 0.7 50 0.166 0.171 0.75 0.9 400 0.187 0.199
simdwallr 0.0 0.9 1 0.171 0.169 0.5 1.0 180 0.208 0.212
simlin n/a n/a 50 0.190 0.199 n/a n/a 80 0.179 0.186
tutability. Here, we cast the problem of measuring distributional similarity as one of
co-occurrence retrieval (CR), for which we can measure precision and recall by analogy
with the way they are measured in document retrieval. This CR framework has then
allowed us to systematically explore various characteristics of distributional similarity
measures.
First, we asked whether lexical substitutability is necessarily symmetric. To this
end, we have explored the merits of symmetry and asymmetry in a similarity measure
by varying the relative importance attached to precision and recall. We have seen that
as the distribution of word B moves away from being identical to that of word A, its
similarity with A can decrease along one or both of two dimensions. When B occurs in
contexts that word A does not, precision is lost but B may remain a high-recall neighbor
of word A. When B does not occur in contexts that A does, recall is lost but B may remain
a high-precision neighbor of word A. Through our experimental work, which is more
thorough than that presented in Weeds and Weir (2003b), we have shown that the kind
of neighbor preferred appears to depend on the application in hand. High-precision
neighbors were more useful in the language modeling task of pseudo-disambiguation
and high-recall neighbors were more highly correlated with WordNet-derived neighbor
sets. Thus, similarity appears to be inherently asymmetric. Further, it would seem
470
Weeds and Weir Co-occurrence Retrieval
Figure 6
Performance of CRMs with respect to ? (at optimal values of k and ?).
unlikely that any single, unparameterized measure of distributional similarity would
be able to do better on both tasks.
Second, we asked whether all contexts are equally important in the calculation of
distributional similarity. To this end, we have explored the way in which frequency
information is utilized using different co-occurrence retrieval models (CRMs). Using
different weight functions, we have investigated the relative importance of different
co-occurrence types. In earlier work (Weeds and Weir 2003b), we saw that using MI
to weight features gave improved performance on the two evaluation tasks over type-
based or token-based CRMs. Here, we have seen that further gains can be made by using
the t-test as a weight function. This leads to significant improvements on the pseudo-
disambiguation task for all nouns and marginal improvements on the WordNet predic-
tion task for low-frequency nouns. To some extent, this supports the findings of Curran
and Moens (2002), who investigated a number of weight functions for distributional
similarity and showed that the t-test performed better than a number of other weight
functions including MI.
Third, we asked whether it is necessary to consider the difference in extent to which
each word appears in each context. To this end, we have herein proposed difference-
Figure 7
Performance of CRMs with respect to ? (at optimal values of k and ?).
471
Computational Linguistics Volume 31, Number 4
weighted versions of each model in which the similarity of two words in respect
of an individual feature is defined using the same principles that we use to define
the similarity of two words in respect of all their features. We have compared these
difference-weighted CRMs to their additive counterparts and shown that difference-
weighting does not seem to be a major factor and does not improve results when using
the best-performing CRMs.
Another important contribution of this work on co-occurrence retrieval is a better
understanding of existing distributional similarity measures. By comparing existing
measures with the CR framework, we can analyze their CR characteristics. As discussed
in Weeds and Weir (2003b), the Dice Coefficient and Jaccard?s Coefficient are exactly
simulated by ? = 1 in the additive type-based model and Lin?s Measure is almost
equivalent to the harmonic mean of precision and recall in the additive MI-based
model. Here, we also show that the L1 Norm is exactly simulated by the (unparame-
terized) difference-weighted token-based model, Hindle?s Measure is exactly simulated
by ? = 0,? = 0 in the additive MI-based model, and the ?-skew divergence measure
is most similar to high-recall settings in the additive t-test based CRM. Knowing that
Lin?s Measure is almost equivalent to the harmonic mean of precision and recall in
the additive MI-based model explains why this measure does badly on the WordNet
prediction task for low-frequency nouns. We have seen that recall is more important
than precision in the WordNet prediction task, whereas the nearest neighbors of a target
noun according to Lin?s Measure have both high precision and high recall. Conversely,
knowing that the ?-skew divergence measure is most closely approximated by high-
recall settings in the additive t-test based model explains why this measure performs
poorly on the pseudo-disambiguation task, since we have seen that high precision is
required for optimal performance on this task.
Finally, our evaluation of measures has been performed over a set of 2,000 nouns,
and we have shown that the performance of distributional similarity techniques for
low-frequency nouns is not significantly lower than for high-frequency nouns. This
suggests that distributional techniques might be used even when there is relatively little
data available. In the distributional domain, this means that we can use probability
estimation techniques for rare words with greater confidence. In the semantic domain,
we might be able to use distributional techniques to extend existing semantic resources
to cover rare or new words or automatically generate domain-, genre-, or dialect-specific
resources.
There are a number of major directions in which this work can be extended. First,
although the set of CRMs defined here is more extensive than that defined in Weeds and
Weir (2003b), it is still not exhaustive, and other models might be proposed. Further,
it would be interesting to combine CRMs with the feature reweighting scheme of
Geffet and Dagan (2004). These authors compare distributional similarity scores with
human judgments of semantic entailment and show that substantial (approximately
10%) improvements over using Lin?s Measure can be achieved by first calculating
similarity using Lin?s Measure and then recalculating similarity using a relative feature
focus score, which indicates how many of a word?s nearest neighbors shared that
feature.
Second, there are other potential application-based tasks that could be used to
evaluate CRMs and distributional similarity methods in general. In particular, we see
potential for the use of distributional similarity methods in prepositional phrase attach-
ment ambiguity resolution. This task has been previously tackled using semantic classes
to predict what is ultimately distributional information. Accordingly, we believe that it
should be possible to do better using the CR framework.
472
Weeds and Weir Co-occurrence Retrieval
Finally, in order to be able to truly rival manually generated thesauri, distri-
butional techniques need to be able to distinguish between different semantic relations
such as synonymy, antonymy, and hyponymy. These are important linguistic distinc-
tions, particularly in the semantic domain, since we are unlikely, say, to want to replace
a word with its antonym. Weeds, Weir, and McCarthy (2004) give preliminary results on
the the use of precision and recall to distinguish between hypernyms and hyponyms in
sets of distributionally related words.
Acknowledgments
This research was supported by an
Engineering and Physical Sciences Research
Council (EPSRC) studentship to the first
author. The authors would like to thank John
Carroll, Mirella Lapata, Adam Kilgarriff, Bill
Keller, Steve Clark, James Curran, Darren
Pearce, Diana McCarthy, and Mark
McLauchlan for helpful discussions and
insightful comments throughout the course
of the research. We would also like to thank
the anonymous reviewers of this paper for
their comments and suggestions.
References
Bernard, John R. L., editor. 1990. The
Macquarie Encyclopedic Thesaurus. The
Macquarie Library, Sydney, Australia.
Briscoe, Edward and John Carroll. 1995.
Developing and evaluating a probabilistic
LR parser of part-of-speech and
punctuation labels. In Proceedings
of the 4th ACL/SIGDAT International
Workshop on Parsing Technologies,
pages 48?58, Cambridge, MA.
Brown, Peter F., Vincent J. Della Pietra, Peter
V. deSouza, Jenifer C. Lai, and Robert L.
Mercer. 1992. Class-based n-gram models
of natural language. Computational
Linguistics, 18(4):467?479.
Budanitsky, Alexander. 1999. Lexical Semantic
Relatedness and its Application in Natural
Language Processing. Ph.D. thesis,
University of Toronto, Ontario.
Budanitsky, Alexander and Graeme Hirst.
2001. Semantic distance in WordNet: An
experimental, application-oriented
evaluation of five measures. In Proceedings
of the NAACL-01 Workshop on WordNet
and Other Lexical Resources, Pittsburgh, PA.
Caraballo, Sharon. 1999. Automatic
construction of a hypernym-labelled noun
hierarchy from text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics (ACL-99),
pages 120?126, College Park, MA.
Carroll, John and Edward Briscoe. 1996.
Apportioning development effort in a
probabilistic LR parsing system through
evaluation. In Proceedings of the ACL/
SIGDAT Conference on Empirical Methods in
Natural Language Processing (EMNLP96),
pages 92?100, Santa Cruz, CA.
Church, Kenneth W., William Gale, Patrick
Hanks, Donald Hindle, and Rosamund
Moon. 1994. Lexical substitutability.
In B. T. S. Atkins and A. Zampolli, editors.
Computational Approaches to the Lexicon.
Oxford University Press, Oxford,
pages 153?177.
Church, Kenneth W. and Patrick Hanks.
1989. Word association norms, mutual
information and lexicography. In
Proceedings of the 27th Annual Conference
of the Association for Computational
Linguistics (ACL-89), pages 76?82,
Vancouver, British Columbia.
Clark, Stephen and David Weir. 2000.
A class-based probabilistic approach to
structural disambiguation. In
Proceedings of the 18th International
Conference on Computational Linguistics
(COLING-00), pages 194?200, Saarbrucken,
Germany.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using
a semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Cover, T. M. and J. A. Thomas. 1991.
Elements of Information Theory. Wiley,
New York.
Curran, James R. and Marc Moens. 2002.
Improvements in automatic thesaurus
extraction. In Proceedings of the
ACL-SIGLEX Workshop on Unsupervised
Lexical Acquisition, pages 59?67,
Philadelphia, PA.
Dagan, Ido, Lillian Lee, and Fernando
Pereira. 1999. Similarity-based models of
word co-occurrence probabilities. Machine
Learning Journal, 34(1?3):43?69.
Dagan, Ido, S. Marcus, and S. Markovitch.
1993. Contextual word similarity and
estimation from sparse data. In Proceedings
of the 35th Annual Meeting of the Association
for Computational Linguistics (ACL-93),
pages 56?63, Columbus, OH.
Even-Zohar, Yair and Dan Roth. 2000. A
classification approach to word prediction.
473
Computational Linguistics Volume 31, Number 4
In Proceedings of the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-00),
pages 124?131, Pittsburg, PA.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Firth, John Rupert. 1957. A synopsis of
linguistic theory 1930-1955. In Studies in
Linguistic Analysis, pages 1?32, Philogical
Society, Oxford. Reprinted in Palmer, F.
(ed.), 1968 Selected Papers of J. R. Firth,
Longman, Harlow.
Fontenelle, Thierry, Walter Bruls, Luc
Thomas, Tom Vanallemeersch, and Jacques
Jansen. 1994. DECIDE, MLAP-Project
93-19, deliverable D-1a: a survey of
collocation extraction tools. Technical
report, University of Liege, Belgium.
Frakes, W. B. and R. Baeza-Yates, editors.
1992. Information Retrieval, Data Structures
and Algorithms. Prentice Hall, New York.
Fung, Pascale and Kathleen McKeown. 1997.
A technical word- and term-translation aid
using noisy parallel corpora across
language groups. Machine Translation,
12(1?2):53?87.
Gale, William, Kenneth W. Church, and
David Yarowsky. 1992. Work on statistical
methods for word sense disambiguation.
In Working notes of the AAAI symposium on
probabilistic approaches to natural language,
pages 54?60, Menlo Park, CA.
Geffet, Maayan and Ido Dagan. 2004. Feature
vector quality and distributional similarity.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING-04), pages 247?253, Geneva,
Switzerland.
Golding, Andrew R. and Dan Roth. 1999. A
winnow-based approach to context-
sensitive spelling correction. Machine
Learning, 34(1?3):182?190.
Grefenstette, Gregory. 1994. Corpus-derived
first-, second- and third-order word
affinities. In Proceedings of Euralex,
pages 279?290, Amsterdam, Holland.
Harris, Zelig S. 1968. Mathematical Structures
of Language. John Wiley, New York.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics
(ACL-1990), pages 268?275, Pittsburgh, PA.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference on
Research in Computational Linguistics
(ROCLING X), Taiwan.
Kaufman, Leonard and Peter J. Rousseeuw.
1990. Finding Groups in Data: An
Introduction to Cluster Analysis. John Wiley,
New York.
Kilgarriff, Adam. 2003. Thesauruses for
natural language processing. In Proceedings
of the Joint Conference on Natural Language
Processing and Knowledge Engineering,
pages 5?13, Beijing, China.
Kilgarriff, Adam and Colin Yallop. 2000.
What?s in a thesaurus. In Second Conference
on Language Resources and Evaluation
(LREC-00), pages 1371?1379, Athens.
Kullback, S. and R.A. Leibler. 1951. On
information and sufficiency. Annals of
Mathematical Statistics, 22:79?86.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics (ACL-1999), pages 23?32,
College Park, MA.
Lee, Lillian. 2001. On the effectiveness of the
skew divergence for statistical language
analysis. Artificial Intelligence and Statistics,
pages 65?72.
Li, Hang. 2002. Word clustering and
disambiguation based on co-occurrence
data. Natural Language Engineering,
8(1):25?42.
Li, Hang and Naoki Abe. 1998. Generalizing
case frames using a thesaurus and the
MDL principle. Computational Linguistics,
24(2):217?244.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and 8th Conference
of the European Chapter of the Association for
Computational Linguistics (ACL-97),
pages 64?71, Madrid, Spain.
Lin, Dekang. 1998a. Automatic retrieval
and clustering of similar words. In
Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics
(COLING-ACL ?98), pages 768?774,
Montreal, Quebec.
Lin, Dekang. 1998b. An information-theoretic
definition of similarity. In Proceedings of
International Conference on Machine
Learning, Madison, WI.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and
Ming Zhou. 2003. Identifying synonyms
474
Weeds and Weir Co-occurrence Retrieval
among distributionally similar words. In
In Proceedings of the 18th International Joint
Conference on Artificial Intelligence
(IJCAI-03), pages 1492?1493.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
McCarthy, Diana, Rob Koeling, and Julie
Weeds. 2004. Ranking WordNet senses.
Technical Report 569, Department of
Informatics, University of Sussex,
Brighton.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL-04), pages 280?287,
Barcelona, Spain.
Miller, G., M. Chodorow, S. Landes, C. Leacock,
and R. Thomas. 1994. Using a semantic
concordance for sense identification. In
Proceedings of the ARPA Human Language
Technology Workshop, Plainsboro, NJ.
Pantel, Patrick and Dekang Lin. 2000.
Word-for-word glossing of contextually
similar words. In Proceedings of the
Conference on Applied Natural Language
Processing / 1st Meeting of the North
American Chapter of the Association
for Computational Linguistics (ANLP-
NAACL-00), pages 78?85, Seattle, WA.
Patwardhan, Siddharth, Satanjeev Banerjee,
and Ted Pedersen. 2003. Using measures of
semantic relatedness for word sense
disambiguation. In Proceedings of the 4th
International Conference on Intelligent Text
Processing and Computational Linguistics,
pages 241?257, Mexico City.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
similar words. In Proceedings of the 30th
Annual Meeting of the Association for
Computational Linguistics (ACL-93),
pages 183?190, Columbus, OH.
Pustejovsky, James. 1995. The generative
lexicon. MIT Press, Cambridge, MA.
Resnik, Philip. 1993. Selection and
Information: A Class-Based Approach
to Lexical Relationships. Ph.D. thesis,
University of Pennsylvania,
Philadelphia, PA.
Roget, Peter. 1911. Thesaurus of English
Words and Phrases. Longmans, Green and
Co., London.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated
lexicon via EM-based clustering. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL-99), pages 104?111, College Park,
MA.
Salton, Gerald and M. J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Schu?tze, Hinrich. 1992. Dimensions of
meaning. In Proceedings of Conference on
Supercomputing, pages 787?796,
Minneapolis, MN.
Schu?tze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
van Rijsbergen, C. J. 1979. Information
Retrieval, second edition. Butterworths,
London.
Weeds, Julie. 2003. Measures and Applications
of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex, Brighton.
Weeds, Julie and David Weir. 2003a.
Finding and evaluating sets of nearest
neighbours. In Proceedings of the 2nd
International Conference on Corpus
Linguistics, pages 879?888,
Lancaster, UK.
Weeds, Julie and David Weir. 2003b. A
general framework for distributional
similarity. In Proceedings of the Conference on
Empirical Methods in Natural Language
Processing (EMNLP-03), pages 81?88,
Sapporo, Japan.
Weeds, Julie, David Weir, and Diana
McCarthy. 2004. Characterising measures
of lexical distributional similarity. In
Proceedings of the 20th International
Conference on Computational Linguistics
(COLING-04), pages 1015?1021, Geneva,
Switzerland.
Xu, Jinxi and Bruce Croft. 1996. Query
expansion using local and global
disambiguation. In Proceedings of the 19th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR-96), pages 4?11, Zurich,
Switzerland.
475

 	
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 539?547,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Optimal Reduction of Rule Length
in Linear Context-Free Rewriting Systems
Carlos Go?mez-Rodr??guez1, Marco Kuhlmann2, Giorgio Satta3 and David Weir4
1 Departamento de Computacio?n, Universidade da Corun?a, Spain (cgomezr@udc.es)
2 Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se)
3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it)
4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk)
Abstract
Linear Context-free Rewriting Systems
(LCFRS) is an expressive grammar formalism
with applications in syntax-based machine
translation. The parsing complexity of an
LCFRS is exponential in both the rank
of a production, defined as the number of
nonterminals on its right-hand side, and a
measure for the discontinuity of a phrase,
called fan-out. In this paper, we present
an algorithm that transforms an LCFRS
into a strongly equivalent form in which
all productions have rank at most 2, and
has minimal fan-out. Our results generalize
previous work on Synchronous Context-Free
Grammar, and are particularly relevant for
machine translation from or to languages that
require syntactic analyses with discontinuous
constituents.
1 Introduction
There is currently considerable interest in syntax-
based models for statistical machine translation that
are based on the extraction of a synchronous gram-
mar from a corpus of word-aligned parallel texts;
see for instance Chiang (2007) and the references
therein. One practical problem with this approach,
apart from the sheer number of the rules that result
from the extraction procedure, is that the parsing
complexity of all synchronous formalisms that we
are aware of is exponential in the rank of a rule,
defined as the number of nonterminals on the right-
hand side. Therefore, it is important that the rules
of the extracted grammar are transformed so as to
minimise this quantity. Not only is this beneficial in
terms of parsing complexity, but smaller rules can
also improve a translation model?s ability to gener-
alize to new data (Zhang et al, 2006).
Optimal algorithms exist for minimising the size
of rules in a Synchronous Context-Free Gram-
mar (SCFG) (Uno and Yagiura, 2000; Zhang et al,
2008). However, the SCFG formalism is limited
to modelling word-to-word alignments in which a
single continuous phrase in the source language is
aligned with a single continuous phrase in the tar-
get language; as defined below, this amounts to
saying that SCFG have a fan-out of 2. This re-
striction appears to render SCFG empirically inad-
equate. In particular, Wellington et al (2006) find
that the coverage of a translation model can increase
dramatically when one allows a bilingual phrase to
stretch out over three rather than two continuous
substrings. This observation is in line with empir-
ical studies in the context of dependency parsing,
where the need for formalisms with higher fan-out
has been observed even in standard, single language
texts (Kuhlmann and Nivre, 2006).
In this paper, we present an algorithm that com-
putes optimal decompositions of rules in the for-
malism of Linear Context-Free Rewriting Systems
(LCFRS) (Vijay-Shanker et al, 1987). LCFRS was
originally introduced as a generalization of sev-
eral so-called mildly context-sensitive grammar for-
malisms. In the context of machine translation,
LCFRS is an interesting generalization of SCFG be-
cause it does not restrict the fan-out to 2, allow-
ing productions with arbitrary fan-out (and arbitrary
rank). Given an LCFRS, our algorithm computes a
strongly equivalent grammar with rank 2 and min-
539
imal increase in fan-out.1 In this context, strong
equivalence means that the derivations of the orig-
inal grammar can be reconstructed using some sim-
ple homomorphism (c.f. Nijholt, 1980). Our contri-
bution is significant because the existing algorithms
for decomposing SCFG, based on Uno and Yagiura
(2000), cannot be applied to LCFRS, as they rely
on the crucial property that components of biphrases
are strictly separated in the generated string: Given a
pair of synchronized nonterminal symbols, the ma-
terial derived from the source nonterminal must pre-
cede the material derived from the target nontermi-
nal, or vice versa. The problem that we solve has
been previously addressed by Melamed et al (2004),
but in contrast to our result, their algorithm does not
guarantee an optimal (minimal) increase in the fan-
out of the resulting grammar. However, this is essen-
tial for the practical applicability of the transformed
grammar, as the parsing complexity of LCFRS is ex-
ponential in both the rank and the fan-out.
Structure of the paper The remainder of the pa-
per is structured as follows. Section 2 introduces the
terminology and notation that we use for LCFRS.
In Section 3, we present the technical background
of our algorithm; the algorithm itself is discussed
in Section 4. Section 5 concludes the paper by dis-
cussing related work and open problems.
General notation The set of non-negative integers
is denoted by N. For i, j ? N, we write [i, j] to
denote the interval { k ? N | i ? k ? j }, and use
[i] as a shorthand for [1, i]. Given an alphabet V , we
write V ? for the set of all (finite) strings over V .
2 Preliminaries
We briefly summarize the terminology and notation
that we adopt for LCFRS; for detailed definitions,
see Vijay-Shanker et al (1987).
2.1 Linear, non-erasing functions
Let V be an alphabet. For natural numbers r ? 0
and f, f1, . . . , fr ? 1, a function
g : (V ?)f1 ? ? ? ? ? (V ?)fr ? (V ?)f
1Rambow and Satta (1999) show that without increasing
fan-out it is not always possible to produce even weakly equiv-
alent grammars.
is called a linear, non-erasing function over V of
type f1 ? ? ? ? ? fr ? f , if it can be defined by an
equation of the form
g(?x1,1, . . . , x1,f1?, . . . , ?xr,1, . . . , xr,fr?) = ?g ,
where ?g = ??g,1, . . . , ?g,f ? is an f -tuple of strings
over the variables on the left-hand side of the equa-
tion and symbols in V that contains exactly one oc-
currence of each variable. We call the value r the
rank of g, the value f its fan-out, and write ?(g)
and ?(g), respectively, to denote these quantities.
Note that, if we assume the variables on the left-
hand side of the defining equation of g to be named
according to the specific schema given above, then g
is uniquely determined by ?g.
2.2 Linear context-free rewriting systems
A linear context-free rewriting system (LCFRS)
is a construct G = (VN , VT , P, S), where: VN is
an alphabet of nonterminal symbols in which each
symbol A ? VN is associated with a value ?(A),
called its fan-out; VT is an alphabet of terminal
symbols; S ? N is a distinguished start symbol with
?(S) = 1; and P is a set of productions of the form
p : A? g(B1, B2, . . . , Br) ,
where A,B1, . . . , Br ? VN , and g is a linear, non-
erasing function over the terminal alphabet VT of
type ?(B1) ? ? ? ? ? ?(Br) ? ?(A). In a deriva-
tion of an LCFRS, the production p can be used to
transform a sequence of r tuples of strings, gener-
ated by the nonterminals B1, . . . , Br, into a single
?(A)-tuple of strings, associated with the nonter-
minal A. The values ?(g) and ?(g) are called the
rank and fan-out of p, respectively, and we write
?(p) and ?(p), respectively, to denote these quan-
tities. The rank and fan-out of G, written ?(G)
and ?(G), respectively, are the maximum rank and
fan-out among all of its productions. Given that
?(S) = 1, a derivation will associate S with a set of
one-component tuples of strings over VT ; this forms
the string language generated by G.
Example 1 The following LCFRS generates the
string language { anbncndn | n ? N }. We only
specify the set of productions; the remaining com-
540
ponents of the grammar are obvious from that.
S ? g1(R) g1(?x1,1, x1,2?) = ?x1,1x1,2?
R? g2(R) g2(?x1,1, x1,2?) = ?ax1,1b, cx1,2d?
R? g3 g3 = ??, ??
The functions g1 and g2 have rank 1; the function g3
has rank 0. The functions g2 and g3 have fan-out 2;
the function g1 has fan-out 1. 2
3 Technical background
The general idea behind our algorithm is to replace
each production of an LCFRS with a set of ?shorter?
productions that jointly are equivalent to the original
production. Before formalizing this idea, we first in-
troduce a specialized representation for the produc-
tions of an LCFRS.
We distinguish between occurrences of symbols
within a string by exploiting two different notations.
Let ? = a1a2 ? ? ? an be a string. The occurrence ai
in ? can be denoted by means of its position index
i ? [n], or else by means of its two (left and right)
endpoints, i?1 and i; here, the left (right) endpoint
denotes a boundary between occurrence ai and the
previous (subsequent) occurrence, or the beginning
(end) of the string ?. Similarly, a substring ai ? ? ? aj
of ? with i ? j can be denoted by the positions
i, i+ 1, . . . , j of its occurrences, or else by means of
its left and right endpoints, i? 1 and j.
3.1 Production representation
For the remainder of this section, let us fix an
LCFRS G = (VN , VT , P, S) and a production
p : A ? g(B1, . . . , Br) of G, with g defined as
in Section 2.1. We define
|p| = ?(g) +
?(g)?
i=1
|?g,i|.
Let $ be a fresh symbol that does not occur inG. We
define the characteristic string of the production p
as
?(p) = ?g,1$ ? ? ? $?g,?(g) ,
and the variable string of p as the string ?N (p) ob-
tained from ?(p) by removing all the occurrences of
symbols in VT .
Example 2 We will illustrate the concepts intro-
duced in this section using the concrete production
p0 : A? g(B1, B2, B3), where
?g = ?x1,1ax2,1x1,2, x3,1bx3,2? .
In this case, we have
?(p0) = x1,1ax2,1x1,2$x3,1bx3,2 , and
?N (p0) = x1,1x2,1x1,2$x3,1x3,2 . 2
Let I be an index set, I ? [r]. Consider the set B of
occurrences Bi in the right-hand side of p such that
i ? I .2 We define the position set of B, denoted
by ?B, as the set of all positions 1 ? j ? |?N (p)|
such that the jth symbol in ?N (p) is a variable of the
form xi,h, for i ? I and some h ? 1.
Example 3 Some position sets of p0 are
?{B1} = {1, 3} ,?{B2} = {2} ,?{B3} = {5, 6} .
2
A position set ?B can be uniquely expressed as the
union of f ? 1 intervals [l1 + 1, r1], . . . , [lf + 1, rf ]
such that ri?1 < li for every 1 < i ? f . Thus we
define the set of endpoints of ?B as
?B = { lj | j ? [f ] } ? { rj | j ? [f ] } .
The quantity f is called the fan-out of ?B, writ-
ten ?(?B). Notice that the fan-out of a position set
?{B} does not necessarily coincide with the fan-out
of the non-terminal B in the underlying LCFRS. A
set with 2f endpoints always corresponds to a posi-
tion set of fan-out f .
Example 4 For our running example, we have
?{B1} = {0, 1, 2, 3}, ?{B2} = {1, 2}, ?{B3} =
{4, 6}. Consequently, the fan-out of ?{B1} is 2, and
the fan-out of ?{B2} and ?{B3} is 1. Notice that the
fan-out of the non-terminal B3 is 2. 2
We drop B from ?B and ?B whenever this set is
understood from the context or it is not relevant.
Given a set of endpoints ? = {i1, . . . , i2f} with
i1 < ? ? ? < i2f , we obtain its corresponding position
set by calculating the closure of ?, defined as
[?] = ?fj=1[i2j?1 + 1, i2j ] .
2To avoid clutter in our examples, we abuse the notation by
not making an explicit distinction between nonterminals and oc-
currences of nonterminals in productions.
541
3.2 Reductions
Assume that r > 2. The reduction of p by the non-
terminal occurrencesBr?1, Br is the ordered pair of
productions (p1, p2) that is defined as follows. Let
?1, . . . , ?n be the maximal substrings of ?(p) that
contain only variables xi,j with r ? 1 ? i ? r and
terminal symbols, and at least one variable. Then
p1 : A? g1(B1, . . . , Br?2, X) and
p2 : X ? g2(Br?1, Br) ,
where X is a fresh nonterminal symbol, the char-
acteristic string ?(p1) is the string obtained from
?(p) by replacing each substring ?i by the vari-
able xr?1,i, and the characteristic string ?(p2) is the
string ?1$ ? ? ? $?n.
Note that the defining equations of neither g1
nor g2 are in the specific form discussed in Sec-
tion 2.1; however, they can be brought into this form
by a consistent renaming of the variables. We will
silently assume this renaming to take place.
Example 5 The reduction of p0 by the nonterminal
occurrences B2 and B3 has p1 : A ? g1(B1, X)
and p2 : X ? g2(B2, B3) with
?(p1) = x1,1x2,1x1,2$x2,2
?(p2) = ax2,1$x3,1bx3,2
or, after renaming and in standard notation,
g1(?x1,1, x1,2?, ?x2,1, x2,2?) = ?x1,1x2,1x1,2, x2,2?
g2(?x1,1?, ?x2,1, x2,2?) = ?ax1,1, x2,1bx2,2? .2
It is easy to check that a reduction provides us with a
pair of productions that are equivalent to the original
production p, in terms of generative capacity, since
g1(B1, . . . , Br?2, g2(Br?1, Br)) = g(B1, . . . , Br)
for all tuples of strings generated from the nontermi-
nalsB1, . . . , Br, respectively. Note also that the fan-
out of production p1 equals the fan-out of p. How-
ever, the fan-out of p2 (the value n) may be greater
than the fan-out of p, depending on the way vari-
ables are arranged in ?(p). Thus, a reduction does
not necessarily preserve the fan-out of the original
production. In the worst case, the fan-out of p2 can
be as large as ?(Br?1) + ?(Br).
1: Function NAIVE-BINARIZATION(p)
2: result? ?;
3: currentProd? p;
4: while ?(currentProd) > 2 do
5: (p1, p2)? any reduction of currentProd;
6: result? result ? p2;
7: currentProd? p1;
8: return result ? currentProd;
Figure 1: The naive algorithm
We have defined reductions only for the last two
occurrences of nonterminals in the right-hand side of
a production p. However, it is easy to see that we can
also define the concept for two arbitrary (not neces-
sarily adjacent) occurrences of nonterminals, at the
cost of making the notation more complicated.
4 The algorithm
Let G be an LCFRS with ?(G) = f and ?(G) = r,
and let f ? ? f be a target fan-out. We will now
present an algorithm that computes an equivalent
LCFRS G? of fan-out at most f ? whose rank is at
most 2, if such an LCFRS exists in the first place.
The algorithm works by exhaustively reducing all
productions in G.
4.1 Naive algorithm
Given an LCFRS production p, a naive algorithm
to compute an equivalent set of productions whose
rank is at most 2 is given in Figure 1. By ap-
plying this algorithm to all the productions in the
LCFRSG, we can obtain an equivalent LCFRS with
rank 2. We will call such an LCFRS a binarization
of G.
The fan-out of the obtained LCFRS will depend
on the nonterminals that we choose for the reduc-
tions in line 5. It is not difficult to see that, in the
worst case, the resulting fan-out can be as high as
d r2e ? f . This occurs when we choose d r2e nonter-minals with fan-out f that have associated variables
in the string ?N (p) that do not occur at consecutive
positions.
The algorithm that we develop in Section 4.3 im-
proves on the naive algorithm in that it can be ex-
ploited to find a sequence of reductions that results
in a binarization of G that is optimal, i.e., leads to
542
an LCFRS with minimal fan-out. The algorithm is
based on a technical concept called adjacency.
4.2 Adjacency
Let p be some production in the LCFRS G, and let
?1,?2 be sets of endpoints, associated with some
sets of nonterminal occurrences in p. We say that?1
and ?2 overlap if the intersection of their closures
is nonempty, that is, if [?1]? [?2] 6= ?. Overlapping
holds if and only if the associated sets of nontermi-
nal occurrences are not disjoint. If ?1 and ?2 do
not overlap, we define their merge as
?(?1,?2) = (?1 ??2) \ (?1 ??2) .
It is easy to see that [?(?1,?2)] = [?1] ? [?2].
We say that ?1 and ?2 are adjacent for a given fan-
out f , written ?1 ?f ?2, if ?1 and ?2 do not
overlap, and ?([?(?1,?2)]) ? f .
Example 6 For the production p0 from Example 2,
we have ?(?{B1},?{B2}) = {0, 3}, showing that?{B1} ?1 ?{B2}. Similarly, we have
?(?{B1},?{B3}) = {0, 1, 2, 3, 4, 6} ,
showing that ?{B1} ?3 ?{B3}, but that neither?{B1} ?2 ?{B3} nor ?{B1} ?1 ?{B3} holds. 2
4.3 Bounded binarization algorithm
The adjacency-based binarization algorithm is given
in Figure 2. It starts with a working set contain-
ing the endpoint sets corresponding to each non-
terminal occurrence in the input production p. Re-
ductions of p are only explored for nonterminal oc-
currences whose endpoint sets are adjacent for the
target fan-out f ?, since reductions not meeting this
constraint would produce productions with fan-out
greater than f ?. Each reduction explored by the al-
gorithm produces a new endpoint set, associated to
the fresh nonterminal that it introduces, and this new
endpoint set is added to the working set and poten-
tially used in further reductions.
From the definition of the adjacency relation?f ,
it follows that at lines 9 and 10 of BOUNDED-
BINARIZATION we only pick up reductions for p
that do not exceed the fan-out bound of f ?. This
implies soundness for our algorithm. Completeness
means that the algorithm fails only if there exists no
binarization for p of fan-out not greater than f ?. This
1: Function BOUNDED-BINARIZATION(p, f ?)
2: workingSet? ?;
3: agenda? ?;
4: for all i from 1 to ?(p) do
5: workingSet? workingSet ? {?{Bi}};
6: agenda? agenda ? {?{Bi}};
7: while agenda 6= ? do
8: ?? pop some endpoint set from agenda;
9: for all ?1 ? workingSet with ?1 ?f ? ? do
10: ?2 = ?(?,?1);
11: if ?2 /? workingSet then
12: workingSet? workingSet ? {?2};
13: agenda? agenda ? {?2};
14: if ?{B1,B2,...,B?(p))} ? workingSet then
15: return true;
16: else
17: return false;
Figure 2: Algorithm to compute a bounded binarization
property is intuitive if one observes that our algo-
rithm is a specialization of standard algorithms for
the computation of the closure of binary relations.
A formal proof of this fact is rather long and te-
dious, and will not be reported here. We notice that
there is a very close similarity between algorithm
BOUNDED-BINARIZATION and the deduction pro-
cedure proposed by Shieber et al (1995) for parsing.
We discuss this more at length in Section 5.
Note that we have expressed the algorithm as a
decision function that will return true if there exists
a binarization of p with fan-out not greater than f ?,
and false otherwise. However, the algorithm can
easily be modified to return a reduction producing
such a binarization, by adding to each endpoint set
? ? workingSet two pointers to the adjacent end-
point sets that were used to obtain it. If the algorithm
is successful, the tree obtained by following these
pointers from the final endpoint set ?{B1,...,B?(p)} ?workingSet gives us a tree of reductions that will
produce a binarization of p with fan-out not greater
than f ?, where each node labeled with the set ?{Bi}
corresponds to the nonterminal Bi, and nodes la-
beled with other endpoint sets correspond to the
fresh nonterminals created by the reductions.
543
4.4 Implementation
In order to implement BOUNDED-BINARIZATION,
we can represent endpoint sets in a canonical way
as 2f ?-tuples of integer positions in ascending order,
and with some special null value used to fill posi-
tions for endpoint sets with fan-out strictly smaller
than f ?. We will assume that the concrete null value
is larger than any other integer.
We also need to provide some appropriate repre-
sentation for the set workingSet, in order to guar-
antee efficient performance for the membership test
and the insertion operation. Both operations can be
implemented in constant time if we represent work-
ingSet as an (2?f ?)-dimensional table with Boolean
entries. Each dimension is indexed by values in
[0, n] plus our special null value; here n is the length
of the string ?N (p), and thus n = O(|p|). However,
this has the disadvantage of using space ?(n2f ?),
even in case workingSet is sparse, and is affordable
only for quite small values of f ?. Alternatively, we
can more compactly represent workingSet as a trie
data structure. This representation has size certainly
smaller than 2f ? ? q, where q is the size of the set
workingSet. However, both membership and inser-
tion operations take now an amount of time O(2f ?).
We now analyse the time complexity of algorithm
BOUNDED-BINARIZATION for inputs p and f ?. We
first focus on the while-loop at lines 7 to 13. As
already observed, the number of possible endpoint
sets is bounded by O(n2f ?). Furthermore, because
of the test at line 11, no endpoint set is ever inserted
into the agenda variable more than once in a sin-
gle run of the algorithm. We then conclude that our
while-loop cycles a number of times O(n2f ?).
We now focus on the choice of the endpoint set
?1 in the inner for-loop at lines 9 to 13. Let us fix ?
as in line 8. It is not difficult to see that any ?1 with
?1 ?f ? ? must satisfy
?(?) + ?(?1)? |? ??1| ? f ?. (1)
Let I ? ?, and consider all endpoint sets ?1 with
? ??1 = I . Given (1), we also have
?(?1) ? f ? + |I| ? ?(?). (2)
This means that, for each ? coming out of the
agenda, at line 9 we can choose all endpoint sets ?1
such that ?1 ?f ? ? by performing the following
steps:
? arbitrarily choose a set I ? ?;
? choose endpoints in set ?1\I subject to (2);
? test whether ?1 belongs to workingSet and
whether ?, ?1 do not overlap.
We claim that, in the above steps, the number
of involved endpoints does not exceed 3f ?. To
see this, we observe that from (2) we can derive
|I| ? ?(?) + ?(?1) ? f ?. The total number
of (distinct) endpoints in a single iteration step is
e = 2?(?) + 2?(?1) ? |I|. Combining with the
above inequality we have
e ? 2?(?) + 2?(?1)? ?(?)? ?(?1) + f ?
= ?(?) + ?(?1) + f ? ? 3f ? ,
as claimed. Since each endpoint takes values in
the set [0, n], we have a total of O(n3f ?) different
choices. For each such choice, we need to clas-
sify an endpoint as belonging to either ?\I , ?1\I ,
or I . This amounts to an additional O(33f ?) dif-
ferent choices. Overall, we have a total number of
O((3n)3f ?) different choices. For each such choice,
the test for membership in workingSet for ?1 takes
constant time in case we use a multi-dimensional ta-
ble, or else O(|p|) in case we use a trie. The ad-
jacency test and the merge operations can easily be
carried out in time O(|p|).
Putting all of the above observations together, and
using the already observed fact that n = O(|p|),
we can conclude that the total amount of time re-
quired by the while-loop at lines 7 to 13 is bounded
byO(|p| ? (3|p|)3f ?), both under the assumption that
workingSet is represented as a multi-dimensional ta-
ble or as a trie. This is also a bound on the running
time of the whole algorithm.
4.5 Minimal binarization of a complete LCFRS
The algorithm defined in Section 4.3 can be used
to binarize an LCFRS in such a way that each rule
in the resulting binarization has the minimum pos-
sible fan-out. This can be done by applying the
BOUNDED-BINARIZATION algorithm to each pro-
duction p, until we find the minimum value for the
544
1: Function MINIMAL-BINARIZATION(G)
2: pb = ? {Set of binarized productions}
3: for all production p of G do
4: f ? = fan-out(p);
5: while not BOUNDED-BINARIZATION(p, f ?)
do
6: f ? = f ? + 1;
7: add result of BOUNDED-BINARIZATION(p,
f ?) to pb; {We obtain the tree from
BOUNDED-BINARIZATION as explained in
Section 4.3 and use it to binarize p}
8: return pb;
Figure 3: Minimal binarization by sequential search
bound f ? for which this algorithm finds a binariza-
tion. For a production with rank r and fan-out f ,
we know that this optimal value of f ? must be in
the interval [f, d r2e ? f ] because binarizing a pro-duction cannot reduce its fan-out, and the NAIVE-
BINARIZATION algorithm seen in Section 4.1 can
binarize any production by increasing fan-out to
d r2e ? f in the worst case.
The simplest way of finding out the optimal value
of f ? for each production is by a sequential search
starting with ?(p) and going upwards, as in the algo-
rithm in Figure 3. Note that the upper bound d r2e ? fthat we have given for f ? guarantees that the while-
loop in this algorithm always terminates.
In the worst case, we may need f ? (d r2e ? 1) + 1executions of the BOUNDED-BINARIZATION algo-
rithm to find the optimal binarization of a production
in G. This complexity can be reduced by changing
the strategy to search for the optimal f ?: for exam-
ple, we can perform a binary search within the inter-
val [f, d r2e ? f ], which lets us find the optimal bina-rization in blog(f ? (d r2e?1)+1)c+1 executions ofBOUNDED-BINARIZATION. However, this will not
result in a practical improvement, since BOUNDED-
BINARIZATION is exponential in the value of f ? and
the binary search will require us to run it on val-
ues of f ? larger than the optimal in most cases. An
intermediate strategy between the two is to apply
exponential backoff to try the sequence of values
f?1+2i (for i = 0, 1, 2 . . .). When we find the first
i such that BOUNDED-BINARIZATION does not fail,
if i > 0, we apply the same strategy to the interval
[f?1+2i?1, f?2+2i], and we repeat this method to
shrink the interval until BOUNDED-BINARIZATION
does not fail for i = 0, giving us our optimal f ?.
With this strategy, the amount of executions of the
algorithm that we need in the worst case is
1
2(dlog(?)e+ dlog(?)e
2) + 1 ,
where ? = f ? (d r2e ? 1) + 1, but we avoid usingunnecessarily large values of f ?.
5 Discussion
To conclude this paper, we now discuss a number of
aspects of the results that we have presented, includ-
ing various other pieces of research that are particu-
larly relevant to this paper.
5.1 The tradeoff between rank and fan-out
The algorithm introduced in this paper can be used
to transform an LCFRS into an equivalent form
with rank 2. This will result into a more effi-
ciently parsable LCFRS, since rank exponentially
affects parsing complexity. However, we must take
into account that parsing complexity is also influ-
enced by fan-out. Our algorithm guarantees a min-
imal increase in fan-out. In practical cases it seems
such an increase is quite small. For example, in
the context of dependency parsing, both Go?mez-
Rodr??guez et al (2009) and Kuhlmann and Satta
(2009) show that all the structures in several well-
known non-projective dependency treebanks are bi-
narizable without any increase in their fan-out.
More in general, it has been shown by Seki et al
(1991) that parsing of LCFRS can be carried out in
time O(n|pM |), where n is the length of the input
string and pM is the production in the grammar with
largest size.3 Thus, there may be cases in which one
has to find an optimal tradeoff between rank and fan-
out, in order to minimize the size of pM . This re-
quires some kind of Viterbi search over the space of
all possible binarizations, constructed as described
at the end of Subsection 4.3, for some appropriate
value of the fan-out f ?.
3The result has been shown for the formalism of multiple
context-free grammars (MCFG), but it also applies to LCFRS,
which are a special case of MCFG.
545
5.2 Extension to general LCFRS
This paper has focussed on string-based LCFRS.
As discussed in Vijay-Shanker et al (1987), LCFRS
provide a more general framework where the pro-
ductions are viewed as generating a set of abstract
derivation trees. These trees can be used to specify
how structures other than tuples of strings are com-
posed. For example, LCFRS derivation trees can be
used to specify how the elementary trees of a Tree
Adjoining Grammar can be composed to produced
derived tree. However, the results in this paper also
apply to non-string-based LCFRS, since by limit-
ing attention to the terminal string yield of whatever
structures are under consideration, the composition
operations can be defined using the string-based ver-
sion of LCFRS that is discussed here.
5.3 Similar algorithmic techniques
The NAIVE-BINARIZATION algorithm given in Fig-
ure 1 is not novel to this paper: it is similar to
an algorithm developed in Melamed et al (2004)
for generalized multitext grammars, a formalism
weakly equivalent to LCFRS that has been intro-
duced for syntax-based machine translation. How-
ever, the grammar produced by our algorithm has
optimal (minimal) fan-out. This is an important im-
provement over the result in (Melamed et al, 2004),
as this quantity enters into the parsing complexity
of both multitext grammars and LCFRS as an expo-
nential factor, and therefore must be kept as low as
possible to ensure practically viable parsing.
Rank reduction is also investigated in Nesson
et al (2008) for synchronous tree-adjoining gram-
mars, a synchronous rewriting formalism based on
tree-adjoining grammars Joshi and Schabes (1992).
In this case the search space of possible reductions
is strongly restricted by the tree structures specified
by the formalism, resulting in simplified computa-
tion for the reduction algorithms. This feature is not
present in the case of LCFRS.
There is a close parallel between the technique
used in the MINIMAL-BINARIZATION algorithm
and deductive parsing techniques as proposed by
Shieber et al (1995), that are usually implemented
by means of tabular methods. The idea of exploit-
ing tabular parsing in production factorization was
first expressed in Zhang et al (2006). In fact, the
particular approach presented here has been used
to improve efficiency of parsing algorithms that use
discontinuous syntactic models, in particular, non-
projective dependency grammars, as discussed in
Go?mez-Rodr??guez et al (2009).
5.4 Open problems
The bounded binarization algorithm that we have
presented has exponential run-time in the value of
the input fan-out bound f ?. It remains an open ques-
tion whether the bounded binarization problem for
LCFRS can be solved in deterministic polynomial
time. Even in the restricted case of f ? = ?(p), that
is, when no increase in the fan-out of the input pro-
duction is allowed, we do not know whether p can be
binarized using only deterministic polynomial time
in the value of p?s fan-out. However, our bounded
binarization algorithm shows that the latter problem
can be solved in polynomial time when the fan-out
of the input LCFRS is bounded by some constant.
Whether the bounded binarization problem can
be solved in polynomial time in the value of the
input bound f ? is also an open problem in the re-
stricted case of synchronous context-free grammars,
a special case of an LCFRS of fan-out two with
a strict separation between the two components of
each nonterminal in the right-hand side of a produc-
tion, as discussed in the introduction. An interesting
analysis of this restricted problem can be found in
Gildea and Stefankovic (2007).
Acknowledgements The work of Carlos Go?mez-
Rodr??guez was funded by Ministerio de Educacio?n
y Ciencia and FEDER (HUM2007-66607-C04) and
Xunta de Galicia (PGIDIT07SIN005206PR, IN-
CITE08E1R104022ES, INCITE08ENA305025ES,
INCITE08PXIB302179PR and Rede Galega de
Procesamento da Linguaxe e Recuperacio?n de Infor-
macio?n). The work of Marco Kuhlmann was funded
by the Swedish Research Council. The work of
Giorgio Satta was supported by MIUR under project
PRIN No. 2007TJNZRE 002. We are grateful to an
anonymous reviewer for a very detailed review with
a number of particularly useful suggestions.
546
References
David Chiang. 2007. Hierarchical phrase-
based translation. Computational Linguistics,
33(2):201?228.
Daniel Gildea and Daniel Stefankovic. 2007. Worst-
case synchronous grammar rules. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for
Computational Linguistics; Proceedings of the
Main Conference, pages 147?154. Association
for Computational Linguistics, Rochester, New
York.
Carlos Go?mez-Rodr??guez, David J. Weir, and John
Carroll. 2009. Parsing mildly non-projective de-
pendency structures. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
A. K. Joshi and Y. Schabes. 1992. Tree adjoining
grammars and lexicalized grammars. In M. Nivat
and A. Podelsky, editors, Tree Automata and Lan-
guages. Elsevier, Amsterdam, The Netherlands.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly
non-projective dependency structures. In 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), Main Conference Poster Sessions, pages
507?514. Sydney, Australia.
Marco Kuhlmann and Giorgio Satta. 2009. Tree-
bank grammar techniques for non-projective de-
pendency parsing. In Twelfth Conference of the
European Chapter of the Association for Compu-
tational Linguistics (EACL). To appear.
I. Dan Melamed, Benjamin Wellington, and Gior-
gio Satta. 2004. Generalized multitext gram-
mars. In 42nd Annual Meeting of the Association
for Computational Linguistics (ACL), pages 661?
668. Barcelona, Spain.
Rebecca Nesson, Giorgio Satta, and Stuart M.
Shieber. 2008. Optimal k-arization of syn-
chronous tree-adjoining grammar. In Proceedings
of ACL-08: HLT, pages 604?612. Association for
Computational Linguistics, Columbus, Ohio.
A. Nijholt. 1980. Context-Free Grammars: Cov-
ers, Normal Forms, and Parsing, volume 93.
Springer-Verlag, Berlin, Germany.
Owen Rambow and Giorgio Satta. 1999. Indepen-
dent parallelism in finite copying parallel rewrit-
ing systems. Theoretical Computer Science,
223(1?2):87?120.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On Multiple Context-
Free Grammars. Theoretical Computer Science,
88(2):191?229.
Stuart M. Shieber, Yves Schabes, and Fernando
Pereira. 1995. Principles and implementation of
deductive parsing. Journal of Logic Program-
ming, 24(1?2):3?36.
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of
two permutations. Algorithmica, 26(2):290?309.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descrip-
tions produced by various grammatical for-
malisms. In 25th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), pages
104?111. Stanford, CA, USA.
Benjamin Wellington, Sonjia Waxmonsky, and
I. Dan Melamed. 2006. Empirical lower bounds
on the complexity of translational equivalence. In
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL), pages 977?984. Sydney, Australia.
Hao Zhang, Daniel Gildea, and David Chiang.
2008. Extracting synchronous grammar rules
from word-level alignments in linear time. In
22nd International Conference on Computational
Linguistics (Coling), pages 1081?1088. Manch-
ester, England, UK.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Human Language Technol-
ogy Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 256?263. New York, USA.
547
Proceedings of ACL-08: HLT, pages 968?976,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Deductive Approach to Dependency Parsing?
Carlos Go?mez-Rodr??guez
Departamento de Computacio?n
Universidade da Corun?a, Spain
cgomezr@udc.es
John Carroll and David Weir
Department of Informatics
University of Sussex, United Kingdom
{johnca,davidw}@sussex.ac.uk
Abstract
We define a new formalism, based on Sikkel?s
parsing schemata for constituency parsers,
that can be used to describe, analyze and com-
pare dependency parsing algorithms. This
abstraction allows us to establish clear rela-
tions between several existing projective de-
pendency parsers and prove their correctness.
1 Introduction
Dependency parsing consists of finding the structure
of a sentence as expressed by a set of directed links
(dependencies) between words. This is an alterna-
tive to constituency parsing, which tries to find a di-
vision of the sentence into segments (constituents)
which are then broken up into smaller constituents.
Dependency structures directly show head-modifier
and head-complement relationships which form the
basis of predicate argument structure, but are not
represented explicitly in constituency trees, while
providing a representation in which no non-lexical
nodes have to be postulated by the parser. In addi-
tion to this, some dependency parsers are able to rep-
resent non-projective structures, which is an impor-
tant feature when parsing free word order languages
in which discontinuous constituents are common.
The formalism of parsing schemata (Sikkel, 1997)
is a useful tool for the study of constituency parsers
since it provides formal, high-level descriptions
of parsing algorithms that can be used to prove
their formal properties (such as correctness), es-
tablish relations between them, derive new parsers
from existing ones and obtain efficient implementa-
tions automatically (Go?mez-Rodr??guez et al, 2007).
The formalism was initially defined for context-free
grammars and later applied to other constituency-
based formalisms, such as tree-adjoining grammars
?Partially supported by Ministerio de Educacio?n y Ciencia
and FEDER (TIN2004-07246-C03, HUM2007-66607-C04),
Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC-
10501PN, PGIDIT05PXIC30501PN, Rede Galega de Proc. da
Linguaxe e RI) and Programa de Becas FPU.
(Alonso et al, 1999). However, since parsing
schemata are defined as deduction systems over sets
of constituency trees, they cannot be used to de-
scribe dependency parsers.
In this paper, we define an analogous formalism
that can be used to define, analyze and compare de-
pendency parsers. We use this framework to provide
uniform, high-level descriptions for a wide range of
well-known algorithms described in the literature,
and we show how they formally relate to each other
and how we can use these relations and the formal-
ism itself to prove their correctness.
1.1 Parsing schemata
Parsing schemata (Sikkel, 1997) provide a formal,
simple and uniform way to describe, analyze and
compare different constituency-based parsers.
The notion of a parsing schema comes from con-
sidering parsing as a deduction process which gener-
ates intermediate results called items. An initial set
of items is directly obtained from the input sentence,
and the parsing process consists of the application of
inference rules (deduction steps) which produce new
items from existing ones. Each item contains a piece
of information about the sentence?s structure, and a
successful parsing process will produce at least one
final item containing a full parse tree for the sentence
or guaranteeing its existence.
Items in parsing schemata are formally defined
as sets of partial parse trees from a set denoted
Trees(G), which is the set of all the possible par-
tial parse trees that do not violate the constraints im-
posed by a grammar G. More formally, an item set
I is defined by Sikkel as a quotient set associated
with an equivalence relation on Trees(G).1
Valid parses for a string are represented by
items containing complete marked parse trees for
that string. Given a context-free grammar G =
1While Shieber et al (1995) also view parsers as deduction
systems, Sikkel formally defines items and related concepts,
providing the mathematical tools to reason about formal prop-
erties of parsers.
968
(N,?, P, S), a marked parse tree for a string
w1 . . . wn is any tree ? ? Trees(G)/root(?) =
S?yield(?) = w1 . . . wn2. An item containing such
a tree for some arbitrary string is called a final item.
An item containing such a tree for a particular string
w1 . . . wn is called a correct final item for that string.
For each input string, a parsing schema?s deduc-
tion steps allow us to infer a set of items, called valid
items for that string. A parsing schema is said to
be sound if all valid final items it produces for any
arbitrary string are correct for that string. A pars-
ing schema is said to be complete if all correct fi-
nal items are valid. A correct parsing schema is one
which is both sound and complete. A correct parsing
schema can be used to obtain a working implemen-
tation of a parser by using deductive engines such
as the ones described by Shieber et al (1995) and
Go?mez-Rodr??guez et al (2007) to obtain all valid fi-
nal items.
2 Dependency parsing schemata
Although parsing schemata were initially defined for
context-free parsers, they can be adapted to different
constituency-based grammar formalisms, by finding
a suitable definition of Trees(G) for each particular
formalism and a way to define deduction steps from
its rules. However, parsing schemata are not directly
applicable to dependency parsing, since their formal
framework is based on constituency trees.
In spite of this problem, many of the dependency
parsers described in the literature are constructive,
in the sense that they proceed by combining smaller
structures to form larger ones until they find a com-
plete parse for the input sentence. Therefore, it
is possible to define a variant of parsing schemata,
where these structures can be defined as items and
the strategies used for combining them can be ex-
pressed as inference rules. However, in order to de-
fine such a formalism we have to tackle some issues
specific to dependency parsers:
? Traditional parsing schemata are used to de-
fine grammar-based parsers, in which the parsing
process is guided by some set of rules which are
used to license deduction steps: for example, an
Earley Predictor step is tied to a particular gram-
mar rule, and can only be executed if such a rule
exists. Some dependency parsers are also grammar-
2wi is shorthand for the marked terminal (wi, i). These are
used by Sikkel (1997) to link terminal symbols to string posi-
tions so that an input sentence can be represented as a set of
trees which are used as initial items (hypotheses) for the de-
duction system. Thus, a sentence w1 . . . wn produces a set of
hypotheses {{w1(w1)}, . . . , {wn(wn)}}.
Figure 1: Representation of a dependency structure with
a tree. The arrows below the words correspond to its as-
sociated dependency graph.
based: for example, those described by Lombardo
and Lesmo (1996), Barbero et al (1998) and Ka-
hane et al (1998) are tied to the formalizations of de-
pendency grammar using context-free like rules de-
scribed by Hays (1964) and Gaifman (1965). How-
ever, many of the most widely used algorithms (Eis-
ner, 1996; Yamada and Matsumoto, 2003) do not use
a formal grammar at all. In these, decisions about
which dependencies to create are taken individually,
using probabilistic models (Eisner, 1996) or classi-
fiers (Yamada and Matsumoto, 2003). To represent
these algorithms as deduction systems, we use the
notion of D-rules (Covington, 1990). D-rules take
the form a ? b, which says that word b can have a
as a dependent. Deduction steps in non-grammar-
based parsers can be tied to the D-rules associated
with the links they create. In this way, we obtain
a representation of the semantics of these parsing
strategies that is independent of the particular model
used to take the decisions associated with each D-
rule.
? The fundamental structures in dependency pars-
ing are dependency graphs. Therefore, as items
for constituency parsers are defined as sets of par-
tial constituency trees, it is tempting to define items
for dependency parsers as sets of partial dependency
graphs. However, predictive grammar-based algo-
rithms such as those of Lombardo and Lesmo (1996)
and Kahane et al (1998) have operations which pos-
tulate rules and cannot be defined in terms of depen-
dency graphs, since they do not do any modifications
to the graph. In order to make the formalism general
enough to include these parsers, we define items in
terms of sets of partial dependency trees as shown in
Figure 1. Note that a dependency graph can always
be extracted from such a tree.
? Some of the most popular dependency parsing
algorithms, like that of Eisner (1996), work by con-
necting spans which can represent disconnected de-
pendency graphs. Such spans cannot be represented
by a single dependency tree. Therefore, our formal-
ism allows items to be sets of forests of partial de-
pendency trees, instead of sets of trees.
969
Taking these considerations into account, we de-
fine the concepts that we need to describe item sets
for dependency parsers:
Let ? be an alphabet of terminal symbols.
Partial dependency trees: We define the set of
partial dependency trees (D-trees) as the set of finite
trees where children of each node have a left-to-right
ordering, each node is labelled with an element of
??(??N), and the following conditions hold:
? All nodes labelled with marked terminals wi ?
(??N) are leaves,
? Nodes labelled with terminals w ? ? do not have
more than one daughter labelled with a marked
terminal, and if they have such a daughter node, it
is labelled wi for some i ? N,
? Left siblings of nodes labelled with a marked ter-
minal wk do not have any daughter labelled wj
with j ? k. Right siblings of nodes labelled with
a marked terminal wk do not have any daughter
labelled wj with j ? k.
We denote the root node of a partial dependency
tree t as root(t). If root(t) has a daughter node la-
belled with a marked terminal wh, we will say that
wh is the head of the tree t, denoted by head(t). If
all nodes labelled with terminals in t have a daughter
labelled with a marked terminal, t is grounded.
Relationship between trees and graphs: Let
t ? D-trees be a partial dependency tree; g(t), its
associated dependency graph, is a graph (V,E)
? V ={wi ? (??N) | wi is the label of a node in
t},
? E ={(wi, wj) ? (??N)2 | C,D are nodes in t
such that D is a daughter of C, wj the label of a
daughter of C, wi the label of a daughter of D}.
Projectivity: A partial dependency tree t ?
D-trees is projective iff yield(t) cannot be written
as . . . wi . . . wj . . . where i ? j.
It is easy to verify that the dependency graph
g(t) is projective with respect to the linear order of
marked terminals wi, according to the usual defi-
nition of projectivity found in the literature (Nivre,
2006), if and only if the tree t is projective.
Parse tree: A partial dependency tree t ?
D-trees is a parse tree for a given string w1 . . . wn
if its yield is a permutation of w1 . . . wn. If its yield
is exactly w1 . . . wn, we will say it is a projective
parse tree for the string.
Item set: Let ? ? D-trees be the set of de-
pendency trees which are acceptable according to a
given grammar G (which may be a grammar of D-
rules or of CFG-like rules, as explained above). We
define an item set for dependency parsing as a set
I ? ?, where ? is a partition of 2?.
Once we have this definition of an item set for
dependency parsing, the remaining definitions are
analogous to those in Sikkel?s theory of constituency
parsing (Sikkel, 1997), so we will not include them
here in full detail. A dependency parsing system is
a deduction system (I, H,D) where I is a depen-
dency item set as defined above, H is a set contain-
ing initial items or hypotheses, and D ? (2(H?I) ?
I) is a set of deduction steps defining an inference
relation `.
Final items in this formalism will be those con-
taining some forest F containing a parse tree for
some arbitrary string. An item containing such a tree
for a particular string w1 . . . wn will be called a cor-
rect final item for that string in the case of nonprojec-
tive parsers. When defining projective parsers, cor-
rect final items will be those containing projective
parse trees for w1 . . . wn. This distinction is relevant
because the concepts of soundness and correctness
of parsing schemata are based on correct final items
(cf. section 1.1), and we expect correct projective
parsers to produce only projective structures, while
nonprojective parsers should find all possible struc-
tures including nonprojective ones.
3 Some practical examples
3.1 Col96 (Collins, 96)
One of the most straightforward projective depen-
dency parsing strategies is the one described by
Collins (1996), directly based on the CYK pars-
ing algorithm. This parser works with dependency
trees which are linked to each other by creating
links between their heads. Its item set is defined as
ICol96 = {[i, j, h] | 1 ? i ? h ? j ? n}, where an
item [i, j, h] is defined as the set of forests containing
a single projective dependency tree t such that t is
grounded, yield(t) = wi . . . wj and head(t) = wh.
For an input string w1 . . . wn, the set of hypothe-
ses is H = {[i, i, i] | 0 ? i ? n + 1}, i.e., the set
of forests containing a single dependency tree of the
form wi(wi). This same set of hypotheses can be
used for all the parsers, so we will not make it ex-
plicit for subsequent schemata.3
The set of final items is {[1, n, h] | 1 ? h ? n}:
these items trivially represent parse trees for the in-
put sentence, where wh is the sentence?s head. The
deduction steps are shown in Figure 2.
3Note that the words w0 and wn+1 used in the definition do
not appear in the input: these are dummy terminals that we will
call beginning of sentence (BOS) and end of sentence (EOS)
marker, respectively; and will be needed by some parsers.
970
Col96 (Collins,96):
R-Link
[i, j, h1]
[j + 1, k, h2]
[i, k, h2]
wh1 ? wh2
L-Link
[i, j, h1]
[j + 1, k, h2]
[i, k, h1]
wh2 ? wh1
Eis96 (Eisner, 96):
Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1, F, F ]
R-Link [i, j, F, F ][i, j, T, F ] wi ? wj
L-Link [i, j, F, F ][i, j, F, T ] wj ? wi
CombineSpans
[i, j, b, c]
[j, k, not(c), d]
[i, k, b, d]
ES99 (Eisner and Satta, 99):
R-Link [i, j, i] [j + 1, k, k][i, k, k] wi ? wk
L-Link [i, j, i] [j + 1, k, k][i, k, i] wk ? wi
R-Combiner [i, j, i] [j, k, j][i, k, i]
L-Combiner [i, j, j] [j, k, k][i, k, k]
YM03 (Yamada and Matsumoto, 2003):
Initter [i, i, i] [i + 1, i + 1, i + 1][i, i + 1]
R-Link
[i, j]
[j, k]
[i, k] wj ? wk L-Link
[i, j]
[j, k]
[i, k] wj ? wi
LL96 (Lombardo and Lesmo, 96):
Initter [(.S), 1, 0] ?(S)?P Predictor
[A(?.B?), i, j]
[B(.?), j + 1, j] B(?)?P
Scanner [A(?. ? ?), i, h? 1] [h, h, h][A(? ? .?), i, h] wh IS A
Completer [A(?.B?), i, j] [B(?.), j + 1, k][A(?B.?), i, k]
Figure 2: Deduction steps of the parsing schemata for some well-known dependency parsers.
As we can see, we use D-rules as side conditions
for deduction steps, since this parsing strategy is not
grammar-based. Conceptually, the schema we have
just defined describes a recogniser: given a set of D-
rules and an input string wi . . . wn, the sentence can
be parsed (projectively) under those D-rules if and
only if this deduction system can infer a correct final
item. However, when executing this schema with a
deductive engine, we can recover the parse forest by
following back pointers in the same way as is done
with constituency parsers (Billot and Lang, 1989).
Of course, boolean D-rules are of limited interest
in practice. However, this schema provides a formal-
ization of a parsing strategy which is independent
of the way linking decisions are taken in a partic-
ular implementation. In practice, statistical models
can be used to decide whether a step linking words
a and b (i.e., having a ? b as a side condition) is
executed or not, and probabilities can be attached to
items in order to assign different weights to different
analyses of the sentence. The same principle applies
to the rest of D-rule-based parsers described in this
paper.
3.2 Eis96 (Eisner, 96)
By counting the number of free variables used in
each deduction step of Collins? parser, we can con-
clude that it has a time complexity of O(n5). This
complexity arises from the fact that a parentless
word (head) may appear in any position in the par-
tial results generated by the parser; the complexity
can be reduced to O(n3) by ensuring that parentless
words can only appear at the first or last position
of an item. This is the principle behind the parser
defined by Eisner (1996), which is still in wide use
today (Corston-Oliver et al, 2006; McDonald et al,
2005a).
The item set for Eisner?s parsing schema is
IEis96 = {[i, j, T, F ] | 0 ? i ? j ? n} ?
{[i, j, F, T ] | 0 ? i ? j ? n} ? {[i, j, F, F ] |
0 ? i ? j ? n}, where each item [i, j, T, F ] is de-
fined as the item [i, j, j] ? ICol96, each item
[i, j, F, T ] is defined as the item [i, j, i] ? ICol96,
and each item [i, j, F, F ] is defined as the set
of forests of the form {t1, t2} such that t1 and
t2 are grounded, head(t1) = wi, head(t2) = wj ,
and ?k ? N(i ? k < j)/yield(t1) = wi . . . wk ?
yield(t2) = wk+1 . . . wj .
Note that the flags b, c in an item [i, j, b, c] indi-
cate whether the words in positions i and j, respec-
tively, have a parent in the item or not. Items with
one of the flags set to T represent dependency trees
where the word in position i or j is the head, while
items with both flags set to F represent pairs of trees
headed at positions i and j, and therefore correspond
to disconnected dependency graphs.
Deduction steps4 are shown in Figure 2. The
set of final items is {[0, n, F, T ]}. Note that these
items represent dependency trees rooted at the BOS
marker w0, which acts as a ?dummy head? for the
sentence. In order for the algorithm to parse sen-
tences correctly, we will need to define D-rules to
allow w0 to be linked to the real sentence head.
3.3 ES99 (Eisner and Satta, 99)
Eisner and Satta (1999) define an O(n3) parser for
split head automaton grammars that can be used
4Alternatively, we could consider items of the form [i, i +
1, F, F ] to be hypotheses for this parsing schema, so we would
not need an Initter step. However, we have chosen to use a stan-
dard set of hypotheses valid for all parsers because this allows
for more straightforward proofs of relations between schemata.
971
for dependency parsing. This algorithm is con-
ceptually simpler than Eis96, since it only uses
items representing single dependency trees, avoid-
ing items of the form [i, j, F, F ]. Its item set is
IES99 = {[i, j, i] | 0 ? i ? j ? n} ? {[i, j, j] |
0 ? i ? j ? n}, where items are defined as in
Collins? parsing schema.
Deduction steps are shown in Figure 2, and the set
of final items is {[0, n, 0]}. (Parse trees have w0 as
their head, as in the previous algorithm).
Note that, when described for head automaton
grammars as in Eisner and Satta (1999), this algo-
rithm seems more complex to understand and imple-
ment than the previous one, as it requires four differ-
ent kinds of items in order to keep track of the state
of the automata used by the grammars. However,
this abstract representation of its underlying seman-
tics as a dependency parsing schema shows that this
parsing strategy is in fact conceptually simpler for
dependency parsing.
3.4 YM03 (Yamada and Matsumoto, 2003)
Yamada and Matsumoto (2003) define a determinis-
tic, shift-reduce dependency parser guided by sup-
port vector machines, which achieves over 90% de-
pendency accuracy on section 23 of the Penn tree-
bank. Parsing schemata are not suitable for directly
describing deterministic parsers, since they work at
a high abstraction level where a set of operations
are defined without imposing order constraints on
them. However, many deterministic parsers can be
viewed as particular optimisations of more general,
nondeterministic algorithms. In this case, if we rep-
resent the actions of the parser as deduction steps
while abstracting from the deterministic implemen-
tation details, we obtain an interesting nondetermin-
istic parser.
Actions in Yamada and Matsumoto?s parser create
links between two target nodes, which act as heads
of neighbouring dependency trees. One of the ac-
tions creates a link where the left target node be-
comes a child of the right one, and the head of a
tree located directly to the left of the target nodes
becomes the new left target node. The other ac-
tion is symmetric, performing the same operation
with a right-to-left link. An O(n3) nondetermin-
istic parser generalising this behaviour can be de-
fined by using an item set IY M03 = {[i, j] |
0 ? i ? j ? n + 1}, where each item [i, j] is de-
fined as the item [i, j, F, F ] in IEis96; and the de-
duction steps are shown in Figure 2.
The set of final items is {[0, n + 1]}. In order for
this set to be well-defined, the grammar must have
no D-rules of the form wi ? wn+1, i.e., it must not
allow the EOS marker to govern any words. If this
is the case, it is trivial to see that every forest in an
item of the form [0, n + 1] must contain a parse tree
rooted at the BOS marker and with yield w0 . . . wn.
As can be seen from the schema, this algorithm
requires less bookkeeping than any other of the
parsers described here.
3.5 LL96 (Lombardo and Lesmo, 96) and
other Earley-based parsers
The algorithms in the above examples are based on
taking individual decisions about dependency links,
represented by D-rules. Other parsers, such as that
of Lombardo and Lesmo (1996), use grammars with
context-free like rules which encode the preferred
order of dependents for each given governor, as de-
fined by Gaifman (1965). For example, a rule of the
form N(Det ? PP ) is used to allow N to have Det
as left dependent and PP as right dependent.
The algorithm by Lombardo and Lesmo (1996)
is a version of Earley?s context-free grammar parser
(Earley, 1970) using Gaifman?s dependency gram-
mar, and can be written by using an item set
ILomLes = {[A(?.?), i, j] | A(??) ? P ?
1 ? i ? j ? n}, where each item [A(?.?), i, j] rep-
resents the set of partial dependency trees rooted at
A, where the direct children of A are ??, and the
subtrees rooted at ? have yield wi . . . wj . The de-
duction steps for the schema are shown in Figure 2,
and the final item set is {[(S.), 1, n]}.
As we can see, the schema for Lombardo and
Lesmo?s parser resembles the Earley-style parser in
Sikkel (1997), with some changes to adapt it to de-
pendency grammar (for example, the Scanner al-
ways moves the dot over the head symbol ?).
Analogously, other dependency parsing schemata
based on CFG-like rules can be obtained by mod-
ifying context-free grammar parsing schemata of
Sikkel (1997) in a similar way. The algorithm by
Barbero et al (1998) can be obtained from the left-
corner parser, and the one by Courtin and Genthial
(1998) is a variant of the head-corner parser.
3.6 Pseudo-projectivity
Pseudo-projective parsers can generate non-
projective analyses in polynomial time by using
a projective parsing strategy and postprocessing
the results to establish nonprojective links. For
example, the algorithm by Kahane et al (1998) uses
a projective parsing strategy like that of LL96, but
using the following initializer step instead of the
972
Initter and Predictor:5
Initter [A(?), i, i ? 1] A(?) ? P ? 1 ? i ? n
4 Relations between dependency parsers
The framework of parsing schemata can be used to
establish relationships between different parsing al-
gorithms and to obtain new algorithms from existing
ones, or derive formal properties of a parser (such as
soundness or correctness) from the properties of re-
lated algorithms.
Sikkel (1994) defines several kinds of relations
between schemata, which fall into two categories:
generalisation relations, which are used to obtain
more fine-grained versions of parsers, and filtering
relations, which can be seen as the reverse of gener-
alisation and are used to reduce the number of items
and/or steps needed for parsing. He gives a formal
definition of each kind of relation. Informally, a
parsing schema can be generalised from another via
the following transformations:
? Item refinement: We say that P1 ir?? P2 (P2 is an
item refinement of P1) if there is a mapping be-
tween items in both parsers such that single items
in P1 are broken into multiple items in P2 and in-
dividual deductions are preserved.
? Step refinement: We say that P1 sr?? P2 if the
item set of P1 is a subset of that of P2 and every
single deduction step in P1 can be emulated by a
sequence of inferences in P2.
On the other hand, a schema can be obtained from
another by filtering in the following ways:
? Static/dynamic filtering: P1
sf/df???? P2 if the item
set of P2 is a subset of that of P1 and P2 allows a
subset of the direct inferences in P16.
? Item contraction: The inverse of item refinement.
P1 ic?? P2 if P2 ir?? P1.
? Step contraction: The inverse of step refinement.
P1 sc?? P2 if P2 sr?? P1.
All the parsers described in section 3 can be re-
lated via generalisation and filtering, as shown in
Figure 3. For space reasons we cannot show formal
proofs of all the relations, but we sketch the proofs
for some of the more interesting cases:
5The initialization step as reported in Kahane?s paper is dif-
ferent from this one, as it directly consumes a nonterminal from
the input. However, using this step results in an incomplete
algorithm. The problem can be fixed either by using the step
shown here instead (bottom-up Earley strategy) or by adding an
additional step turning it into a bottom-up Left-Corner parser.
6Refer to Sikkel (1994) for the distinction between static and
dynamic filtering, which we will not use here.
4.1 YM03 sr?? Eis96
It is easy to see from the schema definitions that
IY M03 ? IEis96. In order to prove the relation
between these parsers, we need to verify that every
deduction step in YM03 can be emulated by a se-
quence of inferences in Eis96. In the case of the
Initter step this is trivial, since the Initters of both
parsers are equivalent. If we write the R-Link step in
the notation we have used for Eisner items, we have
R-Link [i, j, F, F ] [j, k, F, F ]
[i, k, F, F ] wj ? wk
This can be emulated in Eisner?s parser by an
R-Link step followed by a CombineSpans step:
[j, k, F, F ] ` [j, k, T, F ] (by R-Link),
[j, k, T, F ], [i, j, F, F ] ` [i, k, F, F ] (by CombineSpans).
Symmetrically, the L-Link step in YM03 can be
emulated by an L-Link followed by a CombineSpans
in Eis96.
4.2 ES99 sr?? Eis96
If we write the R-Link step in Eisner and Satta?s
parser in the notation for Eisner items, we have
R-Link [i, j, F, T ] [j + 1, k, T, F ][i, k, T, F ] wi ? wk
This inference can be emulated in Eisner?s parser
as follows:
` [j, j + 1, F, F ] (by Initter),
[i, j, F, T ], [j, j + 1, F, F ] ` [i, j + 1, F, F ] (CombineSpans),
[i, j + 1, F, F ], [j + 1, k, T, F ] ` [i, k, F, F ] (CombineSpans),
[i, k, F, F ] ` [i, k, T, F ] (by R-Link).
The proof corresponding to the L-Link step is sym-
metric. As for the R-Combiner and L-Combiner
steps in ES99, it is easy to see that they are partic-
ular cases of the CombineSpans step in Eis96, and
therefore can be emulated by a single application of
CombineSpans.
Note that, in practice, the relations in sections 4.1
and 4.2 mean that the ES99 and YM03 parsers are
superior to Eis96, since they generate fewer items
and need fewer steps to perform the same deduc-
tions. These two parsers also have the interesting
property that they use disjoint item sets (one uses
items representing trees while the other uses items
representing pairs of trees); and the union of these
disjoint sets is the item set used by Eis96. Also note
that the optimisation in YM03 comes from contract-
ing deductions in Eis96 so that linking operations
are immediately followed by combining operations;
while ES99 does the opposite, forcing combining
operations to be followed by linking operations.
4.3 Other relations
If we generalise the linking steps in ES99 so that the
head of each item can be in any position, we obtain a
973
Figure 3: Formal relations between several well-known dependency parsers. Arrows going upwards correspond to
generalisation relations, while those going downwards correspond to filtering. The specific subtype of relation is
shown in each arrow?s label, following the notation in Section 4.
correct O(n5) parser which can be filtered to Col96
just by eliminating the Combiner steps.
From Col96, we can obtain an O(n5) head-corner
parser based on CFG-like rules by an item refine-
ment in which each Collins item [i, j, h] is split into
a set of items [A(?.?.?), i, j, h]. Of course, the for-
mal refinement relation between these parsers only
holds if the D-rules used for Collins? parser corre-
spond to the CFG rules used for the head-corner
parser: for every D-rule B ? A there must be a
corresponding CFG-like rule A ? . . . B . . . in the
grammar used by the head-corner parser.
Although this parser uses three indices i, j, h, us-
ing CFG-like rules to guide linking decisions makes
the h indices unnecessary, so they can be removed.
This simplification is an item contraction which re-
sults in an O(n3) head-corner parser. From here,
we can follow the procedure in Sikkel (1994) to
relate this head-corner algorithm to parsers analo-
gous to other algorithms for context-free grammars.
In this way, we can refine the head-corner parser
to a variant of de Vreught and Honig?s algorithm
(Sikkel, 1997), and by successive filters we reach a
left-corner parser which is equivalent to the one de-
scribed by Barbero et al (1998), and a step contrac-
tion of the Earley-based dependency parser LL96.
The proofs for these relations are the same as those
described in Sikkel (1994), except that the depen-
dency variants of each algorithm are simpler (due
to the absence of epsilon rules and the fact that the
rules are lexicalised).
5 Proving correctness
Another useful feature of the parsing schemata
framework is that it provides a formal way to de-
fine the correctness of a parser (see last paragraph
of Section 1.1) which we can use to prove that our
parsers are correct. Furthermore, relations between
schemata can be used to derive the correctness of
a schema from that of related ones. In this sec-
tion, we will show how we can prove that the YM03
and ES99 algorithms are correct, and use that fact to
prove the correctness of Eis96.
5.1 ES99 is correct
In order to prove the correctness of a parser, we must
prove its soundness and completeness (see section
1.1). Soundness is generally trivial to verify, since
we only need to check that every individual deduc-
tion step in the parser infers a correct consequent
item when applied to correct antecedents (i.e., in this
case, that steps always generate non-empty items
that conform to the definition in 3.3). The difficulty
is proving completeness, for which we need to prove
that all correct final items are valid (i.e., can be in-
ferred by the schema). To show this, we will prove
the stronger result that all correct items are valid.
We will show this by strong induction on the
length of items, where the length of an item ? =
[i, k, h] is defined as length(?) = k ? i + 1. Cor-
rect items of length 1 are the hypotheses of the
schema (of the form [i, i, i]) which are trivially valid.
We will prove that, if all correct items of length m
are valid for all 1 ? m < l, then items of length l
are also valid.
Let [i, k, i] be an item of length l in IES99 (thus,
l = k? i+1). If this item is correct, then it contains
a grounded dependency tree t such that yield(t) =
wi . . . wk and head(t) = wi.
By construction, the root of t is labelled wi. Let
wj be the rightmost daughter of wi in t. Since t
is projective, we know that the yield of wj must be
of the form wl . . . wk, where i < l ? j ? k. If
l < j, then wl is the leftmost transitive dependent of
wj in t, and if k > j, then we know that wk is the
rightmost transitive dependent of wj in t.
Let tj be the subtree of t rooted at wj . Let t1 be
the tree obtained from removing tj from t. Let t2 be
974
the tree obtained by removing all the children to the
right of wj from tj , and t3 be the tree obtained by re-
moving all the children to the left of wj from tj . By
construction, t1 belongs to a correct item [i, l? 1, i],
t2 belongs to a correct item [l, j, j] and t3 belongs to
a correct item [j, k, j]. Since these three items have
a length strictly less than l, by the inductive hypoth-
esis, they are valid. This allows us to prove that the
item [i, k, i] is also valid, since it can be obtained
from these valid items by the following inferences:
[i, l ? 1, i], [l, j, j] ` [i, j, i] (by the L-Link step),
[i, j, i], [j, k, j] ` [i, k, i] (by the L-Combiner step).
This proves that all correct items of length l which
are of the form [i, k, i] are correct under the induc-
tive hypothesis. The same can be proved for items of
the form [i, k, k] by symmetric reasoning, thus prov-
ing that the ES99 parsing schema is correct.
5.2 YM03 is correct
In order to prove correctness of this parser, we fol-
low the same procedure as above. Soundness is
again trivial to verify. To prove completeness, we
use strong induction on the length of items, where
the length of an item [i, j] is defined as j ? i + 1.
The induction step is proven by considering any
correct item [i, k] of length l > 2 (l = 2 is the base
case here since items of length 2 are generated by
the Initter step) and proving that it can be inferred
from valid antecedents of length less than l, so it is
valid. To show this, we note that, if l > 2, either
wi has at least a right dependent or wk has at least a
left dependent in the item. Supposing that wi has a
right dependent, if t1 and t2 are the trees rooted at wi
and wk in a forest in [i, k], we call wj the rightmost
daughter of wi and consider the following trees:
v = the subtree of t1 rooted at wj , u1 = the tree ob-
tained by removing v from t1, u2 = the tree obtained
by removing all children to the right of wj from v,
u3 = the tree obtained by removing all children to
the left of wj from v.
We observe that the forest {u1, u2} belongs to the
correct item [i, j], while {u3, t2} belongs to the cor-
rect item [j, k]. From these two items, we can obtain
[i, k] by using the L-Link step. Symmetric reason-
ing can be applied if wi has no right dependents but
wk has at least a left dependent, and analogously to
the case of the previous parser, we conclude that the
YM03 parsing schema is correct.
5.3 Eis96 is correct
By using the previous proofs and the relationships
between schemata that we explained earlier, it is
easy to prove that Eis96 is correct: soundness is,
as always, straightforward, and completeness can be
proven by using the properties of other algorithms.
Since the set of final items in Eis96 and ES99 are
the same, and the former is a step refinement of the
latter, the completeness of ES99 directly implies the
completeness of Eis96.
Alternatively, we can use YM03 to prove the cor-
rectness of Eis96 if we redefine the set of final items
in the latter to be of the form [0, n+ 1, F, F ], which
are equally valid as final items since they always
contain parse trees. This idea can be applied to trans-
fer proofs of completeness across any refinement re-
lation.
6 Conclusions
We have defined a variant of Sikkel?s parsing
schemata formalism which allows us to represent
dependency parsing algorithms in a simple, declar-
ative way7. We have clarified relations between
parsers which were originally described very differ-
ently. For example, while Eisner presented his algo-
rithm as a dynamic programming algorithm which
combines spans into larger spans, Yamada and Mat-
sumoto?s works by sequentially executing parsing
actions that move a focus point in the input one po-
sition to the left or right, (possibly) creating a de-
pendency link. However, in the parsing schemata
for these algorithms we can see (and formally prove)
that they are related: one is a refinement of the other.
Parsing schemata are also a formal tool that can be
used to prove the correctness of parsing algorithms.
The relationships between dependency parsers can
be exploited to derive properties of a parser from
those of others, as we have seen in several examples.
Although the examples in this paper are cen-
tered in projective dependency parsing, the formal-
ism does not require projectivity and can be used to
represent nonprojective algorithms as well8. An in-
teresting line for future work is to use relationships
between schemata to find nonprojective parsers that
can be derived from existing projective counterparts.
7An alternative framework that formally describes some de-
pendency parsers is that of transition systems (McDonald and
Nivre, 2007). This model is based on parser configurations and
transitions, and has no clear relationship with the approach de-
scribed here.
8Note that spanning tree parsing algorithms based on edge-
factored models, such as the one by McDonald et al (2005b)
are not constructive in the sense outlined in Section 2, so the
approach described here does not directly apply to them. How-
ever, other nonprojective parsers such as (Attardi, 2006) follow
a constructive approach and can be analysed deductively.
975
References
Miguel A. Alonso, Eric de la Clergerie, David Cabrero,
and Manuel Vilares. 1999. Tabular algorithms for
TAG parsing. In Proc. of the Ninth Conference on Eu-
ropean chapter of the Association for Computational
Linguistics, pages 150?157, Bergen, Norway. ACL.
Giuseppe Attardi. 2006. Experiments with a Multilan-
guage Non-Projective Dependency Parser. In Proc. of
the Tenth Conference on Natural Language Learning
(CoNLL-X), pages 166?170, New York, USA. ACL.
Cristina Barbero, Leonardo Lesmo, Vincenzo Lombarlo,
and Paola Merlo. 1998. Integration of syntactic
and lexical information in a hierarchical dependency
grammar. In Proc. of the Workshop on Dependency
Grammars, pages 58?67, ACL-COLING, Montreal,
Canada.
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forest in ambiguous parsing. In Proc. of the
27th Annual Meeting of the Association for Computa-
tional Linguistics, pages 143?151, Vancouver, British
Columbia, Canada, June. ACL.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc. of
the 34th annual meeting on Association for Compu-
tational Linguistics, pages 184?191, Morristown, NJ,
USA. ACL.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using Bayes Point Machines. In Proc. of the main
conference on Human Language Technology Confer-
ence of the North American Chapter of the Association
of Computational Linguistics, pages 160?167, Morris-
town, NJ, USA. ACL.
Jacques Courtin and Damien Genthial. 1998. Parsing
with dependency relations and robust parsing. In Proc.
of the Workshop on Dependency Grammars, pages 88?
94, ACL-COLING, Montreal, Canada.
Michael A. Covington. 1990. A dependency parser for
variable-word-order languages. Technical Report AI-
1990-01, Athens, GA.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head au-
tomaton grammars. In Proc. of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, pages 457?464, Mor-
ristown, NJ, USA. ACL.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. of the
16th International Conference on Computational Lin-
guistics (COLING-96), pages 340?345, Copenhagen,
August.
Haim Gaifman. 1965. Dependency systems and phrase-
structure systems. Information and Control, 8:304?
337.
Carlos Go?mez-Rodr??guez, Jesu?s Vilares, and Miguel A.
Alonso. 2007. Compiling declarative specifications
of parsing algorithms. In Database and Expert Sys-
tems Applications, volume 4653 of Lecture Notes in
Computer Science, pages 529?538, Springer-Verlag.
David Hays. 1964. Dependency theory: a formalism and
some observations. Language, 40:511?525.
Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In COLING-ACL,
pages 646?652.
Vincenzo Lombardo and Leonardo Lesmo. 1996. An
Earley-type recognizer for dependency grammar. In
Proc. of the 16th conference on Computational linguis-
tics, pages 723?728, Morristown, NJ, USA. ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency
parsers. In ACL ?05: Proc. of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
91?98, Morristown, NJ, USA. ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov and Jan
Hajic?. 2005b. Non-projective dependency parsing us-
ing spanning tree algorithms. In HLT ?05: Proc. of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 523?530. ACL.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the Errors of Data-Driven Dependency Parsing
Models. In Proc. of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 122?131.
Joakim Nivre. 2006. Inductive Dependency Parsing
(Text, Speech and Language Technology). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.
Stuart M. Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. Journal of Logic Programming, 24:3?
36.
Klaas Sikkel. 1994. How to compare the structure of
parsing algorithms. In G. Pighizzini and P. San Pietro,
editors, Proc. of ASMICS Workshop on Parsing The-
ory. Milano, Italy, Oct 1994, pages 21?39.
Klaas Sikkel. 1997. Parsing Schemata ? A Framework
for Specification and Analysis of Parsing Algorithms.
Texts in Theoretical Computer Science ? An EATCS
Series. Springer-Verlag, Berlin/Heidelberg/New York.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proc. of 8th International Workshop on Parsing Tech-
nologies, pages 195?206.
976
  
	
	
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 7?12,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Distributional Similarity of Sub-Parses
Julie Weeds, David Weir and Bill Keller
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{juliewe, davidw, billk}@sussex.ac.uk
Abstract
This work explores computing distribu-
tional similarity between sub-parses, i.e.,
fragments of a parse tree, as an extension
to general lexical distributional similarity
techniques. In the same way that lexical
distributional similarity is used to estimate
lexical semantic similarity, we propose us-
ing distributional similarity between sub-
parses to estimate the semantic similarity of
phrases. Such a technique will allow us to
identify paraphrases where the component
words are not semantically similar. We
demonstrate the potential of the method by
applying it to a small number of examples
and showing that the paraphrases are more
similar than the non-paraphrases.
1 Introduction
An expression is said to textually entail another ex-
pression if the meaning of the second expression can
be inferred from the meaning of the first. For exam-
ple, the sentence ?London is an English city,? tex-
tually entails the sentence ?London is in England.?
As discussed by Dagan et al (2005) in their intro-
duction to the first Recognising Textual Entailment
Challenge, identifying textual entailment can be seen
as a subtask of a variety of other natural language
processing (NLP) tasks. For example, Question An-
swering (QA) can be cast as finding an answer which
is entailed by the proposition in the question. Other
identified tasks include summarization, paraphras-
ing, Information Extraction (IE), Information Re-
trieval (IR) and Machine Translation (MT).
The Natural Habitats (NatHab) project1 (Weeds
et al, 2004; Owen et al, 2005) provides an inter-
esting setting in which to study paraphrase and tex-
1http://www.informatics.susx.ac.uk/projects/nathab/
tual entailment recognition as a tool for natural lan-
guage understanding. The aim of the project is to
enable non-technical users to configure their perva-
sive computing environments. They do this by stat-
ing policies in natural language which describe how
they wish their environment to behave. For exam-
ple, a user, who wishes to restrict the use of their
colour printer to the printing of colour documents,
might have as a policy, ?Never print black-and-white
documents on my colour printer.? Similarly, a user,
who wishes to be alerted by email when their mobile
phone battery is low, might have as a policy, ?If my
mobile phone battery is low then send me an email.?
The natural language understanding task is to in-
terpret the user?s utterance with reference to a set
of policy templates and an ontology of services (e.g.
print) and concepts (e.g. document). The use of pol-
icy templates and an ontology restricts the number of
possible meanings that a user can express. However,
there is still considerable variability in the way these
policies can be expressed. Simple variations on the
theme of the second policy above include, ?Send me
an email whenever my mobile phone battery is low,?
and ?If the charge on my mobile phone is low then
email me.? Our approach is to tackle the interpreta-
tion problem by identifying parts of expressions that
are paraphrases of those expressions whose interpre-
tation with respect to the ontology is more directly
encoded. Here, we investigate extending distribu-
tional similarity methods from words to sub-parses.
The rest of this paper is organised as follows. In
Section 2 we discuss the background to our work.
We consider the limitations of an approach based on
lexical similarity and syntactic templates, which mo-
tivates us to look directly at the similarity of larger
units. In Section 3, we introduce our proposed ap-
proach, which is to measure the distributional simi-
larity of sub-parses. In Section 4, we consider exam-
ples from the Pascal Textual Entailment Challenge
7
Datasets2 (Dagan et al, 2005) and demonstrate em-
pirically how similarity can be found between corre-
sponding phrases when parts of the phrases cannot
be said to be similar. In Section 5, we present our
conclusions and directions for further work.
2 Background
One well-studied approach to the identification of
paraphrases is to employ a lexical similarity func-
tion. As noted by Barzilay and Elhadad (2003), even
a lexical function that simply computes word over-
lap can accurately select paraphrases. The prob-
lem with such a function is not in the accuracy of
the paraphrases selected, but in its low recall. One
popular way of improving recall is to relax the re-
quirement for words in each sentence to be identi-
cal in form, to being identical or similar in mean-
ing. Methods to find the semantic similarity of two
words can be broadly split into those which use lex-
ical resources, e.g., WordNet (Fellbaum, 1998), and
those which use a distributional similarity measure
(see Weeds (2003) for a review of distributional sim-
ilarity measures). Both Jijkoun and deRijke (2005)
and Herrara et al (2005) show how such a measure
of lexical semantic similarity might be incorporated
into a system for recognising textual entailment be-
tween sentences.
Previous work on the NatHab project (Weeds et
al., 2004) used such an approach to extend lexi-
cal coverage. Each of the user?s uttered words was
mapped to a set of candidate words in a core lexicon3,
identified using a measure of distributional similar-
ity. For example, the word send is used when talk-
ing about printing or about emailing, and a good
measure of lexical similarity would identify both of
these conceptual services as candidates. The best
choice of candidate was then chosen by optimising
the match between grammatical dependency rela-
tions and paths in the ontology over the entire sen-
tence. For example, an indirect-object relation be-
tween the verb send and a printer can be mapped to
the path in the ontology relating a print request to
its target printer.
As well as lexical variation, our previous work
(Weeds et al, 2004) allowed a certain amount of
syntactic variation via its use of grammatical depen-
dencies and policy templates. For example, the pas-
sive ?paraphrase? of a sentence can be identified by
comparing the sets of grammatical dependency rela-
tions produced by a shallow parser such as the RASP
2http://www.pascal-network.org/Challenges/RTE/
3The core lexicon lists a canonical word form for each
concept in the ontology.
parser (Briscoe and Carroll, 1995). In other words,
by looking at grammatical dependency relations, we
can identify that ?John is liked by Mary,? is a para-
phrase of ?Mary likes John,? and not of ?John likes
Mary.? Further, where there is a limited number of
styles of sentence, we can manually identify and list
other templates for matches over the trees or sets of
dependency relations. For example, ?If C1 then C2?
is the same as ?C2 if C1?.
However, the limitations of this approach, which
combines lexical variation, grammatical dependency
relations and template matching, become increas-
ingly obvious as one tries to scale up. As noted by
Herrera (2005), similarity at the word level is not
required for similarity at the phrasal level. For ex-
ample, in the context of our project, the phrases ?if
my mobile phone needs charging? and ?if my mobile
phone battery is low? have the same intended mean-
ing but it is not possible to obtain the second by
making substitutions for similar words in the first. It
appears that ?X needs charging? and ?battery (of X)
is low? have roughly similar meanings without their
component words having similar meanings. Further,
this does not appear to be due to either phrase being
non-compositional. As noted by Pearce (2001), it is
not possible to substitute similar words within non-
compositional collocations. In this case, however,
both phrases appear to be compositional. Words
cannot be substituted between the two phrases be-
cause they are composed in different ways.
3 Proposal
Recently, there has been much interest in find-
ing words which are distributionally similar e.g.,
Lin (1998), Lee (1999), Curran and Moens (2002),
Weeds (2003) and Geffet and Dagan (2004). Two
words are said to be distributionally similar if they
appear in similar contexts. For example, the two
words apple and pear are likely to be seen as the
objects of the verbs eat and peel, and this adds to
their distributional similarity. The Distributional
Hypothesis (Harris, 1968) proposes a connection be-
tween distributional similarity and semantic simi-
larity, which is the basis for a large body of work
on automatic thesaurus construction using distribu-
tional similarity methods (Curran and Moens, 2002;
Weeds, 2003; Geffet and Dagan, 2004).
Our proposal is that just as words have distribu-
tional similarity which can be used, with at least
some success, to estimate semantic similarity, so do
larger units of expression. We propose that the unit
of interest is a sub-parse, i.e., a fragment (connected
subgraph) of a parse tree, which can range in size
from a single word to the parse for the entire sen-
8
my
mobile
needs
phone charging
my
mobile
phone
low
is
battery
Figure 1: Parse trees for ?my mobile phone needs
charging? and ?my mobile phone battery is low?
tence. Figure 1 shows the parses for the clauses,
?my mobile phone needs charging,? and ?my mobile
phone battery is low? and highlights the fragments
(?needs charging? and ?battery is low?) for which we
might be interested in finding similarity.
In our model, we define the features or contexts of
a sub-parse to be the grammatical relations between
any component of the sub-parse and any word out-
side of the sub-parse. In the example above, both
sub-parses would have features based on their gram-
matical relation with the word phone. The level of
granularity at which to consider grammatical rela-
tions remains a matter for investigation. For exam-
ple, it might turn out to be better to distinguish
between all types of dependent or, alternatively, it
might be better to have a single class which covers
all dependents. We also consider the parents of the
sub-parse as features. In the example, ?Send me an
email if my mobile phone battery is low,? this would
be that the sub-parse modifies the verb send i.e., it
has the feature, <mod-of, send>.
Having defined these models for the unit of inter-
est, the sub-parse, and for the context of a sub-parse,
we can build up co-occurrence vectors for sub-parses
in the same way as for words. A co-occurrence vec-
tor is a conglomeration (with frequency counts) of
all of the co-occurrences of the target unit found in
a corpus. The similarity between two such vectors
or descriptions can then be found using a standard
distributional similarity measure (see Weeds (2003)).
The use of distributional evidence for larger units
than words is not new. Szpektor et al (2004) auto-
matically identify anchors in web corpus data. An-
chors are lexical elements that describe the context
of a sentence and if words are found to occur with
the same set of anchors, they are assumed to be
paraphrases. For example, the anchor set {Mozart,
1756} is a known anchor set for verbs with the mean-
ing ?born in?. However, this use of distributional
evidence requires both anchors, or contexts, to oc-
cur simultaneously with the target word. This dif-
fers from the standard notion of distributional sim-
ilarity which involves finding similarity between co-
occurrence vectors, where there is no requirement for
two features or contexts to occur simulultaneously.
Our work with distributional similarity is a gen-
eralisation of the approach taken by Lin and Pantel
(2001). These authors apply the distributional sim-
ilarity principle to paths in a parse tree. A path
exists between two words if there are grammatical
relations connecting them in a sentence. For exam-
ple, in the sentence ?John found a solution to the
problem,? there is a path between ?found? and ?so-
lution? because solution is the direct object of found.
Contexts of this path, in this sentence, are then the
grammatical relations <ncsubj, John> and <iobj,
problem> because these are grammatical relations
associated with either end of the path. In their work
on QA, Lin and Pantel restrict the grammatical re-
lations considered to two ?slots? at either end of the
path where the word occupying the slot is a noun.
Co-occurrence vectors for paths are then built up us-
ing evidence from multiple occurrences of the paths
in corpus data, for which similarity can then be cal-
culated using a standard metric (e.g., Lin (1998)).
In our work, we extend the notion of distributional
similarity from linear paths to trees. This allows us
to compute distributional similarity for any part of
an expression, of arbitrary length and complexity
(although, in practice, we are still limited by data
sparseness). Further, we do not make any restric-
tions as to the number or types of the grammatical
relation contexts associated with a tree.
4 Empirical Evidence
Practically demonstrating our proposal requires a
source of paraphrases. We first looked at the MSR
paraphrase corpus (Dolan et al, 2004) since it con-
tains a large number of sentences close enough in
meaning to be considered paraphrases. However, in-
spection of the data revealed that the lexical overlap
between the pairs of paraphrasing sentences in this
corpus is very high. The average word overlap (i.e.,
the proportion of exactly identical word forms) cal-
culated over the sentences paired by humans in the
training set is 0.70, and the lowest overlap4 for such
sentences is 0.3. This high word overlap makes this
a poor source of examples for us, since we wish to
study similarity between phrases which do not share
semantically similar words.
4A possible reason for this is that candidate sentences
were first identified automatically.
9
Consequently, for our purposes, the Pascal Textual
Entailment Recognition Challenge dataset is a more
suitable source of paraphrase data. Here the average
word overlap between textually entailing sentences is
0.39 and the lowest overlap is 0. This allows us to
easily find pairs of sub-parses which do not share sim-
ilar words. For example, in paraphrase pair id.19, we
can see that ?reduce the risk of diseases? entails ?has
health benefits?. Similarly in pair id.20, ?may keep
your blood glucose from rising too fast? entails ?im-
proves blood sugar control,? and in id.570, ?charged
in the death of? entails ?accused of having killed.?
In this last example there is semantic similarity
between the words used. The word charged is seman-
tically similar to accused. However, it is not possible
to swap the two words in these contexts since we do
not say ?charged of having killed.? Further, there is
an obvious semantic connection between the words
death and killed, but being different parts of speech
this would be easily missed by traditional distribu-
tional methods.
Consequently, in order to demonstrate the poten-
tial of our method, we have taken the phrases ?reduce
the risk of diseases?, ?has health benefits?, ?charged
in the death of? and ?accused of having killed?, con-
structed corpora for the phrases and their compo-
nents and then computed distributional similarity
between pairs of phrases and their respective com-
ponents. Under our hypotheses, paraphrases will be
more similar than non-paraphrases and there will be
no clear relation between the similarity of phrases as
a whole and the similarity of their components.
We now discuss corpus construction and distribu-
tional similarity calculation in more detail.
4.1 Corpus Construction
In order to compute distributional similarity between
sub-parses, we need to have seen a large number of
occurrences of each sub-parse. Since data sparse-
ness rules out using traditional corpora, such as the
British National Corpus (BNC), we constructed a
corpus for each phrase by mining the web. We also
constructed a similar corpus for each component of
each phrase. For example, for phrase 1, we con-
structed corpora for ?reduce the risk of diseases?,
?reduce? and ?the risk of diseases?. We do this in or-
der to avoid only have occurrences of the components
in the context of the larger phrase. Each corpus was
constructed by sending the phrase as a quoted string
to Altavista. We took the returned list of URLs (up
to the top 1000 where more than 1000 could be re-
turned), removed duplicates and then downloaded
the associated files. We then searched the files for
the lines containing the relevant string and added
Phrase Types Tokens
reduce the risk of diseases 156 389
reduce 3652 14082
the risk of diseases 135 947
has health benefits 340 884
has 3709 10221
health benefits 143 301
charged in the death of 624 1739
charged in 434 1011
the death of 348 1440
accused of having killed 88 173
accused of 679 1760
having killed 569 1707
Table 1: Number of feature types and tokens ex-
tracted for each Phrase
each of these to the corpus file for that phrase. Each
corpus file was then parsed using the RASP parser
(version 3.?) ready for feature extraction.
4.2 Computing Distributional Similarity
First, a feature extractor is run over each parsed cor-
pus file to extract occurrences of the sub-parse and
their features. The feature extractor reads in a tem-
plate for each phrase in the form of dependency re-
lations over lemmas. It checks each sentence parse
against the template (taking care that the same word
form is indeed the same occurrence of the word in the
sentence). When a match is found, the other gram-
matical relations5 for each word in the sub-parse are
output as features. When the sub-parse is only a
word, the process is simplified to finding grammati-
cal relations containing that word.
The raw feature file is then converted into a co-
occurrence vector by counting the occurrences of
each feature type. Table 1 shows the number of fea-
ture types and tokens extracted for each phrase. This
shows that we have extracted a reasonable number
of features for each phrase, since distributional sim-
ilarity techniques have been shown to work well for
words which occur more than 100 times in a given
corpus (Lin, 1998; Weeds and Weir, 2003).
We then computed the distributional similarity be-
tween each co-occurrence vector using the ?-skew
divergence measure (Lee, 1999). The ?-skew diver-
gence measure is an approximation to the Kullback-
Leibler (KL) divergence meassure between two dis-
tributions p and q:
D(p||q) =
?
x
p(x)log
p(x)
q(x)
5We currently retain all of the distinctions between
grammatical relations output by RASP.
10
The ?-skew divergence measure is designed to be
used when unreliable maximum likelihood estimates
(MLE) of probabilities would result in the KL diver-
gence being equal to ?. It is defined as:
dist?(q, r) = D(r||?.q + (1? ?).r)
where 0 ? ? ? 1. We use ? = 0.99, since this
provides a close approximation to the KL divergence
measure. The result is a number greater than or
equal to 0, where 0 indicates that the two distribu-
tions are identical. In other words, a smaller distance
indicates greater similarity.
The reason for choosing this measure is that it
can be used to compute the distance between any
two co-occurrence vectors independent of any infor-
mation about other words. This is in contrast to
many other measures, e.g., Lin (1998), which use the
co-occurrences of features with other words to com-
pute a weighting function such as mutual information
(MI) (Church and Hanks, 1989). Since we only have
corpus data for the target phrases, it is not possible
for us to use such a measure. However, the ?-skew
divergence measure has been shown (Weeds, 2003)
to perform comparably with measures which use MI,
particularly for lower frequency target words.
4.3 Results
The results, in terms of ?-skew divergence scores be-
tween pairs of phrases, are shown in Table 2. Each
set of three lines shows the similarity score between
a pair of phrases and then between respective pairs
of components. In the first two sets, the phrases
are paraphrases whereas in the second two sets, the
phrases are not.
From the table, there does appear to be some po-
tential in the use of distributional similarity between
sub-parses to identify potential paraphrases. In the
final two examples, the paired phrases are not se-
mantically similar, and as we would expect, their re-
spective distributional similarities are less (i.e., they
are further apart) than in the first two examples.
Further, we can see that there is no clear relation
between the similarity of two phrases and the simi-
larity of respective components. However in 3 out of
4 cases, the similarity between the phrases lies be-
tween that of their components. In every case, the
similarity of the phrases is less than the similarity
of the verbal components. This might be what one
would expect for the second example since the com-
ponents ?charged in? and ?accused of? are seman-
tically similar. However, in the first example, we
would have expected to see that the similarity be-
tween ?reduce the risk of diseases? and ?has health
Phrase 1 Phrase 2 Dist.
reduce the risk of diseases has health benefits 5.28
reduce has 4.95
the risk of diseases health benefits 5.58
charged in the death of accused of having killed 5.07
charged in accused of 4.86
the death of having killed 6.16
charged in the death of has health benefits 6.04
charged in has 5.54
the death of health benefits 4.70
reduce the risk of diseases accused of having killed 6.09
reduce accused of 5.77
the risk of diseases having killed 6.31
Table 2: ?-skew divergence scores between pairs of
phrases
benefits? to be greater than either pair of compo-
nents, which it is not. The reason for this is not clear
from just these examples. However, possibilities in-
clude the distributional similarity measure used, the
features selected from the corpus data and a combi-
nation of both. It may be that single words tend to
exhibit greater similarity than phrases due to their
greater relative frequencies. As a result, it may be
necessary to factor in the length or frequency of a
sub-parse into distributional similarity calculations
or comparisons thereof.
5 Conclusions and Further Work
In conclusion, it is clear that components of phrases
do not need to be semantically similar for the encom-
passing phrases to be semantically similar. Thus,
it is necessary to develop techniques which estimate
the semantic similarity of two phrases directly rather
than combining similarity scores calculated for pairs
of words.
Our approach is to find the distributional similar-
ity of the sub-parses associated with phrases by ex-
tending general techniques for finding lexical distri-
butional similarity. We have illustrated this method
for examples, showing how data sparseness can be
overcome using the web.
We have shown that finding the distributional sim-
ilarity between phrases, as outlined here, may have
potential in identifying paraphrases. In our exam-
ples, the distributional similarities of paraphrases
was higher than non-paraphrases. However, obvi-
ously, more extensive evaluation of the technique is
required before drawing more definite conclusions.
In this respect, we are currently in the pro-
cess of developing a gold standard set of similar
phrases from the Pascal Textual Entailment Chal-
11
lenge dataset. This task is not trivial since, even
though pairs of sentences are already identified as
potential paraphrases, it is still necessary to ex-
tract pairs of phrases which convey roughly the same
meaning. This is because 1) some pairs of sentences
are almost identical in word content and 2) some
pairs of sentences are quite distant in meaning sim-
ilarity. Further, it is also desirable to classify ex-
tracted pairs of paraphrases as to whether they are
lexical, syntactic, semantic or inferential in nature.
Whilst lexical (e.g. ?to gather? is similar to ?to col-
lect?) and syntactic (e.g. ?Cambodian sweatshop?
is equivalent to ?sweatshop in Cambodia?) are of in-
terest, our aim is to extend lexical techniques to the
semantic level (e.g. ?X won presidential election? is
similar to ?X became president?). Once our analysis
is complete, the data will be used to evaluate vari-
ations on the technique proposed herein and also to
compare it empirically to other techniques such as
that of Lin and Pantel (2001).
References
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP2003), pages
25?33, Sapporo, Japan.
Edward Briscoe and John Carroll. 1995. Developing and
evaluating a probabilistic lr parser of part-of-speech
and punctuation labels. In 4th ACL/SIGDAT Inter-
national Workshop on Parsing Technologies, pages 48?
58.
Kenneth W. Church and Patrick Hanks. 1989. Word
association norms, mutual information and lexicogra-
phy. In Proceedings of the 27th Annual Conference of
the Association for Computational Linguistics (ACL-
1989), pages 76?82.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In ACL-
SIGLEX Workshop on Unsupervised Lexical Acquisi-
tion, Philadelphia.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of the Recognising Textual En-
tailment Challenge 2005.
Bill Dolan, Chris Brockett, and Chris Quirk. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on Com-
putational Linguistics, Geneva.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Maayan Geffet and Ido Dagan. 2004. Feature vector
quality and distributional similarity. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING-2004), pages 247?253, Geneva.
Zelig S. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Jesus Herrera, Anselmo Penas, and Felisa Verdejo. 2005.
Textual entailment recognition based on dependency
analysis and wordnet. In Proceedings of the Recognis-
ing Textual Entailment Challenge 2005, April.
Valentin Jijkoun and Maarten de Rijke. 2005. Recognis-
ing textual entailment using lexical similarity. In Pro-
ceedings of the Recognising Textual Entailment Chal-
lenge 2005, April.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1999),
pages 23?32.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. Natural Language
Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguistics
and the 17th International Conference on Computa-
tional Linguistics (COLING-ACL ?98), pages 768?774,
Montreal.
Tim Owen, Ian Wakeman, Bill Keller, Julie Weeds, and
David Weir. 2005. Managing the policies of non-
technical users in a dynamic world. In IEEE Workshop
on Policy in Distributed Systems, Stockholm, Sweden,
May.
Darren Pearce. 2001. Synonymy in collocation extrac-
tion. In Proceedings of the NAACL Workshop on
WordNet and Other Lexical Resources: Applications,
Extensions and Customizations, Carnegie Mellon Uni-
versity, Pittsburgh.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of Empirical
Methods in Natural Language Processing (EMNLP)
2004, Barcelona.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2003), Sapporo, Japan.
Julie Weeds, Bill Keller, David Weir, Tim Owen, and
Ian Wakemna. 2004. Natural language expression of
user policies in pervasive computing environments. In
Proceedings of OntoLex2004, LREC Workshop on On-
tologies and Lexical Resources in Distributed Environ-
ments, Lisbon, Portugal, May.
Julie Weeds. 2003. Measures and Applications of Lexical
Distributional Similarity. Ph.D. thesis, Department of
Informatics, University of Sussex.
12
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2249?2259, Dublin, Ireland, August 23-29 2014.
Learning to Distinguish Hypernyms and Co-Hyponyms
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller
Department of Informatics,
University of Sussex,
Brighton, UK
juliewe,D.Clarke,J.P.Reffin,davidw,billk@sussex.ac.uk
Abstract
This work is concerned with distinguishing different semantic relations which exist between
distributionally similar words. We compare a novel approach based on training a linear Support
Vector Machine on pairs of feature vectors with state-of-the-art methods based on distributional
similarity. We show that the new supervised approach does better even when there is minimal
information about the target words in the training data, giving a 15% reduction in error rate over
unsupervised approaches.
1 Introduction
Over recent years there has been much interest in the field of distributional semantics, drawing on the
distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris,
1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and
Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of ?nearest neighbours?)
automatically and applied them in a variety of applications, generally with a good deal of success.
In early research there was much interest in how these automatically generated thesauri compare with
human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000).
More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Dis-
tributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala
et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), pre-
dicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh,
2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), tax-
onomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013).
A primary focus of distributional semantics has been on identifying words which are similar to each
other. However, semantic similarity encompasses a variety of different lexico-semantic and topical re-
lations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix
of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related
words. A central problem here is that whilst most measures of distributional similarity are symmetric,
some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)
which forms the ISA backbone of taxonomies and ontologies such as WordNet (Fellbaum, 1989), and
determines lexical entailment (Geffet and Dagan, 2005), is asymmetric. On the other hand, the co-
hyponymy relation which relates two words unrelated by hyponymy but sharing a (close) hypernym, is
symmetric, as are synonymy and antonymy. Table 1 shows the distributionally nearest neighbours of the
words cat, animal and dog. In the list for cat we can see 2 hypernyms and 13 co-hyponyms
1
.
1
We read cat in the sense domestic cat rather than big cat, hence tiger is a co-hyponym rather than hyponym
of cat.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2249
cat dog 0.32, animal 0.29, rabbit 0.27, bird 0.26, bear 0.26, monkey 0.26, mouse 0.25, pig 0.25,
snake 0.24, horse 0.24, rat 0.24, elephant 0.23, tiger 0.23, deer 0.23, creature 0.23
animal bird 0.36, fish 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29,
human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25
dog cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, fish
0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23
Table 1: Top 15 neighbours of cat, animal and dog generated using Lin?s similarity measure (Lin,
1998) considering all words and dependency features occurring 100 or more times in Wikipedia.
Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can
be useful to be able to distinguish between these different relationships. Consider the following two
sentences.
The cat ran across the road. (1)
The animal ran across the road. (2)
Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability
to determine whether entailment holds between the sentences, and in which direction, depends on the
ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we
know which is the hyponym and which is the hypernym?
In applying distributional semantics to the problem of textual entailment, there is a need to generalise
lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations
is crucial if approaches to the composition of distributional representations of meaning that are currently
receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual
entailment problem.
We formulate the challenge as follows: Consider a set of pairs of similar words ?A,B? where one of
three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are
related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section
2, we discuss existing attempts to address this problem through the use of various directional measures
of distributional similarity.
This paper considers the effectiveness of various supervised approaches, and makes the following
contributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations,
achieving a significant reduction in error rate in comparison to existing state-of-the-art methods based
on the notion of distributional generality. Second, by comparing two different data sets, one built from
BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important
insights into the requirements of a valid evaluation of supervised approaches, and provide a data set
for further research in this area. Third, we show that when learning how to determine an ontological
relationship between a pair of similar words by means of the word?s distributional vectors, quite different
vector operations are useful when identifying different ontological relationships. In particular, using the
difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding
the vectors works well for the co-hyponym task.
2 Related Work
Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the
alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She
found that this measure improved results in language modelling, when a word?s distribution is smoothed
using the distributions of its nearest neighbours.
Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words
tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be
able to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible
2250
contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g.,
?the monstrous animal barked at the intruder?. Weeds et al. (2004) attempt to capture this asymmetry
by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision
and recall are defined as:
P
ww
(u, v) =
?
f?F (u)?F (v)
I(u, f)
?
f?F (u)
I(u, f)
and R
ww
(u, v) =
?
f?F (u)?F (v)
I(v, f)
?
f?F (v)
I(v, f)
where I(n, f) is the pointwise mutual information (PMI) between noun n and feature f and F(n) is the
set of all features f for which I(n, f) > 0.
By comparing the precision and recall of one word?s retrieval of another word?s contexts, they were
able to successfully identify the direction of an entailment relation in 71% of pairs drawn from WordNet.
However, this was not significantly better than a baseline which proposed that the most frequent word
was the most general.
Clarke (2009) formalised the idea of distributional generality using a partially ordered vector space.
He also argued for using a variation of co-occurrence retrieval where precision and recall are defined as:
P
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (u)
I(u, f)
and R
cl
(u, v) =
?
f?F (u)?F (v)
min(I(u, f), I(v, f))
?
f?F (v)
I(v, f)
Lenci and Benotto (2012) took the notion further and hypothesised that more general terms should
have high recall and low precision, which would thus make it possible to distinguish them from other
related terms such as synonyms and co-hyponyms. They proposed a variant of the Clarke (2009) measure
to identify hypernyms:
invCL(u, v) =
2
?
P
cl
(u, v) ? (1?R
cl
(u, v))
Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distin-
guishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009).
Geffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale
of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong cor-
relation between complete inclusion of prominent features and lexical entailment. However, they were
unable to assess this using an off-line corpus due to data sparseness.
Szpektor and Dagan (2008) found that the P
ww
measure tends to promote relationships between infre-
quent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed
using the geometric average of P
ww
and the symmetric similarity measure of Lin (1998) in order to
penalise low frequency words.
Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identi-
fying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similar-
ities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric
similarity measures previously proposed perform much better than symmetric similarity measures on
a directionality detection experiment, and that their method and that of Clarke (2009) outperform the
others with statistical significance. They also show that their measure is superior when used for term
expansion in an event detection task.
Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate
that support vector machines can generalise entailment relations between quantifier phrases to entailment
involving unseen quantifiers. They compare the performance of their system with the balAPinc measure.
The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large
corpora to find patterns that are indicative of hyponymy. For example, the pattern ?NP
X
and otherNP
Y
?
is an indication that NP
X
is a NP
Y
, i.e. that NP
X
is a hyponym of NP
Y
. They use machine learning
to identify other such patterns from known hyponym-hypernym pairs, and then use these patterns to find
new relations in the corpus. The transitivity relation of the taxonomy is enforced by searching only over
valid taxonomies and evaluating the likelihood of each taxonomy given the available evidence (Snow
2251
et al., 2006). The approach is similar to ours in providing a supervised method of learning semantic
relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms
themselves. Our approach is therefore more generally applicable to systems which compose distribu-
tional representations of meaning.
Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution.
In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional
similarity measure, WeightedCosine
2
, performs best. Also of note, Mikolov et al. (2013) propose a vector
offset method to capture syntactic and semantic regularities between word representations learnt by a
recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms
and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis.
Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which
is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative
than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS
dataset (Baroni and Lenci, 2011), this measure outperforms P
ww
at both discriminating hypernym test
pairs from other types of relation and at determining the direction of the entailment relation.
3 Methodology
The code used to perform our experiments has been open sourced, and is available online.
3
3.1 Vector Representations
Distributional information was collected for all of the nouns from Wikipedia provided they had oc-
curred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted
text using wp2txt
4
. This was part-of-speech tagged, lemmatised and dependency parsed using the Malt
Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were ex-
tracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive
point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The
total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345.
Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds be-
tween two similar words A and B. Support vector machines (SVMs), which are effective across a variety
of classification scenarios, learn a boundary between two classes from a set of positive and negative ex-
ample vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we
do not start with a single vector, but with two distributional vectors v
A
and v
B
for the words A and B,
respectively. These vectors must be combined in some way to produce the SVM?s input, and a number
of ways were considered, defined in Table 2. Of these operations, the vector difference (used by svm-
DIFF and knnDIFF) and direct sum (used by svmCAT) are asymmetric, whereas the sum and pointwise
multiplication (used by svmADD and svmMULT) are symmetric.
We now motivate the use of each of these operations. First, we note that pointwise multiplication
(svmMULT) is intersective. Similar vectors will have a large intersection and it might be possible to
learn the features that nouns occurring in different semantic relations should share. However, it does
not retain any information about non-shared features and it is symmetric so it is difficult to see how it
would be possible to use it to distinguish hypernyms from hyponyms. Pointwise addition (svmADD)
effectively performs the union of the features, giving emphasis to the shared features. Whilst it does
retain information about the non-shared features, it is also symmetric, making it difficult again to see
how it would be useful in determining the direction of an entailment relation
Vector difference (as used in svmDIFF and knnDIFF), on the other hand, is asymmetric. Further,
we might expect a small difference vector (containing many zeroes) to be indicative of similar nouns.
Further, considering the majority sign of features in this difference vector might indicate the direction of
2
The details of this measure are unpublished.
3
https://github.com/SussexCompSem/learninghypernyms
4
https://github.com/yohasebe/wp2txt
2252
entailment. Using an SVM, we might expect to be able to effectively learn which of these features should
be ignored and which should be combined, to decide the correct direction of entailment in the majority
number of cases in our training data. However, note that if one uses vector difference it is impossible to
distinguish between the case where a feature occurred with both nouns (to the same extent) and the case
where a feature occurs with neither noun. Accordingly, a small difference vector may indicate that both
nouns do not occur in many distinct contexts. A possible solution to this problem is to use the direct
sum of the vectors (i.e., the concatenation of the two vectors) which retains all of the information from
the original vectors. Finally, we consider the use of the single vector corresponding to the second word
(svmSING) as a baseline. High performance by this operation would indicate that we can learn features
of words which tend to be hypernyms (or co-hyponyms) without any regard to the other word in the
putative relationship.
We also note that the behaviour of these methods may differ depending on the weighting used for vec-
tors. For example, PMI is the log of a ratio of probabilities and therefore one might expect vector addition
where vectors are weighted using PMI to correspond to multiplication where vectors are weighted using
frequency or probability. However, the use of positive PMI (where negative PMI scores are regarded
equal to zero), which is consistent with other work in this area, means that this correspondence is lost.
Because of the nature of our datasets, we were concerned that systems could learn information about
the taxonomy from the relations in the training data, without making use of information in the vectors
themselves. To investigate this, we constructed random vectors to be used in place of the vectors derived
from Wikipedia. The dimensionality of the random vectors was chosen to be 1000 since this substantially
exceeds the average number (398) of non-zero features in the Wikipedia vectors.
3.2 Classifiers
We constructed linear SVMs for each of the vector operations outlined in Section 3.1. We used linear
SVMs for speed and simplicity, since the point is to compare the different vector representations of
the pairings. For comparison, we also constructed a number of supervised, unsupervised, and weakly
supervised classifiers. These are listed in Table 2. For the linear SVMs and kNN classifier, we used the
scikit-learn implementations with default settings. For k nearest neighbours, we performed a parameter
search, using nested cross-validation, varying k between 1 and 50.
For weakly supervised approaches, we evaluated the measure on the training set, then found the best
threshold p on the training set that best divides the two classes using that measure. When classifying, we
determine that the relation holds if the value of the measure exceeds p.
svmDIFF A linear SVM trained on the vector difference v
B
? v
A
svmMULT A linear SVM trained on the pointwise product vector v
B
? v
A
svmADD A linear SVM trained on the vector sum v
B
+ v
A
svmCAT A linear SVM trained on the vector concatenation v
B
? v
A
svmSING A linear SVM trained on the vector v
B
knnDIFF k nearest neighbours (knn) trained on the vector difference v
B
? v
A
.1 < k < 50
widthdiff width(B) > width(A)? rel(A,B) where width(A) is number of non-zero features in A
singlewidth width(B) > p? rel(A,B)
cosineP sim
cos
(A,B) > p? rel(A,B) where sim
cos
(A,B) is cosine similarity using PPMI
linP sim
lin
(A,B) > p? rel(A,B) (Lin, 1998)
CRdiff P
ww
(A,B) > R
ww
(A,B)? rel(A,B) (Weeds et al., 2004)
clarkediff P
cl
(A,B) > R
cl
(A,B)? rel(A,B) (Clarke, 2009)
invCLP invCL(A,B) > p? rel(A,B) (Lenci and Benotto, 2012)
balAPincP balAPinc(A,B) > p? rel(A,B) (Kotlerman et al., 2010)
most freq The most frequent label in the training data is assigned to every test point.
Table 2: Implemented classifiers
2253
3.3 Data Sets
One of key the challenges of this work has been to construct a data set which accurately and validly tests
our hypotheses. All four of our datasets detailed below are available online
5
.
In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the
supervised classifiers making use of artefacts of the data. This would not only make it unfair to compare
the supervised approaches with the unsupervised approaches, but also make it unlikely that our results
would be generalisable to other data. Here, we outline the requirements for the data sets, the importance
of which is demonstrated by our initial results for a data set which does not satisfy all of them.
There should be an equal number of positive and negative examples of a semantic relation. Thus,
random guessing or labelling with the most frequently seen label in the training data will yield 50%
accuracy and precision. An advantage of incorporating this requirement means that evaluation can be in
terms of simple accuracy (or error rate).
It should not be possible to do well simply by considering the distributional similarity of the terms.
Hence, the negative examples need to be pairs of equally similar words, but where the relationship under
consideration does not hold.
It should not be possible to do well by pre-supposing an entailment relation and guessing the direction.
For example, it has been shown (Weeds et al., 2004) that given a pair of entailing words selected from
WordNet, over 70% of the time the more frequent word is also the entailed word.
It should not be possible to do well using ontological information learnt about one or both of the
words from the training data that is not generalisable to their distributional representations. For example,
it should not be possible for the classifier simply to learn directly from the training pairs ?cat ISA
mammal? and ?mammal ISA animal? that ?cat ISA animal?. Furthermore, we must ensure that
a classifier cannot learn that a particular word is near the top of the ontological hierarchy, and, as a
result, do well by guessing that a particular pairing probably has an entailment relation. For example,
given many pairs such as ?cat ISA animal?, ?dog ISA animal?, a system which guessed ?rabbit
ISA animal? but not ?animal ISA rabbit? would do better than random guessing. Whilst both
of these types of information could be useful in a hybrid system, they do not require any distributional
information and therefore we would not be learning anything about the distributional features of animal
which make it likely to be a hypernym.
3.3.1 BLESS
We have constructed two data sets from BLESS (Baroni and Lenci, 2011) which is a collection of ex-
amples of hypernyms, co-hyponyms, meronyms and random unrelated words for each of 200 concrete,
largely monosemous nouns. We will refer to these 200 nouns as the BLESS concepts.
hyponym
BLESS
is a set of 1976 labelled pairs of nouns. For each BLESS concept, 80% of the hypernyms
were randomly selected to provide positive examples of entailment. The remaining hypernyms for the
given concept were reversed and taken with the same number of co-hyponyms, meronyms and random
words to form negative examples of entailment. A filter was applied to ensure that duplicate pairs were
not included (e.g., if ?cat,animal? is a positive pair then ?animal,cat? cannot be a negative pair).
cohyponym
BLESS
is a set of 5835 labelled pairs of nouns. For each BLESS concept, the co-hyponyms
were taken as positive examples of this relation. The same total number of (and split evenly between)
hypernyms, meronyms and random words was taken to form the negative examples. The order of 50%
of the pairs was reversed and again duplicate pairs were disallowed.
In both cases the pairs are labelled as positive or negative for the specified semantic relation and in
both cases there are equal (?1) numbers of positive and negative examples. For 99% of the generated
BLESS pairs, both nouns had associated vectors harvested from Wikipedia. If a noun does not have an
associated vector, the classifiers use a zero vector.
5
https://github.com/SussexCompSem/learninghypernyms
2254
3.3.2 WordNet
We constructed two data sets using WordNet. Whilst these data sets are similar in size to the BLESS
data sets they more adequately satisfy the requirements laid out above
6
. We constructed a list of all non-
rare, largely monosemous, single word terms in WordNet. To be considered non-rare, a word needed to
have occurred in SemCor at least once (i.e. frequency information is provided about it in the WordNet
package) and to have occurred in Wikipedia at least 100 times. To be considered largely monosemous,
the predominant sense of the word needed to account for over 50% of the occurrences in the SemCor
frequency information provided with WordNet. This led to a list of 7613 nouns.
hyponym
WN
is a set of 2564 labelled pairs of nouns constructed in the following way. Pairs ?A,B? were
found in the list of nouns where B is an ancestor of A (i.e., A lexically entails B). Each found pair is
added either as a positive or a negative in the ratio 2:1 provided that the reverse pairing has not already
been added and provided that each word has not previously been used in that position. Co-hyponym
pairs (i.e., words which share a direct hypernym) were also found within the list of nouns. Each found
pair is added to the data set (as a negative) provided the reverse pairing has not already been added, and
provided that neither word has already been seen in that position in a pairing (either in the entailment
pairs or the co-hyponym pairs). The same number of co-hyponym pairs as hypernym-hyponym negatives
is selected. This provides a balanced data set where half of the pairs are positive examples of entailment
and the other half are semantically similar but not entailing.
cohyponym
WN
is a set of 3771 labelled pairs of nouns. It was constructed in the same way as hyponym
WN
except the same number of co-hyponym pairs were selected as the total number of entailment pairs (in
either direction). These co-hyponym pairs were labelled as positive and the entailment pairs were labelled
as negative. Thus, this provides a balanced data set where half of the pairs are positive examples of co-
hyponyms and the other half, the negative examples, are entailment pairs (with direction unspecified)
In both these sets, the average path distance between entailment pairs is 1.64, whereas path distance
between co-hyponym pairs is 2.
3.4 Experimental Setup
Most of our experiments were carried out using an implementation of five-fold cross-validation using
each combination of data set, vector set and classifier. In this setup, the pairs are randomly partitioned
into five subsets, one subset is held out for testing whilst the classifiers are trained on the remaining four,
and this process is repeated using each subset as the test set.
In initial experiments with the BLESS datasets, the SVM classifiers were able to achieve classification
accuracy of over 95% for hyponym
BLESS
and over 90% for cohyponym
BLESS
. However, the results us-
ing random vectors were not significantly different from using the distributional vectors harvested from
Wikipedia. This indicated that the classifiers were learning ontological information implicit in the train-
ing data. In order to address this, when using the BLESS datasets, we removed any pair from the training
data if either word was present in the test data. In order to preserve a reasonable amount of training data,
we implemented this approach with ten-fold cross-validation. In all subsequent experiments, across all
datasets and classifiers, we found performance by the random vectors was no higher than 52%. This
indicates that the performance seen in Table 3 is due to learning from distributional features rather than
any ontological information implicit in the training set.
4 Results
In Table 3, we compare average accuracy for a number of different classifiers on each of two tasks,
distinguishing hyponyms and distinguishing co-hyponyms, on each of the two datasets.
Looking at the results for the hyponym
BLESS
data set, we can see that the SVM methods do generally
outperform the unsupervised methods. However, the best performing model is svmSING, suggesting
that, for this data set, it is best to try to learn the distributional features of more general terms, rather than
comparing the vector representations of the two terms under consideration.
6
Note that imposing these requirements on the BLESS data sets would lead to very small data sets, since information is only
provided for 200 nouns.
2255
dataset svmDIFF svmMULT svmADD svmCAT svmSING knnDIFF
hyponym
BLESS
0.74 0.56 0.66 0.68 0.75 0.54
cohyponym
BLESS
0.62 0.39 0.41 0.40 0.40 0.58
hyponym
WN
0.75 0.45 0.37 0.74 0.69 0.50
cohyponym
WN
0.37 0.60 0.68 0.64 0.58 0.50
dataset most freq cosineP linP widthdiff singlewidth CRdiff invCLP balAPincP
hyponym
BLESS
0.54 0.53 0.54 0.56 0.58 0.52 0.54 0.54
cohyponym
BLESS
0.61 0.79 0.78 - - - - -
hyponym
WN
0.50 0.53 0.52 0.70 0.65 0.70 0.66 0.53
cohyponym
WN
0.50 0.50 0.55 - - - - -
Table 3: Accuracy Figures for the data sets generated from BLESS and WordNet (standard errors <
0.02). For cohyponyms, results for measures designed to detect hyponymy have been omitted. We also
omit results of clarkediff as these were consistently the same or less than CRdiff.
On the corresponding co-hyponym task, using the cohyponym
BLESS
data set, we see the best performing
classifier is the cosine measure. The cosine measure is able to perform relatively well here because a
substantial proportion of the negative examples (25%) are random unrelated words which will have low
cosine scores. It is also consistent with earlier work (e.g., (Lenci and Benotto, 2012)) which suggests
that measures such as the cosine measure ?prefer? words in symmetric semantic relationships such as co-
hyponymy. The poor performance of the SVM methods here can perhaps be explained by the paucity of
the training data in this experimental set up with this data set. If, for example, our test concept is robin,
our approach requires that we will not have any training pairs containing robin, or any training pairs
containing any of the words to which robin is related in the test set. In a dataset as small as BLESS,
this requirement effectively removes all knowledge of the distributional features of words in the target
domain. Hence, the need for a larger dataset as we have extracted from WordNet.
Looking at the results for the hyponym
WN
data set, the directional SVM methods (svmDIFF and svm-
CAT) substantially outperform the symmetric SVM methods, and their performance is significantly better
(at the 0.01% level) than the unsupervised methods. Also of note is the substantial difference between
svmDIFF and knnDIFF. Both of these methods are trained on the differences of vectors. However, the
linear SVM outperforms kNN by 19?25%. This may suggest that the shape of the vector space inhabited
by the positive entailment pairs is particularly conducive for learning a linear SVM. Positive and negative
pairs are close together (as evidenced by the poor performance of kNN), but generally linearly separable.
Looking at the results for the cohyponym
WN
data set, it is clear that the unsupervised methods cannot
distinguish the co-hyponym pairs from the entailing pairs. The supervised SVM methods do substantially
better, with the best performance achieved by svmADD and svmCAT. Both of these methods essentially
retain information about all of the features of both words. svmMULT does much better than svmDIFF,
which suggests that the shared features are more indicative than the non-shared features for this task.
The reasonably high performance of svmSING on both data sets suggests that words which have co-
hyponyms in the data set tend to inhabit a somewhat different part of the feature space to words which
are included as entailed words in the data set. We hypothesise that there are specific features which more
general words tend to share (regardless of their topic) which makes it possible to identify more general
words from more specific words. This is completely consistent with very recent results using SLQS, a
new entropy-based measure (Santus et al., 2014). Here, the authors hypothesise that the most typical
contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms,
with some promising results. It would be plausible to hypothesise that svmSING is learning which nouns
typically have less informative contexts and are therefore likely to by hypernyms.
Given prior work, the performance of the balAPincP measure is lower than expected on the
hyponym
WN
dataset. Our task is slightly different to that of (Kotlerman et al., 2010), since we are deter-
mining the existence (or not) of hyponymy, rather than the direction of entailment for pairs where it is
known that a relationship exists. It could be that the measure is particularly suited to the latter task.
2256
5 Conclusions and Further Work
We have shown that it is possible to predict to a large extent whether or not there is a specific semantic
relation between two words given their distributional vectors, using a supervised approach based on
linear SVMs. The increase in accuracy over unsupervised methods is significant at the 0.01% level and
corresponds to a substantial absolute reduction in error rate (over 15%).
We have also shown that the choice of vector operation is significant. Whilst concatenating the vectors,
and therefore retaining all of the information from both vectors including direction, generally performs
well, we have also shown that different vector operations are useful in establishing different relationships.
In particular, the vector difference operation, which loses information about the original vectors, achieved
performance indistinguishable from concatenation on the entailment task, where the classifier is required
to distinguish hyponyms from other semantically related words including hypernyms. On the other
hand, the addition operation, which also loses information, outperformed concatenation by 4% (which
is statistically significant at the 0.01% level) on the coordinate task, where the classifier is required to
distinguish co-hyponyms from hyponyms and hypernyms. Hence the nature of the relationship one is
trying to establish between words determines the nature of the operation one should perform on their
associated vectors.
We have also shown that it is possible to outperform state-of-the-art unsupervised methods even when
a data set has been constructed without ontological information, and when target words have not previ-
ously been seen in that position of a relationship in the training data. Hence, we believe the supervised
methods are learning characteristics of the underlying feature space which are generalisable to new words
(inhabiting the same feature space).
In future work, we intend to apply this approach to the problem of labelling the distributional neigh-
bours found for a given word with specific semantic relations. We also plan to investigate the use of
bag-of-words (windowed) vectors instead of grammatical relations for this task.
Finally, we believe that the data sets constructed from WordNet, which we publish alongside this
paper, can be used as a useful benchmark in evaluating future advances in this area, both for supervised
and unsupervised methods.
Acknowledgements
This work was funded by UK EPSRC project EP/IO37458/1 ?A Unified Model of Compositional and
Distributional Compositional Semantics: Theory and Applications?.
References
Marco Baroni and Alessandro Lenci. 2011. How we BLESSed distributional semantic evaluation. In Proceedings
of the GEMS 2011 workshop on Geometric Models of Natural Language Semantics, EMNLP 2011.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word
level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 23?32. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Uppsala, Sweden, July. Association for Computational Linguistics.
Shane Bergsma, Aditya Bhargava, Hua He, and Grzegorz Kondrak. 2010. Predicting the semantic composi-
tionality of prefix verbs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 293?303, Cambridge, MA, October. Association for Computational Linguistics.
Danushka Bollegala, David Weir, and John Carroll. 2011. Using multiple sources to construct a sentiment sen-
sitive thesaurus for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies (ACL-HLT 2011).
2257
Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information, and lexicography.
In Proceedings of the 27th Annual Meeting on Association for Computational Linguistics, ACL ?89, pages
76?83, Stroudsburg, PA, USA. Association for Computational Linguistics.
Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the
Workshop of Geometric Models for Natural Language Semantics.
James Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.
Georgiana Dinu and Stefan Thater. 2012. Saarland: Vector-based models of semantic textual similarity. In
Proceedings of the First Joint Conference on Lexical and Computational Semantics.
Christaine Fellbaum, editor. 1989. WordNet: An Electronic Lexical Database. The MIT Press, Cambridge,
Massachusetts.
Trevor Fountain and Mirella Lapata. 2012. Taxonomy induction using hierarchical random graphs. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 466?476, Montr?eal, Canada, June.
Maayan Geffet and Ido Dagan. 2005. Lexical entailment and the distributional inclusion hypothesis. In Proceed-
ings of the 43rd meeting of the Association for Computational Liuguistics (ACL), pages 107?114.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen Clark, Bob Coecke, and Stephen Pulman. 2011. Concrete
sentence spaces for compositional distributional models of meaning. Proceedings of the 9th International Con-
ference on Computational Semantics (IWCS 2011), pages 125?134.
Zelig Harris. 1954. Distributional structure. Word, 10:146?162.
Mitesh Khapra, Anup Kulkarni, Saurabh Sohoney, and Pushpak Bhattacharyya. 2010. All words domain adapted
WSD: Finding a middle ground between supervision and unsupervision. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1532?1541, Uppsala, Sweden, July.
Adam Kilgarriff and Colin Yallop. 2000. What?s in a thesaurus? In Proceedings of the 2nd International
Conference on Language Resources and Evaluation (LREC2000).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional simi-
larity for lexical inference. Special Issue of Natural Language Engineering on Distributional Lexical Semantics,
4(16):359?389.
Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages 25?32, College Park, Maryland, USA, June.
Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceed-
ings of the First Joint Conference on Lexical and Computational Semantics (*Sem).
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th International
Conference on Computational Linguistics (COLING 1998).
Tara McIntosh. 2010. Unsupervised discovery of negative categories in lexicon bootstrapping. In Proceedings of
the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356?365, Cambridge, MA,
October. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word rep-
resentations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 746?751, Atlanta, Georgia, June.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for
lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING 2012, pages
1781?1796, Mumbai, India, December.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08:
HLT, pages 236?244, Columbus, Ohio, June. Association for Computational Linguistics.
Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the ACL workshop on
Incremental Parsing, pages 50?57.
2258
Marek Rei and Ted Briscoe. 2013. Parser lexicalisation through self-learning. In Proceedings of the 2013 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 391?400, Atlanta, Georgia, June. Association for Computational Linguistics.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte Im Walde. 2014. Chasing hypernyms in vector
spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for
Computational Linguistics, pages 38?42, Gothenburg, Sweden, April.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Processing Systems 17.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2006. Semantic taxonomy induction from heterogenous evidence.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 801?808. Association for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1201?1211.
Gy?orgy Szarvas, Chris Biemann, and Iryna Gurevych. 2013. Supervised all-words lexical substitution using
delexicalized features. In Proceedings of the 2013 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, pages 1131?1141, Atlanta, Georgia, June.
Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856, Manchester, UK,
August.
Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark
Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,
pages 81?88.
Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.
In Proceedings of Coling 2004, pages 1015?1021, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Julie Weeds, David Weir, and Jeremy Reffin. 2014. Distributional composition using higher-order dependency
vectors. In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality,
EACL 2014, Gothenburg, Sweden, April.
Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Proceedings of the Second
Symposium on Quantum Interaction, Oxford, UK, pages 1?8.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1212?1222, Jeju Island, Korea, July. Association for Computational Linguistics.
Chen Zhang and Joyce Chai. 2010. Towards conversation entailment: An empirical investigation. In Proceedings
of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756?766, Cambridge,
MA, October. Association for Computational Linguistics.
2259
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 115?119, Dublin, Ireland, August 23-29 2014.
Method51 for Mining Insight from Social Media Datasets
Simon Wibberley
University of Sussex
simon.wibberley
@sussex.ac.uk
Jeremy Reffin
University of Sussex
j.p.reffin
@sussex.ac.uk
David Weir
University of Sussex
d.j.weir
@sussex.ac.uk
Abstract
We present Method51, a social media analysis software platform with a set of accompanying
methodologies. We discuss a series of case studies illustrating the platform?s application, and
motivating our methodological proposals.
1 Introduction
Social scientists wish to apply language processing technology on social media datasets to answer soci-
ological questions. To that end, the technology should support methodologies that allow analysts to gain
valuable insight from the datasets under examination. In previous work we have argued for the impor-
tance of agility when dealing with social media datasets (Wibberley et al., 2013). In this paper we present
a series of case studies, carried out on Twitter, that illustrate the importance of that agile paradigm, and
how they have motivated the development of several additional methodologies, including ?Twitcident?,
?Patterns of Use?, and ?Russian Doll? analysis. First, we present Method51
1
, the technological counter-
part to our methodological paradigm.
2 Method51
Method51 uses active learning, coupled with a Na??ve Bayes model, to allow social scientists to construct
chains of linked, bespoke classifiers. The framework, initially an extension of DUALIST (Settles, 2011),
utilises an EM step to harness information from large amounts of unlabelled data, and allows the analyst
to expedite learning by specifying features that are highly indicative of a class. Using this approach,
a classifier can be trained within minutes (Settles, 2011). This enables analysts to evolve the way that
the data is being analysed without significant loss of effort. Method51 provides significant additional
functionality including collaborative gold standard and classifier construction, processing pipeline con-
struction, data collection and storage, data visualisation, various filtering and processing modules, and
time-based data selection. Figure 1a show the pipeline construction interface, which allows analysts to
knit together chains of bespoke classifiers.
Figure 1b shows the ?Coding? interface which is the primary point of contact between the analyst and
the data, where documents and features are labelled. Classifier evaluation statistics are also displayed, so
analysts can rapidly assess whether the data and technology are amenable to their analytical approach.
Method51 aims to put social science researchers at the centre of the data exploration process. Insight is
generated through the iterative interaction of the subject matter expert and the data itself.
Method51 combines two strands of existing work. The first body of work employs tailored automatic
data analysis, using supervised machine-learning approaches (Carvalho et al., 2011; Papacharissi and de
Fatima Oliveira, 2012; Meraz and Papacharissi, 2013; Hopkins and King, 2010; Burnap et al., 2013b).
A second body of work focusses on providing user interfaces that enable researchers to customise their
processing pipeline, based on the requirements of their investigation (Blessing et al., 2013; Black et al.,
2012; Burnap et al., 2013a).
1
Method51 has been released under an open source license, and is available at https://github.com/simonwibberley/method51
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
115
(a) Pipeline Construction Interface
(b) Coding interface
3 Case Studies
Over the past 18 months we have conducted a wide range of studies using Method51. Three illustrative
case studies are presented here in chronological order; each investigation highlighted new analytical
challenges and therefore motivated methodological and technological development.
3.1 EU Sentiment
In the first study, we examined the attitudes of EU citizens towards the Eurozone crisis of 2013 (Bartlett et
al., textitforthcoming). We set out with a preconceived structure and methodology. We tracked 6 topics,
referencing EU institutions or prominent people, and collected Tweets in 3 languages (English, French,
and German) to create 18 distinct streams of messages. For each stream, we constructed a bespoke
classification pipeline with a common analytical architecture of three successive layers: a classifier for
relevancy, a classifier to determine whether Tweets were attitudinal, and a classifier to determine the
polarity of the sentiment being expressed.
We found that, broadly, the data did not fit the pre-conceived analytical architecture neatly and that
classifier performance was a poor fit with human judgements, particularly for the attitudinal and senti-
ment layers. Table 1 illustrates the range of performance of classifiers measured against human-annotated
gold standard. Although relevancy classifiers performed adequately, the reliability of attitudinal and sen-
timent classifiers varies widely across streams.
Investigation revealed a number of underlying issues and prompted a series of methodological re-
sponses:
Need for flexible architecture We observed that each stream presented different challenges that could
only be appropriately addressed by a bespoke architecture tailored to the anatomy of the stream. For
example, relevancy classification was only sometimes required. The ?Euro? stream was targeted towards
conversation about the Euro currency, but required relevancy filtering as the query ?#euro? matched con-
versations regarding a wide variety of other topics such as sport competitions. Conversely, the ?Barroso?
stream, tracking messages regarding the president of the European Commission, Jos?e Manuel Barroso,
was of sufficiently high precision not to warrant relevancy filtering.
?Twitcident? analysis We observed that attitudes were typically only exposed when some event in the
world ?provoked? a burst of reactions that was related to the topic of interest. These response bursts,
which usually occur over a matter of hours to days, have been labelled ?Twitcidents?. We found that the
nature of the response ? and how this should be mapped onto the broader topic ? was unique to the
event itself. The ?Twitcident? analysis principle states that each event needs to be studied separately in
order to be correctly interpreted. Using a common classification architecture, or otherwise aggregating
116
results across Twitcidents, is likely to create a misleading picture of how opinions are being shaped by
events over time. As an example, a speech by the UK Prime Minister expressing a sceptical view of
the EU prompted many enthusiastic responses. Messages that commented positively on his speech were
contributing evidence of negative sentiment towards the EU. Clearly the reaction to that speech had to be
analysed separately in order to allow for this reversal of sentiment polarity.
Exploratory ?Patterns of Use? analysis The poor fit between the data and the imposed classification
architecture prompted us to adopt a new approach to constructing pipelines. The framework enables
analysts to ?fail fast?: engage actively with the data and explore how it is structured, before committing
to the pipeline framework. This is feasible because Method51 enables classifiers to be built quickly.
This ?Patterns of Use? analysis is inspired by Grounded Theory (Glaser et al., 1968), and mandates
that classification categories should arise from an unbiased examination of the available data. Categories
arise naturally from the analyst?s engagement with the data.
3.2 Father?s Day
Our initial ideas about ?Patterns of Use? analysis were explored further in the Father?s Day study. Our
aim here was to identify users likely to respond positively to a targeted advertisement for Father?s Day
gift ideas, that assessment being driven by the content of a Tweet sent by the user. We collected and
analysed Tweets mentioning Father?s Day in the days leading up to the event, and our first attempts at
analysing underlying patterns prompted a revision to our methodology.
?Russian Doll? approach We observed that at any stage the data tends to be dominated by one pattern of
usage, obscuring other underlying patterns. We developed a ?Russian Doll? approach, which mandates
that at each layer a classifier is built to unpack from the data Tweets that match this most prominent
pattern of usage. With this usage pattern stripped out, new structure is typically revealed in the remaining
data, which can in turn be unpacked using simple bespoke classifiers. Chained together, this pipeline of
classifiers removes successive different patterns of usage to expose underlying structure.
In this case, our a-priori expectation was that the stream would contain marketing Tweets, general
conversation about Father?s Day, and our target subset: people expressing uncertainty about what gift to
get. Our analysis revealed, however, a significant critical class of Tweets (and Tweeters) the presence of
which was unexpected ? a category for which targeted marketing Tweets would be wholly inappropriate.
The content of the Tweets matching this category (dubbed ?Sad/Distressed?) was negative, with Tweeters
venting sad or angry feelings towards absent fathers, generally though family breakdown or bereavement.
It was only through this careful patterns of use analysis that we identified this critical subgroup.
The final architecture consisted of two layers that dealt with existing marketing Tweets, a layer that
assessed the suitability of the Tweet as a potential target for a marketing campaign, and a final layer
that identified explicit requests for gift ideas. This pipeline showed good performance as measured by
F-scores against gold-standard annotated data (see Table 2). The unexpected ?Sad/Distressed? category
constituted a high proportion (10%) of all Tweets in the data set. Of the remaining tweets, 50% were
classified ?Marketing?, 36% were classified ?Miscellaneous? comments about Father?s Day, and 4% as
?Gift Idea Request?, our target group.
3.3 Mark Duggan
Our final case study illustrates an analysis conducted using the ?Twitcident? and ?Patterns of Use? tech-
niques. The aim was to dissect the online reaction to developments in the Mark Duggan inquest, an
enquiry into the shooting by police in London of a young black man. Over the course of about a month,
Tweets regarding Mark Duggan were collected with a view to analysing reactions as the case progressed.
The response showed the familiar ?Twitcident? pattern (see Figure 2). Twitcidents were examined indi-
vidually, culminating in an analysis of the large response on Twitter to the final verdict. Four specific
categories of response were identified and analysed. These were: (i) ?No Justice? ? where a Tweet
included accusations of institutional racism and/or claims that Duggan was unarmed; (ii) ?Justice? ?
that Duggan ?had it coming?, and/or that Duggan was armed; (iii) ?Riot? ? warning of possible rioting,
117
Range Split
Relevance 0.5 - 0.9 2-way
Attitudinal 0.3 - 0.9 2-way
Sentiment 0.1 - 0.8 3-way
Table 1: Range of F1-scores of EU Sentiment
Individual Marketing
F1 0.873 0.844
(a) Marketing level 1
Suitable Sad Marketing
F1 0.785 0.564 0.227
(c) Suitability
Individual Marketing
F1 0.834 0.490
(b) Marketing level 2
Request Other
F1 0.583 0.959
(d) Gift request
Table 2: F-scores of Russion Dolls Analysis
Justice No Justice Riot Watching
F1 0.636 0.842 0.737 0.451
Split 58% 17% 7% 18%
Table 3: Verdict classifier performance
making calls for calm; and (iv) ?Watching? ? people neutrally expressing interest in a case. Pipeline
performance was good as measured by F-scores against gold-standard (see Table 3)
This case further illustrates how patterns of response are specific to a particular situation, greatly
limiting the usefulness of pre-defined classifiers (e.g. for sentiment) in real-world investigations.
Figure 2: Mark Duggan ?Twitcidents?
4 Discussion
We will end with a discussion of how we have interpreted the developments in methodology described
above, and how that interpretation leads to an intuitive framework for further work.
We have presented the application of three distinct methodologies for mining insight from social me-
dia data. The insights gained from each case study are the result of three interdependent factors; (i) the
question that is being posed, (ii) the extend to which the answers to the question reside in the data, and
(iii) the extent to which the technology is capable of addressing the question given the data. We encapsu-
late this line of interpretation as ?Question, Data, and Technology?. By considering these factors analysts
can remain plastic about how the ?Data? and ?Technology? can mutually constrain and inform the ?Ques-
tion?. For example, if answers to the question are not represented in the data in a form the technology
can recognise, then the question should be revised. Conversely, the technology may reveal unexpected
characteristics in the data that contribute towards the analysts understanding of what the question should
be. Careful alignment between all three tend to result in valuable insights being discovered.
Crucial to this alignment is assessing the performance of the technology on the data, given the question
being posed. Method51 provides some functionality towards supporting this, such as accuracy and F-
scores. However, these measures indirectly indicate whether the classifier is behaving sensibly by virtue
118
of the gold standard evaluation data being an i.i.d sample of the unlabelled data.
The behaviour of classifiers on unlabelled data forms a crucial role in how the technology supports the
analyst in their investigation. Directly exposing that behaviour and developing an informed understand-
ing of the relationship between ?Question, Data, and Technology? would only expedite and contribute
towards reliable insights being discovered. Incorporating technology into Method51 that exposes how
unlabelled data are effected by analytical processes is an area for further work.
In general, we have found that considering the interaction between ?Question, Data, and Technology?
provides an intuitive framework for refining the focus of methodological and technological development
towards demonstrably useful innovations.
Acknowledgments
This work was supported by the ESRC National Centre for Research Methods grant DU/512589110.
We are grateful to our collaborators at the Centre for the Analysis of social media, Jamie Bartlett and
Carl Miller for valuable contributions to this work. We thank the anonymous reviewers for their helpful
comments. This work was partially supported by the Open Society Foundation.
References
J. Bartlett, Miller C, J. Reffin, D. Weir, and Wibberley S. forthcoming. Vox digitas.
http://www.demos.co.uk/publications.
W. Black, R. Procter, S. Gray, and S. Ananiadou. 2012. A data and analysis resource for an experiment in text
mining a collection of micro-blogs on a political topic. In LREC. ELRA.
A. Blessing, J. Sonntag, F. Kliche, U. Heid, J. Kuhn, and M. Stede. 2013. Towards a tool for interactive concept
building for large scale analysis in the humanities. In LaTeCH, Social Sciences, and Humanities. ACL.
P. Burnap, N. Avis, and O. Rana. 2013a. Making sense of self-reported socially significant data using computa-
tional methods. International Journal of Social Research Methodology.
P. Burnap, O. Rana, N. Avis, M. Williams, W. Housley, A. Edwards, J. Morgan, and L. Sloan. 2013b. Detect-
ing tension in online communities with computational twitter analysis. Technological Forecasting and Social
Change.
P. Carvalho, L. Sarmento, J. Teixeira, and M. Silva. 2011. Liars and saviors in a sentiment annotated corpus of
comments to political debates. In ACL: Human Language Technologies.
Barney G Glaser, Anselm L Strauss, and Elizabeth Strutzel. 1968. The discovery of grounded theory; strategies
for qualitative research. Nursing Research.
D. J. Hopkins and G. King. 2010. A method of automated nonparametric content analysis for social science.
American Journal of Political Science.
Sharon Meraz and Zizi Papacharissi. 2013. Networked gatekeeping and networked framing on #egypt. The
International Journal of Press/Politics, 18(2):138?166.
Zizi Papacharissi and Maria de Fatima Oliveira. 2012. Affective news and networked publics: the rhythms of
news storytelling on #egypt. Journal of Communication, 62(2):266?282.
B. Settles. 2011. Closing the loop: Fast, interactive semi-supervised annotation with queries on features and
instances. In Empirical Methods in Natural Language Processing.
Simon Wibberley, David Weir, and Jeremy Reffin. 2013. Language technology for agile social media science. In
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Human-
ities, pages 36?42, Sofia, Bulgaria, August. Association for Computational Linguistics.
119
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 132?141,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus
for Cross-Domain Sentiment Classification
Danushka Bollegala
The University of Tokyo
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@
iba.t.u-tokyo.ac.jp
David Weir
School of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
sussex.ac.uk
John Carroll
School of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
j.a.carroll@
sussex.ac.uk
Abstract
We describe a sentiment classification method
that is applicable when we do not have any la-
beled data for a target domain but have some
labeled data for multiple other domains, des-
ignated as the source domains. We automat-
ically create a sentiment sensitive thesaurus
using both labeled and unlabeled data from
multiple source domains to find the associa-
tion between words that express similar senti-
ments in different domains. The created the-
saurus is then used to expand feature vectors
to train a binary classifier. Unlike previous
cross-domain sentiment classification meth-
ods, our method can efficiently learn from
multiple source domains. Our method signif-
icantly outperforms numerous baselines and
returns results that are better than or com-
parable to previous cross-domain sentiment
classification methods on a benchmark dataset
containing Amazon user reviews for different
types of products.
1 Introduction
Users express opinions about products or services
they consume in blog posts, shopping sites, or re-
view sites. It is useful for both consumers as well
as for producers to know what general public think
about a particular product or service. Automatic
document level sentiment classification (Pang et al,
2002; Turney, 2002) is the task of classifying a given
review with respect to the sentiment expressed by
the author of the review. For example, a sentiment
classifier might classify a user review about a movie
as positive or negative depending on the sentiment
expressed in the review. Sentiment classification
has been applied in numerous tasks such as opinion
mining (Pang and Lee, 2008), opinion summariza-
tion (Lu et al, 2009), contextual advertising (Fan
and Chang, 2010), and market analysis (Hu and Liu,
2004).
Supervised learning algorithms that require la-
beled data have been successfully used to build sen-
timent classifiers for a specific domain (Pang et al,
2002). However, sentiment is expressed differently
in different domains, and it is costly to annotate
data for each new domain in which we would like
to apply a sentiment classifier. For example, in the
domain of reviews about electronics products, the
words ?durable? and ?light? are used to express pos-
itive sentiment, whereas ?expensive? and ?short bat-
tery life? often indicate negative sentiment. On the
other hand, if we consider the books domain the
words ?exciting? and ?thriller? express positive sen-
timent, whereas the words ?boring? and ?lengthy?
usually express negative sentiment. A classifier
trained on one domain might not perform well on
a different domain because it would fail to learn the
sentiment of the unseen words.
Work in cross-domain sentiment classification
(Blitzer et al, 2007) focuses on the challenge of
training a classifier from one or more domains
(source domains) and applying the trained classi-
fier in a different domain (target domain). A cross-
domain sentiment classification system must over-
come two main challenges. First, it must identify
which source domain features are related to which
target domain features. Second, it requires a learn-
ing framework to incorporate the information re-
132
garding the relatedness of source and target domain
features. Following previous work, we define cross-
domain sentiment classification as the problem of
learning a binary classifier (i.e. positive or negative
sentiment) given a small set of labeled data for the
source domain, and unlabeled data for both source
and target domains. In particular, no labeled data is
provided for the target domain.
In this paper, we describe a cross-domain senti-
ment classification method using an automatically
created sentiment sensitive thesaurus. We use la-
beled data from multiple source domains and unla-
beled data from source and target domains to rep-
resent the distribution of features. We represent a
lexical element (i.e. a unigram or a bigram of word
lemma) in a review using a feature vector. Next, for
each lexical element we measure its relatedness to
other lexical elements and group related lexical ele-
ments to create a thesaurus. The thesaurus captures
the relatedness among lexical elements that appear
in source and target domains based on the contexts
in which the lexical elements appear (their distribu-
tional context). A distinctive aspect of our approach
is that, in addition to the usual co-occurrence fea-
tures typically used in characterizing a word?s dis-
tributional context, we make use, where possible, of
the sentiment label of a document: i.e. sentiment la-
bels form part of our context features. This is what
makes the distributional thesaurus sensitive to senti-
ment. Unlabeled data is cheaper to collect compared
to labeled data and is often available in large quan-
tities. The use of unlabeled data enables us to ac-
curately estimate the distribution of words in source
and target domains. Our method can learn from a
large amount of unlabeled data to leverage a robust
cross-domain sentiment classifier.
We model the cross-domain sentiment classifica-
tion problem as one of feature expansion, where we
append additional related features to feature vectors
that represent source and target domain reviews in
order to reduce the mismatch of features between the
two domains. Methods that use related features have
been successfully used in numerous tasks such as
query expansion (Fang, 2008), and document classi-
fication (Shen et al, 2009). However, feature expan-
sion techniques have not previously been applied to
the task of cross-domain sentiment classification.
In our method, we use the automatically created
thesaurus to expand feature vectors in a binary clas-
sifier at train and test times by introducing related
lexical elements from the thesaurus. We use L1 reg-
ularized logistic regression as the classification al-
gorithm. (However, the method is agnostic to the
properties of the classifier and can be used to expand
feature vectors for any binary classifier). L1 regular-
ization enables us to select a small subset of features
for the classifier. Unlike previous work which at-
tempts to learn a cross-domain classifier using a sin-
gle source domain, we leverage data from multiple
source domains to learn a robust classifier that gen-
eralizes across multiple domains. Our contributions
can be summarized as follows.
? We describe a fully automatic method to create
a thesaurus that is sensitive to the sentiment of
words expressed in different domains.
? We describe a method to use the created the-
saurus to expand feature vectors at train and test
times in a binary classifier.
2 A Motivating Example
To explain the problem of cross-domain sentiment
classification, consider the reviews shown in Ta-
ble 1 for the domains books and kitchen appliances.
Table 1 shows two positive and one negative re-
view from each domain. We have emphasized in
boldface the words that express the sentiment of
the authors of the reviews. We see that the words
excellent, broad, high quality, interesting, and
well researched are used to express positive senti-
ment in the books domain, whereas the word disap-
pointed indicates negative sentiment. On the other
hand, in the kitchen appliances domain the words
thrilled, high quality, professional, energy sav-
ing, lean, and delicious express positive sentiment,
whereas the words rust and disappointed express
negative sentiment. Although high quality would
express positive sentiment in both domains, and dis-
appointed negative sentiment, it is unlikely that we
would encounter well researched in kitchen appli-
ances reviews, or rust or delicious in book reviews.
Therefore, a model that is trained only using book
reviews might not have any weights learnt for deli-
cious or rust, which would make it difficult for this
model to accurately classify reviews of kitchen ap-
pliances.
133
books kitchen appliances
+ Excellent and broad survey of the development of
civilization with all the punch of high quality fiction.
I was so thrilled when I unpack my processor. It is
so high quality and professional in both looks and
performance.
+ This is an interesting and well researched book. Energy saving grill. My husband loves the burgers
that I make from this grill. They are lean and deli-
cious.
- Whenever a new book by Philippa Gregory comes
out, I buy it hoping to have the same experience, and
lately have been sorely disappointed.
These knives are already showing spots of rust de-
spite washing by hand and drying. Very disap-
pointed.
Table 1: Positive (+) and negative (-) sentiment reviews in two different domains.
sentence Excellent and broad survey of
the development of civilization.
POS tags Excellent/JJ and/CC broad/JJ
survey/NN1 of/IO the/AT
development/NN1 of/IO civi-
lization/NN1
lexical elements
(unigrams)
excellent, broad, survey, devel-
opment, civilization
lexical elements
(bigrams)
excellent+broad, broad+survey,
survey+development, develop-
ment+civilization
sentiment fea-
tures (lemma)
excellent*P, broad*P, sur-
vey*P, excellent+broad*P,
broad+survey*P
sentiment fea-
tures (POS)
JJ*P, NN1*P, JJ+NN1*P
Table 2: Generating lexical elements and sentiment fea-
tures from a positive review sentence.
3 Sentiment Sensitive Thesaurus
One solution to the feature mismatch problem out-
lined above is to use a thesaurus that groups differ-
ent words that express the same sentiment. For ex-
ample, if we know that both excellent and delicious
are positive sentiment words, then we can use this
knowledge to expand a feature vector that contains
the word delicious using the word excellent, thereby
reducing the mismatch between features in a test in-
stance and a trained model. Below we describe a
method to construct a sentiment sensitive thesaurus
for feature expansion.
Given a labeled or an unlabeled review, we first
split the review into individual sentences. We carry
out part-of-speech (POS) tagging and lemmatiza-
tion on each review sentence using the RASP sys-
tem (Briscoe et al, 2006). Lemmatization reduces
the data sparseness and has been shown to be effec-
tive in text classification tasks (Joachims, 1998). We
then apply a simple word filter based on POS tags to
select content words (nouns, verbs, adjectives, and
adverbs). In particular, previous work has identified
adjectives as good indicators of sentiment (Hatzi-
vassiloglou and McKeown, 1997; Wiebe, 2000).
Following previous work in cross-domain sentiment
classification, we model a review as a bag of words.
We select unigrams and bigrams from each sentence.
For the remainder of this paper, we will refer to un-
igrams and bigrams collectively as lexical elements.
Previous work on sentiment classification has shown
that both unigrams and bigrams are useful for train-
ing a sentiment classifier (Blitzer et al, 2007). We
note that it is possible to create lexical elements both
from source domain labeled reviews as well as from
unlabeled reviews in source and target domains.
Next, we represent each lexical element u using a
set of features as follows. First, we select other lex-
ical elements that co-occur with u in a review sen-
tence as features. Second, from each source domain
labeled review sentence in which u occurs, we cre-
ate sentiment features by appending the label of the
review to each lexical element we generate from that
review. For example, consider the sentence selected
from a positive review of a book shown in Table 2.
In Table 2, we use the notation ?*P? to indicate posi-
tive sentiment features and ?*N? to indicate negative
sentiment features. The example sentence shown in
Table 2 is selected from a positively labeled review,
and generates positive sentiment features as shown
in Table 2. In addition to word-level sentiment fea-
tures, we replace words with their POS tags to create
134
POS-level sentiment features. POS tags generalize
the word-level sentiment features, thereby reducing
feature sparseness.
Let us denote the value of a feature w in the fea-
ture vector u representing a lexical element u by
f(u, w). The vector u can be seen as a compact rep-
resentation of the distribution of a lexical element u
over the set of features that co-occur with u in the re-
views. From the construction of the feature vector u
described in the previous paragraph, it follows that
w can be either a sentiment feature or another lexical
element that co-occurs with u in some review sen-
tence. The distributional hypothesis (Harris, 1954)
states that words that have similar distributions are
semantically similar. We compute f(u, w) as the
pointwise mutual information between a lexical ele-
ment u and a feature w as follows:
f(u, w) = log
(
c(u,w)
N
?n
i=1 c(i,w)
N ?
?m
j=1 c(u,j)
N
)
(1)
Here, c(u,w) denotes the number of review sen-
tences in which a lexical element u and a feature
w co-occur, n and m respectively denote the total
number of lexical elements and the total number of
features, and N =
?n
i=1
?m
j=1 c(i, j). Pointwise
mutual information is known to be biased towards
infrequent elements and features. We follow the dis-
counting approach of Pantel & Ravichandran (2004)
to overcome this bias.
Next, for two lexical elements u and v (repre-
sented by feature vectors u and v, respectively), we
compute the relatedness ?(v, u) of the feature v to
the feature u as follows,
?(v, u) =
?
w?{x|f(v,x)>0} f(u, w)
?
w?{x|f(u,x)>0} f(u, w)
. (2)
Here, we use the set notation {x|f(v, x) > 0} to
denote the set of features that co-occur with v. Re-
latedness of a lexical element u to another lexical
element v is the fraction of feature weights in the
feature vector for the element u that also co-occur
with the features in the feature vector for the ele-
ment v. If there are no features that co-occur with
both u and v, then the relatedness reaches its min-
imum value of 0. On the other hand if all features
that co-occur with u also co-occur with v, then the
relatedness , ?(v, u), reaches its maximum value of
1. Note that relatedness is an asymmetric measure
by the definition given in Equation 2, and the relat-
edness ?(v, u) of an element v to another element u
is not necessarily equal to ?(u, v), the relatedness of
u to v.
We use the relatedness measure defined in Equa-
tion 2 to construct a sentiment sensitive thesaurus in
which, for each lexical element u we list lexical el-
ements v that co-occur with u (i.e. f(u, v) > 0) in
descending order of relatedness values ?(v, u). In
the remainder of the paper, we use the term base en-
try to refer to a lexical element u for which its related
lexical elements v (referred to as the neighbors of u)
are listed in the thesaurus. Note that relatedness val-
ues computed according to Equation 2 are sensitive
to sentiment labels assigned to reviews in the source
domain, because co-occurrences are computed over
both lexical and sentiment elements extracted from
reviews. In other words, the relatedness of an ele-
ment u to another element v depends upon the sen-
timent labels assigned to the reviews that generate u
and v. This is an important fact that differentiates
our sentiment-sensitive thesaurus from other distri-
butional thesauri which do not consider sentiment
information.
Moreover, we only need to retain lexical elements
in the sentiment sensitive thesaurus because when
predicting the sentiment label for target reviews (at
test time) we cannot generate sentiment elements
from those (unlabeled) reviews, therefore we are
not required to find expansion candidates for senti-
ment elements. However, we emphasize the fact that
the relatedness values between the lexical elements
listed in the sentiment-sensitive thesaurus are com-
puted using co-occurrences with both lexical and
sentiment features, and therefore the expansion can-
didates selected for the lexical elements in the tar-
get domain reviews are sensitive to sentiment labels
assigned to reviews in the source domain. Using
a sparse matrix format and approximate similarity
matching techniques (Sarawagi and Kirpal, 2004),
we can efficiently create a thesaurus from a large set
of reviews.
4 Feature Expansion
Our feature expansion phase augments a feature vec-
tor with additional related features selected from the
135
sentiment-sensitive thesaurus created in Section 3 to
overcome the feature mismatch problem. First, fol-
lowing the bag-of-words model, we model a review
d using the set {w1, . . . , wN}, where the elements
wi are either unigrams or bigrams that appear in the
review d. We then represent a review d by a real-
valued term-frequency vector d ? RN , where the
value of the j-th element dj is set to the total number
of occurrences of the unigram or bigram wj in the
review d. To find the suitable candidates to expand a
vector d for the review d, we define a ranking score
score(ui,d) for each base entry in the thesaurus as
follows:
score(ui,d) =
?N
j=1 dj?(wj , ui)
?N
l=1 dl
(3)
According to this definition, given a review d, a base
entry ui will have a high ranking score if there are
many words wj in the review d that are also listed
as neighbors for the base entry ui in the sentiment-
sensitive thesaurus. Moreover, we weight the re-
latedness scores for each word wj by its normal-
ized term-frequency to emphasize the salient uni-
grams and bigrams in a review. Recall that related-
ness is defined as an asymmetric measure in Equa-
tion 2, and we use ?(wj , ui) in the computation of
score(ui,d) in Equation 3. This is particularly im-
portant because we would like to score base entries
ui considering all the unigrams and bigrams that ap-
pear in a review d, instead of considering each uni-
gram or bigram individually.
To expand a vector, d, for a review d, we first
rank the base entries, ui using the ranking score
in Equation 3 and select the top k ranked base en-
tries. Let us denote the r-th ranked (1 ? r ? k)
base entry for a review d by vrd. We then extend the
original set of unigrams and bigrams {w1, . . . , wN}
by the base entries v1d, . . . , v
k
d to create a new vec-
tor d? ? R(N+k) with dimensions corresponding to
w1, . . . , wN , v1d, . . . , v
k
d for a review d. The values
of the extended vector d? are set as follows. The
values of the first N dimensions that correspond to
unigrams and bigrams wi that occur in the review d
are set to di, their frequency in d. The subsequent k
dimensions that correspond to the top ranked based
entries for the review d are weighted according to
their ranking score. Specifically, we set the value of
the r-th ranked base entry vrd to 1/r. Alternatively,
one could use the ranking score, score(vrd, d), itself
as the value of the appended base entries. However,
both relatedness scores as well as normalized term-
frequencies can be small in practice, which leads to
very small absolute ranking scores. By using the
inverse rank, we only take into account the rela-
tive ranking of base entries and ignore their absolute
scores.
Note that the score of a base entry depends on a
review d. Therefore, we select different base en-
tries as additional features for expanding different
reviews. Furthermore, we do not expand each wi
individually when expanding a vector d for a re-
view. Instead, we consider all unigrams and bi-
grams in d when selecting the base entries for ex-
pansion. One can think of the feature expansion pro-
cess as a lower dimensional latent mapping of fea-
tures onto the space spanned by the base entries in
the sentiment-sensitive thesaurus. The asymmetric
property of the relatedness (Equation 2) implicitly
prefers common words that co-occur with numerous
other words as expansion candidates. Such words
act as domain independent pivots and enable us to
transfer the information regarding sentiment from
one domain to another.
Using the extended vectors d? to represent re-
views, we train a binary classifier from the source
domain labeled reviews to predict positive and neg-
ative sentiment in reviews. We differentiate the ap-
pended base entries vrd from wi that existed in the
original vector d (prior to expansion) by assigning
different feature identifiers to the appended base en-
tries. For example, a unigram excellent in a feature
vector is differentiated from the base entry excellent
by assigning the feature id, ?BASE=excellent? to the
latter. This enables us to learn different weights for
base entries depending on whether they are useful
for expanding a feature vector. We use L1 regu-
larized logistic regression as the classification algo-
rithm (Ng, 2004), which produces a sparse model in
which most irrelevant features are assigned a zero
weight. This enables us to select useful features for
classification in a systematic way without having to
preselect features using heuristic approaches. The
regularization parameter is set to its default value
of 1 for all the experiments described in this paper.
136
5 Experiments
5.1 Dataset
To evaluate our method we use the cross-domain
sentiment classification dataset prepared by Blitzer
et al (2007). This dataset consists of Amazon prod-
uct reviews for four different product types: books
(B), DVDs (D), electronics (E) and kitchen appli-
ances (K). There are 1000 positive and 1000 neg-
ative labeled reviews for each domain. Moreover,
the dataset contains some unlabeled reviews (on av-
erage 17, 547) for each domain. This benchmark
dataset has been used in much previous work on
cross-domain sentiment classification and by eval-
uating on it we can directly compare our method
against existing approaches.
Following previous work, we randomly select 800
positive and 800 negative labeled reviews from each
domain as training instances (i.e. 1600?4 = 6400);
the remainder is used for testing (i.e. 400 ? 4 =
1600). In our experiments, we select each domain in
turn as the target domain, with one or more other do-
mains as sources. Note that when we combine more
than one source domain we limit the total number
of source domain labeled reviews to 1600, balanced
between the domains. For example, if we combine
two source domains, then we select 400 positive and
400 negative labeled reviews from each domain giv-
ing (400 + 400) ? 2 = 1600. This enables us to
perform a fair evaluation when combining multiple
source domains. The evaluation metric is classifica-
tion accuracy on a target domain, computed as the
percentage of correctly classified target domain re-
views out of the total number of reviews in the target
domain.
5.2 Effect of Feature Expansion
To study the effect of feature expansion at train time
compared to test time, we used Amazon reviews for
two further domains, music and video, which were
also collected by Blitzer et al (2007) but are not
part of the benchmark dataset. Each validation do-
main has 1000 positive and 1000 negative labeled
reviews, and 15000 unlabeled reviews. Using the
validation domains as targets, we vary the number
of top k ranked base entries (Equation 3) used for
feature expansion during training (Traink) and test-
ing (Testk), and measure the average classification
0 200 400 600 800 10000
200
400
600
800
1000  
Traink 
Test k
0.776
0.778
0.78
0.782
0.784
0.786
Figure 1: Feature expansion at train vs. test times.
B D K B+D B+K D+K B+D+K50
55
60
65
70
75
80
85
Source Domains
Accu
racy 
on el
ectro
nics d
omai
n
Figure 2: Effect of using multiple source domains.
accuracy. Figure 1 illustrates the results using a heat
map, where dark colors indicate low accuracy val-
ues and light colors indicate high accuracy values.
We see that expanding features only at test time (the
left-most column) does not work well because we
have not learned proper weights for the additional
features. Similarly, expanding features only at train
time (the bottom-most row) also does not perform
well because the expanded features are not used dur-
ing testing. The maximum classification accuracy is
obtained when Testk = 400 and Traink = 800, and
we use these values for the remainder of the experi-
ments described in the paper.
5.3 Combining Multiple Sources
Figure 2 shows the effect of combining multiple
source domains to build a sentiment classifier for
the electronics domain. We see that the kitchen do-
main is the single best source domain when adapt-
ing to the electronics target domain. This behavior
137
0 200 400 600 80040
4550
5560
6570
7580
85
Positive/Negative instances
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 3: Effect of source domain labeled data.
0 0.2 0.4 0.6 0.8 150
55
60
65
70
Source unlabeled dataset size
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 4: Effect of source domain unlabeled data.
is explained by the fact that in general kitchen appli-
ances and electronic items have similar aspects. But
a more interesting observation is that the accuracy
that we obtain when we use two source domains is
always greater than the accuracy if we use those do-
mains individually. The highest accuracy is achieved
when we use all three source domains. Although
not shown here for space limitations, we observed
similar trends with other domains in the benchmark
dataset.
To investigate the impact of the quantity of source
domain labeled data on our method, we vary the
amount of data from zero to 800 reviews, with equal
amounts of positive and negative labeled data. Fig-
ure 3 shows the accuracy with the DVD domain as
the target. Note that source domain labeled data is
used both to create the sentiment sensitive thesaurus
as well as to train the sentiment classifier. When
there are multiple source domains we limit and bal-
ance the number of labeled instances as outlined in
Section 5.1. The amount of unlabeled data is held
constant, so that any change in classification accu-
0 0.2 0.4 0.6 0.8 150
55
60
65
70
Target unlabeled dataset size
Accur
acy
 
 
B E K B+E B+K E+K B+E+K
Figure 5: Effect of target domain unlabeled data.
racy is directly attributable to the source domain la-
beled instances. Because this is a binary classifica-
tion task (i.e. positive vs. negative sentiment), a ran-
dom classifier that does not utilize any labeled data
would report a 50% classification accuracy. From
Figure 3, we see that when we increase the amount
of source domain labeled data the accuracy increases
quickly. In fact, by selecting only 400 (i.e. 50% of
the total 800) labeled instances per class, we achieve
the maximum performance in most of the cases.
To study the effect of source and target domain
unlabeled data on the performance of our method,
we create sentiment sensitive thesauri using differ-
ent proportions of unlabeled data. The amount of
labeled data is held constant and is balanced across
multiple domains as outlined in Section 5.1, so any
changes in classification accuracy can be directly at-
tributed to the contribution of unlabeled data. Figure
4 shows classification accuracy on the DVD target
domain when we vary the proportion of source do-
main unlabeled data (target domain?s unlabeled data
is fixed).
Likewise, Figure 5 shows the classification ac-
curacy on the DVD target domain when we vary
the proportion of the target domain?s unlabeled data
(source domains? unlabeled data is fixed). From Fig-
ures 4 and 5, we see that irrespective of the amount
being used, there is a clear performance gain when
we use unlabeled data from multiple source domains
compared to using a single source domain. How-
ever, we could not observe a clear gain in perfor-
mance when we increase the amount of the unla-
beled data used to create the sentiment sensitive the-
saurus.
138
Method K D E B
No Thesaurus 72.61 68.97 70.53 62.72
SCL 80.83 74.56 78.43 72.76
SCL-MI 82.06 76.30 78.93 74.56
SFA 81.48 76.31 75.30 77.73
LSA 79.00 73.50 77.66 70.83
FALSA 80.83 76.33 77.33 73.33
NSS 77.50 73.50 75.50 71.46
Proposed 85.18 78.77 83.63 76.32
Within-Domain 87.70 82.40 84.40 80.40
Table 3: Cross-domain sentiment classification accuracy.
5.4 Cross-Domain Sentiment Classification
Table 3 compares our method against a number of
baselines and previous cross-domain sentiment clas-
sification techniques using the benchmark dataset.
For all previous techniques we give the results re-
ported in the original papers. The No Thesaurus
baseline simulates the effect of not performing any
feature expansion. We simply train a binary clas-
sifier using unigrams and bigrams as features from
the labeled reviews in the source domains and ap-
ply the trained classifier on the target domain. This
can be considered to be a lower bound that does
not perform domain adaptation. SCL is the struc-
tural correspondence learning technique of Blitzer
et al (2006). In SCL-MI, features are selected us-
ing the mutual information between a feature (uni-
gram or bigram) and a domain label. After selecting
salient features, the SCL algorithm is used to train a
binary classifier. SFA is the spectral feature align-
ment technique of Pan et al (2010). Both the LSA
and FALSA techniques are based on latent semantic
analysis (Pan et al, 2010). For the Within-Domain
baseline, we train a binary classifier using the la-
beled data from the target domain. This upper base-
line represents the classification accuracy we could
hope to obtain if we were to have labeled data for the
target domain. Note that this is not a cross-domain
classification setting. To evaluate the benefit of us-
ing sentiment features on our method, we give a NSS
(non-sentiment sensitive) baseline in which we cre-
ate a thesaurus without using any sentiment features.
Proposed is our method.
From Table 3, we see that our proposed method
returns the best cross-domain sentiment classifica-
tion accuracy (shown in boldface) for the three do-
mains kitchen appliances, DVDs, and electronics.
For the books domain, the best results are returned
by SFA. The books domain has the lowest number
of unlabeled reviews (around 5000) in the dataset.
Because our method relies upon the availability of
unlabeled data for the construction of a sentiment
sensitive thesaurus, we believe that this accounts for
our lack of performance on the books domain. How-
ever, given that it is much cheaper to obtain unla-
beled than labeled data for a target domain, there is
strong potential for improving the performance of
our method in this domain. The analysis of vari-
ance (ANOVA) and Tukey?s honestly significant dif-
ferences (HSD) tests on the classification accuracies
for the four domains show that our method is sta-
tistically significantly better than both the No The-
saurus and NSS baselines, at confidence level 0.05.
We therefore conclude that using the sentiment sen-
sitive thesaurus for feature expansion is useful for
cross-domain sentiment classification. The results
returned by our method are comparable to state-of-
the-art techniques such as SCL-MI and SFA. In par-
ticular, the differences between those techniques and
our method are not statistically significant.
6 Related Work
Compared to single-domain sentiment classifica-
tion, which has been studied extensively in previous
work (Pang and Lee, 2008; Turney, 2002), cross-
domain sentiment classification has only recently re-
ceived attention in response to advances in the area
of domain adaptation. Aue and Gammon (2005) re-
port a number of empirical tests into domain adap-
tation of sentiment classifiers using an ensemble of
classifiers. However, most of these tests were un-
able to outperform a simple baseline classifier that
is trained using all labeled data for all domains.
Blitzer et al (2007) apply the structural corre-
spondence learning (SCL) algorithm to train a cross-
domain sentiment classifier. They first chooses a set
of pivot features using pointwise mutual informa-
tion between a feature and a domain label. Next,
linear predictors are learnt to predict the occur-
rences of those pivots. Finally, they use singular
value decomposition (SVD) to construct a lower-
dimensional feature space in which a binary classi-
139
fier is trained. The selection of pivots is vital to the
performance of SCL and heuristically selected pivot
features might not guarantee the best performance
on target domains. In contrast, our method uses all
features when creating the thesaurus and selects a
subset of features during training using L1 regular-
ization. Moreover, we do not require SVD, which
has cubic time complexity so can be computation-
ally expensive for large datasets.
Pan et al (2010) use structural feature alignment
(SFA) to find an alignment between domain spe-
cific and domain independent features. The mu-
tual information of a feature with domain labels is
used to classify domain specific and domain inde-
pendent features. Next, spectral clustering is per-
formed on a bipartite graph that represents the re-
lationship between the two sets of features. Fi-
nally, the top eigenvectors are selected to construct
a lower-dimensional projection. However, not all
words can be cleanly classified into domain spe-
cific or domain independent, and this process is con-
ducted prior to training a classifier. In contrast, our
method lets a particular lexical entry to be listed as
a neighour for multiple base entries. Moreover, we
expand each feature vector individually and do not
require any clustering. Furthermore, unlike SCL and
SFA, which consider a single source domain, our
method can efficiently adapt from multiple source
domains.
7 Conclusions
We have described and evaluated a method to
construct a sentiment-sensitive thesaurus to bridge
the gap between source and target domains in
cross-domain sentiment classification using multi-
ple source domains. Experimental results using a
benchmark dataset for cross-domain sentiment clas-
sification show that our proposed method can im-
prove classification accuracy in a sentiment classi-
fier. In future, we intend to apply the proposed
method to other domain adaptation tasks.
Acknowledgements
This research was conducted while the first author
was a visiting research fellow at Sussex university
under the postdoctoral fellowship of the Japan Soci-
ety for the Promotion of Science (JSPS).
References
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
Technical report, Microsoft Research.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP 2006.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
ACL 2007, pages 440?447.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In COL-
ING/ACL 2006 Interactive Presentation Sessions.
Teng-Kai Fan and Chia-Hui Chang. 2010. Sentiment-
oriented contextual advertising. Knowledge and Infor-
mation Systems, 23(3):321?344.
Hui Fang. 2008. A re-examination of query expansion
using lexical resources. In ACL 2008, pages 139?147.
Z. Harris. 1954. Distributional structure. Word, 10:146?
162.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In ACL 1997, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD 2004, pages 168?177.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In ECML 1998, pages 137?142.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009.
Rated aspect summarization of short comments. In
WWW 2009, pages 131?140.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In ICML 2004.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
WWW 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In EMNLP 2002, pages 79?
86.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In NAACL-
HLT?04, pages 321 ? 328.
Sunita Sarawagi and Alok Kirpal. 2004. Efficient set
joins on similarity predicates. In SIGMOD ?04, pages
743?754.
140
Dou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, Qiang
Yang, Zheng Chen, and Ying Li. 2009. Exploit-
ing term relationship to boost text classification. In
CIKM?09, pages 1637 ? 1640.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL 2002, pages 417?424.
Janyce M. Wiebe. 2000. Learning subjective adjective
from corpora. In AAAI 2000, pages 735?740.
141
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613?623,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning to Predict Distributions of Words Across Domains
Danushka Bollegala
Department of Computer Science
University of Liverpool
Liverpool,
L69 3BX, UK
danushka.bollegala@
liverpool.ac.uk
David Weir
Department of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
d.j.weir@
sussex.ac.uk
John Carroll
Department of Informatics
University of Sussex
Falmer, Brighton,
BN1 9QJ, UK
j.a.carroll@
sussex.ac.uk
Abstract
Although the distributional hypothesis has
been applied successfully in many natural
language processing tasks, systems using
distributional information have been lim-
ited to a single domain because the dis-
tribution of a word can vary between do-
mains as the word?s predominant mean-
ing changes. However, if it were pos-
sible to predict how the distribution of
a word changes from one domain to an-
other, the predictions could be used to
adapt a system trained in one domain to
work in another. We propose an unsuper-
vised method to predict the distribution of
a word in one domain, given its distribu-
tion in another domain. We evaluate our
method on two tasks: cross-domain part-
of-speech tagging and cross-domain sen-
timent classification. In both tasks, our
method significantly outperforms compet-
itive baselines and returns results that are
statistically comparable to current state-
of-the-art methods, while requiring no
task-specific customisations.
1 Introduction
The Distributional Hypothesis, summarised by the
memorable line of Firth (1957) ? You shall know
a word by the company it keeps ? has inspired a
diverse range of research in natural language pro-
cessing. In such work, a word is represented by
the distribution of other words that co-occur with
it. Distributional representations of words have
been successfully used in many language process-
ing tasks such as entity set expansion (Pantel et al,
2009), part-of-speech (POS) tagging and chunk-
ing (Huang and Yates, 2009), ontology learning
(Curran, 2005), computing semantic textual sim-
ilarity (Besanc?on et al, 1999), and lexical infer-
ence (Kotlerman et al, 2012).
However, the distribution of a word often varies
from one domain
1
to another. For example, in
the domain of portable computer reviews the word
lightweight is often associated with positive sen-
timent bearing words such as sleek or compact,
whereas in the movie review domain the same
word is often associated with negative sentiment-
bearing words such as superficial or formulaic.
Consequently, the distributional representations of
the word lightweight will differ considerably be-
tween the two domains. In this paper, given the
distribution w
S
of a word w in the source domain
S, we propose an unsupervised method for pre-
dicting its distributionw
T
in a different target do-
main T .
The ability to predict how the distribution of a
word varies from one domain to another is vital
for numerous adaptation tasks. For example, un-
supervised cross-domain sentiment classification
(Blitzer et al, 2007; Aue and Gamon, 2005) in-
volves using sentiment-labeled user reviews from
the source domain, and unlabeled reviews from
both the source and the target domains to learn
a sentiment classifier for the target domain. Do-
main adaptation (DA) of sentiment classification
becomes extremely challenging when the distribu-
tions of words in the source and the target domains
are very different, because the features learnt from
the source domain labeled reviews might not ap-
pear in the target domain reviews that must be
classified. By predicting the distribution of a word
across different domains, we can find source do-
main features that are similar to the features in
target domain reviews, thereby reducing the mis-
match of features between the two domains.
We propose a two-step unsupervised approach
to predict the distribution of a word across do-
mains. First, we create two lower dimensional la-
1
In this paper, we use the term domain to refer to a col-
lection of documents about a particular topic, for example
reviews of a particular kind of product.
613
tent feature spaces separately for the source and
the target domains using Singular Value Decom-
position (SVD). Second, we learn a mapping from
the source domain latent feature space to the tar-
get domain latent feature space using Partial Least
Square Regression (PLSR). The SVD smoothing
in the first step both reduces the data sparseness in
distributional representations of individual words,
as well as the dimensionality of the feature space,
thereby enabling us to efficiently and accurately
learn a prediction model using PLSR in the sec-
ond step. Our proposed cross-domain word dis-
tribution prediction method is unsupervised in the
sense that it does not require any labeled data in
either of the two steps.
Using two popular multi-domain datasets, we
evaluate the proposed method in two prediction
tasks: (a) predicting the POS of a word in a tar-
get domain, and (b) predicting the sentiment of a
review in a target domain. Without requiring any
task specific customisations, systems based on our
distribution prediction method significantly out-
perform competitive baselines in both tasks. Be-
cause our proposed distribution prediction method
is unsupervised and task independent, it is poten-
tially useful for a wide range of DA tasks such en-
tity extraction (Guo et al, 2009) or dependency
parsing (McClosky et al, 2010). Our contribu-
tions are summarised as follows:
? Given the distribution w
S
of a word w in a
source domain S, we propose a method for
learning its distribution w
T
in a target do-
main T .
? Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain POS tagger.
? Using the learnt distribution prediction
model, we propose a method to learn a cross-
domain sentiment classifier.
To our knowledge, ours is the first successful at-
tempt to learn a model that predicts the distribu-
tion of a word across different domains.
2 Related Work
Learning semantic representations for words us-
ing documents from a single domain has received
much attention lately (Vincent et al, 2010; Socher
et al, 2013; Baroni and Lenci, 2010). As we have
already discussed, the semantics of a word varies
across different domains, and such variations are
not captured by models that only learn a single se-
mantic representation for a word using documents
from a single domain.
The POS of a word is influenced both by its
context (contextual bias), and the domain of the
document in which it appears (lexical bias). For
example, the word signal is predominately used
as a noun in MEDLINE, whereas it appears pre-
dominantly as an adjective in the Wall Street Jour-
nal (WSJ) (Blitzer et al, 2006). Consequently, a
tagger trained on WSJ would incorrectly tag sig-
nal in MEDLINE. Blitzer et al (2006) append
the source domain labeled data with predicted piv-
ots (i.e. words that appear in both the source and
target domains) to adapt a POS tagger to a tar-
get domain. Choi and Palmer (2012) propose
a cross-domain POS tagging method by training
two separate models: a generalised model and a
domain-specific model. At tagging time, a sen-
tence is tagged by the model that is most similar
to that sentence. Huang and Yates (2009) train a
Conditional Random Field (CRF) tagger with fea-
tures retrieved from a smoothing model trained us-
ing both source and target domain unlabeled data.
Adding latent states to the smoothing model fur-
ther improves the POS tagging accuracy (Huang
and Yates, 2012). Schnabel and Sch?utze (2013)
propose a training set filtering method where they
eliminate shorter words from the training data
based on the intuition that longer words are more
likely to be examples of productive linguistic pro-
cesses than shorter words.
The sentiment of a word can vary from one do-
main to another. In Structural Correspondence
Learning (SCL) (Blitzer et al, 2006; Blitzer et
al., 2007), a set of pivots are chosen using point-
wise mutual information. Linear predictors are
then learnt to predict the occurrence of those piv-
ots, and SVD is used to construct a lower dimen-
sional representation in which a binary classifier
is trained. Spectral Feature Alignment (SFA) (Pan
et al, 2010) also uses pivots to compute an align-
ment between domain specific and domain inde-
pendent features. Spectral clustering is performed
on a bipartite graph representing domain specific
and domain independent features to find a lower-
dimensional projection between the two sets of
features. The cross-domain sentiment-sensitive
thesaurus (SST) (Bollegala et al, 2011) groups
together words that express similar sentiments in
614
different domains. The created thesaurus is used to
expand feature vectors during train and test stages
in a binary classifier. However, unlike our method,
SCL, SFA, or SST do not learn a prediction model
between word distributions across domains.
Prior knowledge of the sentiment of words, such
as sentiment lexicons, has been incorporated into
cross-domain sentiment classification. He et al
(2011) propose a joint sentiment-topic model that
imposes a sentiment-prior depending on the oc-
currence of a word in a sentiment lexicon. Pono-
mareva and Thelwall (2012) represent source and
target domain reviews as nodes in a graph and ap-
ply a label propagation algorithm to predict the
sentiment labels for target domain reviews from
the sentiment labels in source domain reviews. A
sentiment lexicon is used to create features for a
document. Although incorporation of prior senti-
ment knowledge is a promising technique to im-
prove accuracy in cross-domain sentiment classi-
fication, it is complementary to our task of distri-
bution prediction across domains.
The unsupervised DA setting that we consider
does not assume the availability of labeled data for
the target domain. However, if a small amount of
labeled data is available for the target domain, it
can be used to further improve the performance of
DA tasks (Xiao et al, 2013; Daum?e III, 2007).
3 Distribution Prediction
3.1 In-domain Feature Vector Construction
Before we tackle the problem of learning a model
to predict the distribution of a word across do-
mains, we must first compute the distribution of
a word from a single domain. For this purpose, we
represent a word w using unigrams and bigrams
that co-occur with w in a sentence as follows.
Given a document H, such as a user-review of
a product, we split H into sentences, and lemma-
tize each word in a sentence using the RASP sys-
tem (Briscoe et al, 2006). Using a standard stop
word list, we filter out frequent non-content un-
igrams and select the remainder as unigram fea-
tures to represent a sentence. Next, we generate
bigrams of word lemmas and remove any bigrams
that consists only of stop words. Bigram features
capture negations more accurately than unigrams,
and have been found to be useful for sentiment
classification tasks. Table 1 shows the unigram
and bigram features we extract for a sentence us-
ing this procedure. Using data from a single do-
sentence This is an interesting and well researched book
unigrams this, is, an, interesting, and, well, researched,
(surface) book
unigrams this, be, an, interest, and, well, research, book
(lemma)
unigrams interest, well, research, book
(features)
bigrams this+be, be+an, an+interest, interest+and,
(lemma) and+well, well+research, research+book
bigrams an+interest, interest+and, and+well,
(features) well+research, research+book
Table 1: Extracting unigram and bigram features.
main, we construct a feature co-occurrence ma-
trix A in which columns correspond to unigram
features and rows correspond to either unigram or
bigram features. The value of the element a
ij
in
the co-occurrence matrix A is set to the number of
sentences in which the i-th and j-th features co-
occur.
Typically, the number of unique bigrams is
much larger than that of unigrams. Moreover, co-
occurrences of bigrams are rare compared to co-
occurrences of unigrams, and co-occurrences in-
volving a unigram and a bigram. Consequently,
in matrix A, we consider co-occurrences only be-
tween unigrams vs. unigrams, and bigrams vs.
unigrams. We consider each row in A as repre-
senting the distribution of a feature (i.e. unigrams
or bigrams) in a particular domain over the uni-
gram features extracted from that domain (repre-
sented by the columns of A). We apply Positive
Pointwise Mutual Information (PPMI) to the co-
occurrence matrix A. This is a variation of the
Pointwise Mutual Information (PMI) (Church and
Hanks, 1990), in which all PMI values that are less
than zero are replaced with zero (Lin, 1998; Bul-
linaria and Levy, 2007). Let F be the matrix that
results when PPMI is applied to A. Matrix F has
the same number of rows, n
r
, and columns, n
c
, as
the raw co-occurrence matrix A.
Note that in addition to the above-mentioned
representation, there are many other ways to rep-
resent the distribution of a word in a particular do-
main (Turney and Pantel, 2010). For example,
one can limit the definition of co-occurrence to
words that are linked by some dependency relation
(Pado and Lapata, 2007), or extend the window
of co-occurrence to the entire document (Baroni
and Lenci, 2010). Since the method we propose
in Section 3.2 to predict the distribution of a word
across domains does not depend on the particular
615
feature representation method, any of these alter-
native methods could be used.
To reduce the dimensionality of the feature
space, and create dense representations for words,
we perform SVD on F. We use the left singu-
lar vectors corresponding to the k largest singular
values to compute a rank k approximation
?
F, of
F. We perform truncated SVD using SVDLIBC
2
.
Each row in
?
F is considered as representing a word
in a lower k (n
c
) dimensional feature space cor-
responding to a particular domain. Distribution
prediction in this lower dimensional feature space
is preferrable to prediction over the original fea-
ture space because there are reductions in overfit-
ting, feature sparseness, and the learning time. We
created two matrices,
?
F
S
and
?
F
T
from the source
and target domains, respectively, using the above
mentioned procedure.
3.2 Cross-Domain Feature Vector Prediction
We propose a method to learn a model that can
predict the distribution w
T
of a word w in the
target domain T , given its distribution w
S
in
the source domain S. We denote the set of
features that occur in both domains by W =
{w
(1)
, . . . , w
(n)
}. In the literature, such features
are often referred to as pivots, and they have been
shown to be useful for DA, allowing the weights
learnt to be transferred from one domain to an-
other. Various criteria have been proposed for se-
lecting a small set of pivots for DA, such as the
mutual information of a word with the two do-
mains (Blitzer et al, 2007). However, we do not
impose any further restrictions on the set of pivots
W other than that they occur in both domains.
For each word w
(i)
? W , we denote the cor-
responding rows in
?
F
S
and
?
F
T
by column vec-
tors w
(i)
S
and w
(i)
T
. Note that the dimensional-
ity of w
(i)
S
and w
(i)
T
need not be equal, and we
may select different numbers of singular vectors
to approximate
?
F
S
and
?
F
T
. We model distribu-
tion prediction as a multivariate regression prob-
lem where, given a set {(w
(i)
S
,w
(i)
T
)}
n
i=1
consist-
ing of pairs of feature vectors selected from each
domain for the pivots in W , we learn a mapping
from the inputs (w
(i)
S
) to the outputs (w
(i)
T
).
We use Partial Least Squares Regression
(PLSR) (Wold, 1985) to learn a regression model
using pairs of vectors. PLSR has been applied in
2
http://tedlab.mit.edu/
?
dr/SVDLIBC/
Algorithm 1 Learning a prediction model.
Input: X, Y, L.
Output: Prediction matrix M.
1: Randomly select ?
l
from columns in Y
l
.
2: v
l
= X
l
>?
l
/
?
?
?
?
X
l
>?
l
?
?
?
?
3: ?
l
= X
l
v
l
4: q
l
= Y
l
>?
l
/
?
?
?
?
Y
l
>?
l
?
?
?
?
5: ?
l
= Y
l
q
l
6: If ?
l
is unchanged go to Line 7; otherwise go to Line 2
7: c
l
= ?
l
>?
l
/
?
?
?
??
l
>?
l
?
?
?
?
8: p
l
= X
l
>?
l
/?
l
>?
l
9: X
l+1
= X
l
? ?
l
p
l
>
and Y
l+1
= Y
l
? c
l
?
l
q
l
>
.
10: Stop if l = L; otherwise l = l + 1 and return to Line 1.
11: Let C = diag(c
1
, . . . , c
L
), and V = [v
1
. . .v
L
]
12: M = V(P
>
V)
?1
CQ
>
13: return M
Chemometrics (Geladi and Kowalski, 1986), pro-
ducing stable prediction models even when the
number of samples is considerably smaller than
the dimensionality of the feature space. In particu-
lar, PLSR fits a smaller number of latent variables
(10? 100 in practice) such that the correlation be-
tween the feature vectors for pivots in the two do-
mains are maximised in this latent space.
Let X and Y denote matrices formed by ar-
ranging respectively the vectors w
(i)
S
s and w
(i)
T
in
rows. PLSR decomposes X and Y into a series of
products between rank 1 matrices as follows:
X ?
L
?
l=1
?
l
p
l
>
= ?P
>
(1)
Y ?
L
?
l=1
?
l
q
l
>
= ?Q
>
. (2)
Here, ?
l
, ?
l
, p
l
, and q
l
are column vectors, and
the summation is taken over the rank 1 matrices
that result from the outer product of those vectors.
The matrices, ?, ?, P, and Q are constructed re-
spectively by arranging ?
l
, ?
l
, p
l
, and q
l
vectors
as columns.
Our method for learning a distribution predic-
tion model is shown in Algorithm 1. It is based on
the two block NIPALS routine (Wold, 1975; Rosi-
pal and Kramer, 2006) and iteratively discovers L
pairs of vectors (?
l
,?
l
) such that the covariances,
Cov(?
l
,?
l
), are maximised under the constraint
||p
l
|| = ||q
l
|| = 1. Finally, the prediction matrix,
M is computed using ?
l
,?
l
,p
l
, q
l
. The predicted
distribution
?
w
T
of a word w in T is given by
?
w
T
= Mw
S
. (3)
616
Our distribution prediction learning method is un-
supervised in the sense that it does not require
manually labeled data for a particular task from
any of the domains. This is an important point,
and means that the distribution prediction method
is independent of the task to which it may subse-
quently be applied. As we go on to show in Sec-
tion 6, this enables us to use the same distribution
prediction method for both POS tagging and sen-
timent classification.
4 Domain Adaptation
The main reason that a model trained only on the
source domain labeled data performs poorly in
the target domain is the feature mismatch ? few
features in target domain test instances appear in
source domain training instances. To overcome
this problem, we use the proposed distribution pre-
diction method to find those related features in the
source domain that correspond to the features ap-
pearing in the target domain test instances.
We consider two DA tasks: (a) cross-domain
POS tagging (Section 4.1), and (b) cross-domain
sentiment classification (Section 4.2). Note that
our proposed distribution prediction method can
be applied to numerous other NLP tasks that in-
volve sequence labelling and document classifica-
tion.
4.1 Cross-Domain POS Tagging
We represent each word using a set of features
such as capitalisation (whether the first letter of the
word is capitalised), numeric (whether the word
contains digits), prefixes up to four letters, and
suffixes up to four letters (Miller et al, 2011).
Next, for each word w in a source domain labeled
(i.e. manually POS tagged) sentence, we select its
neighbours u
(i)
in the source domain as additional
features. Specifically, we measure the similarity,
sim(u
(i)
S
,w
S
), between the source domain distri-
butions of u
(i)
and w, and select the top r simi-
lar neighbours u
(i)
for each word w as additional
features for w. We refer to such features as dis-
tributional features in this work. The value of a
neighbour u
(i)
selected as a distributional feature
is set to its similarity score sim(u
(i)
S
,w
S
). Next,
we train a CRF model using all features (i.e. cap-
italisation, numeric, prefixes, suffixes, and distri-
butional features) on source domain labeled sen-
tences.
We train a PLSR model, M, that predicts the
target domain distribution Mu
(i)
S
of a word u
(i)
in
the source domain labeled sentences, given its dis-
tribution, u
(i)
S
. At test time, for each word w that
appears in a target domain test sentence, we mea-
sure the similarity, sim(Mu
(i)
S
,w
T
), and select
the most similar r words u
(i)
in the source domain
labeled sentences as the distributional features for
w, with their values set to sim(Mu
(i)
S
,w
T
). Fi-
nally, the trained CRF model is applied to a target
domain test sentence.
Note that distributional features are always se-
lected from the source domain during both train
and test times, thereby increasing the number of
overlapping features between the trained model
and test sentences. To make the inference tractable
and efficient, we use a first-order Markov factori-
sation, in which we consider all pairwise combi-
nations between the features for the current word
and its immediate predecessor.
4.2 Cross-Domain Sentiment Classification
Unlike in POS tagging, where we must individ-
ually tag each word in a target domain test sen-
tence, in sentiment classification we must classify
the sentiment for the entire review. We modify the
DA method presented in Section 4.1 to satisfy this
requirement as follows.
Let us assume that we are given a set
{(x
(i)
S
, y
(i)
)}
n
i=1
of n labeled reviews x
(i)
S
for the
source domain S. For simplicity, let us consider
binary sentiment classification where each review
x
(i)
is labeled either as positive (i.e. y
(i)
= 1) or
negative (i.e. y
(i)
= ?1). Our cross-domain bi-
nary sentiment classification method can be easily
extended to the multi-class setting as well. First,
we lemmatise each word in a source domain la-
beled review x
(i)
S
, and extract both unigrams and
bigrams as features to represent x
(i)
S
by a binary-
valued feature vector. Next, we train a binary clas-
sification model, ?, using those feature vectors.
Any binary classification algorithm can be used
to learn ?. In our experiments, we used L2 reg-
ularised logistic regression.
Next, we train a PLSR model, M, as described
in Section 3.2 using unlabeled reviews in the
source and target domains. At test time, we rep-
resent a test target review H using a binary-valued
feature vector h of unigrams and bigrams of lem-
mas of the words in H, as we did for source do-
main labeled train reviews. Next, for each feature
w
(j)
extracted from H, we measure the similarity,
617
sim(Mu
(i)
S
,w
(j)
T
), between the target domain dis-
tribution of w
(j)
, and each feature (unigram or bi-
gram) u
(i)
in the source domain labeled reviews.
We score each source domain feature u
(i)
for its
relatedness to H using the formula:
score(u
(i)
,H) =
1
|H|
|H|
?
j=1
sim(Mu
(i)
S
,w
(j)
T
) (4)
where |H| denotes the total number of features ex-
tracted from the test review H. We select the top
scoring r features u
(i)
as distributional features for
H, and append those to h. The corresponding val-
ues of those distributional features are set to the
scores given by Equation 4. Finally, we classify
h using the trained binary classifier ?. Note that
given a test review, we find the distributional fea-
tures that are similar to all the words in the test re-
view from the source domain. In particular, we do
not find distributional features independently for
each word in the test review. This enables us to
find distributional features that are consistent with
all the features in a test review.
4.3 Model Choices
For both POS tagging and sentiment classifica-
tion, we experimented with several alternative
approaches for feature weighting, representation,
and similarity measures using development data,
which we randomly selected from the training in-
stances from the datasets described in Section 5.
For feature weighting for sentiment classifica-
tion, we considered using the number of occur-
rences of a feature in a review and tf-idf weight-
ing (Salton and Buckley, 1983). For representa-
tion, we considered distributional features u
(i)
in
descending order of their scores given by Equa-
tion 4, and then taking the inverse-rank as the val-
ues for the distributional features (Bollegala et al,
2011). However, none of these alternatives re-
sulted in performance gains. With respect to simi-
larity measures, we experimented with cosine sim-
ilarity and the similarity measure proposed by Lin
(1998); cosine similarity performed consistently
well over all the experimental settings. The feature
representation was held fixed during these similar-
ity measure comparisons.
For POS tagging, we measured the effect of
varying r, the number of distributional features,
using a development dataset. We observed that
setting r larger than 10 did not result in signifi-
cant improvements in tagging accuracy, but only
increased the train time due to the larger feature
space. Consequently, we set r = 10 in POS tag-
ging. For sentiment analysis, we used all features
in the source domain labeled reviews as distri-
butional features, weighted by their scores given
by Equation 4, taking the inverse-rank. In both
tasks, we parallelised similarity computations us-
ing BLAS
3
level-3 routines to speed up the com-
putations. The source code of our implementation
is publicly available
4
.
5 Datasets
To evaluate DA for POS tagging, following Blitzer
et al (2006), we use sections 2 ? 21 from Wall
Street Journal (WSJ) as the source domain labeled
data. An additional 100, 000 WSJ sentences from
the 1988 release of the WSJ corpus are used as the
source domain unlabeled data. Following Schn-
abel and Sch?utze (2013), we use the POS labeled
sentences in the SACNL dataset (Petrov and Mc-
Donald, 2012) for the five target domains: QA fo-
rums, Emails, Newsgroups, Reviews, and Blogs.
Each target domain contains around 1000 POS
labeled test sentences and around 100, 000 unla-
beled sentences.
To evaluate DA for sentiment classification,
we use the Amazon product reviews collected by
Blitzer et al (2007) for four different product cat-
egories: books (B), DVDs (D), electronic items
(E), and kitchen appliances (K). There are 1000
positive and 1000 negative sentiment labeled re-
views for each domain. Moreover, each domain
has on average 17, 547 unlabeled reviews. We use
the standard split of 800 positive and 800 negative
labeled reviews from each domain as training data,
and the remainder for testing.
6 Experiments and Results
For each domain D in the SANCL (POS tag-
ging) and Amazon review (sentiment classifica-
tion) datasets, we create a PPMI weighted co-
occurrence matrix F
D
. On average, F
D
created
for a target domain in the SANCL dataset con-
tains 104, 598 rows and 65, 528 columns, whereas
those numbers in the Amazon dataset are 27, 397
and 35, 200 respectively. In cross-domain senti-
ment classification, we measure the binary senti-
ment classification accuracy for the target domain
3
http://www.openblas.net/
4
http://www.csc.liv.ac.uk/
?
danushka/
software.html
618
test reviews for each pair of domains (12 pairs in
total for 4 domains). On average, we have 40, 176
pivots for a pair of domains in the Amazon dataset.
In cross-domain POS tagging, WSJ is always
the source domain, whereas the five domains in
SANCL dataset are considered as the target do-
mains. For this setting we have 9822 pivots on
average. The number of singular vectors k se-
lected in SVD, and the number of PLSR dimen-
sions L are set respectively to 1000 and 50 for the
remainder of the experiments described in the pa-
per. Later we study the effect of those two param-
eters on the performance of the proposed method.
The L-BFGS (Liu and Nocedal, 1989) method is
used to train the CRF and logistic regression mod-
els.
6.1 POS Tagging Results
Table 2 shows the token-level POS tagging accu-
racy for unseen words (i.e. words that appear in the
target domain test sentences but not in the source
domain labeled train sentences). By limiting the
evaluation to unseen words instead of all words,
we can evaluate the gain in POS tagging accuracy
solely due to DA. The NA (no-adapt) baseline sim-
ulates the effect of not performing any DA. Specif-
ically, in POS tagging, a CRF trained on source
domain labeled sentences is applied to target do-
main test sentences, whereas in sentiment classi-
fication, a logistic regression classifier trained us-
ing source domain labeled reviews is applied to the
target domain test reviews. The S
pred
baseline di-
rectly uses the source domain distributions for the
words instead of projecting them to the target do-
main. This is equivalent to setting the prediction
matrix M to the unit matrix. The T
pred
baseline
uses the target domain distribution w
T
for a word
w instead of Mw
S
. If w does not appear in the
target domain, then w
T
is set to the zero vector.
The S
pred
and T
pred
baselines simulate the two al-
ternatives of using source and target domain dis-
tributions instead of learning a PLSR model. The
DA method proposed in Section 4.1 is shown as
the Proposed method. Filter denotes the train-
ing set filtering method proposed by Schnabel and
Sch?utze (2013) for the DA of POS taggers.
From Table 2, we see that the Proposed method
achieves the best performance in all five domains,
followed by the T
pred
baseline. Recall that the
T
pred
baseline cannot find source domain words
that do not appear in the target domain as distri-
Target NA S
pred
T
pred
Filter Proposed
QA 67.34 68.18 68.75 57.08 69.28
?
Emails 65.62 66.62 67.07 65.61 67.09
Newsgroups 75.71 75.09 75.57 70.37 75.85
?
Reviews 56.36 54.60 56.68 47.91 56.93
?
Blogs 76.64 54.78 76.90 74.56 76.97
?
Table 2: POS tagging accuracies on SANCL.
butional features for the words in the target do-
main test reviews. Therefore, when the overlap be-
tween the vocabularies used in the source and the
target domains is small, T
pred
cannot reduce the
mismatch between the feature spaces. Poor perfor-
mance of the S
pred
baseline shows that the distri-
butions of a word in the source and target domains
are different to the extent that the distributional
features found using source domain distributions
are inadequate. The two baselines S
pred
and T
pred
collectively motivate our proposal to learn a distri-
bution prediction model from the source domain
to the target. The improvements of Proposed over
the previously proposed Filter are statistically sig-
nificant in all domains except the Emails domain
(denoted by ? in Table 2 according to the Bino-
mial exact test at 95% confidence). However, the
differences between the T
pred
and Proposed meth-
ods are not statistically significant.
6.2 Sentiment Classification Results
In Figure 1, we compare the Proposed cross-
domain sentiment classification method (Section
4.2) against several baselines and the current state-
of-the-art methods. The baselines NA, S
pred
, and
T
pred
are defined similarly as in Section 6.1. SST
is the Sentiment Sensitive Thesaurus proposed by
Bollegala et al (2011). SST creates a single distri-
bution for a word using both source and target do-
main reviews, instead of two separate distributions
as done by the Proposed method. SCL denotes
the Structural Correspondence Learning method
proposed by Blitzer et al (2006). SFA denotes
the Spectral Feature Alignment method proposed
by Pan et al (2010). SFA and SCL represent the
current state-of-the-art methods for cross-domain
sentiment classification. All methods are evalu-
ated under the same settings, including train/test
split, feature spaces, pivots, and classification al-
gorithms so that any differences in performance
can be directly attributable to their domain adapt-
ability. For each domain, the accuracy obtained
by a classifier trained using labeled data from that
619
E?>B D?>B K?>B5055
6065
7075
8085
Accura
cy
B?>E D?>E K?>E50
60
70
80
90
Accura
cy
B?>D E?>D K?>D5055
6065
7075
8085
Accura
cy
 
 
NA SFA SST SCL Spred Tpred ProposedB?>K E?>K D?>K
50
60
70
80
90
Accura
cy
Figure 1: Cross-Domain sentiment classification.
domain is indicated by a solid horizontal line in
each sub-figure. This upper baseline represents
the classification accuracy we could hope to obtain
if we were to have labeled data for the target do-
main. Clopper-Pearson 95% binomial confidence
intervals are superimposed on each vertical bar.
From Figure 1 we see that the Proposed method
reports the best results in 8 out of the 12 domain
pairs, whereas SCL, SFA, and S
pred
report the
best results in other cases. Except for the D-E set-
ting in which Proposed method significantly out-
performs both SFA and SCL, the performance of
the Proposed method is not statistically signifi-
cantly different to that of SFA or SCL.
The selection of pivots is vital to the perfor-
mance of SFA. However, unlike SFA, which re-
quires us to carefully select a small subset of pivots
(ca. less than 500) using some heuristic approach,
our Proposed method does not require any pivot
selection. Moreover, SFA projects source domain
reviews to a lower-dimensional latent space, in
which a binary sentiment classifier is subsequently
trained. At test time SFA projects a target review
into this lower-dimensional latent space and ap-
plies the trained classifier. In contrast, our Pro-
posed method predicts the distribution of a word
in the target domain, given its distribution in the
source domain, thereby explicitly translating the
source domain reviews to the target. This property
enables us to apply the proposed distribution pre-
diction method to tasks other than sentiment anal-
ysis such as POS tagging where we must identify
distributional features for individual words.
10 100 200 300 400 500 600 700 8000.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
PLSR dimensions
Accu
racy
 
 
E??>BD??>B
Figure 2: The effect of PLSR dimensions.
Unlike our distribution prediction method,
which is unsupervised, SST requires labeled data
for the source domain to learn a feature mapping
between a source and a target domain in the form
of a thesaurus. However, from Figure 1 we see
that in 10 out of the 12 domain-pairs the Proposed
method returns higher accuracies than SST.
To evaluate the overall effect of the number of
singular vectors k used in the SVD step, and the
number of PLSR components L used in Algorithm
1, we conduct two experiments. To evaluate the ef-
fect of the PLSR dimensions, we fixed k = 1000
and measured the cross-domain sentiment classi-
fication accuracy over a range of L values. As
shown in Figure 2, accuracy remains stable across
a wide range of PLSR dimensions. Because the
time complexity of Algorithm 1 increases linearly
with L, it is desirable that we select smaller L val-
620
1000 1500 2000 2500 3000
0.58
0.6
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
SVD dimensions
Accu
racy
 
 E??>BD??>B
Figure 3: The effect of SVD dimensions.
Measure Distributional features
sim(u
S
, w
S
) thin (0.1733), digestible (0.1728),
small+print (0.1722)
sim(u
T
, w
T
) travel+companion (0.6018), snap-in
(0.6010), touchpad (0.6016)
sim(u
S
, w
T
) segregation (0.1538), participation
(0.1512), depression+era (0.1508)
sim(Mu
S
, w
T
) small (0.2794), compact (0.2641),
sturdy (0.2561)
Table 3: Top 3 distributional features u ? S for
the word lightweight (w).
ues in practice.
To evaluate the effect of the SVD dimensions,
we fixed L = 100 and measured the cross-domain
sentiment classification accuracy for different k
values as shown in Figure 3. We see an overall
decrease in classification accuracy when k is in-
creased. Because the dimensionality of the source
and target domain feature spaces is equal to k, the
complexity of the least square regression problem
increases with k. Therefore, larger k values result
in overfitting to the train data and classification ac-
curacy is reduced on the target test data.
As an example of the distribution prediction
method, in Table 3 we show the top 3 similar
distributional features u in the books (source) do-
main, predicted for the electronics (target) domain
word w = lightweight, by different similarity
measures. Bigrams are indicted by a + sign and
the similarity scores of the distributional features
are shown within brackets.
Using the source domain distributions for both
u and w (i.e. sim(u
S
, w
S
)) produces distribu-
tional features that are specific to the books do-
main, or to the dominant adjectival sense of hav-
ing no importance or influence. On the other
hand, using target domain distributions for u and
w (i.e. sim(u
T
, w
T
)) returns distributional fea-
tures of the dominant nominal sense of lower in
weight frequently associated with electronic de-
vices. Simply using source domain distributions
u
S
(i.e. sim(u
S
, w
T
)) returns totally unrelated dis-
tributional features. This shows that word distribu-
tions in source and target domains are very differ-
ent and some adaptation is required prior to com-
puting distributional features.
Interestingly, we see that by using the dis-
tributions predicted by the proposed method
(i.e. sim(Mu
S
, w
T
)) we overcome this problem
and find relevant distributional features from the
source domain. Although for illustrative purposes
we used the word lightweight, which occurs in
both the source and the target domains, our pro-
posed method does not require the source domain
distribution w
S
for a word w in a target domain
document. Therefore, it can find distributional fea-
tures even for words occurring only in the target
domain, thereby reducing the feature mismatch
between the two domains.
7 Conclusion
We proposed a method to predict the distribution
of a word across domains. We first create a distri-
butional representation for a word using the data
from a single domain, and then learn a Partial
Least Square Regression (PLSR) model to pre-
dict the distribution of a word in a target domain
given its distribution in a source domain. We eval-
uated the proposed method in two domain adapta-
tion tasks: cross-domain POS tagging and cross-
domain sentiment classification. Our experiments
show that without requiring any task-specific cus-
tomisations to our distribution prediction method,
it outperforms competitive baselines and achieves
comparable results to the current state-of-the-art
domain adaptation methods.
References
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case
study. Technical report, Microsoft Research.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673 ? 721.
Romaric Besanc?on, Martin Rajman, and Jean-C?edric
Chappelier. 1999. Textual similarities based on a
621
distributional approach. In Proc. of DEXA, pages
180 ? 184.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of EMNLP, pages 120 ?
128.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proc. of ACL, pages 440 ? 447.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a senti-
ment sensitive thesaurus for cross-domain sentiment
classification. In Proc. of ACL/HLT, pages 132 ?
141.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proc. of
COLING/ACL Interactive Presentation Sessions.
John A. Bullinaria and Jospeh P. Levy. 2007. Ex-
tracting semantic representations from word co-
occurrence statistics: A computational study. Be-
havior Research Methods, 39(3):510 ? 526.
Jinho D. Choi and Martha Palmer. 2012. Fast and
robust part-of-speech tagging using dynamic model
selection. In Proc. of ACL Short Papers, volume 2,
pages 363 ? 367.
Kenneth W. Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Computational Linguistics, 16(1):22 ? 29,
March.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 26 ? 33.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proc. of ACL, pages 256 ? 263.
John R. Firth. 1957. A synopsis of linguistic theory
1930-55. Studies in Linguistic Analysis, pages 1 ?
32.
Paul Geladi and Bruce R. Kowalski. 1986. Partial
least-squares regression: a tutorial. Analytica Chim-
ica Acta, 185(0):1 ? 17.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named en-
tity recognition. In Proc. of NAACL, pages 281 ?
289.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proc. of
ACL/HLT, pages 123 ? 131.
Fei Huang and Alexander Yates. 2009. Distributional
representations for handling sparsity in supervised
sequence-labeling. In ACL-IJCNLP?09, pages 495
? 503.
Fei Huang and Alexander Yates. 2012. Biased repre-
sentation learning for domain adaptation. In Proc.
of EMNLP/CoNLL, pages 1313 ? 1323.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2012. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359 ? 389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL, pages 768 ? 774.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503 ? 528.
David McClosky, Eugene Charniak, and Mark John-
son. 2010. Automatic domain adaptation for pars-
ing. In Proc. of NAACL/HLT, pages 28 ? 36.
John E. Miller, Manabu Torii, and K. Vijay-Shanker.
2011. Building domain-specific taggers without an-
notated (domain) data. In Proc. of EMNLP/CoNLL,
pages 1103 ? 1111.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161 ?
199.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain sen-
timent classification via spectral feature alignment.
In Proc. of WWW, pages 751 ? 760.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proc. of EMNLP, pages 938 ? 947.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Notes of
the 1st SANCL Workshop.
Natalia Ponomareva and Mike Thelwall. 2012. Do
neighbours help? an exploration of graph-based al-
gorithms for cross-domain sentiment classification.
In Proc. of EMNLP, pages 655 ? 665.
Roman Rosipal and Nicole Kramer. 2006. Overview
and recent advances in partial least squares. In
C. Saunders et al, editor, SLSFS?05, volume 3940 of
LNCS, pages 34 ? 51, Berlin Heidelberg. Springer-
Verlag.
G. Salton and C. Buckley. 1983. Introduction to
Modern Information Retreival. McGraw-Hill Book
Company.
Tobias Schnabel and Hinrich Sch?utze. 2013. Towards
robust cross-domain domain adaptation for part-of-
speech tagging. In Proc. of IJCNLP, pages 198 ?
206.
622
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proc. of EMNLP, pages 1631 ? 1642.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of seman-
tics. Journal of Aritificial Intelligence Research,
37:141 ? 188.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antonie Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. Journal of Machine Learning
Research, 11:3371 ? 3408.
Herman Wold. 1975. Path models with latent vari-
ables: the NIPALS approach. In H. M. Blalock
et al, editor, Quantitative socialogy: international
perspective on mathematical and statistical model-
ing, pages 307 ? 357. Academic.
Herman Wold. 1985. Partial least squares. In Samel
Kotz and Norman L. Johnson, editors, Encyclopedia
of the Statistical Sciences, pages 581 ? 591. Wiley.
Min Xiao, Feipeng Zhao, and Yuhong Guo. 2013.
Learning latent word representations for domain
adaptation using supervised word clustering. In
Proc. of EMNLP, pages 152 ? 162.
623
Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38?44,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Semantic Composition with Quotient Algebras
Daoud Clarke
University of Hertfordshire
Hatfield, UK
daoud@metrica.net
Rudi Lutz
University of Sussex
Brighton, UK
rudil@sussex.ac.uk
David Weir
University of Sussex
Brighton, UK
davidw@sussex.ac.uk
Abstract
We describe an algebraic approach for
computing with vector based semantics.
The tensor product has been proposed as
a method of composition, but has the un-
desirable property that strings of different
length are incomparable. We consider how
a quotient algebra of the tensor algebra can
allow such comparisons to be made, offer-
ing the possibility of data-driven models of
semantic composition.
1 Introduction
Vector based techniques have been exploited in a
wide array of natural language processing appli-
cations (Schu?tze, 1998; McCarthy et al, 2004;
Grefenstette, 1994; Lin, 1998; Bellegarda, 2000;
Choi et al, 2001). Techniques such as latent se-
mantic analysis and distributional similarity anal-
yse contexts in which terms occur, building up a
vector of features which incorporate aspects of the
meaning of the term. This idea has its origins in
the distributional hypothesis of Harris (1968), that
words with similar meanings will occur in similar
contexts, and vice-versa.
However, there has been limited attention paid
to extending this idea beyond individual words,
so that the distributional meaning of phrases and
whole sentences can be represented as vectors.
While these techniques work well at the word
level, for longer strings, data becomes extremely
sparse. This has led to various proposals explor-
ing methods for composing vectors, rather than de-
riving them directly from the data (Landauer and
Dumais, 1997; Foltz et al, 1998; Kintsch, 2001;
Widdows, 2008; Clark et al, 2008; Mitchell and
Lapata, 2008; Erk and Pado, 2009; Preller and
Sadrzadeh, 2009). Many of these approaches use
a pre-defined composition operation such as ad-
dition (Landauer and Dumais, 1997; Foltz et al,
1998) or the tensor product (Smolensky, 1990;
Clark and Pulman, 2007; Widdows, 2008) which
contrasts with the data-driven definition of com-
position developed here.
2 Tensor Algebras
Following the context-theoretic semantics of
Clarke (2007), we take the meaning of strings as
being described by a multiplication on a vector
space that is bilinear with respect to the addition
of the vector space, i.e.
x(y + z) = xy + xz (x+ y)z = xz + yz
It is assumed that the multiplication is associative,
but not commutative. The resulting structure is an
associative algebra over a field ? or simply an
algebra when there is no ambiguity.
One commonly used bilinear multiplication op-
erator on vector spaces is the tensor product (de-
noted ?), whose use as a method of combining
meaning was first proposed by Smolensky (1990),
and has been considered more recently by Clark
and Pulman (2007) and Widdows (2008), who also
looked at the direct sum (which Widdows calls
the direct product, denoted ?).
We give a very brief account of the tensor prod-
uct and direct sum in the finite-dimensional case;
see (Halmos, 1974) for formal and complete defi-
nitions. Roughly speaking, if u1, u2, . . . un form
an orthonormal basis for a vector space U and
v1, v2, . . . vm form an orthonormal basis for vector
space V , then the space U ?V has dimensionality
nm with an orthonormal basis formed by the set
of all ordered pairs (ui, vj), denoted by ui ? vj ,
of the individual basis elements. For arbitrary el-
ements u =
?n
i=1 ?iui and v =
?m
j=1 ?jvj the
tensor product of u and v is then given by
u? v =
n?
i
m?
j
?i?j ui ? vj
38
For two finite dimensional vector spaces U and
V (over a field F ) of dimensionality n and m re-
spectively, the direct sum U ? V is defined as the
cartesian product U ? V together with the oper-
ations (u1, v1) + (u2, v2) = (u1 + u2, v1 + v2),
and a(u1, v1) = (au1, av1), for u1, u2 ? U ,
v1, v2 ? V and a ? F . In this case the vectors
u1, u2, . . . un, v1, v2, . . . vm form an orthonormal
set of basis vectors in U ? V , which is thus of
dimensionality n + m. In this case one normally
identifies U with the set of vectors in U ?V of the
form (u, 0), and V with the set of vectors of the
form (0, v). This construction makes U ? V iso-
morphic to V ?U , and thus the direct sum is often
treated as commutative, as we do in this paper.
The motivation behind using the tensor product
to combine meanings is that it is very fine-grained.
So, if, for example, red is represented by a vector
u consisting of a feature for each noun that is mod-
ified by red, and apple is represented by a vector
v consisting of a feature for each verb that occurs
with apple as a direct object, then red apple will
be represented by u ? v with a non-zero compo-
nent for every pair of non-zero features (one from
u and one from v). So, there is a non-zero ele-
ment for each composite feature, something that
has been described as red, and something that has
been done with an apple, for example, sky and eat.
Both ? and ? are intuitively appealing as se-
mantic composition operators, since u and v are
reconstructible from each of u? v and u? v, and
thus no information is lost in composing u and v.
Conversely, this is not possible with ordinary vec-
tor addition, which also suffers from the fact that it
is strictly commutative (not simply up to isomor-
phism like?), whereas natural language composi-
tion is in general manifestly non-commutative.
We make use of a construction called the tensor
algebra on a vector space V (where V is a space
of context features), defined as:
T (V ) = R? V ? (V ? V )? (V ? V ? V )? ? ? ?
Any element of T (V ) can be described as a sum of
components with each in a different tensor power
of V . Multiplication is defined as the tensor prod-
uct on these components, and extended linearly to
the whole of T (V ). We define the degree of a vec-
tor u in T (V ) to be the tensor power of its high-
est dimensional non-zero component, and denote
it deg(u); so for example, both v?v and u?(v?v)
have degree two, for 0 6= u, v ? V . We restrict
T (V ) to only contain vectors of finite degree.
A standard way to compare elements of a vector
space is to make use of an inner product, which
provides a measure of semantic distance on that
space. Assuming we have an inner product ??, ?? on
V , T (V ) can be given an inner product by defining
??, ?? = ?? for ?, ? ? R, and
?x1 ? y1, x2 ? y2? = ?x1, x2??y1, y2?
for x1, y1, x2, y2 ? V , and then extending this in-
ductively (and by linearity) to the whole of T (V ).
We assume that words are associated with vec-
tors in V , and that the higher tensor powers repre-
sent strings of words. The problem with the tensor
product as a method of composition, given the in-
ner product as we have defined it, is that strings
of different lengths will have orthogonal vectors,
clearly a serious problem, since strings of different
lengths can have similar meanings. In our previous
example, the vector corresponding to the concept
red apple lives in the vector space U ? V , and so
we have no way to compare it to the space V of
nouns, even though red apple should clearly be re-
lated to apple.
Previous work has not made full use of the ten-
sor product space; only tensor products are used,
not sums of tensor products, giving us the equiva-
lent of the product states of quantum mechanics.
Our approach imposes relations on the vectors of
the tensor product space that causes some product
states to become equivalent to entangled states,
containing sums of tensor products of different de-
grees. This allows strings of different lengths to
share components. We achieve this by construct-
ing a quotient algebra.
3 Quotient Algebras
An ideal I of an algebra A is a sub-vector space
of A such that xa ? I and ax ? I for all a ? A
and all x ? I . An ideal introduces a congruence
? on A defined by x ? y if and only if x? y ? I .
For any set of elements ? ? A there is a unique
minimal ideal I? containing all elements of ?; this
is called the ideal generated by ?. The quotient
algebra A/I is the set of all equivalence classes
defined by this congruence. Multiplication is de-
fined on A/I by the multiplication on A, since ?
is a congruence.
By adding an element x ? y to the generating
set ? of an ideal, we are saying that we want to
set x ? y to zero in the quotient algebra, which
has the effect of setting x equal to y. Thus, if we
39
have a set of pairs of vectors that we wish to make
equal in the quotient algebra, we put their differ-
ences in the generating set of the ideal. Note that
putting a single vector v in the generating set can
have knock-on effects, since all products of v with
elements of A will also end up in the ideal.
Although we have an inner product defined on
T (V ), we are not aware of any satisfactory method
for defining an inner product on T (V )/I , a con-
sequence of the fact that both T (V ) and I are
not complete. Instead, we define an inner prod-
uct on a space which contains the quotient algebra,
T (V )/I . Rather than considering all elements of
the ideal when computing the quotient, we con-
sider a sub-vector space of the ideal, limiting our-
selves to the space Gk generated from ? by only
allowing multiplication by elements up to a certain
degree, k.
Let us denote the vector subspace generated by
linearity alone (no multiplications) from a sub-
set ? of T (V ) by G(?). Also suppose B =
{e1, . . . , eN} is a basis for V . We then define
the spaces Gk as follows. Define sets ?k (k =
0, 1, 2, . . .) inductively as follows:
?0 = ?
?k = ?k?1 ? {(ei ? ?k?1)|ei ? B}
? {(?k?1 ? ei)|ei ? B}
Define
Gk = G(?k)
We note that
G0 ? G1 ? . . . Gk ? . . . ? I ? T (V )
form an increasing sequence of linear vector sub-
spaces of T (V ), and that
I =
??
k=0
Gk
This means that for any x ? I there exists a small-
est k such that for all k? ? k we have that x ? Gk? .
Lemma. Let x ? I, x 6= 0 and let deg(x) = d.
Then for all k ? d ? mindeg(?) we have that
x ? Gk, where mindeg(?) is defined to be the
minimum degree of the non-zero components oc-
curring in the elements of ?.
Proof. We first note that for x ? I it must
be the case that deg(x) ? mindeg(?) since I
is generated from ?. Therefore we know d ?
mindeg(?) ? 0. We only need to show that
x ? Gd?mindeg(?). Let k
? be the smallest in-
teger such that x ? Gk? . Since x 6? Gk??1 it
must be the case that the highest degree term of
x comes from V ? Gk??1 ? Gk??1 ? V . There-
fore k? + mindeg(?) ? d ? k? + maxdeg(?).
From this it follows that the smallest k? for which
x ? Gk? satisfies k? ? d ? mindeg(?), and we
know x ? Gk for all k ? k?. In particular x ? Gk
for k ? d?mindeg(?).
We show that T (V )/Gk (for an appropriate
choice of k) captures the essential features of
T (V )/I in terms of equivalence:
Proposition. Let deg(a ? b) = d and let k ?
d ? mindeg(?). Then a ? b in T (V )/Gk if and
only if a ? b in T (V )/I .
Proof. Since Gk ? I , the equivalence class of an
element a in T (V )/I is a superset of the equiva-
lence class of a in T (V )/Gk, which gives the for-
ward implication. The reverse follows from the
lemma above.
In order to define an inner product on
T (V )/Gk, we make use of the result of Berbe-
rian (1961) that if M is a finite-dimensional
linear subspace of a pre-Hilbert space P , then
P = M ? M?, where M? is the orthogonal
complement of M in P . In our case this implies
T (V ) = Gk ? G?k and that every element
x ? T (V ) has a unique decomposition as
x = y + x?k where y ? Gk and x
?
k ? G
?
k . This
implies that T (V )/Gk is isomorphic to G?k , and
that for each equivalence class [x]k in T (V )/Gk
there is a unique corresponding element x?k ? G
?
k
such that x?k ? [x]k. This element x
?
k can be
thought of as the canonical representation of all
elements of [x]k in T (V )/Gk, and can be found
by projecting any element in an equivalence class
onto G?k . This enables us to define an inner
product on T (V )/Gk by ?[x]k, [y]k?k = ?x?k, y
?
k?.
The idea behind working in the quotient algebra
T (V )/I rather than in T (V ) is that the elements
of the ideal capture differences that we wish to ig-
nore, or alternatively, equivalences that we wish to
impose. The equivalence classes in T (V )/I repre-
sent this imposition, and the canonical representa-
tives in I? are elements which ignore the distinc-
tions between elements of the equivalence classes.
40
However, by using Gk, for some k, instead of
the full ideal I , we do not capture some of the
equivalences implied by I . We would, therefore,
like to choose k so that no equivalences of impor-
tance to the sentences we are considering are ig-
nored. While we have not precisely established a
minimal value for k that achieves this, in the dis-
cussion that follows, we set k heuristically as
k = l ?mindeg(?)
where l is the maximum length of the sentences
currently under consideration, and ? is the gen-
erating set for the ideal I . The intuition behind
this is that we wish all vectors occurring in ? to
have some component in common with the vec-
tor representation of our sentences. Since com-
ponents in the ideal are generated by multipli-
cation (and linearity), in order to allow the ele-
ments of ? containing the lowest degree compo-
nents to potentially interact with our sentences,
we will have to allow multiplication of those el-
ements (and all others) by components of degree
up to l ?mindeg(?).
Given a finite set ? ? T (V ) of elements gen-
erating the ideal I , to compute canonical repre-
sentations, we first compute a generating set ?k
for Gk following the inductive definition given
earlier, and removing any elements that are not
linearly independent using a standard algorithm.
Using the Gram-Schmidt process (Trefethen and
Bau, 1997), we then calculate an orthonormal ba-
sis ?? for Gk, and, by a simple extension of Gram-
Schmidt, compute the projection of a vector u onto
G?k using the basis ?
?.
We now show how ?, the set of vectors gener-
ating the ideal, can be constructed on the basis of
a tree-bank, ensuring that the vectors for any two
strings of the same grammatical type are compa-
rable.
4 Data-driven Composition
Suppose we have a tree-bank, its associated tree-
bank grammar G, and a way of associating a con-
text vector with every occurrence of a subtree in
the tree-bank (where the vectors indicate the pres-
ence of features occurring in that particular con-
text). The context vector associated with a spe-
cific occurrence of a subtree in the tree-bank is an
individual context vector.
We assume that for every rule, there is a distin-
guished non-terminal on the right hand side which
we call the head. We also assume that for every
production pi there is a linear function ?pi from the
space generated by the individual context vectors
of the head to the space generated by the individ-
ual context vectors of the left hand side. When
there is no ambiguity, we simply denote this func-
tion ?.
Let X? be the sum over all individual vectors of
subtrees rooted withX in the tree-bank. Similarly,
for each Xj in the right-hand-side of the rule pii :
X ? X1 . . . Xr(pii), where r(pi) is the rank of pi,
let i?,j be the sum over the individual vectors of
those subtrees rooted with Xj where the subtree
occurs as the jth daughter of a local tree involving
the production pii in the tree-bank.
For each rule pi : X ? X1 . . . Xr with headXh
we add vectors
?pi,i = ?(ei)?X?1?. . .?X?h?1?ei?X?h+1?. . .?X?r
for each basis element ei of VXh to the generating
set. The reasoning behind this is to ensure that the
meaning corresponding to a vector associated with
the head of a rule is maintained as it is mapped to
the vector space associated with the left hand side
of the rule.
It is often natural to assume that the individual
context vector of a non-terminal is the same as the
individual context vector of its head. In this case,
we can take ? to be the identity map. In particular,
for a rule of the form pi : X ? X1, then ?pi,i is
zero.
It is important to note at this point that we have
presented only one of many ways in which a gram-
mar could be used to generate an ideal. In partic-
ular, it is possible to add more vectors to the ideal,
allowing more fine-grained distinctions, for exam-
ple through the use of a lexicalised grammar.
For each sentence w, we compute the tensor
product w? = a?1 ? a?2 ? ? ? ? ? a?n where the string
of words a1 . . . an form w, and each a?i is a vector
in V . For a sentence w we find an element w?O of
the orthogonal complement of Gk in T (V ) such
that w?O ? [w?], where [w?] denotes the equivalence
class of w? given the subspace Gk.
5 Example
We show how our formalism applies in a simple
example. Assume we have a corpus which
consists of the following sentences:
41
ap
pl
e
bi
g
ap
pl
e
re
d
ap
pl
e
ci
ty
bi
g
ci
ty
re
d
ci
ty
bo
ok
bi
g
bo
ok
re
d
bo
ok
apple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080
big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11
red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33
city 1.0 0.26 0.24 0.0 0.0 0.0
big city 1.0 0.33 0.0 0.0 0.0
red city 1.0 0.0 0.0 0.0
book 1.0 0.26 0.24
big book 1.0 0.33
red book 1.0
Figure 1: Similarities between phrases
see red apple see big city
buy apple visit big apple
read big book modernise city
throw old small red book see modern city
buy large new book
together with the following productions.
1. N? ? Adj N?
2. N? ? N
where N and Adj are terminals representing nouns
and adjectives, along with rules for the terminals.
We consider the space of adjective/noun phrases,
generated by N?, and define the individual context
of a noun to be the verb it occurs with, and the in-
dividual context of an adjective to be the noun it
modifies. For each rule, we take ? to be the iden-
tity map, so the vector spaces associated with N
and N?, and the vector space generated by indi-
vidual contexts of the nouns are all the same. In
this case, the only non-zero vectors which we add
to the ideal are those for the second rule (ignoring
the first rule, since we do not consider verbs in this
example except as contexts), which has the set of
vectors
?i = ei ? A?dj? ei
where i ranges over the basis vectors for contexts
of nouns: see, buy , visit , read ,modernise , and
A?dj = 2eapple + 2ebook + ecity
In order to compute canonical representations
of vectors, we take k = 1.
5.1 Discussion
Figure 1 shows the similarity between the noun
phrases in our sample corpus. Note that the vec-
tors we have put in the generating set describe only
compositionality of meaning ? thus for example
the similarity of the non-compositional phrase big
apple to city is purely due to the distributional
similarity between apple and city and composition
with the adjective big.
Our preliminary investigations indicate that the
cosine similarity values are very sensitive to the
particular corpus and features chosen; we are cur-
rently investigating other ways of measuring and
computing similarity.
One interesting feature in the results is how ad-
jectives alter the similarity between nouns. For ex-
ample, red apple and red city have the same sim-
ilarity as apple and city, which is what we would
expect from a pure tensor product. This also ex-
plains why all phrases containing book are disjoint
to those containing city, since the original vector
for book is disjoint to city.
The contribution that the quotient algebra gives
is in comparing the vectors for nouns with those
for noun-adjective phrases. For example, red ap-
ple has components in common with apple, as we
would expect, which would not be the case with
just the tensor product.
6 Conclusion and Further Work
We have presented the outline of a novel approach
to semantic composition that uses quotient alge-
bras to compare vector representations of strings
of different lengths.
42
The dimensionality of the construction we use
increases exponentially in the length of the sen-
tence; this is a result of our use of the tensor prod-
uct. This causes a problem for computation us-
ing longer phrases; we hope to address this in fu-
ture work by looking at the representations we use.
For example, product states can be represented in
much lower dimensions by representing them as
products of lower dimensional vectors.
The example we have given would seem to in-
dicate that we intend putting abstract (syntactic)
information about meaning into the set of generat-
ing elements of the ideal. However, there is no rea-
son that more fine-grained aspects of meaning can-
not be incorporated, even to the extent of putting
in vectors for every pair of words. This would
automatically incorporate information about non-
compositionality of meaning. For example, by in-
cluding the vector ?big apple ? b?ig ? a?pple , we
would expect to capture the fact that the term big
apple is non-compositional, and more similar to
city than we would otherwise expect.
Future work will also include establishing the
implications of varying the constant k and explor-
ing different methods for choosing the set ? that
generates the ideal. We are currently preparing
an experimental evaluation of our approach, using
vectors obtained from large corpora.
7 Acknowledgments
We are grateful to Peter Hines, Stephen Clark, Pe-
ter Lane and Paul Hender for useful discussions.
The first author also wishes to thank Metrica for
supporting this research.
References
Jerome R. Bellegarda. 2000. Exploiting latent se-
mantic information in statistical language modeling.
Proceedings of the IEEE, 88(8):1279?1296.
Sterling K. Berberian. 1961. Introduction to Hilbert
Space. Oxford University Press.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for text
segmentation. In Proceedings of the 2001 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 109?117.
Stephen Clark and Stephen Pulman. 2007. Combin-
ing symbolic and distributional models of meaning.
In Proceedings of the AAAI Spring Symposium on
Quantum Interaction, pages 52?55, Stanford, CA.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008),
pages 133?140, Oxford, UK.
Daoud Clarke. 2007. Context-theoretic Semantics
for Natural Language: an Algebraic Framework.
Ph.D. thesis, Department of Informatics, University
of Sussex.
Katrin Erk and Sebastian Pado. 2009. Paraphrase as-
sessment in structured vector space: Exploring pa-
rameters and datasets. In Proceedings of the EACL
Workshop on Geometrical Methods for Natural Lan-
guage Semantics (GEMS).
P. W. Foltz, W. Kintsch, and T. K. Landauer. 1998.
The measurement of textual coherence with latent
semantic analysis. Discourse Process, 15:285?307.
Gregory Grefenstette. 1994. Explorations in auto-
matic thesaurus discovery. Kluwer Academic Pub-
lishers, Dordrecht, NL.
Paul Halmos. 1974. Finite dimensional vector spaces.
Springer.
Zellig Harris. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
W. Kintsch. 2001. Predication. Cognitive Science,
25:173?202.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato?s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211?
240.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics and the 17th International Conference
on Computational Linguistics (COLING-ACL ?98),
pages 768?774, Montreal.
43
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In ACL ?04: Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics, page 279, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June. Association for Computational Linguistics.
Anne Preller and Mehrnoosh Sadrzadeh. 2009. Bell
states and negation in natural languages. In Pro-
ceedings of Quantum Physics and Logic.
Heinrich Schu?tze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Paul Smolensky. 1990. Tensor product variable bind-
ing and the representation of symbolic structures in
connectionist systems. Artificial Intelligence, 46(1-
2):159?216, November.
Lloyd N. Trefethen and David Bau. 1997. Numerical
Linear Algebra. SIAM.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Ox-
ford, UK.
44
Algebraic Approaches to Compositional Distributional Semantics
Daoud Clarke
University of Hertfordshire
daoud@metrica.net
David Weir
University of Sussex
davidw@sussex.ac.uk
Rudi Lutz
University of Sussex
rudil@sussex.ac.uk
Abstract
The question of how to compose meaning in distributional representations of meaning has re-
cently been recognised as a central issue in computational linguistics. In this paper we describe
three general and powerful tools that can be used to describe composition in distributional seman-
tics: quotient algebras, learning of finite dimensional algebras, and the construction of algebras from
semigroups.
1 Introduction
Vector based representations of meaning have wide application in natural language processing. While
these techniques work well at the word level, for longer strings, data becomes extremely sparse. The
question of how the principle of compositionality might apply for such representations has thus been
recognised as an important one (Widdows, 2008; Clark et al, 2008).
Context-theoretic semantics (Clarke, 2007) is a framework for composing meanings in vector based
semantics, in which the composition of the meaning of strings is described by a multiplication on a real
vector space A that is bilinear with respect to the addition of the vector space, i.e.
x(y + z) = xy + xz (x + y)z = xz + yz (?x)(?y) = ??xy
where x, y, z ? A and ?, ? ? R. It is assumed that the multiplication is associative, but not commutative.
The resulting structure is an associative algebra over a field ? or simply an algebra when there is no
ambiguity. Clarke (2007) gives a mathematical model of meaning as context, and shows that under this
model, the meaning of natural language expressions can be described by an algebra. The framework
is also applied to models of textual entailment, and logical and ontological representations of natural
language meaning.
In this paper, we identify three general techniques for constructing algebras.
? Using quotient algebras to impose relations on a free algebra, as described in (Clarke et al, 2010).
? Defining finite-dimensional algebras using matrices. Any finite-dimensional algebra can be de-
scribed in this way; we have investigated the possibility of learning such algebras using least
squares regression.
? Constructing algebras from a semigroup to give it vector space properties. We sketch a possible
method of using this technique, identified by Clarke (2007), to endow logical semantics with a
vector space nature.
This paper presents a preliminary consideration of these general techniques, and our goal is simply to
show that they are worthy of further exploration.
325
ap
pl
e
bi
g
ap
pl
e
re
d
ap
pl
e
ci
ty
bi
g
ci
ty
re
d
ci
ty
bo
ok
bi
g
bo
ok
re
d
bo
ok
apple 1.0 0.26 0.24 0.52 0.13 0.12 0.33 0.086 0.080
big apple 1.0 0.33 0.13 0.52 0.17 0.086 0.33 0.11
red apple 1.0 0.12 0.17 0.52 0.080 0.11 0.33
city 1.0 0.26 0.24 0.0 0.0 0.0
big city 1.0 0.33 0.0 0.0 0.0
red city 1.0 0.0 0.0 0.0
book 1.0 0.26 0.24
big book 1.0 0.33
red book 1.0
Figure 1: Cosine similarity values between phrases
see red apple see big city
buy apple visit big apple
read big book modernise city
throw old small red book see modern city
buy large new book
Figure 2: The corpus used to compute the vectors
that formed the generating set for the ideal.
2 Quotient Algebras
One commonly used bilinear multiplication operator on vector spaces is the tensor product (denoted ?),
whose use as a method of combining meaning was first proposed by Smolensky (1990), and has been
considered more recently by Clark and Pulman (2007) and Widdows (2008), who also looked at the
direct sum (which Widdows calls the direct product, denoted ?).
The tensor algebra on a vector space V (where V is a space of context features) is defined as:
T (V ) = R? V ? (V ? V )? (V ? V ? V )? ? ? ?
Any element of T (V ) can be described as a sum of components with each in a different tensor power
of V . Multiplication is defined as the tensor product on these components, and extended linearly to the
whole of T (V ).
Previous work has not made full use of the tensor product space; only tensor products are used,
not sums of tensor products, giving us the equivalent of the product states of quantum mechanics. Our
approach imposes relations on the vectors of the tensor product space that causes some product states
to become equivalent to entangled states, containing sums of tensor products of different degrees. This
allows strings of different lengths to share components. We achieve this by constructing a quotient
algebra.
An ideal I of an algebra A is a sub-vector space of A such that xa ? I and ax ? I for all a ? A
and all x ? I . An ideal introduces a congruence ? on A defined by x ? y if and only if x? y ? I . For
any set of elements ? ? A there is a unique minimal ideal I? containing all elements of ?; this is called
the ideal generated by ?. The quotient algebra A/I is the set of all equivalence classes defined by this
congruence. Multiplication is defined on A/I by the multiplication on A, since ? is a congruence.
Elements that are congruent with respect to the ideal have equivalence classes that are equal in the
quotient algebra. The construction is thus a way of imposing relations between vector elements: we
simply choose a set of pairs that we wish to be equal, and put their difference in the generating set ?.
Clarke et al (2010), showed how an inner product can be computed for elements of the quotient
algebra by taking the quotient of a finite dimensional subspace of the ideal and how a treebank could be
used to identify suitable elements to put into the generating set for the ideal in such a way that strings of
different lengths become comparable. Figure 1 shows similarities between adjective phrases computed
using vectors derived from the corpus in figure 2. The construction allows many properties of the tensor
product to carry over into the quotient algebra, for example the similarity of red book to red apple is the
same as the similarity of book to apple, as we would expect from the tensor product. Unlike the tensor
product, strings of different length are comparable, so for example, the similarity of apple to red apple
is non-zero. The benefit of using quotient algebras for compositional distributional semantics lies in
this ability to extend the favourable properties of the tensor product by imposing linguistically plausible
relations between vectors.
326
3 Learning Finite-dimensional Algebras
Quotient algebras are useful constructions when we have a small number of relations which we wish
to impose on the tensor algebra. In highly lexicalised grammars, the number of relations we wish to
impose may become so large that the ideal generates the whole vector space, and is thus useless, since
the resulting quotient space will be trivial. An alternative to this is to restrict the space of exploration to
finite-dimensional algebras. In this case, we can explore the space of possible products in relation to the
set of relations we wish to hold; in other words, we can view this as an optimisation problem in which
we want to find the best possible product given the required relations.
We apply this to the situation where we obtain a vector x? for each individual word and pair of
words in sequence. We then find the product that best fits these observed vectors. Given a set W =
{w1, w2 . . . wm} of words, we want to define a product  to minimise the difference between w?i  w?j
and w?iwj , for 1 ? i, j ? m. Specifically, we can define this as minimising
?
i,j
?w?iwj ? w?i  w?j?
If word vectors have n dimensions, then  is defined by an n3 dimensional vector, which we denote frst
for 1 ? r, s, t ? n, where (er  es)t = frst and e is the vector with 1 in every component, and vt is the
tth component of v.
We can view this as a linear model:
(w?iwj)t = ijt +
n?
r,s=1
(w?i)r(w?j)sfrst
where we have m2 statistical units to learn n2 parameters relating to the tth component of the vector
space. Since these parameters are independent for each value of t, each set of n2 parameters can be learnt
in parallel. We are currently exploring ways of learning these parameters. The form of the equation
above suggests the use of least squares, and we have performed some experiments using this method
using a corpus extracted from the ukWaC corpus (Ferraresi et al, 2008). We extracted a list of verb
adjective?noun sequences, and used latent semantic analysis (Deerwester et al, 1990) to generate n-
dimensional vectors for the 160 most common adjectives and nouns, and pairs of these adjectives and
nouns. Our initial results indicate that the learnt parameters tend to get very large when using least
squares to find the parameters, leading to poor results; we plan to investigate other methods such as
linear optimisation.
Guevara (2010) proposed a related method of learning composition which used linear regression to
learn how components compose. His model is however much more restrictive than ours in that the value
of a component in the product depends only on that same component in the composed vectors, whereas
in our model, the value of the component can depend on all components in the composed vectors.
Baroni and Zamparelli (2010) took a similar approach, in which adjectives are modelled as matrices
acting on the space of nouns, and the matrices are learnt using least squares regression. The algebra
products we propose learning are more general than matrix products; in addition we do not need to
distinguish between words which are represented as matrices and words which are represented as vectors.
4 Constructing Algebras from Semigroups
Whilst the previous two techniques we have discussed are very general, and allow corpus data to be easily
incorporated into the composition definition, our implementations are currently a long way from being
able to represent the complexities of natural language semantics that is currently possible with logical
semantics. This has become the standard method of representing natural language meaning, originating
in the work of Montague (1973), however there is currently no way to incorporate statistical features of
meaning that are described by the distributional hypothesis of Harris (1968).
327
Term Context vector
fish (0, 0, 1)
big (1, 2, 0)
Figure 3: Example context vectors for terms.
ni = (N , ?x nouni(x))
ai = (N /N , ?p?y adji(y) ? p.y)
Figure 4: Equations describing syntax and semantics
of adjectives and nouns.
In related work, Clark et al (2008) described a method of composing meanings which they noted
was a generalisation of Montague semantics. However, their version of Montague semantics assumed a
particular model, and thus effectively mapped sentences to truth values. This omits much of the power
of Montague semantics in which sentences are mapped to logical forms which then provide restrictions
on the set of allowable models, allowing, for example, entailments to be computed between sentences.
We will sketch a method by which Montague semantics can be described within the context-theoretic
framework. We follow a standard method of representing logic in language, but instead of representing
words using logic, we represent an individual dimension of meaning of a word by a logical form ? we
call this dimension a ?aspect?. The general scheme is to represent aspects as elements of a semigroup,
from which we form an algebra. Words are then represented as weighted sums over individual aspects.
We define a set S of all aspects as the set of pairs (s, ?), where s is the syntactic type of an aspect
(for example in the Lambek calculus) and ? is the semantics of the aspect (for example described in
the lambda calculus). We can extend S by defining a product on such pairs reducing each element to a
normal form. This defines a semigroup: the Lambek calculus can be described in terms of a residuated
lattice, which is a partially ordered semigroup (Lambek, 1958), and the lambda calculus is equivalent to a
Cartesian closed category under ?-equivalence (Lambek, 1985), which can be considered as a semigroup
with additional structure.
Given any semigroup S we can construct an algebra L1(S) of real-valued functions on S which are
finite under the L1 norm with multiplication defined by convolution:
(u ? v)(x) =
?
y,z?S:yz=x
u(y)v(z).
For example, suppose we have context vectors for the terms big and fish as described in Figure
3. We represent the syntax and semantics of adjectives and nouns by elements ai and ni respectively
of a semigroup S (Figure 4), where we assume equivalence under ?-reduction is accounted for. The
predicates adji and nounj correspond to aspects, in this case each dimension i of the three dimensions
in the context vectors has a corresponding adji and nouni. We may then represent the vectors for these
terms as elements of the algebra b?ig = a1 + 2a2 and ? = n3, where we equate an element u of the
semigroup with the function in the algebra L1(S) which maps u to 1 and every other element to zero.
Then b?ig ? = a1n3 + 2a2n3, where
ainj = (N,?x(nounj(x) ? adji(x))).
Note that the elements ai form a commutative, idempotent subsemigroup of S, so they have a semilattice
structure. In order for this structure to carry over to the vector structure in the algebra, we would need
a more sophisticated construction, such as a C? enveloping algebra; we leave the investigation of this
possibility to further work.
5 Discussion
We have presented our initial investigations into the application of three powerful methods of construct-
ing algebras to representing natural language semantics. Each of these approaches has potential use in
representing meaning; here we have only touched the surface of what is possible with each technique. We
328
hope that with further work, these methods will lead to a true synthesis between logical and distributional
approaches to natural language semantics.
6 Acknowledgments
We are grateful to Peter Hines, Stephen Clark and Peter Lane for useful discussions. The first author also
wishes to thank Metrica for supporting this research.
References
Baroni, M. and R. Zamparelli (2010). Nouns are vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP 2010), East Stroudsburg PA: ACL, pp. 1183?1193.
Clark, S., B. Coecke, and M. Sadrzadeh (2008). A compositional distributional model of meaning. In
Proceedings of the Second Quantum Interaction Symposium (QI-2008), Oxford, UK, pp. 133?140.
Clark, S. and S. Pulman (2007). Combining symbolic and distributional models of meaning. In Proceed-
ings of the AAAI Spring Symposium on Quantum Interaction, Stanford, CA, pp. 52?55.
Clarke, D. (2007). Context-theoretic Semantics for Natural Language: an Algebraic Framework. Ph. D.
thesis, Department of Informatics, University of Sussex.
Clarke, D., R. Lutz, and D. Weir (2010, July). Semantic composition with quotient algebras. In Proceed-
ings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, Uppsala, Sweden,
pp. 38?44. Association for Computational Linguistics.
Deerwester, S., S. Dumais, G. Furnas, T. Landauer, and R. Harshman (1990). Indexing by latent semantic
analysis. Journal of the American Society for Information Science 41(6), 391?407.
Ferraresi, A., E. Zanchetta, M. Baroni, and S. Bernardini (2008). Introducing and evaluating ukwac, a
very large web-derived corpus of english. In Workshop Programme, pp. 47.
Guevara, E. (2010). A Regression Model of Adjective-Noun Compositionality in Distributional Seman-
tics. ACL 2010, 33.
Harris, Z. (1968). Mathematical Structures of Language. Wiley, New York.
Lambek, J. (1958). The mathematics of sentence structure. American Mathematical Monthly 65, 154?
169.
Lambek, J. (1985, May). Cartesian closed categories and typed lambda-calculi. In G. Cousineau, P.-L.
Curien, and B. Robinet (Eds.), Combinators and Functional Programming Languages, Lecture Notes
in Computer Science. Springer-Verlag.
Montague, R. (1973). The proper treatment of quantification in ordinary English. Dordrecht, Holland:
D. Reidel Publishing Co.
Smolensky, P. (1990, November). Tensor product variable binding and the representation of symbolic
structures in connectionist systems. Artificial Intelligence 46(1-2), 159?216.
Widdows, D. (2008). Semantic vector products: Some initial investigations. In Proceedings of the
Second Symposium on Quantum Interaction, Oxford, UK.
329
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 36?42,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Language Technology for Agile Social Media Science
Simon Wibberley
Department of Informatics
University of Sussex
sw206@susx.ac.uk
Jeremy Reffin
Department of Informatics
University of Sussex
j.p.reffin@susx.ac.uk
David Weir
Department of Informatics
University of Sussex
davidw@susx.ac.uk
Abstract
We present an extension of the DUALIST
tool that enables social scientists to engage
directly with large Twitter datasets. Our
approach supports collaborative construc-
tion of classifiers and associated gold stan-
dard data sets. The tool can be used to
build classifier cascades that decomposes
tweet streams, and provide analysis of tar-
geted conversations. A central concern is
to provide an environment in which social
science researchers can rapidly develop an
informed sense of what the datasets look
like. The intent is that they develop, not
only an informed view as to how the data
could be fruitfully analysed, but also how
feasible it is to analyse it in that way.
1 Introduction
In recent years, automatic social media analysis
(SMA) has emerged, not only as a major focus
of attention within the academic NLP community,
but as an area that is of increasing interest to a va-
riety of business and public sectors organisations.
Among the many social media platforms in use to-
day, the one that has received the most attention is
Twitter, the second most popular social media net-
work in the world with over 400 million tweets
sent each day. The popularity of Twitter as a tar-
get of SMA derives from both the public nature
of tweets, and the availability of the Twitter API
which provides a variety of flexible methods for
scraping tweets from the live Twitter stream.
A plethora of social media monitoring plat-
forms now exist, that are mostly concerned with
providing product marketing oriented services1.
For example, brand monitoring services seek to
provide companies with an understanding of what
1http://wiki.kenburbary.com/social-media-monitoring-
wiki lists 230 Social Media Monitoring Solutions
is being said about their brands and products,
with language processing technology being used
to capture relevant comments or conversations and
apply some form of sentiment analysis (SA), in or-
der to derive insights into what is being said. This
paper forms part of a growing body of work that
is attempting to broaden the scope of SMA be-
yond the realm of product marketing, and into ar-
eas of concern to social scientists (Carvalho et al,
2011; Diakopoulos and Shamma, 2010; Gonzalez-
Bailon et al, 2010; Marchetti-Bowick and Cham-
bers, 2012; O?Connor et al, 2010; Tumasjan et al,
2011; Tumasjan et al, 2010).
Social media presents an enormous opportunity
for the social science research community, consti-
tuting a window into what large numbers of people
are talking. There are, however, significant obsta-
cles facing social scientists interested in making
use of big social media datasets, and it is important
for the NLP research community to gain a better
understanding as to how language technology can
support such explorations.
A key requirement, and the focus of this paper,
is agility: the social scientist needs to be able to
engage with the data in a way that supports an it-
erative process, homing in on a way of analysing
the data that is likely to produce valuable insight.
Given what is typically a rather broad topic as a
starting point, there is a need to see what issues re-
lated to that topic are being discussed and to what
extent. It can be important to get a feeling for the
kind of language being used in these discussions,
and there is a need to rapidly assess the accuracy
of the automated decision making. There is little
value in developing an analysis of the data on an
approach that relies on the technology making de-
cisions that are so nuanced that the method being
used is highly unreliable. As the answers to these
questions are being exposed, insights emerge from
the data, and it becomes possible for the social sci-
entist to progressively refine the topics that are be-
36
ing targetted, and ultimately create a way of au-
tomatically analysing the data that is likely to be
insightful.
Supporting this agile methodology presents se-
vere challenges from an NLP perspective, where
the predominant approaches use classifiers that
involve supervised machine learning. The need
for substantial quantities of training data, and
the detrimental impact on performance that re-
sults when applying them to ?out-of-domain? data
mean that exisiting approaches cannot support the
agility that is so important when social scientists
engage with big social media datasets.
We describe a tool being developed in collab-
oration with a team of social scientists to support
this agile methodology. We have built a frame-
work based on DUALIST, an active learning tool
for building classifiers (Settles, 2011; Settles and
Zhu, 2012). This framework provides a way for
a group of social scientists to collaboratively en-
gage with a stream of tweets, with a goal of con-
structing a chain (or cascade) of automatic docu-
ment classification layers that isolate and analyse
targeted conversions on Twitter. Section 4 dis-
cusses ways in which the design of our frame-
work is intended to support the agile methodol-
ogy mentioned above, with particular emphasis on
the value of DUALIST?s active learning approach,
and the crucial role of the collaborative gold stan-
dard and model building activities. Section 4.3
discusses additional data processing step that have
been introduced to increase the frameworks use-
fulness, and section 5 introduces some projects to
which the framework is being applied.
2 Related Work
Work that focuses on addressing sociological
questions with SMA broadly fall into one of three
categories.
? Approaches that employ automatic data analy-
sis without tailoring the analysis to the specifics of
the situation e.g. (Tumasjan et al, 2010; Tumas-
jan et al, 2011; O?Connor et al, 2010; Gonzalez-
Bailon et al, 2010; Sang and Bos, 2012; Bollen
et al, 2011). This body of research involves lit-
tle or no manual inspection of the data. An an-
alytical technique is selected a-priori, applied to
the SM stream, and the results from that analy-
sis are then aligned with a real-world phenomenon
in order to draw predictive or correlative conclu-
sions about social media. A typical approach is
to predict election outcomes by counting mentions
of political parties and/or politicians as ?votes? in
various ways. Further content analysis is then
overlaid, such as sentiment or mood anlysis, in
an attempt to improve performance. However the
generic language-analysis techniques that are ap-
plied lead to little or no gain, often causing ad-
justments to target question to something with less
strict assessment criteria, such as poll trend instead
of election outcome (Tumasjan et al, 2010; Tu-
masjan et al, 2011; O?Connor et al, 2010; Sang
and Bos, 2012). This research has been criticised
for applying out-of-domain techniques in a ?black
box? fashion, and questions have been raised as
to how sensitive the results are to parameters cho-
sen (Gayo-Avello, 2012; Jungherr et al, 2012).
? Approaches that employ manual analysis of
the data by researchers with a tailored analyti-
cal approach (Bermingham and Smeaton, 2011;
Castillo et al, 2011).This approach reflects tra-
ditional research methods in the social sciences.
Through manual annotation effort, researchers en-
gage closely with the data in a manual but in-
teractive fashion, and this effort enables them to
uncover patterns in the data and make inferences
as to how SM was being used in the context of
the sociocultural phenomena under investigation.
This research suffers form either being restricted
to fairly small datasets.
? Approaches that employ tailored automatic
data analysis, using a supervised machine-learning
approach(Carvalho et al, 2011; Papacharissi and
de Fatima Oliveira, 2012; Meraz and Papacharissi,
2013; Hopkins and King, 2010). This research in-
fers properties of the SM data using statistics from
their bespoke machine learning analysis. Mannual
annotation effort is required to train the classifiers
and is typically applied in a batch process at the
commencement of the investigation.
Our work aims to expand this last category, im-
proving the quality of research by capturing more
of the insight-provoking engagement with the data
seen in more traditional research.
3 DUALIST
Our approach is built around DUALIST (Settles,
2011; Settles and Zhu, 2012), an open-source
project designed to enable non-technical analysts
to build machine-learning classifiers by annotat-
ing documents with just a few minutes of effort.
37
In Section 4, we discuss various ways in which
we have extended DUALIST, including function-
ality allowing multiple annotators to work in par-
allel; incorporating functionality to create ?gold-
standard? test sets and measure inter-annotator
agreement; and supporting on-going performance
evaluation against the gold standard during the
process of building a classifier. DUALIST pro-
vides a graphical interface with which an annota-
tor is able to build a Na??ve Bayes? classifier given
a collection of unlabelled documents. During the
process of building a classifier, the annotator is
presented with a selection of documents (in our
case tweets) that he/she has an opportunity to la-
bel (with one of the class labels), and, for each
class, a selection of features (tokens) that the an-
notator has an opportunity to mark as being strong
features for that class.
Active learning is used to select both the docu-
ments and the features being presented for annota-
tion. Documents are selected on the basis of those
that the current model is most uncertain about
(as measured by posterior class entropy), and fea-
tures are selected for a given class on the basis
of those with highest information gain occurring
frequently with that class. After a batch of docu-
ments and features have been annotated, a revised
model is built using both the labelled data and the
current model?s predictions for the remaining un-
labelled data, through the use of the Expectation-
Maximization algorithm. This new model is then
used as the basis for selecting the set of documents
and features that will be presented to the annotator
for the next iteration of the model building pro-
cess. Full details can be found in Settles (2011).
The upshot of this is two-fold: not only can a
reasonable model be rapidly created, but the re-
searcher is exposed to an interesting non-uniform
sample of the training data. Examples that are rel-
atively easy for the model to classify, i.e. those
with low entropy, are ranked lower in the list of
unlabelled data awaiting annotation. The effect of
this is that the training process facilitates a form of
data exploration that exposes the user to the hard-
est border cases.
4 Extending DUALIST for Social Media
Science Research
This section describes ways in which we have ex-
tended DUALIST to provide an integrated data ex-
ploration tool for social scientists. As outlined in
the introduction, our vision is that a team of social
scientists will be able to use this tool to collabora-
tively work towards the construction of a cascade
of automatic document classification layers that
carve up an incoming Twitter data stream in order
to pick out one or more targeted ?conversations?,
and provide an analysis of what is being discussed
in each of these ?conversations?. In what follows,
we refer to the social scientists as the researchers
and the activity during which the researchers are
working towards delivering a useful classifier cas-
cade as data engagement.
4.1 Facilitating data engagement
When embarking on the process of building one
of the classifiers in the cascade, researchers bring
preconceptions as to the basis for the classifica-
tion. It is only when engaging with the data that
it becomes possible to develop an adequate clas-
sification policy. For example, when looking for
tweets that express some attitude about a targeted
issue, one needs a policy as to how a tweet that
shares a link to an opinion piece on that topic
without any further comment should be classified.
There are a number of ways in which we support
the classification policy development process.
? One of the impacts of the active learning ap-
proach adopted in DUALIST is that by presenting
tweets that the current model is most unsure of,
DUALIST will very rapidly expose issues around
how to make decisions on boundary cases.
? We have extended DUALIST to allow multi-
ple researchers to build a classifier concurrently.
In addition to reducing the time it takes to build
classifiers, this fosters a collaborative approach to
classification policy development.
? We have added functionality that allows for the
collaborative construction of gold standard data
sets. Not only does this provide feedback dur-
ing the model building process as to when perfor-
mance begins to plateau, but, as a gold standard
is being built, researchers are shown the current
inter-annotator agreement score, and are shown
examples of tweets where there is disagreement
among annotators. This constitutes yet another
way in which researchers are confronted with the
most problematic examples.
4.2 Building classifier cascades
Having considered issues that relate to the con-
struction of an individual classifier, we end this
38
section by briefly considering issues relating to
the classifier cascade. The Twitter API provides
basic boolean search functionality that is used to
scrape the Twitter stream, producing the input to
the cascade. A typical strategy is to select query
terms for the boolean search with a view to achiev-
ing a reasonably high recall of relevant tweets2.
An effective choice of query terms that actually
achieves this is one of the things that is not well
understood in advance, but which we expect to
emerge during the data engagement phase. Cap-
turing an input stream that contains a sufficiently
large proportion of interesting (relevant) tweets is
usually achieved at the expense of precision (the
proportion of tweets in the stream being scraped
that are relevant). As a result, the first task that is
typically undertaken during the data engagement
phase involves building a relevancy classifier, to
be deployed at the top of the classifier cascade,
that is designed to filter out irrelevant tweets from
the stream of tweets being scraped.
When building the relevancy classifier, the re-
searchers begin to see how well their preconcep-
tions match the reality of the data stream. It is only
through the process of building this classifier that
the researchers begin to get a feel for the compo-
sition of the relevant data stream. This drives the
researcher?s conception as to how best to divide
up the stream into useful sub-streams, and, as a
result, provides the first insights into an appropri-
ate cascade architecture. Our experience is that in
many cases, classifiers at upper levels of the cas-
cade are involved in decomposing data streams in
useful ways, and classifiers that are lower down
in the cascade are designed to measure some facet
(e.g. sentiment polarity) of the material on some
particular sub-stream.
4.3 Tools for Data Analysis
As social scientists are starting to engage with
real-world data using this framework, it has
emerged that certain patterns of downstream data
analysis are of particular use.
Time series analysis. For many social phenom-
ena, the timing and sequence of social media mes-
sages are of critical importance, particularly for a
platform such as Twitter. Our framework supports
tweet volume analysis across any time frame, al-
2In many cases it is very hard to estimate recall since there
is no way to estimate accurately the volume of relevant tweets
in the full Twitter stream.
lowing researchers to review changes over time
in any classifier?s input or output tweet flows
(classes). This extends the common approach of
sentiment tracking over time to tracking over time
any attitudinal (or other) response whose essen-
tial features can be captured by a classifier of this
kind. These class-volume-by-time-interval plots
can provide insight into how and when the stream
changes in response to external events.
Link analysis. It is becoming apparent that link
sharing (attaching a URL to a tweet, typically
pointing to a media story) is an important aspect of
how information propagates through social media,
particularly on Twitter. For example, the mean-
ing of a tweet can sometimes only be discerned by
inspecting the link to which it points. We are in-
troducing to the framework automatic expansion
of shortened URLs and the ability to inspect link
URL contents, allowing researchers to interpret
tweets more rapidly and accurately. A combina-
tion of link analysis with time series analysis is
also providing researchers with insights into how
mainstream media stories propagate through soci-
ety and shape opinion in the social media age.
Language use analysis. Once a classifier has
been initially established, the framework analyses
the language employed in the input tweets using
an information gain (IG) measure. High IG fea-
tures are those that have occurrence distributions
that closely align the document classification dis-
tributions; essentially they are highly indicative of
the class. This information is proving useful to so-
cial science researchers for three purposes. First,
it helps identify the words and phrases people em-
ploy to convey a particular attitude or opinion in
the domain of interest. Second, it can provide in-
formation on how the language employed shifts
over time, for example as new topics are intro-
duced or external events occur. Third, it can be
used to select candidate keywords with which to
augment the stream?s boolean scraper query. In
this last case, however, we need to augment the
analysis; many high IG terms make poor scraper
terms because they are poorly selective in the more
general case (i.e. outside of the context of the ex-
isting query-selected sample). We take a sample
using the candidate term alone with the search API
and estimate the relevancy precision of the scraped
tweet sample by passing the tweets through the
first-level relevancy classifier. The precision of the
39
new candidate term can be compared to the preci-
sion of existing terms and a decision made.
5 Applications and Extensions
The framework?s flexibility enables it to be applied
to any task that can be broken down into a series of
classification decisions, or indeed where this ap-
proach materially assists the social scientist in ad-
dressing the issue at hand. In order to explore its
application, our framework is being applied to a
variety of tasks:
Identifying patterns of usage. People use the
same language for different purposes; the frame-
work is proving to be a valuable tool for eluci-
dating these usage patterns and for isolating data
sets that illustrate these patterns. As an example,
the authors (in collaboration with a team of so-
cial scientists) are studying the differing ways in
which people employ ethnically and racially sensi-
tive language in conversations on-line. The frame-
work has helped to reveal and isolate a number of
distinct patterns of usage.
Tracking changes in opinion over time. Sen-
timent classifiers trained in one domain perform
poorly when applied to another domain, even
when the domains are apparently closely related
(Pang and Lee, 2008). Traditionally, this has
forced a choice between building bespoke clas-
sifiers (at significant cost), or using generic sen-
timent classifiers (which sacrifice performance).
The ability to rapidly construct sentiment classi-
fiers that are specifically tuned to the precise do-
main can significantly increase classifier perfor-
mance without imposing major additional costs.
Moving beyond sentiment, with these bespoke
classifiers it is in principle possible to track over
time any form of opinion that is reflected in lan-
guage. In a second study, the authors are (in col-
laboration with a team of social scientists) build-
ing cascades of bespoke classifiers to investigate
shifts in citizens? attitudes over time (as expressed
in social media) to a range of political and social
issues arising across the European Union.
Entity disambiguation. References to individ-
uals are often ambiguous. In the general case,
word sense disambiguation is most success-
fully performed by supervised-learning classifiers
(Ma`rquez et al, 2006), and the low cost of pro-
ducing classifiers using this framework makes this
approach practical for situations where we require
repeated high recall, high precision searches of
large data sets for a specific entity. As an example,
this approach is being employed in the EU attitu-
dinal survey study.
Repeated complex search. In situations where
a fixed but complex search needs to be performed
repeatedly over a relatively long period of time,
then a supervised-learning classifier can be ex-
pected both to produce the best results and to be
cost-effective in terms of the effort required to
train it. The authors have employed this approach
in a commercial environment (Lyra et al, 2012),
and the ability to train classifiers more quickly
with this framework reduces the cost still further
and makes this a practical approach in a wider
range of circumstances.
With regard to extension of the framework, we
have identified a number of avenues for expansion
and improvement that will significantly increase
its usefulness and applicability to real-world sce-
narios, and we have recently commenced an 18-
month research programme to formalise and ex-
tend the framework and its associated methodol-
ogy for use in social science research3.
Conclusions and Future Work
We describe an agile analysis framework built
around the DUALIST tool designed to support ef-
fective exploration of large twitter data sets by
social scientists. The functionality of DUAL-
IST has been extended to allow the scraping of
tweets through access to the Twitter API, collab-
orative construction of both gold standard data
sets and Na??ve Bayes? classifiers, an Information
Gain-based method for automatic discovery of
new search terms, and support for the construction
of classifier cascades. Further extensions currently
under development include grouping tweets into
threads conversations, and automatic clustering of
relevant tweets in order to discover subtopics un-
der discussion.
Acknowledgments
We are grateful to our collaborators at the Cen-
tre for the Analysis of social media, Jamie Bartlett
and Carl Miller for valuable contributions to this
work. We thank the anonymous reviewers for their
helpful comments. This work was partially sup-
ported by the Open Society Foundation.
3Towards a Social Media Science, funded by the UK
ESRC National Centre for Research Methods.
40
References
[Bermingham and Smeaton2011] Adam Bermingham
and Alan F Smeaton. 2011. On using Twitter to
monitor political sentiment and predict election
results. In Proceedings of the Workshop on Senti-
ment Analysis where AI meets Psychology (SAAIP),
IJCNLP 2011, pages 2?10.
[Bollen et al2011] Johan Bollen, Alberto Pepe, and
Huina Mao. 2011. Modeling public mood and emo-
tion: Twitter sentiment and socio-economic phe-
nomena. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media,
pages 450?453.
[Carvalho et al2011] Paula Carvalho, Lu??s Sarmento,
Jorge Teixeira, and Ma?rio J. Silva. 2011. Liars and
saviors in a sentiment annotated corpus of comments
to political debates. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, pages 564?568, Stroudsburg, PA,
USA.
[Castillo et al2011] Carlos Castillo, Marcelo Mendoza,
and Barbara Poblete. 2011. Information credibility
on Twitter. In Proceedings of the 20th International
Conference on World wide web, pages 675?684.
[Diakopoulos and Shamma2010] Nicholas A Di-
akopoulos and David A Shamma. 2010. Charac-
terizing debate performance via aggregated Twitter
sentiment. In Proceedings of the 28th international
conference on Human factors in computing systems,
pages 1195?1198.
[Gayo-Avello2012] Daniel Gayo-Avello. 2012. I
wanted to predict elections with twitter and all i
got was this lousy paper a balanced survey on elec-
tion prediction using Twitter data. arXiv preprint
arXiv:1204.6441.
[Gonzalez-Bailon et al2010] Sandra Gonzalez-Bailon,
Rafael E Banchs, and Andreas Kaltenbrunner. 2010.
Emotional reactions and the pulse of public opin-
ion: Measuring the impact of political events on
the sentiment of online discussions. arXiv preprint
arXiv:1009.4019.
[Hopkins and King2010] Daniel J. Hopkins and Gary
King. 2010. A method of automated nonparametric
content analysis for social science. American Jour-
nal of Political Science, 54(1):229?247.
[Jungherr et al2012] Andreas Jungherr, Pascal Ju?rgens,
and Harald Schoen. 2012. Why the Pirate Party
won the German election of 2009 or the trouble
with predictions: A response to Tumasjan, Sprenger,
Sander, & Welpe. Social Science Computer Review,
30(2):229?234.
[Lyra et al2012] Matti Lyra, Daoud Clarke, Hamish
Morgan, Jeremy Reffin, and David Weir. 2012.
Challenges in applying machine learning to media
monitoring. In Proceedings of Thirty-second SGAI
International Conference on Artificial Intelligence
(AI-2012).
[Marchetti-Bowick and Chambers2012] Micol
Marchetti-Bowick and Nathanael Chambers.
2012. Learning for microblogs with distant su-
pervision: political forecasting with Twitter. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 603?612.
[Ma`rquez et al2006] Llu??s Ma`rquez, Gerard Escudero,
David Mart??nez, and German Rigau. 2006. Su-
pervised corpus-based methods for wsd. In Eneko
Agirre and Philip Edmonds, editors, Word Sense
Disambiguation, volume 33 of Text, Speech and
Language Technology, pages 167?216. Springer
Netherlands.
[Meraz and Papacharissi2013] Sharon Meraz and Zizi
Papacharissi. 2013. Networked gatekeeping and
networked framing on #egypt. The International
Journal of Press/Politics, 18(2):138?166.
[O?Connor et al2010] Brendan O?Connor, Ramnath
Balasubramanyan, Bryan R Routledge, and Noah A
Smith. 2010. From tweets to polls: Linking text
sentiment to public opinion time series. In Proceed-
ings of the International AAAI Conference on We-
blogs and Social Media, pages 122?129.
[Pang and Lee2008] Bo Pang and Lillian Lee. 2008.
Opinion mining and sentiment analysis. Founda-
tions and trends in Information Retrieval, 2(1-2):1?
135.
[Papacharissi and de Fatima Oliveira2012] Zizi Pa-
pacharissi and Maria de Fatima Oliveira. 2012.
Affective news and networked publics: the rhythms
of news storytelling on #egypt. Journal of Commu-
nication, 62(2):266?282.
[Sang and Bos2012] Erik Tjong Kim Sang and Johan
Bos. 2012. Predicting the 2011 dutch senate elec-
tion results with Twitter. Proceedings of the Euro-
pean Chapter of the Association for Computational
Linguistics 2012, page 53.
[Settles and Zhu2012] Burr Settles and Xiaojin Zhu.
2012. Behavioral factors in interactive training of
text classifiers. In Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 563?567.
[Settles2011] Burr Settles. 2011. Closing the loop:
Fast, interactive semi-supervised annotation with
queries on features and instances. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 1467?1478.
[Tumasjan et al2010] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2010. Predicting elections with Twitter: What 140
characters reveal about political sentiment. In Pro-
ceedings of the fourth international AAAI confer-
ence on weblogs and social media, pages 178?185.
41
[Tumasjan et al2011] Andranik Tumasjan, Timm O
Sprenger, Philipp G Sandner, and Isabell M Welpe.
2011. Election forecasts with Twitter how 140 char-
acters reflect the political landscape. Social Science
Computer Review, 29(4):402?418.
42
Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11?20,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Distributional Composition using Higher-Order Dependency Vectors
Julie Weeds, David Weir and Jeremy Reffin
Department of Informatics
University of Sussex
Brighton, BN1 9QH, UK
{J.E.Weeds, D.J.Weir, J.P.Reffin}@sussex.ac.uk
Abstract
This paper concerns how to apply compo-
sitional methods to vectors based on gram-
matical dependency relation vectors. We
demonstrate the potential of a novel ap-
proach which uses higher-order grammat-
ical dependency relations as features. We
apply the approach to adjective-noun com-
pounds with promising results in the pre-
diction of the vectors for (held-out) ob-
served phrases.
1 Introduction
Vector space models of semantics characterise the
meaning of a word in terms of distributional fea-
tures derived from word co-occurrences. The most
widely adopted basis for word co-occurrence is
proximity, i.e. that two words (or more generally
lexemes) are taken to co-occur when they occur
together within a certain sized window, or within
the same sentence, paragraph, or document. Lin
(1998), in contrast, took the syntactic relationship
between co-occurring words into account: the dis-
tributional features of a word are based on the
word?s grammatical dependents as found in a de-
pendency parsed corpus. For example, observing
that the word glass appears as the indirect object
of the verb fill, provides evidence that the word
glass has the distributional feature iobj:fill, where
iobj denotes the inverse indirect object grammati-
cal relation. The use of grammatical dependents as
word features has been exploited in the discovery
of tight semantic relations, such as synonymy and
hypernymy, where an evaluation against a gold
standard such as WordNet (Fellbaum, 1998) can
be made (Lin, 1998; Weeds and Weir, 2003; Cur-
ran, 2004).
Pado and Lapata (2007) took this further by
considering not just direct grammatical depen-
dents, but also including indirect dependents.
Thus, observing the sentence She filled her glass
slowly would provide evidence that the word glass
has the distributional feature iobj:advmod:slowly
where iobj:advmod captures the indirect depen-
dency relationship between glass and slowly in the
sentence.
Note that Pado and Lapata (2007) included
a basis mapping function that gave their frame-
work flexibility as to how to map paths such as
iobj:advmod:slowly onto the basis of the vector
space. Indeed, the instantiation of their framework
that they adopt in their experiments uses a ba-
sis mapping function that removes the dependency
path to leave just the word, so iobj:advmod:slowly
would be mapped to slowly.
In this paper, we are concerned with the prob-
lem of distributional semantic composition. We
show that the idea that the distributional seman-
tics of a word can be captured with higher-order
dependency relationships, provides the basis for
a simple approach to compositional distributional
semantics. While our approach is quite gen-
eral, dealing with arbitrarily high-order depen-
dency relationships, and the composition of ar-
bitrary phrases, in this paper we consider only
first and second order dependency relations, and
adjective-noun composition.
In Section 2, we illustrate our proposal by
showing how second order dependency relations
can play a role in computing the semantics of
adjective-noun composition. In Section 3 we de-
scribe a number of experiments that are intended
to evaluate the approach, with the results presented
in Section 4.
The basis for our evaluation follows Baroni and
11
Zamparelli (2010) and Guevara (2010). Typically,
compositional distributional semantic models can
be used to generate an (inferred) distributional
vector for a phrase from the (observed) distribu-
tional vectors of the phrase?s constituents. One
of the motivations for doing this is that the ob-
served distributional vectors for most phrases tend
to be very sparse, a consequence of the frequency
with which typical phrases occur in even large cor-
pora. However, there are phrases that occur suffi-
ciently frequently that a reasonable characterisa-
tion of their meaning can be captured with their
observed distributional vector. Such phrases can
be exploited in order to assess the quality of a
model of composition. This is achieved by mea-
suring the distributional similarity of the observed
and inferred distributional vectors for these high
frequency phrases.
The contributions of this paper are as follows.
We propose a novel approach to phrasal composi-
tion which uses higher order grammatical depen-
dency relations as features. We demonstrate its
potential in the context of adjective-noun compo-
sition by comparing (held-out) observed and in-
ferred phrasal vectors. Further, we compare dif-
ferent vector operations, different feature associa-
tion scores and investigate the effect of weighting
features before or after composition.
2 Composition with Higher-order
Dependencies
Consider the problem of adjective-noun compo-
sition. For example, what is the meaning of the
phrase small child? How does it relate to the
meanings of the lexemes small and child? Figure 1
shows a dependency analysis for the sentence The
very small wet child cried loudly. Tables 1 and
2 show the grammatical dependencies (with other
open-class words) for the lexemes small and child
which would be extracted from it.
the/D very/R small/J wet/J child/N cry/V loudly/R
amod
amod
advmod
det
nsubj advmod
Figure 1: Example Dependency Tree
From Table 1 we see what kinds of (higher-
order) dependency paths appear in the distribu-
tional features of adjectives such as small. Simi-
larly, Table 2 indicates this for nouns such as child.
1st-order advmod:very/R
amod:child
2nd-order amod:amod:wet/J
amod:nsubj:cry/V
3rd-order amod:nsubj:advmod:loudly/R
Table 1: Grammatical Dependencies of small
1st-order amod:wet/J
amod:small/J
nsubj:cry/V
2nd-order amod:advmod:very/R
nsubj:advmod:loudly/R
Table 2: Grammatical Dependencies of child
It is clear that with a conventional grammatical
dependency-based approach where only first or-
der dependencies for small and child would be
considered, there will be very little overlap be-
tween the features of nouns and adjectives because
quite different grammatical relations are used in
the two types of vectors, and correspondingly lex-
emes with different parts of speech appear at the
end of these paths.
However, as our example illustrates, it is possi-
ble to align the 2nd-order feature space of adjec-
tives with the 1st-order feature space of nouns. In
this example, we have evidence that children cry
and that small things cry. Consequently, in order
to compose an adjective with a noun, we would
want to align 2nd-order features of the adjective
with 1st-order features of the noun; this gives us a
prediction of the first order features of the noun in
the context of the adjective
1
.
This idea extends in a straightforward way be-
yond adjective-noun composition. For example, it
is possible to align the 3rd order features of ad-
jectives with 2nd order features of nouns, which is
something that would be useful if one wanted to
compose verbs with their arguments. These argu-
ments will include adjective-noun compounds and
therefore adjective-noun compounds require 2nd-
order features which can be aligned with the first
order features of the verbs. This is, however, not
1
Note that it would also be possible to align 2nd-order
features of the noun with 1st-order features of the adjective,
resulting in a prediction of the first order features of the ad-
jective in the context of the noun.
12
something that we will pursue further in this paper.
We now clarify how features vectors are aligned
and then composed. Suppose that the lexemes w
1
and w
2
which we wish to compose are connected
by relation r. Let w
1
be the head of the relation
and w
2
be the dependent. In our example, w
1
is
child, w
2
is small and r is amod. We first pro-
duce a reduced vector for w
2
which is designed
to lie in a comparable feature space as the vector
for w
1
. To do this we take the set of 2nd order
features of w
2
which start with the relation r? and
reduce them to first order features (by removing
the r? at the start of the path). So in our example,
we create a reduced vector for small where fea-
tures amod:nsubj:x for some token x are reduced
to nsubj:x, features amod:amod:x for some token
x are reduced to the feature amod:x, and features
amod:nsubj:advmod:x for some token x are re-
duced to nsubj:advmod:x. Once the vector for w
2
has been reduced, it can be composed with the vec-
tor for w
1
using standard vector operations.
In Section 3 we describe experiments that ex-
plore the effectiveness of this approach to distri-
butional composition by measuring the similarity
of composed vectors with observed vectors for a
set of frequently occurring adjective-noun pairs
(details given below). We evaluate a number of
instantiations of our approach, and in particular,
there are three aspects of the model where alter-
native solutions are available: the choice of which
vector composition operation to use; the choice of
how to weight dependency features; and the ques-
tion as to whether feature weighting should take
place before or after composition.
Vector composition operation. We consider
each of the following seven alternatives: pointwise
addition (add), pointwise multiplication (mult),
pointwise geometric mean
2
(gm), pointwise max-
imum (max), pointwise minimum (min), first ar-
gument (hd), second argument (dp). The latter
two operations simply return the first (respectively
second) of the input vectors.
Feature weighting. We consider three options.
Much work in this area has used positive pointwise
mutual information (PPMI) (Church and Hanks,
1989) to weight the features. However, PPMI is
known to over-emphasise low frequency events,
and as a result there has been a recent shift to-
wards using positive localised mutual information
2
The geometric mean of x and y is
?
(x ? y).
PPMI(x, y) =
{
I(x, y) if I(x, y) > 0
0 otherwise
where I(x, y) = log
P (x,y)
P (x).P (y)
PLMI(x, y) =
{
L(x, y) if L(x, y) > 0
0 otherwise
where L(x, y) = P (x, y).log(
P (x,y)
P (x).P (y)
PNPMI(x, y) =
{
N(x, y) if N(x, y) > 0
0 otherwise
where N(x, y) =
1
?log(P (y)
.log
P (x,y)
P (x).P (y)
Table 3: Feature Association Scores
(PLMI) (Scheible et al., 2013) and positive nor-
malised point wise mutual information (PNPMI)
(Bouma, 2009). For definitions, see Table 3.
Timing of feature weighting. We consider two
alternatives: we can weight features before com-
position so that the composition operation is ap-
plied to weighted vectors, or we can compose vec-
tors prior to feature weighting, in which case the
composition operation is applied to unweighted
vectors, and feature weighting is applied in the
context of making a similarity calculation. In other
work, the former order is often implied. For exam-
ple, Boleda et al. (2013) state that they use ?PMI
to weight the co-occurrence matrix?. However, if
we allow the second order, features which might
have a zero association score in the context of the
the individual lexemes, could be considered sig-
nificant in the context of the phrase.
3 Evaluation
Our experimental evaluation of the approach is
based on the assumption, which is commonly
made elsewhere, that where there is a reasonable
amount of corpus data available for a phrase, this
will generate a good estimate of the vector of the
phrase. It has been shown (Turney, 2012; Baroni
and Zamparelli, 2010) that such ?observed? vec-
tors are indeed reasonable for adjective-noun and
noun-noun compounds. Hence, in order to evalu-
ate the compositional models under consideration
here, we compare observed phrasal vectors with
inferred phrasal vectors, where the comparison is
made using the cosine measure. We note that it is
13
not possible to draw conclusions from the absolute
value of the cosine score since this would favour
models which always assign higher cosine scores.
Hence, we draw conclusions from the change in
cosine score with respect to a baseline within the
same model.
Methodology
For each noun and adjective which occur more
than a threshold number of times in a corpus, we
first extract conventional first order dependency
vectors. The features of these lexemes define the
semantic space, and feature probabilities (for use
in association scores) are calculated from this data.
Given a list of adjective-noun phrases, we ex-
tract first order vectors for the nouns and second
order vectors for the adjectives, which we refer to
as observed constituent vectors. We also extract
first order vectors for the nouns in the context of
the adjective, which we refer to as the observed
phrasal vector.
For each adjective-noun pair, we build bespoke
constituent vectors for the adjective and noun, in
which we remove all counts which arise from co-
occurrences with that specific adjective-noun pair.
It is these constituent vectors that are used as the
basis for inferring the vector for that particular
adjective-noun phrase.
Our rationale for this is as follows. Without this
modification, the observed constituent vectors will
contain co-occurrences which are due to the ob-
served adjective-noun vector co-occurrences. To
see why this is undesirable, suppose that one of the
adjective-noun phrases was small child. We take
the observed vector for small child to be what we
are calling the observed phrasal vector for child (in
the context of small). Suppose that when building
the observed phrasal vector, we observe the phrase
the small child cried. This will lead to a count for
the feature nsubj:cry in the observed phrasal vec-
tor for child.
But if we are not careful, this same phrase will
contribute to counts in the constituent vectors for
small and child, producing counts for the features
amod:nsubj:cry and nsubj:cry, in their respective
vectors. To see why these counts should not be in-
cluded when building the constituent vectors that
we compose to produce inferred vectors for the
adjective-noun phrase small child, consider the
case where all of the evidence for small things be-
ing things that can cry and children being things
that can crying comes from having observed the
phrase small children crying. Despite not having
learnt anything about the composition of small and
child in general, we would be able to infer the cry
feature for the phrase. An adequate model of com-
position should be able to infer this on the basis
that other small things have been seen to cry, and
that non-small children have been seen to cry.
Here, we compare the proposed approach,
based on higher order dependencies, with the
standard method of composing conventional first-
order dependency vectors. The vector operation,
hd provides a baseline for comparison which is
the same in both approaches. This baseline corre-
sponds to a composition model where the first or-
der dependencies of the phrase (i.e. the noun in the
context of the adjective) are taken to be the same
as the first order dependencies of the uncontextu-
alized noun. For example, if we have never seen
the phrase small child before, we would assume
that it means the same as the head word child.
We hypothesise that it is not possible to im-
prove on this baseline using traditional first-order
dependency relation vectors, since the vector for
the modifier does not contain features of the right
type, but that with the proposed approach, the in-
ferred vector for a phrase such as small child will
be closer than observed vector for child to the ob-
served vector for small child. We also ask the re-
lated question of whether our inferred vector for
small child is closer than the constituent vector for
small to the observed vector for small child. This
comparison is achieved through use of the vector
operation dp that ignores the vector for the head,
simply returning a first-order vector derived from
the dependent.
Experimental Settings
Our corpus is a mid-2011 dump of WikiPedia.
This has been part-of-speech tagged, lemmatised
and dependency parsed using the Malt Parser
(Nivre, 2004). All major grammatical dependency
relations involving open class parts of speech
(nsubj, dobj, iobj, conj, amod, advmod, nnmod)
have been extracted for all POS-tagged and lem-
matised nouns and adjectives occurring 100 or
more times. In past work with conventional de-
pendency relation vectors we found that using a
feature threshold of 100, weighting features with
PPMI and a cosine similarity score work well.
For experimental purposes, we have taken
14
spanish british african japanese
modern classical female natural
digital military medical musical
scientific free black white
heavy common small large
strong short long good
similar previous future original
former subsequent next possible
Table 4: Adjectives considered
32 of the most frequently occurring adjectives
(see Table 4). These adjectives include ones
which would generally be considered intersective
(e.g., female), subsective (e.g,, long) and non-
subsective/intensional (e.g., former) (Pustejovsky,
2013) . For all of these adjectives there are at least
100 adjective-noun phrases which occur at least
100 times in the corpus. We randomly selected
50 of the phrases for each adjective. Note that
our proposed method does not require any hyper
parameters to be set during training, nor does it
require a certain number of phrases per adjective.
For the purpose of these experiments we have a list
of 1600 adjective-noun phrases, all of which occur
at least 100 times in WikiPedia.
4 Results and Discussion
Tables 5 and 6 summarise the average cosines for
the proposed higher-order dependency approach
and the conventional first-order dependency ap-
proach, respectively. In each case, we consider
each combination of vector operation, feature as-
sociation score, and composition timing (i.e. be-
fore, or after, vector weighting).
Table 7 shows the average improvement over
the baseline (hd), for each combination of exper-
imental variables, when considering the proposed
higher-order dependency approach. Note that this
is an average of paired differences (and not the dif-
ference of the averages in Table 6). For brevity, we
omit the results for PNPMI here, since there do not
appear to be substantial differences between using
PPMI and PNPMI. To indicate statistical signifi-
cance, we show estimated standard errors in the
means. All differences are statistically significant
(under a paired t-test) except those marked ?.
From Table 5, we see that none of the com-
positional operations on conventional dependency
vectors are able to beat the baseline of selecting
the head vector (hd). This is independent of the
choice of association measure and the order in
which weighting and composition are carried out.
For the higher order dependency vectors (Tables
6 and 7), we note, in contrast, that some com-
positional operations produce large increases in
cosine score compared to the head vector alone
(hd). Table 7 examines the statistical significance
of these differences. We find that for the inter-
sective composition operations (mult, min, and
gm), performance is statistically superior to using
the head alone in all experimental conditions stud-
ied. By contrast, additive measures (add, max)
typically have no impact, or decrease performance
marginally relative to the head alone. An explana-
tion for these significant differences is that inter-
sective vector operations are able to encapsulate
the way that an adjective disambiguates and spe-
cialises the sense of the noun that it is modifying.
We also note that the alternative baseline, dp,
which estimates the features of a phrase to be the
aggregation of all things which are modified by
the adjective, performs significantly worse than
the standard baseline, hd, which estimates the fea-
tures of a phrase to be the features of the head
noun. This is consistent with the intuition that the
distributional vector for small child should more
similar to the vector for child than it is to the vec-
tor for the things that can be small.
Considering the different intersective opera-
tions, mult appears to be the best choice when
the feature association score is PPMI or PNPMI
and gm appears to be the best choice when the fea-
ture association score is PLMI.
Further, PLMI consistently gives all of the vec-
tor pairings higher cosine scores than PPMI. Since
PLMI assigns less weight to low frequency event
and more weight to high frequency events, this
suggests that all of the composition methods, in-
cluding the baseline (hd), do better at predicting
the high frequency co-occurrences. This is not sur-
prising as these will more likely have been seen
with the phrasal constituents in other contexts.
Our final observation, based on Table 6, is that
the best order in which to carry out weighting and
composition appears to depend on the choice of
feature association score. In general, it appears
better to weight the features and then compose
vectors. This is always true when using PNPMI
or PLMI. However, using PPMI, the highest per-
formance is achieved by composing the raw vec-
tors using multiplication and then weighing the
15
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x? s x? s x? s x? s x? s x? s
add 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
max 0.12 (0.06) 0.13 (0.05) 0.15 (0.16) 0.11 (0.05) 0.12 (0.06) 0.22 (0.20)
mult 0.06 (0.05) 0.06 (0.06) 0.06 (0.11) 0.07 (0.05) 0.07 (0.12) 0.07 (0.05)
min 0.05 (0.05) 0.06 (0.05) 0.04 (0.09) 0.05 (0.04) 0.05 (0.04) 0.04 (0.08)
gm 0.06 (0.05) 0.06 (0.05) 0.07 (0.11) 0.05 (0.04) 0.06 (0.04) 0.08 (0.11)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
Table 5: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Con-
ventional First-Order Dependency Based Approach.
weight:compose compose:weight
PPMI PNPMI PLMI PPMI PNPMI PLMI
x? s x? s x? s x? s x? s x? s
add 0.14 (0.06) 0.16 (0.06) 0.29 (0.21) 0.10 (0.04) 0.12 (0.05) 0.29 (0.22)
max 0.10 (0.04) 0.11 (0.04) 0.27 (0.21) 0.10 (0.04) 0.11 (0.04) 0.26 (0.21)
mult 0.30 (0.12) 0.33 (0.12) 0.40 (0.29) 0.34 (0.10) 0.32 (0.10) 0.32 (0.27)
min 0.26 (0.11) 0.27 (0.11) 0.40 (0.24) 0.24 (0.10) 0.25 (0.10) 0.37 (0.23)
gm 0.27 (0.11) 0.29 (0.11) 0.46 (0.20) 0.26 (0.10) 0.27 (0.10) 0.44 (0.22)
dp 0.10 (0.05) 0.10 (0.05) 0.20 (0.20) 0.10 (0.05) 0.10 (0.05) 0.20 (0.20)
hd 0.13 (0.07) 0.15 (0.07) 0.28 (0.22) 0.13 (0.07) 0.15 (0.07) 0.28 (0.22)
Table 6: Means and Standard Deviations for Cosines Between Observed and Predicted Vectors for Pro-
posed Higher-Order Dependency Based Approach
remaining features. This can be explained by
considering the recall and precision of the com-
posed vector?s prediction of the observed vec-
tor. If we compose using gm before weighting
vectors, we increase the recall of the prediction,
but decrease precision. Whether we use PPMI,
PNPMI or PLMI, recall of features increases from
88.8% to 99.5% and precision drops from 5.5% to
4.8%. If we compose using mult before weight-
ing vectors, contrary to expectation, recall de-
creases and precision increases. Whether we use
PPMI, PNPMI or PLMI, recall of features de-
creases from 88.8% to 59.4% but precision in-
creases from 5.5% to 18.9%. Hence, multiplica-
tion of the raw vectors is causing a lot of potential
shared features to be ?lost? when the weighting
is subsequently carried out (since multiplication
stretches out the value space). This leads to an
increase in cosines when PPMI is used for weight-
ing, and a decrease in cosines when PLMI is used.
Hence, it appears that the features being removed
by multiplying the raw vectors before weighting
must be low frequency co-occurrences, which are
not observed with the phrase.
5 Related Work
In this work, we bring together ideas from sev-
eral different strands of distributional semantics:
incorporating syntactic information into the distri-
butional representation of a lexeme; representing
phrasal meaning by creating distributional repre-
sentations through composition; and representing
word meaning in context by modifying the distri-
butional representation of a word.
The use of syntactic structure in distributional
representations is not new. Two of the earliest
proponents of distributional semantics, Lin (1998)
and Lee (1999) used features based on first order
dependency relations between words in their dis-
tributional representations. More recently, Pado
and Lapata (2007) propose a semantic space based
on dependency paths. This model outperformed
traditional word-based models which do not take
syntax into account in a synonymy relation detec-
tion task and a prevalent sense acquisition task.
The problem of representing phrasal meaning
has traditionally been tackled by taking vector rep-
resentations for words (Turney and Pantel, 2010)
and combining them using some function to pro-
16
weight:compose compose:weight
PPMI PLMI PPMI PLMI
x? s
x?
x? s
x?
x? s
x?
x? s
x?
add 0.01 (0.001) ?0.004 (0.003) -0.03 (0.001) ?0.006 (0.004)
max -0.03 (0.001) -0.01 (0.003) -0.04 (0.001) -0.02 (0.003)
mult 0.16 (0.002) 0.11 (0.006) 0.21 (0.002) 0.03 (0.006)
min 0.13 (0.001) 0.11 (0.007) 0.10 (0.001) 0.09 (0.007)
gm 0.14 (0.001) 0.18 (0.005) 0.12 (0.001) 0.16 (0.005)
dp -0.03 (0.002) -0.09 (0.007) -0.04 (0.002) -0.09 (0.007)
Table 7: Means and Standard Errors for Increases in Cosine with respect to the hd Baseline for Proposed
Higher-Order Dependency Based Approach. All differences statistically significant (under a paired t-
test) except those marked ?.
duce a data structure that represents the phrase
or sentence. Mitchell and Lapata (2008, 2010)
found that simple additive and multiplicative func-
tions applied to proximity-based vector represen-
tations were no less effective than more com-
plex functions when performance was assessed
against human similarity judgements of simple
paired phrases.
The simple functions evaluated by Mitchell and
Lapata (2008) are generally acknowledged to have
serious theoretical limitations in their treatment
of composition. How can a commutative func-
tion such as multiplication or addition provide dif-
ferent interpretations for different word orderings
such as window glass and glass window? The
majority of attempts to rectify this have offered
a more complex, non-commutative function ?
such as weighted addition ? or taken the view
that some or all words are no longer simple vec-
tors. For example, in the work of Baroni and
Zamparelli (2010) and Guevara (2010), an adjec-
tive is viewed as a modifying function and rep-
resented by a matrix. Coecke et al. (2011) and
Grefenstette et al. (2013) also incorporate the no-
tion of function application from formal seman-
tics. They derived function application from syn-
tactic structure, representing functions as tensors
and arguments as vectors. The MV-RNN model
of Socher et al. (2012) broadened the Baroni and
Zamparelli (2010) approach; all words, regardless
of part-of-speech, were modelled with both a vec-
tor and a matrix. This approach also shared fea-
tures with Coecke et al. (2011) in using syntax
to guide the order of phrasal composition. These
higher order structures are typically learnt or in-
duced using a supervised machine learning tech-
nique. For example, Baroni and Zamparelli (2010)
learnt their adjectival matrixes by performing re-
gression analysis over pairs of observed nouns and
adjective-noun phrases. As a consequence of the
computational expense of the machine learning
techniques involved, implementations of these ap-
proaches typically require a considerable amount
of dimensionality reduction.
A long-standing topic in distributional seman-
tics has been the modification of a canonical repre-
sentation of a lexeme?s meaning to reflect the con-
text in which it is found. Typically, a canonical
vector for a lexeme is estimated from all corpus
occurrences and the vector then modified to reflect
the instance context (Lund and Burgess, 1996;
Erk and Pad?o, 2008; Mitchell and Lapata, 2008;
Thater et al., 2009; Thater et al., 2010; Thater et
al., 2011; Van de Cruys et al., 2011; Erk, 2012).
As described in Mitchell and Lapata (2008, 2010),
lexeme vectors have typically been modified using
simple additive and multiplicative compositional
functions. Other approaches, however, share with
our proposal the use of syntax to drive modifica-
tion of the distributional representation (Erk and
Pad?o, 2008; Thater et al., 2009; Thater et al., 2010;
Thater et al., 2011). For example, in the SVS rep-
resentation of Erk and Pad?o (2008), a word was
represented by a set of vectors: one which en-
codes its lexical meaning in terms of distribution-
ally similar words
3
, and one which encodes the
selectional preferences of each grammatical rela-
tion it supports. A word?s meaning vector was up-
dated in the context of another word by combining
it with the appropriate selectional preferences vec-
3
These are referred to as second-order vectors using
the terminology of Grefenstette (1994) and Sch?utze (1998).
However, this refers to a second-order affinity between the
words and is not related to the use of grammatical depen-
dency relations.
17
tor of the contextualising word.
Turney (2012) offered a model of phrasal level
similarity which combines assessments of word-
level semantic relations. This work used two
different word-level distributional representations
to encapsulate two types of similarity. Distribu-
tional similarity calculated from proximity-based
features was used to estimate domain similarity
and distributional similarity calculated from syn-
tactic pattern based features is used to estimate
functional similarity. The similarity of a pair of
compound noun phrases was computed as a func-
tion of the similarities of the components. Cru-
cially different from other models of phrasal level
similarity, it does not attempt to derive modified
vectors for phrases or words in context.
6 Conclusions and Further Work
Vectors based on grammatical dependency rela-
tions are known to be useful in the discovery of
tight semantic relations, such as synonymy and
hypernymy, between lexemes (Lin, 1998; Weeds
and Weir, 2003; Curran, 2004). It would be use-
ful to be able to extend these methods to deter-
mine similarity between phrases (of potentially
different lengths). However, conventional ap-
proaches to composition, which have been ap-
plied to proximity-based vectors, cannot sensibly
be used on vectors that are based on grammatical
dependency relations.
In our approach, we consider the vector for a
phrase to be the vector for the head lexeme in
the context of the other phrasal constituents. Like
Pado and Lapata (2007), we extend the concept
of a grammatical dependency relation feature to
include dependency relation paths which incor-
porate higher-order dependencies between words.
We have shown how it is possible to align the de-
pendency path features for words of different syn-
tactic types, and thus produce composed vectors
which predict the features of one constituent in the
context of the other constituent.
In our experiments with AN compounds, we
have shown that these predicted vectors are closer
than the head constituent?s vector to the observed
phrasal vector. We have shown this is true even
when the observed phrase is in fact unobserved,
i.e. when its co-occurrences do not contribute to
the constituents? vectors. Consistent with work us-
ing proximity-based vectors, we have found that
intersective operations perform substantially bet-
ter than additive operations. This can be under-
stood by viewing the intersective operations as en-
capsulating the way that adjectives can specialise
the meaning of the nouns that they modify.
We have investigated the interaction between
the vector operation used for composition, the fea-
ture association score and the timing of applying
feature weights. We have found that multiplication
works best if using PPMI to weight features, but
that geometric mean is better if using the increas-
ingly popular PLMI weighting measure. Whilst
applying an intersective composition operation be-
fore applying feature weighting does allow more
features to be retained in the predicted vector (it
is possible to achieve 99.5% recall), in general,
this does not correspond with an increase in co-
sine scores. In general, the corresponding drop in
precision (i.e., the over-prediction of unobserved
features) causes the cosine to decrease. The one
exception to this is using multiplication with the
PPMI feature weighting score. Here we actually
see a drop in recall, and an increase in precision
due to the nature of multiplication and PPMI.
One assumption that has been made throughout
the work, is that the observed phrasal vector pro-
vides a good estimate of the distributional repre-
sentation of the phrase and, consequently, the best
composition method is the one which returns the
most similar prediction. However, in general, we
notice that while the recall of the compositional
methods is good, the precision is very low. Lack of
precision may be due to the prevalence of plausi-
ble, but unobserved, co-occurrences of the phrase.
Consequently, this introduces uncertainty into the
conclusions which can be drawn from a study such
as this. Further work is required to develop effec-
tive intrinsic and extrinsic evaluations of models
of composition.
A further interesting area of study is whether
distributional models that include higher-order
grammatical dependencies can tell us more about
the lexical semantics of a word than the conven-
tional first-order models, for example by distin-
guishing semantic relations such as synonymy,
antonymy, hypernymy and co-hyponymy.
Acknowledgements
This work was funded by UK EPSRC project
EP/IO37458/1 ?A Unified Model of Composi-
tional and Distributional Compositional Seman-
tics: Theory and Applications?.
18
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing.
Gemma Boleda, Marco Baroni, The Nghia Pham, and
Louise McNally. 2013. Intensionality was only al-
leged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of the 10th Inter-
national Conference on Computational Semantics
(IWCS 2013) ? Long Papers, pages 35?46, Pots-
dam, Germany, March. Association for Computa-
tional Linguistics.
Gerlof Bouma. 2009. Normalised (point wise) mu-
tual information in collocation extraction, from form
to meaning: Processing texts automatically. In Pro-
ceedings of the Biennial International Conference of
the German Society for Computational Linguistics
and Language Technology.
Kenneth Ward Church and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of the 27th Annual Meeting
on Association for Computational Linguistics, ACL
?89, pages 76?83, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2011. Mathematical foundations for a com-
positional distributed model of meaning. Linguistic
Analysis, 36(1-4):345?384.
James Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Katrin Erk and Sebastian Pad?o. 2008. A structured
vector space model for word meaning in context.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
897?906, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635?653.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for compo-
sitional distributional semantics. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013).
Gregory Grefenstette. 1994. Corpus-derived first, sec-
ond and third-order word affinities. In Proceedings
of Euralex 1994.
Emiliano Guevara. 2010. A Regression Model of
Adjective-Noun Compositionality in Distributional
Semantics. In Proceedings of the ACL GEMS Work-
shop, pages 33?37.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 25?32, College Park, Maryland, USA, June.
Association for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th Inter-
national Conference on Computational Linguistings
(COLING 1998).
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
mentation, and Computers, 28:203?208.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio,
June. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Joakim Nivre. 2004. Incrementality in determinis-
tic dependency parsing. In Proceedings of the ACL
Workshop on Incremental Parsing, pages 50?57.
Sebastian Pado and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
James Pustejovsky. 2013. Inference patterns with in-
tensional adjectives. In Proceedings of the IWCS
Workshop on Interoperable Semantic Annotation,
Potsdam,Germany, March. Association for Compu-
tational Linguistics.
Silke Scheible, Sabine Schulte im Walde, and Sylvia
Springorum. 2013. Uncovering distributional dif-
ferences between synonyms and antonyms in a word
space model. In Proceedings of the International
Joint Conference on Natural Language Processing,
pages 489?497, Nagoya, Japan.
Heinrich Sch?utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97?
123.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Stefan Thater, Georgiana Dinu, and Manfred Pinkal.
2009. Ranking paraphrases in context. In Proceed-
ings of the 2009 Workshop on Applied Textual Infer-
ence, pages 44?47, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
19
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948?957,
Uppsala, Sweden, July. Association for Computa-
tional Linguistics.
Stefan Thater, Hagen Frstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2011).
P. D. Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter D. Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1012?1022, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81?88, Sapporo,
Japan.
20
