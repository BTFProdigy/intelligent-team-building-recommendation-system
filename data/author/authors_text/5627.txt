Representation and Recognition Method 
for Multi-Word Translation Units 
in Korean-to-Japanese MT System 
Kyonghi Moon 
Dept. of Computer Science & Engineering 
Pohang Univ. of Science and Technology 
San 31 Hyoia-dong Nam-gu, Pohang 790-784 
Republic of Korea 
khmoon @ kle.po stech.ac.kr 
Jong-Hyeok Lee 
Dept. of Computer Science & Engineering 
Pohang Univ. of Science and Technology 
San 31 Hyoja-dong Nam-gu, Pohang 790-784 
Republic of Korea 
jhlee@postech.ac.kr 
Abstract 
Due to grammatical similarities, even a 
one-to-one mapping between Korean and 
Japanese words (or morphemes) can usually 
result in a high quality Korean-to-Japanese 
machine translation. However, multi-word 
translation units (MWTU) such as idioms, 
compound words, etc., need an n-to-m 
mapping, and their component words often 
do not appear adjacently, resulting in a 
discontinuous MWTU. During translation, 
the MWTU should be treated as one lexical 
item rather than a phrase. In this paper, we 
define the types of MWTUs and propose 
their representation a d recognition method 
depending on their characteristics in 
Korean-to-Japanese MT system. In an 
experimental evaluation, the proposed 
method turned out to be very effective in 
handling MWTUs, showing an average 
recognition accuracy of 98.4% and a fast 
recognition time. 
1 Introduction 
As a transfer problem in a machine 
translation (MT), lexical and structural 
differences exist between source and target 
languages, which requires l-n, m-n, or n-1 
mapping strategies for machine translation 
system. For such mapping strategies, we need to 
treat several (n, or m) words (or morphemes) as 
a single translation unit. Although some 
researches (D.Santos,1990; Linden E.,1990; 
Yoon Sung Hoe, 1992; Ha Gyu Lee, 1994; 
D.Arnold,1994) employ the term "idiom" for 
these units, we prefer MWTU (Multi-Word 
Translation Unit) because it is a more general 
and broader term for MT environment. 
Up to now, some reseamh as focused on 
recognition and transfer of MWTUs, although 
very little research has been undertaken for 
Korean-to-Japanese machine translation systems 
(Seen-He Kim,1997). In previous researches, 
some tended to simplify the problem by treating 
only special types of MWTUs, while others had 
some recognition errors and took too much 
recognition time because they did not restrict he 
recognition scope (D.Santos,1990; Yoon Sung 
Hee,1992; Ha Gyu Lee, 1994; Seen-He 
Kim, 1997). 
For a Korean-to-English MT, Lee and Kim 
(Ha Gyu Lee,1994) uses only weak restrictions 
like adjacent inforlnation for recognition scope. 
However, their method needs stronger 
restrictions to resolve recognition errors and to 
speed up the process. Although some differences 
exist depending on which kinds of source and 
target languages are dealt with, MWTUs in 
Korean-to-Japanese MT frequently have their 
component words close together, so that one can 
predict he location of their separated component 
words. For this reason, we can enhance the 
recognition accuracy and time effectively by 
restricting the recognition scope according to the 
characteristics of an MWTU rather than taking 
the whole sentence as the scope. 
Moreover, the method by Lee and Kim (Ha 
Gyu Lee,1994) deals with only surface-level 
consistency without considering word order 
because Korean has ahnost free word order. It is 
obvious that the method can deal with variable 
544 
word-order MWTUs, but some incorrect 
recognition results arc possible whcn meaning 
changes according to word order. Because 
MWTUs to be treated in Korean-to-Japanese 
MT have an almost fixed word order sequence, 
their meaning may vary if the word order is 
changed. In (1), both sentences have the same 
lexical words (or morphemes), but while the first 
sentence must be treated as an MWTU, the 
second, which has the different sequence from 
the first, does not have the meaning of an 
MWTU. In (1), the words surrounded with a box 
are an essential component morpheme for an 
MWTU. 
(big) (nose) (get hurt) 
/*(1) had a b i t~)er ience  */ 
(nose) (get hurt) (big) 
/* It is serious (that I) got hurt in my nose */ 
In this paper, to solve the word order 
problem and thus enhance a recognition 
accuracy and time for MWTUs, we fix the word 
order in an MWTU and define the recognition 
scope of component words according to their 
characteristics. Based on it, then we propose a 
representation and recognition method of 
MWTUs for a Korean-to-Japanese MT system. 
In the rest of this paper, details will be presented 
about lhese proposed ideas, logclher with some 
evalualion results. For representing Korean and 
Japanese expressions, the 1994-SK (ROK 
Ministry of Education) and the Kunrei 
Romanization systems are used respectively. 
2 Processing of MWTUs 
In developing MT systems, we frequently 
contact with some differences in word spacing, 
grammar, and so on, between sotuve and target 
languages. But the method and degree of 
difficulty of handling them highly depend upon 
the nature of the source and target hmguage in 
the MT system. In this paper, we treat the 
representation and recognition methods of 
MWTUs according to their characteristics for 
only a Korean-to-Japanese MT system. 
2.1 Types of MWTU 
There call be 1-1, l-m, n-l, and n-m 
mapping relations of morphemes between source 
and target language in machine translation. Due 
to the grammatical similarities of Korean and 
Japanese, Korean-to-Japanese machine 
translation systems have been developed under 
the direct MT strategy, which assumes a 1-1 
mapping relation. But a uniform application of 
this 1-1 mapping relation will easily result in an 
unnatural translation. 
It is not difficult to handle a 1-1 and l-m 
mapping relations in Korean-to-Japanese MT 
system although it uses only direct MT strategy, 
because it is easy to recognize only one 
morpheme in source language, Korean. It is also 
due to the fact that Japanese correspondences 
have characteristics of non-spacing and 
continuity, which allows several words to be 
treated as a single word. In this reason, we need 
to consider just types with n-I and n-m mapping 
relations. Table 1 shows the types of MWTUs to 
be handled in Korean-to-Japanese MT. 
The compound words in Table 1 are the 
units that must be translated into one Japanese 
morpheme though they are conlpound words ill 
Korean. For example, "wodett peuroseseo" is a 
Korean compound word which consists of two 
morphemes "wodeu" and "l)euroseseo", but its 
Japanese equivalent is only one morpheme, 
"walmro". The Korean word '),eojju -co be 
l-dal" is also a compound word, made by 2 
lexical morphemes "yeoiju" and "be" and 1 
functional morpheme "-eo", but it also 
corresponds to only one Japanese equivalent 
morpheme, "ukagal-u\]". in these cases, the 
Korean compound words shoukl be recognized 
as one unit to be transformed into one Japanese 
morpheme. 
We can classify verbal nouns into 2 types 
according to their Japanese quivalents. Table 2 
shows them. If we define a Korean verbal noun 
as X and its equivalent in Japanese as X', and 
another single word in Japanese as Y, we can 
describe the two types of relations between 
Korean and Japanese verbal nouns as below. 
Although the type 1 satisfies l:l mapping 
relation, the type 2 does not. So, for the type2, 
the verbal noun, X (e.g., "chuka") and "ha\[-da\]" 
need to be recognized as a single unit to be 
transformed into a Japanese quivalent, Y.
545 
5) Idiom :: :: 
~\] l-&,l 
(congratulation) (do) 
(noise) (play) 
(thing) (equal) 
(bi\[~) (nose) \[,,,~, I(~et hurt) 
(first) (see) 
(in favor of) 
/* ask */ 
iwal-ul 
/* congratulate */ 
sawa\[-gu\] 
/* disturb */ 
soul-da\] 
/,I: seen l  *'/ 
hide -i me -hi a \[-u\] 
/* have a bitter experience */ 
hazime -masi -te 
/* How do you do */ 
-110 t(l111(~ -I10 
/* lbr */ 
ITable 2\] Types of verbal nouns 
X + ha\[-dal 
X + hal-dal 
Japanese 
X'  .t- SHl"tl 
Y 
Collocation patterns are the units that 
frequently co-occurr in sentences and affect the 
semantics of each other. There are two kinds of 
collocation patterns. In one, each component 
morpheme is translated into different equivalents, 
such as "dambae \[-reul\] piu\[-&ll(smoke)" 
corresponding to "tabako -o su\[-u\]", and in the 
other, all component morphemes must be 
translated into one Japanese morpheme with an 
equivalent meaning, such as "soran \[-eul\] 
piu\[-da\]" corresponding to "sawa\[-gu\]". While 
the morphemes in the former case have a l-to-1 
mapping relation, the morphemes in the latter 
case have an n-to-1 mapping relation and 
therefore, must be treated as a single morpheme. 
While some modalitics consist of only one 
morpheme like "-eot" or "-da", there are also 
some modalities made up of several morphemes 
like "-neun geot gat". Accordingly, the latter 
must be handled as an MWTU. 
An Idiom is a general idiomatic unit 
defined in a dictionary. Generally, since an 
idiom does not reflect literal meaning itself, 
translating their component morphemes 
individually results in very different meaning, In 
this case, it must be treated as a single unit. 
A colloquial idiomatic phrase is also 
composed of several morphemes, but it is 
recognized like a single unit word. For instance, 
the Korean greeting "cheoeum bee 1) -get 
-seumnida" corresponds to "hazime -masi -le" 
in Japanese. In this case, a 1-to-I mapping 
transformation results in an unnatural translation. 
Therefore, it also should be recognized as 
MWTUs. 
Moreover, MWTUs can be used for groups 
of words that can give a more natural translation 
when they are treated as one unit. We will call 
these groups of words semi-words. 
2.2 The Characteristics of MWTUs 
To minimize the recognition time and 
recognition error rate of MWTUs, we need to 
represent MWTUs according to their 
characteristics. The following shows the 
characteristics of MWTUs. 
1) Fixed word order 
All of the 7 types of MWTUs in Table 1 
have a fixed word order sequence, even though 
Korean and Japanese are known as free word 
order languages. Expressions uch as "keu -n ko 
dachi" and "-neun geot gat" nmst be recognized 
546 
as MWTUs, but their meaning may be changed 
from thin of MWTUs if the word order sequence 
has been changed. This provides a good 
characteristic for simply representing MWTUs. 
2) Extension by insertion o1' other words 
For some kinds of MWTUs, it is possible to 
insert some grammatical morphemes or other 
words between their component n~orplaemes of 
an MWTU. "-do" in (2) , "-reul" and "-reul geu 
-ege" in (3) are those cases. 
(go) (means) (is) 
/* (l) can go */ 
/* (1) can go, too */ 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  (3 )  
(a favor) (owe) 
/* be obliged to */ 
~ -reul ~ -da 
/* be obliged to */ 
, ~  -reul eg~ -egg ~ -da 
(he) 
/* be obliged to him */ 
According to this feature, the relations 
between immediately located two component 
morphemes of MWTUs can be classified as 
follows: 
A. tightly connected : the relation that no 
morpheme can be inserted between them 
B. loosely connected : the relation that some 
morphemes can be inserted between them. 
B-I. Only particles mad endings of a 
word are allowed to be inserted between 
them. 
B-2. Any kinds of morphemes can be 
inserted between them. 
\[Figure I\] Relations between two adjacent 
component morphemes of MWTUs 
3) Strong cohesion 
Although some MWTUs have 
characteristics of extension by insertion of other 
words, component morphemes in an MWTU 
have strong cohesion, not only logically but also 
physically. This means that tile recognition o1' an 
MWTU is possible by local comparison of its 
physical ocation. But it does not imply that the 
scope is limited in a simple sentence structure. 
4) The predictable recognition scope of 
MWTUs 
It is possible to predict tile recognition 
scope between two adjacent component 
morphemes of MWTUs, according to the above 
characteristics. The scope can be predicted as 
follows l'or each type of MWTUs shown in 
Table 1. 
Component morphemes of a compound 
word are corltiguous to the next Olle, so their 
scopes are predictable. 
Both verbal nouns and collocation patterns 
have the l'orm combined with "Noun" and 
"Verb", where other words can be inserted 
between them. But in the case (51' 
"Noun+Verb+Verb", which is the fern1 that 
another verb is inserted between the noun and 
verb, its meaning may be different in that of an 
MWTU. So ttae scope of the "Verb" can be 
limited up to the position of the first verb 
appearing after the "Noun", that is, the position 
where the POS(part-ofspeech) appears. 
Component morphemes of a modality have 
an especially strong cohesion. So at most, one 
particle is often inserted next to the bound noun. 
From this, we can predict the next component 
morpheme apart from pro component at most in 
distance 2. 
idioms, colloquial idiomatic phrases and 
senti-words consist of various colnponenl 
morphemes, which results in various scopes for 
MWTU recognition. The scopes of each 
conlpollellt ll\]Ol'phellles froul pl*e-colllponellt 
morphemes can be determined by distance 1, 
distance 2, or infinity. But inl'inite scope can 
also be limited by the position which the POS of 
the component morpheme appears. 
2.3 Representation of MWTU 
The representation f an MWTU must be 
considered in order to enhance recognition 
accuracy and speed up the process. Accordingly, 
in this paper, we propose representation method 
(51' MWTUs according to the characteristics 
mentioned in section 2.2. 
One basic rule for MWTU representation is 
that an MWTU is composed of only lexical 
morphemes if possible, that is, grammatical 
547 
morphemes uch as particles and the endings of 
a word will be extracted in the representation 
because of the above characteristics which are 
freely inserted and omitted. However, 
grammatical morphemes affecting the meanings 
of MWTUs must be described. 
Next, according to the characteristics 
described in section 2.2, we need to represent 
recognition scopes between adjacent component 
morphemes and POS of each component 
morpheme for the restriction of recognition 
scope. 
m,(POS,, d,2) m2(POS 2, d2~) ... m,(POS,, d,, m) ... 
m (POS,,, d,,.,,+,) 
m~: i-th COlnponent morpheme o1' an MWTU 
POS~ : POSofm~ 
d~.~+ x : maximum distance from m, tom~+~ 
\[Figure 2\] Representation of an MWTU 
d~,~+~ has 4 kinds of values according to 
Figure 1. For the case of A, d~,~+, is 1, for the case 
of B-l, it is 2, for the case of B-2, it is ~, mad 
then for the last component morpheme, it is 
always 0 because (n+l)-th component 
morpheme doesn't exist. 
The examples of MWTUs described by 
above representation are shown in Figure 3. 
? wodeu(N,1)proseseo(N,O) ~ wapuro 
(word) (processor) /* word processor */ 
? yeojju(V, 1) -eo(mC, 1 ) bo(V,0) ~ ukaga 
(ask) (see) /* ask */ 
? keu(ADJ, 1 ) -n(mT, l ) ko(N,2) dachi(V,O) 
(big) (nose) (get hurt) 
hidoinwnia /* have a bitter experience */
? -neunOnT,l) geot(ND,2) gat(ADJ,O) ~ sou 
(thing) (equal) /* seem */ 
? chuka(N,oo) ha(V,0) ~ iwa 
(congratulation) (do) /* congratulation*/ 
? -reul(j,1 ) wiha(V, 1) -n(mT,0) ~ notameno 
(in favor of) /* for */ 
? sesang(N, 2) muljeong(N, oo) moreu(V,O) 
(world) (condition) (don't know) 
seziniuto I* be ignorant of the world */ 
? jal(B,l) meok (V,l) -eot(e,l) -seumnidaOnT,O) 
(well) (eat) 
gotisousamadesita 
/* I have enjoyed my dinner very much */ 
\[Figure 3\] Examples of MWTUs 
Each MWTU is entered into the dictionary 
as an entry word such as the general morphemes 
as shown in Figure 4. Additionally, for 
recognition, we made the first component 
morpheme of the MWTU have an MWTU field, 
which is composed of MWTUs starting from the 
entry word. This means that only one access to 
the dictionary is needed after an MWTU is 
confirmed. Figure 4 shows the dictionary 
structure for an MWTU. 
4____  
;(mouth) (use) 
/* speak carelessly */ 
\[Connection i fo. for K~ 
\[Semantic info., Colloc~ 
\[Japanese quivalence, 
Janane, se ..... \] 
(ip) 
prcan\] 
tion pattern\] 
Connection 
\[Connection i fo. lot Kcrean, 
MWTU {ip(N ~ wlli(y;O), ip(N, 
bareu(V,O) ..... } \] 
\[Semantic info., Collocation pattern\] 
\[Japanese equivalence, Connection info. \['or 
.lanane~e ..... 1 
info. for 
~) 
\[Figure 4\] Dictionary for an MWTU 
2.4 Recogn i t ion  o f  MWTU 
Some rules are required in order to 
recognize MWTUs represented like those in 
section 2.3. 
First, the recognition scope of m~+~ after 
recognizing m~ is decided by POS~+, and d~.~+ c For 
restricting the recognition scope maximally 
while preventing other recognition errors, we 
formulated recognition scopes of each 
component morphemes of an MWTU as follows. 
RS(Recognition Scope) = min\[real_dist~<, d,+,\] 
real dist~+~ : the distance fi'om ln~ tothe i~oint 
- ' that the POS of In\[+ ~appears at 
first in an input sentence 
d~ ~+~ : maximum distance from m~ to in ,+, 
\[Figure 5\] Recognition scope 
In (4), for an MWTU "ip(N,oo) nolli(V,O), 
the recognition scope of "nolli" is 3 because dl, 2 
is oo and real_dist,, 2 is 3, which is fi'om 6-3. For 
an MWTU, "-ji(mC,2) an(V,0), the recognition 
scope of "an" is 1 because d3. 2 is 2 and real_dist,, 2 
is 1, which is from 12-11. Therefore, we can 
recognize MWTUs by a small comparison. 
548 
position 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 
Korean: , ,e -ga~-eu l  geureoke ~- ,nyeon bi,um-eul bat ~ ~-get-neut,ya ?...(4) 
(you) (mlmth) (in that manner) (u/e) (censure) (receive)\(nqt) 
speak carelessly in lhat manncr,'~u may be censured. */ 
Japanese: anatrt -ga sou nuka -se -ha hinan -o uke na -i ka "~ 
(you) (in that manner) (speak carelessly) (censure) (receive) (not) 
/* If you speak carelessly in that manner, you may be censured. */ 
position l 2 
Korean "~- reu l  
(a favor) 
I 
Japanese (a): !mlzi -nagmTa 
(deploring) 
Japanese (b): 
3 4 5 6 7 8 9 10 11 12 
hanta, -her -myeo ~ -neun hae -reul barabo -at -da ....... (5) 
(deploring) (?we or set) (sun) (look) 
/ 
~"~' /*Denlorine his circumstance. I looked at a settine sun.*/ 
osewanina -ru hi -o nagame -ta (X) 
(be obligated to) (sun) (look) 
/*Deploring, I looked at a sun which I am obligated to.*/ 
minoue -o tanzi -nagara irihi -o nagame -ta (O) 
(circumstance) (deploring) (a setting sun) (look) 
/*Deploring his circumstance, he looked at a setting sun.*/ 
\[Figure 6\] Recognition examples 
This Recognition rule can also prohibit 
some recognition errors generated from 
urlrlecessary comparisons. For instance, the 
recognition scope of "ji" in an MWTU 
"sinse(N,oo),ji(V,0)" was limited by 2, which is 
the minimum value between d~=(oo) and 
real_distj.2(3-1=2). So it prohibits errors, such as 
Japanese (a) in (5), occurring when an MWTU is 
recognized in whole sentence. 
The second rule states that morphemes 
inserted between the component morphemes of 
the recognized MWTU must be rearranged in 
the following manner: 
1) ff inserted morphemes are lexical 
morphemes, they are rearranged to the front of 
the MWTU. "geureoke(in that manner)" in (4) is 
such a case. 
2) If they are grammatical morphemes, they 
are ignored when they directly follow any 
component of the MWTU, and they are 
transl~rred to the front of the MWTU together 
with the inserted lexieal morphemes when thcy 
follow any inserted lexical morphemes. In (4), 
"-eul" is the former case. If any grammatical 
morpheme such as "-do" or "-ha" is attached 
after "geureoke", it will be the latter case. 
Third, if a morpheme is the common subset 
of the two MWTUs, we select the one such that 
its first component morpheme locates in the 
pre-position. This rule is used to reduce the 
recognition time by skipping morphemes which 
are subsets of the pre-confirmed MWTUs 
Fourth, we select he superset of MWTU in 
case that two or more MWTUs starting from a 
same morpheme are recognized and one is the 
superset of the others. For" example, let us 
consider two MWTUs: '~iamsi -man -yo (wait a 
moment)" and 'ijamsi -man(for a little while)", ff 
",jamsi -man-yo" is recognized, '~iamsi -man" 
can also be recognized and '~amsi -man -yo" is 
the supcrset of "jamsi -man". In this case, we 
select he supersct, '~antsi -man -yo". 
549 
3 Evaluat ion 
To demonstrate the efficiency of our 
proposed method, we applied it to a 
Korean-to-Japanese machine translation system 
(COBALT-K/J), and evaluated its recognition 
accuracy and recognition time. COBALT-K/J 
consists of about 150,000 general purpose words 
and 7,500 MWTUs. For the test corpus, we 
arbitrary extracted 2,808 sentences from a 10 
million word corpus, the KIBS (Korean 
Information Base System). MWTUs registered 
in the dictionary appeared 3,647 times in them. 
Table 3 shows the evaluation results 
classified by the types of MWTUs. 
\[Table 3\] Evaluation results on the recognition 
of MWTUs 
~i ,  Accur 
~u~o,~ 
33 32 97.0% 
:A)g:No! i
son  
918 907 
C0116~afio ..... 33 29 
1326 i 292 
5 5 
Coil0quial 
~3 83 
1249 ! 242 
> tola! 3,647 3,590 
98.8% 1.05 
87.9% 1.82 
97.4% 1.02 
100% 1.3 
100% 1.08 
99.4% t 1.01 
98.4% 1.03 
In Table 3, idioms, collocation patterns and 
compound words have a very low frequency 
while verbal nouns, modalities and semi-words 
have a relatively high frequency. Nevertheless, 
98.4% of the test samples were recognized 
correctly. In order to recognize an MWTU, it 
needed only 1.03 comparisons per each 
component morpheme of the MWTU on the 
average. This shows the effectiveness and the 
speed of our proposed method for treating 
MWTUs in Korean-to-Japanese MT. 
Conclusion 
In this paper, we classified the different 
kinds o1' MWTUs and proposed a representation 
and recognition method for them in a 
Korean-to-Japanese MT. 
MWTUs in Korean-to-Japanese MT have 
the characteristics of fixed word order, strong 
cohesion, predictable scope of its component 
morphemes, extension by other words, etc. 
Accordingly, we enhanced accuracy and 
recognition time by representing and 
recognizing MWTUs according to their 
characteristics. 
In our experiment, 98.4% of the test 
samples were recognized correctly, which shows 
the effectiveness of our proposed method. In 
future work, we will research in more strict 
recognition restrictions and plan to extract 
MWTUs from a corpus automatically. 
References 
D. Santos(1990), Lexical gaps and idioms in 
machine translation, 13" International 
Conference of Computational Linguistics. 
Coling 90, Finland, pp. 330-335. 
Linden E., Wessel K. (1990), Ambiguio~ 
resolution and the retriewE of idioms: two 
aM)roaches, 13'" International Conference of 
Computational Linguistics. Coling 90, Finland, 
pp. 245-248. 
Yoon Sung Hee (1992), Idiomatical and 
Collocational Approach to English-Korean 
Machine Translation., Proceedings of 
1CCPOL '92, pp.56-60. 
Ha Gyu Lee, Yung Taek Kim (1994), 
Representation arm Recognition of Korean 
Idioms for Machine Translation, Journal of the 
Korean Information Science Society, Vol. 21, 
No. 1, pp.139-149 (written in Korean). 
Seon-Ho Kim (1997), Lexicon-Based Approach 
to Recognition and Tran,sfer of Multi-Word 
Translation Units hz Korean-Japanese 
Machine 7)'anslation, MS Thesis, Pohang 
University of Science and Technology (written 
in Korean). 
D.Arnold, L.Balkan, R. Lee Hurnphreys, 
S.Meijer, L.sadler (1994), Machine 
Transhttion, Blackwell, USA. 
550 
Word Sense Disambiguation in a Korean-to-Japanese 
MT System Using Neural Networks 
You-Jin Chung, Sin-Jae Kang, Kyong-Hi Moon, and Jong-Hyeok Lee 
Div. of Electrical and Computer Engineering, Pohang University of Science and Technology (POSTECH) 
and Advanced Information Technology Research Center(AlTrc) 
San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA, 790-784 
{prizer,sjkang,khmoon,jhlee}@postech.ac.kr 
 
Abstract  
This paper presents a method to resolve 
word sense ambiguity in a 
Korean-to-Japanese machine translation 
system using neural networks. The 
execution of our neural network model is 
based on the concept codes of a thesaurus. 
Most previous word sense disambiguation 
approaches based on neural networks have 
limitations due to their huge feature set size. 
By contrast, we reduce the number of 
features of the network to a practical size by 
using concept codes as features rather than 
the lexical words themselves. 
Introduction 
Korean-to-Japanese machine translation (MT) 
employs a direct MT strategy, where a Korean 
homograph may be translated into a different 
Japanese equivalent depending on which sense 
is used in a given context. Thus, word sense 
disambiguation (WSD) is essential to the 
selection of an appropriate Japanese target word. 
Much research on word sense disambiguation 
has revealed that several different types of 
information can contribute to the resolution of 
lexical ambiguity. These include surrounding 
words (an unordered set of words surrounding a 
target word), local collocations (a short sequence 
of words near a target word, taking word order 
into account), syntactic relations (selectional 
restrictions), parts of speech, morphological 
forms, etc (McRoy, 1992, Ng and Zelle, 1997). 
Some researchers use neural networks in 
their word sense disambiguation systems 
Because of its strong capability in classification 
(Waltz et al, 1985, Gallant, 1991, Leacock et al, 
1993, and Mooney, 1996). Since, however, most 
such methods require a few thousands of 
features or large amounts of hand-written data 
for training, it is not clear that the same neural 
network models will be applicable to real world 
applications. 
We propose a word sense disambiguation 
method that combines both the neural net-based 
approach and the work of Li et al(2000), 
especially focusing on the practicality of the 
method for application to real world MT 
systems. To reduce the number of input features 
of neural networks to a practical size, we use 
concept codes of a thesaurus as features. 
In this paper, Yale Romanization is used to 
represent Korean expressions. 
1 System Architecture 
Our neural network method consists of two 
phases. The first phase is the construction of the 
feature set for the neural network; the second 
phase is the construction and training of the 
neural network. (see Figure 1.) 
For practical reasons, a reasonably small 
number of features is essential to the design of a 
neural network. To construct a feature set of a 
reasonable size, we adopt Li?s method (2000), 
based on concept co-occurrence information 
(CCI). CCI are concept codes of words which 
co-occur with the target word for a specific 
syntactic relation. 
In accordance with Li?s method, we 
automatically extract CCI from a corpus by 
constructing a Korean sense-tagged corpus. To 
accomplish this, we apply a Japanese-to-Korean 
MT system. Next, we extract CCI from the 
constructed corpus through partial parsing and 
scanning. To eliminate noise and to reduce the 
number of CCI, refinement proceesing is applied 
Japanese Corpus
COBALT-J/K
(Japanese-to-Korean
MT system)
Sense Tagged
Korean Corpus
Partial Parsing
& Pattern Scanning
Raw CCI
CCI Refinement
Processing
Refined CCI
Feature Set Construction Neural Net Construction
Feature Set
Network
Construction
Neural Network
Network
Learning
Stored in
MT Dictionary
Network
Parameters
Figure 1. System Architecture 
noun
nature  character          society institute  things 
0            1                     7          8          9
astro- calen- animal        pheno-
nomy    dar                          mena
00      01              06             09
goods drugs  food       stationary    machine
90      91       92             96             99
orga- ani- sin- intes- egg    sex
nism  mal              ews   tine    
060    061               066   067    068   069
supp- writing- count- bell
lies      tool      book
960     961      962               969
?????
?????
?????
????? ?????
?????
?????
?????
?????
?????
L1
L2
L3
L4
Figure 2. Concept hierarchy of the Kadokawa
thesaurus
to the extracted raw CCI. After completing 
refinement processing, we use the remaining 
CCI as features for the neural network. The 
trained network parameters are stored in a 
Korean-to-Japanese MT dictionary for WSD in 
translation. 
2 Construction of Refined Feature Set 
2.1 Automatic Construction of Sense-tagged 
Corpus 
For automatic construction of the sense-tagged 
corpus, we used a Japanese-to-Korean MT 
system called COBALT-J/K1. In the transfer 
dictionary of COBALT-J/K, nominal and verbal 
words are annotated with concept codes of the 
Kadokawa thesaurus (Ohno and Hamanishi, 
1
1
C
d
C
t
n
c
a
1
J
N
 
1
T
p
The quality of the constructed sense-tagged 
corpus is a critical issue. To evaluate the quality, 
we collected 1,658 sample sentences (29,420 
eojeols2) from the corpus and checked their 
precision. The total number of errors was 789, 
and included such errors as morphological 
analysis, sense ambiguity resolution and 
unknown words. It corresponds to the accuracy 
of 97.3% (28,631 / 29,420 eojeols). 
Because almost all Japanese common nouns 
represented by Chinese characters are 
monosemous little transfer ambiguity is 
exhibited in Japanese-to-Korean translation. In 
our test, the number of ambiguity resolution 
errors was 202 and it took only 0.69% of the 
overall corpus (202 / 29,420 eojeols). 
Considering the fact that the overall accuracy of 
the constructed corpus exceeds 97% and only a 
few sense ambiguity resolution errors were 
 
 
 
 
 
. 
, 
 
 
 
 981), which has a 4-level hierarchy of about 
,100 semantic classes, as shown in Figure 2. 
oncept nodes in level L1, L2 and L3 are further 
ivided into 10 subclasses. 
We made a slight modification of 
OBALT-J/K to enable it to produce Korean 
ranslations from a Japanese text, with all 
ominal words tagged with specific concept 
odes at level L4 of the Kadokawa thesaurus. As 
 result, a Korean sense-tagged corpus of 
,060,000 sentences can be obtained from the 
apanese corpus (Asahi Shinbun, Japanese 
ewspaper of Economics, etc.). 
                                                    
 COBALT-J/K (Collocation-Based Language 
ranslator from Japanese to Korean) is a high-quality 
ractical MT system developed by POSTECH.                                                      
found in the Japanese-to-Korean translation of
nouns, we regard the generated sense-tagged
corpus as highly reliable. 
2.2 Extraction of Raw CCI 
Unlike English, Korean has almost no syntactic
constraints on word order as long as the verb
appears in the final position. The variable word
order often results in discontinuous constituents
Instead of using local collocations by word order
Li et al (2000) defined 13 patterns of CCI for
homographs using syntactically related words in
a sentence. Because we are concerned only with
2 An Eojeol is a Korean syntactic unit consisting of a
content word and one or more function words. 
Table 2. Concept codes and frequencies in CFP 
({<Ci,fi>}, type2, nwun(eye)) 
Code Freq. Code Freq. Code Freq. Code Freq.
103 4 107 8 121 7 126 4 
143 8 160 5 179 7 277 4 
320 8 331 6 416 7 419 12
433 4 501 13 503 10 504 11
505 6 507 12 508 27 513 5 
530 6 538 16 552 4 557 7 
573 5 709 5 718 5 719 4 
733 5 819 4 834 4 966 4 
987 9 other* 210     
? ?other? in the table means the set of concept codes 
with the frequencies less than 4. 
Table 1. Structure of CCI Patterns 
CCI type Structure of pattern 
type0 unordered co-occurrence words 
type1 noun + noun  or  noun + noun 
type2 noun + uy + noun 
type3 noun + other particles + noun 
type4 noun + lo/ulo + verb 
type5 noun + ey + verb 
type6 noun + eygey + verb 
type7 noun + eyse + verb 
type8 noun + ul/lul + verb 
type9 noun + i/ka + verb 
type10 verb + relativizer + noun 
noun homographs, we adopt 11 patterns from 
them excluding verb patterns, as shown in Table 
1. The words in bold indicate the target 
homograph and the words in italic indicate 
Korean particles. 
For a homograph W, concept frequency 
patterns (CFPs), i.e., ({<C1,f1>,<C2,f2>, ... , 
<Ck,fk>}, typei, W(Si)), are extracted from the 
sense-tagged training corpus for each CCI type i 
by partial parsing and pattern scanning, where k 
is the number of concept codes in typei, fi is the 
frequency of concept code Ci appearing in the 
corpus, typei is an CCI type i, and W(Si) is a 
homograph W with a sense Si. All concepts in 
CFPs are three-digit concept codes at level L4 in 
the Kadokawa thesaurus. Table 2 demonstrates 
an example of CFP that can co-occur with the 
homograph ?nwun(eye)? in the form of the CCI 
type2 and their frequencies. 
2.3 CCI Refinement Processing 
The extracted CCI are too numerous and too 
noisy to be used in a practical system, and must 
to be further selected. To eliminate noise and to 
reduce the number of CCI to a practical size, we 
apply the refinement processing to the extracted 
CCI. CCI refinement processing is composed of 
2 processes: concept code discrimination and 
concept code generalization. 
2.3.1 Concept Code Discrimination 
In the extracted CCI, the same concept code may 
appear for determining the different meanings of 
a homograph. To select the most probable 
concept codes, which frequently co-occur with 
the target sense of a homograph, Li defined the 
discrimination value of a concept code using 
Shannon?s entropy (Shannon, 1951). A concept 
code with a small entropy has a large 
discrimination value. If the discrimination value 
of the concept code is larger than a threshold, 
the concept code is selected as useful 
information for deciding the word sense. 
Otherwise, the concept code is discarded. 
2.3.2 Concept Code Generalization 
After concept discrimination, co-occurring 
concept codes in each CCI type must be further 
selected and the code generalized. To perform 
code generalization, Li adopted to Smadja?s 
work (Smadja, 1993) and defined the code 
strength using a code frequency and a standard 
deviation in each level of the concept hierarchy. 
The generalization filter selects the concept 
codes with a strength larger than a threshold. We 
perform this generalizaion processing on the 
Kadokawa thesaurus level L4 and L3. 
After processing, the system stores the 
re
ty
re
fe
s
e
3
3
B
th
u
s
refined conceptual patterns ({C1, C2, C3, ...}, 
pei, W(Si)) as a knowledge source for WSD of 
al texts. These refined CCI are used as input 
atures for the neural network. The more 
pecific description of the CCI extraction is 
xplained in Li (2000). 
 Construction of Neural Network 
.1 Neural Network Architecture 
ecause of its strong capability for classification, 
e multilayer feedforward neural network is 
sed in our sense classification system. As 
hown in Figure 3, each node in the input layer 
presents a concept code in CCI of a target 
. .
CCI type i2
CCI type i1
input CCI type 0
input
CCI type 1
input
CCI type 8
input
CCI type 2
input
74
26
022
078
080
50
696
028
419
38
23
239
323
nwun1 (snow)
nwun2 (eye)
...
Figure 5. The Resulting Network for ?nwun? 
w
r
n
n
To determine a good topology for the network, 
we implemented a 2-layer (no hidden layer) and 
a 3-layer (with a single hidden layer of 5 nodes) 
network and compared their performance. The 
comparison result is given in Section 5. 
Each homograph has a network of its own. 
Figure 43 demonstrates a construction example 
of the input layer for the homograph ?nwun? 
with the sense ?snow? and ?eye?. The left side is 
the extracted CCI for each sense after refinement 
processing. We construct the input layer for 
?nwun? by merely integrating the concept codes 
in both senses. The resulting input layer is 
partitioned into several subgroups depending on 
their CCI types, i.e., type 0, type 1, type 2 and 
type 8. Figure 5 shows the overall network 
architecture for ?nwun?. 
 
3  
f  
c
3.2 Network Learning 
We selected 875 Korean homographs requring 
the WSD processing in a Korean-to-Japanese 
translation. Among the selected nouns, 736 
nouns (about 84%) had two senses and the other 
139 nouns had more than 3 senses. Using the 
extracted CCI, we constructed neural networks 
and trained network parameters for each 
homograph. The training patterns were also 
extracted from the previously constructed 
sense-tagged corpus. 
The average number of input features (i.e. 
input nodes) of the constructed networks was 
approximately 54.1 and the average number of 
senses (i.e. output nodes) was about 2.19. In the 
case of a 2-layer network, the total number of 
parameters (synaptic weights) needed to be 
trained is about 118 (54.1?2.19) for each 
homograph. This means that we merely need 
storage for 118 floating point numbers (for 
s
fe
re
? CCI type 0 : {26, 022}
? CCI type 1 : {080, 696}
nwun1 (snow)
CCI type 0
input
CCI type 1
74
26
022
078
080
Refined CCI
4
O
c
k
o
s
F
K                                                    
 The concept codes in Figure 4 are simplified ones
or the ease of illustration. In reality there are 87
? CCI type 8 : {38, 239}
Total 13 concept codes
integrate input
CCI type 8
input
CCI type 2
input
13 nodes
nwun2 (eye)
? CCI type 0 : {74, 078}
? CCI type 2 : {50, 028, 419}
? CCI type 8 : {23, 323}
50
696
028
419
38
23
239
323
Figure 4. Construction of Input layer for ?nwun?...
..
Output
(senses of the 
target word)
Inputs Outputs
..
Hidden
Layers
input
CCI type ik
input
...
Figure 3. Topology of Neural Network 
ord and each node in the output layer 
epresents the sense of a target word. The 
umber of hidden layers and the number of 
odes in a hidden layer are another crucial issue. oncept codes for ?nwun?. Cynaptic weights) and 54 integers (for input 
atures) for each homograph, which is a 
asonable size to be used in real applications. 
 Word Sense Disambiguation 
ur WSD approach is a hybrid method, which 
ombines the advantage of corpus-based and 
nowledge-based methods. Figure 6 shows our 
verall WSD algorithm. For a given homograph, 
ense disambiguation is performed as follows. 
irst, we search a collocation dictionary. The 
orean-to-Japanese translation system 
OBALT-K/J has an MWTU (Multi-Word 
{078}
CCI type 0 CCI type 0
input
CCI type 1
input
CCI type 8
input
CCI type 2
input
CCI type 1
nwunmwul-i   katuk-han   kunye-uy   nwun-ul   po-mye
input               : ??? ??? ??? ?? ?? ?
[078] [274]concept code  : [503] [331]targetword
CCI type        : (type 0) (type 0) (type 2) (type 8)
CCI type 2
CCI type 8
{none}
{503}
{331}
078
022
74
26
028
50
696
080
239
38
23
419
323
Input Layer
Similarity
Calculation
{274}
(0.000)
(0.285)
(0.250)
(1.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.000)
(0.250)
similarity values
Figure 7. Construction of Input Pattern by Using
Concept Similarity Calculation 
Neural Networks
Select the most frequent sense
Success
Success
Answer
NO
NO
NO
YES
YES
YES
Selectional Restrictions of the Verb
Collocation Dictionary
Success
 
Figure 6. The Proposed WSD Algorithm 
Translation Units) dictionary, which contains 
idioms, compound words, collocations, etc. If a 
collocation of the target word exists in the 
MWTU dictionary, we simply determine the 
sense of the target word to the sense found in the 
dictionary. This method is based on the idea of 
?one sense per collocation?. Next, we verify the 
selectional restriction of the verb described in 
the dictionary. If we cannot find any matched 
patterns for selectional restrictions, we apply the 
neural network approach. WSD in the neural 
network stage is performed in the following 3 
steps. 
Step 1. Extract CCI from the context of the 
target word. The window size of the context is a 
single sentence. Consider, for example, the 
sentence in Figure 7 which has the meaning of 
?Seeing her eyes filled with tears, ??. The 
target word is the homograph ?nwun?. We 
extract its CCI from the sentence by partial 
parsing and pattern scanning. In Figure 7, the 
words ?nwun? and ?kunye(her)? with the concept 
code 503 have the relation of <noun + uy + 
noun>, which corresponds to ?CCI type 2? in 
Table 1. There is no syntactic relation between 
the words ?nwun? and ?nwunmul(tears)? with the 
concept code 078, so we assign ?CCI type 0? to 
the concept code 078. 
Similarly, we can obtain all pairs of CCI types 
and their concept codes appearing in the context. 
All the extracted <CCI-type: concept codes> 
pairs are as follows: {<type 0: 078,274>, <type 
2: 503>, <type 8: 331>}. 
Step 2. Obtain the input pattern for the 
network by calculating concept similarities 
between the features of the input nodes and the 
concept code in the extracted <CCI-type: 
concept codes>. Concept similarity calculation 
is performed only between the concept codes 
with the same CCI-type. The calculated concept 
similarity score is assigned to each input node as 
the input value to the network. 
Csim(Ci, Pj) in Equation 1 is used to calculate 
the concept similarity between Ci and Pj, where 
MSCA(Ci, Pj) is the most specific common 
ancestor of concept codes Ci and Pj, and weight 
is a weighting factor reflecting that Ci as a 
descendant of Pj is preferable to other cases. 
That is, if Ci is a descendant of Pj, we set weight 
to 1. Otherwise we set weight to 0.5. 
weight
PlevelClevel
PCMSCAlevel
PCCsim
ji
ji
ji ?+
?=
)()(
)),((2
),(  (1)
The similarity values between the target 
(all 0.000)
(0.375)
(0.857)
(0.667)
(0.285)
(0.250) (0.250)
L1
L2
L3
L4
?
Ci
P1
P2
P3
P4
P5 P5
TOP
Figure 8. Concept Similarity on the Kadokawa
Thesaurus Hierarchy 
concept Ci and each Pj on the Kadokawa 
thesaurus hierarchy are shown in Figure 8. 
These similarity values are computed using 
Equation 1. For example, in ?CCI-type 0? part 
calculation, the relation between the concept 
codes 274 and 26 corresponds to the relation 
between Ci and P4 in Figure 8. So we assign the 
similarity 0.285 to the input node labeled by 26. 
As another example, the concept codes 503 and 
50 have a relation between Ci and P2 and we 
obtain the similarity 0.857. If more than two 
concept codes exist in one CCI-type, such as 
<CCI-type 0: 078, 274>, the maximum 
similarity value among them is assigned to the 
input node, as in Equation 2. 
In Equation 2, Ci is the concept code of the 
input node, and Pj is the concept codes in the 
<CCI-type: concept codes> pair which has the 
same CCI-type as Ci. 
By adopting this concept similarity calculation, 
we can achieve a broad coverage of the method. 
If we use the exact matching scheme instead of 
concept similarity, we may obtain only a few 
concept codes matched with the features. 
Consequently, sense disambiguation would fail 
because of the absence of clues. 
Step 3. Feed the obtained input pattern to the 
neural network and compute activation strengths 
for each output node. Next, select the sense of 
the node that has a larger activation value than 
all other output node. If the activation strength is 
lower than the threshold, it will be discarded and 
his 
5 Experimental Evaluation 
For an experimental evaluation, 10 ambiguous 
Korean nouns were selected, along with a total 
of 500 test sentences in which one homograph 
appears. In order to follow the ambiguity 
distribution described in Section 3.2, we set the 
number of test nouns with two senses to 8 (80%). 
The test sentences were randomly selected from 
the KIBS (Korean Information Base System) 
corpus. 
The experimental results are shown in Table 3, 
where result A is the case when the most 
frequent sense was taken as the answer. To 
compare it with our approach (result C), we also 
performed the experiment using Li?s method 
(result B). For sense disambiguation, Li?s 
method features which are similar to our method. 
However, unlike our method, which combines 
all features by using neural networks, Li 
considers only one clue at each decision step. As 
shown in the table, our approach exceeded Li?s 
)),((max)( jiPi PCCsimCInputVal i
=    (2)
Table 3. Comparison of WSD Results 
Precision (%) Word Sense No (A) (B) (C)
father & child 33 pwuca
rich man 17 
66 64 72
liver 37 kancang
soy source 13 
74 84 74
housework 39 kasa
words of song 11 
78 68 82
shoes 45 kwutwu
word of mouth 5 
90 70 92
eye 42 nwun
snow 8 
84 80 86
container 41 yongki 82 72 88
the network will not make any decisions. T
process is represented in Figure 9. courage 9 
doctor 27 uysa
intention 23 
54 80 84
district 27 cikwu
the earth 23 
54 84 92
whole body 39 
one?s past 6 censin
telegraph 5 
78 84 80
one?s best 27 
military strength 13 
electric power 7 
cenlyek
past record 3 
54 50 72
Average Precision 71.4 73.6 82.2
? (A) : Baseline   (B) : Li?s method 
(C) : Proposed method (using a 2-layer NN) 
nwun1 (snow)
nwun2 (eye)
...
threshold
(0.000)
(0.285)
(0.250)
(1.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.000)
(0.250)
Figure 9. Sense Disambiguation for ?nwun? 
in most of the results except ?kancang? and 
?censin?. This result shows that word sense  
disambiguation can be improved by combining 
several clues together (e.g. neural networks) 
rather than using them independently (e.g. Li?s 
method). 
The performance for each stage of the 
proposed method is shown in Table 4. Symbols 
COL, VSR, NN and MFS in the table indicate 4 
stages of our method in Figure 6, respectively. 
In the NN stage, the 3-layer model did not show 
a performance superior  to the 2-layer model 
because of the lack of training samples. Since 
the 2-layer model has fewer parameters to be 
trained, it is more efficient to generalize for 
limited training corpora than the 3-layer model. 
Conclusion 
To resolve sense ambiguities in 
Korean-to-Japanese MT, this paper has proposed 
a practical word sense disambiguation method 
using neural networks. Unlike most previous 
approaches based on neural networks, we reduce 
the number of features for the network to a 
practical size by using concept codes rather than 
lexical words. In an experimental evaluation, the 
proposed WSD model using a 2-layer network 
achieved an average precision of 82.2% with an 
improvement over Li?s method by 8.6%. This 
result is very promising for real world MT 
systems. 
We plan further research to improve precision 
and to expand our method for verb homograph 
disambiguation. 
Acknowledgements 
This work was supported by the Korea Science 
and Engineering Foundation (KOSEF)  through 
the Advanced Information Technology Research 
Center(AITrc). 
Table 4. Average Precision and Coverage 
for Each Stage of thePproposed Method 
 
<Case 1 : 2-layer NN> 
 COL VSR NN MFS 
Avg. Prec 100.0% 91.2% 86.3% 56.1%
Avg. Cov 3.6% 6.8% 73.2% 16.4%
 
<Case 2 : 3-layer NN> 
 COL VSR NN MFS 
Avg. Prec 100.0% 91.2% 87.1% 56.0%
Avg. Cov 3.6% 6.8% 72.5% 17.1%
 
References 
Gallant S. (1991) A Practical Approach for 
Representing Context and for Performing Word 
Sense Disambiguation Using Neural Networks. 
Neural Computation, 3/3, pp. 293-309 
Leacock C., Twell G. and Voorhees E. (1993) 
Corpus-based Statistical Sense Resolution. In 
Proceedings of the ARPA Human Language 
Technology Workshop, San Francisco, Morgan 
Kaufman, pp. 260-265 
Li H. F., Heo N. W., Moon K. H., Lee J. H. and Lee 
G. B. (2000) Lexical Transfer Ambiguity 
Resolution Using Automatically-Extracted Concept 
Co-occurrence Information. International Journal 
of Computer Processing of Oriental Languages, 
13/1, pp. 53-68  
McRoy S. (1992) Using Multiple Knowledge Sources 
for Word Sense Discrimination. Computational 
Linguistics, 18/1, pp. 1-30 
Mooney R. (1996) Comparative Experiments on 
Disambiguating Word Senses: An Illustration of 
the Role of Bias in Machine Learning. In 
Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, 
Philadelphia, PA, pp. 82-91 
Ng, H. T. and Zelle J. (1997) Corpus-Based 
Approaches to Semantic Interpretation in Natural 
Language Processing. AI Magazine, 18/4, pp. 
45-64 
Ohno S. and Hamanishi M. (1981) New Synonym 
Dictionary. Kadokawa Shoten, Tokyo 
Smadja F. (1993) Retrieving Collocations from Text: 
Xtract. Computational Linguistics, 19/1, pp. 
143-177 
Waltz D. L. and Pollack J. (1985) Massively Parallel 
Parsing: A Strongly Interactive Model of Natural 
Language Interpretation. Cognitive Science, 9, pp. 
51-74 
 
