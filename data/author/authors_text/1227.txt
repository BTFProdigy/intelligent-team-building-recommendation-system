R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 838 ? 848, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Integrating Punctuation Rules and Na?ve Bayesian  
Model for Chinese Creation Title Recognition 
Conrad Chen and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering,  
National Taiwan University, Taipei, Taiwan 
drchen@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw 
http://nlg.csie.ntu.edu.tw/ 
Abstract. Creation titles, i.e. titles of literary and/or artistic works, comprise 
over 7% of named entities in Chinese documents. They are the fourth large sort 
of named entities in Chinese other than personal names, location names, and 
organization names. However, they are rarely mentioned and studied before. 
Chinese title recognition is challenging for the following reasons. There are few 
internal features and nearly no restrictions in the naming style of titles. Their 
lengths and structures are varied. The worst of all, they are generally composed 
of common words, so that they look like common fragments of sentences. In 
this paper, we integrate punctuation rules, lexicon, and na?ve Bayesian models 
to recognize creation titles in Chinese documents. This pioneer study shows a 
precision of 0.510 and a recall of 0.685 being achieved. The promising results 
can be integrated into Chinese segmentation, used to retrieve relevant informa-
tion for specific titles, and so on. 
1   Introduction 
Named entities are important constituents to identify roles, meanings, and relation-
ships in natural language sentences. However, named entities are productive, so that it 
is difficult to collect them in a lexicon exhaustively. They are usually ?unknown? 
when we process natural language sentences. Recognizing named entities in docu-
ments is indispensable for many natural language applications such as information 
retrieval [2], summarization [3], question answering [7], and so on. 
Identifying named entities is even harder in Chinese than in many Indo-European 
languages like English. In Chinese, there are no delimiters to mark word boundaries and 
no special features such as capitalizations to indicate proper nouns, which constitute 
huge part of named entities. In the past, various approaches [1, 4, 10] have been pro-
posed to recognize Chinese named entities. Most of them just focused on MUC-style 
named entities [8], i.e., personal names, location names, and organization names. The 
extensive studies cover nearly 80% of named entities in real documents [1]. Although 
the performance of such kinds of named entity recognizers is satisfiable, the rest 20% of 
named entities are so far rarely mentioned and often ignored in previous studies.  
These rarely mentioned ones belong to various sorts, such as terminologies, aliases 
and nicknames, brands, etc. These sorts may not occur as frequently as personal 
names or location names in a corpus, but the importance of the former in documents 
 Integrating Punctuation Rules and Na?ve Bayesian Model 839 
of specific domains is no less than that of the latter. For example, knowing names of 
dishes would be very important to understand articles about cooking. Among these 
rarely addressed named entities, titles of creations, such as book names, song titles, 
sculpture titles, etc., are one of the most important sorts. According to Chen & Lee 
(2004)?s study [1] of Academia Sinica Balanced Corpus (abbreviated ASBC corpus 
hereafter), about 7% of named entities are titles of creations. In other words, more 
than one-third of rarely mentioned named entities are titles of creations.  
Chinese title recognition is challenging for the following reasons. There are no 
limitations in length and structures of titles. They might be a common word, e.g. ??
?? (Mistakes, a Chinese poem), a phrase, e.g. ??????? (Norwegian Wood, a 
song), a sentence, e.g. ?????????? (Don?t Cry for Me Argentina, a song), 
or even like nothing, e.g. ????????? (Rub ? Undescribable, a Chinese poetry 
collection). Besides, the choice of characters to name titles has no obvious prefer-
ences. Till now, few publications touch on Chinese title recognition. There are even 
no available corpora with titles being tagged.  
Several QA systems, such as Sekine and Nobata (2004) [9], used fixed patterns and 
dictionaries to recognize part of titles in English or Japanese. Lee et al (2004) [6] pro-
posed an iterative method that constructs patterns and dictionaries to recognize English 
titles. Their method cannot be adapted to Chinese, however, because the most important 
feature employed is capitalization, which does not exist in Chinese. 
In this paper, we propose a pioneer study of Chinese title recognition. An approach 
of integrating punctuation rules, lexicon, and na?ve Bayesian models is employed to 
recognize creation titles in Chinese documents. Section 2 discusses some cues for Chi-
nese title recognition. Section 3 gives a system overview.  Punctuation rules and title 
gazetteer identify part of titles and filter out part of non-titles. The rest of undetermined 
candidates are verified by na?ve Bayesian model. Section 4 addresses which features 
may be adopted in training na?ve Bayesian model. Section 5 lists the training and testing 
materials, and shows experimental results.  Section 6 concludes there marks. 
2   Cues for Chinese Creation Title Recognition 
Titles discussed in this paper cover a wide range of creations, including literature, 
music, painting, sculpture, dance, drama, movies, TV or radio programs, books, 
newspapers, magazines, research papers, albums, PC games, etc. All of these titles are 
treated as a single sort because they share the same characteristics, i.e., they are 
named by somebody with creativity, and thus there are nearly no regularity or limita-
tions on their naming styles.  
The challenging issue is that, unlike MUC-style named entities (MUC7, 1998), ti-
tles are usually composed of common words, and most of them have no internal fea-
tures like surnames or entity affixes, e.g. ??? (City) in ????? (Taipei City). In 
other words, most titles might look just like common strings in sentences. Thus it is 
even more difficult to decide which fragment of sentences might be a title than to 
determine if some fragment is a title. 
For the lack of internal features, external features or context information must be 
found to decide boundaries of titles. Table 1 shows some words preceding or follow-
ing titles in one-tenth sampling of ASBC corpus with titles tagged manually. We can 
840 C. Chen and H.-H. Chen 
observe that quotation marks are widely used. This is because writers usually quote 
titles with punctuation marks to make them clear for readers. The most common used 
ones are the two pairs of quotation marks ???? and ????. About 40% of titles are 
quoted in ???? or ???? in our test corpus. However, labeling proper nouns is 
only one of their functions.  Quotation marks are extensively used in various pur-
poses, like dialogues, emphasis, novel words, etc. In our analysis, only less than 7% 
of strings quoted in ???? or ???? are creation titles.  It means the disambiguation 
of the usages of quotation marks is necessary. 
Table 1. Preceding and Following Words of Titles in One-tenth Sampling of ASBC Corpus 
Preceding 
Word Frequency 
Following 
Word Frequency 
? 450     ? 442     
? 216     ? 216     
? 44     ? 56     
? 31     ? 32     
? 25     ? 26     
? 24     ? 16     
? 19     ? 13     
? 16     ? 11     
? 9     ? 7     
? 7     ? 6     
? 6     ? 6     
? 6     ? 6     
The most powerful external feature of creation titles is French quotes ????, 
which is defined to represent book names in the set of standard Simplified Chinese 
punctuation marks of China [5]. However, they are not standard punctuation marks in 
Traditional Chinese. Besides the usage of French quotes to mark book names, writers 
often use them to label various types of creation titles. According to our analysis on 
Web searching and the sampling corpus, about 20% of occurrences of titles in Tradi-
tional Chinese documents are quoted in ????, and nearly no strings other than titles 
would be quoted in ????. This punctuation mark shows a very powerful cue to deal 
with title recognition. 
Nevertheless, there are still 40% of titles without any marks around. These un-
marked titles usually stand for widely known or classic creations. In other words, 
these famous works are supposed to be mentioned in many documents many times. 
Such kinds of titles are extensively known by people like a common vocabulary. A 
lexicon of famous creations should cover a large part of these common titles. 
3   System Overview 
Based on the analyses in Section 2, we propose some punctuation rules that exploit 
the external features of titles to recognize possible boundaries of titles in Chinese 
 Integrating Punctuation Rules and Na?ve Bayesian Model 841 
documents. Most strings that cannot be titles are filtered by these rules. Titles with 
strong evidences like ?? ? ? are also identified by these rules. The rest undecided 
strings are denoted as ?possible titles.? To verify these candidates is somewhat similar 
to solve word sense disambiguation problem. Na?ve Bayesian classifier is adopted to 
tell whether a candidate is really a title or not. The overview of our system is shown 
in Figure 1. 
 
Fig. 1. System Overview 
 
Fig. 2. Decision Tree of Punctuation Rules and Lexicon 
Figure 2 shows the applications of the punctuation rules and the title lexicon, 
which are illustrated as a decision tree. HR1 exploits French quotes ???? to identify 
titles like ??????? (Taohua Shan, a traditional Chinese drama by Kong, Shang-
Ren) and ?????????? (El General En Su Laberinto, a novel by Garcia 
Marquez). HR2a and HR2b then look up the title lexicon to find famous titles like ??
???? (Cien Anos de Soledad) and ?????? (Romance of Three Kingdoms). 
HR3 limits our recognition scope to strings quoted in quotation marks, and HR4 and 
842 C. Chen and H.-H. Chen 
HR5 filter out a major sort of non-titles quoted in quotation marks, dialogues, such as 
????????????? (I said, ?the audience should be careful!?) and ????
???????????? (Rene Descarte said, ?I think, therefore I am.?). 
The title lexicon we use is acquired from the catalogues of the library of our uni-
versity. These titles are sent to Google as query terms. Only the ones that have ever 
been quoted in ?? ? ? in the first 1,000 returned summaries are kept. The remained 
titles are checked manually and those ones that possibly form a fragment of a com-
mon sentence are dropped to avoid false alarms. After filtering, there are about 7,200 
entries in this lexicon. Although the lexicon could cover titles of books only, it is still 
useful because books are the major sort of creations.  
The punctuation rules and lexicon divide all the strings of a document into three 
groups ? say, titles, non-titles, and possible titles. All strings that cannot be definitely 
identified by the punctuation rules and lexicon are marked as ?possible? titles. These 
possible titles are then verified by the second mechanism, the na?ve Bayesian model. 
The na?ve Bayesian model will be specified in the next section. 
4   Na?ve Bayesian Model 
Na?ve Bayesian classifier is widely used in various classification problems in natural 
language processing. Since it is simple to implement and easy to train, we adopt it in 
our system to verify the possible titles suggested by the decision tree.  
Na?ve Bayesian classification is based on the assumption that each feature being 
observed is independent of one another. The goal is to find the hypothesis that would 
maximize the posterior probability P(H|F), where H denotes the classifying hypothe-
ses and F denotes the features that determine H. According to Bayesian rule, the pos-
terior probability can be rewritten as: 
P(H | F) = P(H) P(F | H) / P(F) (1) 
Since P(F) is always the same under different hypotheses, we only need to find which 
hypothesis would obtain the maximal value of P(H)P(F|H). Besides, under the inde-
pendence assumption, Equation (1) is rewritten into: 
P(H | F) = P(H) ? P( fi | H)     where F = { f1, f2,?, fn } (2) 
In our system, we have two hypotheses: 
H1: candidate S is a title 
H2: candidate S is not a title 
Four features shown below will be considered. The detail will be discussed in the 
subsequent paragraphs. 
F1: Context  
F2: Component 
F3: Length 
F4: Recurrence 
 Integrating Punctuation Rules and Na?ve Bayesian Model 843 
Context. To exploit contextual features, our system adopts a word-based, position-
free unigram context model with a window size of 5. In other words, our context 
model can be viewed as a combination of ten different contextual features of the na?ve 
Bayesian classifier, five of them are left context and the other five are right context. It 
can be represented as:  
P(Fcontext|H) = P(L5, L4, L3, L2, L1, R1, R2, R3, R4, R5 | H) (3) 
Where Li and Ri denote preceding and following words of the possible title we want to 
verify, and H denotes the hypothesis.   
If we postulate that the contextual features are independent of each other, then 
equation (3) can be transformed to: 
P(Fcontext|H) = ? P(Li |H) ? P(Ri | H) (4) 
Equation (4) assumes that the distance from a contextual word to a possible title is 
not concerned both in training and testing. The reason is that we do not have a realis-
tic, vast, and well-tagged resource for training. On the other hand, if we want to ex-
ploit it in testing, we need a well-tagged corpus to learn the best weights we should 
assign to contextual words of different distances. 
Component. Context deals with features surroundings titles. In contrast, Component 
further considers the features within titles.  Similar to the above discussion, our 
component model is also a word-based, position-free unigram model. A possible title 
will be segmented into a word sequence by standard maximal matching. The words in 
the segmentation results are viewed as the ?components? of the possible title, and the 
component model can be represented as: 
P(Fcomp|H) = P(C1?Cn | H) = ? P(Ci | H) (5) 
Where Ci denotes the component of the possible title we want to verify, and H de-
notes the hypothesis. 
Similar to the context model, the position of a component word is not concerned 
both in training and testing. Besides the availability issue of large training corpus, the 
lengths of possible titles are varied so that positional information is difficult to be 
exploited. Different titles consist of different number of component words. There are 
no straightforward or intuitive ways of using positional information. 
Length. The definition of Length feature is the number of characters that constitute 
the possible title. It can be represented as:  
P(Flength|H) = P(the length of S | H) (6) 
Where S denotes the possible title to be verified and H denotes the hypothesis that S 
is a title. 
Recurrence. The definition of Recurrence feature is number of occurrences of the 
possible title in the input document. It can be represented as: 
P(FRec|H) = P(the appearing times of S | H) (7) 
Where S denotes the possible title to be verified and H denotes the hypothesis that S 
is a title. 
844 C. Chen and H.-H. Chen 
5   Experiment Results 
The estimation of P(H) and P(F|H) is the major issue in na?ve Bayesian model. There 
are no corpora with titles being tagged available. To overcome this problem, we used 
two different resources in our training process. The first one is a collection of about 
300,000 titles, which is acquired from library catalogues of our university. This col-
lection is used to estimate Component and Length features of titles. Besides, these 
titles are regarded as queries and submitted to Google. The returned summaries are 
segmented by maximal matching and then used to estimate Context features of titles. 
Since titles are usually composed of common words, not all query terms in retrieved 
results by Google are a title. Therefore, only the results with query terms quoted in 
French quotes ???? are adopted, which include totally 1,183,451 web page summa-
ries. Recall that French quotes are a powerful cue to recognize creation titles, which 
was discussed in Section 2. 
The second resource used in training is ASBC corpus. Since titles in ASBC corpus 
are not specially tagged and we are short-handed to tag them by ourselves, a compro-
mised approach is adopted. First, the decision tree shown in Figure 2 is used to group 
all strings of the training corpus into titles, non-titles, and possible titles. All titles thus 
extracted are used to estimate the Recurrence feature of titles, and all possible titles 
are treated as non-titles to estimate all features of non-titles. Since the probability of 
possible titles being titles are much less than being non-titles, the bias of the rough 
estimation is supposed to be tolerable.  
We separate one-tenth of ASBC corpus and tag it manually as our testing data. The 
rest nine-tenth is used for training. There are totally 610,760 words in this piece of 
data, and 982 publication or creation titles are found. During execution of our system, 
the testing data are segmented by maximal matching to obtain context and component 
words of possible titles. To estimate P(H), we randomly select 100 possible titles 
from the training part of ASBC corpus, and classify them into titles and non-titles 
manually. Then we count the probability of hypotheses from this small sample to 
approximate P(H).  
Table 2 shows the performance of the decision tree proposed in Figure 2 under the 
testing data. If we treat HR2a and HR2b as a single rule that asks ?Is the string an 
entry in the title lexicon and not in a general dictionary??, we could view our rules as 
an ordered sequence of decisions. Each rule tells if a part of undecided strings are 
titles or non-titles, which is denoted in the column ?Decision Type? of Table 2. The 
column ?Decided? shows how many strings can be decided by the corresponding 
rules, while the columns of ?Undecided Titles? and ?Undecided Non-Titles? denote 
how many titles and non-titles are remained in the testing data after applying the cor-
responding rule. The correctness of the decision is denoted in the columns of ?Cor-
rect? and ?Wrong?. 
Table 2 shows that these five rules are very good clues to recognize titles. HR1, 
HR2, HR4 and HR5 have precisions of 100%, 94.01%, 99.15%, and 100% respec-
tively. Because the number of non-titles is much larger than that of titles, the actual 
precision of HR3 is comparatively meaningless. These rules could efficiently solve a 
large part of the problem. The rest possible titles are then classified by the na?ve 
Bayesian classifier. The performance is listed in Table 3.  We try different combina-
tions of the four features.  F1, F2, F3, and F4 denote Context, Component, Length, 
 Integrating Punctuation Rules and Na?ve Bayesian Model 845 
and Recurrence, respectively.  The number of True Positives, True Negatives, and 
False Positives are listed.  Precision, recall and F-measure are considered as metrics to 
evaluate the performance.  
Table 2. Performance of Decision Tree in Figure 2 
 Decision 
Type Decided Correct Wrong 
Undecided 
Titles 
Undecided 
Non-Titles 
HR1 Title 216 216 0 766 ~|corpus|2/2 
HR2 Title 167 126 411 640 ~|corpus|2/2 
HR3 Non-Title ~|corpus|2/2 ~|corpus|2/2 186 454 5812 
HR4 Non-Title 1997 1980 17 437 3832 
HR5 Non-Title 372 372 0 437 3458 
Note that there are two different numbers in the False Positive, Precision, and F-
measure columns in Table 3. The left number shows the total number of false positive 
errors, and the right one ignores the errors caused by other sorts of named entities. 
This is because many false positive errors come from other types of named entities. 
For example, in the sentence ???????????????????????
??? (attend 1994 35th International Mathematical Olympiad), ?????????
????????????? (?1994 35th International Mathematical Olympiad?) is 
a contest name, however, ill-recognized as a title by our system. Because there are 
various sorts of ill-recognized named entities and most of them have not been thor-
oughly studied, there are no efficient ways available to solve these false alarms. For-
tunately, in many applications, there would be little harm incorrectly recognizing 
these named entities as titles.  
The other major source of false positive errors is appearances of monosyllabic 
words. For example, in the sentence ????????????? (?Render Good for 
Evil? is Lao Tzu?s speech), ?????? (?Render Good for Evil?) are ill-recognized 
as titles. The reason might be that many context and component words of titles are 
named entities or unknown words. During training, these named entities are neither 
tagged nor recognized, so that most of these named entities are segmented into se-
quences of monosyllabic words. Therefore, while the na?ve Bayesian classifier en-
counters monosyllabic context or component words, it would prefer recognizing the 
possible title as a title.  
From Table 3, we could observe that Context and Component are supportive in 
both precision and recall. Length boosts precision but decreases recall while Recur-
rence is on the contrary. The combination of F1+F2+F3 obtains the best F-measure, 
but the combination of all features might be more useful in practical applications, 
                                                          
1
  Total 31 of them can be easily corrected by a maximal-matching-driven segmentation. For 
example, ???? (x?n j?ng, Heart Sutra, a Buddha book) in ?????? (y?ng x?n j?ng y?ng) 
is an entry in the title lexicon. However, maximal matching prefers the segmentation of ??
? / ??? (y?ng x?n/j?ng y?ng) than ?? / ?? / ?? (y?ng/x?n j?ng/y?ng), so that this false 
alarm would be recovered. 
846 C. Chen and H.-H. Chen 
since it only sacrifices 1.4% of precision but gains 3% of recall in comparison with 
the former. Table 4 summaries the total performance of our creation title recognition 
system.  It achieves the F-measure of 0.585. 
Table 3. Performance of the Na?ve Bayesian Classifier Using Different Features 
  
True  
Positive 
True  
Negative
False  
Positive Precision Recall F-measure 
F1 277 160 959 / 772 0.224 / 0.264 0.634 0.331 / 0.373 
F2 153 284 532 / 332 0.223 / 0.315 0.350 0.273 / 0.332 
F1 + F3 273 164 859 / 676 0.241 / 0.288 0.625 0.348 / 0.394 
F2 + F3 148 289 453 / 247 0.246 / 0.375 0.339 0.285 / 0.356 
F1 + F2 288 149 976 / 722 0.228 / 0.285 0.659 0.339 / 0.398 
F1 + F4 289 148 1067 / 867 0.213 / 0.250 0.661 0.322 / 0.363 
F2 + F4 169 268 695 / 467 0.196 / 0.266 0.387 0.260 / 0.315 
F1 + F2 + F3 286 151 888 / 631 0.244 / 0.312 0.654 0.355 / 0.422 
F1 + F3 + F4 285 152 946 / 750 0.232 / 0.275 0.652 0.342 / 0.387 
F2 + F3 + F4 164 273 542 / 320 0.232 / 0.339 0.375 0.287 / 0.356 
All 299 138 967 / 703 0.236 / 0.298 0.684 0.351 / 0.416 
Table 4. Performance of the Title Recognition System 
 
True 
Positive 
True 
Negative
False  
Positive Precision Recall F-measure 
Decision Tree 342 203   41 / 10 0.915 / 0.978 0.685 0.783 / 0.806 
Na?ve Bayes-
ian 299 138  967 / 703 0.236 / 0.298 0.684 0.351 / 0.416 
Total 641 341 1008 / 713 0.424 / 0.510 0.685 0.524 / 0.585 
6   Conclusion 
This paper presents a pioneer study of Chinese title recognition.  It achieves the preci-
sion of 0.510 and the recall of 0.685.  The experiments reveal much valuable informa-
tion and experiences for further researches. 
First, the punctuation rules proposed in this paper are useful to recognize creation 
titles with a high precision. They can relief our burdens in building more resources, 
make supervised learning feasible, and give us some clues in similar studies like rec-
ognition of other sorts of named entities. These useful rules are also helpful for those 
applications needing high accuracies. For example, we can exploit these rules on an 
information retrieval system to filter out noises and show only the information about 
the requested creation or the publication. 
Second, na?ve Bayesian classifier could achieve a comparable recall on the verifi-
cation of possible titles. Since we only adopt simple features and use a rough estima-
tion in feature model building, the result shows that na?ve Bayesian classifier is  
 Integrating Punctuation Rules and Na?ve Bayesian Model 847 
practicable in recognizing creation titles. In future works, we may find other useful  
features and adopt more sophisticated models in na?ve Bayesian classifier to seek a 
higher performance, especially in precision.  
Third, our result shows that recognizing rarely seen sorts of named entities is 
practicable. Because un-recognized named entities might significantly affect subse-
quent applications in Chinese, in particular, segmentation, we should not ignore the 
problems introduced by Non-MUC style named entities. Our study suggests that the 
recognition of these rarely mentioned named entities is promising. The perform-
ances of many applications, such as natural language parsing and understanding, 
might be boosted through adding the mechanism of recognizing these rare  
named entities. 
Finally, our research can also be extended to other oriental languages, such as 
Japanese, in which there are no explicit features like specialized delimiters or capitali-
zations to mark creation titles. Just as Chinese, un-recognized named entities in these 
languages might affect the performances of natural language applications. Recogniz-
ing Non-MUC style named entities is an indispensable task to process these  
languages. 
Acknowledgement 
Research of this paper was partially supported by National Science Council, Taiwan, 
under the contract NSC94-2752-E001-001-PAE. 
References 
1. Chen, Conrad and Lee, Hsi-Jian. 2004. A Three-Phase System for Chinese Named Entity 
Recognition, Proceedings of ROCLING XVI, 2004, 39-48. 
2. Chen, Hsin-Hsi, Ding, Yung-Wei and Tsai, Shih-Chung. 1998. Named Entity Extraction 
for Information Retrieval, Computer Processing of Oriental Languages, Special Issue on 
Information Retrieval on Oriental Languages, 12(1), 1998, 75-85. 
3. Chen, Hsin-Hsi, Kuo, June-Jei, Huang, Sheng-Jie, Lin, Chuan-Jie and Wung, Hung-Chia. 
2003. A Summarization System for Chinese News from Multiple Sources, Journal of 
American Society for Information Science and Technology, 54(13), November 2003, 
1224-1236. 
4. Chen, Zheng, W. Y. Liu, and F. Zhang. 2002. A New Statistical Approach to Personal 
Name Extraction, Proceedings of ICML 2002, 67-74. 
5. Gong, Chian-Yian and Liu, Yi-Ling. 1996. Use of Punctuation Mark. GB/T15834-1995. 
http://202.205.177.129/moe-dept/yuxin-/content/gfbz/ managed/020.htm 
6. Lee, Joo-Young, Song, Young-In, Kim, Sang-Bum, Chung, Hoojung and Rim, Hae-
Chang. 2004. Title Recognition Using Lexical Pattern and Entity Dictionary, Proceedings 
of AIRS04, 342-348. 
7. Lin, Chuan-Jie, Chen, Hsin-Hsi, Liu, Che-Chia, Tsai, Ching-Ho and Wung, Hung-Chia. 
2001. Open Domain Question Answering on Heterogeneous Data, Proceedings of ACL 
Workshop on Human Language Technology and Knowledge Management, July 6-7 2001, 
Toulouse France, 79-85. 
848 C. Chen and H.-H. Chen 
8. MUC7. 1998. Proceedings of 7th Message Understanding Conference, Fairfax, VA, 1998, 
http://www.itl.nist.gov/iaui/894.02/related_projects/muc/index.html. 
9. Sakine, Satoshi and Nobata, Chikashi. 2004.  Definition, Dictionaries and Tagger for Ex-
tended Named Entity Hierarchy, Proceedings of LREC04. 
10. Sun, Jian, J. F. Gao, L. Zhang, M. Zhou, and C. N. Huang. 2002. Chinese Named Entity 
Identification Using Class-based Language Model, Proceedings of the 19th International 
Conference on Computational Linguistics, Taipei, 967-973 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 81?88,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A High-Accurate Chinese-English NE Backward Translation System 
Combining Both Lexical Information and Web Statistics 
 
 
Conrad Chen Hsin-Hsi Chen 
Department of Computer Science and Information Engineering, National 
Taiwan University, Taipei, Taiwan 
drchen@nlg.csie.ntu.edu.tw hhchen@csie.ntu.edu.tw 
 
  
 
Abstract 
Named entity translation is indispensable 
in cross language information retrieval 
nowadays. We propose an approach of 
combining lexical information, web sta-
tistics, and inverse search based on 
Google to backward translate a Chinese 
named entity (NE) into English. Our sys-
tem achieves a high Top-1 accuracy of 
87.6%, which is a relatively good per-
formance reported in this area until pre-
sent. 
1 Introduction 
Translation of named entities (NE) attracts much 
attention due to its practical applications in 
World Wide Web. The most challenging issue 
behind is: the genres of NEs are various, NEs are 
open vocabulary and their translations are very 
flexible. 
Some previous approaches use phonetic simi-
larity to identify corresponding transliterations, 
i.e., translation by phonetic values (Lin and Chen, 
2002; Lee and Chang, 2003). Some approaches 
combine lexical (phonetic and meaning) and se-
mantic information to find corresponding transla-
tion of NEs in bilingual corpora (Feng et al, 
2004; Huang et al, 2004; Lam et al, 2004). 
These studies focus on the alignment of NEs in 
parallel or comparable corpora.  That is called 
?close-ended? NE translation. 
In ?open-ended? NE translation, an arbitrary 
NE is given, and we want to find its correspond-
ing translations. Most previous approaches ex-
ploit web search engine to help find translating 
candidates on the Internet. Al-Onaizan and 
Knight (2003) adopt language models to generate 
possible candidates first, and then verify these 
candidates by web statistics. They achieve a Top-
1 accuracy of about 72.6% with Arabic-to-
English translation. Lu et al (2004) use statistics 
of anchor texts in web search result to identify 
translation and obtain a Top-1 accuracy of about 
63.6% in translating English out-of-vocabulary 
(OOV) words into Traditional Chinese. Zhang et 
al. (2005) use query expansion to retrieve candi-
dates and then use lexical information, frequen-
cies, and distances to find the correct translation. 
They achieve a Top-1 accuracy of 81.0% and 
claim that they outperform state-of-the-art OOV 
translation techniques then. 
In this paper, we propose a three-step ap-
proach based on Google to deal with open-ended 
Chinese-to-English translation. Our system inte-
grates various features which have been used by 
previous approaches in a novel way.  We observe 
that most foreign Chinese NEs would have their 
corresponding English translations appearing in 
their returned snippets by Google. Therefore we 
combine lexical information and web statistics to 
find corresponding translations of given Chinese 
foreign NEs in returned snippets. A highly effec-
tive verification process, inverse search, is then 
adopted and raises the performance in a signifi-
cant degree. Our approach achieves an overall 
Top-1 accuracy of 87.6% and a relatively high 
Top-4 accurracy of 94.7%.   
2 Background 
Translating NEs, which is different from translat-
ing common words, is an ?asymmetric? transla-
tion. Translations of an NE in various languages 
can be organized as a tree according to the rela-
tions of translation language pairs, as shown in 
Figure 1. The root of the translating tree is the 
NE in its original language, i.e., initially de-
81
nominated. We call the translation of an NE 
along the tree downward as a ?forward transla-
tion?. On the contrary, ?backward translation? is 
to translate an NE along the tree upward. 
 
Figure 1. Translating tree of ?Cien a?os soledad?. 
Generally speaking, forward translation is eas-
ier than backward translation. On the one hand, 
there is no unique answer to forward translation. 
Many alternative ways can be adopted to forward 
translate an NE from one language to another. 
For example, ?Jordan? can be translated into ??
?  (Qiao-Dan)?, ???  (Qiao-Deng)?, ??? 
(Yue-Dan)?, and so on. On the other hand, there 
is generally one unique corresponding term in 
backward translation, especially when the target 
language is the root of the translating tree.  
In addition, when the original NE appears in 
documents in the target language in forward 
translation, it often comes together with a corre-
sponding translation in the target language 
(Cheng et al, 2004). That makes forward transla-
tion less challenging. In this paper, we focus our 
study on Chinese-English backward translation, 
i.e., the original language of NE and the target 
language in translation is English, and the source 
language to be translated is Chinese.  
There are two important issues shown below 
to deal with backward translation of NEs or 
OOV words.  
? Where to find the corresponding translation? 
? How to identify the correct translation?  
NEs seldom appear in multi-lingual or even 
mono-lingual dictionaries, i.e., they are OOV or 
unknown words. For unknown words, where can 
we find its corresponding translation? A bilin-
gual corpus might be a possible solution. How-
ever, NEs appear in a vast context and bilingual 
corpora available can only cover a small propor-
tion. Most text resources are monolingual. Can 
we find translations of NEs in monolingual cor-
pora? While mentioning a translated name during 
writing, sometimes we would annotate it with its 
original name in the original foreign language, 
especially when the name is less commonly 
known. But how often would it happen? With 
our testing data, which would be introduced in 
Section 4, over 97% of translated NEs would 
have its original NE appearing in the first 100 
returned snippets by Google. Figure 2 shows 
several snippets returned by Google which con-
tains the original NE of the given foreign NE.  
Figure 2. Several Traditional Chinese snippets of 
?????? returned by Google which contains 
the translation ?The Old Man and the Sea?. 
When translations can be found in snippets, 
the next work would be identifying which name 
is the correct translation of NEs. First we should 
know how NEs would be translated. The com-
monest case is translating by phonetic values, or 
so-called transliteration. Most personal names 
and location names are transliterated. NEs may 
also be translated by meaning. It is the way in 
which most titles and nicknames and some or-
ganization names would be translated. Another 
common case is translating by phonetic values 
for some parts and by meaning for the others. For 
example, ?Sears Tower? is translated into ???
? (Xi-Er-Si) ? ? (tower)? in Chinese. NEs 
would sometimes be translated by semantics or 
contents of the entity it indicates, especially with 
movies. Table 1 summarizes the possible trans-
lating ways of NEs. From the above discussion, 
we may use similarities in phonetic values, 
meanings of constituent words, semantics, and so 
CEPS ???-- ????;-1  
??, ???????????????????. ???
?, Symbolic Means of the Author "The Old Man and the 
Sea" ... ??, ??????????????????? 
??????????????????????????
???????????? ... 
www.ceps.com.tw/ec/ecjnlarticleView.aspx?jnlcattype=1& 
jnlptype=4&jnltype=29&jnliid=1370&i... - 26k - ???? - ?
??? 
 
.:JSDVD Mall:. ????-????  
????-???? ? ?????-????(DTS) ? ????
??? ? ?????? 16-?? ? ?? ? ????? ? ???
?-????? ? ????-??? ... ????-????. The 
Old Man and The Sea. 4715320115018, ????????
? ... 
mall.jsdvd.com/product_info.php?products_id=3198 - 48k - ?
??? - ???? - ????  
82
on to identify corresponding translations. Besides 
these linguistic features, non-linguistic features 
such as statistical information may also help use 
well. We would discuss how to combine these 
features to identify corresponding translation in 
detail in the next section.  
3 Chinese-to-English NE Translation 
As we have mentioned in the last section, we 
could find most English translations in Chinese 
web page snippets. We thus base our system on 
web search engine: retrieving candidates from 
returned snippets, combining both linguistic and 
statistical information to find the correct transla-
tion. Our system can be split into three steps: 
candidate retrieving, candidate evaluating, and 
candidate verifying. An overview of our system 
is given in Figure 3.  
 
Figure 3. An Overview of the System. 
In the first step, the NE to be translated, GN, 
is sent to Google to retrieve traditional Chinese 
web pages, and a simple English NE recognition 
method and several preprocessing procedures 
are applied to obtain possible candidates from 
returned snippets. In the second step, four fea-
tures (i.e., phonetic values, word senses, recur-
rences, and relative positions) are exploited to 
give these candidates a score. In the last step, the 
candidates with higher scores are sent to Google 
again. Recurrence information and relative posi-
tions concerning with the candidate to be veri-
fied of GN in returned snippets are counted 
along with the scores to decide the final ranking 
of candidates. These three steps will be detailed 
in the following subsections. 
3.1 Retrieving Candidates 
Before we can identify possible candidates, we 
must retrieve them first. In the returned tradi-
tional Chinese snippets by Google, there are still 
many English fragments. Therefore, the first 
task our system would do is to separate these 
English fragments into NEs and non-NEs. We 
propose a simple method to recognize possible 
NEs. All fragments conforming to the following 
properties would be recognized as NEs: 
? The first and the last word of the fragment 
are numerals or capitalized. 
? There are no three or more consequent low-
ercase words in the fragment. 
? The whole fragment is within one sentence. 
After retrieving possible NEs in returned snip-
pets, there are still some works to do to make a 
Translating Way Description Examples 
Translating by Pho-
netic Values 
The translation would have a similar 
pronunciation to its original NE. 
 ?New York? and ???(pronounced as Niu-
Yue)? 
Translating by Mean-
ing 
The translation would have a similar or a 
related meaning to its original NE. 
?? (red)? (chamber)? (dream)? and ?The 
Dream of the Red Chamber? 
Translating by Pho-
netic Values for Some 
Parts and by Meaning 
for the Others 
The entire NE is supposed to be trans-
lated by its meaning and the name parts 
are transliterated. 
 ?Uncle Tom?s Cabin? and ???(pronounced 
as Tang-Mu)???(uncle?s)??(cabin)? 
Translating by Both 
Phonetic Values and 
Meaning 
The translation would have both a similar 
pronunciation and a similar meaning to 
its original NE. 
?New Yorker? and ???(pronounced as Niu-
Yue)?(people, pronounced as Ke)? 
Translating NEs by 
Heterography 
The NE is translated by these hetero-
graphic words in neighboring languages. 
???? and ?Yokohama?, ?????? and 
?Ichiro Suzuki? 
Translating by Se-
mantic or Content 
The NE is translated by its semantic or 
the content of the entity it refers to. 
?The Mask? and ??? (modern)? (great)?
(saint)? 
Parallel Names NE is initially denominated as more than 
one name or in more than one language. 
????(Sun Zhong-Shan)? and ?Sun Yat-Sen? 
Table 1. Possible translating ways of NEs. 
83
finer candidate list for verification. First, there 
might be many different forms for a same NE. 
For example, ?Mr. & Mrs. Smith? may also ap-
pear in the form of ?Mr. and Mrs. Smith?, ?Mr. 
And Mrs. Smith?, and so on. To deal with these 
aliasing forms, we transform all different forms 
into a standard form for the later ranking and 
identification. The standard form follows the 
following rules: 
? All letters are transformed into upper cases. 
? Words consist ???s are split. 
? Symbols are rewritten into words. 
For example, all forms of ?Mr. & Mrs. Smith? 
would be transformed into ?MR. AND MRS. 
SMITH?. 
The second work we should complete before 
ranking is filtering useless substrings. An NE 
may comprise many single words. These com-
ponent words may all be capitalized and thus all 
substrings of this NE would be fetched as candi-
dates of our translation work. Therefore, sub-
strings which always appear with a same preced-
ing and following word are discarded here, since 
they would have a zero recurrence score in the 
next step, which would be detailed in the next 
subsection. 
3.2 Evaluating Candidates 
After candidate retrieving, we would obtain a 
sequence of m candidates, C1, C2, ?, Cm. An 
integrated evaluating model is introduced to ex-
ploit four features (phonetic values, word senses, 
recurrences, and relative positions) to score 
these m candidates, as the following equation 
suggests: 
),(),(
),(
GNCLScoreGNCSScore
GNCScore
ii
i
?
=
 
LScore(Ci,GN) combines phonetic values and 
word senses to evaluate the lexical similarity 
between Ci and GN. SScore(Ci,GN) concerns 
both recurrences information and relative posi-
tions to evaluate the statistical relationship be-
tween Ci and GN. These two scores are then 
combined to obtain Score(Ci,GN). How to esti-
mate LScore(Cn, GN) and SScore(Cn, GN) would 
be discussed in detail in the following subsec-
tions. 
3.2.1 Lexical Similarity 
The lexical similarity concerns both phonetic 
values and word senses. An NE may consist of 
many single words. These component words 
may be translated either by phonetic values or 
by word senses. Given a translation pair, we 
could split them into fragments which could be 
bipartite matched according to their translation 
relationships, as Figure 4 shows.  
 
Figure 4. The translation relationships of ???
??????. 
To identify the lexical similarity between two 
NEs, we could estimate the similarity scores be-
tween the matched fragment pairs first, and then 
sum them up as a total score. We postulate that 
the matching with the highest score is the correct 
matching. Therefore the problem becomes a 
weighted bipartite matching problem, i.e., given 
the similarity scores between any fragment pairs, 
to find the bipartite matching with the highest 
score. In this way, our next problem is how to 
estimate the similarity scores between fragments.  
We treat an English single word as a fragment 
unit, i.e., each English single word corresponds 
to one fragment. An English candidate Ci con-
sisting of n single words would be split into n 
fragment units, Ci1, Ci2, ?, Cin. We define a Chi-
nese fragment unit that it could comprise one to 
four characters and may overlap each other. A 
fragment unit of GN can be written as GNab, 
which denotes the ath to bth characters of GN, 
and b - a < 4. The linguistic similarity score be-
tween two fragments is:  
)},(),,({
),(
ijabijab
ijab
CGNWSSimCGNPVSimMax
CGNLSim =
 
Where PVSim() estimates the similarity in pho-
netic values while WSSim() estimate it in word 
senses.  
 Phonetic Value 
In this paper, we adopt a simple but novel 
method to estimate the similarity in phonetic 
values. Unlike many approaches, we don?t in-
troduce an intermediate phonetic alphabet sys-
tem for comparison. We first transform the Chi-
nese fragments into possible English strings, and 
then estimate the similarity between transformed 
strings and English candidates in surface strings, 
as Figure 5 shows. However, similar pronuncia-
tions does not equal to similar surface strings. 
Two quite dissimilar strings may have very simi-
lar pronunciations. Therefore, we take this strat-
84
egy: generate all possible transformations, and 
regard the one with the highest similarity as the 
English candidate. 
 
Figure 5. Phonetic similarity estimation of our 
system. 
Edit distances are usually used to estimate the 
surface similarity between strings. However, the 
typical edit distance does not completely satisfy 
the requirement in the context of translation 
identification. In translation, vowels are an unre-
liable feature. There are many variations in pro-
nunciation of vowels, and the combinations of 
vowels are numerous. Different combinations of 
vowels may have a same phonetic value, how-
ever, same combinations may pronounce totally 
differently. The worst of all, human often arbi-
trarily determine the pronunciation of unfamiliar 
vowel combinations in translation. For these rea-
sons, we adopt the strategy that vowels can be 
ignored in transformation. That is to say when it 
is hard to determine which vowel combination 
should be generated from given Chinese frag-
ments, we can only transform the more certain 
part of consonants. Thus during the calculation 
of edit distances, the insertion of vowels would 
not be calculated into edit distances. Finally, the 
modified edit distance between two strings A 
and B is defined as follow: 
??
? =
=
??
?
=
??
??
?
??
??
?
+??
+?
+?
=
=
=
?
?
?
?
?
?
else
BAif
tsRep
consonantaisBif
vowlaisBif
tIns
tsReptsED
tsED
tInstsED
tsED
ssED
ttED
ts
t
t
BA
BA
BA
BA
BA
BA
,1
,0),(
,1
,0)(
),()1,1(
,1),1(
),()1,(
min),(
)0,(
),0(
 
The modified edit distances are then transformed 
to similarity scores: 
)}(),(max{
))(),((1),(
BLenALen
BLenALenEDBAPVSim BA??=  
Len() denotes the length of the string. In the 
above equation, the similarity scores are ranged 
from 0 to 1. 
We build the fixed transformation table manu-
ally. All possible transformations from Chinese 
transliterating characters to corresponding Eng-
lish strings are built. If we cannot precisely indi-
cate which vowel combination should be trans-
formed, or there are too many possible combina-
tions, we ignores vowels. Then we use a training 
set of 3,000 transliteration names to examine 
possible omissions due to human ignorance.  
 Word Senses 
More or less similar to the estimation of pho-
netic similarity, we do not use an intermediate 
representation of meanings to estimate word 
sense similarity. We treat the English transla-
tions in the C-E bilingual dictionary (reference 
removed for blind review) directly as the word 
senses of their corresponding Chinese word en-
tries. We adopt a simple 0-or-1 estimation of 
word sense similarity between two strings A and 
B, as the following equation suggests: 
???
???
?
=
dictionary in the
  ofon  translatia is  if ,1
dictionary in the    
  ofon  translatianot  is  if,0
),( AB
AB
BAWSSim  
All the Chinese foreign names appearing in test 
data is removed from the dictionary. 
From the above equations we could derive 
that LSim() of fragment pairs is also ranged from 
0 to 1. Candidates to be evaluated may comprise 
different number of component words, and this 
would result the different scoring base of the 
weighted bipartite matching. We should normal-
ize the result scores of bipartite matching. As a 
result, the following equation is applied: 
?
?
?
???
?
?
?
?
???
?
+??
=
?
?
GN
abCGNLSim
C
CGNLSim
GNCLScore
ijab
ijab
CGN ijab
i
CGN ijab
i
in  characters of # Total
 )1(),(
,
in   wordsof # Total
),(
min
),(
 and  pairs matched all
 and  pairs matched all  
3.2.2 Statistical Similarity 
Two pieces of information are concerned to-
gether to estimate the statistical similarity: recur-
rences and relative positions. A candidate Ci 
might appear l times in the returned snippets, as 
Ci,1, Ci,2, ?, Ci,l. For each Ci,k, we find the dis-
85
tance between it and the nearest GN in the re-
turned snippets, and then compute the relative 
position scores as the following equation: 
? ? 14/),(
1),(
,
,
+
=
ki
ki CGNDistance
GNCRP  
In other words, if the candidate is adjacent to the 
given NE, it would have a relative position score 
of 1. Relative position scores of all Ci,k would be 
summed up to obtain the primitive statistical 
score: 
PSS(Ci, GN) = ?k RP(Cn,k, GN) 
As we mentioned before, since the impreci-
sion of NE recognition, most substrings of NEs 
would also be recognized as candidates. This 
would result a problem. There are often typos in 
the information provided on the Internet. If some 
component word of an NE is misspelled, the 
substrings constituted by the rest words would 
have a higher statistical score than the correct 
NE. To prevent such kind of situations, we in-
troduce entropy of the context of the candidate. 
If a candidate has a more varied context, it is 
more possible to be an independent term instead 
of a substring of other terms. Entropy provides 
such a property: if the possible cases are more 
varied, there is higher entropy, and vice versa. 
Entropy function here concerns the possible 
cases of the most adjacent word at both ends of 
the candidate, as the following equation suggests: 
??
??
?
??
=
=
?
i iCT irNPTir
i
NCNCTNCNCT
CEntropy
else ,/log/
1context  possible of #  while,1
) ofContext (
 
Where NCTr and NCi denote the appearing times 
of the rth context CTr and the candidate Ci in the 
returned snippets respectively, and NPTi denotes 
the total number of different cases of the context 
of Ci. Since we want to normalize the entropy to 
0~1, we take NPTi as the base of the logarithm 
function. 
While concerning context combinations, only 
capitalized English word is discriminated. All 
other words would be viewed as one sort 
?OTHER?. For example, assuming the context 
of ?David? comprises three times of (Craig, 
OTHER), three times of (OTHER, Stern), and 
six times of (OTHER, OTHER), then: 
946.0)
12
6log
12
6
12
3log
12
3
12
3log
12
3(
)David"" ofContext (
333 =?+?+?
=Entropy
 
Next we use Entropy(Context of Ci) to weight 
the primitive score PSS(Ci, GN) to obtain the 
final statistical score.: 
)() ofContext (
)(
,GNCPSSCEntropy
,GNCSScore
ii
i
?
=
 
3.3 Verifying Candidates 
In evaluating candidate, we concern only the 
appearing frequencies of candidates when the 
NE to be translated is presented. In the other 
direction, we should also concern the appearing 
frequencies of the NE to be translated when the 
candidate is presented to prevent common words 
getting an improper high score in evaluation. We 
perform the inverse search approach for this 
sake. Like the evaluation of statistical scores in 
the last step, candidates are sent to Google to 
retrieve Traditional Chinese snippets, and the 
same equation of SScore() is computed concern-
ing the candidate. However, since there are too 
many candidates, we cannot perform this proc-
ess on all candidates. Therefore, an elimination 
mechanism is adopted to select candidates for 
verification. The elimination mechanism works 
as follows: 
1. Send the Top-3 candidates into Google for 
verification. 
2. Count SScore(GN, Ci). (Notice that the or-
der of the parameter is reversed.) Re-weight 
Score(Ci, GN) by multiplying SScore(GN, 
Ci) 
3. Re-rank candidates 
4. After re-ranking, if new candidates become 
the Top-3 ones, redo the first step. Other-
wise end this process. 
The candidates have been verified would be re-
corded to prevent duplicate re-weighting and 
unnecessary verification.  
There is one problem in verification we 
should concern. Since we only consider recur-
rence information in both directions, but not co-
occurrence information, this would result some 
problem when dealing rarely used translations. 
For example, ?Peter Pan? can be translated into 
????? or ????? (both pronounced as Bi-
De-Pan) in Chinese, but most people would use 
the former translation. Thus if we send ?Peter 
Pan? to verification when translating ?????, 
we would get a very low score.  
To deal with this situation, we adopt the strat-
egy of disbelieving verification in some situa-
86
tions. If all candidates have scores lower than 
the threshold, we presume that the given NE is a 
rarely used translation. In this situation, we use 
only Score(Cn, GN) estimated by  the evaluation 
step to rank its candidates, without multiplying 
SScore(GN, Ci) of the inverse search. The 
threshold is set to 1.5 by heuristic, since we con-
sider that a commonly used translation is sup-
posed to have their SScore() larger than 1 in both 
directions.   
4 Experiments 
To evaluate the performance of our system, 15 
common users are invited to provide 100 foreign 
NEs per user. These users are asked to simulate 
a scenario of using web search machine to per-
form cross-lingual information retrieval. The 
proportion of different types of NEs is roughly 
conformed to the real distribution, except for 
creation titles. We gathers a larger proportion of 
creation titles than other types of NEs, since the 
ways of translating creation titles is less regular 
and we may use them to test how much help 
could the web statistics provide. 
After removing duplicate entries provided by 
users, finally we obtain 1,119 nouns. Among 
them 7 are not NEs, 65 are originated from Ori-
ental languages (Chinese, Japanese, and Korean), 
and the rest 1,047 foreign NEs are our main ex-
perimental subjects. Among these 1,047 names 
there are 455 personal names, 264 location 
names, 117 organization names, 196 creation 
titles, and 15 other types of NEs.  
Table 2 and Figure 5 show the performance of 
the system with different types of NEs. We 
could observe that the translating performance is 
best with location names. It is within our expec-
tation, since location names are one of the most 
limited NE types. Human usually provide loca-
tion names in a very limited range, and thus 
there are less location names having ambiguous 
translations and less rare location names in the 
test data. Besides, because most location names 
are purely transliterated, it can give us some 
clues about the performance of our phonetic 
model.  
Our system performs worst with creation titles. 
One reason is that the naming and translating 
style of creation titles are less formulated. Many 
titles are not translated by lexical information, 
but by semantic information or else. For exam-
ple, ?Mr. & Mrs. Smith? is translated into ???
???(Smiths? Mission)? by the content of the 
creation it denotes. Another reason is that many 
titles are not originated from English, such as ?le 
Nozze di Figaro?. It results the C-E bilingual 
dictionary cannot be used in recognizing word 
sense similarity. A more serious problem with 
titles is that titles generally consist of more sin-
gle words than other types of NEs. Therefore, in 
the returned snippets by Google, the correct 
translation is often cut off. It would results a 
great bias in estimating statistical scores.  
Table 3 compares the result of different fea-
ture combinations. It considers only foreign NEs 
in the test data. From the result we could con-
clude that both statistical and lexical features are 
helpful for translation finding, while the inverse 
search are the key of our system to achieve a 
good performance. 
60%
65%
70%
75%
80%
85%
90%
95%
100%
1 5 9 13 17 21 25 29
Ranking
Re
ca
ll a
t T
OP
 N
PER
LOC
ORG
Title
Other
Oriental
Non-NE
 
Figure 5. Curve of recall versus ranking. 
Top-1 Top-2 Top-4 Top-M 
 Total Num Recall Num Recall Num Recall Num Recall 
PER 455 408 89.7% 430 94.5% 436 95.8% 443 97.3% 
LOC 264 242 91.7% 252 95.5% 253 95.8% 264 100.0% 
ORG 117 98 83.8% 106 90.6% 108 92.3% 114 97.4% 
TITLE 196 151 77.0% 168 85.7% 181 92.3% 189 96.4% 
Other 15 10 66.7% 13 86.7% 14 93.3% 15 100.0% 
All NE 1047 909 87.6% 969 92.6% 992 94.7% 1025 97.9% 
Oriental 65 47 72.3% 52 80.0% 55 84.6% 60 92.3% 
Non-NE 7 6 85.7% 6 85.7% 6 85.7% 7 100.0% 
Overall 1119 962 86.0% 1027 91.8% 1053 94.1% 1092 97.6% 
Table 2. Experiment results of our system with different NE types. 
 
87
Top-1 Top-2 Top-4 
 Num Recall Num Recall Num Recall 
SScore 540 51.6% 745 71.2% 887 84.7% 
LScore 721 68.9% 789 75.4% 844 80.6% 
SScore + LScore 837 79.9% 916 87.5% 953 91.0% 
+ Inverse Search 909 87.6% 969 92.6% 992 94.7% 
Table 3. Experiment results of our system with different feature combinations. 
 
From the result we could also find that our 
system has a high recall of 94.7% while consid-
ering top 4 candidates. If we only count in the 
given NEs with their correct translation appear-
ing in the returned snippets, the recall would go 
to 96.8%. This achievement may be not yet good 
enough for computer-driven applications, but it 
is certainly a good performance for user querying. 
5 Conclusion 
In this study we combine several relatively sim-
ple implementations of approaches that have 
been proposed in the previous studies and obtain 
a very good performance. We find that the Inter-
net is a quite good source for discovering NE 
translations. Using snippets returned by Google 
we can efficiently reduce the number of the pos-
sible candidates and acquire much useful infor-
mation to verify these candidates. Since the 
number of candidates is generally less than proc-
essing with unaligned corpus, simple models can 
performs filtering quite well and the over-fitting 
problem is thus prevented. 
From the failure cases of our system, (see Ap-
pendix A) we could observe that the performance 
of this integrated approach could still be boosted 
by more sophisticated models, more extensive 
dictionaries, and more delicate training mecha-
nisms. For example, performing stemming or 
adopting a more extensive dictionary might en-
hance the accuracy of estimating word sense 
similarity; the statistic formula can be replaced 
by more formal measures such as co-occurrences 
or mutual information to make a more precise 
assessment of statistical relationship. These tasks 
would be our future works in developing a more 
accurate and efficient NE translation system.  
Reference 
Al-Onaizan, Yaser and Kevin Knight. 2002. Translat-
ing Named Entities Using Monolingual and Bilin-
gual Resources. ACL 2002: 400-408. 
Cheng, Pu-Jen, J.W. Teng, R.C. Chen, J.H. Wang, 
W.H. Lu, and L.F. Chien. Translating unknown 
queries with web corpora for cross-language in-
formation retrieval. SIGIR 2004: 146-153. 
Feng, Donghui, Lv Y., and Zhou M. 2004. A New 
Approach for English-Chinese Named Entity 
Alignment. EMNLP 2004: 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 2003. 
Improving Named Entity Translation Combining 
Phonetic and Semantic Similarities. HLT-NAACL 
2004: 281-288. 
Lam, Wai, Ruizhang Huang, and Pik-Shan Cheung. 
2004. Learning phonetic similarity for matching 
named entity translations and mining new transla-
tions. SIGIR 2004: 289-296. 
Lee, Chun-Jen and Jason S. Chang. 2003. Acquisition 
of. English-Chinese Transliterated Word Pairs 
from Parallel-Aligned Texts. HLT-NAACL 2003. 
Workshop on Data Driven MT: 96-103. 
Lin, Wei-Hao and Hsin-Hsi Chen. 2002. Backward 
Machine Transliteration by Learning Phonetic 
Similarity. Proceedings of CoNLL-2002: 139-145. 
Lu, Wen-Hsiang, Lee-Feng Chien, and Hsi-Jian Lee. 
2004. Anchor Text Mining for Translation of Web 
Queries: A Transitive Translation Approach. ACM 
Transactions on Information Systems 22(2): 242-
269. 
Zhang, Ying, Fei Huang, and Stephan Vogel. 2005. 
Mining translations of OOV terms from the web 
through cross-lingual query expansion. SIGIR 
2005: 669-670. 
Zhang, Ying and Phil Vines. 2004. Using the web for 
automated translation extraction in cross-language 
information retrieval. SIGIR 2004: 162-169.  
Appendix A. Some Failure Cases of Our 
System 
GN Top 1  Correct Translation Rank 
?? CBS SADDAM HUSSEIN 2 
??? JERSEY NEW JERSEY 2 
???? ONLINE ARABIAN NIGHTS 2 
???? ROYCE ROLLS ROYCE 2 
????? NBA JULIUS ERVING 2 
??? LAVIGNE AVRIL LAVIGNE 2 
?? JK JK. ROWLING 2 
???? RICKY DAVIS CELTICS 8 
???? MONET IMPRESSION SUNRISE 9 
?? TUPOLEV TU USSR 33 
????? NBA MEDVENDENKO N/A 
????? TOS SYMPHONY NO. 5 N/A 
???? AROUND03 CUORE N/A 
??? JACK LAYTON DEMOCRATIC PARTY N/A 
88
