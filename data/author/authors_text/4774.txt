67
68
69
70
71
72
73
74
75
News-Oriented Automatic Chinese Keyword Indexing 
Li Sujian1 
lisujian@pku.
edu.cn 
Wang Houfeng1 
wanghf@pku.edu.
cn 
Yu Shiwen1 
Yusw@pku.edu.
cn 
Xin Chengsheng2 
csxin@peoplemail.
com.cn 
1Institute of Computational Linguistics, Peking University, 100871 
2The Information Center of PEOPLE?S DAILY, 100733 
 
 
Abstract 
In our information era, keywords are very 
useful to information retrieval, text clus-
tering and so on.  News is always a do-
main attracting a large amount of 
attention.  However, the majority of news 
articles come without keywords, and in-
dexing them manually costs highly.  Aim-
ing at news articles? characteristics and 
the resources available, this paper intro-
duces a simple procedure to index key-
words based on the scoring system.  In the 
process of indexing, we make use of some 
relatively mature linguistic techniques and 
tools to filter those meaningless candidate 
items.  Furthermore, according to the hi-
erarchical relations of content words, 
keywords are not restricted to extracting 
from text. These methods have improved 
our system a lot.  At last experimental re-
sults are given and analyzed, showing that 
the quality of extracted keywords are sat-
isfying. 
1 Introduction 
With more and more information flowing into our 
life, it is very important to lead people to gain 
more important information in time as short as 
possible.  Keywords are a good solution, which 
give a brief summary of a document?s content.  
With keywords, people can quickly find what they 
are most interested in and read them carefully.  
That will save us a lot of time.  In addition, key-
words are also useful to the research of information 
retrieval, text clustering, and topic search [Frank 
1999].  Manually indexing keywords will cost 
highly.  Thus, automatically indexing keywords 
from text is of great interests. 
News is always the main domain that people 
pay a large amount of attention to.  Unfortunately, 
only a small fraction of documents in this field 
have keywords.  However, compared to unre-
stricted text, news articles are relatively easy to 
extract keywords from, because they have the fol-
lowing characteristics.  Firstly, a news document is 
always short in length, and usually, only important 
words or phrases repeat.  Secondly, as a rule, the 
purpose of news articles is to illustrate an event or 
a thing for readers.  Then this kind of articles usu-
ally place more emphasis on some name entities 
such as persons, places, organizations and so on.  
Lastly, important content often occurs the first time 
in the title, or in the anterior part of the whole text, 
especially the first paragraph or the first sentence 
in every paragraph.  These characteristics will help 
us in keywords indexing. 
Several methods have been proposed for ex-
tracting English keywords from text.  For example, 
Witten[1999] adopted Na?ve Bayes techniques, and 
Turney[1999] combined decision trees and genetic 
algorithm in his system.  These systems achieved 
satisfying results. However, they need a large 
amount of training documents with keywords, 
which are just what we are in need of now.  For the 
Chinese language, some researchers adopt the 
structure of PAT tree and make use of mutual in-
formation to obtain keywords [Chien 1997, Yang 
2002].  Unfortunately, the construction of PAT tree 
will cost a lot of space and time.  In this paper, 
aiming at the characteristics of news-oriented arti-
cles, resources and techniques of current situation, 
we will introduce a simple procedure to index 
keywords from text. Section 2 will describe the 
architecture of the whole system.  In section 3, we 
will introduce every module in detail, including 
how to obtain candidate keywords, how to filter 
out the meaningless items, and how to score possi-
ble keyword candidates according to their feature 
values.  In section 4, experimental results will be 
given and analyzed.  At last, we will end with the 
conclusion. 
2 
3 
3.1 
System Overview 
Keyword indexing can also be called keyword ex-
traction.  The definition of a keyword is not re-
stricted to one word in our conception.  Here, a 
keyword can be seen as a Chinese character string, 
which might consist of more than one Chinese 
word.  These character strings can summarize the 
content of the document they are in. 
Aiming at the task of keywords indexing, our 
system is designed and composed of three modules.  
As in figure 1, the first module is to recognize 
some Chinese character strings according to their 
frequency, and pick out those named entities in the 
text as the candidate keywords.  The second mod-
ule is a filter to remove all the meaningless charac-
ter strings from the set of candidates. And the third 
module is a selector, which evaluates every candi-
date according to its feature values and choose 
from the candidate set those keywords with higher 
score.  The higher score a character string has, the 
more content it will cover of the article it is in. 
 In our system, there are three kinds of lexicons. 
The lexicon of proper nouns is used to recognize 
named entities.  The general lexicon includes Chi-
nese words in common use, which is adopted for 
the segmentation and POS tagging of the text.  And 
the lexicon of content words is used to expand the 
set of keywords.  They will be introduced in detail 
in the following section. 
System Design 
Recognizer Module 
Character strings Recognizer
Selector
Filter
Segmentation
filter
Keywords
expansion
Feature
computation
Chinese character
strings
POS tagging
filter
Filter of items with
punctuations and
function words
Filter of overlapped
and dependent items
Recognizer through
Frequency Statistics
Named Entities
Recognizer
Original text
Candidate items with
feature values
Fig.1. System Architecture
keywords
Proper Nouns
Lexicon
Content Words
Lexicon
General
Lexicon
It can be seen that one document is composed of a 
set of character strings.  Every character string has 
its frequency in the document.  In general, those 
character strings that occur several times can re-
flect the topic of the document.  So, we take them 
out as keyword candidates. In addition, named en-
tities, such as person names, place names, organi-
zations, translation terms, titles of person and so on, 
are usually very important for the document with-
out reference to their frequency.  They will also be 
picked out from the text by named entities recog-
nizer and input into the filter module with other 
character strings.  
Unlike English, there are no explicit word 
boundaries in Chinese sentences, which makes it 
especially difficult to tell whether a character 
string is composed of one word or more than one 
word.  Due to this characteristic, we don?t use a 
dictionary, but get those character strings only ac-
cording to their frequency statistics.  We set a 
threshold value as 2 for the Chinese character 
strings considering the length of news documents.  
Suppose that a character string is c1c2?cn, and 
f(c1c2?cn) represents its frequency, then we ex-
tract c1c2?cn from text only if f(c1c2?cn) equals to 
or is more than 2.  That is, only a character string 
occurs two or more than two times, it can be se-
lected as a candidate keyword. 
There are two kinds of named entities.  The first 
are those which have rules of composition, mainly 
Chinese names and foreign terms.  They can be 
recognized with statistical and rule-based methods 
combined.  Chinese names are composed of family 
names and first names, whose lengths are respec-
tively 1 or 2 Chinese characters.  Furthermore, 
there is a relatively stable set of family names, 
which often provide the anchor to search a name. 
For foreign terms, there are a relatively set of Chi-
nese characters which are generally used as 
translation characters.  Due to the limitation of the 
paper?s length, we don?t introduce the process of 
recognition in detail here.  The other kind of 
named entities is mainly composed of proper 
nouns which represent names of places, organiza-
tions, person titles, etc.  They often occur in news 
documents, but don?t have rules of composition.  
Thus, we collect such words into our proper nouns 
lexicon.  Then the module can find these named 
entities through looking up in this lexicon. 
3.2 Filter Module 
So far, Chinese character strings are generated only 
through frequency statistics. Thus, some of them 
stand out just because of simple repetition and are 
probably not meaningful units of language.  We 
need to filter out those meaningless items.  As in 
figure 1, we adopt four kinds of filters in filter 
module. They work as follows. 
(1) Filter of Overlapped and Dependent Items  
 
evident that such character strings can?t serve as 
For two character strings S1 and S2, with S1 as a 
substring of S2, and the frequency of S1 is equal to 
that of S2, then S1 is overlapped by S2. In fact, we 
can set a threshold td for f(S1)-f(S2), where the 
function f(.) represents the frequency of some 
character string. If the value of f(S1)-f(S2) is less 
than td, then the string S1 is dependent on S2.  
Here, the overlapped and dependent substring will 
be removed from the candidate set. 
(2) Filter of Items with Punctuations and Func-
tion words 
The recognizer module treats equally all symbols 
in the text, such as Chinese characters and 
punctuations, etc. Thus when conducting the 
process of frequency statistics, for a character 
string, there might exist some punctuations and 
function words such as ???, ???, ???, ???, etc. 
These punctuations and function words usually 
occur in the head or tail of a character string.  It?s 
character strings can?t serve as keywords of an ar-
ticle, and they should be deleted from the candi-
date set. 
(3) Segmentation Filter 
We find the first occurrence position of every can-
didate keyword and get the sentence at the position.  
Then the sentence is segmented.  According to the 
segmented result, we can verify whether the char-
acter string is meaningful.  First of all, we get the 
segmentation result of the character string in the 
segmented sentence.  Suppose the character string 
ci?cj in the original text with the sentence 
c1c2?ci-1ci?cjcj+1?cn as its context, if the 
segmentation tool segments ci-1ci or cjcj+1 into one 
word, then ci?cj will not be regarded as an inte-
grated unit.  That is, this item will be seen as 
meaningless and filtered out from the set of candi-
date keywords.  Here we don?t adopt the method of 
conducting frequency statistics of words after seg-
mentation, but use segmentation tool after fre-
quency statistics of character strings.  There are 
some reasons. Above all, although the segmenta-
tion technique is relatively mature, its precision is 
still not high enough.  Then, for the same character 
string, its segmentation results often differ in dif-
ferent sentences.  Thus, it?s difficult to compute the 
frequency of a character string precisely.  Further-
more, now we only need to segment one sentence 
for a candidate keyword.  That will save us a great 
deal of time. 
(4) POS Filter 
Because keywords provide a brief summary for 
one document, they should be words or phrases 
that represent some meaning units such as nouns 
and noun phrases.  Therefore, a single word whose 
part of speech is preposition, adverb, adjective, or 
conjunctive is filtered out.  At the same time, verb 
phrases, adjective phrases, preposition phrases are 
also excluded from the candidate set.  The same as 
segmentation filter, we only do the POS tagging 
for the sentence where every candidate keyword 
occurs the first time.  If a candidate item is made of 
more than one word, it will have a sequence of 
POS tags according to which we can assign a 
phrase category.  The POS tags or phrase catego-
ries are the basis for POS filtering. 
Only conducting frequency statistics of charac-
ter strings can?t refine the candidate set well, and 
we utilize the relatively mature linguistic segmen-
tation and POS tagging techniques so that we can 
further improve the quality of the candidate key-
words.  Here, the general lexicon with about 
60,000 Chinese words is applied to the processes 
of segmentation and POS tagging. 
3.3 Selector Module 
After several filtering, now we can get a reduced 
set of candidate keywords.  Most character strings 
in the set are meaningful and reflect the content of 
the document to some extent.  For every candidate 
now, we adopt several features to describe it. The 
features include frequency, length, position of the 
first occurrence, part of speech and whether it is a 
proper noun or in a pair of specific punctuations, as 
in table 1.  At the same time, through the process-
ing of several linguistic tools in filter module, we 
can assign a value to every feature in every candi-
date item.   
feature meaning of feature 
freq Frequency of an item 
len Length of an item 
is_noun Whether an item is a noun phrase 
in_title Whether the first occurrence of an item is in the title of one document 
in_seg1 
Whether the first occurrence of an 
item is in the first paragraph of one 
document 
is_proper 
Whether an item is a proper noun, 
for example: person name, organi-
zation, translation term, place 
name, title of a person etc. 
in_sign 
Whether an item is bracketed by a 
pair of specific punctuations such 
as ???? and ????. 
Table 1. Features of candidate keywords 
We can find that the candidate set is still too 
large to select from it the keywords.  Then we will 
conduct feature calculation to refine the candidate 
set.  We have known that every candidate item has 
a feature-value set.  These feature values are our 
basis to evaluate every candidate item.  We com-
pute a score for every candidate keyword through 
the module of feature computation.  The higher the 
score, the more relevant the candidate is to the 
document. 
We compute the percentage how much manually 
indexed keywords of different lengths cover in the 
set of automatically generated candidates.  As in 
figure 2, Length represents the length of keywords 
and percentage denotes the corresponding percent-
age that keywords of this length are in the set.  The 
higher the percentage, the more likely the key-
words of this length are to be selected.  Therefore, 
we can make a conclusion that the score of a can-
didate is directly proportional to the percentage of 
its length.  Then we can acquire the relation be-
tween score and length of a candidate.  At the same 
time, we can also see that the score is directly 
proportional to a candidate?s frequency.  In 
addition, score is relevant to other features in table 
1.  Thus, we get formula 1, as following. 
 
Fig. 2. Relations between Percentage Selected 
and Length of Keywords 
??
???=
???= ??
otherwise0
feature i  thesatisfiesck  if1)(
)1.7)((
100ln)()(
th
)(
Ffi
2
ckf
fw
cklen
ckFreqckscore
i
cki
i
 (1) 
Where ck represents a candidate keyword, the 
function freq(ck) gets the frequency of ck, len(ck) 
represents its length, that is, the number of Chinese 
characters every item includes. F represents all the 
binary features of a candidate keyword as in table 
1. Every feature except the features of freq and len 
are denoted by fi.  fi(ck) is a binary function and its 
value is 0 or 1.  If a candidate item ck satisfies the 
ith feature, then the value is set to 1, otherwise, it?s 
set to 0.  wi is the corresponding weight of feature 
fi.  For features is_noun, in_title, in_seg1, 
is_proper and in_sign, we set their weights to 7, 13, 
5, 11 and 3 respectively by experience.  After each 
candidate keyword gets a score, we choose those 
whose scores rank higher as keywords.   
??
(physical
training)
????
(physical
management)
????
(sports)
??(track
and field)
??
(ball) ...
...
???
(pingpang)
???
(badminton)
??
(football) ...
Fig.3. A Sample Tree Structure of Content Words
 
Now the keywords we get are all selected from 
the original text.  However, some keywords may 
express the content of the document, but they don?t 
occur in the text.  Therefore, we have constructed 
one list of content words with hierarchical relations 
as in figure 3.  That is content words lexicon.  The 
lexicon contains about 1,200 words which are of-
ten used as keywords.  As the content words lexi-
con available now, we can look up in it and expand 
obtained keywords to a higher level, i.e., if a se-
lected keyword has a parent in the lexicon, the par-
ent word will be expanded as a keyword. 
4 Experimental Results and Analysis 
We select 37 news articles from China Daily as our 
testing material from which experts have manually 
extracted keywords.  There are 23 articles about 
national politics, 10 articles of international poli-
tics, and 4 sports news articles.  Here, we auto-
matically extracted keywords from them and 
evaluated the results with the standard measures of 
precision and recall, which are defined as follows: 
Where P represents precision, and R represents 
recall. In general, these two measures in one sys-
tem are opposite to each other.  When precision is 
higher, recall will be lower. Otherwise, when pre-
cision is improved, recall will decrease.  In table 2, 
we illustrate our experimental results.  The first 
three rows give measures for articles about differ-
ent styles and the figures in parentheses represent 
the number of articles.  The fourth row gives the 
average measure of our system.  For comparison, 
we also illustrate the results of Chien?s [1997] 
PAT-tree-based method from his experiments in 
the last row.  From this table, we can see that more 
emphasis is placed on precision in Chien?s system.  
However, we incline to enhancing recall when pre-
cision and recall are assured relatively balanced.  
When precision is lower, perhaps more noise is 
introduced into the set of candidate keywords.  Be-
cause we have adopted segmentation and POS tag-
ging tools which can verify whether a candidate 
character string is a meaningful unit and found that 
the noise introduced now is more or less relevant 
to the content of the article, we don?t have to worry 
more about precision.    Therefore, we hope to 
generate more keywords automatically under the 
condition that the number of noise words is ac-
cepted. 
 
 Recall Precision
National politics (23) 0.452 0.401 
International Politics (10) 0.644 0.594 
Sports news (4) 0.629 0.482 
Average 0.523 0.462 
Chien?s (exact match) 0.30 0.43 
Table 2. Experimental Results 
It has to be pointed out that there are no satis-
factory results in extracting keywords from texts 
[Chien, 1997].  Although some keywords extracted 
are the same as manually extracted ones in mean-
ing, they are often different due to one or two 
characters mismatched.  According to our analysis 
of experimental results, though only 46% of ex-
tracted keywords appear in the set of manual key-
words, the rest are also relevant to the text and 
adapt to the need of information retrieval.  At the 
same time, about 52% of the manual keywords are 
generated by the automatically indexing method, 
however, we can often find a substitute for most of 
the rest in the set of automatically generated key-
words.   
manually indexing keywords ofnumber 
recognized  keywords genuine ofnumber R
llyautomatica indexing keywords ofnumber 
recognized  keywords genuine ofnumber P
=
=
Most of the keywords missed occur only once 
in the text, but they are mostly proper nouns of 
places, organizations or titles of person.  And this 
reveals that we need to further improve the tech-
niques to recognize proper nouns. 
5 Conclusion and Future Work 
We have described a system for automatically in-
dexing keywords from texts.  One document is in-
putted into the recognizer module, the filter 
module and the selector module consecutively, 
with keywords output.  Here we utilize the mature 
techniques available now such as string frequency 
statistics, segmentation and POS tagging tools.  
Then, according to features, we propose our 
method to evaluate directly every candidate key-
word and select those with higher scores as key-
words.  At the same time, we break through the 
tradition of generating keywords only from the 
original text and acquire some keywords through 
looking up in the lexicon of content words with 
hierarchical relations. The experimental results 
show that our system can perform comparably to 
the state of the art. 
Owing to the limit of the training corpus, the 
parameters in scoring formula are set by experi-
ence values.  With our method, we can cumulate 
more and more documents with keywords.  Then 
we can adopt machine-learning methods to conduct 
keyword indexing, which can make parameters 
more objective.  That will be our further work. 
References 
[Chien 1997] Chien, L. F., PAT-Tree-Based Keyword 
Extraction for Chinese Information Retrieval, Pro-
ceedings of the ACM SIGIR International Confer-
ence on Information Retrieval, 1997, pp. 50--59.  
[Frank 1999] Frank E., Paynter G.W., Witten I.H., Gut-
win C., and Nevill-Manning C.G., Domain-specific 
keyphrase extraction, Proc. Sixteenth International 
Joint Conference on Artificial Intelligence, Morgan 
Kaufmann Publishers, San Francisco, CA, 1999, pp. 
668-673. 
[Lai 2002] Yu-Sheng Lai, Chung-Hsien Wu, Meaning-
ful term extraction and discriminative term selection 
in text categorization via unknown-word methodol-
ogy, ACM Transactions on Asian Language Informa-
tion Processing (TALIP), Vol.1, No.1, March 2002, 
pp. 34-64. 
[Liu 1998] Liu Ting, Wu Yan, Wang Kaizhu, An Chi-
nese Word Automatic Segmentation System Based 
on String Frequency Statistics Combined with Word 
Matching, Journal of Chinese Information Processing, 
Vol.12, No.1, 1998, pp. 17-25. 
[Ong 1999] T. Ong and H. Chen,  Updateable PAT-Tree 
Approach to Chinese Key Phrase Extraction Using 
Mutual Information: A Linguistic Foundation for 
Knowledge Management, Proceedings of the Second 
Asian Digital Libaray Conference, Taipei, Taiwan, 
Novemeber 8-9, 1999.  
[Turney 1999] Turney, P.D., Learning to Extract Key-
phrases from Text, NRC Technical Report ERB-1057, 
National Research Council, Canada, 1999. 
[Witten 1999] Witten I.H., Paynter G.W., Frank E., 
Gutwin C., and Nevill-Manning C.G., KEA: Practical 
automatic keyphrase extraction, Proc. DL '99, 1999, 
pp. 254-256.  
[Yang 2002] Wenfeng Yang, Chinese keyword extrac-
tion based on max-duplicated strings of the docu-
ments, Proceedings of the 25th annual international 
ACM SIGIR conference on Research and develop-
ment in information retrieval, 2002, pp. 439-440. 
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1197?1207, Dublin, Ireland, August 23-29 2014.
Query-focused Multi-Document Summarization: Combining a Topic
Model with Graph-based Semi-supervised Learning
Yanran Li and Sujian Li
?
Key Laboratory of Computational Linguistics,
Peking University, MOE, China
{liyanran,lisujian}@pku.edu.cn
Abstract
Graph-based learning algorithms have been shown to be an effective approach for query-focused
multi-document summarization (MDS). In this paper, we extend the standard graph ranking algo-
rithm by proposing a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised
learning approach based on topic modeling techniques. Experimental results on TAC datasets
show that by considering topic information, we can effectively improve the summary perfor-
mance.
1 Introduction
Query-focused multi-document summarization (MDS) can facilitate users to grasp the main idea of the
documents according to the users? concern. In query-focused summarization, one query is firstly pro-
posed at the beginning of the documents. Then according to the given query and its influence on sen-
tences, a ranking score is assigned to each of the sentences and higher ranked sentences are picked into
a summary.
Among existing approaches, graph-based semi-supervised learning algorithms have been shown to be
an effective way to impose a query?s influence on sentences (Zhou et al, 2003; Zhou et al, 2004; Wan
et al, 2007). Specifically, a weighted network is constructed where each sentence is modeled as a node
and relationships between sentences are modeled as directed or undirected edges. With the assumption
that a query is the most important node, initially, a positive score is assigned to the query and zero to the
remaining nodes. All nodes then spread their ranking scores to their nearby neighbors via the weighted
network. This spreading process is repeated until a global stable state is achieved, and all nodes obtain
their final ranking scores.
The primary disadvantage of existing learning method is that sentences are ranked without considering
topic level information. As we know, a collection of related documents usually covers a few different
topics. For example, the specific event ?Quebec independence? may involve the topics such as ?leader
in independence movement?, ?referendum?, ?related efforts in independence movement? and so on. It
is important to discover the latent topics when summarizing a document collection, because sentences in
an important topic would be more important than those talking about trivial topics (Hardy et al, 2002;
Harabagiu and Lacatusu, 2005; Otterbacher et al, 2005; Wan and Yang, 2008).
The topic models (Blei et al, 2003) offer a good opportunity for the topic-level information modeling
by offering clear and rigorous probabilistic interpretations over other existing clustering techniques. So
far, LDA has been widely used in summarization task by discovering topics latent in the document
collections (Daume and Marcu, 2006; Haghighi and Vanderwende, 2009; Jin et al, 2010; Mason and
Charniak, 2011; Delort and Alfonseca, 2012). However, as far as we know, how to combine topic
information and semi-supervised learning into a unified framework has seldom been exploited.
In this paper, inspired by the graph-based semi-supervised strategy and topic models, we propose
a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach for
?
correspondence author
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:// creativecommons.org/licenses/by/4.0/
1197
query-focused MDS. By using two revised versions of LDA topic model (See Section 2), our approach
naturally models the relations between topics and sentences, and further use these relations to construct
the two-layer graph. Experiments on the TAC datasets demonstrate that we can improve summarization
performance under the framework of two-layer graph-based semi-supervised learning.
The rest of this paper is organized as follows: Section 2 describes our LDA based topic models, W-
LDA and S-LDA. Section 3 presents the construction of the two-layer graph and the semi-supervised
learning and the experimental results are provided in Section 4. Then, Section 5 describes related work
on query-focused multi-document summarization and topic modeling techniques and we conclude this
paper in Section 6.
2 Topic Modeling
2.1 Model Description
As discussed in Section 1, a collection of documents often involves different topics related to a specific
event. The basic idea of our summarization approach is to discover the latent topics and cluster sentences
according to the topics. Inspired by (Chemudugunta et al, 2006) and (Li et al, 2011), we find 4 types of
words in the text: (1) Stop words that occur frequently in the text. (2) Background words that describe
the general information about an event, such as ?Quebec? and ?independence?. (3) Aspect words talking
about topics across the corpus. (4) Document-specific words that are local to a single document and do
not appear across different corpus. Similar ideas can also be found in many LDA based summarization
techniques (Haghighi and Vanderwende, 2009; Li et al, 2011; Delort and Alfonseca, 2012).
Stop words can easily be filtered out by a standard list of stopwords. We use a background word
distribution ?
B
to model vocabularies commonly used in the document collection. We assume that there
are K aspect topics shared across corpus and each topic is associated with a topic-word distribution
?
k
, k ? [1,K]. For each document m, there is a document-specific word distribution ?
m
, m ? [K +
1,K + M ]. Each word w is modeled as a mixture of background topics, document-specific topics or
aspect topics. We use a latent parameter y
w
to denote whether it is a background word, a document-
specific word or an aspect word. y
w
is sampled from a multinomial distribution with parameter pi.
2.2 W-LDA and S-LDA
We describe two models: a word level model W-LDA and a sentence level S-LDA. Their difference only
lies in whether the words within a sentence are generated from the same topic.
W-LDA: Figure 1 and Figure 3 show the graphical model and generation process of W-LDA, which is
based on Chemudugunta et al?s work (2007). Using the Gibbs sampling technique, in each iteration two
latent parameters y
w
and z
w
are sampled simultaneously as follows:
P (y
w
= 0) ?
N
m0,?w
+ ?
N
m,?w
+ 3?
E
w
B
+ ?
?
w
?
E
w
?
B
+ V ?
(1)
P (y
w
= 1) ?
N
m1,?w
+ ?
N
m,?w
+ 3?
E
w
m
+ ?
?
w
?
E
w
?
m
+ V ?
(2)
P (y
w
= 2, z
w
= k) ?
N
m2,?w
+ ?
N
m,?w
+ 3?
?
C
k
m
+ ?
?
k
?
C
k
m
+K?
E
w
k
+ ?
?
w
?
E
w
?
k
+ V ?
(3)
where N
m0,?w
, N
m1,?w
and N
m2,?w
denote the number of words assigned to background, document-
specific and aspect topic in current document. N
m,?w
denotes the total number of words in current
document. E
w
B
, E
w
m
andE
w
k
are the number of times that wordw appears in background topic, document-
specific topic and aspect topic k. C
k
m
denotes the number of words assigned to topic k in current docu-
ment.
With one Gibbs sampling, we can make the following estimation:
?
w
k
=
E
w
k
+ ?
?
w
?
E
w
?
k
+ V ?
(4)
1198
Then, the probability that a sentence s is generated from topic k is computed based on the probability
that each of its aspect words is generated from topic k:
P (s|z
s
= k) =
?
w?s,y
w
=2
?
w
k
(5)
Figure 1: Graphical model for W-LDA
Figure 2: Graphical model for S-LDA
1. Draw background distribution ?
B
? Dir(?)
2. For each document m:
draw doc proportion vector ?
m
? Dir(?)
draw doc proportion vector pi
m
? Dir(?)
draw doc specific distribution ?
m
? Dir(?)
3. For each topic k:
draw topic distribution ?
k
? Dir(?)
4. For each word w in document m:
(a) draw y
w
? Multi(pi
m
)
(b) if y
w
= 0: draw w ? ?
B
if y
w
= 1: draw w ? ?
m
if y
w
= 2:
draw z
w
? Multi(?
m
)
w ? Multi(?
z
w
)
Figure 3: Generation process for W-LDA
1. Draw background distribution ?
B
? Dir(?)
2. For each document m:
draw doc proportion vector ?
m
? Dir(?)
draw doc proportion vector pi
m
? Dir(?)
draw doc specific distribution ?
m
? Dir(?)
3. For each topic k:
draw topic distribution ?
k
? Dir(?)
4. For each sentence s in document m:
4.1 draw z
s
? Multi(?
m
)
4.2 for each word in sentence s:
(a) draw y
w
? Multi(pi
m
)
(b) if y
w
= 0: draw w ? ?
B
if y
w
= 1: draw w ? ?
m
if y
w
= 2: draw w ? Multi(?
z
w
)
Figure 4: Generation process of S-LDA
S-LDA: In S-LDA, each sentence is treated as a whole and words within a sentence are generated
from the same topic (Gruber et al., 2007). Its graphical model and generated process are shown in Figure
2 and Figure 4. In S-LDA, we firstly sample the topic z
s
for each sentence as follows:
P (z
s
= k|z
?s
, y, w) ?
?(
?
w
?
E
w
?
k
+ V ?)
?(
?
w
?
E
w
?
k
+N
A
s
+ V ?)
?
?
w?s,y
w
=2
?(E
w
k
+N
w
s
+ ?)
?(E
w
k
+ ?)
?
C
k
m
+ ?
?
k
?
C
k
?
m
+K?
(6)
C
k
m
denotes the number of sentences in document m assigned to topic k. N
A
s
denotes the number of
aspect words in current sentence. Then y
w
is sampled.
In our experiments, we set hyperparameters ? = 1, ? = 0.5, ? = 0.01. We run 500 burn-in iterations
through all documents in the collection to stabilize the distribution of z and y before collecting samples.
3 Graph-based Semi-supervised Learning
As stated before, the consideration of higher level information (i.e. topics) would be helpful for sentence
ranking in summarization. In our two-layer graph, the upper layer is composed of topic nodes and the
lower layer is composed of sentences nodes, among which there is one node representing the query.
1199
Formally, given a document set D, let G =< V
s
, V
t
, E > be the two-layer graph, where V
s
=
{s
1
, s
2
, ..., s
N
} denotes the set of all the sentence nodes and s
1
is the query. V
t
= {z
1
, z
2
, ..., z
K
}
corresponds to all the topic nodes. The collection of edges E in the graph consists of the relations
within layers and between layers. And the edge weights are measured according to the similarities
between nodes, which are computed based on the topic distribution from our two topic model extensions.
Specifically, we introduce four edge weight matrices
?
W
N?K
,
?
W
K?N
, U and P to describe the sentence-
to-topic relations, the topic-to-sentence relations, the sentence-to-sentence relations and the topic-to-
topic relations respectively.
Firstly, the row-normalized edge weight matrices
?
W
N?K
and
?
W
K?N
denotes the similarity matrix
between sentences and topics,
?
W
i,j
=
sim(s
i
, z
k
)
?
k
?
sim(s
i
, z
k
?
)
?
W
i,j
=
sim(s
i
, z
k
)
?
j
sim(s
j
, z
k
)
(7)
where sim(s
i
, z
k
) = p(s
i
|z
s
i
= z
k
) is the probability that the sentence is generated from that topic
calculated in Equation (5).
The edge weight matrix U describe the sentence-to-sentence relations. In the same way, the simi-
larity between two sentences is the cosine similarity between their topic distributions, sim(s
i
, s
j
) =
1
C
1
?
k
p(s
i
|z
s
i
= k) ? p(s
j
|z
s
j
= z
k
), where C
1
=
?
?
k
p
2
(s
i
|z
s
i
= k)
?
?
k
p
2
(s
j
|z
s
j
= k) is the
normalized factor. Since the row-normalization process will make the sentence-to-sentence relation ma-
trix asymmetric, we adopt the following strategy: let Sim(s) denote the similarity matrix between sen-
tences, where Sim(s)(i, j) = sim(s
i
, s
j
) and D denotes the diagonal matrix with (i, i)-element equal
to the sum of the i
th
row of Sim(s). Edge weight matrix between sentences U is calculated as follows:
U = D
?
1
2
Sim(s)D
?
1
2
(8)
Then, the edge weight matrix between topics P is the normalized symmetric matrix of the similairty
matrix between two topics. The cosine similarity between two topics is calculated according to word-
topic distribution.
sim(z
i
, z
j
) =
1
C
2
?
w
p(w|z
i
)p(w|z
j
) =
1
C
2
?
w
?
w
z
i
?
w
z
j
(9)
where C
2
=
?
?
w
p
2
(w|z
i
) ?
?
?
w
p
2
(w|z
j
) is the normalized factor.
We further transform the task to an optimizing problem based on the assumption that closely related
nodes (sentences and topics) tend to have similar scores. So we would give more penalty for the differ-
ence between closely related nodes with regard to edge weight matrices
?
W
N?K
,
?
W
K?N
, U and P . This
motivates the following optimization function ?(f, g) in Equation (10) similar to the graph harmonic
function(Zhu et al, 2003). f denotes the sentence score vector and g denotes the topic score vector.
Intuitively, ?(f, g) measures the sum of difference between graph nodes; the more they differ, the larger
?(f, g) would be.
?(f, g) = a
?
0?i,j?N
U
i,j
(f
i
? f
j
)
2
+ a
?
0?i,j?K
P
i,j
(g
i
? g
j
)
2
+ (1? a)
?
0?i?N
?
0?j?K
?
W
ij
(f
i
? g
j
)
2
+ (1? a)
?
0?i?N
?
0?j?K
?
W
ij
(g
i
? f
j
)
2
(10)
The score vectors can be achieved by minimizing the function in Equation (10). That is,
(f, g)=argmin
f,g
?(f, g). We can get the following equations (details are shown in Appendix).
f = aUf +
1
2
(1? a)(
?
W +
?
W
T
)g
g = aPg +
1
2
(1? a)(
?
W
T
+
?
W )f
(11)
1200
Equation (11) conforms to our intuition: (1) A sentence would be important if it is heavily connected
with many important sentences and a topic would be important if it is closely related to other important
topics. (2) A sentence would be important if it is expressing an important topic, and in turn a topic would
be important if it is referred by an important sentence. Based on Equation (11), the ranking algorithm is
designed in a semi-supervised way, where the score of the labeled query is fixed to the largest score of 1
during each iteration, as shown in Figure 5. Then, our algorithm iteratively calculates the score of topics
and sentences until convergence
1
.
Input: The sentence set {s
1
, s
2
, ..., s
N
}, topic set
{z
1
, z
2
, ..., z
K
}, edge weight matrix
?
W ,
?
W , U
and P . s
1
is the query.
Output: Sentence score vector f and topic score
vector g.
BEGIN
1. Initialization, k=0:
f
0
= (1, 0, 0, ..., 0)
T
, g
0
= (0, 0, ..., 0)
T
2. Update sentence score vector
f
k+1
= aUf
k
+
1
2
(1? a)(
?
W +
?
W
T
)g
k
3. Update topic score vector
g
k+1
= aPg
k
+
1
2
(1? a)(
?
W
T
+
?
W )f
k
4. fix the score of query in f
k+1
to 1.
5. k=k+1 Go to Step 2 until convergence.
END
Figure 5: Sentence Ranking Algorithm
Input: The sentence set S = {s
1
, s
2
, ..., s
N
},
sentence score vector f
Output: Summary Y.
BEGIN:
1. Initialization: Y = ?, X = {S ? s
1
}.
2. while word num is less than 100:
(a) s
m
= argmax
s
i
?X
f(s
i
)
(b) If sim(s
m
, s) < Th
sem
, for all s ? Y :
Y = Y + {s
m
}
(c) X = x? {s
m
}
END
Figure 6: Sentence Selection Algorithm
3.1 Summary Generation
Sentence compression can largely improve summarization quality (Zajic et al, 2007; Peng et al, 2011).
Since sentence compression is not the main task in this paper, we just use the revised sentence compres-
sion techniques in (Li et al, 2011).Here, we remove the redundant modifiers such as adverbials, relative
clause modifiers, abbreviations, participials and infinitive modifiers for each sentence.
As for the sentence selection process, sentences with higher ranking score are selected into the sum-
mary. Then Maximum Marginal Relevance (MMR)(Goldstein et al, 1999) is further used for redundancy
removal. We just apply a simple greedy algorithm for sentence selection as shown in Figure 6. We use Y
to denote the summary set which contains the selected summary sentences. The algorithm first initializes
Y to ? and X as the set {S ? s
1
}. During each iteration, we select the highest ranked sentence s
j
from
the sentence set X. We need to assure that the value of semantic similarity between two sentences is less
than Th
sem
. Th
sem
denotes the threshold for the cosine similarity between two sentences and is set to
0.5 in our model.
4 Experiments
The query-focused MDS task defined in TAC (Text Analysis Conference) evaluations requires generating
a concise and well organized summary for a collection of related documents according to a given query.
The query usually consists of a narrative/question sentence. Our experiment data is composed of TAC
(2008-2009) data
2
, which contain 48 and 44 document collections respectively. We use docset-A data
sets in TAC which has 10 documents per collection. The average numbers of sentences per document
in TAC2008 and TAC2009 are 252 and 243 respectively, and the system-generated summary is limited
to 100 words. It is noted that the corpus of TAC2008 and TAC2009 are similar. In our experiment, we
apply the optimal topic number trained on TAC2008 dataset to TAC2009 dataset.
1
In our experiments, if |f
k
i
? f
k+1
1
| ? 0.0001(1 ? i ? N) and |g
k
i
? g
k+1
1
| ? 0.0001(1 ? i ? T ), iteration stops.
2
TAC data sets are for the update summarization tasks, where the summarization for docset-A can be seen the query-focused
summarization task referred in this paper.
1201
1 2
3 4
Figure 7: ROUGE score via (1)(2) topic number and (3)(4) parameter a on TAC2008.
As for evaluation metrics, we use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin,
2004) measures. ROUGE measures summary quality by counting overlapping units such as the n-gram,
word sequences and word pairs between the candidate summary and the reference summary. We report
ROUGE-1, ROUGE-2, and ROUGE-SU4
3
scores and their corresponding 95% confidential intervals,
to evaluate the performance of the system-generated summaries. As a preprocessing step, stopwords
are firstly removed with a list of 598 stop words and the remaining words are then stemmed using
PorterStemmer.
4
.
4.1 Parameter Tuning
There are two parameters to tune in our model. The first parameter is a in Equation (11) that controls
the tradeoff between influence from topics and from sentences. The second one is the topic number
K in LDA topic model. The combination of the two factors makes it hard to find a global optimized
solution. So we apply a gradient search strategy. At first, parameter a is fixed to a given value. Then
the performance of using different topic numbers is evaluated. After that, we fix the topic number to the
value which has achieved the best performance, and conduct experiments to find an appropriate value for
a. Here, we use TAC2008 as training data and test our model on TAC2009.
First, a is set to 0.5, then we change topic number K from 2 to 20 at the interval of 2. The ROUGE
score reaches their peaks when the topic number is around 12, as shown in Figure 7(1) and Figure 7(2).
Then we fix the number of K to 12 and change the value of parameter a from 0 to 1 with the interval
of 0.1. When the value of a is set to 0, the model degenerates into a one-layer graph ranking algorithm
where topic clustering information is neglected. As we can see from Figure 7(3) and Figure 7(4) , the
ROUGE scores reach their peaks around 0.6 and then drop afterwards. Thus, the topic number is set to
12 and a is set to 0.6 in the test dataset.
3
Jackknife scoring for ROUGE is used in order to compare with the human summaries.
4
http://tartarus.org/martin/PorterStemmer/
1202
4.2 Baseline Comparison
We firstly compare W-LDA and S-LDA with other clustering approaches. To be fair, we use the identical
sentence compression techniques and preprocessing methods for all baselines. Summaries are truncated
to the same length of 100 words.
Standard-LDA: A simplified version of W-LDA without considering the background or document-
specific information.
K-means: Using the K-means clustering algorithm for graph construction. We firstly randomly select
K sentences as initial centroid for clusters and then iteratively assign a sentence to each cluster. The
centroid is recomputed until convergence. The similarity between nodes in the graph (sentence or cluster)
is computed using the standard cosine measure based on the tf-idf information. K is set to 12, the same
as topic number in LDA.
Agglomerative: a bottom-up hierarchical clustering algorithm and starts with the sentences as indi-
vidual clusters and, at each step, merges the most similar or closest pair of clusters, until the number of
the clusters reduces to the desired number K = 12.
Divisive: a top-down hierarchical clustering algorithm and starts with one, all-inclusive cluster and,
at each step, splits the largest cluster until the number of clusters increases to the desired number K,
K = 12.
Approach Rouge-1 Rouge-2 Rouge-SU4
W-LDA 0.3791 (0.3702-0.3880) 0.1092 (0.1047-0.1135) 0.1382 (0.1350-0.1414)
S-LDA 0.3802 (0.3721-0.3883) 0.1109 (0.1061-0.1157) 0.1398 (0.1342-0.1454)
Standard LDA 0.3702 (0.3614-0.3790) 0.1012 (0.0960-0.1064) 0.1292 (0.1242-0.1344)
K-means 0.3658 (0.3582-0.3734) 0.1046 (0.0992-0.1080) 0.1327 (0.1263-0.1391)
Agglomerative 0.3681 (0.3612-0.3750) 0.1042 (0.091-0.1093) 0.1319 (0.1266-0.1272)
Divisive 0.3676 (0.3610-0.3742) 0.1021 (0.0981-0.1061) 0.1320 (0.1275-0.1365)
Table 1: Comparison with other clustering baselines.
Table 1 presents the performance of different clustering algorithms for summarization. Traditional
clustering algorithms such as K-means, Agglomerative and Divisive clustering achieve comparative re-
sults. Compared with traditional clustering algorithms, LDA based models (W-LDA, S-LDA, Standard-
LDA) achieve better results. This can be explained by the clear and rigorous probabilistic interpretation
of topic models. Background information and document-specific information would influence the per-
formance of topic modeling (Chemudugunta et al, 2006), that is why S-LDA and W-LDA achieve better
ROUGE performance than the standard LDA. We can also see that S-LDA is slightly better than W-LDA
in regard with ROUGE performance. The reason can be explained as follows: The aim of topic mod-
eling in this task is to cluster sentences according to their topics. So treating sentence as a unit in topic
modeling would be better than treating it as a set of independent words. In addition, forcing the words in
one sentence to share the same aspect topic can ensure semantic cohesion of the mined topics.
Next, we compare our model with the following widely used summarization approaches.
Manifold: One-layer graph-based semi-supervised approach developed byWan et al.(2008). Sentence
relations are calculated according to tf ? idf and topic information is neglected.
LexRank: An unsupervised graph-based summarization approach(Erkan and Radev, 2004), which is
a revised version of the famous web ranking algorithm PageRank.
KL-Divergence: The approach developed by (Lin et al, 2006) by using a KL-divergence based sen-
tence selection strategy.
KL(P
s
||Q
d
) =
?
w
P (w)log
P (w)
Q(w)
(12)
where P
s
is the unigram distribution of candidate summary and Q
d
denotes the unigram distribution of
document collection. Since this approach is designed for general summarization, query influence is not
considered.
1203
Hiersum: A LDA based approach proposed by (Haghighi and Vanderwende, 2009), where unigram
distribution is calculated from LDA topic model in Equation (12).
MEAD: A centroid based summary algorithm by (Radev et al, 2004). Cluster centroids in MEAD
consists of words which are central not only to one article in a cluster, but to all the articles. Similarity is
measured by using tf ? idf .
Approach Rouge-1 Rouge-2 Rouge-SU4
W-LDA 0.3891 (0.3802-0.3980) 0.1192 (0.1147-0.1235) 0.1482 (0.1450-0.1514)
S-LDA 0.3902 (0.3821-0.3983) 0.1209 (0.1161-0.1257) 0.1498 (0.1442-0.1554)
Manifold 0.3581 (0.3508-0.3656) 0.1007 (0.0952-0.1062) 0.1267 (0.1214-0.1320)
LexRank 0.3442 (0.3381-0.3502) 0.0817 (0.0782-0.0852) 0.1106 (0.1064-0.1148)
KL-divergence 0.3468 (0.3410-0.3526) 0.0820 (0.0782-0.0858) 0.1117 (0.1073-0.1161)
Hiersum 0.3599 (0.3526-0.3672) 0.1004 (0.0956-0.1052) 0.1280 (0.1221-0.1339)
MEAD 0.3451 (0.3390-0.3512) 0.0862 (0.0817-0.0907) 0.1131 (0.1080-0.1182)
Table 2: Performance comparison with baselines
Performance is presented at Table 2. We can find that ROUGE performance of one-layer graph rank-
ing algorithms such as Manifold and LexRank, where topic information is neglected, achieve worse
results than all two-layer models where topic information is considered (See Table 1). This verifies our
previous claim (Hardy et at., 2002; Harabagiu and Lacatusu, 2005; Wan and Yang, 2008) that the con-
sideration of topic information will improve summarization performance. S-LDA and W-LDA achieve
better performance than KL-divergence and Hiersum. This is because the sentence selection strategy for
KL-divergence and Hiersum tries to select sentence best representing the document as shown in Equation
(12), but do not consider the influence of query.
4.3 Manual Evaluation
W-LDA and S-LDA get comparative ROUGE scores. To obtain a more accurate measure to decide which
approach is better, we perform a simple user study concerning the following aspects on 40 randomly
selected topics in TAC2009: (1) Overall quality. (2) Focus: Whether the summary contains less irrelevant
content? (3) Responsiveness: Whether the summary is responsive to the query. (4) Non-Redundancy:
Whether the summary is non-redundant. Each respect is rated from 1 (very poor) to 5 (very good). Four
native speakers who are Ph.D. students in computer science (none are authors) performed the task.
The average score and standard deviation for W-LDA and S-LDA are displayed in Table 3. We can see
that the two models almost tie in foucs and non-redundancy. This is because two models use the same
sentence selection strategy based on MMR for redundancy removal and propagation model to impose
the query?s influence on sentences. S-LDA outperforms W-LDA in overall ranking and responsiveness
ranking. This implies that treating sentence as a unit in topic modeling would be preferable to just
treating it as a series of independent words.
S-LDA W-LDA
Overall 3.98? 0.52 3.58? 0.55
Focus 3.65? 0.54 3.35? 0.61
Responsiveness 3.73? 0.43 3.38? 0.46
Non-Redundancy 3.48? 0.51 3.45? 0.48
Table 3: Manual evaluation for S-LDA and W-LDA.
5 Related Work
Graph-based ranking approaches have been hot these days for both generic and query-focused summa-
rization (Zhou et al, 2003; Zhou et al, 2004; Erkan and Radev, 2004; Wan et al, 2007; Wei et al, 2008).
Commonly used graph-based ranking algorithms are mainly inspired by the link analysis algorithm in
web research such as PageRank (Page et al, 1999). (Wan et al, 2007) proposed the approach that treated
1204
the task of query-focused MDS as a semi-supervised learning task, in which the query is treated as a
labeled node, and sentences as unlabeled nodes. Then the scores of sentences are determined from the
manifold learning algorithm proposed by (Zhou et al, 2003) or the harmonic approach proposed by (Zhu
et al, 2003).
It is worthy of noting that researchers have found that by considering topic level information, the sum-
marization performance can be effectively improved (Hardy et al, 2002; Wan and Yang, 2008; Harabagiu
and Lacatusu, 2005). For example, (Otterbacher et al, 2005) models documents as a stochastic graph and
calculates sentence ranking scores with a topic-sensitive version of PageRank. (Wan and Yang, 2008)
developed a two-layer graph by clustering sentences by using standard clustering algorithms such as
K-means or agglomerate clustering. However, his algorithm is for general summarization where the
influence of query is not considered.
A significant portion of recent work incorporates LDA topic models (Blei et al, 2008) in summarization
tasks for their clear and rigorous probabilistic topic interpretations (Daume and Marcu, 2006; Titov and
McDonald, 2008; Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Li et al, 2013a; Li
et al, 2013b). (Haghighi and Vanderwende, 2009) introduced a LDA based model called Hiersum to
find the subtopics or aspects by combining KL-divergence criterion for selecting relevant sentences.
AYESSUM (Daume and Marcu, 2006) and the Special Words and Background model (Chemudugunta
et al, 2006) are very similar to Hiersum. In the same way, (Delort and Alfonseca, 2012) tried to use LDA
to model different levels of information for novelty detection in update summarization. Furthermore,
(Paul and Dredze, 2013) extends their f-LDA to jointly model combinations of drug, aspect and route of
administration as an exploratory tool for extractive summarization.
6 Conclusions and Future Work
In this paper, we propose a two-layer graph-based semi-supervised algorithm for query-focused MDS.
Topic modeling techniques are used for sentence clustering and further graph construction. By consider-
ing different kinds of information such as background or document-specific information, our two LDA
topic model extensions achieve better results than traditional clustering algorithms.
One primary disadvantage of our models is that it is hard to decide the topic numberK in LDA models
and how to define topic number is still a open problem in LDA topic models. From Figure 7, we can
see that summarization performance is sensitive to topic number. We train the value of topic number
on TAC2008 dataset and test the model on TAC2009. Such process makes sense because the corpus
sizes and contents of two datasets are similar. But it would be hard to extend optimal topic number in
TAC2008 to other datasets. Using non-parametric topic modeling techniques where topic number does
not have to be predefined is one of our future works.
Acknowledgements
Thanks Jiwei Li for the insightful reviews and careful polishment. We also thank the three anonymous
reviewers for their helpful comments. This work was partially supported by National High Technology
Research and Development Program of China (No. 2012AA011101), National Key Basic Research Pro-
gram of China (No. 2014CB340504), National Natural Science Foundation of China (No. 61273278),
and National Key Technology R&D Program (No: 2011BAH10B04-03).
References
Edoardo M. Airoldi, Blei D M, Fienberg S E, et al. Mixed membership stochastic blockmodels[J]. In The Journal
of Machine Learning Research, 2008, 9(1981-2014): 3.
David Blei, Andrew Ng and Micheal Jordan. 2003. Latent dirichlet allocation. In The Journal of Machine Learning
Research.
Chaltanya Chemudugunta, Padhraic Smyth and Mark Steyers. Modeling General and Specific Aspects of Docu-
ments with a Probabilistic Topic Model.. In Advances in Neural Information Processing Systems 19: Proceed-
ings of the 2006 Conference.
1205
Hal Daume and Daniel Marcu H. 2006. Bayesian Query-Focused Summarization. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and the 44th annual meeting of the Association for Com-
putational Linguistics, pages 305-312.
Jean-Yves Delort and Enrique Alfonseca. DualSum: a topic-model based approach for update summarization. In
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.
Gune Erkan and Dragomir Radev. 2004. Lexrank: graph-based lexical centrality as salience in text summarization.
In Journal of Artificial Intelligence Research.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and Jaime Carbonell. 1999. Summarizing Text Documents: Sen-
tence Selection and EvaluationMetrics. In Proceedings of the 22nd annual international ACM SIGIR conference
on Research and development in information retrieval.
Amit Gruber, Yair Weiss and Michal Rosen-Zvi. Hidden topic Markov models. In International Conference on
Artificial Intelligence and Statistics. 2007
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 362370.
Sanda Harabagiu and Finley Lacatusu. 2005. Topic themes for multi-document summarization. In Proceedings of
the 28th annual international ACM SIGIR conference on Research and development in information retrieval.
Hilda Hardy, Nobuyuki Shimizu, Tomek Strzakowski, Liu Ting, Xinyang Zhang and Bowden Wize. 2002. Cross-
document summarization by concept classification. In Proceedings of the 25th annual international ACM SIGIR
conference on Research and development in information retrieval.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The summarization systems at tac 2010. In Proceedings of the
third Text Analysis Conference, TAC-2010.
Jiwei Li and Sujian Li. 2013. Evolutionary Hierarchical Dirichlet Process for Timeline Summarization. In ACL
2013.
Jiwei Li and Claire Cardie. 2014. Timeline Generation: Tracking individuals on Twitter. In WWW 2014.
Peng Li, Yinglin Wang, Wei Gao and Jiang Jing. 2011. Generating aspect-oriented multi-document summariza-
tion with event-aspect model. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2006. An information-theoretic approach to auto-
matic evaluation of summaries. In Proceedings of the main conference on Human Language Technology Con-
ference of the North American Chapter of the Association of Computational Linguistics.
Chin-Yew Lin. Improving summarization performance by sentence compression: a pilot study. In Proceedings the
sixth international workshop on Information retrieval with Asian languages.
Rebecca Mason and Eugene Charniak. 2011. Extractive multi-document summaries should explicitly not contain
document-specific content. In proceedings of ACL HLT.
Jahna Otterbacher, Gne Erkan, and Dragomir R. Radev. Using random walks for question-focused sentence re-
trieval. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural
Language Processing. Association for Computational Linguistics, 2005.
Lawrence Page, Sergey Brin, Rajeev Motwani and Terry Winograd. 1999. The Pagerank Citation Ranking: Bring-
ing Order to the Web. Technical report, Stanford Digital Libraries.
Michael J. Paul and Mark Dredze. Drug extraction from the web: Summarizing drug experiences with multi-
dimensional topic models. In Proceedings of NAACL-HLT. 2013.
Wei-Ting Peng, Wei-Ta Chu, Chia-Han Chang, et al. Editing by viewing: automatic home video summarization by
viewing behavior analysis[J]. In Multimedia, IEEE Transactions on, 2011, 13(3): 539-550.
Dragomir Radev, Allison T, Blair-Goldensohn S, et al. MEAD-a platform for multidocument multilingual text
summarization[C]. In Proceedings of the 4th International Conference on Language Resources and Evaluation,
2004.
1206
Ivan Titov and Ryan McDonald. 2008. Modeling on- line reviews with multi-grain topic models. In International
World Wide Web Conference.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2007. Manifold-ranking based topic-focused multi-document sum-
marization. In Proceedings of International Joint Conference on Artificial Intelligence.
Xiaojun Wan and Jianwu Yang. 2008. Multi-document Summarization using cluster-based link analysis. In Pro-
ceedings of the 31st annual international ACM SIGIR conference on Research and development in information
retrieval.
Furu Wei, Wenjie Li, Qin Lu, and Yanxiang He. 2008. Query-sensitive mutual reinforcement chain and its appli-
cation in query-oriented multi-document summarization. In Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in information retrieval.
David Zajic, et al. Multi-candidate reduction: Sentence compression as a tool for document summarization tasks.
In Information Processing & Management 43.6 (2007): 1549-1570.
Dengzhong Zhou, Jason Weston, Arthur Gretton, Olivier Bousquet and Bernhard Schlkopf. 2003. Ranking on Data
Manifolds. In Proceedings of the Conference on Advances in Neural Information Processing Systems.
Dengyou Zhou, Olivier Bousquet, Thomas Navin and JasonWeston. 2004. Learning with Local and Global Con-
sistency. In Proceedings of Advances in neural information processing systems.
Xiaojin Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian fields and functions. In
Proceedings of the 20th International Joint Conference on Machine Learning, 2003.
APPENDIX
To optimize ?(f, g), shown in Equation (10), we set the partial derivative with respect to f
m
to 0, for
m ? [1, N ]. Let ?
mn
denote the index function as follows:
?
mn
=
{
1 if m = n
0 if m ?= n
0 =
??(f, g)
f
t
= 2a
?
i,j
U
i,j
(f
i
? f
j
)(?
it
? ?
jt
) + 2(1? a)
?
?
i,j
?
W
ij
(f
i
? g
j
)?
it
? 2(1? a)
?
i,j
?
W
ij
(g
i
? f
j
)?
jt
= 2(1? a)
?
j
?
W
tj
(f
t
? g
j
) + 2(1? a)
?
i
?
W
it
(g
i
? f
t
)
+ 2a
?
j
U
tj
(f
t
? f
j
) + 2a
?
i
U
it
(f
i
? f
t
)
= f
t
[4a
?
j
U
tj
+ 2(1? a)
?
j
?
W
tj
+ 2(1? a)
?
j
?
W
jt
]
? 4a
?
j
U
tj
f
j
? 2(1? a)
?
j
?
W
tj
g
j
? 2(1? a)
?
j
?
W
jt
g
j
?
j
U
tj
= 1
?
j
?
W
tj
= 1
?
j
?
W
jt
= 1
f
t
= a
?
j
U
tj
f
j
+
1
2
(1? a)[
?
j
(
?
W
tj
+
?
W
jt
)g
j
]
So we have:
f = aUf +
1
2
(1? a)(
?
W +
?
W
T
)g
A similar approach is used to obtain the second part of Equation (11).
1207
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1245?1254, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Joint Learning for Coreference Resolution with Markov Logic
Yang Song1, Jing Jiang2, Wayne Xin Zhao3, Sujian Li1, Houfeng Wang1
1Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China
2School of Information Systems, Singapore Management University, Singapore
3School of Electronics Engineering and Computer Science, Peking University, China
{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.com
Abstract
Pairwise coreference resolution models must
merge pairwise coreference decisions to gen-
erate final outputs. Traditional merging meth-
ods adopt different strategies such as the best-
first method and enforcing the transitivity con-
straint, but most of these methods are used
independently of the pairwise learning meth-
ods as an isolated inference procedure at the
end. We propose a joint learning model which
combines pairwise classification and mention
clustering with Markov logic. Experimen-
tal results show that our joint learning sys-
tem outperforms independent learning sys-
tems. Our system gives a better performance
than all the learning-based systems from the
CoNLL-2011 shared task on the same dataset.
Compared with the best system from CoNLL-
2011, which employs a rule-based method,
our system shows competitive performance.
1 Introduction
The task of noun phrase coreference resolution is to
determine which mentions in a text refer to the same
real-world entity. Many methods have been pro-
posed for this problem. Among them the mention-
pair model (McCarthy and Lehnert, 1995) is one of
the most influential ones and can achieve the state-
of-the-art performance (Bengtson and Roth, 2008).
The mention-pair model splits the task into three
parts: mention detection, pairwise classification and
mention clustering. Mention detection aims to iden-
tify anaphoric noun phrases, including proper nouns,
common noun phrases and pronouns. Pairwise clas-
sification takes a pair of detected anaphoric noun
phrase candidates and determines whether they re-
fer to the same entity. Because these classification
decisions are local, they do not guarantee that can-
didate mentions are partitioned into clusters. There-
fore a mention clustering step is needed to resolve
conflicts and generate the final mention clusters.
Much work has been done following the mention-
pair model (Soon et al 2001; Ng and Cardie, 2002).
In most work, pairwise classification and mention
clustering are done sequentially. A major weak-
ness of this approach is that pairwise classification
considers only local information, which may not be
sufficient to make correct decisions. One way to
address this weakness is to jointly learn the pair-
wise classification model and the mention cluster-
ing model. This idea has been explored to some
extent by McCallum and Wellner (2005) using con-
ditional undirected graphical models and by Finley
and Joachims (2005) using an SVM-based super-
vised clustering method.
In this paper, we study how to use a different
learning framework, Markov logic (Richardson and
Domingos, 2006), to learn a joint model for both
pairwise classification and mention clustering un-
der the mention-pair model. We choose Markov
logic because of its appealing properties. Markov
logic is based on first-order logic, which makes
the learned models readily interpretable by humans.
Moreover, joint learning is natural under the Markov
logic framework, with local pairwise classification
and global mention clustering both formulated as
weighted first-order clauses. In fact, Markov logic
has been previously used by Poon and Domingos
(2008) for coreference resolution and achieved good
1245
results, but it was used for unsupervised coreference
resolution and the method was based on a different
model, the entity-mention model.
More specifically, to combine mention cluster-
ing with pairwise classification, we adopt the com-
monly used strategies (such as best-first clustering
and transitivity constraint), and formulate them as
first-order logic formulas under the Markov logic
framework. Best-first clustering has been previously
studied by Ng and Cardie (2002) and Bengtson and
Roth (2008) and found to be effective. Transitivity
constraint has been applied to coreference resolution
by Klenner (2007) and Finkel and Manning (2008),
and also achieved good performance.
We evaluate Markov logic-based method on the
dataset from CoNLL-2011 shared task. Our ex-
periment results demonstrate the advantage of joint
learning of pairwise classification and mention clus-
tering over independent learning. We examine
best-first clustering and transitivity constraint in our
methods, and find that both are very useful for coref-
erence resolution. Compared with the state of the
art, our method outperforms a baseline that repre-
sents a typical system using the mention-pair model.
Our method is also better than all learning systems
from the CoNLL-2011 shared task based on the re-
ported performance. Even with the top system from
CoNLL-2011, our performance is still competitive.
In the rest of this paper, we first describe a stan-
dard pairwise coreference resolution system in Sec-
tion 2. We then present our Markov logic model for
pairwise coreference resolution in Section 3. Exper-
imental results are given in Section 4. Finally we
discuss related work in Section 5 and conclude in
Section 6.
2 Standard Pairwise Coreference
Resolution
In this section, we describe standard learning-based
framework for pairwise coreference resolution. The
major steps include mention detection, pairwise
classification and mention clustering.
2.1 Mention Detection
For mention detection, traditional methods include
learning-based and rule-based methods. Which kind
of method to choose depends on specific dataset. In
this paper, we first consider all the noun phrases
in the given text as candidate mentions. With-
out gold standard mention boundaries, we use a
well-known preprocessing tool from Stanford?s NLP
group1 to extract noun phrases. After obtaining all
the extracted noun phrases, we also use a rule-based
method to remove some erroneous candidates based
on previous studies (e.g. Lee et al(2011), Uryupina
et al(2011)). Some examples of these erroneous
candidates include stop words (e.g. uh, hmm), web
addresses (e.g. http://www.google.com),
numbers (e.g. $9,000) and pleonastic ?it? pronouns.
2.2 Pairwise Classification
For pairwise classification, traditional learning-
based methods usually adopt a classification model
such as maximum entropy models and support vec-
tor machines. Training instances (i.e. positive and
negative mention pairs) are constructed from known
coreference chains, and features are defined to rep-
resent these instances.
In this paper, we build a baseline system that uses
maximum entropy models as the classification algo-
rithm. For generation of training instances, we fol-
low the method of Bengtson and Roth (2008). For
each predicted mention m, we generate a positive
mention pair between m and its closest preceding
antecedent, and negative mention pairs by pairing m
with each of its preceding predicted mentions which
are not coreferential with m. To avoid having too
many negative instances, we impose a maximum
sentence distance between the two mentions when
constructing mention pairs. This is based on the in-
tuition that for each anaphoric mention, its preced-
ing antecedent should appear quite near it, and most
coreferential mention pairs which have a long sen-
tence distance can be resolved using string match-
ing. During the testing phase, we generate men-
tion pairs for each mention candidate with each of
its preceding mention candidates and use the learned
model to make coreference decisions for these men-
tion pairs. We also impose the sentence distance
constraint and use string matching for mention pairs
with a sentence distance exceeding the threshold.
1http://nlp.stanford.edu/software/corenlp.shtml
1246
2.3 Mention Clustering
After obtaining the coreferential results for all men-
tion pairs, some clustering method should be used to
generate the final output. One strategy is the single-
link method, which links all the mention pairs that
have a prediction probability higher than a threshold
value. Two other alternative methods are the best-
first clustering method and clustering with the tran-
sitivity constraint. Best-first clustering means that
for each candidate mention m, we select the best
one from all its preceding candidate mentions based
on the prediction probabilities. A threshold value
is given to filter out those mention pairs that have a
low probability to be coreferential. Transitivity con-
straint means that if a and b are coreferential and
b and c are coreferential, then a and c must also be
coreferential. Previous work has found that best-first
clustering and transitivity constraint-based cluster-
ing are better than the single-link method. Finally
we remove all the singleton mentions.
3 Markov Logic for Pairwise Coreference
Resolution
In this section, we present our method for joint
learning of pairwise classification and mention clus-
tering using Markov logic. For mention detection,
training instance generation and postprocessing, our
method follows the same procedures as described in
Section 2. In what follows, we will first describe
the basic Markov logic networks (MLN) framework,
and then introduce the first-order logic formulas we
use in our MLN including local formulas and global
formulas which perform pairwise classification and
mention clustering respectively. Through this way,
these two isolated parts are combined together, and
joint learning and inference can be performed in a
single framework. Finally we present inference and
parameter learning methods.
3.1 Markov Logic Networks
Markov logic networks combine Markov networks
with first-order logic (Richardson and Domingos,
2006; Riedel, 2008). A Markov logic network con-
sists of a set of first-order clauses (which we will re-
fer to as formulas in the rest of the paper) just like in
first-order logic. However, different from first-order
logic where a formula represents a hard constraint,
in an MLN, these constraints are softened and they
can be violated with some penalty. An MLN M
is therefore a set of weighted formulas {(?i, wi)}i,
where ?i is a first order formula andwi is the penalty
(the formula?s weight). These weighted formulas
define a probability distribution over sets of ground
atoms or so-called possible worlds. Let y denote a
possible world, then we define p(y) as follows:
p(y) = 1
Z
exp
(
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
)
. (1)
Here each c is a binding of free variables in ?i to
constants. Each f?ic represents a binary feature func-
tion that returns 1 if the ground formula we get by
replacing the free variables in ?i with the constants
in c under the given possible world y is true, and 0
otherwise. n?i denotes the number of free variables
of a formula ?i. Cn?i is the set of all bindings for the
free variables in ?i. Z is a normalization constant.
This distribution corresponds to a Markov network
where nodes represent ground atoms and factors rep-
resent ground formulas.
Each formula consists of a set of first-order predi-
cates, logical connectors and variables. Take the fol-
lowing formula as one example:
(?i, wi) : headMatch(a, b)?(a ?= b) ? coref (a, b).
The formula above indicates that if two different
candidate mentions a and b have the same head
word, then they are coreferential. Here a and b are
variables which can represent any candidate men-
tion, headMatch and coref are observed predicate
and hidden predicate respectively. An observed
predicate is one whose value is known from the ob-
servations when its free variables are assigned some
constants. A hidden predicate is one whose value is
not known from the observations. From this exam-
ple, we can see that headMatch is an observed pred-
icate because we can check whether two candidate
mentions have the same head word. coref is a hid-
den predicate because this is something we would
like to predict.
3.2 Formulas
We use two kinds of formulas for pairwise classi-
fication and mention clustering, respectively. For
1247
describing the attributes ofmi
mentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).
entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...
genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.
numberType(i,n) mi has number type SINGULAR, PLURAL or UN.
hasHead(i,h) mi has head word h, here h can represent all possible head words.
firstMention(i) mi is the first mention in its sentence.
reflexive(i) mi is reflexive.
possessive(i) mi is possessive.
definite(i) mi is definite noun phrase.
indefinite(i) mi is indefinite noun phrase.
demonstrative(i) mi is demonstrative.
describing the attributes of relations betweenmj andmi
mentionDistance(j,i,m) Distance between mj and mi in mentions.
sentenceDistance(j,i,s) Distance between mj and mi in sentences.
bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NO
and AGREE UN).
closestMatch(j,i,c) mj is the first agreement in number and gender when looking backward
from mi: CAGREE YES, CAGREE NO and CAGREE UN.
exactStrMatch(j,i) Exact strings match between mj and mi.
pronounStrMatch(j,i) Both are pronouns and their strings match.
nopronounStrMatch(j,i) Both are not pronouns and their strings match.
properStrMatch(j,i) Both are proper names and their strings match.
headMatch(j,i) Head word strings match between mj and mi.
subStrMatch(j,i) Sub-word strings match between mj and mi.
animacyMatch(j,i) Animacy types match between mj and mi.
nested(j,i) mj/i is included in mi/j .
c command(j,i) mj/i C-Commands mi/j .
sameSpeaker(j,i) mj and mi have the same speaker.
entityTypeMatch(j,i) Entity types match between mj and mi.
alias(j,i) mj/i is an alias of mi/j .
srlMatch(j,i) mj and mi have the same semantic role.
verbMatch(j,i) mj and mi have semantic role for the same verb.
Table 1: Observed predicates.
pairwise classification, because the decisions are lo-
cal, we use a set of local formulas. For mention
clustering, we use global formulas to implement
best-first clustering or transitivity constraint. We
naturally combine pairwise classification with men-
tion clustering via local and global formulas in the
Markov logic framework, which is the essence of
?joint learning? in our work.
3.2.1 Local Formulas
A local formula relates any observed predicates to
exactly one hidden predicate. For our problem, we
define a list of observed predicates to describe the
properties of individual candidate mentions and the
relations between two candidate mentions, shown in
Table 1. For our problem, we have only one hidden
predicate, i.e. coref. Most of our local formulas are
from existing work (e.g. Soon et al(2001), Ng and
Cardie (2002), Sapena et al(2011)). They are listed
in Table 2, where the symbol ?+? indicates that for
every value of the variable preceding ?+? there is a
separate weight for the corresponding formula.
3.2.2 Global Formulas
Global formulas are designed to add global con-
straints for hidden predicates. Since in our problem
there is only one hidden predicate, i.e. coref, our
global formulas incorporate correlations among dif-
ferent ground atoms of the coref predicates. Next we
will show the best-first and transitivity global con-
straints. Note that we treat them as hard constraints
so we do not set any weights for these global formu-
las.
1248
Lexical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? exactStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? pronounStrMatch (j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? properStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nopronounStrMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? headMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? subStrMatch(j,i) ? j ?= i ? coref(j,i)
hasHead(j,h1+) ? hasHead(i,h2+) ? j ?= i ? coref(j,i)
Grammatical Features
mentionType(j,t1+) ? mentionType(i,t2+) ? genderType(j,g1+) ? genderType(i,g2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? numberType(j,n1+) ? numberType(i,n2+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? bothMatch(j,i,b+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? closestMatch(j,i,c+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? animacyMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? nested(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? c command(j,i) ? j ?= i ? coref(j,i)
(mentionType(j,t1+) ? mentionType(i,t2+)) ? j ?= i ? coref(j,i)
(reflexive(j) ? reflexive(i)) ? j ?= i ? coref(j,i)
(possessive(j) ? possessive(i)) ? j ?= i ? coref(j,i)
(definite(j) ? definite(i)) ? j ?= i ? coref(j,i)
(indefinite(j) ? indefinite(i)) ? j ?= i ? coref(j,i)
(demonstrative(j) ? demonstrative(i)) ? j ?= i ? coref(j,i)
Distance and position Features
mentionType(j,t1+) ? mentionType(i,t2+) ? sentenceDistance(j,i,s+) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? mentionDistance (j,i,m+) ? j ?= i ? coref(j,i)
(firstMention(j) ? firstMention(i)) ? j ?= i ? coref(j,i)
Semantic Features
mentionType(j,t1+) ? mentionType(i,t2+) ? alias(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? sameSpeaker(j,i) ? j?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? entityTypeMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? srlMatch(j,i) ? j ?= i ? coref(j,i)
mentionType(j,t1+) ? mentionType(i,t2+) ? verbMatch(j,i) ? j ?= i ? coref(j,i)
(entityType(j,e1+) ? entityType(i,e2+)) ? j ?= i ? coref(j,i)
Table 2: Local Formulas.
Best-First constraint:
coref(j, i) ? ?coref(k, i) ?j, k < i(k ?= j) (2)
Here we assume that coref(j,i) returns true if can-
didate mentions j and i are coreferential and false
otherwise. Therefore for each candidate mention i,
we should only select at most one candidate mention
j to return true for the predicate coref(j,i) from all its
preceding candidate mentions.
Transitivity constraint:
coref(j, k)?coref(k, i)?j < k < i ? coref(j, i) (3)
coref(j, k)?coref(j, i)?j < k < i ? coref(k, i) (4)
coref(j, i)?coref(k, i)?j < k < i ? coref(j, k) (5)
With the transitivity constraint, it means for given
mentions j, k and i, if any two pairs of them are
coreferential, then the third pair of them should be
also coreferential.
We use best-first clustering and transitivity con-
straint in our joint learning model respectively. De-
tailed comparisons between them will be shown in
Section 4.
3.3 Inference
We use MAP inference which is implemented by In-
teger Linear Programming (ILP). Its objective is to
maximize a posteriori probability as follows. Here
we use x to represent all the observed ground atoms
and y to represent the hidden ground atoms. For-
mally, we have
y? = argmax
y
p(y|x) ? argmax
y
s(y, x),
where
s(y, x) =
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y, x). (6)
Each hidden ground atom can only takes a value of
either 0 or 1. And global formulas should be satis-
fied as hard constraints when inferring the best y?. So
1249
the problem can be easily solved using ILP. Detailed
introduction about transforming groundMarkov net-
works in Markov logic into an ILP problem can be
found in (Riedel, 2008).
3.4 Parameter Learning
For parameter learning, we employ the online
learner MIRA (Crammer and Singer, 2003), which
establishes a large margin between the score of the
gold solution and all wrong solutions to learn the
weights. This is achieved by solving the quadratic
program as follows
min ? wt ?wt?1 ? . (7)
s.t. s(yi, xi)? s(y?, xi) ? L(yi, y?)
?y? ?= yi, (yi, xi) ? D
Here D = {(yi, xi)}Ni=1 represents N training in-
stances (each instance represents one single docu-
ment in the dataset) and t represents the number of
iterations. In our problem, we adopt 1-best MIRA,
which means that in each iteration we try to find wt
which can guarantee the difference between the right
solution yi and the best solution y? (i.e. the one with
the highest score s(y?, xi), equivalent to y? in Section
3.3)) is at least as big as the loss L(yi, y?), while
changing wt?1 as little as possible. The number of
false ground atoms of coref predicate is selected as
loss function in our experiments. Hard global con-
straints (i.e. best-first clustering or transitivity con-
straint) must be satisfied when inferring the best y?
in each iteration, which can make learned weights
more effective.
4 Experiments
In this section, we will first describe the dataset and
evaluation metrics we use. We will then present the
effect of our joint learning method, and finally dis-
cuss the comparison with the state of the art.
4.1 Data Set
We use the dataset from the CoNLL-2011 shared
task, ?Modeling Unrestricted Coreference in
OntoNotes? (Pradhan et al 2011)2. It uses the En-
glish portion of the OntoNotes v4.0 corpus. There
are three important differences between OntoNotes
2http://conll.cemantix.org/2011/
and another well-known coreference dataset from
ACE. First, OntoNotes does not label any singleton
entity cluster, which has only one reference in the
text. Second, only identity coreference is tagged in
OntoNotes, but not appositives or predicate nomi-
natives. Third, ACE only considers mentions which
belong to ACE entity types, whereas OntoNotes
considers more entity types. The shared task is to
automatically identify both entity coreference and
event coreference, although we only focus on entity
coreference in this paper. We don?t assume that
gold standard mention boundaries are given. So we
develop a heuristic method for mention detection.
See details in Section 2.1.
The training set consists of 1674 documents from
newswire, magazine articles, broadcast news, broad-
cast conversations and webpages, and the develop-
ment set consists of 202 documents from the same
source. For training set, there are 101264 mentions
from 26612 entities. And for development set, there
are 14291 mentions from 3752 entities (Pradhan et
al., 2011).
4.2 Evaluation Metrics
We use the same evaluation metrics as used in
CoNLL-2011. Specifically, for mention detection,
we use precision, recall and the F-measure. A men-
tion is considered to be correct only if it matches
the exact same span of characters in the annotation
key. For coreference resolution, MUC (Vilain et al
1995), B-CUBED (Bagga and Baldwin, 1998) and
CEAF-E (Luo, 2005) are used for evaluation. The
unweighted average F score of them is used to com-
pare different systems.
4.3 The Effect of Joint Learning
To assess the performance of our method, we set up
several variations of our system to compare with the
joint learning system. The MLN-Local system uses
only the local formulas described in Table 2 with-
out any global constraints under the MLN frame-
work. By default, the MLN-Local system uses the
single-link method to generate clustering results.
The MLN-Local+BF system replaces the single-link
method with best-first clustering to infer mention
clustering results after learning the weights for all
the local formulas. The MLN-Local+Trans sys-
tem replaces the best-first clustering with transitivity
1250
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84
MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85
MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97
MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50
MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57
Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.
constraint. The MLN-Joint system is a joint model
for both pairwise classification and mention cluster-
ing. It can combine either best-first clustering or en-
forcing transitivity constraint with pairwise classifi-
cation, and we denote these two variants of MLN-
Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-
spectively.
To compare the performance of the various sys-
tems above, we use 10-fold cross validation on
the training dataset. We empirically find that our
method has a fast convergence rate, to learn the
MLN model, we set the number of iterations to be
10.
The performance of these compared systems is
shown in Table 3. To provide some context for
the performance of this task, we report the median
average F-score of the official results of CoNLL-
2011, which is 50.12 (Pradhan et al 2011). We can
see that MLN-Local achieves an average F-score of
56.84, which is well above the median score. When
adding best-first or transitivity constraint which
is independent of pairwise classification, MLN-
Local+BF and MLN-Local+Trans achieve better re-
sults of 57.85 and 57.97. Most of all, we can see
that the joint learning model (MLN-Joint(BF) or
MLN-Joint(Trans)) significantly outperforms inde-
pendent learning model (MLN-Local+BF or MLN-
Local+Trans) no matter whether best-first clustering
or transitivity constraint is used (based on a paired 2-
tailed t-test with p < 0.05) with the score of 58.50
or 58.57, which shows the effectiveness of our pro-
posed joint learning method.
Best-first clustering and transitivity constraint
are very useful in Markov logic framework, and
both MLN-Local and MLN-Joint benefit from them.
For MLN-Joint, these two clustering methods re-
sult in similar performance. But actually, transi-
tivity is harder than best-first, because it signifi-
cantly increases the number of formulas for con-
straints and slows down the learning process. In
our experiments, we find that MLN-Joint(Trans)3 is
much slower than MLN-Joint(BF). Overall, MLN-
Joint(BF) has a good trade-off between effectiveness
and efficiency.
4.4 Comparison with the State of the Art
In order to compare our method with the state-of-
the-art systems, we consider the following systems.
We implemented a traditional pairwise coreference
system using Maximum Entropy as the base classi-
fier and best-first clustering to link the results. We
used the same set of local features in MLN-Joint.
We refer to this system as MaxEnt+BF. To replace
best-first clustering with transitivity constraint, we
have another system named as MaxEnt+Trans. We
also consider the best 3 systems from CoNLL-2011
shared task. Chang?s system uses ILP to perform
best-first clustering after training a pairwise corefer-
ence model. Sapena?s system uses a relaxation label-
ing method to iteratively perform function optimiza-
tion for labeling each mention?s entity after learning
the weights for features under a C4.5 learner. Lee?s
system is a purely rule-based one. They use a battery
of sieves by precision (from highest to lowest) to it-
eratively choose antecedent for each mention. They
obtained the highest score in CoNLL-2011.
Table 4 shows the comparisons of our system with
the state-of-the-art systems on the development set
of CoNLL-2011. From the results, we can see that
our joint learning systems are obviously better than
3For MLN-Joint(Trans), not all training instances can be
learnt in a reasonable amount of time, so we set up a time out
threshold of 100 seconds. If the model cannot response in 100
seconds for some training instance, we remove it from the train-
ing set.
1251
System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F F
MLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60
MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67
MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19
MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31
Lee?s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60
Sapena?s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61
Chang?s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35
Table 4: Comparisons with state-of-the-art systems on the development dataset.
MaxEnt+BF and MaxEnt+Trans. They also out-
perform the learning-based systems of Sapena et al
(2011) and Chang et al(2011), and perform com-
petitively with Lee?s system (Lee et al 2011). Note
that Lee?s system is purely rule-based, while our
methods are developed in a theoretically sound way,
i.e., Markov logic framework.
5 Related Work
Supervised noun phrase coreference resolution has
been extensively studied. Besides the mention-pair
model, two other commonly used models are the
entity-mention model (Luo et al 2004; Yang et al
2008) and ranking models (Denis and Baldridge,
2008; Rahman and Ng, 2009). Interested readers
can refer to the literature review by Ng (2010).
Under the mention-pair model, Klenner (2007)
and Finkel and Manning (2008) applied Integer Lin-
ear Programming (ILP) to enforce transitivity on the
pairwise classification results. Chang et al(2011)
used the same ILP technique to incorporate best-first
clustering and generate the mention clusters. In all
these studies, however, mention clustering is com-
bined with pairwise classification only at the infer-
ence stage but not at the learning stage.
To perform joint learning of pairwise classifi-
cation and mention clustering, in (McCallum and
Wellner, 2005), each mention pair corresponds to
a binary variable indicating whether the two men-
tions are coreferential, and the dependence between
these variables is modeled by conditional undirected
graphical models. Finley and Joachims (2005) pro-
posed a general SVM-based framework for super-
vised clustering that learns item-pair similarity mea-
sures, and applied the framework to noun phrase
coreference resolution. In our work, we take a differ-
ent approach and apply Markov logic. As we have
shown in Section 3, given the flexibility of Markov
logic, it is straightforward to perform joint learning
of pairwise classification and mention clustering.
In recent years, Markov logic has been widely
used in natural language processing problems (Poon
and Domingos, 2009; Yoshikawa et al 2009; Che
and Liu, 2010). For coreference resolution, the most
notable one is unsupervised coreference resolution
by Poon and Domingos (2008). Poon and Domin-
gos (2008) followed the entity-mention model while
we follow the mention-pair model, which are quite
different approaches. To seek good performance in
an unsupervised way, Poon and Domingos (2008)
highly rely on two important strong indicators:
appositives and predicate nominatives. However,
OntoNotes corpus (state-of-art NLP data collection)
on coreference layer for CoNLL-2011 has excluded
these two conditions of annotations (appositives and
predicate nominatives) from their judging guide-
lines. Compared with it, our methods are more ap-
plicable for real dataset. Huang et al(2009) used
Markov logic to predict coreference probabilities
for mention pairs followed by correlation cluster-
ing to generate the final results. Although they also
perform joint learning, at the inference stage, they
still make pairwise coreference decisions and clus-
ter mentions sequentially. Unlike their method, We
formulate the two steps into a single framework.
Besides combining pairwise classification and
mention clustering, there has also been some work
that jointly performs mention detection and coref-
erence resolution. Daume? and Marcu (2005) de-
veloped such a model based on the Learning as
1252
Search Optimization (LaSO) framework. Rahman
and Ng (2009) proposed to learn a cluster-ranker
for discourse-new mention detection jointly with
coreference resolution. Denis and Baldridge (2007)
adopted an Integer Linear Programming (ILP) for-
mulation for coreference resolution which models
anaphoricity and coreference as a joint task.
6 Conclusion
In this paper we present a joint learning method with
Markov logic which naturally combines pairwise
classification and mention clustering. Experimental
results show that the joint learning method signifi-
cantly outperforms baseline methods. Our method
is also better than all the learning-based systems in
CoNLL-2011 and reaches the same level of perfor-
mance with the best system.
In the future we will try to design more global
constraints and explore deeper relations between
training instances generation and mention cluster-
ing. We will also attempt to introduce more predi-
cates and transform structure learning techniques for
MLN into coreference problems.
Acknowledgments
Part of the work was done when the first author
was a visiting student in the Singapore Manage-
ment University. And this work was partially sup-
ported by the National High Technology Research
and Development Program of China(863 Program)
(No.2012AA011101), the National Natural Science
Foundation of China (No.91024009, No.60973053,
No.90920011), and the Specialized Research Fund
for the Doctoral Program of Higher Education of
China (Grant No. 20090001110047).
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In The First International
Conference on Language Resources and Evaluation
Workshop on Linguistics Coreference, pages 563?566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference pro-
tocols for coreference resolution. In CoNLL Shared
Task, pages 40?44, Portland, Oregon, USA. Associa-
tion for Computational Linguistics.
Wanxiang Che and Ting Liu. 2010. Jointly modeling
wsd and srl with markov logic. In Chu-Ren Huang
and Dan Jurafsky, editors, COLING, pages 161?169.
Tsinghua University Press.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
III Hal Daume? and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In HLT ?05: Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 97?104, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
EMNLP, pages 660?669.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
ACL (Short Papers), pages 45?48. The Association for
Computer Linguistics.
T. Finley and T. Joachims. 2005. Supervised clustering
with support vector machines. In International Con-
ference on Machine Learning (ICML), pages 217?224.
Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-
jun Chen. 2009. Coreference resolution using markov
logic networks. In Proceedings of Computational Lin-
guistics and Intelligent Text Processing: 10th Interna-
tional Conference, CICLing 2009.
M. Klenner. 2007. Enforcing consistency on coreference
sets. In RANLP.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-
hatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of the ACL, pages 135?142.
1253
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT/EMNLP, pages 25?
32.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems, pages 905?912. MIT Press.
J. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In ACL, pages 1396?
1411. The Association for Computer Linguistics.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
EMNLP, pages 650?659.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of map inference for markov logic. In UAI,
pages 468?475. AUAI Press.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011. Re-
laxcor participation in conll shared task on coreference
resolution. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning:
Shared Task, pages 35?39, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Massimo
Poesio. 2011. Multi-metric optimization for coref-
erence: The unitn / iitp / essex submission to the 2011
conll shared task. In Proceedings of the Fifteenth Con-
ference on Computational Natural Language Learn-
ing: Shared Task, pages 61?65, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-
nis Connolly, and Lynette Hirschman. 1995. Amodel-
theoretic coreference scoring scheme. In MUC, pages
45?52.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, Ting
Liu, and Sheng Li. 2008. An entity-mention model for
coreference resolution with inductive logic program-
ming. In ACL, pages 843?851. The Association for
Computer Linguistics.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In ACL/AFNLP,
pages 405?413. The Association for Computer Lin-
guistics.
1254
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1?11,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Event-based Time Label Propagation for Automatic Dating of News Articles
Tao Ge Baobao Chang? Sujian Li Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
No.5 Yiheyuan Road, Haidian District, Beijing, P.R.China, 100871
{getao,chbb,lisujian,szf}@pku.edu.cn
Abstract
Since many applications such as timeline sum-
maries and temporal IR involving temporal
analysis rely on document timestamps, the
task of automatic dating of documents has
been increasingly important. Instead of using
feature-based methods as conventional mod-
els, our method attempts to date documents
in a year level by exploiting relative tempo-
ral relations between documents and events,
which are very effective for dating documents.
Based on this intuition, we proposed an event-
based time label propagation model called
confidence boosting in which time label in-
formation can be propagated between docu-
ments and events on a bipartite graph. The ex-
periments show that our event-based propaga-
tion model can predict document timestamps
in high accuracy and the model combined with
a MaxEnt classifier outperforms the state-of-
the-art method for this task especially when
the size of the training set is small.
1 Introduction
Time is an important dimension of any informa-
tion space and can be useful in information re-
trieval, question-answering systems and timeline
summaries. In the applications involving tempo-
ral analysis, document timestamps are very useful.
For instance, temporal information retrieval mod-
els take into consideration the document?s creation
time for document retrieval and ranking (Kalczyn-
ski and Chou, 2005; Berberich et al, 2007) for bet-
ter dealing with time-sensitive queries; some infor-
?Corresponding author
mation retrieval applications such as Google Scholar
can list articles published during the time a user
specifies for better satisfying users? needs. In addi-
tion, timeline summarization techniques (Hu et al,
2011; Binh Tran et al, 2013) and some event-event
ordering models (Chambers and Jurafsky, 2008;
Yoshikawa et al, 2009) also rely on the timestamps.
Unfortunately, many documents on the web do not
have a credible timestamp, as Chambers (2012) re-
ported. Therefore, it is significant to date docu-
ments, that is to predict document creation time.
One typical method for dating document is based
on temporal language models, which were first used
for dating by de Jong et al (2005). They learned
language models (unigram) for specific time periods
and scored articles with normalized log-likelihood
ratio scores. The other typical approach for the task
was proposed by Nathanael Chambers (2012). In
Chambers?s work, discriminative classifiers ? max-
imum entropy (MaxEnt) classifiers were used by
incorporating linguistic features and temporal con-
straints for training, which outperforms the previous
temporal language models on a subset of Gigaword
Corpus (Graff et al, 2003).
However, the conventional methods have some
limitations because they predict creation time of
documents mainly based on feature-based models
without understanding content of documents, which
may lead to wrong predictions in some cases. For
instance, assume that D1 and D2 are documents
whose content is given as follows:
(D1) Sudan last year accused Eritrea of
backing an offensive by rebels in the east-
ern border region.
1
(D2) Two years ago, Sudan accused Er-
itrea of backing an offensive by rebels in
the eastern border region.
SinceD1 andD2 share many important features, the
previous dating methods are very likely to predict
the same timestamp for the two documents. How-
ever, it will be easy to infer that the creation time of
D1 should be one year earlier than that of D2 if we
analyze the content of the two documents.
Unlike the previous methods, this paper exploits
relative temporal relations between events and doc-
uments for dating documents on the basis of an un-
derstanding of document content.
It is known that each event in a news article has
a relative temporal relation with the document. By
analyzing the relative temporal relation, time of the
event can be known if we know the document times-
tamp; on the other hand, if the time of an event is
known, it can also be used to predict the creation
time of documents mentioning the event, which can
be best demonstrated with the above-mentioned ex-
ample of D1 and D2. In the example, ?last year?
is an important cue to infer that the event mentioned
by the documents occurred in 2002 if we know the
timestamp of D1 is 2003. With the information that
the event occurred in 2002, it can also be inferred
from the temporal expression ?Two years ago? that
D2 was written in 2004. In this way, the timestamp
of the labeled document (D1) is propagated to the
unlabeled document (D2) through the event both of
them mention, which is the main intuition of this pa-
per.
In fact, this intuition seems practical to date doc-
uments on the web because web data is very re-
dundant. Many documents on the web can be con-
nected via events because an event is usually men-
tioned by different documents. According to our
analysis of a collection of news articles spanning 5
years, it is found that an event is mentioned by 3.44
news articles on average; on the other hand, a doc-
ument usually refers to multiple events. Therefore,
if one knows a document timestamp, time of events
the document mentions can be obtained by analyz-
ing the relative temporal relations between the doc-
ument and the events. Likewise, if the time of an
event is known, then it can be used to predict cre-
ation time of the documents which mention it.
Based on the intuition, we proposed an event-
based time label propagation model called confi-
dence boosting in which timestamps are propagated
according to relative temporal relations between
documents and events. In this way, documents can
be dated with an understanding of content so that
this model can date document more credibly. To our
knowledge, it is the first time that the relative tempo-
ral relations between documents and events are ex-
ploited for dating documents, which is proved to be
effective by the experimental results.
2 Event-based Time Label Propogation
As mentioned above, the relative temporal relations
between documents and events are useful for dat-
ing documents. By analyzing the temporal relations,
even if there are only a small number of documents
labeled with timestamps, this information can be
propagated to documents connected with them on a
bipartite graph using breadth first traversal (BFS).
Figure 1: An example of BFS-based propagation
As shown in figure 1, there are two kinds of nodes
in the bipartite graph. A document node is a single
document while an event node represents an event.
The edge between a document node and an event
node means that the document mentions the event.
Also, the edge carries the information of the rela-
tive temporal relation between the document and the
event. The label propagation from node i to node j
will occur if BFS condition which is defined as fol-
lows is satisfied:{
eij ? E
i ? L and j /? L
(BFS condition)
When the timestamp of i is propagated to j:
Y (j) = Y (i) + ?(i, j)
L = L ? {j}
where E is the set of edges of the bipartite graph,
eij denotes the edge between node i and j, L is the
set of nodes which have been already labeled with
timestamps, Y (i) is the year of node i and ?(i, j) is
the relative temporal relation between node i and j.
2
In figure 1, the timestamp of document D1 is 2003,
which is known. This information can be propagated
to its adjacent nodes i.e. the event nodes it men-
tions according to the relative temporal relations.
Then, these event nodes propagate their timestamps
to other documents which mention them. By re-
peating this process, the timestamp of the document
can be propagated to documents which are reachable
from the initially labeled document on the bipartite
graph.
Although the BFS-based propagation process can
propagate timestamps from few labeled documents
to a large number of unlabeled ones, it has two short-
comings for this task. First, once one timestamp is
propagated incorrectly, this error will lead to more
mistakes in the following propagations. If such an
error occurred at the beginning of the propagation
process, it would lead to propagation of errors. Sec-
ond, BFS-based method cannot address conflict of
predictions during propagation, which is shown in
figure 2.
Figure 2: Conflict of predictions during propagation
To address the problems of the BFS-based
method, we proposed a novel propagation model
called confidence boosting model which improves
the BFS-based model by optimizing the global con-
fidence of the bipartite graph. In the confidence
boosting model, every node in the bipartite graph
has a confidence which measures the credibility of
the predicted timestamp of the node. When the
timestamp of a node is propagated to other nodes,
its confidence will be also propagated to the tar-
get nodes with some loss. The loss of confi-
dence is called confidence decay. Formally, the
confidence decay process is described as follows:
c(j) = c(i)? ?(i, j)
where c(i) denotes confidence of node i and
?(i, j) is the decay factor from node i to
node j. For guaranteeing that timestamps
can be propagated on the bipartite graph cred-
ibly, we define the following condition which
is called CB (Confidence Boosting) condition:{
eij ? E
c(i)? ?(i, j) > c(j)
(CB condition)
In the confidence boosting model, propagation from
node i to node j will occur only if CB condition is
satisfied. When timestamps are propagated on the
bipartite graph, timestamps and confidence of nodes
will be updated dynamically. A node with high con-
fidence is more active than nodes with low confi-
dence to propagate its timestamp because a node
with high confidence is more likely to satisfy the CB
condition for propagating its timestamp. Moreover,
a prediction with low confidence can be corrected by
the prediction with high confidence. Therefore, the
confidence boosting model can address both prop-
agation of errors and conflict of predictions which
cannot be tackled by the BFS-based model.
However, there are challenges for running such
propagation models in practice. First, the relative
temporal relations between documents and events
are usually unavailable. Second, events extracted
from different documents do not have any connec-
tion even if they refer to the same event. There-
fore, each event is connected with only one docu-
ment in the bipartite graph and thus cannot prop-
agate its timestamp to other documents unless we
perform event coreference resolution. Third, propa-
gations from generic events are very likely to lead to
propagation errors because generic events can hap-
pen in any year. Also, how to set the confidence and
decay factors reasonably in practice for a confidence
boosting model is worthy of investigation. All these
challenges for the propagation models and their cor-
responding solutions will be discussed in Section 3.
3 Details of Event-based Propagation
Models
In this section, details of the event-based time la-
bel propagation models including challenges and
their corresponding solutions are presented. We first
discuss the event extraction and processing involv-
ing relative temporal relation mining, event coref-
erence resolution and distinguishing specific extrac-
tions from generic ones in Section 3.1. Then, we
show the confidence boosting algorithm in detail in
Section 3.2.
3
3.1 Event extraction and processing
As mentioned in previous sections, events play a key
role in the propagation models. We define an event
as a Subject-Predicate-Object (SPO) triple. To ex-
tract events from raw text, an open information ex-
traction software - ReVerb (Fader et al, 2011) is
used. ReVerb is a program that automatically iden-
tifies and extracts relationships from English sen-
tences. It takes raw text as input and outputs SPO
triples which are called extractions.
However, extractions extracted by ReVerb cannot
be used directly for our propagation models for three
main reasons. First, the relative temporal relations
between documents and the extractions are unavail-
able. Second, the extractions extracted from differ-
ent documents do not have any connection even if
they refer to the same event. Third, propagations
from generic events are very likely to lead to propa-
gation errors.
For addressing the three challenges for the prop-
agation models, we first presented a rule-based
method for mining the relative temporal relations be-
tween extractions and documents in Section 3.1.1.
Then, an efficient event coreference resolution
method is introduced in Section 3.1.2. Finally, the
method for distinguishing specific extractions from
generic ones is shown in Section 3.1.3.
3.1.1 Relative temporal relation mining
We used a rule-based method to extract temporal
expressions and used Stanford parser (De Marneffe
et al, 2006) to analyze association between the tem-
poral expressions and the extractions. Specifically,
we define that an extraction is associated with a tem-
poral expression if there is an arc from the predicate
of the extraction to the temporal expression in the
dependency tree. For a certain extraction, there are
the following four cases whose instances are shown
in table 1 for handling.
Case 1: The extraction is associated with an abso-
lute temporal expressions with year mentions in the
sentence.
In this case, the time of the extraction is equal to
the year mention:
Y (ex) = Y earMention
For the example in table 1, Y (ex) = 1999.
Case 2: The extraction is associated with a relative
temporal expression (not involving year) in the sen-
Case Instance
1 In 1999, South Korea exported 89,000
tons of pork to Japan.
2
In April, however, the BOI investments
showed marked improvement.
Last month, Kazini vowed to resign his
top army job.
3 Julius Erving moved with his family to
Florida three years ago.
4 The meeting focused on ways to revive
the stalled Mideast peace process.
Table 1: Instances of various temporal expressions
tence.
In this case, the time of the extraction is equal to
the creation time of the document:
Y (ex) = Y (d)
Case 3: The extraction is associated with a relative
temporal expression (involving specific year gap) in
the sentence.
In this case, the time of the extraction is computed
as follows:
Y (ex) = Y (d)? Y earGap
For the example in table 1, Y (ex) = Y (d)? 3.
Case 4: The extraction is not associated with any
temporal expression in the sentence or the other
cases.
In this case, it is difficult to recognize the rela-
tive temporal relations. However, timeliness can be
leveraged to determine the relations as a heuristic
method. It is known that timeliness is an important
feature of news so that events reported by a news ar-
ticle usually took place a couple of days or weeks
before the article was written. Therefore, we heuris-
tically consider the year of the extraction is the same
with that of its source document in this case:
Y (ex) = Y (d)
In the cases except case 1, the relative tempo-
ral relation between an extraction and the docu-
ment it comes from can be determined. To evalu-
ate the performance of the rule-based method, we
sampled 3,000 extractions from documents written
in the year of 1995-1999 of Gigaword corpus and
manually labeled these extractions with a timestamp
based on their context and their corresponding docu-
ment timestamps as golden standard. Table 2 shows
4
the accuracy of each case which will be used as a
part of the decay factor in the confidence boosting
model.
Case Accuracy
1 0.774(168/217)
2 0.994(844/849)
3 0.836(281/336)
4 0.861(1376/1598)
Total 0.890(2669/3000)
Table 2: Accuracy of the four cases
We define the set of these determined relative tem-
poral relations R as follows:
R = {rd,ex|d = doc(ex), ex ? C2 ? C3 ? C4}
rd,ex =< d, ex, ?(d, ex) >
?(d, ex) = ??(ex, d) = {0,?1,?2,?3, ...}
where Ck is the set of extractions in case k and
doc(ex) is the document which extraction ex comes
from. rd,ex is a triple describing the relative tempo-
ral relation between d and ex. For example, triple
rd,ex =< d, ex,?1 > means that the time of ex-
traction ex is one year before the time of document
d.
3.1.2 Event coreference resolution
Extractions from different documents have no
connections. However, there are a great number of
extractions referring to the same event. For find-
ing such coreferential event extractions efficiently,
hierarchical agglomerative clustering (HAC) is used
to cluster highly similar extractions into one cluster.
We use cosine to measure the similarity between ex-
tractions and select bag of words as features. Note
that it is less meaningful to cluster the extractions
from the same document because coreferential ex-
tractions from the same document are not helpful for
timestamp propagations. For this reason, similarity
between extractions from the same documents is set
to 0.
For HAC, selection of threshold is important. If
the threshold is set too high, only a few extractions
can be clustered despite high purity; on the contrary,
if the threshold is set too low, purity of clusters will
descend. In fact, selection of threshold is a trade-off
between the precision and recall of event corefer-
ence resolution. For selecting a suitable threshold,
extractions from documents written in 1995-1999
are used as a development set.
In practice, it is difficult for us to directly evalu-
ate the performance of the coreference resolution of
event extractions without golden standard which re-
quires much labors for manual annotations. Alterna-
tively, entropy which measures the purity of clusters
is used for evaluation because it can indirectly re-
flect the precision of coreference resolution to some
extent:
Entropy = ?
?
j
nj
n
?
i
P (i, j)? log2 P (i, j)
where P (i, j) is the probability of finding an extrac-
tion whose timestamp is i in the cluster j, nj is the
number of items in cluster j and n is the total num-
ber of extractions. Note that timestamp of an extrac-
tion is assigned based on its document timestamp
using the method proposed in Section 3.1.1.
Figure 3 shows the effect of selection of the
threshold on cluster performance. It can be found
that when the threshold reaches 0.8, the entropy
starts descending gently and is low enough. Since
we want to find as many coreferential extractions as
possible on the premise that the precision is good,
the threshold is set to 0.8. Note that extractions
which are single in one cluster will be filtered out
because they do not have any connections with any
other documents.
0.6 0.65 0.7 0.75 0.8 0.85 0.90.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
Threshold
Ent
ropy
Figure 3: Entropy of clusters under different thresholds
3.1.3 Distinguishing specific events from
generic ones
Not all extractions extracted by ReVerb refer to
a specific event. For instance, the extraction ?Ger-
many?s DAX index was down 0.2 percent? is un-
desirable for our task because it refers to a generic
5
event and this event may occur in any year. In other
words, it is not able to indicate a certain timestamp
and thus propagations from a generic event node are
very likely to result in propagation errors. In con-
trast, the extraction ?some of the provinces in China
were hit by SARS? refers to a specific event which
took place in 2003. For our task, such specific event
extractions which are associated with one certain
timestamp are desirable. For the sake of distinguish-
ing such extractions from the generic ones, a Max-
Ent classifier is used to classify extractions as either
specific ones or generic ones.
Training Set Generation A training set is indis-
pensable for training a MaxEnt classifier. In order
to generate training examples, we performed HAC
discussed in Section 3.1.2 for event coreference res-
olution on extractions from all documents written
in May and June of 1995-1999 and then analyzed
each cluster. If extractions in a cluster have different
timestamps, then the extractions in this cluster will
be labeled as generic extractions (negative); other-
wise, extractions in the cluster are labeled as spe-
cific ones (positive). In this way, the training set can
be generated without manually labeling. To avoid
bias of positive and negative examples, we sampled
3,500 positive examples and 3,500 negative exam-
ples to train the model.
Feature Selection The following features were se-
lected for training:
Named Entities: People and places are often dis-
cussed during specific time periods, particularly in
news genre. Intuitively, if an extraction contains
specific named entities then this extraction is less
likely to be a generic event. If an extraction con-
tains named entities, types and uninterrupted tokens
of the named entities will be included as features.
Numeral: According to our analysis of the train-
ing set generated by the above-mentioned method,
generic extractions usually contain numerals. For
example, the extraction ?15 people died in this ac-
cident? and the extraction ?225 people died in this
accident? have the same tokens except numerals and
they are labeled as a generic event because they are
clustered into one group due to high similarity but
they in fact refer to different events happening in
different years. Therefore, if an extraction contains
numerals, the feature ?NUM? will be included.
Bag of words: Bag of words can also be an indicator
of specific extractions and generic ones. For exam-
ple, an extraction containing ?stock?, ?index?, ?fell?
and ?exchange? is probably a generic one.
The model obtained after training can be used to
predict whether an extraction is a specific one. We
define P (S = 1|ex) as the probability that an ex-
traction is a specific one, which can be provided by
the classifier. Extractions whose probability to be a
specific one is less than 0.05 are filtered out. For the
other extractions, this probability is used as a part of
the decay factor in the confidence boosting model,
which will be discussed in detail in Section 3.2.
3.2 Confidence boosting
After extracting and processing the event extrac-
tions, relative temporal relations between documents
and events can be constructed. This can be for-
mally represented by a bipartite graph G=?V,E?.
There are two kinds of nodes on the bipartite graph:
document nodes and event nodes. Slightly dif-
ferent with the event node mentioned in Section
2, an event node in practice is a cluster of coref-
erential extractions and it can be connected with
multiple document nodes. Note that the bipar-
tite graph does not contain any isolate node. For
briefness, we define DNode as the set of docu-
ment nodes and ENode as the set of event nodes.
The set of edges E is formally defined as follows:
E = {eij , eji|i ? DNode, j ? ENode, ri,j ? R}
where R is the set of relative temporal relations de-
fined as Section 3.1.1.
3.2.1 Confidence and decay factor
As mentioned in Section 2, the confidence of a
node measures the credibility of the predicted times-
tamp. According to the definition, we set the confi-
dence of initially labeled nodes to 1 and set confi-
dence of nodes without any timestamp to 0 in prac-
tice. When the timestamp of a node is propagated
to another node, its confidence will be propagated to
the target node with some loss, as discussed in Sec-
tion 2. The confidence loss is caused by two factors
in practice. The first one is the credibility of the rel-
ative temporal relation between two nodes and the
other one depends on whether an extraction refers to
a specific event.
Relative temporal relations between documents
6
and extractions we mined using the rule-based
method in Section 3.1.1 are not absolutely correct.
The credibility of the relations has an effect on the
confidence decay. Formally, we used pi(i, j) to de-
note the credibility of the relative temporal relation
between node i and node j. The credibility of a rel-
ative temporal relation in each case can be estimated
through table 2. If the credibility of the relative tem-
poral relation between i and j is low, propagation
from node i to j probably leads to error. Therefore,
the confidence loss should be much in this case. On
contrary, if the relation is highly credible, it will be
less likely that propagation errors occur. Therefore,
the confidence loss should be little.
In addition, whether an extraction refers to a
generic event or a specific one exerts an impact on
the confidence loss. If an extraction refers to a
generic event, then the extractions in the same clus-
ter with it probably have different timestamps. Since
our propagation model assumes that extractions in a
cluster are coreferent and thus they should have the
same timestamp, propagations from a generic event
node are very likely to result in propagation errors.
Therefore, the timestamp of a generic event node
in fact is less credible for propagations and confi-
dence of such event nodes should be low for limiting
propagations from the nodes. For this reason, prop-
agation from a document node to a generic event
node leads to much loss of confidence. We define
the probability that an event node refers to a specific
event as follows:
P (S = 1|enode) =
1
|C|
?
ex?C
P (S = 1|ex)
where C is the set of extractions in the event node
and P (S = 1|ex) is the probability that an extrac-
tion refers to a specific event, which can be provided
by the MaxEnt classifier discussed in Section 3.1.3.
Considering the two factors for confidence loss,
we formally define the decay factor by (1).
?(s, t) = (1)
{
pi(s, t) if t ? DNode
pi(s, t)? P (S = 1|t) otherwise
3.2.2 Confidence boosting algorithm
In confidence boosting model, the propagation
from i to j will occur only if the CB condition is
Figure 4: Algorithm of confidence boosting
satisfied. The confidence boosting propagation pro-
cess can be described as figure 4.
Whenever timestamps are propagated to other
nodes, the global confidence of the bipartite graph
will increase. For this reason, this propagation pro-
cess is called confidence boosting. In this model,
a node with high confidence is more active than
nodes with low confidence to propagate its times-
tamp. Moreover, a prediction with low confidence
can be corrected by the prediction with high con-
fidence. Therefore, the confidence boosting model
can alleviate the problem of propagation of errors
to some extent and handle conflict of predictions.
Thus, it can propagate timestamps more credibly
than the BFS-based model. It can also be proved
that each node on the bipartite graph must reach the
highest confidence it can reach so that the global
confidence of the bipartite graph must be optimal
when the confidence boosting propagation process
ends regardless of propagation orders, which will be
discussed in Section 3.2.3.
3.2.3 Proof of the optimality of confidence
boosting
Proof by contradiction can be used to prove that
propagation orders do not affect the optimality of the
confidence boosting model.
Proof Assume by contradiction that there is some
node that does not reach its highest confidence it can
reach when a confidence boosting process in propa-
gation order A ends:
?vt s.t. cA(vt) < c?(vt)
where cA(vt) is the confidence of vt when the
propagation process in order A ends and c?(vt) is
the highest confidence that vt can reach. Assume
that (v1, v2, ? ? ? , vt?1, vt) is the optimal propagation
7
path from the propagation source node v1 to the
node vt that leads to the highest confidence of vt,
which means that c?(vt) = c?(vt?1) ? ?(vt?1, vt),
c?(vt?1) = c?(vt?2) ? ?(vt?2, vt?1), ..., c?(v2) =
c?(v1) ? ?(v1, v2). Then according to CB condi-
tion, since cA(vt?1) ? ?(vt?1, vt) ? cA(vt) <
c?(vt) = c?(vt?1) ? ?(vt?1, vt), the inequality
cA(vt?1) < c?(vt?1) must hold. Similarly, it can be
easily inferred that cA(vt?2) < c?(vt?2) and finally
cA(v1) < c?(v1). Since v1 is the source node whose
timestamp is initially labeled and its confidence is 1,
the inequality cA(v1) < c?(v1) cannot hold. Thus,
the assumption that cA(vt) < c?(vt) cannot be sat-
isfied. Therefore, it can be proved that each node
on the bipartite graph must reach the highest con-
fidence it can reach so that the global confidence of
the bipartite graph must be optimal when confidence
boosting propagation process ends no matter what
order time labels are propagated in.
4 Experiments
In this section, we evaluate the performance of our
time label propagation models and different auto-
matic document dating models on the Gigaword
dataset. We first present the experimental setting.
Then we show experimental results and perform an
analysis.
4.1 Experimental Setting
Dataset To simulate the environment of the web
where data is very redundant, we use all documents
written in April, June, July and September of 2000-
2004 of Gigaword Corpus as dataset instead of sam-
pling a subset of documents from each period. The
dataset contains 900,199 news articles.
Pre-processing Many extractions extracted by Re-
Verb are short and uninformative and do not carry
any valuable information for propagating temporal
information. Also, some extractions do not refer
to events which already happened. These extrac-
tions may affect the performance of event corefer-
ence resolution and the rule-based method proposed
in Section 3.1.1 for mining relative temporal rela-
tions. Therefore, we filter out these undesirable ex-
tractions in advance with a rule-based method. The
rules are shown in table 3. This preprocessing re-
moves large numbers of ?bad? extractions which are
undesirable for our task. As a result, not only com-
putation efficiency but also precision of event coref-
erence resolution will be improved.
Rule1 If the number of tokens of the extrac-
tion is less than 5 then this extraction
will be filtered out.
Rule2 If the maximum idf of terms of the ex-
traction is less than 3.0 then this ex-
traction will be filtered out.
Rule3 If the tense of the extraction is not past
tense then this extraction will be fil-
tered out.
Rule4 If the extraction is the content of di-
rect quotation then this extraction will
be filtered out.
Table 3: Pre-processing Rules
|DNode| 550,124
|ENode| 968,064
|E| 3,104,666
Table 4: Basic information of the bi-partite graph
Basic information of the document-event bipartite
graph constructed is shown in table 4.
Evaluation To evaluate the performance of the
propagation models for the task of dating on differ-
ent sizes of the training set, we used different sizes
of the labeled documents for training and consid-
ered the remaining documents as the test set. Note
that the training set is randomly sampled from the
dataset. To be more persuasive, we repeated above
experiments for five times.
However, in the time label propagation process,
not all documents can be labeled. For those doc-
uments which cannot be labeled in the process of
propagation, a MaxEnt classifier serves as a comple-
mentary approach to predict their timestamps. For
the MaxEnt classifier, unigrams and named entities
are simply selected as features and the initially la-
beled documents as well as documents labeled dur-
ing propagation process are used for training.
Baseline methods are temporal language models
proposed by de Jong et al (2005) and the state-of-
the-art discriminative classifier with linguistic fea-
tures and temporal constraints which was proposed
8
Initially Labeled 1k 5k 10k 50k 100k 200k 500k
Reached Min 443980 448653 453022 484562 518603 599724 732701
Reached Max 444266 448998 454028 484996 519333 579878 732799
Reached Avg 444107 448742 453786 484622 519110 579835 732758
Prop Ratio 444.1 89.7 45.4 9.7 5.2 2.9 1.5
Prop acc(BFS) 0.438 0.515 0.551 0.646 0.691 0.725 0.775
Prop acc(CB) 0.494 0.569 0.603 0.701 0.746 0.776 0.807
Table 5: Performance of Propagation
Initially Labeled 1k 5k 10k 50k 100k 200k 500k
Temporal LMs 0.277 0.323 0.353 0.412 0.422 0.425 0.420
Maxent(Unigrams) 0.326 0.378 0.407 0.486 0.517 0.553 0.590
Maxent(Unigrams+NER) 0.331 0.383 0.418 0.506 0.549 0.590 0.665
Chambers?s 0.331 0.386 0.423 0.524 0.571 0.615 0.690
BFS+Maxent 0.459 0.508 0.533 0.595 0.626 0.658 0.707
CB+Maxent 0.486 0.535 0.559 0.624 0.655 0.685 0.726
Table 6: Overall accuracy of dating models
by Nathanael Chambers (2012). In Chambers?s joint
model, the interpolation parameter ? is set to 0.35
which is considered optimal in his work.
4.2 Experimental Results
Table 5 shows the performance of propagation mod-
els where Reached denotes the number of docu-
ments labeled when the propagation process ends,
prop ratio and prop accuracy are defined as follows:
Prop Ratio =
#ReachedDocNodes
#LabeledDocNodes
Prop Accuracy =
#CorrectDocNodes?#LabeledDocNodes
#ReachedDocNodes?#LabeledDocNodes
where #LabeledDocNodes is the number of ini-
tially labeled document nodes which are documents
in the training set and #ReachedDocNodes is the
number of document nodes labeled when the propa-
gation process ends.
Note that prop ratio and accuracy in table 5 are
the mean of the prop ratio and accuracy of the five
groups of experiments. It is clear that confidence
boosting model improves the prop accuracy over
BFS-based model. When only 1,000 documents
are initially labeled with timestamps, the confidence
boosting model can propagate their timestamps to
more than 400,000 documents with an accuracy of
0.494, approximately 12.8% relative improvement
over the BFS counterpart, which proves effective-
ness of the confidence boosting model.
However, as shown in table 5, hardly can the prop-
agation process propagate timestamps to all doc-
uments. One reason is that the number of docu-
ment nodes on the bipartite graph is only 550,124,
approximately 61.1% of all documents. The other
documents may not mention events which are also
mentioned by other documents, which means they
are isolate and thus are excluded from the bipartite
graph. Also, the event coreference resolution phase
does not guarantee finding all coreferential extrac-
tions; in other words, recall of event coreference res-
olution is not 100%. The other reason is that some
documents are unreachable from the initially labeled
nodes even if they are in the bipartite graph.
The overall accuracy of different dating models
is shown in table 6. As with table 5, overall accu-
racy in table 6 is the average performance of mod-
els in the five groups of experiments. As reported
by Nathanael Chambers (2012), the discriminative
classifier performs much better than the temporal
language models on the Gigaword dataset. In the
case of 500,000 training examples, the Maxent clas-
sifier using unigram features outperforms the tem-
poral language models by 40.5% relative accuracy.
If the size of the training set is large enough, named
9
entities and linguistic features as well as temporal
constraints will improve the overall accuracy sig-
nificantly. However, if the size of the training set
is small, these features will not result in much im-
provement.
Compared with the previous models, the propaga-
tion models predict the document timestamps much
more accurately especially in the case where the size
of the training set is small. When the size of the
training set is 1,000, our BFS-based model and con-
fidence boosting model combined with the MaxEnt
classifier outperform Chambers?s joint model which
is considered the state-of-the-art model for the task
of automatic dating of documents by 38.7% and
46.8% relative accuracy respectively. This is be-
cause the feature-based methods are not very reli-
able especially when the size of the training set is
small. In contrast, our propagation models can pre-
dict timestamps of documents with an understand-
ing of document content, which allows our method
to date documents more credibly than the baseline
methods. Also, by comparing table 5 with table 6,
it can be found that prop accuracy is almost always
higher than overall accuracy, which also verifies that
the propagation models are more credible for dat-
ing document than the feature-based models. More-
over, data is so redundant that a great number of
documents can be connected with events they share.
Therefore, even if a small number of documents are
labeled, the labeled information can be propagated
to large numbers of articles through the connections
between documents and events according to relative
time relations. Even if the size of the training set
is large, e.g. 500,000, our propagation models still
outperform the state-of-the-art dating method. Ad-
ditionally, some event nodes on the bipartite graph
may be labeled with a timestamp during the process
of propagation as a byproduct. The temporal infor-
mation of the events would be useful for other tem-
poral analysis tasks.
5 Related Work
In addition to work of de Jong et al (2005) and
Chambers (2012) introduced in previous sections,
there is also other research focusing on the task of
document dating. Kanhabua and Norvag (2009) im-
proved temporal language models by incorporating
temporal entropy and search statistics and apply-
ing two filtering techniques to the unigrams in the
model. Kumar et al (2011) is also based on the
temporal language models, but more historically-
oriented, which models the timeline from the present
day back to the 18th century. In addition, they used
KL-divergence instead of normalized log likelihood
ratio to measure differences between a document
and a time period?s language model.
However, these methods are based on tempo-
ral language models so they also suffer from the
problem of the method of de Jong et al (2005).
Therefore, they inevitably make wrong predictions
in some cases, just as mentioned in Section 1. Com-
pared with these methods, our event-based propaga-
tion models exploit relative temporal relations be-
tween documents and events for dating document
on a basis of an understanding of document content,
which is more reasonable and also proved to be more
effective by the experimental results.
6 Conclusion
The main contribution of this paper is exploiting
relative temporal relations between events and doc-
uments for the document dating task. Different
with the conventional work which dates documents
with feature-based methods, we proposed an event-
based time label propagation model called confi-
dence boosting in which timestamps are propagated
on a document-event bipartite graph according to
relative temporal relations between documents and
events for dating documents on a basis of an under-
standing of document content. We discussed chal-
lenges for the propagation models and gave the cor-
responding solutions in detail. The experimental re-
sults show that our event-based propagation model
can predict document timestamps in high accuracy
and the model combined with a MaxEnt classifier
outperforms the state-of-the-art method on a data-
redundant dataset.
Acknowledgements
We thank the anonymous reviewers for their valu-
able suggestions. This paper is supported by
NSFC Project 61075067, NSFC Project 61273318
and National Key Technology R&D Program (No:
2011BAH10B04-03).
10
References
Klaus Berberich, Srikanta Bedathur, Thomas Neumann,
and Gerhard Weikum. 2007. A time machine for
text search. In Proceedings of the 30th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 519?526.
ACM.
Giang Binh Tran, Mohammad Alrifai, and Dat
Quoc Nguyen. 2013. Predicting relevant news events
for timeline summaries. In Proceedings of the 22nd
international conference on World Wide Web compan-
ion, pages 91?92. International World Wide Web Con-
ferences Steering Committee.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
698?706. Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers-
Volume 1, pages 98?106. Association for Computa-
tional Linguistics.
FMG de Jong, Henning Rode, and Djoerd Hiemstra.
2005. Temporal language models for the disclosure
of historical text. Royal Netherlands Academy of Arts
and Sciences.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449?454.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1535?1545. Association for Computational Linguis-
tics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2003. English gigaword. Linguistic Data Consortium,
Philadelphia.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic ret-
rospection. In Data Mining (ICDM), 2011 IEEE 11th
International Conference on, pages 260?269. IEEE.
Pawel Jan Kalczynski and Amy Chou. 2005. Temporal
document retrieval model for business news archives.
Information processing management, 41(3):635?650.
Nattiya Kanhabua and Kjetil N?rva?g. 2009. Us-
ing temporal language models for document dating.
In Machine Learning and Knowledge Discovery in
Databases, pages 738?741. Springer.
Abhimanu Kumar, Matthew Lease, and Jason Baldridge.
2011. Supervised language modeling for temporal res-
olution of texts. In Proceedings of the 20th ACM in-
ternational conference on Information and knowledge
management, pages 2069?2072. ACM.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language: Volume 1-Volume 1, pages 405?
413. Association for Computational Linguistics.
11
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1774?1778,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Joint Learning of Chinese Words, Terms and Keywords
Ziqiang Cao
1
Sujian Li
1
Heng Ji
2
1
Key Laboratory of Computational Linguistics, Peking University, MOE, China
2
Computer Science Department, Rensselaer Polytechnic Institute, USA
{ziqiangyeah, lisujian}@pku.edu.cn jih@rpi.edu
Abstract
Previous work often used a pipelined
framework where Chinese word segmen-
tation is followed by term extraction and
keyword extraction. Such framework suf-
fers from error propagation and is un-
able to leverage information in later mod-
ules for prior components. In this paper,
we propose a four-level Dirichlet Process
based model (DP-4) to jointly learn the
word distributions from the corpus, do-
main and document levels simultaneously.
Based on the DP-4 model, a sentence-wise
Gibbs sampler is adopted to obtain proper
segmentation results. Meanwhile, terms
and keywords are acquired in the sampling
process. Experimental results have shown
the effectiveness of our method.
1 Introduction
For Chinese language which does not contain ex-
plicitly marked word boundaries, word segmenta-
tion (WS) is usually the first important step for
many Natural Language Processing (NLP) tasks
including term extraction (TE) and keyword ex-
traction (KE). Generally, Chinese terms and key-
words can be regarded as words which are repre-
sentative of one domain or one document respec-
tively. Previous work of TE and KE normally used
the pipelined approaches which first conducted
WS and then extracted important word sequences
as terms or keywords.
It is obvious that the pipelined approaches are
prone to suffer from error propagation and fail to
leverage information for word segmentation from
later stages. Here, we provide one example in the
disease domain, to demonstrate the common prob-
lems in current pipelined approaches and propose
the basic idea of our joint learning of words, terms
and keywords.
Example: @??(thrombocytopenia) (with)
{? (heparinoid) 	(have) s?(relation).
This is a correctly segmented Chinese sen-
tence. The document containing the example sen-
tence mainly talks about the property of ?{?
 (heparinoid)? which can be regarded as one key-
word of the document. At the same time, the
word@??(thrombocytopenia) appears fre-
quently in the disease domain and can be treated
as a domain-specific term.
However, for such a simple sentence, current
segmentation tools perform poorly. The segmen-
tation result with the state-of-the-art Conditional
Random Fields (CRFs) approach (Zhao et al.,
2006) is as follows:
@(blood platelet) ?(reduction) ?(symptom)
{(of same kind) ?(liver) 	(always)s?(relation)
where @?? is segmented into three com-
mon Chinese words and {? is mixed with its
neighbors.
In a text processing pipeline of WS, TE and
KE, it is obvious that imprecise WS results will
make the overall system performance unsatisfy-
ing. At the same time, we can hardly make use of
domain-level and document-level information col-
lected in TE and KE to promote the performance
of WS. Thus, one question comes to our minds:
can words, terms and keywords be jointly learned
with consideration of all the information from the
corpus, domain, and document levels?
Recently, the hierarchical Dirichlet process
(HDP) model has been used as a smoothed bigram
model to conduct word segmentation (Goldwater
et al., 2006; Goldwater et al., 2009). Meanwhile,
one strong point of the HDP based models is that
they can model the diversity and commonality in
multiple correlated corpora (Ren et al., 2008; Xu
et al., 2008; Zhang et al., 2010; Li et al., 2012;
Chang et al., 2014). Inspired by such existing
work, we propose a four-level DP based model,
1774
0G1GwH mwH imjw imwH1mwH NmmwH3? ?????jmN2?1?0? M| |V
Figure 1: DP-4 Model
named DP-4, to adapt to three levels: corpus, do-
main and document. In our model, various DPs
are designed to reflect the smoothed word distri-
butions in the whole corpus, different domains and
different documents. Same as the DP based seg-
mentation models, our model can be easily used
as a semi-supervised framework, through exerting
on the corpus level the word distributions learned
from the available segmentation results. Refer-
ring to the work of Mochihashi et al. (2009), we
conduct word segmentation using a sentence-wise
Gibbs sampler, which combines the Gibbs sam-
pling techniques with the dynamic programming
strategy. During the sampling process, the impor-
tance values of segmented words are measured in
domains and documents respectively, and words,
terms and keywords are jointly learned.
2 DP-4 Model
Goldwater et al. (2006) applied the HDP model on
the word segmentation task. In essence, Goldwa-
ter?s model can be viewed as a bigram language
model with a unigram back-off. With the lan-
guage model, word segmentation is implemented
by a character-based Gibbs sampler which repeat-
edly samples the possible word boundary posi-
tions between two neighboring words, conditioned
on the current values of all other words. How-
ever, Goldwater?s model can be deemed as mod-
eling the whole corpus only, and does not distin-
guish between domains and documents. To jointly
learn the word information from the corpus, do-
main and document levels, we extend Goldwater?s
model by adding two levels (domain level and doc-
ument level) of DPs, as illustrated in Figure 1.
2.1 Model Description
M DPs (H
m
w
;1 ? m ? M ) are designed specif-
ically to word w to model the bigram distribu-
tions in each domain and these DPs share an
overall base measure H
w
, which is drawn from
DP (?
0
, G
1
) and gives the bigram distribution for
the whole corpus. Assuming the m
th
domain in-
cludes N
m
documents, we use H
m
j
w
(1 ? j ?
N
m
) to model the bigram distribution of the i
th
document in the domain. Usually, given a do-
main, the bigram distributions of different docu-
ments are not conditionally independent and simi-
lar documents exhibit similar bigram distributions.
Thus, the bigram distribution of one document is
generated according to both the bigram distribu-
tion of the domain and the bigram distributions
of other documents in the same domain. That is,
H
m
j
w
? g(?
3
, H
m
w
, H
m
?j
w
) where H
m
?j
w
repre-
sents the bigram distributions of the documents in
the m
th
domain except the j
th
document. Assum-
ing the j
th
document in the m
th
domain contains
N
j
m
words, each word is drawn according toH
m
j
w
.
That is, w
m
j
i
? H
m
j
w
(1 ? i ? N
j
m
). Thus, our
four-level DP model can be summarized formally
as follows:
G
1
? DP (?
0
, G
0
) ;H
w
? DP (?
1
, G
1
)
H
m
w
? DP (?
2
, H
w
) ;H
m
j
w
? g
(
?
3
, H
m
w
, H
m
?j
w
)
w
m
j
i
|w
i?1
= w ? H
d
w
Here, we provide for our model the Chinese
Restaurant Process (CRP) metaphor, which can
create a partition of items into groups. In our
model, the word type of the previous word w
i?1
corresponds to a restaurant and the current word
w
i
corresponds to a customer. Each domain is
analogous to a floor in a restaurant and a room de-
notes a document. Now, we can see that there are
|V | restaurants and each restaurant consists of M
floors. Them
th
floor containsN
m
rooms and each
room has an infinite number of tables with infinite
seating capacity. Customers enter a specific room
on a specific floor of one restaurant and seat them-
selves at a table with the label of a word type. Dif-
ferent from the standard HDP, each customer sits
at an occupied table with probability proportional
to both the numbers of customers already seated
there and the numbers of customers with the same
word type seated in the neighboring rooms, and at
an unoccupied table with probability proportional
to both the constant ?
3
and the probability that the
1775
customers with the same word type are seated on
the same floor.
2.2 Model Inference
It is important to build an accurate G
0
which de-
termines the prior word distribution p
0
(w). Sim-
ilar to the work of Mochihashi et al. (2009), we
consider the dependence between characters and
calculate the prior distribution of a word w
i
using
the string frequency statistics (Krug, 1998):
p
0
(w
i
) =
n
s
(w
i
)
?
n
s
(.)
(1)
where n
s
(w
i
) counts the character string com-
posed of w
i
and the symbol ?.? represents any
word in the vocabulary V .
Then, with the CRP metaphor, we can obtain the
expected word unigram and bigram distributions
on the corpus level according to G
1
and H
w
:
p
1
(w
i
) =
n (w
i
) + ?
0
p
0
(w
i
)
?
n (.) + ?
0
(2)
p
2
(w
i
|w
i?1
= w) =
n
w
(w
i
) + ?
1
p
1
(w
i
)
?
n
w
(.) + ?
1
(3)
where the subscript numbers indicate the corre-
sponding DP levels. n(w
i
) denotes the number of
w
i
and n
w
(w
i
) denotes the number of the bigram
< w,w
i
> occurring in the corpus. Next, we can
easily get the bigram distribution on the domain
level by extending to the third DP.
p
m
3
(w
i
|w
i?1
= w) =
n
m
w
(w
i
) + ?
2
p
2
(w
i
|w
i?1
)
?
n
m
w
(.) + ?
2
(4)
where n
m
w
(w
i
) is the number of the bigram <
w,w
i
> occurring in the m
th
domain.
To model the bigram distributions on the docu-
ment level, it is beneficial to consider the influence
of related documents in the same domain (Wan
and Xiao, 2008). Here, we only consider the in-
fluence from theK most similar documents with a
simple similarity metric s(d
1
, d
2
) which calculates
the Chinese character overlap ratio of two docu-
ments d
1
and d
2
. Let d
j
m
denote the j
th
document
in the m
th
domain and d
j
m
[k](1 ? k ? K) the K
most similar documents. d
j
m
can be deemed to be
?lengthened? by d
j
m
[k](1 ? k ? K). Therefore,
we estimate the count of w
i
in d
j
m
as:
t
d
j
m
w
(w
i
) = n
d
j
m
w
(w
i
)+
?
k
s(d
j
m
[k], d
j
m
)n
d
j
m
[k]
w
(w
i
)
(5)
where n
d
j
m
[k]
w
(w
i
) denotes the count of the bigram
< w,w
i
> occurring in d
j
m
[k]. Next, we model
the bigram distribution in d
j
m
as a DP with the base
measure H
m
w
:
p
d
j
m
4
(w
i
|w
i?1
= w) =
t
d
j
m
w
(w
i
) + ?
3
p
m
3
(w
i
|w
i?1
)
?
t
d
j
m
w
(.) + ?
3
(6)
With CRP, we can also easily estimate the un-
igram probabilities p
m
3
(w
i
) and p
d
j
m
4
(w
i
) respec-
tively on the domain and document levels, through
combining all the restaurants.
To measure whether a word is eligible to be a
term, the score function TH
m
(?) is defined as:
TH
m
(w
i
) =
p
m
3
(w
i
)
p
1
(w
i
)
(7)
This equation is inspired by the work of Nazar
(2011), which extracts terms with consideration of
both the frequency in the domain corpus and the
frequency in the general reference corpus. Similar
to Eq. 7, we define the functionKH
d
j
m
(?) to judge
whether w
i
is an appropriate keyword.
KH
d
j
m
(w
i
) =
p
d
j
m
4
(w
i
)
p
1
(w
i
)
(8)
During each sampling, we make use of Eqs. (7)
and (8) to identify the most possible terms and
keywords. Once a word is identified as a term
or keyword, it will drop out of the sampling pro-
cess in the following iterations. Its CRP explana-
tion is that some customers (terms and keywords)
find their proper tables and keep sitting there after-
wards.
2.3 Sentence-wise Gibbs Sampler
The character-based Gibbs sampler for word seg-
mentation (Goldwater et al., 2006) is extremely
slow to converge, since there exists high correla-
tion between neighboring words. Here, we intro-
duce the sentence-wise Gibbs sampling technique
as well as efficient dynamic programming strat-
egy proposed by Mochihashi et al. (2009). The
basic idea is that we randomly select a sentence
in each sampling process and use the Viterbi al-
gorithm (Viterbi, 1967) to find the optimal seg-
mentation results according to the word distribu-
tions derived from other sentences. Different from
Mochihashi?s work, once terms or keywords are
1776
identified, we do not consider them in the segmen-
tation process. Due to space limitation, the algo-
rithm is not detailed here and can be referred in
(Mochihashi et al., 2009).
3 Experiment
3.1 Data and Setting
It is indeed difficult to find a standard evaluation
corpus for our joint tasks, especially in different
domains. As a result, we spent a lot of time to col-
lect and annotate a new corpus
1
composed of ten
domains (including Physics, Computer, Agricul-
ture, Sports, Disease, Environment, History, Art,
Politics and Economy) and each domain is com-
posed of 200 documents. On average each doc-
ument consists of about 4800 Chinese characters.
For these 2000 documents, three annotators have
manually checked the segmented words, terms and
keywords as the gold standard results for evalu-
ation. As we know, there exists a large amount
of manually-checked segmented text for the gen-
eral domain, which can be used as the training data
for further segmentation. As with other nonpara-
metric Bayesian models (Goldwater et al., 2006;
Mochihashi et al., 2009), our DP-4 model can be
easily amenable to semi-supervised learning by
imposing the word distributions of the segmented
text on the corpus level. The news texts pro-
vided by Peking University (named PKU corpus)
2
is used as the training data. This corpus contains
about 1,870,000 Chinese characters and has been
manually segmented into words.
In our experiments, the concentration coeffi-
cient (?
0
) is finally set to 20 and the other three
(?
1?3
) are set to 15. The parameter K which con-
trols the number of similar documents is set to 3.
3.2 Performance Evaluation
The following baselines are implemented for com-
parison of segmentation results: (1) Forward max-
imum matching (FMM) algorithm with a vocab-
ulary compiled from the PKU corpus; (2) Re-
verse maximum matching (RMM) algorithm with
the compiled vocabulary; (3) Conditional Random
Fields (CRFs)
3
based supervised algorithm trained
from the PKU corpus; (4) HDP based semi-
supervised algorithm (Goldwater et al., 2006) us-
1
Nine domains are from http://www.datatang.
com/data/44139 and we add an extra Disease domain.
2
http://icl.pku.edu.cn
3
We adopt CRF++(http://crfpp.googlecode.
com/svn/trunk/doc/index.html)
ing the PKU corpus. The strength of Mochi-
hashi et al. (2009)?s NPYLM based segmentation
model is its speed due to the sentence-wise sam-
pling technique, and its performance is similar to
Goldwater et al. (2006)?s model. Thus, we do not
consider the NPYLM based model for compari-
son here. Then, the segmentation results of FMM,
RMM, CRF, and HDP methods are used respec-
tively for further extracting terms and keywords.
We use the mutual information to identify the can-
didate terms or keywords composed of more than
two segmented words. As for DP-4, this recogni-
tion process has been done implicitly during sam-
pling. To measure the candidate terms or key-
words, we refer to the metric in Nazar (2011) to
calculate their importance in some specific domain
or document.
The metrics of F
1
and the out-of-vocabulary
Recall (OOV-R) are used to evaluate the segmenta-
tion results, referring to the gold standard results.
The second and third columns of Table 1 show the
F
1
and OOV-R scores averaged on the 10 domains
for all the compared methods. Our method sig-
nificantly outperforms FMM, RMM and HDP ac-
cording to t-test (p-value ? 0.05). From the seg-
mentation results, we can see that the FMM and
RMM methods are highly dependent on the com-
piled vocabulary and their identified OOV words
are mainly the ones composed of a single Chinese
character. The HDP method is heavily influenced
by the segmented text, but it also exhibits the abil-
ity of learning new words. Our method only shows
a slight advantage over the CRF approach. We
check our segmentation results and find that the
performance of the DP-4 model is depressed by
the identified terms and keywords which may be
composed of more than two words in the gold
standard results, because the DP-4 model always
treats the term or keyword as a single word. For
example, in the gold standard, ??W?((Lingnan
Culture)? is segmented into two words ??W? and
???, ?pn??(data interface)? is segmented
into ?pn? and ???? and so on. In fact, our seg-
mentation results correctly treat ??W?? and ?p
n??? as words.
To evaluate the TE and KE performance, the top
50 (TE-50) and 100 (TE-100) accuracy are mea-
sured for the identified terms of one domain, while
the top 5 (KE-5) and 10 (KE-10) accuracy for the
keywords in one document, are shown in the right
four columns of Table 1. We can see that DP-
1777
4 performs significantly better than all the other
methods in TE and KE results.
As for the ten domains, we find our approach
behaves much better than the other approaches on
the following three domains: Disease, Physics and
Computer. It is because the language of these
three domains is much different from that of the
general domain (PKU corpus), while the rest do-
mains are more similar to the general domain.
Method F1 OOV-R TE-50 TE-100 KE-5 KE-10
FMM 0.796 0.136 0.420 0.360 0.476 0.413
RMM 0.794 0.136 0.424 0.352 0.478 0.414
HDP 0.808 0.356 0.672 0.592 0.552 0.506
CRF 0.817 0.330 0.624 0.560 0.543 0.511
DP-4 0.821 0.374 0.704 0.640 0.571 0.545
Table 1: Comparison of WS, TE and KE Perfor-
mance (averaged on the 10 domains).
4 Conclusion
This paper proposes a four-level DP based model
to construct the word distributions from the cor-
pus, domain and document levels simultaneously,
through which Chinese words, terms and key-
words can be learned jointly and effectively. In
the future, we plan to explore how to combine
more features such as part-of-speech tags into our
model.
Acknowledgments
We thank the three anonymous reviewers for
their helpful comments. This work was par-
tially supported by National High Technology Re-
search and Development Program of China (No.
2012AA011101), National Key Basic Research
Program of China (No. 2014CB340504), Na-
tional Natural Science Foundation of China (No.
61273278), and National Key Technology R&D
Program (No: 2011BAH10B04-03). The contact
author of this paper, according to the meaning
given to this role by Peking University, is Sujian
Li.
References
Baobao Chang, Wenzhe Pei, and Miaohong Chen.
2014. Inducing word sense with automatically
learned hidden concepts. In Proceedings of COL-
ING 2014, pages 355?364, Dublin, Ireland, August.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 673?
680.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Manfred Krug. 1998. String frequency: A cognitive
motivating factor in coalescence, language process-
ing, and linguistic change. Journal of English Lin-
guistics, 26(4):286?320.
Jiwei Li, Sujian Li, Xun Wang, Ye Tian, and Baobao
Chang. 2012. Update summarization using a multi-
level hierarchical dirichlet process model. In Pro-
ceedings of Coling 2012, pages 1603?1618, Mum-
bai, India.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested pitman-yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100?108.
Rogelio Nazar. 2011. A statistical approach to term
extraction. IJES, International Journal of English
Studies, 11(2):159?182.
Lu Ren, David B. Dunson, and Lawrence Carin. 2008.
The dynamic hierarchical dirichlet process. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 824?831.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, pages 260?269.
Xiaojun Wan and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In AAAI, volume 8, pages 855?860.
Tianbing Xu, Zhongfei Zhang, Philip S. Yu, and
Bo Long. 2008. Dirichlet process based evolution-
ary clustering. In ICDM?08, pages 648?657.
Jianwen Zhang, Yangqiu Song, Changshui Zhang, and
Shixia Liu. 2010. Evolutionary hierarchical dirich-
let processes for multiple correlated time-varying
corpora. In Proceedings of the 16th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, pages 1079?1088, New York, NY,
USA.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, volume 1082117.
1778
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1846?1851,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Constructing Information Networks Using One Single Model
Qi Li
?
Heng Ji
?
Yu Hong
??
Sujian Li
?
?
Computer Science Department, Rensselaer Polytechnic Institute, USA
?
School of Computer Science and Technology, Soochow University, China
?
Key Laboratory of Computational Linguistics, Peking University, MOE, China
?
{liq7,hongy2,jih}@rpi.edu,
?
lisujian@pku.edu.cn
Abstract
In this paper, we propose a new frame-
work that unifies the output of three infor-
mation extraction (IE) tasks - entity men-
tions, relations and events as an informa-
tion network representation, and extracts
all of them using one single joint model
based on structured prediction. This novel
formulation allows different parts of the
information network fully interact with
each other. For example, many rela-
tions can now be considered as the re-
sultant states of events. Our approach
achieves substantial improvements over
traditional pipelined approaches, and sig-
nificantly advances state-of-the-art end-to-
end event argument extraction.
1 Introduction
Information extraction (IE) aims to discover entity
mentions, relations and events from unstructured
texts, and these three subtasks are closely inter-
dependent: entity mentions are core components
of relations and events, and the extraction of rela-
tions and events can help to accurately recognize
entity mentions. In addition, the theory of eventu-
alities (D?olling, 2011) suggested that relations can
be viewed as states that events start from and result
in. Therefore, it is intuitive but challenging to ex-
tract all of them simultaneously in a single model.
Some recent research attempted to jointly model
multiple IE subtasks (e.g., (Roth and Yih, 2007;
Riedel and McCallum, 2011; Yang and Cardie,
2013; Riedel et al., 2009; Singh et al., 2013; Li et
al., 2013; Li and Ji, 2014)). For example, Roth and
Yih (2007) conducted joint inference over entity
mentions and relations; Our previous work jointly
extracted event triggers and arguments (Li et al.,
2013), and entity mentions and relations (Li and
Ji, 2014). However, a single model that can ex-
tract all of them has never been studied so far.
Asif Mohammed Hanif detonated explosives in Tel Aviv
AttackPerson Weapon Geopolitical Entity
Place
InstrumentAttacker
Agent-Artifact
Physical
x1 x2 x3 x4 x5 x6 x7 x8x:
y:
Figure 1: Information Network Representation.
Information nodes are denoted by rectangles. Ar-
rows represent information arcs.
For the first time, we uniformly represent the IE
output from each sentence as an information net-
work, where entity mentions and event triggers are
nodes, relations and event-argument links are arcs.
We apply a structured perceptron framework with
a segment-based beam-search algorithm to con-
struct the information networks (Collins, 2002; Li
et al., 2013; Li and Ji, 2014). In addition to the per-
ceptron update, we also apply k-best MIRA (Mc-
Donald et al., 2005), which refines the perceptron
update in three aspects: it is flexible in using var-
ious loss functions, it is a large-margin approach,
and it can use mulitple candidate structures to tune
feature weights.
In an information network, we can capture the
interactions among multiple nodes by learning
joint features during training. In addition to the
cross-component dependencies studied in (Li et
al., 2013; Li and Ji, 2014), we are able to cap-
ture interactions between relations and events. For
example, in Figure 1, if we know that the Person
mention ?Asif Mohammed Hanif ? is an Attacker
of the Attack event triggered by ?detonated?, and
the Weapon mention ?explosives? is an Instrument,
we can infer that there exists an Agent-Artifact
relation between them. Similarly we can infer
the Physical relation between ?Asif Mohammed
Hanif ? and ?Tel Aviv?.
However, in practice many useful interactions
are missing during testing because of the data spar-
1846
sity problem of event triggers. We observe that
21.5% of event triggers appear fewer than twice in
the ACE?05
1
training data. By using only lexical
and syntactic features we are not able to discover
the corresponding nodes and their connections. To
tackle this problem, we use FrameNet (Baker and
Sato, 2003) to generalize event triggers so that
semantically similar triggers are clustered in the
same frame.
The following sections will elaborate the de-
tailed implementation of our new framework.
2 Approach
We uniformly represent the IE output from each
sentence as an information network y = (V,E).
Each node v
i
? V is represented as a triple
?u
i
, v
i
, t
i
? of start index u
i
, end index v
i
, and node
type t
i
. A node can be an entity mention or an
event trigger. A particular type of node is ? (nei-
ther entity mention nor event trigger), whose max-
imal length is always 1. Similarly, each infor-
mation arc e
j
? E is represented as ?u
j
, v
j
, r
j
?,
where u
j
and v
j
are the end offsets of the nodes,
and r
j
is the arc type. For instance, in Fig-
ure 1, the event trigger ?detonated? is represented
as ?4, 4, Attack?, the entity mention ?Asif Mo-
hammed Hanif ? is represented as ?1, 3, Person?,
and their argument arc is ?4, 3, Attacker?. Our
goal is to extract the whole information network y
for a given sentence x.
2.1 Decoding Algorithm
Our joint decoding algorithm is based on ex-
tending the segment-based algorithm described in
our previous work (Li and Ji, 2014). Let x =
(x
1
, ..., x
m
) be the input sentence. The decoder
performs two types of actions at each token x
i
from left to right:
? NODEACTION(i, j): appends a new node
?j, i, t? ending at the i-th token, where i? d
t
<
j ? i, and d
t
is the maximal length of type-t
nodes in training data.
? ARCACTION(i, j): for each j < i, incremen-
tally creates a new arc between the nodes ending
at the j-th and i-th tokens respectively: ?i, j, r?.
After each action, the top-k hypotheses are se-
lected according to their features f(x, y
?
) and
1
http://www.itl.nist.gov/iad/mig//tests/ace
weights w:
best
k
y
?
?buffer
f(x, y
?
) ?w
Since a relation can only occur between a pair of
entity mentions, an argument arc can only occur
between an entity mention and an event trigger,
and each edge must obey certain entity type con-
straints, during the search we prune invalid AR-
CACTIONs by checking the types of the nodes
ending at the j-th and the i-th tokens. Finally, the
top hypothesis in the beam is returned as the final
prediction. The upper-bound time complexity of
the decoding algorithm is O(d ? b ? m
2
), where d
is the maximum size of nodes, b is the beam size,
and m is the sentence length. The actual execution
time is much shorter, especially when entity type
constraints are applied.
2.2 Parameter Estimation
For each training instance (x, y), the structured
perceptron algorithm seeks the assignment with
the highest model score:
z = argmax
y
?
?Y(x)
f(x, y
?
) ?w
and then updates the feature weights by using:
w
new
= w + f(x, y)? f(x, z)
We relax the exact inference problem by the afore-
mentioned beam-search procedure. The stan-
dard perceptron will cause invalid updates be-
cause of inexact search. Therefore we apply early-
update (Collins and Roark, 2004), an instance of
violation-fixing methods (Huang et al., 2012). In
the rest of this paper, we override y and z to denote
prefixes of structures.
In addition to the simple perceptron update, we
also apply k-best MIRA (McDonald et al., 2005),
an online large-margin learning algorithm. During
each update, it keeps the norm of the change to
feature weights w as small as possible, and forces
the margin between y and the k-best candidate z
greater or equal to their loss L(y, z). It is formu-
lated as a quadratic programming problem:
min ?w
new
?w?
s.t. w
new
f(x, y)?w
new
f(x, z) ? L(y, z)
?z ? best
k
(x,w)
We employ the following three loss functions
for comparison:
1847
Freq. Relation Type Event Type Arg-1 Arg-2 Example
159 Physical Transport Artifact Destination He
(arg-1)
was escorted
(trigger)
into Iraq
(arg-2)
.
46 Physical Attack Target Place Many people
(arg-1)
were in the cafe
(arg-2)
during the blast
(trigger)
.
42 Agent-Artifact Attack Attacker Instrument Terrorists
(arg-1)
might use
(trigger)
the devices
(arg-2)
as weapons.
41 Physical Transport Artifact Origin The truck
(arg-1)
was carrying
(trigger)
Syrians fleeing the war in Iraq
(arg-2)
.
33 Physical Meet Entity Place They
(arg-1)
have reunited
(trigger)
with their friends in Norfolk
(arg-2)
.
32 Physical Die Victim Place Two Marines
(arg-1)
were killed
(trigger)
in the fighting in Kut
(arg-2)
.
28 Physical Attack Attacker Place Protesters
(arg-1)
have been clashing
(trigger)
with police in Tehran
(arg-2)
.
26 ORG-Affiliation End-Position Person Entity NBC
(arg-2)
is terminating
(trigger)
freelance reporter Peter Arnett
(arg-1)
.
Table 1: Frequent overlapping relation and event types in the training set.
? The first one is F
1
loss:
L
1
(y, z) = 1?
2 ? |y ? z|
|y|+ |z|
When counting the numbers, we treat each node
and arc as a single unit. For example, in Fig-
ure 1, |y| = 6.
? The second one is 0-1 loss:
L
2
(y, z) =
{
1 y 6= z
0 y = z
It does not discriminate the extent to which z
deviates from y.
? The third loss function counts the difference be-
tween y and z:
L
3
(y, z) = |y|+ |z| ? 2 ? |y ? z|
Similar to F
1
loss function, it penalizes both
missing and false-positive units. The difference
is that it is sensitive to the size of y and z.
2.3 Joint Relation-Event Features
By extracting three core IE components in a joint
search space, we can utilize joint features over
multiple components in addition to factorized fea-
tures in pipelined approaches. In addition to the
features as described in (Li et al., 2013; Li and
Ji, 2014), we can make use of joint features be-
tween relations and events, given the fact that
relations are often ending or starting states of
events (D?olling, 2011). Table 1 shows the most
frequent overlapping relation and event types in
our training data. In each partial structure y
?
dur-
ing the search, if both arguments of a relation par-
ticipate in an event, we compose the correspond-
ing argument roles and relation type as a joint fea-
ture for y
?
. For example, for the structure in Fig-
ure 1, we obtain the following joint relation-event
features:
Attacker Instrument
Agent-Artifact
Attacker Place
Physical
Split Sentences Mentions Relations Triggers Arguments
Train 7.2k 25.7k 4.8k 2.8k 4.5k
Dev 1.7k 6.3k 1.2k 0.7k 1.1k
Test 1.5k 5.3k 1.1k 0.6k 1.0k
Table 2: Data set
0 20 40 60 80 100Number of instances0
2
4
6
8
10
12
14
Freq
uenc
y
Trigger WordsFrame IDs
Figure 2: Distribution of triggers and their frames.
2.4 Semantic Frame Features
One major challenge of constructing information
networks is the data sparsity problem in extract-
ing event triggers. For instance, in the sen-
tence: ?Others were mutilated beyond recogni-
tion.? The Injure trigger ?mutilated? does not oc-
cur in our training data. But there are some sim-
ilar words such as ?stab? and ?smash?. We uti-
lize FrameNet (Baker and Sato, 2003) to solve
this problem. FrameNet is a lexical resource for
semantic frames. Each frame characterizes a ba-
sic type of semantic concept, and contains a num-
ber of words (lexical units) that evoke the frame.
Many frames are highly related with ACE events.
For example, the frame ?Cause harm? is closely
related with Injure event and contains 68 lexical
units such as ?stab?, ?smash? and ?mutilate?.
Figure 2 compares the distributions of trigger
words and their frame IDs in the training data. We
can clearly see that the trigger word distribution
suffers from the long-tail problem, while Frames
reduce the number of triggers which occur only
1848
Methods
Entity Mention (%)
Relation (%)
Event Trigger (%)
Event Argument (%)
P R F
1
P R F
1
P R F
1
P R F
1
Pipelined Baseline
83.6 75.7 79.5
68.5 41.4 51.6 71.2 58.7 64.4 64.8 24.6 35.7
Pipeline + Li et al. (2013) N/A 74.5 56.9 64.5 67.5 31.6 43.1
Li and Ji (2014) 85.2 76.9 80.8 68.9 41.9 52.1 N/A
Joint w/ Avg. Perceptron 85.1 77.3 81.0 70.5 41.2 52.0 67.9 62.8 65.3 64.7 35.3 45.6
Joint w/ MIRA w/ F
1
Loss 83.1 75.3 79.0 65.5 39.4 49.2 59.6 63.5 61.5 60.6 38.9 47.4
Joint w/ MIRA w/ 0-1 Loss 84.2 76.1 80.0 65.4 41.8 51.0 65.6 61.0 63.2 60.5 39.6 47.9
Joint w/ MIRA w/ L
3
Loss 85.3 76.5 80.7 70.8 42.1 52.8 70.3 60.9 65.2 66.4 36.1 46.8
Table 3: Overall performance on test set.
once in the training data from 100 to 60 and al-
leviate the sparsity problem. For each token, we
exploit the frames that contain the combination of
its lemma and POS tag as features. For the above
example, ?Cause harm? will be a feature for ?mu-
tilated?. We only consider tokens that appear in
at most 2 frames, and omit the frames that occur
fewer than 20 times in our training data.
3 Experiments
3.1 Data and Evaluation
We use ACE?05 corpus to evaluate our method
with the same data split as in (Li and Ji, 2014). Ta-
ble 2 summarizes the statistics of the data set. We
report the performance of extracting entity men-
tions, relations, event triggers and arguments sep-
arately using the standard F
1
measures as defined
in (Ji and Grishman, 2008; Chan and Roth, 2011):
? An entity mention is correct if its entity type (7
in total) and head offsets are correct.
? A relation is correct if its type (6 in total) and the
head offsets of its two arguments are correct.
? An event trigger is correct if its event subtype
(33 in total) and offsets are correct.
? An argument link is correct if its event subtype,
offsets and role match those of any of the refer-
ence argument mentions.
In this paper we focus on entity arguments while
disregard values and time expressions because
they can be most effectively extracted by hand-
crafted patterns (Chang and Manning, 2012).
3.2 Results
Based on the results of our development set, we
trained all models with 21 iterations and chose the
beam size to be 8. For the k-best MIRA updates,
we set k as 3. Table 3 compares the overall perfor-
mance of our approaches and baseline methods.
Our joint model with perceptron update out-
performs the state-of-the-art pipelined approach
in (Li et al., 2013; Li and Ji, 2014), and further
improves the joint event extraction system in (Li
et al., 2013) (p < 0.05 for entity mention extrac-
tion, and p < 0.01 for other subtasks, accord-
ing to Wilcoxon Signed RankTest). For the k-
best MIRA update, the L
3
loss function achieved
better performance than F
1
loss and 0-1 loss on
all sub-tasks except event argument extraction. It
also significantly outperforms perceptron update
on relation extraction and event argument extrac-
tion (p < 0.01). It is particularly encouraging to
see the end output of an IE system (event argu-
ments) has made significant progress (12.2% ab-
solute gain over traditional pipelined approach).
3.3 Discussions
3.3.1 Feature Study
Rank Feature Weight
1 Frame=Killing Die 0.80
2 Frame=Travel Transport 0.61
3 Physical(Artifact, Destination) 0.60
4 w
1
=?home? Transport 0.59
5 Frame=Arriving Transport 0.54
6 ORG-AFF(Person, Entity) 0.48
7 Lemma=charge Charge-Indict 0.45
8 Lemma=birth Be-Born 0.44
9 Physical(Artifact,Origin) 0.44
10 Frame=Cause harm Injure 0.43
Table 4: Top Features about Event Triggers.
Table 4 lists the weights of the most significant
features about event triggers. The 3
rd
, 6
th
, and
9
th
rows are joint relation-event features. For in-
stance, Physical(Artifact, Destination) means the
arguments of a Physical relation participate in a
Transport event as Artifact and Destination. We
can see that both the joint relation-event features
1849
and FrameNet based features are of vital impor-
tance to event trigger labeling. We tested the im-
pact of each type of features by excluding them in
the experiments of ?MIRA w/ L
3
loss?. We found
that FrameNet based features provided 0.8% and
2.2% F
1
gains for event trigger and argument la-
beling respectively. Joint relation-event features
also provided 0.6% F
1
gain for relation extraction.
3.3.2 Remaining Challenges
Event trigger labeling remains a major bottleneck.
In addition to the sparsity problem, the remain-
ing errors suggest to incorporate external world
knowledge. For example, some words act as trig-
gers for some certain types of events only when
they appear together with some particular argu-
ments:
? ?Williams picked up the child again and this
time, threw
Attack
her out the window.?
The word ?threw? is used as an Attack event
trigger because the Victim argument is a ?child?.
? ?Ellison to spend $10.3 billion to get
Merge Org
his company.? The common word ?get? is
tagged as a trigger of Merge Org, because its
object is ?company?.
? ?We believe that the likelihood of them
using
Attack
those weapons goes up.?
The word ?using? is used as an Attack event
trigger because the Instrument argument is
?weapons?.
Another challenge is to distinguish physical and
non-physical events. For example, in the sentence:
? ?we are paying great attention to their ability to
defend
Attack
on the ground.?,
our system fails to extract ?defend? as an Attack
trigger. In the training data, ?defend? appears mul-
tiple times, but none of them is tagged as Attack.
For instance, in the sentence:
? ?North Korea could do everything to defend it-
self. ?
?defend? is not an Attack trigger since it does not
relate to physical actions in a war. This challenge
calls for deeper understanding of the contexts.
Finally, some pronouns are used to refer to ac-
tual events. Event coreference is necessary to rec-
ognize them correctly. For example, in the follow-
ing two sentences from the same document:
? ?It?s important that people all over the world
know that we don?t believe in the war
Attack
.?,
? ?Nobody questions whether this
Attack
is right
or not.?
?this? refers to ?war? in its preceding contexts.
Without event coreference resolution, it is difficult
to tag it as an Attack event trigger.
4 Conclusions
We presented the first joint model that effectively
extracts entity mentions, relations and events
based on a unified representation: information
networks. Experiment results on ACE?05 cor-
pus demonstrate that our approach outperforms
pipelined method, and improves event-argument
performance significantly over the state-of-the-art.
In addition to the joint relation-event features, we
demonstrated positive impact of using FrameNet
to handle the sparsity problem in event trigger la-
beling.
Although our primary focus in this paper is in-
formation extraction in the ACE paradigm, we be-
lieve that our framework is general to improve
other tightly coupled extraction tasks by capturing
the inter-dependencies in the joint search space.
Acknowledgments
We thank the three anonymous reviewers for their
insightful comments. This work was supported by
the U.S. Army Research Laboratory under Coop-
erative Agreement No. W911NF-09-2-0053 (NS-
CTA), U.S. NSF CAREER Award under Grant
IIS-0953149, U.S. DARPA Award No. FA8750-
13-2-0041 in the Deep Exploration and Filtering
of Text (DEFT) Program, IBM Faculty Award,
Google Research Award, Disney Research Award
and RPI faculty start-up grant. The views and con-
clusions contained in this document are those of
the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the U.S. Government. The U.S. Gov-
ernment is authorized to reproduce and distribute
reprints for Government purposes notwithstanding
any copyright notation here on.
References
Collin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proc. ACL, pages 161?164.
Yee Seng Chan and Dan Roth. 2011. Exploiting
syntactico-semantic structures for relation extrac-
tion. In Proc. ACL, pages 551?560.
1850
Angel X. Chang and Christopher Manning. 2012. Su-
time: A library for recognizing and normalizing time
expressions. In Proc. LREC, pages 3735?3740.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL, pages 111?118.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1?8.
Johannes D?olling. 2011. Aspectual coercion and even-
tuality structure. pages 189?226.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proc.
HLT-NAACL, pages 142?151.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Proc.
ACL.
Qi Li and Heng Ji. 2014. Incremental joint extraction
of entity mentions and relations. In Proc. ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proc. ACL, pages 73?82.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proc. ACL, pages 91?98.
Sebastian Riedel and Andrew McCallum. 2011. Fast
and robust joint models for biomedical event extrac-
tion. In Proc. EMNLP.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic ap-
proach to bio-molecular event extraction. In Proc.
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing: Shared Task.
Dan Roth and Wen-tau Yih. 2007. Global inference
for entity and relation identification via a lin- ear
programming formulation. In Introduction to Sta-
tistical Relational Learning. MIT.
Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-
ing Zheng, and Andrew McCallum. 2013. Joint
inference of entities, relations, and coreference. In
Proc. CIKM Workshop on Automated Knowledge
Base Construction.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proc. ACL,
pages 1640?1649.
1851
Proceedings of the ACL 2010 Conference Short Papers, pages 296?300,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Key Phrase Extraction Approach: Learning from 
Title Phrases through a Document Semantic Network 
 
Decong Li1, Sujian Li1, Wenjie Li2, Wei Wang1, Weiguang Qu3 
1Key Laboratory of Computational Linguistics, Peking University  
2Department of Computing, The Hong Kong Polytechnic University 
3 School of Computer Science and Technology, Nanjing Normal University 
{lidecong,lisujian, wwei }@pku.edu.cn   cswjli@comp.polyu.edu.hk  wgqu@njnu.edu.cn 
  
  
Abstract 
It is a fundamental and important task to ex-
tract key phrases from documents. Generally, 
phrases in a document are not independent in 
delivering the content of the document. In or-
der to capture and make better use of their re-
lationships in key phrase extraction, we sug-
gest exploring the Wikipedia knowledge to 
model a document as a semantic network, 
where both n-ary and binary relationships 
among phrases are formulated. Based on a 
commonly accepted assumption that the title 
of a document is always elaborated to reflect 
the content of a document and consequently 
key phrases tend to have close semantics to the 
title, we propose a novel semi-supervised key 
phrase extraction approach in this paper by 
computing the phrase importance in the se-
mantic network, through which the influence 
of title phrases is propagated to the other 
phrases iteratively. Experimental results dem-
onstrate the remarkable performance of this 
approach. 
1 Introduction 
Key phrases are defined as the phrases that ex-
press the main content of a document. Guided by 
the given key phrases, people can easily under-
stand what a document describes, saving a great 
amount of time reading the whole text. Conse-
quently, automatic key phrase extraction is in 
high demand. Meanwhile, it is also fundamental 
to many other natural language processing appli-
cations, such as information retrieval, text clus-
tering and so on.  
Key phrase extraction can be normally cast as 
a ranking problem solved by either supervised or 
unsupervised methods. Supervised learning re-
quires a large amount of expensive training data, 
whereas unsupervised learning totally ignores 
human knowledge. To overcome the deficiencies 
of these two kinds of methods, we propose a 
novel semi-supervised key phrase extraction ap-
proach in this paper, which explores title phrases 
as the source of knowledge.  
It is well agreed that the title has a similar role 
to the key phrases. They are both elaborated to 
reflect the content of a document. Therefore, 
phrases in the titles are often appropriate to be 
key phrases. That is why position has been a 
quite effective feature in the feature-based key 
phrase extraction methods (Witten, 1999), i.e., if 
a phrase is located in the title, it is ranked higher.  
However, one can only include a couple of 
most important phrases in the title prudently due 
to the limitation of the title length, even though 
many other key phrases are all pivotal to the un-
derstanding of the document. For example, when 
we read the title ?China Tightens Grip on the 
Web?, we can only have a glimpse of what the 
document says. On the other hand, the key 
phrases, such as ?China?, ?Censorship?, ?Web?, 
?Domain name?, ?Internet?, and ?CNNIC?, etc. 
can tell more details about the main topics of the 
document. In this regard, title phrases are often 
good key phrases but they are far from enough. 
If we review the above example again, we will 
find that the key phrase ?Internet? can be in-
ferred from the title phrase ?Web?. As a matter 
of fact, key phrases often have close semantics to 
title phrases. Then a question comes to our minds: 
can we make use of these title phrases to infer 
the other key phrases?  
To provide a foundation of inference, a seman-
tic network that captures the relationships among 
phrases is required. In the previous works (Tur-
dakov and Velikhov, 2008), semantic networks 
are constructed based on the binary relations, and 
the semantic relatedness between a pair of phras-
es is formulated by the weighted edges that con-
nects them. The deficiency of these approaches is 
the incapability to capture the n-ary relations 
among multiple phrases. For example, a group of 
296
phrases may collectively describe an entity or an 
event.  
In this study, we propose to model a semantic 
network as a hyper-graph, where vertices 
represent phrases and weighted hyper-edges 
measure the semantic relatedness of both binary 
relations and n-ary relations among phrases. We 
explore a universal knowledge base ? Wikipedia 
? to compute the semantic relatedness. Yet our 
major contribution is to develop a novel semi-
supervised key phrase extraction approach by 
computing the phrase importance in the semantic 
network, through which the influence of title 
phrases is propagated to the other phrases itera-
tively.  
The goal of the semi-supervised learning is to 
design a function that is sufficiently smooth with 
respect to the intrinsic structure revealed by title 
phrases and other phrases. Based on the assump-
tion that semantically related phrases are likely 
to have similar scores, the function to be esti-
mated is required to assign title phrases a higher 
score and meanwhile locally smooth on the con-
structed hyper-graph. Zhou et al?s work (Zhou 
2005) lays down a foundation for our semi-
supervised phrase ranking algorithm introduced 
in Section 3. Experimental results presented in 
Section 4 demonstrate the effectiveness of this 
approach. 
2 Wikipedia-based Semantic Network 
Construction  
Wikipedia1 is a free online encyclopedia, which 
has unarguably become the world?s largest col-
lection of encyclopedic knowledge. Articles are 
the basic entries in the Wikipedia, with each ar-
ticle explaining one Wikipedia term. Articles 
contain links pointing from one article to another. 
Currently, there are over 3 million articles and 90 
million links in English Wikipedia. In addition to 
providing a large vocabulary, Wikipedia articles 
also contain a rich body of lexical semantic in-
formation expressed via the extensive number of 
links. During recent years, Wikipedia has been 
used as a powerful tool to compute semantic re-
latedness between terms in a good few of works 
(Turdakov 2008).   
We consider a document composed of the 
phrases that describe various aspects of entities 
or events with different semantic relationships. 
We then model a document as a semantic net-
work formulated by a weighted hyper-graph 
                                               
1 www.wikipedia.org 
 
G=(V, E, W), where each vertex vi?V (1?i?n) 
represents a phrase, each hyper-edge ej?E 
(1?j?m) is a subset of V, representing binary re-
lations or n-ary relations among phrases, and the 
weight w(ej) measures the semantic relatedness 
of ej.  
By applying the WSD technique proposed by 
(Turdakov and Velikhov, 2008), each phrase is 
assigned with a single Wikipedia article that de-
scribes its meaning. Intuitively, if the fraction of 
the links that the two articles have in common to 
the total number of the links in both articles is 
high, the two phrases corresponding to the two 
articles are more semantically related. Also, an 
article contains different types of links, which are 
relevant to the computation of semantic related-
ness to different extent. Hence we adopt the 
weighted Dice metric proposed by (Turdakov 
2008) to compute the semantic relatedness of 
each binary relation, resulting in the edge weight  
w(eij), where eij is an edge connecting the phrases 
vi and vj. 
To define the n-ary relations in the semantic 
network, a proper graph clustering technique is 
needed. We adopt the weighted Girvan-Newman 
algorithm (Newman 2004) to cluster phrases (in-
cluding title phrases) by computing their bet-
weenness centrality. The advantage of this algo-
rithm is that it need not specify a pre-defined 
number of clusters. Then the phrases, within 
each cluster, are connected by a n-ary relation. n-
ary relations among the phrases in the same clus-
ter are then measured based on binary relations. 
The weight of a hyper-edge e is defined as: 
( ) ( )| | ij ije e
w e w ee
?
?
? ?
  (1) 
where |e| is the number of the vertices in e, eij is 
an edge with two vertices included in e and ? ? 0 
is a parameter balancing the relative importance 
of n-ary hyper-edges compared with binary ones.  
3 Semi-supervised Learning from Title 
Given the document semantic network 
represented as a phrase hyper-graph, one way to 
make better use of the semantic information is to 
rank phrases with a semi-supervised learning 
strategy, where the title phrases are regarded as 
labeled samples, while the other phrases as unla-
beled ones. That is, the information we have at 
the beginning about how to rank phrases is that 
the title phrases are the most important phrases. 
Initially, the title phrases are assigned with a pos-
itive score of 1 indicating its importance and oth-
297
er phrases are assigned zero. Then the impor-
tance scores of the phrases are learned iteratively 
from the title phrases through the hyper-graph. 
The key idea behind hyper-graph based semi-
supervised ranking is that the vertices which 
usually belong to the same hyper-edges should 
be assigned with similar scores. Then, we have 
the following two constraints: 
1. The phrases which have many incident hy-
per-edges in common should be assigned similar 
scores. 
2. The given initial scores of the title phrases 
should be changed as little as possible. 
Given a weighted hyper-graph G, assume a 
ranking function f over V, which assigns each 
vertex v an importance score f(v). f can be 
thought as a vector in Euclid space R|V|. For the 
convenience of computation, we use an inci-
dence matrix H to represent the hypergraph, de-
fined as: 
0, if ( , ) 1, if 
v eh v e v e
??? ? ??
   (2) 
Based on the incidence matrix, we define the 
degrees of the vertex v and the hyper-edge e as 
   (3) 
and 
   (4) 
Then, to formulate the above-mentioned con-
straints, let  denote the initial score vector, then 
the importance scores of the phrases are learned 
iteratively by solving the following optimization 
problem: 
| |
2arg min { ( ) }Vf R f f y?? ? ? ?
 (5) 
2
{ , }
1 1 ( ) ( )( ) ( )2 ( ) ( ) ( )e E u v e
f u f vf w ee d u d v?? ?
? ?? ? ?? ?
? ?? ?
 (6) 
where ?> 0 is the parameter specifying the 
tradeoff between the two competitive items. Let 
Dv and De denote the diagonal matrices contain-
ing the vertex and the hyper-edge degrees re-
spectively, W denote the diagonal matrix con-
taining the hyper-edge weights, f* denote the so-
lution of (6).  Zhou has given the solution (Zhou, 
2005) as. 
* * (1 )f f y? ?? ? ? ?   (7) 
where 
1/2 1 1/2Tv e vD HWD H D? ? ?? ?  and 1/ ( 1)? ?? ? . 
Using an approximation algorithm (e.g. Algo-
rithm 1), we can finally get a vector f 
representing the approximate phrase scores. 
Algorithm 1: PhraseRank(V, T, a, b) 
Input: Title phrase set = {v1,v2,?,vt},the set of other 
phrases ={vt+1,vt+2,?,vn}, parameters ? and ?, con-
vergence threshold ? 
Output: The approximate phrase scores f  
Construct a document semantic network for all the 
phrases {v1,v2,?,vn} using the method described  in 
section 2. 
Let 
1/2 1 1/2Tv e vD HWD H D? ? ? ?? ;  
Initialize the score vector y as 1,1iy i t? ? ? , and  
0,jy t j n? ? ?
; 
Let , k = 0; 
REPEAT  
1 (1 )k kf f y?? ?? ? ? ?; 
, ; 
; 
UNTIL  
END 
Finally we rank phrases in descending order of 
the calculated importance scores and select those 
highest ranked phrases as key phrases. Accord-
ing to the number of all the candidate phrases, 
we choose an appropriate proportion, i.e. 10%, of 
all the phrases as key phrases. 
4 Evaluation 
4.1 Experiment Set-up  
We first collect all the Wikipedia terms to com-
pose of a dictionary. The word sequences that 
occur in the dictionary are identified as phrases. 
Here we use a finite-state automaton to accom-
plish this task to avoid the imprecision of pre-
processing by POS tagging or chunking. Then, 
we adopt the WSD technique proposed by (Tur-
dakov and Velikhov 2008) to find the corres-
ponding Wikipedia article for each phrase. As 
mentioned in Section 2, a document semantic 
network in the form of a hyper-graph is con-
structed, on which Algorithm 1 is applied to rank 
the phrases.  
To evaluate our proposed approach, we select 
200 pieces of news from well-known English 
media. 5 to 10 key phrases are manually labeled 
in each news document and the average number 
of the key phrases is 7.2 per document. Due to 
the abbreviation and synonymy phenomena, we 
construct a thesaurus and convert all manual and 
automatic phrases into their canonical forms 
when evaluated. The traditional Recall, Precision 
and F1-measure metrics are adopted for evalua-
tion. This section conducts two sets of experi-
ment: (1) to examine the influence of two para-
meters: ? and ?, on the key phrase extraction 
performance; (2) to compare with other well 
known state-of-art key phrase extraction ap-
proaches. 
298
4.2 Parameter tuning  
The approach involves two parameters: ? (??0) 
is a relation factor balancing the influence of n-
ary relations and binary relations; ? (0???1) is a 
learning factor tuning the influence from the title 
phrases. It is hard to find a global optimized so-
lution for the combination of these two factors. 
So we apply a gradient search strategy. At first, 
the learning factor is set to ?=0.8. Different val-
ues of ? ranging from 0 to 3 are examined. Then, 
given that ? is set to the value with the best per-
formance, we conduct experiments to find an 
appropriate value for ?. 
4.2.1 ?: Relation Factor 
First, we fix the learning factor ? as 0.8 random-
ly and evaluate the performance by varying ? 
value from 0 to 3. When ?=0, it means that the 
weight of n-ary relations is zero and only binary 
relations are considered. As we can see from 
Figure 1, the performance is improved in most 
cases in terms of F1-measure and reaches a peak 
at ?=1.8. This justifies the rational to incorpo-
rate n-ary relations with binary relations in the 
document semantic network. 
 
Figure 1. F1-measures with ? in [0 3] 
4.2.2 ?: Learning factor  
Next, we set the relation factor ?=1.8, we in-
spect the performance with the learning factor ? 
ranging from 0 to 1. ?=1 means that the ranking 
scores learn from the semantic network without 
any consideration of title phrases. As shown in 
Figure 2, we find that the performance almost 
keep a smooth fluctuation as ? increases from 0 
to 0.9, and then a diving when ?=1. This proves 
that title phrases indeed provide valuable infor-
mation for learning.  
 
Figure 2. F1-measure with ? in [0,1] 
4.3 Comparison with Other Approaches  
Our approach aims at inferring important key 
phrases from title phrases through a semantic 
network. Here we take a method of synonym 
expansion as the baseline, called WordNet ex-
pansion here. The WordNet2 expansion approach 
selects all the synonyms of the title phrases in the 
document as key phrases. Afterwards, our ap-
proach is evaluated against two existing ap-
proaches, which rely on the conventional seman-
tic network and are able to capture binary rela-
tions only. One approach combines the title in-
formation into the Grineva?s community-based 
method (Grineva et al, 2009), called title-
community approach. The title-community ap-
proach uses the Girvan-Newman algorithm to 
cluster phrases into communities and selects 
those phrases in the communities containing the 
title phrases as key phrases. We do not limit the 
number of key phrases selected. The other one is 
based on topic-sensitive LexRank (Otterbacher et 
al., 2005), called title-sensitive PageRank here. 
The title-sensitive PageRank approach makes use 
of title phrases to re-weight the transitions be-
tween vertices and picks up 10% top-ranked 
phrases as key phrases.  
Approach Precision Recall F1 
Title-sensitive Pa-
geRank (d=0.15) 
34.8% 39.5% 37.0% 
Title-community 29.8% 56.9% 39.1% 
Our approach 
(?=1.8, ?=0.5) 
39.4% 44.6% 41.8% 
WordNet expansion 
(baseline) 
7.9%  32.9% 12.5% 
Table 1. Comparison with other approaches 
Table 1 summarizes the performance on the 
test data. The results presented in the table show 
that our approach exhibits the best performance 
among all the four approaches. It follows that the 
key phrases inferred from a document semantic 
network are not limited to the synonyms of title 
phrases. As the title-sensitive PageRank ap-
                                               
2 http://wordnet.princeton.edu 
299
proach totally ignores the n-ary relations, its per-
formance is the worst. Based on binary relations, 
the title-community approach clusters phrases 
into communities and each community can be 
considered as an n-ary relation. However, this 
approach lacks of an importance propagation 
process. Consequently, it has the highest recall 
value but the lowest precision. In contrast, our 
approach achieves the highest precision, due to 
its ability to infer many correct key phrases using 
importance propagation among n-ary relations.  
5 Conclusion  
This work is based on the belief that key phrases 
tend to have close semantics to the title phrases. 
In order to make better use of phrase relations in 
key phrase extraction, we explore the Wikipedia 
knowledge to model one document as a semantic 
network in the form of hyper-graph, through 
which the other phrases learned their importance 
scores from the title phrases iteratively. Experi-
mental results demonstrate the effectiveness and 
robustness of our approach. 
 
Acknowledgments 
The work described in this paper was partially 
supported by NSFC programs (No: 60773173, 
60875042 and 90920011), and Hong Kong RGC 
Projects (No: PolyU5217/07E).  We thank the 
anonymous reviewers for their insightful com-
ments. 
References  
David Milne, Ian H. Witten. 2008. An Effective, 
Low-Cost Measure of Semantic Relatedness 
Obtained from Wikipedia Links. In Wikipedia 
and AI workshop at the AAAI-08 Conference, 
Chicago, US. 
Dengyong Zhou, Jiayuan Huang and Bernhard 
Sch?lkopf. 2005. Beyond Pairwise Classifica-
tion and Clustering Using Hypergraphs. MPI 
Technical Report, T?bingen, Germany. 
Denis Turdakov and Pavel Velikhov. 2008. Semantic 
relatedness metric for wikipedia concepts 
based on link analysis and its application to 
word sense disambiguation. In Colloquium on 
Databases and Information Systems (SYRCoDIS). 
Ian H. Witten, Gordon W. Paynter, Eibe Frank , Carl 
Gutwin , Craig G. Nevill-Manning. 1999.  KEA: 
practical automatic keyphrase extraction, In 
Proceedings of the fourth ACM conference on Dig-
ital libraries, pp.254-255, California, USA. 
Jahna Otterbacher, Gunes Erkan and Dragomir R. 
Radev. 2005. Using Random Walks for Ques-
tion-focused Sentence Retrieval. In Proceedings 
of HLT/EMNLP 2005, pp. 915-922, Vancouver, 
Canada. 
Maria Grineva, Maxim Grinev and Dmitry Lizorkin. 
2009. Extracting key terms from noisy and 
multitheme documents, In Proceedings of the 
18th international conference on World wide web, 
pp. 661-670, Madrid, Spain.  
Michael Strube and Simone Paolo Ponzetto. 
2006.WikiRelate! Computing Semantic Rela-
tedness using Wikipedia. In Proceedings of the 
21st National Conference on Artificial Intelligence, 
pp. 1419?1424, Boston, MA. 
M. E. J. Newman. 2004. Analysis of Weighted Net-
works.  Physical Review E 70, 056131. 
 
300
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217?221,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
TopicSpam: a Topic-Model-Based Approach for Spam Detection
Jiwei Li , Claire Cardie
School of Computer Science
Cornell University
Ithaca, NY, 14853
jl3226@cornell.edu
cardie@cs.cornell.edu
Sujian Li
Laboratory of Computational Linguistics
Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn
Abstract
Product reviews are now widely used by
individuals and organizations for decision
making (Litvin et al, 2008; Jansen, 2010).
And because of the profits at stake, peo-
ple have been known to try to game the
system by writing fake reviews to promote
target products. As a result, the task of de-
ceptive review detection has been gaining
increasing attention. In this paper, we pro-
pose a generative LDA-based topic mod-
eling approach for fake review detection.
Our model can aptly detect the subtle dif-
ferences between deceptive reviews and
truthful ones and achieves about 95% ac-
curacy on review spam datasets, outper-
forming existing baselines by a large mar-
gin.
1 Introduction
Consumers rely increasingly on user-generated
online reviews to make purchase decisions. Pos-
itive opinions can result in significant financial
gains. This gives rise to deceptive opinion spam
(Ott et al, 2011; Jindal et al, 2008), fake reviews
written to sound authentic and deliberately mis-
lead readers. Previous research has shown that
humans have difficulty distinguishing fake from
truthful reviews, operating for the most part at
chance (Ott et al, 2011). Consider, for example,
the following two hotel reviews. One is truthful
and the other is deceptive1:
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate, and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided. Their service was amazing,
1The first example is a deceptive review.
and we absolutely loved the beautiful indoor pool. I
would recommend staying here to anyone.
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The hotel
is central to both Navy Pier and Michigan Ave. so we
walked, trolleyed, and cabbed all around the area. We
ate the breakfast buffet on both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 (instead of the adult $20). The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we stay in Chicago next time.
Because of the difficulty of recognizing deceptive
opinions, there has been a widespread and growing
interest in developing automatic, usually learning-
based methods to help users identify deceptive re-
views (Ott et al, 2011; Jindal et al, 2008; Jindal
et al, 2010; Li et al, 2011; Lim et al, 2011; Wang
et al, 2011).
The state-of-the-art approach treats the task of
spam detection as a text categorization prob-
lem and was first introduced by Jindal and Liu
(2009) who trained a supervised classifier to dis-
tinguish duplicated reviews (assumed deceptive)
from original ones (assumed truthful). Since then,
many supervised approaches have been proposed
for spam detection. Ott et al (2011) employed
standard word and part-of-speech (POS) n-gram
features for supervised learning and built a gold?
standard opinion dataset of 800 reviews. Lim et
al. (2010) proposed the inclusion of user behavior-
based features and found that behavior abnormali-
ties of reviewers could predict spammers, without
using any textual features. Li et al (2011) care-
fully explored review-related features based on
content and sentiment, training a semi-supervised
classifier for opinion spam detection. However,
the disadvantages of standard supervised learning
methods are obvious. First, they do not gener-
ally provide readers with a clear probabilistic pre-
217
diction of how likely a review is to be deceptive
vs. truthful. Furthermore, identifying features that
provide direct evidence against deceptive reviews
is always a hard problem.
LDA topic models (Blei et al, 2003) have
widely been used for their ability to model latent
topics in document collection. In LDA, each docu-
ment is presented as a mixture distribution of top-
ics and each topic is presented as a mixture distri-
bution of words. Researchers also integrated dif-
ferent levels of information into LDA topic mod-
els to model the specific knowledge that they are
interested in, such as user-specific information
(Rosen-zvi et al, 2006), document-specific infor-
mation (Li et al, 2010) and time-specific infor-
mation (Diao et al, 2012). Ramage et al (2009)
developed a Labeled LDA model to define a one-
to-one correspondence between LDA latent topics
and tags. Chemudugunta et al (2008) illustrated
that by considering background information and
document-specific information, we can largely im-
prove the performance of topic modeling.
In this paper, we propose a Bayesian approach
called TopicSpam for deceptive review detection.
Our approach, which is a variation of Latent
Dirichlet Allocation (LDA) (Blei et al, 2003),
aims to detect the subtle differences between the
topic-word distributions of deceptive reviews vs.
truthful ones. In addition, our model can give
a clear probabilistic prediction on how likely a
review should be treated as deceptive or truth-
ful. Performance is tested on dataset from Ott et
al.(2011) that contains 800 reviews of 20 Chicago
hotels. Our model achieves more than 94% accu-
racy on that dataset.
2 TopicSpam
We are presented with four subsets of ho-
tel reviews, M = {Mi}i=4i=1, representing
deceptive train, truthful train, deceptive test
and truthful test data, respectively. Each re-
view r is comprised of a number of words r =
{wt}t=nrt=1 . Input for the TopicSpam algorithm is
the datasets M ; output is the label (deceptive,
truthful) for each review inM3 andM4. V denotes
vocabulary size.
2.1 Details of TopicSpam
In TopicSpam, each document is modeled as a
bag of words, which are assumed to be gener-
ated from a mixture of latent topics. Each word
is associated with a latent variable that specifies
Figure 1: Graphical Model for TopicSpam
the topic from which it is generated. Words in a
document are assumed to be conditionally inde-
pendent given the hidden topics. A general back-
ground distribution ?B and hotel-specific distri-
butions ?Hj (j = 1, ..., 20) are first introduced
to capture the background information and hotel-
specific information. To capture the difference
between deceptive reviews and truthful reviews,
TopicSpam also learns a deceptive topic distribu-
tion ?D and truthful topic distribution ?T . The
generative model of TopicSpam is shown as fol-
lows:
? For a training review in r1j ? M1, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?D.
? For a training review in r2j ? M2, words are
originated from one of the three different top-
ics: ?B , ?Hj and ?T .
? For a test review in rmj ? Mm,m = 3, 4,
words are originated from one of the four dif-
ferent topics: ?B , ?Hj ?D and ?T .
The generation process of TopicSpam is shown
in Figure 1 and the corresponding graphical
model is illustrated in Figure 2. We use
? = (?G, ?Hi , ?D, ?T ) to represent the asym-
metric priors for topic-word distribution genera-
tion. In our experiments, we set ?G = 0.1,
and ?Hi = ?D = ?T = 0.01. The intu-
ition for the asymmetric priors is that there should
be more words assigned to the background topic.
? = [?B, ?Hi , ?D, ?T ] denotes the priors for
the document-level topic distribution in the LDA
model. We set ?B = 2 and ?T = ?D = ?Hi = 1,
reflecting the intuition that more words in each
document should cover the background topic.
2.2 Inference
We adopt the collapsed Gibbs sampling strategy to
infer the latent parameters in TopicSpam. In Gibbs
218
1. sample ?G ? Dir(?G)
2. sample ?D ? Dir(?D)
3. sample ?T ? Dir(?T )
4. for each hotel j ? [1, N ]: sample ?Hj ? ?H
5. for each review r
if i=1: sample ?r ? Dir(?B, ?Hj , ?D)
if i=2: sample ?r ? Dir(?B, ?Hj , ?T )
if i=3: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
if i=4: sample ?r ? Dir(?B, ?Hj , ?D, ?T )
for each word w in R
sample z ? ?r sample w ? ?z
Figure 2: Generative Model for TopicSpam
sampling, for each word w in review r, we need
to calculate P (zw|w, z?w, ?, ?) in each iteration,
where z?w denotes the topic assignments for all
words except that of the current word zw.
P (zw = m|z?w, i, j, ?, ?)
Nmr + ?m?
m?(Nm
?
r + ??m)
? E
w
m + ?m?V
w? Ewm + V ?m
(1)
where Nmr denotes the number of times that topic
m appears in current review r and Ewm denotes the
number of times that word w is assigned to topic
m. After each sampling iteration, the latent pa-
rameters can be estimated using the following for-
mulas:
?mr =
Nmr + ?m?
m?(Nm
?
r + ?m)
?(w)m =
Ewm + ?m?
w? Ew
?
m + V ?m(2)
2.3 Labeling the Test Data
For each review r in the test data, let NDr denote
the number of words generated from the decep-
tive topic and NTr , the number of words generated
from the truthful topic. The decision for whether a
review is deceptive or truthful is made as follows:
? if NDr > NTr , r is deceptive.
? if NDr < NTr , r is truthful.
? if NDr = NTr , it is hard to decide.
Let P(D) denote the probability that r is deceptive
and P(T) denote the probability that r is truthful.
P (D) = N
D
r
NDr +NTr
P (T ) = N
T
r
NDr +NTr
(3)
3 Experiments
3.1 System Description
Our experiments are conducted on the dataset
from Ott et al(2011), which contains reviews of
the 20 most popular hotels on TripAdvisor in the
Chicago areas. There are 20 truthful and 20 decep-
tive reviews for each of the chosen hotels (800 re-
views total). Deceptive reviews are gathered using
Amazon Mechanical Turk2. In our experiments,
we adopt the same 5-fold cross-validation strat-
egy as in Ott et al, using the same data partitions.
Words are stemmed using PorterStemmer3.
3.2 Baselines
We employ a number of techniques as baselines:
TopicTD: A topic-modeling approach that only
considers two topics: deceptive and truthful.
Words in deceptive train are all generated from
the deceptive topic and words in truthful train
are generated from the truthful topic. Test docu-
ments are presented with a mixture of the decep-
tive and truthful topics.
TopicTDB: A topic-modeling approach that
only considers background, deceptive and truthful
information.
SVM-Unigram: Using SVMlight(Joachims,
1999) to train linear SVM models on unigram fea-
tures.
SVM-Bigram: Using SVMlight(Joachims,
1999) to train linear SVM models on bigram fea-
tures.
SVM-Unigram-Removal1: In SVM-Unigram-
Removal, we first train TopicSpam. Then words
generated from hotel-specific topics are removed.
We use the remaining words as features in SVM-
light.
SVM-Unigram-Removal2: Same as SVM-
Unigram-removal-1 but removing all background
words and hotel-specific words.
Experimental results are shown in Table 14.
As we can see, the accuracy of TopicSpam is
0.948, outperforming TopicTD by 6.4%. This il-
lustrates the effectiveness of modeling background
and hotel-specific information for the opinion
spam detection problem. We also see that Top-
icSpam slightly outperforms TopicTDB, which
2https://www.mturk.com/mturk/.
3http://tartarus.org/martin/PorterStemmer/
4Reviews with NDr = NTr are regarded as incorrectly
classified by TopicSpam.
219
Approach Accuracy T-P T-R T-F D-P D-R D-F
TopicSpam 0.948 0.954 0.942 0.944 0.941 0.952 0.946
TopicTD 0.888 0.901 0.878 0.889 0.875 0.897 0.886
TopicTDB 0.931 0.938 0.926 0.932 0.925 0.937 0.930
SVM-Unigram 0.884 0.899 0.865 0.882 0.870 0.903 0.886
SVM-Bigram 0.896 0.901 0.890 0.896 0.891 0.903 0.897
SVM-Unigram-Removal1 0.895 0.906 0.889 0.898 0.887 0.907 0.898
SVM-Unigram-Removal2 0.822 0.852 0.806 0.829 0.793 0.840 0.817
Table 1: Performance for different approaches based on nested 5-fold cross-validation experiments.
neglects hotel-specific information. By check-
ing the results of Gibbs sampling, we find that
this is because only a small number of words
are generated by the hotel-specific topics. Top-
icTD and SVM-Unigram get comparative accu-
racy rates. This can be explained by the fact
that both models use unigram frequency as fea-
tures for the classifier or topic distribution train-
ing. SVM-Unigram-Removal1 is also slightly
better than SVM-Unigram. In SVM-Unigram-
removal1, hotel-specific words are removed for
classifier training. So the first-step LDA model
can be viewed as a feature selection process for the
SVM, giving rise to better results. We can also see
that the performance of SVM-Unigram-removal2
is worse than other baselines. This can be ex-
plained as follows: for example, word ?my? has
large probability to be generated from the back-
ground topic. However it can also be generated by
deceptive topic occasionaly but can hardly be gen-
erated from the truthful topic. So the removal of
these words results in the loss of useful informa-
tion, and leads to low accuracy rate.
Our topic-modeling approach uses word fre-
quency as features and does not involve any fea-
ture selection process. Here we present the re-
sults of the sample reviews from Section 1. Stop
words are labeled in black, background topics (B)
in blue, hotel specific topics (H) in orange, de-
ceptive topics (D) in red and truthful topic (T) in
green.
1. My husband and I stayed for two nights at the Hilton
Chicago. We were very pleased with the accommoda-
tions and enjoyed the service every minute of it! The
bedrooms are immaculate,and the linens are very soft.
We also appreciated the free wifi, as we could stay in
touch with friends while staying in Chicago. The bath-
room was quite spacious, and I loved the smell of the
shampoo they provided not like most hotel shampoos.
Their service was amazing,and we absolutely loved the
beautiful indoor pool. I would recommend staying here
to anyone.
[B,H,D,T]=[41,6,10,1] p(D)=0.909 P(T)=0.091
2. We stayed at the Sheraton by Navy Pier the first week-
end of November. The view from both rooms was spec-
tacular (as you can tell from the picture attached). They
also left a plate of cookies and treats in the kids room
upon check-in made us all feel very special. The ho-
tel is central to both Navy Pier and Michigan Ave. so
we walked, trolleyed, and cabbed all around the area.
We ate the breakfast buffet both mornings and thought
it was pretty good. The eggs were a little runny. Our
six year old ate free and our two eleven year old were
$14 ( instead of the adult $20) The rooms were clean,
the concierge and reception staff were both friendly
and helpful...we will definitely visit this Sheraton again
when we?re in Chicago next time.
[B,H,D,T]=[80,15,3,18] p(D)=0.143 P(T)=0.857
background deceptive truthful Hilton
hotel hotel room Hilton
stay my ) palmer
we chicago ( millennium
room will but lockwood
! room $ park
Chicago very bathroom lobby
my visit location line
great husband night valet
I city walk shampoo
very experience park dog
Omni Amalfi Sheraton James
Omni Amalfi tower James
pool breakfast Sheraton service
plasma view pool spa
sundeck floor river bar
chocolate bathroom lake upgrade
indoor cocktail navy primehouse
request morning indoor design
pillow wine shower overlook
suitable great kid romantic
area room theater home
Table 2: Top words in different topics from Topic-
Spam
4 Conclusion
In this paper, we propose a novel topic model
for deceptive opinion spam detection. Our model
achieves an accuracy of 94.8%, demonstrating its
effectiveness on the task.
5 Acknowledgements
We thank Myle Ott for his insightful comments and sugges-
tions. This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, and by a gift from Google.
220
References
David Blei, Andrew Ng and Micheal Jordan. Latent
Dirichlet alocation. 2003. In Journal of Machine
Learning Research.
Carlos Castillo, Debora Donato, Luca Becchetti, Paolo
Boldi, Stefano Leonardi Massimo Santini, and Se-
bastiano Vigna. A reference collection for web
spam. In Proceedings of annual international ACM
SIGIR conference on Research and development in
information retrieval, 2006.
Chaltanya Chemudugunta, Padhraic Smyth and Mark
Steyers. Modeling General and Specific Aspects of
Documents with a Probabilistic Topic Model.. In
Advances in Neural Information Processing Systems
19: Proceedings of the 2006 Conference.
Paul-Alexandru Chirita, Jorg Diederich, and Wolfgang
Nejdl. MailRank: using ranking for spam detection.
In Proceedings of ACM international conference on
Information and knowledge management. 2005.
Harris Drucke, Donghui Wu, and Vladimir Vapnik.
2002. Support vector machines for spam categoriza-
tion. In Neural Networks.
Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim.
In Proceeding of the 50th Annual Meeting of the As-
sociation for Computational Linguistics. 2012
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in
kernel methods.
Jack Jansen. 2010. Online product research. In Pew In-
ternet and American Life Project Report.
Nitin Jindal, and Bing Liu. Opinion spam and analysis.
2008. In Proceedings of the international conference
on Web search and web data mining
Nitin Jindal, Bing Liu, and Ee-Peng Lim. Finding
Unusual Review Patterns Using Unexpected Rules.
2010. In Proceedings of the 19th ACM international
conference on Information and knowledge manage-
ment
Pranam Kolari, Akshay Java, Tim Finin, Tim Oates and
Anupam Joshi. Detecting Spam Blogs: A Machine
Learning Approach. In Proceedings of Association
for the Advancement of Artificial Intelligence. 2006.
Peng Li, Jing Jiang and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
Learning to identify review Spam. 2011. In Proceed-
ings of the Twenty-Second international joint confer-
ence on Artificial Intelligence.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. Detecting Product Re-
view Spammers Using Rating Behavior. 2010. In
Proceedings of the 19th ACM international confer-
ence on Information and knowledge management.
Stephen Litvina, Ronald Goldsmithb and Bing Pana.
2008. Electronic word-of-mouth in hospitality
and tourism management. Tourism management,
29(3):458468.
Juan Martinez-Romo and Lourdes Araujo. Web Spam
Identification Through Language Model Analysis.
In AIRWeb. 2009.
Arjun Mukherjee, Bing Liu and Natalie Glance. Spot-
ting Fake Reviewer Groups in Consumer Reviews.
In Proceedings of the 18th international conference
on World wide web, 2012.
Alexandros Ntoulas, Marc Najork, Mark Manasse and
Dennis Fetterly. Detecting Spam Web Pages through
Content Analysis. In Proceedings of international
conference on World Wide Web 2006
Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Han-
cock. Finding deceptive opinion spam by any stretch
of the imagination. 2011. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
Bo Pang and Lillian Lee. Opinion mining and senti-
ment analysis. In Found. Trends Inf. Retr.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher D. Manning. Labeled LDA: a super-
vised topic model for credit attribution in multi-
labeled corpora. 2009. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing 2009.
Michal Rosen-zvi, Thomas Griffith, Mark Steyvers and
Padhraic Smyth. The author-topic model for authors
and documents. In Proceedings of the 20th confer-
ence on Uncertainty in artificial intelligence.
Guan Wang, Sihong Xie, Bing Liu and Philip Yu. Re-
view Graph based Online Store Review Spammer
Detection. 2011. In Proceedings of 11th Interna-
tional Conference of Data Mining.
Baoning Wu, Vinay Goel and Brian Davison. Topical
TrustRank: using topicality to combat Web spam.
In Proceedings of international conference on World
Wide Web 2006 .
Kyang Yoo and Ulrike Gretzel. 2009. Compari-
son of Deceptive and Truthful Travel Reviews.
InInformation and Communication Technologies in
Tourism 2009.
221
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 556?560,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evolutionary Hierarchical Dirichlet Process for Timeline Summarization
Jiwei Li
School of Computer Science
Cornell University
Ithaca, NY, 14853
jl3226@cornell.edu
Sujian Li
Laboratory of Computational Linguistics
Peking University
Bejing, P.R.China, 150001
lisujian@pku.edu.cn
Abstract
Timeline summarization aims at generat-
ing concise summaries and giving read-
ers a faster and better access to under-
stand the evolution of news. It is a new
challenge which combines salience rank-
ing problem with novelty detection. Pre-
vious researches in this field seldom ex-
plore the evolutionary pattern of topics
such as birth, splitting, merging, develop-
ing and death. In this paper, we develop
a novel model called Evolutionary Hier-
archical Dirichlet Process(EHDP) to cap-
ture the topic evolution pattern in time-
line summarization. In EHDP, time vary-
ing information is formulated as a series
of HDPs by considering time-dependent
information. Experiments on 6 different
datasets which contain 3156 documents
demonstrates the good performance of our
system with regard to ROUGE scores.
1 Introduction
Faced with thousands of news articles, people usu-
ally try to ask the general aspects such as the
beginning, the evolutionary pattern and the end.
General search engines simply return the top rank-
ing articles according to query relevance and fail
to trace how a specific event goes. Timeline sum-
marization, which aims at generating a series of
concise summaries for news collection published
at different epochs can give readers a faster and
better access to understand the evolution of news.
The key of timeline summarization is how to
select sentences which can tell readers the evolu-
tionary pattern of topics in the event. It is very
common that the themes of a corpus evolve over
time, and topics of adjacent epochs usually exhibit
strong correlations. Thus, it is important to model
topics across different documents and over differ-
ent time periods to detect how the events evolve.
The task of timelime summarization is firstly
proposed by Allan et al(2001) by extracting clus-
ters of noun phases and name entities. Chieu et
al.(2004) built a similar system in unit of sentences
with interest and burstiness. However, these meth-
ods seldom explored the evolutionary character-
istics of news. Recently, Yan et al(2011) ex-
tended the graph based sentence ranking algorithm
used in traditional multi-document summarization
(MDS) to timeline generation by projecting sen-
tences from different time into one plane. They
further explored the timeline task from the opti-
mization of a function considering the combina-
tion of different respects such as relevance, cover-
age, coherence and diversity (Yan et al, 2011b).
However, their approaches just treat timeline gen-
eration as a sentence ranking or optimization prob-
lem and seldom explore the topic information lied
in the corpus.
Recently, topic models have been widely used
for capturing the dynamics of topics via time.
Many dynamic approaches based on LDA model
(Blei et al, 2003) or Hierarchical Dirichelt Pro-
cesses(HDP) (Teh et al, 2006) have been pro-
posed to discover the evolving patterns in the cor-
pus as well as the snapshot clusters at each time
epoch (Blei and Lafferty, 2006; Chakrabarti et al,
2006; Wang and McCallum, 2007; Caron et al,
2007; Ren et al, 2008; Ahmed and Xing, 2008;
Zhang et al, 2010).
In this paper, we propose EHDP: a evolution-
ary hierarchical Dirichlet process (HDP) model
for timeline summarization. In EHDP, each HDP
is built for multiple corpora at each time epoch,
and the time dependencies are incorporated into
epochs under the Markovian assumptions. Topic
popularity and topic-word distribution can be in-
ferred from a Chinese Restaurant Process (CRP).
Sentences are selected into timelines by consider-
ing different aspects such as topic relevance, cov-
erage and coherence. We built the evaluation sys-
556
tems which contain 6 real datasets and perfor-
mance of different models is evaluated accord-
ing to the ROUGE metrics. Experimental results
demonstrate the effectiveness of our model .
2 EHDP for Timeline Summarization
2.1 Problem Formulation
Given a general query Q = {wqi}i=Qni=1 , we firstly
obtain a set of query related documents. We no-
tate different corpus as C = {Ct}t=Tt=1 according
to their published time where Ct = {Dti}i=Nti=1 de-
notes the document collection published at epoch
t. Document Dti is formulated as a collection of
sentences {stij}j=Ntij=1 . Each sentence is presented
with a series of words stij = {wtijl}
l=Ntij
l=1 and as-
sociated with a topic ?tij . V denotes the vocabu-
lary size. The output of the algorithm is a series
of timelines summarization I = {It}t=Tt=1 where
It ? Ct
2.2 EHDP
Our EHDP model is illustrated in Figure 2. Specif-
ically, each corpus Ct is modeled as a HDP. These
HDP shares an identical base measure G0, which
serves as an overall bookkeeping of overall mea-
sures. We use Gt0 to denote the base measure at
each epoch and draw the local measureGti for each
document at time t from Gt0. In EHDP, each sen-
tence is assigned to an aspect ?tij with the consid-
eration of words within current sentence.
To consider time dependency information in
EHDP, we link all time specific base measures Gt0
with a temporal Dirichlet mixture model as fol-
lows:
Gt0 ? DP (?t,
1
KG0+
1
K
??
?=0
F (v, ?)?Gt??0 ) (1)
where F (v, ?) = exp(??/v) denotes the expo-
nential kernel function that controls the influence
of neighboring corpus. K denotes the normaliza-
tion factor where K = 1 + ???=0 F (v, ?). ? is
the time width and ? is the decay factor. In Chi-
nese Restaurant Process (CRP), each document is
referred to a restaurant and sentences are com-
pared to customers. Customers in the restaurant
sit around different tables and each table btin is as-
sociated with a dish (topic) ?tin according to the
dish menu. Let mtk denote the number of ta-
bles enjoying dish k in all restaurants at epoch t,
mtk =
?Nt
i=1
?Ntib
n=1 1(?tin = k). We redefine
for each epoch t ? [1, T ]
1. draw global measure
Gt0 ? DP (?, 1KG0 + 1K
??
?=0 F (v, ?)Gt??0 )2. for each document Dti at epoch t,
2.1 draw local measure Gti ? DP (?,Gt0)
2.2 for each sentence stij in Dti
draw aspect ?tij ? Gti
for w ? stij draw w ? f(w)|?tij
Figure 1: Generation process for EHDP
another parameter Mtk to incorporate time depen-
dency into EHDP.
Mtk =
??
?=0
F (v, ?) ?mt??,k (2)
Let ntib denote the number of sentences sitting
around table b, in document i at epoch t. In CRP
for EHDP, when a new customer stij comes in,
he can sit on the existing table with probability
ntib/(nti?1+?), sharing the dish (topic) ?tib served
at that table or picking a new table with probabil-
ity ?/(nti ? 1 + ?). The customer has to select
a dish from the global dish menu if he chooses a
new table. A dish that has already been shared in
the global menu would be chosen with probability
M tk/(
?
kM tk+?) and a new dish with probability
?/(
?
kM tk + ?).
?tij |?ti1, ..., ?tij?1, ? ?
?
?tb=?ij
ntib
nti ? 1 + ?
??jb +
?
nti ? 1 + ?
??newjb
?newti |?, ? ?
?
k
Mtk?
iMti + ?
??k +
??
iMti + ?
G0 (3)
We can see that EHDP degenerates into a series of
independent HDPs when ? = 0 and one global
HDP when ? = T and v = ?, as discussed in
Amred and Xings work (2008).
2.3 Sentence Selection Strategy
The task of timeline summarization aims to pro-
duce a summary for each time and the generated
summary should meet criteria such as relevance ,
coverage and coherence (Li et al, 2009). To care
for these three criteria, we propose a topic scoring
algorithm based on Kullback-Leibler(KL) diver-
gence. We introduce the decreasing logistic func-
tion ?(x) = 1/(1 + ex) to map the distance into
interval (0,1).
557
Figure 2: Graphical model of EHDP.
Relevance: the summary should be related with
the proposed query Q.
FR(It) = ?(KL(It||Q))
Coverage: the summary should highly generalize
important topics mentioned in document collec-
tion at epoch t.
FCv(It) = ?(KL(It||Ct))
Coherence: News evolves over time and a good
component summary is coherent with neighboring
corpus so that a timeline tracks the gradual evolu-
tion trajectory for multiple correlative news.
FCh(It) =
??=?/2
?=??/2 F (v, ?) ? ?(KL(It||Ct??))
??=?/2
?=??/2 F (v, ?)
Let Score(It) denote the score of the summary
and it is calculated in Equ.(4).
Score(It) = ?1FR(It)+?2FCv(It)+?3FCh(It)
(4)?
i ?i = 1. Sentences with higher score are se-
lected into timeline. To avoid aspect redundancy,
MMR strategy (Goldstein et al, 1999) is adopted
in the process of sentence selection.
3 Experiments
3.1 Experiments set-up
We downloaded 3156 news articles from selected
sources such as BBC, New York Times and CNN
with various time spans and built the evaluation
systems which contains 6 real datasets. The news
belongs to different categories of Rule of Interpre-
tation (ROI) (Kumaran and Allan, 2004). Detailed
statistics are shown in Table 1. Dataset 2(Deep-
water Horizon oil spill), 3(Haiti Earthquake) and
5(Hurricane Sandy) are used as training data and
New Source Nation News Source Nation
BBC UK New York Times US
Guardian UK Washington Post US
CNN US Fox News US
ABC US MSNBC US
Table 1: New sources of datasets
News Subjects (Query) #docs #epoch
1.Michael Jackson Death 744 162
2.Deepwater Horizon oil spill 642 127
3.Haiti Earthquake 247 83
4.American Presidential Election 1246 286
5.Hurricane Sandy 317 58
6.Jerry Sandusky Sexual Abuse 320 74
Table 2: Detailed information for datasets
the rest are used as test data. Summary at each
epoch is truncated to the same length of 50 words.
Summaries produced by baseline systems and
ours are automatically evaluated through ROUGE
evaluation metrics (Lin and Hovy, 2003). For
the space limit, we only report three ROUGE
ROUGE-2-F and ROUGE-W-F score. Reference
timeline in ROUGE evaluation is manually gener-
ated by using Amazon Mechanical Turk1. Work-
ers were asked to generate reference timeline for
news at each epoch in less than 50 words and we
collect 790 timelines in total.
3.2 Parameter Tuning
To tune the parameters ?(i = 1, 2, 3) and v in our
system, we adopt a gradient search strategy. We
firstly fix ?i to 1/3. Then we perform experiments
on with setting different values of v/#epoch in
the range from 0.02 to 0.2 at the interval of 0.02.
We find that the Rouge score reaches its peak at
round 0.1 and drops afterwards in the experiments.
Next, we set the value of v is set to 0.1 ? #epoch
and gradually change the value of ?1 from 0 to 1
with interval of 0.05, with simultaneously fixing
?2 and ?3 to the same value of (1 ? ?1)/2. The
performance gets better as ?1 increases from 0 to
0.25 and then declines. Then we set the value of
?1 to 0.25 and change the value of ?2 from 0 to
0.75 with interval of 0.05. And the value of ?2 is
set to 0.4, and ?3 is set to 0.35 correspondingly.
3.3 Comparison with other topic models
In this subsection, we compare our model with
4 topic model baselines on the test data. Stand-
HDP(1): A topic approach that models different
time epochs as a series of independent HDPs with-
out considering time dependency. Stand-HDP(2):
1http://mturk.com
558
M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Stand-HDP(1) 0.080 0.127 0.075 0.134 0.072 0.138
Stand-HDP(2) 0.077 0.124 0.072 0.127 0.071 0.131
Dyn-LDA 0.080 0.129 0.073 0.130 0.077 0.134
Stan-LDA 0.072 0.117 0.065 0.122 0.071 0.121
Table 3: Comparison with topic models
M.J. Death US Election S. Sexual Abuse
System R2 RW R2 RW R2 RW
EHDP 0.089 0.130 0.081 0.154 0.086 0.152
Centroid 0.057 0.101 0.054 0.098 0.060 0.132
Manifold 0.053 0.108 0.060 0.111 0.069 0.128
ETS 0.078 0.120 0.073 0.130 0.075 0.135
Chieu 0.064 0.107 0.064 0.122 0.071 0.131
Table 4: Comparison with other baselines
A global HDP which models the whole time span
as a restaurant. The third baseline, Dynamic-
LDA is based on Blei and Laffery(2007)?s work
and Stan-LDA is based on standard LDA model.
In LDA based models, aspect number is prede-
fined as 80 2. Experimental results of different
models are shown in Table 2. As we can see,
EHDP achieves better results than the two stan-
dard HDP baselines where time information is not
adequately considered. We also find an interesting
result that Stan-HDP performs better than Stan-
LDA. This is partly because new aspects can be
automatically detected in HDP. As we know, how
to determine topic number in the LDA-based mod-
els is still an open problem.
3.4 Comparison with other baselines
We implement several baselines used in tradi-
tional summarization or timeline summarization
for comparison. (1) Centroid applies the MEAD
algorithm (Radev et al, 2004) according to the
features including centroid value, position and
first-sentence overlap. (2) Manifold is a graph
based unsupervised method for summarization,
and the score of each sentence is got from the
propagation through the graph (Wan et al, 2007).
(3) ETS is the timeline summarization approach
developed by Yan et al, (2011a), which is a graph
based approach with optimized global and local
biased summarization. (4) Chieu is the time-
line system provided by (Chieu and Lee, 2004)
utilizing interest and bursty ranking but neglect-
ing trans-temporal news evolution. As we can
see from Table 3, Centroid and Manifold get
the worst results. This is probably because meth-
ods in multi-document summarization only care
2In our experiments, the aspect number is set as 50, 80,
100 and 120 respectively and we select the best performed
result with the aspect number as 80
about sentence selection and neglect the novelty
detection task. We can also see that EHDP under
our proposed framework outputs existing timeline
summarization approaches ETS and chieu. Our
approach outputs Yan et al,(2011a)s model by
6.9% and 9.3% respectively with regard to the av-
erage score of ROUGE-2-F and ROUGE-W-F.
4 Conclusion
In this paper we present an evolutionary HDP
model for timeline summarization. Our EHDP ex-
tends original HDP by incorporating time depen-
dencies and background information. We also de-
velop an effective sentence selection strategy for
candidate in the summaries. Experimental results
on real multi-time news demonstrate the effective-
ness of our topic model.
Oct. 3, 2012
S1: The first debate between President Obama and Mitt Rom-
ney, so long anticipated, quickly sunk into an unenlightening
recitation of tired talking points and mendacity. S2. Mr. Rom-
ney wants to restore the Bush-era tax cut that expires at the end
of this year and largely benefits the wealthy
Oct. 11, 2012
S1: The vice presidential debate took place on Thursday, Oc-
tober 11 at Kentucky?sCentre College, and was moderated by
Martha Raddatz. S2: The first and only debate between Vice
President Joe Biden and Congressman Paul Ryan focused on
domestic and foreign policy. The domestic policy segments in-
cluded questions on health care, abortion
Oct. 16, 2012
S1. President Obama fights back in his second debate with Mitt
Romney, banishing some of the doubts he raised in their first
showdown. S2: The second debate dealt primarily with domes-
tic affairs and include some segues into foreign policy. includ-
ing taxes, unemployment, job creation, the national debt, energy
and women?s rights, both legal and
Table 5: Selected timeline summarization gener-
ated by EHDP for American Presidential Election
5 Acknowledgement
This research has been supported by NSFC grants
(No.61273278), National Key Technology RD
Program (No:2011BAH1B0403), National 863
Program (No.2012AA011101) and National So-
cial Science Foundation (No.12ZD227).
References
Amr Ahmed and Eric Xing. Dynamic non-parametric
mixture models and the recurrent chinese restaurant
process. 2008. In SDM.
559
James Allan, Rahul Gupta and Vikas Khandelwal.
Temporal summaries of new topics. 2001. In Pro-
ceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval
David Blei, Andrew Ng and Micheal Jordan. 2003.
Latent dirichlet alocation. In Journal of Machine
Learning Research.
David Blei and John Lafferty. Dynamic topic models.
2006. In Proceedings of the 23rd international con-
ference on Machine learning.
Francois Carol, Manuel Davy and Arnaud Doucet.
Generalized poly urn for time-varying dirichlet pro-
cess mixtures. 2007. In Proceedings of the Interna-
tional Conference on Uncertainty in Artificial Intel-
ligence.
Deepayan Chakrabarti, Ravi Kumar and Andrew
Tomkins. Evolutionary Clustering. InProceedings of
the 12th ACM SIGKDD international conference
Knowledge discoveryand data mining.
Hai-Leong Chieu and Yoong-Keok Lee. Query based
event extraction along a timeline. In Proceedings of
the 27th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval
Giridhar Kumaran and James Allan. 2004. Text classifi-
cation and named entities for new event detection. In
Proceedings of the 27th annual international ACM
SIGIR04.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha
and Yong Yu. Enhancing diversity, coverage and bal-
ance for summarization through structure learning.
In Proceedings of the 18th international conference
on World wide web.
Chin-Yew Lin and Eduard Hovy. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of the Human Language Technology
Conference of the NAACL. 2003.
Dragomar Radev, Hongyan. Jing, and Malgorzata Stys.
2004. Centroid-based summarization of multiple
documents. In Information Processing and Manage-
ment.
Lu Ren, David Dunson and Lawrence Carin. The dy-
namic hierarchical Dirichlet process. 2008. In Pro-
ceedings of the 25th international conference on
Machine Learning.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao.
2007. Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence.
Xuerui Wang and Andrew MaCallum. Topics over
time: a non-Markov continuous-time model of topi-
cal trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
Yee Whye Teh, Michael Jordan, Matthew Beal and
David Blei. Hierarchical Dirichlet Processes. In
American Statistical Association.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li and Yan Zhang. 2011a. Evolutionary
Timeline Summarization: a Balanced Optimization
Framework via Iterative Substitution. In Proceed-
ings of the 34th international ACM SIGIR confer-
ence on Research and development in Information
Retrieval.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Jahna Otterbacher, Xiaoming Li and Yan Zhang.
Timeline Generation Evolutionary Trans-Temporal
Summarization. 2011b. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Jianwen Zhang, Yangqiu Song, Changshui Zhang and
Shixia Liu. 2010. Evolutionary Hierarchical Dirich-
let Processes for Multiple Correlated Time-varying
Corpora. In Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
560
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 25?35,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Text-level Discourse Dependency Parsing 
 
Sujian Li1 Liang Wang1 Ziqiang Cao1 Wenjie Li2 
1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 
2 Department of Computing, The Hong Kong Polytechnic University, HongKong 
{lisujian,intfloat,ziqiangyeah}@pku.edu.cn 
cswjli@comp.polyu.edu.hk 
  
 
Abstract 
Previous researches on Text-level discourse 
parsing mainly made use of constituency 
structure to parse the whole document into 
one discourse tree. In this paper, we present 
the limitations of constituency based dis-
course parsing and first propose to use de-
pendency structure to directly represent the 
relations between elementary discourse 
units (EDUs). The state-of-the-art depend-
ency parsing techniques, the Eisner algo-
rithm and maximum spanning tree (MST) 
algorithm, are adopted to parse an optimal 
discourse dependency tree based on the arc-
factored model and the large-margin learn-
ing techniques. Experiments show that our 
discourse dependency parsers achieve a 
competitive performance on text-level dis-
course parsing.  
1 Introduction 
It is widely agreed that no units of the text can be 
understood in isolation, but in relation to their 
context. Researches in discourse parsing aim to 
acquire such relations in text, which is funda-
mental to many natural language processing ap-
plications such as question answering, automatic 
summarization and so on. 
One important issue behind discourse parsing 
is the representation of discourse structure. Rhe-
torical Structure Theory (RST) (Mann and 
Thompson, 1988), one of the most influential 
discourse theories, posits a hierarchical genera-
tive tree representation, as illustrated in Figure 1. 
The leaves of a tree correspond to contiguous 
text spans called Elementary Discourse Units 
(EDUs)1. The adjacent EDUs are combined into 
                                                          
1 EDU segmentation is a relatively trivial step in discourse 
parsing. Since our work focus here is not EDU segmenta-
tion but discourse parsing. We assume EDUs are already 
known. 
the larger text spans by rhetorical relations (e.g., 
Contrast and Elaboration) and the larger text 
spans continue to be combined until the whole 
text constitutes a parse tree. The text spans 
linked by rhetorical relations are annotated as 
either nucleus or satellite depending on how sali-
ent they are for interpretation. It is attractive and 
challenging to parse the whole text into one tree.  
Since such a hierarchical discourse tree is 
analogous to a constituency based syntactic tree 
except that the constituents in the discourse trees 
are text spans, previous researches have explored 
different constituency based syntactic parsing 
techniques (eg. CKY and chart parsing) and var-
ious features (eg. length, position et al) for dis-
course parsing (Soricut and Marcu, 2003; Joty et 
al., 2012; Reitter, 2003; LeThanh et al, 2004; 
Baldridge and Lascarides, 2005; Subba and Di 
Eugenio, 2009; Sagae, 2009; Hernault et al, 
2010b; Feng and Hirst, 2012). However, the ex-
isting approaches suffer from at least one of the 
following three problems. First, it is difficult to 
design a set of production rules as in syntactic 
parsing, since there are no determinate genera-
tive rules for the interior text spans. Second, the 
different levels of discourse units (e.g. EDUs or 
larger text spans) occurring in the generative 
process are better represented with different fea-
tures, and thus a uniform framework for dis-
course analysis is hard to develop. Third, to 
reduce the time complexity of the state-of-the-art 
constituency based parsing techniques, the ap-
proximate parsing approaches are prone to trap 
in local maximum. 
In this paper, we propose to adopt the depend-
ency structure in discourse representation to 
overcome the limitations mentioned above. Here 
is the basic idea: the discourse structure consists 
of EDUs which are linked by the binary, asym-
metrical relations called dependency relations. A 
dependency relation holds between a subordinate 
EDU called the dependent, and another EDU on 
25
which it depends called the head, as illustrated in 
Figure 2. Each EDU has one head. So, the de-
pendency structure can be seen as a set of head-
dependent links, which are labeled by functional 
relations. Now, we can analyze the relations be-
tween EDUs directly, without worrying about 
any interior text spans. Since dependency trees 
contain much fewer nodes and on average they 
are simpler than constituency based trees, the 
current dependency parsers can have a relatively 
low computational complexity. Moreover, con-
cerning linearization, it is well known that de-
pendency structures can deal with non-projective 
relations, while constituency-based models need 
the addition of complex mechanisms like trans-
formations, movements and so on. In our work, 
we adopt the graph based dependency parsing 
techniques learned from large sets of annotated 
dependency trees. The Eisner (1996) algorithm 
and maximum spanning tree (MST) algorithm 
are used respectively to parse the optimal projec-
tive and non-projective dependency trees with 
the large-margin learning technique (Crammer 
and Singer, 2003). To the best of our knowledge, 
we are the first to apply the dependency structure 
and introduce the dependency parsing techniques 
into discourse analysis.  
The rest of this paper is organized as follows. 
Section 2 formally defines discourse dependency 
structure and introduces how to build a discourse 
dependency treebank from the existing RST cor-
pus. Section 3 presents the discourse parsing ap-
proach based on the Eisner and MST algorithms. 
Section 4 elaborates on the large-margin learning 
technique as well as the features we use. Section 
5 discusses the experimental results. Section 6 
introduces the related work and Section 7 con-
cludes the paper. 
1
e2
e1-e2
*
e3
e1- 2
*
-e3
e1 e2
e1-e2
*
e3
e1-e2-e3
*
e1 e2
e1
*
-e2
e3
e1-e2-e3
*
e1
e2
e1
*
-e2
e3
e1
*
-e2-e3
e1 e2
e2
*
-e3
e3
e1-e2
*
-e3
e1 e2
e3 e1 e2
e3
e1
*
-e2-e3
e2
*
-e3 e2-e3
*
e1
*
-e2-e3
1 2 3 4
5 6 7 8
e1
e2 e3
e1- 2- 3
*
e2-e3
*
 
Figure 1: Headed Constituency based Discourse Tree Structure (e1,e2 and e3 denote three EDUs, 
and * denotes the NUCLEUS constituent) 
e1 e2 e3 e1 e2 e3 e1 e2 e3 e1 e2 e3
e1 e2 e3e1 e2 e3 e1 e2 e3e1 e2 e3
1' 2' 3' 4'
5' 6' 7' 8' 9'
e1 e2 e3
e0e0 e0 e0
e0 e0 e0 e0 e0
 
Figure 2: Discourse Dependency Tree Structures (e1,e2 and e3 denote three EDUS, and the directed 
arcs  denote one dependency relations. The artificial e0 is also displayed here. ) 
2 Discourse Dependency Structure and 
Tree Bank 
2.1 Discourse Dependency Structure 
Similar to the syntactic dependency structure 
defined by McDonald (2005a, 2005b), we insert 
an artificial EDU e0 in the beginning for each 
document and label the dependency relation link-
ing from e0 as ROOT. This treatment will sim-
plify both formal definitions and computational 
implementations. Normally, we assume that each 
EDU should have one and only one head except 
for e0. A labeled directed arc is used to represent 
the dependency relation from one head to its de-
pendent. Then, discourse dependency structure 
can be formalized as the labeled directed graph, 
where nodes correspond to EDUs and labeled 
arcs correspond to labeled dependency relations. 
26
We assume that the text2  T is composed of 
n+1 EDUs including the artificial e0. That is 
T=e0 e1 e2 ? en. Let R={r1,r2, ? ,rm} denote a 
finite set of functional relations that hold be-
tween two EDUs. Then a discourse dependency 
graph can be denoted by G=<V, A> where V de-
notes a set of nodes and A denotes a set of la-
beled directed arcs, such that for the text T=e0 e1 
e2 ? en and the label set R the following holds: 
(1) V = { e0, e1, e2, ? en } 
(2) A ? V? R ? V, where <ei, r, ej>?A represents 
an arc from the head ei to the dependent ej 
labeled with the relation r. 
(3) If <ei, r, ej>?A then <ek, r?, ej>?A for all k?i  
(4) If <ei, r, ej>?A then <ei, r?, ej>?A for all r??r 
The third condition assures that each EDU has 
one and only one head and the fourth tells that 
only one kind of dependency relation holds be-
tween two EDUs. According to the definition, 
we illustrate all the 9 possible unlabeled depend-
ency trees for a text containing three EDUs in 
Figure 2. The dependency trees 1? to 7? are pro-
jective while 8? and 9? are non-projective with 
crossing arcs. 
2.2 Our Discourse Dependency Treebank  
To automatically conduct discourse dependency 
parsing, constructing a discourse dependency 
treebank is fundamental. It is costly to manually 
construct such a treebank from scratch. Fortu-
nately, RST Discourse Treebank (RST-DT) 
(Carlson et al, 2001) is an available resource to 
help with.  
A RST tree constitutes a hierarchical structure 
for one document through rhetorical relations. A 
total of 110 fine-grained relations (e.g. Elabora-
tion-part-whole and List) were used for tagging 
RST-DT. They can be categorized into 18 classes 
(e.g. Elaboration and Joint). All these relations 
can be hypotactic (?mononuclear?) or paratactic 
(?multi-nuclear?). A hypotactic relation holds 
between a nucleus span and an adjacent satellite 
span, while a paratactic relation connects two or 
more equally important adjacent nucleus spans. 
For convenience of computation, we convert the 
n-ary (n>2) RST trees3 to binary trees through 
adding a new node for the latter n-1 nodes and 
assume each relation is connected to only one 
nucleus4. This departure from the original theory 
                                                          
2 The two terms ?text? and ?document? are used inter-
changeably and represent the same meaning. 
3 According to our statistics, there are totally 381 n-ary rela-
tions in RST-DT.  
4 We set the first nucleus as the only nucleus. 
is not such a major step as it may appear, since 
any nucleus is known to contribute to the essen-
tial meaning. Now, each RST tree can be seen as 
a headed constituency based binary tree where 
the nuclei are heads and the children of each 
node are linearly ordered. Given three EDUs5, 
Figure 1 shows the possible 8 headed constituen-
cy based trees where the superscript * denotes 
the heads (nuclei). We use dependency trees to 
simulate the headed constituency based trees.  
Contrasting Figure 1 with Figure 2, we use 
dependency tree 1? to simulate binary trees 1 and 
8, and dependency tress 2?- 7? to simulate binary 
trees 2-7 correspondingly. The rhetorical rela-
tions in RST trees are kept as the functional rela-
tions which link the two EDUs in dependency 
trees. With this kind of conversion, we can get 
our discourse dependency treebank. It is worth 
noting that the non-projective trees like 8? and 9? 
do not exist in our dependency treebank, though 
they are eligible according to the definition of 
discourse dependency graph.  
3 Discourse Dependency Parsing 
3.1 System Overview 
As stated above, T=e0 e1 ?en represents an input 
text (document) where ei denotes the i
th EDU of 
T. We use V to denote all the EDU nodes and 
V?R?V-0 (V-0 =V-{e0}) denote all the possible 
discourse dependency arcs. The goal of discourse 
dependency parsing is to parse an optimal span-
ning tree from V?R?V-0. Here we follow the arc 
factored method and define the score of a de-
pendency tree as the sum of the scores of all the 
arcs in the tree. Thus, the optimal dependency 
tree for T is a spanning tree with the highest 
score and obtained through the function DT(T,w): 
0
0
0
, ,
, ,
( , )( , )
( , , )
( , , )f
T
T
i j T
T
i j T
G V R V
G V R V i j
e r e G
G V R V i j
e r e G
TDT T argmax
argmax e r e
argmax e r
s ore T G
e
c
?
?
?
?
? ? ?
? ? ?
? ??
? ? ?
? ??
?
?
? ?
?
?
w
w
where GT means a possible spanning tree with 
( , )Tscore T G  and ?(       ) denotes the score of 
the arc <ei, r, ej> which is calculated according to 
its feature representation f(ei,r,ej) and a weight 
vector w. 
Next, two basic problems need to be solved: 
how to find the dependency tree with the highest 
                                                          
5 We can easily get al possible headed binary trees for one 
more complex text containing more than three EDUs, by 
extending the 8 possible situations for three EDUs.  
27
score for T given all the arc scores (i.e. a parsing 
problem), and how to learn and compute the 
scores of arcs according to a set of arc features 
(i.e. a learning problem).  
The following of this section addresses the 
first problem. Given the text T, we first reduce 
the multi-digraph composed of all possible arcs 
to the digraph. The digraph keeps only one arc 
<ei, r, ej> between two nodes which satisfies 
?(       )                  . Thus, we can 
proceed with a reduction from labeled parsing to 
unlabeled parsing. Next, two algorithms, i.e. the 
Eisner algorithm and MST algorithm, are pre-
sented to parse the projective and non-projective 
unlabeled dependency trees respectively. 
3.2 Eisner Algorithm 
It is well known that projective dependency pars-
ing can be handled with the Eisner algorithm 
(1996) which is based on the bottom-up dynamic 
programming techniques with the time complexi-
ty of O(n3). The basic idea of the Eisner algo-
rithm is to parse the left and right dependents of 
an EDU independently and combine them at a 
later stage. This reduces the overhead of index-
ing heads. Only two binary variables, i.e. c and d, 
are required to specify whether the heads occur 
leftmost or rightmost and whether an item is 
complete. 
 
Eisner(T,  ?) 
Input: Text T=e0 e1? en; Arc scores ?(ei,ej) 
1   Instantiate E[i, i, d, c]=0.0 for all i, d, c 
2   For m := 1 to n 
3       For i := 1 to n 
4          j = i + m 
5          if j> n then break;  
6          # Create subgraphs with c=0 by adding arcs 
7         E[i, j, 0, 0]=maxi?q?j (E[i,q,1,1]+E[q+1,j,0,1]+?(ej,ei)) 
8         E[i, j, 1, 0]=maxi?q?j (E[i,q,1,1]+E[q+1,j,0,1]+?(ei,ej)) 
9          # Add corresponding left/right subgraphs 
10        E[i, j, 0, 1]=maxi?q?j (E[i,q,0,1]+E[q,j,0,0] 
11        E[i, j, 1, 1]=maxi?q?j (E[i,q,1,0]+E[q,j,1,1]) 
Figure 3: Eisner Algorithm 
Figure 3 shows the pseudo-code of the Eisner 
algorithm. A dynamic programming table 
E[i,j,d,c] is used to represent the highest scored 
subtree spanning ei to ej. d indicates whether ei is 
the head (d=1) or ej is head (d=0). c indicates 
whether the subtree will not take any more de-
pendents (c=1) or it needs to be completed (c=0). 
The algorithm begins by initializing all length-
one subtrees to a score of 0.0. In the inner loop, 
the first two steps (Lines 7 and 8) are to construct 
the new dependency arcs by taking the maximum 
over all the internal indices (i?q?j) in the span, 
and calculating the value of merging the two sub-
trees and adding one new arc. The last two steps 
(Lines 10 and 11) attempt to achieve an optimal 
left/right subtree in the span by adding the corre-
sponding left/right subtree to the arcs that have 
been added previously. This algorithm considers 
all the possible subtrees. We can then get the 
optimal dependency tree with the score 
E[0,n,1,1] . 
3.3 Maximum Spanning Tree Algorithm  
As the bottom-up Eisner Algorithm must main-
tain the nested structural constraint, it cannot 
parse the non-projective dependency trees like 8? 
and 9? in Figure 2. However, the non-projective 
dependency does exist in real discourse. For ex-
ample, the earlier text mainly talks about the top-
ic A with mentioning the topic B, while the latter 
text gives a supplementary explanation for the 
topic B. This example can constitute a non-
projective tree and its pictorial diagram is exhib-
ited in Figure 4. Following the work of McDon-
ald (2005b), we formalize discourse dependency 
parsing as searching for a maximum spanning 
tree (MST) in a directed graph. 
... ...
A A AB B
...
 
Figure 4: Pictorial Diagram of Non-projective 
Trees 
Chu and Liu (1965) and Edmonds (1967) in-
dependently proposed the virtually identical al-
gorithm named the Chu-Liu/Edmonds algorithm, 
for finding MSTs on directed graphs (McDonald 
et al 2005b). Figure 5 shows the details of the 
Chu-Liu/Edmonds algorithm for discourse pars-
ing. Each node in the graph greedily selects the 
incoming arc with the highest score. If one tree 
results, the algorithm ends. Otherwise, there 
must exist a cycle. The algorithm contracts the 
identified cycle into a single node and recalcu-
lates the scores of the arcs which go in and out of 
the cycle. Next, the algorithm recursively call 
itself on the contracted graph. Finally, those arcs 
which go in or out of one cycle will recover 
themselves to connect with the original nodes in 
V. Like McDonald et al (2005b), we adopt an 
efficient implementation of the Chu-
Liu/Edmonds algorithm that is proposed by Tar-
jan (1997) with O(n2) time complexity. 
 
28
Chu-Liu-Edmonds(G, ?) 
Input: Text T=e0 e1? en; Arc scores ?(ei,ej) 
1      A? = {<ei, ej>| ei = argmax ?(ei,ej); 1?j?|V|} 
2      G? = (V, A?) 
3      If G? has no cycles, then return G?  
4      Find an arc set AC that is a cycle in G? 
5      <GC, ep> = contract(G, AC, ?) 
6      G = (V, A)=Chu-Liu-Edmonds(GC, ?) 
7      For the arc <ei,eC> where ep(ei,eC)=ej: 
8              A=A?AC?{<ei,ej)}-{<ei,eC>, <a(ej),ej>} 
9      For the arc <eC, ei> where ep(eC ,ei)=ej:  
10            A=A?{<ej,ei>}-{<eC,ei>} 
11    V = V 
12    Return G 
Contract(G=(V,A), AC, ?) 
1   Let GC be the subgraph of G excluding nodes in C 
2   Add a node eC to GC denoting the cycle C 
3   For ej ?V-C : ?ei?C <ei,ej>?A 
4        Add arc <eC,ej> to GC with  
ep(eC,ej)=          ?(ei,ej) 
5        ?(eC,ej) = ?(ep(eC,ej),ej) 
6    For ei ?V-C: ?ej?C   (ei,ej)?A 
7         Add arc <ei,eC> to GC with 
                  ep(ei,eC)= =           [?(ei,ej)-?(a(ei),ej)] 
8         ?(ei,eC) =?(ei,ej)-?(a(ei),ej)+score(C) 
9   Return <GC, ep> 
Figure 5: Chu-Liu/Edmonds MST Algorithm 
4 Learning 
In Section 3, we assume that the arc scores are 
available. In fact, the score of each arc is calcu-
lated as a linear combination of feature weights. 
Thus, we need to determine the features for arc 
representation first. With referring to McDonald 
et al (2005a; 2005b), we use the Margin Infused 
Relaxed Algorithm (MIRA) to learn the feature 
weights based on a training set of documents 
annotated with dependency structures ? ?? ? 1, Ni iT ?iy  
where yi denotes the correct dependency tree for 
the text Ti. 
4.1 Features 
Following (Feng and Hirst, 2012; Lin et al, 2009; 
Hernault et al, 2010b), we explore the following 
6 feature types combined with relations to repre-
sent each labeled arc <ei, r, ej> . 
(1) WORD: The first one word, the last one 
word, and the first bigrams in each EDU, the pair 
of the two first words and the pair of the two last 
words in the two EDUs are extracted as features. 
(2) POS: The first one and two POS tags in each 
EDU, and the pair of the two first POS tags in 
the two EDUs are extracted as features. 
(3) Position: These features concern whether the 
two EDUs are included in the same sentence, and 
the positions where the two EDUs are located in 
one sentence, one paragraph, or one document. 
(4) Length: The length of each EDU.  
(5) Syntactic:  POS tags of the dominating nodes 
as defined in Soricut and Marcu (2003) are ex-
tracted as features. We use the syntactic trees 
from the Penn Treebank to find the dominating 
nodes,. 
(6) Semantic similarity: We compute the se-
mantic relatedness between the two EDUs based 
on WordNet. The word pairs are extracted from 
(ei, ej) and their similarity is calculated. Then, we 
can get a weighted complete bipartite graph 
where words are deemed as nodes and similarity 
as weights. From this bipartite graph, we get the 
maximum weighted matching and use the aver-
aged weight of the matches as the similarity be-
tween ei and ej. In particular, we use 
path_similarity, wup_similarity, res_similarity, 
jcn_similarity and lin_similarity provided by the 
nltk.wordnet.similarity (Bird et. al., 2009) pack-
age for calculating word similarity. 
As for relations, we experiment two sets of 
relation labels from RST-DT. One is composed 
of 19 coarse-grained relations and the other 111 
fine-grained relations6.  
4.2 MIRA based Learning 
Margin Infused Relaxed Algorithm (MIRA) is an 
online algorithm for multiclass classification and 
is extended by Taskar et al (2003) to cope with 
structured classification.  
 
MIRA   Input: a training set ? ?? ? 1, Ni iT ?iy  
1      w0 = 0; v = 0; j = 0  
2      For iter := 1 to K 
3            For i := 1 to N 
4                   update w according to ? ?,iT iy : 
1min j j? ?w w
 
                  s.t.  ( , ) ( , ') ( , ')
where  ' ( , )
i i i i i i
j
i i
s T s T L
DT T
? ?
?
y y y y
y w
 
5                      v = v + wj ; 
6                      j = j+1   
7       w = v/(K*N) 
Figure 6: MIRA based Learning 
Figure 6 gives the pseudo-code of the MIRA 
algorithm (McDonld et al, 2005b). This algo-
rithm is designed to update the parameters w us-
ing a single training instance ? ?,iT iy  in each 
iteration. On each update, MIRA attempts to 
keep the norm of the change to the weight vector 
                                                          
6 19 relations include the original 18 relation in RST-DT 
plus one artificial ROOT relation. The 111 relations also 
include the ROOT relation. 
29
as small as possible, which is subject to con-
structing the correct dependency tree under con-
sideration with a margin at least as large as the 
loss of the incorrect dependency trees. We define 
the loss of a discourse dependency tree 'iy  (de-
noted by ( , ')i iL y y  ) as the number of the EDUs 
that have incorrect heads. Since there are expo-
nentially many possible incorrect dependency 
trees and thus exponentially many margin con-
straints, here we relax the optimization and stay 
with a single best dependency tree 
' ( , )ji iDT T?y w  which is parsed under the weight 
vector wj. In this algorithm, the successive up-
dated values of w are accumulated and averaged 
to avoid overfitting.  
5 Experiments 
5.1 Preparation 
We test our methods experimentally using the 
discourse dependency treebank which is built as 
in Section 2. The training part of the corpus is 
composed of 342 documents and contains 18,765 
EDUs, while the test part consists of 38 docu-
ments and 2,346 EDUs. The number of EDUs in 
each document ranges between 2 and 304. Two 
sets of relations are adopted. One is composed of 
19 relations and Table 1 shows the number of 
each relation in the training and test corpus. The 
other is composed of 111 relations. Due to space 
limitation, Table 2 only lists the 10 highest-
distributed relations with regard to their frequen-
cy in the training corpus.  
The following experiments are conducted: (1) 
to measure the parsing performance with differ-
ent relation sets and different feature types; (2) to 
compare our parsing methods with the state-of-
the-art discourse parsing methods.  
 
Relations Train Test Relations Train Test 
Elaboration 6879 796 Temporal 426 73 
Attribution 2641 343 ROOT 342 38 
Joint 1711 212 Compari. 273 29 
Same-unit 1230 127 Condition 258 48 
Contrast 944 146 Manner. 191 27 
Explanation 849 110 Summary 188 32 
Background 786 111 Topic-Cha. 187 13 
Cause 785 82 Textual 147 9 
Evaluation 502 80 TopicCom. 126 24 
Enablement 500 46 Total 18765 2346 
Table 1: Coarse-grained Relation Distribution 
 
 
Relations Train Test 
Elaboration-additional 2912 312 
Attribution 2474 329 
Elaboration-object-attribute-e 2274 250 
List 1690 206 
Same-unit 1230 127 
Elaboration-additional-e 747 69 
Circumstance 545 80 
Explanation-argumentative 524 70 
Purpose 430 43 
Contrast 358 64 
Table 2: 10 Highest Distributed Fine-grained 
Relations 
5.2 Feature Influence on Two Relation Sets 
So far, researches on discourse parsing avoid 
adopting too fine-grained relations and the rela-
tion sets containing around 20 labels are widely 
used. In our experiments, we observe that adopt-
ing a fine-grained relation set can even be helpful 
to building the discourse trees. Here, we conduct 
experiments on two relation sets that contain 19 
and 111 labels respectively. At the same time, 
different feature types are tested their effects on 
discourse parsing.  
Method Features Unlabeled 
Acc. 
Labeled 
Acc. 
Eisner 1+2 0.3602 0.2651 
1+2+3 0.7310 0.4855 
1+2+3+4 0.7370 0.4868 
1+2+3+4+5 0.7447 0.4957 
1+2+3+4+5+6 0.7455 0.4983 
MST 1+2 0.1957 0.1479 
1+2+3 0.7246 0.4783 
1+2+3+4 0.7280 0.4795 
1+2+3+4+5 0.7340 0.4915 
1+2+3+4+5+6 0.7331 0.4851 
Table 3: Performance Using Coarse-grained Re-
lations. 
Method Feature types Unlabeled 
Acc. 
Labeled 
Acc. 
Eisner 1+2 0.3743 0.2421 
1+2+3 0.7451 0.4079 
1+2+3+4 0.7472 0.4041 
1+2+3+4+5 0.7506 0.4254 
1+2+3+4+5+6 0.7485 0.4288 
MST 1+2 0.2080 0.1300 
1+2+3 0.7366 0.4054 
1+2+3+4 0.7468 0.4071 
1+2+3+4+5 0.7494 0.4288 
1+2+3+4+5+6 0.7460 0.4309 
Table 4: Performance Using Fine-grained Rela-
tions. 
Based on the MIRA leaning algorithm, the 
Eisner algorithm and MST algorithm are used to 
parse the test documents respectively. Referring 
to the evaluation of syntactic dependency parsing, 
30
we use unlabeled accuracy to calculate the ratio 
of EDUs that correctly identify their heads, la-
beled accuracy the ratio of EDUs that have both 
correct heads and correct relations. Table 3 and 
Table 4 show the performance on two relation 
sets. The numbers (1-6) represent the corre-
sponding feature types described in Section 4.1.  
From Table 3 and Table 4, we can see that the 
addition of more feature types, except the 6th fea-
ture type (semantic similarity), can promote the 
performance of relation labeling, whether using 
the coarse-grained 19 relations and the fine-
grained 111 relations. As expected, the first and 
second types of features (WORD and POS) are 
the ones which play an important role in building 
and labeling the discourse dependency trees. 
These two types of features attain similar per-
formance on two relation sets. The Eisner algo-
rithm can achieve unlabeled accuracy around 
0.36 and labeled accuracy around 0.26, while 
MST algorithm achieves unlabeled accuracy 
around 0.20 and labeled accuracy around 0.14. 
The third feature type (Position) is also very 
helpful to discourse parsing. With the addition of 
this feature type, both unlabeled accuracy and 
labeled accuracy exhibit a marked increase. Es-
pecially, when applying MST algorithm on dis-
course parsing, unlabeled accuracy rises from 
around 0.20 to around 0.73. This result is con-
sistent with Hernault?s work (2010b) whose ex-
periments have exhibited the usefulness of those 
position-related features. The other two types of 
features which are related to length and syntactic 
parsing, only promote the performance slightly.  
As we employed the MIRA learning algorithm, 
it is possible to identify which specific features 
are useful, by looking at the weights learned to 
each feature using the training data. Table 5 se-
lects 10 features with the highest weights in ab-
solute value for the parser which uses the coarse-
grained relations, while Table 6 selects the top 
10 features for the parser using the fine-grained 
relations. Each row denotes one feature: the left 
part before the symbol ?&? is from one of the 6 
feature types and the right part denotes a specific 
relation. From Table 5 and Table 6, we can see 
that some features are reasonable. For example, 
The sixth feature in Table 5 represents that the 
dependency relation is preferred to be labeled 
Explanation with the fact that ?because? is the 
first word of the dependent EDU. From these 
two tables, we also observe that most of the 
heavily weighted features are usually related to 
those highly distributed relations. When using 
the coarse-grained relations, the popular relations 
(eg. Elaboration, Attribution and Joint) are al-
ways preferred to be labeled. When using the 
fine-grained relations, the large relations includ-
ing List and Elaboration-object-attribute-e are 
given the precedence of labeling. This phenome-
non is mainly caused by the sparseness of the 
training corpus and the imbalance of relations. 
To solve this problem, the augment of training 
corpus is necessary. 
 
 Feature description Weight 
1 
Last two words in dependent EDU are  
?appeals court?  & Joint 
0.475 
2 
First word in dependent EDU is ?racked? 
& Elaboration 
0.445 
3 
First two words in head EDU are ?I ?d? 
& Attribution 
0.324 
4 
Last word in dependent EDU is ?in?  
& Elaboration 
-0.323 
5 
The res_similarity between two EDUs is 0  
& Elaboration 
0.322 
6 
First word in dependent EDU is ?because? 
& Explanation 
0.306 
7 First POS in head EDU is ?DT? & Joint -0.299 
8 
First two words in dependent EDU are ?that 
required? & Elaboration 
0.287 
9 
First two words in dependent EDU are ?that 
the? & Elaboration 
0.277 
10 
First word in dependent EDU is ?because? 
& Cause 
0.265 
Table 5: Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 
 Features Weight 
1 Last two words in dependent EDU are ?ap-
peals court?  & List 
0.576 
2 First two words in head EDU are ?I ?d?  
& Attribution 
0.385 
3 First two words in dependent EDU is ?that 
the? & Elaboration-object-attribute-e 
0.348 
4 First POS in head EDU is ?DT? & List -0.323 
5 Last word in dependent EDU is ?in? & List -0.286 
6 First word in dependent EDU is ?racked? & 
Elaboration-object-attribute-e 
0.445 
7 First two word pairs are <?In an?,?But 
even?>  & List 
-0.252 
8 Dependent EDU has a dominating node 
tagged ?CD?& Elaboration-object-attribute-e 
-0.244 
9 First two words in dependent EDU are ?pa-
tents disputes? & Purpose 
0.231 
10 First word in dependent EDU is ?to?  
& Purpose 
0.230 
Table 6: Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 
Unlike previous discourse parsing approaches, 
our methods combine tree building and relation 
labeling into a uniform framework naturally. 
This means that relations play a role in building 
the dependency tree structure. From Table 3 and 
Table 4, we can see that fine-grained relations 
are more helpful to building unlabeled discourse 
31
trees more than the coarse-grained relations. The 
best result of unlabeled accuracy using 111 rela-
tions is 0.7506, better than the best performance 
(0.7447) using 19 relations. We can also see that 
the labeled accuracy using the fine-grained rela-
tions can achieve 0.4309, only 0.06 lower than 
the best labeled accuracy (0.4915) using the 
coarse-grained relations. 
In addition, comparing the MST algorithm 
with the Eisner algorithm, Table 3 and Table 4 
show that their performances are not significant-
ly different from each other. But we think that 
MST algorithm has more potential in discourse 
dependency parsing, because our converted dis-
course dependency treebank contains only pro-
jective trees and somewhat suppresses the MST 
algorithm to exhibit its advantage of parsing non-
projective trees. In fact, we observe that some 
non-projective dependencies produced by the 
MST algorithm are even reasonable than what 
they are in the dependency treebank. Thus, it is 
important to build a manually labeled discourse 
dependency treebank, which will be our future 
work. 
5.3 Comparison with Other Systems  
The state-of-the-art discourse parsing methods 
normally produce the constituency based dis-
course trees. To comprehensively evaluate the 
performance of a labeled constituency tree, the 
blank tree structure (?S?), the tree structure with 
nuclearity indication (?N?), and the tree structure 
with rhetorical relation indication but no nuclear-
ity indication (?R?) are evaluated respectively 
using the F measure (Marcu 2000).  
To compare our discourse parsers with others, 
we adopt MIRA and Eisner algorithm to conduct 
discourse parsing with all the 6 types of features 
and then convert the produced projective de-
pendency trees to constituency based trees 
through their correspondence as stated in Section 
2. Our parsers using two relation sets are named 
Our-coarse and Our-fine respectively. The in-
putted EDUs of our parsers are from the standard 
segmentation of RST-DT. Other text-level dis-
course parsing methods include: (1) Percep-
coarse: we replace MIRA with the averaged per-
ceptron learning algorithm and the other settings 
are the same with Our-coarse; (2) HILDA-
manual and HILDA-seg are from Hernault 
(2010b)?s work, and their inputted EDUs are 
from RST-DT and their own EDU segmenter 
respectively; (3) LeThanh indicates the results 
given by LeThanh el al. (2004), which built a 
multi-level rule based parser and used 14 rela-
tions evaluated on 21 documents from RST-DT; 
(4) Marcu denotes the results given by Mar-
cu(2000)?s decision-tree based parser which used 
15 relations evaluated on unspecified documents.  
Table 7 shows the performance comparison 
for all the parsers mentioned above. Human de-
notes the manual agreement between two human 
annotators. From this table, we can see that both 
our parsers perform better than all the other 
parsers as a whole, though our parsers are not 
developed directly for constituency based trees. 
Our parsers do not exhibit obvious advantage 
than HILDA-manual on labeling the blank tree 
structure, because our parsers and HILDA-
manual all perform over 94% of Human and this 
performance level somewhat reaches a bottle-
neck to promote more. However, our parsers 
outperform the other parsers on both nuclearity 
and relation labeling. Our-coarse achieves 94.2% 
and 91.8% of the human F-scores, on labeling 
nuclearity and relation respectively, while Our-
fine achieves 95.2% and 87.6%. We can also see 
that the averaged perceptron learning algorithm, 
though simple, can achieve a comparable per-
formance, better than HILDA-manual. The 
parsers HILDA-seg, LeThanh and Marcu use 
their own automatic EDU segmenters and exhibit 
a relatively low performance. This means that 
EDU segmentation is important to a practical 
discourse parser and worth further investigation. 
  
 S N R 
Our-coarse 82.9 73.0 60.6 
Our-fine 83.4 73.8 57.8 
Percep-coarse 82.3 72.6 59.4 
HILDA-manual 83.0 68.4 55.3 
HILDA-seg 72.3 59.1 47.8 
LeThanh 53.7 47.1 39.9 
Marcu 44.8 30.9 18.8 
Human 88.1 77.5 66.0 
Table 7: Full Parser Evaluation 
 MAFS WAFS Acc 
Our-coarse 0.454 0.643 66.84 
Percep-coarse 0.438 0.633 65.37 
Feng 0.440 0.607 65.30 
HILDA-manual 0.428 0.604 64.18 
Baseline - - 35.82 
Table 8: Relation Labeling Performance  
To further compare the performance of rela-
tion labeling, we follow Hernault el al. (2010a) 
and use Macro-averaged F-score (MAFS) to 
evaluate each relation. Due to space limitation, 
we do not list the F scores for each relation. 
Macro-averaged F-score is not influenced by the 
number of instances that are contained in each 
32
relation. Weight-averaged F-score (WAFS) 
weights the performance of each relation by the 
number of its existing instances. Table 8 com-
pares our parser Our-coarse with other parsers 
HILDA-manual, Feng (Feng and Hirst, 2012) 
and Baseline. Feng (Feng and Hirst, 2012) can 
be seen as a strengthened version of HILDA 
which adopts more features and conducts feature 
selection. Baseline always picks the most fre-
quent relation (i.e. Elaboration). From the results, 
we find that Our-coarse consistently provides 
superior performance for most relations over 
other parsers, and therefore results in higher 
MAFS and WAFS.  
6 Related Work 
So far, the existing discourse parsing techniques 
are mainly based on two well-known treebanks. 
One is the Penn Discourse TreeBank (PDTB) 
(Prasad et al, 2007) and the other is RST-DT.  
PDTB adopts the predicate-arguments repre-
sentation by taking an implicit/explicit connec-
tive as a predication of two adjacent sentences 
(arguments). Then the discourse relation between 
each pair of sentences is annotated independently 
to characterize its predication. A majority of re-
searches regard discourse parsing as a classifica-
tion task and mainly focus on exploiting various 
linguistic features and classifiers when using 
PDTB (Wellner et al, 2006; Pitler et al, 2009; 
Wang et al, 2010). However, the predicate-
arguments annotation scheme itself has such a 
limitation that one can only obtain the local dis-
course relations without knowing the rich context. 
In contrast, RST and its treebank enable peo-
ple to derive a complete representation of the 
whole discourse. Researches have begun to in-
vestigate how to construct a RST tree for the 
given text. Since the RST tree is similar to the 
constituency based syntactic tree except that the 
constituent nodes are different, the syntactic 
parsing techniques have been borrowed for dis-
course parsing (Soricut and Marcu, 2003; 
Baldridge and Lascarides, 2005; Sagae, 2009; 
Hernault et al, 2010b; Feng and Hirst, 2012). 
Soricut and Marcu (2003) use a standard bottom-
up chart parsing algorithm to determine the dis-
course structure of sentences. Baldridge and Las-
carides (2005) model the process of discourse 
parsing with the probabilistic head driven parsing 
techniques. Sagae (2009) apply a transition based 
constituent parsing approach to construct a RST 
tree for a document. Hernault et al (2010b) de-
velop a greedy bottom-up tree building strategy 
for discourse parsing. The two adjacent text 
spans with the closest relations are combined in 
each iteration. As the extension of Hernault?s 
work, Feng and Hirst (2012) further explore var-
ious features aiming to achieve better perfor-
mance. However, as analyzed in Section 1, there 
exist three limitations with the constituency 
based discourse representation and parsing. We 
innovatively adopt the dependency structure, 
which can be benefited from the existing RST-
DT, to represent the discourse. To the best of our 
knowledge, this work is the first to apply de-
pendency structure and dependency parsing 
techniques in discourse analysis. 
7 Conclusions 
In this paper, we present the benefits and feasi-
bility of applying dependency structure in text-
level discourse parsing. Through the correspond-
ence between constituency-based trees and de-
pendency trees, we build a discourse dependency 
treebank by converting the existing RST-DT. 
Based on dependency structure, we are able to 
directly analyze the relations between the EDUs 
without worrying about the additional interior 
text spans, and apply the existing state-of-the-art 
dependency parsing techniques which have a 
relatively low time complexity. In our work, we 
use the graph based dependency parsing tech-
niques learned from the annotated dependency 
trees. The Eisner algorithm and the MST algo-
rithm are applied to parse the optimal projective 
and non-projective dependency trees respectively 
based on the arc-factored model. To calculate the 
score for each arc, six types of features are ex-
plored to represent the arcs and the feature 
weights are learned based on the MIRA learning 
technique. Experimental results exhibit the effec-
tiveness of the proposed approaches. In the fu-
ture, we will focus on non-projective discourse 
dependency parsing and explore more effective 
features. 
Acknowledgments 
This work was partially supported by National 
High Technology Research and Development 
Program of China (No. 2012AA011101), Na-
tional Key Basic Research Program of China (No. 
2014CB340504), National Natural Science 
Foundation of China (No. 61273278), and Na-
tional Key Technology R&D Program (No: 
2011BAH10B04-03). We also thank the three 
anonymous reviewers for their helpful comments. 
33
References 
Jason Baldridge and Alex Lascarides. 2005. Probabil-
istic Head-driven Parsing for Discourse Structure. 
In Proceedings of the Ninth Conference on Com-
putational Natural Language Learning, pages 96?
103. 
Steven Bird, Ewan Klein, and Edward Loper. 2009. 
Natural Language Processing with Python ? Ana-
lyzing Text with the Natural Language Toolkit. 
O?Reilly. 
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 
2001. Building a Discourse-tagged Corpus in the 
Framework of Rhetorical Structure Theory. Pro-
ceedings of the Second SIGdial Workshop on Dis-
course and Dialogue-Volume 16, pages 1?10. 
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the 
Shortest Arborescence of a Directed Graph, Sci-
ence Sinica, v.14, pp.1396-1400.  
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative Online Algorithms for Multiclass Prob-
lems. JMLR. 
Jack Edmonds. 1967. Optimum Branchings, J. Re-
search of the National Bureau of Standards, 71B, 
pp.233-240.  
Jason Eisner. 1996. Three New Probabilistic Models 
for Dependency Parsing: An Exploration. In Proc. 
COLING. 
Vanessa Wei Feng and Graeme Hirst. Text-level Dis-
course Parsing with Rich Linguistic Features, Pro-
ceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, pages 
60?68, Jeju, Republic of Korea, 8-14 July 2012. 
Hugo Hernault, Danushka Bollegala, and Mitsuru 
Ishizuka. 2010a. A Semi-supervised Approach to 
Improve Classification of Infrequent Discourse Re-
lations Using Feature Vector Extension. In Pro-
ceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
399?409, Cambridge, MA, October. Association 
for Computational Linguistics. 
Hugo Hernault, Helmut Prendinger, David A. duVerle, 
and Mitsuru Ishizuka. 2010b. HILDA: A Discourse 
Parser Using Support Vector Machine Classifica-
tion. Dialogue and Discourse, 1(3):1?33. 
Shafiq Joty, Giuseppe Carenini and Raymond T. Ng. 
A Novel Discriminative Framework for Sentence-
level Discourse Analysis. EMNLP-CoNLL '12 
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and Computational Natural Language Learning 
Stroudsburg, PA, USA. 
Huong LeThanh, Geetha Abeysinghe, and Christian 
Huyck. 2004. Generating Discourse Structures for 
Written Texts. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics, 
pages 329? 335. 
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. 
Recognizing Implicit Discourse Relations in the 
Penn Discourse Treebank. In Proceedings of the 
2009 Conference on Empirical Method in Natural 
Language Processing, Vol. 1, EMNLP?09, pages 
343-351. 
William Mann and Sandra Thompson. 1988. Rhetori-
cal Structure Theory: Toward a Functional Theory 
of Text Organization. Text, 8(3):243?281. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press, 
Cambridge, MA, USA. 
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005a. Online Large-Margin Training of De-
pendency Parsers, 43rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
2005) .  
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and 
Jan Hajic. 2005b. Non-projective Dependency 
Parsing using Spanning Tree Algorithms, Proceed-
ings of HLT/EMNLP 2005. 
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. 
Automatic Sense Prediction for Implicit Discourse 
Relations in Text, In Proc. of the 47th ACL. pages 
683-691.  
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan 
Lee, Aravind Joshi, Livio Robaldo, and Bonnie 
Webber. 2007. The Penn Discourse Treebank 2.0 
Annotation Manual. The PDTB Research Group, 
December. 
David Reitter. 2003. Simple Signals for Complex 
Rhetorics: On Rhetorical Analysis with Rich-
feature Support Vector Models. LDV Forum, 
18(1/2):38?52. 
Kenji Sagae. 2009. Analysis of discourse structure 
with syntactic dependencies and data-driven shift-
reduce parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages 
81-84. 
Radu Soricut and Daniel Marcu. 2003. Sentence level 
discourse parsing using syntactic and lexical in-
formation. In Proceedings of the 2003 Conference 
34
of the North American Chapter of the Association 
for Computational Linguistics on Human Lan-
guage Technology, Volume 1, pages 149?156. 
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic in-
formation. In Proceedings of Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 566?574. 
Robert Endre Tarjan, 1977. Finding Optimum 
Branchings, Networks, v.7, pp.25-35. 
Ben Taskar, Carlos Guestrin and Daphne Koller. 2003. 
Max-margin Markov Networks. In Proc. NIPS. 
Bonnie Webber. 2004. D-LTAG: Extending Lexical-
ized TAG to Discourse. Cognitive Science, 
28(5):751?779. 
Wen Ting Wang, Jian Su and Chew Lim Tan. 2010. 
Kernel based Discourse Relation Recognition with 
Temporal Ordering Information, In Proc. of 
ACL?10. pages 710-719. 
Ben Wellner, James Pustejovsky, Catherine Havasi, 
Anna Rumshisky and Roser Sauri. 2006. Classifi-
cation of Discourse Coherence Relations: an Ex-
ploratory Study Using Multiple Knowledge 
Sources. In Proc.of the 7th SIGDIAL Workshop on 
Discourse and Dialogue. pages 117-125. 
 
35
Transactions of the Association for Computational Linguistics, 1 (2013) 89?98. Action Editor: Noah Smith.
Submitted 12/2012; Published 5/2013. c?2013 Association for Computational Linguistics.
A Novel Feature-based Bayesian Model for Query Focused Multi-document
Summarization
Jiwei Li
School of Computer Science
Carnegie Mellon University
bdlijiwei@gmail.com
Sujian Li
Laboratory of Computational Linguistics
Peking University
lisujian@pku.edu.cn
Abstract
Supervised learning methods and LDA based topic
model have been successfully applied in the field of
multi-document summarization. In this paper, we
propose a novel supervised approach that can in-
corporate rich sentence features into Bayesian topic
models in a principled way, thus taking advantages of
both topic model and feature based supervised learn-
ing methods. Experimental results on DUC2007,
TAC2008 and TAC2009 demonstrate the effective-
ness of our approach.
1 Introduction
Query-focused multi-document summarization
(Nenkova et al, 2006; Wan et al, 2007; Ouyang et
al., 2010) can facilitate users to grasp the main idea
of documents. In query-focused summarization, a
specific topic description, such as a query, which
expresses the most important topic information is
proposed before the document collection, and a
summary would be generated according to the given
topic.
Supervised models have been widely used in sum-
marization (Li, et al, 2009, Shen et al, 2007,
Ouyang et al, 2010). Supervised models usually re-
gard summarization as a classification or regression
problem and use various sentence features to build a
classifier based on labeled negative or positive sam-
ples. However, existing supervised approaches sel-
dom exploit the intrinsic structure among sentences.
This disadvantage usually gives rise to serious prob-
lems such as unbalance and low recall in summaries.
Recently, LDA-based (Blei et al, 2003) Bayesian
topic models have widely been applied in multi-
document summarization in that Bayesian ap-
proaches can offer clear and rigorous probabilis-
tic interpretations for summaries(Daume and Marcu,
2006; Haghighi and Vanderwende, 2009; Jin et al,
2010; Mason and Charniak, 2011; Delort and Alfon-
seca, 2012). Exiting Bayesian approaches label sen-
tences or words with topics and sentences which are
closely related with query or can highly generalize
documents are selected into summaries. However,
LDA topic model suffers from the intrinsic disad-
vantages that it only uses word frequency for topic
modeling and can not use useful text features such as
position, word order etc (Zhu and Xing, 2010). For
example, the first sentence in a document may be
more important for summary since it is more likely
to give a global generalization about the document.
It is hard for LDA model to consider such informa-
tion, making useful information lost.
It naturally comes to our minds that we can im-
prove summarization performance by making full
use of both useful text features and the latent seman-
tic structures from by LDA topic model. One related
work is from Celikyilmaz and Hakkani-Tur (2010).
They built a hierarchical topic model called Hybh-
sum based on LDA for topic discovery and assumed
this model can produce appropriate scores for sen-
tence evaluation. Then the scores are used for tun-
ing the weights of various features that helpful for
summary generation. Their work made a good step
of combining topic model with feature based super-
vised learning. However, what their approach con-
fuses us is that whether a topic model only based
on word frequency is good enough to generate an
appropriate sentence score for regression. Actually,
how to incorporate features into LDA topic model
has been a open problem. Supervised topic models
such as sLDA(Blei and MacAuliffe 2007) give us
some inspiration. In sLDA, each document is asso-
ciated with a labeled feature and sLDA can integrate
such feature into LDA for topic modeling in a prin-
89
cipled way.
With reference to the work of supervised LDA
models, in this paper, we propose a novel sentence
feature based Bayesian model S-sLDA for multi-
document summarization. Our approach can natu-
rally combine feature based supervised methods and
topic models. The most important and challeng-
ing problem in our model is the tuning of feature
weights. To solve this problem, we transform the
problem of finding optimum feature weights into an
optimization algorithm and learn these weights in
a supervised way. A set of experiments are con-
ducted based on the benchmark data of DUC2007,
TAC2008 and TAC2009, and experimental results
show the effectiveness of our model.
The rest of the paper is organized as follows. Sec-
tion 2 describes some background and related works.
Section 3 describes our details of S-sLDA model.
Section 4 demonstrates details of our approaches,
including learning, inference and summary gener-
ation. Section 5 provides experiments results and
Section 6 concludes the paper.
2 Related Work
A variety of approaches have been proposed
for query-focused multi-document summarizations
such as unsupervised (semi-supervised) approaches,
supervised approaches, and Bayesian approaches.
Unsupervised (semi-supervised) approaches such
as Lexrank (Erkan and Radex, 2004), manifold
(Wan et al, 2007) treat summarization as a graph-
based ranking problem. The relatedness between
the query and each sentence is achieved by impos-
ing querys influence on each sentence along with
the propagation of graph. Most supervised ap-
proaches regard summarization task as a sentence
level two class classification problem. Supervised
machine learning methods such as Support Vector
Machine(SVM) (Li, et al, 2009), Maximum En-
tropy (Osborne, 2002) , Conditional Random Field
(Shen et al, 2007) and regression models (Ouyang
et al, 2010) have been adopted to leverage the rich
sentence features for summarization.
Recently, Bayesian topic models have shown their
power in summarization for its clear probabilistic
interpretation. Daume and Marcu (2006) proposed
Bayesum model for sentence extraction based on
query expansion concept in information retrieval.
Haghighi and Vanderwende (2009) proposed topic-
sum and hiersum which use a LDA-like topic model
and assign each sentence a distribution over back-
ground topic, doc-specific topic and content topics.
Celikyilmaz and Hakkani-Tur (2010) made a good
step in combining topic model with supervised fea-
ture based regression for sentence scoring in sum-
marization. In their model, the score of training
sentences are firstly got through a novel hierarchi-
cal topic model. Then a featured based support vec-
tor regression (SVR) is used for sentence score pre-
diction. The problem of Celikyilmaz and Hakkani-
Turs model is that topic model and feature based re-
gression are two separate processes and the score of
training sentences may be biased because their topic
model only consider word frequency and fail to con-
sider other important features. Supervised feature
based topic models have been proposed in recent
years to incorporate different kinds of features into
LDA model. Blei (2007) proposed sLDA for doc-
ument response pairs and Daniel et al (2009) pro-
posed Labeled LDA by defining a one to one corre-
spondence between latent topic and user tags. Zhu
and Xing (2010) proposed conditional topic random
field (CTRF) which addresses feature and indepen-
dent limitation in LDA.
3 Model description
3.1 LDA and sLDA
The hierarchical Bayesian LDA (Blei et al, 2003)
models the probability of a corpus on hidden topics
as shown in Figure 1(a). Let K be the number of
topics , M be the number of documents in the cor-
pus and V be vocabulary size. The topic distribution
of each document ?m is drawn from a prior Dirichlet
distribution Dir(?), and each document word wmn
is sampled from a topic-word distribution ?z spec-
ified by a drawn from the topic-document distribu-
tion ?m. ? is a K ?M dimensional matrix and each
?k is a distribution over the V terms. The generat-
ing procedure of LDA is illustrated in Figure 2. ?m
is a mixture proportion over topics of document m
and zmn is a K dimensional variable that presents
the topic assignment distribution of different words.
Supervised LDA (sLDA) (Blei and McAuliffe
2007) is a document feature based model and intro-
90
Figure 1: Graphical models for (a) LDA model and (b)
sLDA model.
1. Draw a document proportion vector ?m|? ? Dir(?)
2. For each word in m
(a)draw topic assignment zmn|? ?Multi(?zmn)
(b)draw word wmn|zmn, ? ?Multi(?zmn)
Figure 2: Generation process for LDA
duces a response variable to each document for topic
discovering, as shown in Figure 1(b). In the gener-
ative procedure of sLDA, the document pairwise la-
bel is draw from y|??zm, ?, ?2 ? p(y|??zm, ?, ?2), where??zm = 1N
?N
n=1 zm,n.
3.2 Problem Formulation
Here we firstly give a standard formulation of the
task. Let K be the number of topics, V be the vo-
cabulary size and M be the number of documents.
Each document Dm is represented with a collection
of sentence Dm = {Ss}s=Nms=1 where Nm denotes
the number of sentences in mth document. Each
sentence is represented with a collection of words
{wmsn}n=Nmsn=1 where Nms denotes the number of
words in current sentence. ???Yms denotes the feature
vector of current sentence and we assume that these
features are independent.
3.3 S-sLDA
zms is the hidden variable indicating the topic of
current sentence. In S-sLDA, we make an assump-
tion that words in the same sentence are generated
from the same topic which was proposed by Gruber
(2007). zmsn denotes the topic assignment of cur-
rent word. According to our assumption, zmsn =
Figure 3: Graph model for S-sLDA model
1. Draw a document proportion vector ?m|? ? Dir(?)
2. For each sentence in m
(a)draw topic assignment zms|? ?Multi(?zmn)
(b)draw feature vector ???Yms|zms, ? ? p(???Yms|zms, ?)
(c)for each word wmsn in current sentence
draw wmsn|zms, ? ?Multi(?zms)
Figure 4: generation process for S-sLDA
zms for any n ? [1, Nms]. The generative approach
of S-sLDA is shown in Figure 3 and Figure 4. We
can see that the generative process involves not only
the words within current sentence, but also a series
of sentence features. The mixture weights over fea-
tures in S-sLDA are defined with a generalized lin-
ear model (GLM).
p(???Yms|zms, ?) =
exp(zTms?)
???Yms?
zms exp(zTms?)
???Yms
(1)
Here we assume that each sentence has T features
and ???Yms is a T ? 1 dimensional vector. ? is a
K ? T weight matrix of each feature upon topics,
which largely controls the feature generation proce-
dure. Unlike s-LDA where ? is a latent variable esti-
mated from the maximum likelihood estimation al-
gorithm, in S-sLDA the value of ? is trained through
a supervised algorithm which will be illustrated in
detail in Section 3.
3.4 Posterior Inference and Estimation
Given a document and labels for each sentence, the
posterior distribution of the latent variables is:
p(?, z1:N |w1:N , Y, ?, ?1:K , ?) =
?
m p(?m|?)
?
s[p(zms|?m)p(
???Yms|zms, ?)
?
n p(wmsn|zmsn, ?zmsn ]?
d?p(?m|?)
?
z
?
s[p(zms|?m)p(
???Yms|zms, ?)
?
n p(wmsn|?zmsn)](2)
Eqn. (2) cannot be efficiently computed. By
applying the Jensens inequality, we obtain a
lower bound of the log likelihood of document
p(?, z1:N |w1:N ,
???Yms, ?, ?1:K , ?) ? L, where
L =
?
ms
E[logP (zms|?)] +
?
ms
E[logP (???Yms|zms, ?)]+
?
m
E[logP (?|?)] +
?
msn
E[logP (wmsn|zms, ?)] +H(q)
(3)
91
where H(q) = ?E[logq] and it is the entropy of
variational distribution q is defined as
q(?, z|?, ?) =
?
mk
q(?m|?)
?
sn
q(zmsn|?ms) (4)
here ? a K-dimensional Dirichlet parameter vector
and multinomial parameters. The first, third and
forth terms of Eqn. (3) are identical to the corre-
sponding terms for unsupervised LDA (Blei et al,
2003). The second term is the expectation of log
probability of features given the latent topic assign-
ments.
E[logP (???Yms|zms, ?)] =
E(zms)T ?
???Yms ? log
?
zms
exp(zTms?
???Yms)
(5)
where E(zms)T is a 1 ? K dimensional vector
[?msk]k=Kk=1 . The Bayes estimation for S-sLDA
model can be got via a variational EM algorithm. In
EM procedure, the lower bound is firstly minimized
with respect to ? and ?, and then minimized with ?
and ? by fixing ? and ?.
E-step:
The updating of Dirichlet parameter ? is identical
to that of unsupervised LDA, and does not involve
feature vector ???Yms.
?newm ? ?+
?
s?m
?s (6)
?newsk ? exp{E[log?m|?] +
Nms?
n=1
E[log(wmsn|?1:K)]+
T?
t=1
?ktYst} = exp[?(?mk)??(
K?
k=1
?mk) +
T?
t=1
?ktYst]
(7)
where ?(?) denotes the log ? function. ms denotes
the document that current sentence comes from and
Yst denotes the tth feature of sentence s.
M-step:
The M-step for updating ? is the same as the pro-
cedure in unsupervised LDA, where the probability
of a word generated from a topic is proportional to
the number of times this word assigned to the topic.
?newkw =
M?
m=1
Nm?
s=1
Nms?
n=1
1(wmsn = w)?kms (8)
4 Our Approach
4.1 Learning
In this subsection, we describe how we learn the fea-
ture weight ? in a supervised way. The learning pro-
cess of ? is a supervised algorithm combined with
variational inference of S-sLDA. Given a topic de-
scription Q1 and a collection of training sentences S
from related documents, human assessors assign a
score v(v = ?2,?1, 0, 1, 1) to each sentence in S.
The score is an integer between?2 (the least desired
summary sentences) and +2 (the most desired sum-
mary sentences), and score 0 denotes neutral atti-
tude. Ov = {ov1, ov2, ..., vvk}(v = ?2,?1, 0, 1, 2)
is the set containing sentences with score v. Let ?Qk
denote the probability that query is generated from
topic k. Since query does not belong to any docu-
ment, we use the following strategy to leverage ?Qk
?Qk =
?
w?Q
?kw?
1
M
M?
m=1
exp[?(?mk)??(
K?
k=1
?mk)]
(9)
In Equ.(9), ?w?Q ?kw denotes the probability that
all terms in query are generated from topic k
and 1M
?M
m=1 exp[?(?mk)??(
?K
k=1 ?mk)] can be
seen as the average probability that all documents in
the corpus are talking about topic k. Eqn. (9) is
based on the assumption that query topic is relevant
to the main topic discussed by the document corpus.
This is a reasonable assumption and most previous
LDA summarization models are based on similar as-
sumptions.
Next, we define ?Ov ,k for sentence set Ov, which
can be interpreted as the probability that all sen-
tences in collection Ov are generated from topic k.
?Ov,k =
1
|Ov|
?
s?Ov
?sk, k ? [1,K], v ? [?2, 2] (10)
|Ov| denotes the number of sentences in set Ov. In-
spired by the idea that desired summary sentences
would be more semantically related with the query,
we transform problem of finding optimum ? to the
following optimization problem:
min?L(?) =
v=2?
v=?2
v ?KL(Ov||Q);
T?
t=1
?kt = 1 (11)
1We select multiple queries and their related sentences for
training
92
where KL(Ov||Q) is the Kullback-Leibler diver-
gence between the topic and sentence set Ov as
shown in Eqn.(12).
KL(Ov||Q) =
K?
k=1
?Ovklog
?Ovk
?Qk
(12)
In Eqn. (11), we can see that O2, which contain de-
sirable sentences, would be given the largest penalty
for its KL divergence from Query. The case is just
opposite for undesired set.
Our idea is to incorporate the minimization pro-
cess of Eqn.(11) into variational inference process
of S-sLDA model. Here we perform gradient based
optimization method to minimize Eqn.(11). Firstly,
we derive the gradient of L(?) with respect to ?.
?L(?)
?xy
=
v=2?
v=?2
v ? ?KL(Qv||Q)??xy
(13)
?KL(Qv||Q)
??xy
=
K?
k=1
1
|Qv|
(1 + log
?
s?Qv
|Qv|
)
?
s?Qv
??sk
??xy
?
K?
k=1
1
|Qv|
?
s?Qv
?Qsk
?xy
?
K?
k=1
1
Qv
?
s?Qv?sk
?Qk
??sk
??xy
(14)
For simplification, we regard ? and ? as constant
during updating process of ?, so ??Qk??xy = 0.2 We canfurther get first derivative for each labeled sentence.
??sk
?xy
?
?
???????
???????
Ysyexp[?(?msi)??(
K?
k=1
?msk) +
T?
t=1
?ktYsy]
?
?
w?s
?kw if k = x
0 if k 6= x
(15)
4.2 Feature Space
Lots of features have been proven to be useful for
summarization (Louis et al, 2010). Here we dis-
cuss several types of features which are adopted in
S-sLDA model. The feature values are either binary
or normalized to the interval [0,1]. The following
features are used in S-sLDA:
Cosine Similarity with query: Cosine similarity is
based on the tf-idf value of terms.
2This is reasonable because the influence of ? and ? have
been embodied in ? during each iteration.
Local Inner-document Degree Order: Local Inner
document Degree Order is a binary feature which
indicates whether Inner-document Degree (IDD) of
sentence s is the largest among its neighbors. IDD
means the edge number between s and other sen-
tences in the same document.
Document Specific Word: 1 if a sentence contains
document specific word, 0 otherwise.
Average Unigram Probability (Nenkova and Van-
derwende, 2005; Celikyilmaz and Hakkani-Tur
2010): As for sentence s, p(s) = ?w?s 1|s|pD(w),
where pD(w) is the observed unigram probability in
document collection.
In addition, we also use the commonly used fea-
tures including sentence position, paragraph po-
sition, sentence length and sentence bigram fre-
quency.
E-step
initialize ?0sk := 1/K for all i and s.
initialize ?mi := ?mi +N)m/K for all i.
initialize ?kt = 0 for all k and t.
while not convergence
for m = 1 : M
update ?t+1m according to Eqn.(6)
for s = 1 : Nm
for k = 1 : K
update ?t+1sk according to Eqn.(7)
normalize the sum of ?t+1sk to 1.Minimize L(?) according to Eqn.(11)-(15).
M-step:
update ? according to Eqn.(8)
Figure 5: Learning process of ? in S-sLDA
4.3 Sentence Selection Strategy
Next we explain our sentence selection strategy. Ac-
cording to our intuition that the desired summary
should have a small KL divergence with query, we
propose a function to score a set of sentences Sum.
We use a decreasing logistic function ?(x) = 1/(1+
ex) to refine the score to the range of (0,1).
Score(Sum) = ?(KL(sum||Q)) (16)
Let Sum? denote the optimum update summary. We
can get Sum? by maximizing the scoring function.
Sum? = arg max
Sum?S&&words(Sum)?L
Score(Sum)
(17)
93
1. Learning: Given labeled set Ov, learn the feature
weight vector ? using algorithm in Figure 5.
2. Given new data set and ?, use algorithm in section
3.3 for inference. (The only difference between
this step and step (1) is that in this step we do not
need minimize L(?).
3. Select sentences for summarization from algo-
rithm in Figure 6.
Figure 6: Summarization Generation by S-sLDA.
A greedy algorithm is applied by adding sentence
one by one to obtain Sum?. We use G to denote
the sentence set containing selected sentences. The
algorithm first initializes G to ? and X to SU . Dur-
ing each iteration, we select one sentence from X
which maximize Score(sm ?G). To avoid topic re-
dundancy in the summary, we also revise the MMR
strategy (Goldstein et al, 1999; Ouyang et al, 2007)
in the process of sentence selection. For each sm,
we compute the semantic similarity between sm and
each sentence st in set Y in Eqn.(18).
cos?sem(sm, st) =
?
k ?smk?stk??
k ?2smk
??
k ?2stk
(18)
We need to assure that the value of semantic similar-
ity between two sentences is less than Thsem. The
whole procedure for summarization using S-sLDA
model is illustrated in Figure 6. Thsem is set to 0.5
in the experiments.
5 Experiments
5.1 Experiments Set-up
The query-focused multi-document summarization
task defined in DUC3(Document Understanding
Conference) and TAC4(Text Analysis Conference)
evaluations requires generating a concise and well
organized summary for a collection of related news
documents according to a given query which de-
scribes the users information need. The query
usually consists of a title and one or more narra-
tive/question sentences. The system-generated sum-
maries for DUC and TAC are respectively limited to
3http://duc.nist.gov/.
4http://www.nist.gov/tac/.
250 words and 100 words. Our experiment data is
composed of DUC 2007, TAC5 2008 and TAC 2009
data which have 45, 48 and 44 collections respec-
tively. In our experiments, DUC 2007 data is used
as training data and TAC (2008-2009) data is used
as the test data.
Stop-words in both documents and queries are
removed using a stop-word list of 598 words, and
the remaining words are stemmed by Porter Stem-
mer6. As for the automatic evaluation of summa-
rization, ROUGE (Recall-Oriented Understudy for
Gisting Evaluation) measures, including ROUGE-
1, ROUGE-2, and ROUGE-SU47 and their corre-
sponding 95% confidence intervals, are used to eval-
uate the performance of the summaries. In order to
obtain a more comprehensive measure of summary
quality, we also conduct manual evaluation on TAC
data with reference to (Haghighi and Vanderwende,
2009; Celikyilmaz and Hakkani-Tur, 2011; Delort
and Alfonseca, 2011).
5.2 Comparison with other Bayesian models
In this subsection, we compare our model with the
following Bayesian baselines:
KL-sum: It is developed by Haghighi and
Vanderwende (Lin et al, 2006) by using a KL-
divergence based sentence selection strategy.
KL(Ps||Qd) =
?
w
P (w)logP (w)Q(w) (19)
where Ps is the unigram distribution of candidate
summary andQd denotes the unigram distribution of
document collection. Sentences with higher ranking
score is selected into the summary.
HierSum: A LDA based approach proposed by
Haghighi and Vanderwende (2009), where unigram
distribution is calculated from LDA topic model in
Equ.(14).
Hybhsum: A supervised approach developed by
Celikyilmaz and Hakkani-Tur (2010).
For fair comparison, baselines use the same pro-
precessing methods with our model and all sum-
5Here, we only use the docset-A data in TAC, since TAC
data is composed of docset-A and docset-B data, and the docset-
B data is mainly for the update summarization task.
6http://tartarus.org/ martin/PorterStemmer/.
7Jackknife scoring for ROUGE is used in order to compare
with the human summaries.
94
maries are truncated to the same length of 100
words. From Table 1 and Table 2, we can
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3724 0.1030 0.1342
approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)
Hybhsum 0.3703 0.1007 0.1314(0.3600-0.3806) (0.0952-0.1059) (0.1241-0.1387)
HierSum 0.3613 0.0948 0.1278(0.3374-0.3752) (0.0899-0.0998) (0.1197-0.1359)
KLsum 0.3504 0.0917 0.1234(0.3411-0.3597) (0.0842-0.0992) (0.1155-0.1315)
StandLDA 0.3368 0.0797 0.1156(0.3252-0.3386) (0.0758-0.0836) (0.1072-0.1240)
Table 1: Comparison of Bayesian models on TAC2008
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3903 0.1223 0.1488
approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)
Hybhsum 0.3824 0.1173 0.1436(0.3686-0.3952) (0.1132-0.1214) (0.1358-0.1514)
HierSum 0.3706 0.1088 0.1386(0.3624-0.3788) (0.0950-0.1144) (0.1312-0.1464)
KLsum 0.3619 0.0972 0.1299(0.3510-0.3728) (0.0917-0.1047) (0.1213-0.1385)
StandLDA 0.3552 0.0847 0.1214(0.3447-0.3657) (0.0813-0.0881) (0.1141-0.1286)
Table 2: Comparison of Bayesian models on TAC2009
see that among all the Bayesian baselines, Hybh-
sum achieves the best result. This further illus-
trates the advantages of combining topic model with
supervised method. In Table 1, we can see that
our S-sLDA model performs better than Hybhsum
and the improvements are 3.4% and 3.7% with re-
spect to ROUGE-2 and ROUGE-SU4 on TAC2008
data. The comparison can be extended to TAC2009
data as shown in Table 2: the performance of S-
sLDA is above Hybhsum by 4.3% in ROUGE-2
and 5.1% in ROUGE-SU4. It is worth explaining
that these achievements are significant, because in
the TAC2008 evaluation, the performance of the top
ranking systems are very close, i.e. the best system
is only 4.2% above the 4th best system on ROUGE-
2 and 1.2% on ROUGE-SU4.
5.3 Comparison with other baselines.
In this subsection, we compare our model with some
widely used models in summarization.
Manifold: It is the one-layer graph based semi-
supervised summarization approach developed by
Wan et al(2008). The graph is constructed only con-
sidering sentence relations using tf-idf and neglects
topic information.
LexRank: Graph based summarization approach
(Erkan and Radev, 2004), which is a revised version
of famous web ranking algorithm PageRank. It is
an unsupervised ranking algorithms compared with
Manifold.
SVM: A supervised method - Support Vector Ma-
chine (SVM) (Vapnik 1995) which uses the same
features as our approach.
MEAD: A centroid based summary algorithm by
Radev et al (2004). Cluster centroids in MEAD
consists of words which are central not only to one
article in a cluster, but to all the articles. Similarity
is measure using tf-idf.
At the same time, we also present the top three
participating systems with regard to ROUGE-2 on
TAC2008 and TAC2009 for comparison, denoted as
(denoted as SysRank 1st, 2nd and 3rd)(Gillick et al,
2008; Zhang et al, 2008; Gillick et al, 2009; Varma
et al, 2009). The ROUGE scores of the top TAC
system are directly provided by the TAC evaluation.
From Table 3 and Table 4, we can see that
our approach outperforms the baselines in terms of
ROUGE metrics consistently. When compared with
the standard supervised method SVM, the relative
improvements over the ROUGE-1, ROUGE-2 and
ROUGE-SU4 scores are 4.3%, 13.1%, 8.3% respec-
tively on TAC2008 and 7.2%, 14.9%, 14.3% on
TAC2009. Our model is not as good as top par-
ticipating systems on TAC2008 and TAC2009. But
considering the fact that our model neither uses sen-
tence compression algorithm nor leverage domain
knowledge bases like Wikipedia or training data,
such small difference in ROUGE scores is reason-
able.
5.4 Manual Evaluations
In order to obtain a more accurate measure of sum-
mary quality for our S-sLDA model and Hybhsum,
we performed a simple user study concerning the
following aspects: (1) Overall quality: Which sum-
mary is better overall? (2) Focus: Which summary
contains less irrelevant content? (3)Responsiveness:
Which summary is more responsive to the query.
(4) Non-Redundancy: Which summary is less re-
dundant? 8 judges who specialize in NLP partic-
ipated in the blind evaluation task. Evaluators are
presented with two summaries generated by S-sLDA
95
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3724 0.1030 0.1342
approach (0.3660-0.3788) (0.0999-0.1061) (0.1290-0.1394)
SysRank 1st 0.3742 0.1039 0.1364(0.3639-0.3845) (0.0974-0.1104) (0.1285-0.1443)
SysRank 2nd 0.3717 0.0990 0.1326(0.3610-0.3824 (0.0944-0.1038) (0.1269-0.1385)
SysRank 3rd 0.3710 0.0977 0.1329(0.3550-0.3849) (0.0920-0.1034) (0.1267-0.1391)
PageRank 0.3597 0.0879 0.1221(0.3499-0.3695) (0.0809-0.0950) (0.1173-0.1269)
Manifold 0.3621 0.0931 0.1243(0.3506-0.3736) (0.0868-0.0994) (0.1206-0.1280)
SVM 0.3588 0.0921 0.1258(0.3489-0.3687) (0.0882-0.0960) (0.1204-0.1302)
MEAD 0.3558 0.0917 0.1226(0.3489-0.3627) (0.0882-0.0952) (0.1174-0.1278)
Table 3: Comparison with baselines on TAC2008
Methods ROUGE-1 ROUGE-2 ROUGE-SU4
Our 0.3903 0.1223 0.1488
approach (0.3819-0.3987) (0.1167-0.1279) (0.1446-0.1530)
SysRank 1st 0.3917 0.1218 0.1505(0.3778-0.4057) (0.1122-0.1314) (0.1414-0.1596)
SysRank 2nd 0.3914 0.1212 0.1513(0.3808-0.4020) (0.1147-0.1277) (0.1455-0.1571)
SysRank 3rd 0.3851 0.1084 0.1447(0.3762-0.3932) (0.1025-0.1144) (0.1398-0.1496)
PageRank 0.3616 0.0849 0.1249(0.3532-0.3700) (0.0802-0.0896) (0.1221-0.1277)
Manifold 0.3713 0.1014 0.1342(0.3586-0.3841) (0.0950-0.1178) (0.1299-0.1385)
SVM 0.3649 0.1028 0.1319(0.3536-0.3762) (0.0957-0.1099) (0.1258-0.1380)
MEAD 0.3601 0.1001 0.1287(0.3536-0.3666) (0.0953-0.1049) (0.1228-0.1346)
Table 4: Comparison with baselines on TAC2009
and Hybhsum, as well as the four questions above.
Then they need to answer which summary is better
(tie). We randomly select 20 document collections
from TAC 2008 data and randomly assign two sum-
maries for each collection to three different evalua-
tors to judge which model is better in each aspect.
As we can see from Table 5, the two models al-
most tie with respect to Non-redundancy, mainly
because both models have used appropriate MMR
strategies. But as for Overall quality, Focus and
Our(win) Hybhsum(win) Tie
Overall 37 14 9
Focus 32 18 10
Responsiveness 33 13 14
Non-redundancy 13 11 36
Table 5: Comparison with baselines on TAC2009
Responsiveness, S-sLDA model outputs Hybhsum
based on t-test on 95% confidence level. Ta-
ble 6 shows the example summaries generated re-
spectively by two models for document collection
D0803A-A in TAC2008, whose query is ?Describe
the coal mine accidents in China and actions taken?.
From table 6, we can see that each sentence in these
two summaries is somewhat related to topics of coal
mines in China. We also observe that the summary
in Table 6(a) is better than that in Table 6(b), tend-
ing to select shorter sentences and provide more in-
formation. This is because, in S-sLDA model, topic
modeling is determined simultaneously by various
features including terms and other ones such as sen-
tence length, sentence position and so on, which
can contribute to summary quality. As we can see,
in Table 6(b), sentences (3) and (5) provide some
unimportant information such as ?somebody said?,
though they contain some words which are related
to topics about coal mines.
(1)China to close at least 4,000 coal mines this year:
official (2)By Oct. 10 this year there had been 43 coal
mine accidents that killed 10 or more people, (3)Offi-
cials had stakes in coal mines. (4)All the coal mines
will be closed down this year. (5) In the first eight
months, the death toll of coal mine accidents rose
8.5 percent last year. (6) The government has issued
a series of regulations and measures to improve the
coun.try?s coal mine safety situation. (7)The mining
safety technology and equipments have been sold to
countries. (8)More than 6,000 miners died in accidents
in China
(1) In the first eight months, the death toll of coal mine
accidents across China rose 8.5 percent from the same
period last year. (2)China will close down a number of
ill-operated coal mines at the end of this month, said
a work safety official here Monday. (3) Li Yizhong,
director of the National Bureau of Production Safety
Supervision and Administration, has said the collusion
between mine owners and officials is to be condemned.
(4)from January to September this year, 4,228 people
were killed in 2,337 coal mine accidents. (5) Chen
said officials who refused to register their stakes in
coal mines within the required time
Table 6: Example summary text generated by systems
(a)S-sLDA and (b) Hybhsum. (D0803A-A, TAC2008)
96
6 Conclusion
In this paper, we propose a novel supervised ap-
proach based on revised supervised topic model for
query-focused multi document summarization. Our
approach naturally combines Bayesian topic model
with supervised method and enjoy the advantages of
both models. Experiments on benchmark demon-
strate good performance of our model.
Acknowledgments
This research work has been supported by
NSFC grants (No.90920011 and No.61273278),
National Key Technology R&D Program
(No:2011BAH1B0403), and National High Tech-
nology R&D Program (No.2012AA011101). We
also thank the three anonymous reviewers for their
helpful comments. Corresponding author: Sujian
Li.
References
David Blei and Jon McAuliffe. Supervised topic models.
2007. In Neural Information Processing Systems
David Blei, Andrew Ng and Micheal Jordan. Latent
dirichlet alocation. In The Journal of Machine Learn-
ing Research, page: 993-1022.
Charles Broyden. 1965. A class of methods for solv-
ing nonlinear simultaneous equations. In Math. Comp.
volume 19, page 577-593.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A Hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics. page:
815-825
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal and
Jaime Carbonell. 1999. Summarizing Text Docu-
ments: Sentence Selection and Evaluation Metrics. In
Proceedings of the 22nd annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, page: 121-128.
Amit Grubber, Micheal Rosen-zvi and Yair Weiss. 2007.
Hidden Topic Markov Model. In Artificial Intelligence
and Statistics.
Hal Daume and Daniel Marcu H. 2006. Bayesian Query-
Focused Summarization. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, page 305-312.
Gune Erkan and Dragomir Radev. 2004. Lexrank: graph-
based lexical centrality as salience in text summariza-
tion. In J. Artif. Intell. Res. (JAIR), page 457-479.
Dan Gillick, Benoit Favre, Dilek Hakkani-Tur, The ICSI
Summarization System at TAC, TAC 2008.
Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur,
Berndt Bohnet, Yang Liu, Shasha Xie. The ICSI/UTD
Summarization System at TAC 2009. TAC 2009
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 362370.
Feng Jin, Minlie Huang, and Xiaoyan Zhu. 2010. The
summarization systems at tac 2010. In Proceedings of
the third Text Analysis Conference, TAC-2010.
Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha and
Yong Yu. 2009. Enhancing diversity, coverage and bal-
ance for summarization through structure learning. In
Proceedings of the 18th international conference on
World wide web, page 71-80.
Chin-Yew Lin, Guihong Gao, Jianfeng Gao and Jian-Yun
Nie. 2006. An information-theoretic approach to au-
tomatic evaluation of summaries. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page:462-470.
Annie Louis, Aravind Joshi, Ani Nenkova. 2010. Dis-
course indicators for content selection in summariza-
tion. In Proceedings of the 11th Annual Meeting of
the Special Interest Group on Discourse and Dialogue,
page:147-156.
Tengfei Ma, Xiaojun Wan. 2010. Multi-document sum-
marization using minimum distortion, in Proceedings
of International Conference of Data Mining. page
354363.
Rebecca Mason and Eugene Charniak. 2011. Extractive
multi-document summaries should explicitly not con-
tain document-specific content. In proceedings of ACL
HLT, page:49-54.
Ani Nenkova and Lucy Vanderwende. The impact of fre-
quency on summarization. In Tech. Report MSR-TR-
2005-101, Microsoft Research, Redwood, Washing-
ton, 2005.
Ani Nenkova, Lucy Vanderwende and Kathleen McKe-
own. 2006. A compositional context sensitive multi-
document summarizer: exploring the factors that inu-
ence summarization. In Proceedings of the 29th an-
nual International ACM SIGIR Conference on Re-
97
search and Development in Information Retrieval,
page 573-580.
Miles Osborne. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Work-
shop on Automatic Summarization, Volume 4 page:1-
8.
Jahna Otterbacher, Gunes Erkan and Dragomir Radev.
2005. Using random walks for question-focused sen-
tence retrieval. In Proceedings of the Conference on
Human Language Technology and Empirical Methods
in Natural Language Processing, page 915-922
You Ouyang, Wenjie Li, Sujian Li and Qin Lua. 2011.
Applying regression models to query-focused multi-
document summarization. In Information Processing
and Management, page 227-237.
You Ouyang, Sujian. Li, and Wenjie. Li. 2007, Develop-
ing learning strategies for topic-based summarization.
In Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, page: 7986.
Daniel Ramage, David Hall, Ramesh Nallapati and
Christopher Manning. 2009. Labeled LDA: A super-
vised topic model for credit attribution in multi-labeled
corpora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Vol 1, page 248-256.
Dou She, Jian-Tao Sun, Hua Li, Qiang Yang and
Zheng Chen. 2007. Document summarization using
conditional random elds. In Proceedings of Inter-
national Joint Conference on Artificial Intelligence,
page: 28622867.
V. Varma, V. Bharat, S. Kovelamudi, P. Bysani, S. GSK,
K. Kumar N, K. Reddy, N. Maganti , IIIT Hyderabad
at TAC 2009. TAC2009
Xiaojun Wan and Jianwu Yang. 2008. Multi-document
Summarization using cluster-based link analysis. In
Proceedings of the 31st annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, page: 299-306.
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2007.
Manifold-ranking based topic-focused multi-
document summarization. In Proceedings of In-
ternational Joint Conference on Artificial Intelligence,
page 2903-2908.
Furu Wei, Wenjie Li, Qin Lu and Yanxiang He. 2008. Ex-
ploiting Query-Sensitive Similarity for Graph-Based
Query-Oriented Summarization. In Proceedings of the
31st annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
page 283-290.
Jin Zhang, Xueqi Cheng, Hongbo Xu, Xiaolei Wang, Yil-
ing Zeng. ICTCAS?s ICTGrasper at TAC 2008: Sum-
marizing Dynamic Information with Signature Terms
Based Content Filtering, TAC 2008.
Dengzhong Zhou, Jason Weston, Arthur Gretton, Olivier
Bousquet and Bernhard Schlkopf. 2003. Ranking on
Data Manifolds. In Proceedings of the Conference on
Advances in Neural Information Processing Systems,
page 169-176.
Jun Zhu and Eric Xing. 2010. Conditional Topic Random
Fields. In Proceedings of the 27th International Con-
ference on Machine Learning.
Xiaojin Zhu, Zoubin Ghahramani and John Laf-
ferty. 2003. Semi-supervised Learning using Gaussian
Fields and Harmonic Functions. In Proceedings of In-
ternational Conference of Machine Learning, page:
912-919.
98
