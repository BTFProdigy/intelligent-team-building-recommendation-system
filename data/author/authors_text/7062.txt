Coling 2008: Companion volume ? Posters and Demonstrations, pages 173?176
Manchester, August 2008
Entailment-based Question Answering  
for Structured Data 
Bogdan Sacaleanu?, Constantin Orasan?, Christian Spurk?, Shiyan 
Ou?, Oscar Ferrandez?, Milen Kouylekov? and Matteo Negri?  
?LT-Lab, DFKI GmbH / Saarbr?cken, Germany 
?RIILP, University of Wolverhampton / Wolverhampton, UK 
?Fondazione Bruno Kessler (FBK) / Trento, Italy 
?University of Alicante / Alicante, Spain 
 
 
Abstract  
This paper describes a Question Answer-
ing system which retrieves answers from 
structured data regarding cinemas and 
movies. The system represents the first 
prototype of a multilingual and multi-
modal QA system for the domain of tour-
ism. Based on specially designed domain 
ontology and using Textual Entailment as 
a means for semantic inference, the sys-
tem can be used in both monolingual and 
cross-language settings with slight ad-
justments for new input languages. 
1 Introduction 
Question Answering over structured data has 
been traditionally addressed through a deep 
analysis of the question in order to reconstruct a 
logical form, which is then translated in the query 
language of the target data (Androutsopoulos et 
al, 1995, Popescu et al 2003). This approach im-
plies a complex mapping between linguistic ob-
jects (e.g. lexical items, syntactic structures) and 
against data objects (e.g. concepts and relations 
in a knowledge base). Unfortunately, such a 
mapping requires extensive manual work, which 
in many cases represents a bottleneck preventing 
the realization of large scale and portable natural 
language interfaces to structured data.  
This paper presents the first prototype of a 
question answering system which can answer 
questions in several languages about movies and 
cinema using a multilingual ontology and textual 
                                                 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
entailment. The remainder of the paper is struc-
tured as follows: Section 2 presents the concept 
of entailment-based question answering; Section 
3 describes our prototype which implements this 
concept; A brief evaluation is presented in Sec-
tion 4, followed by conclusions in Section 5. 
2 Entailment-based QA 
Recently Textual Entailment (TE) has been pro-
posed as a unifying framework for applied se-
mantics (Dagan and Glickman, 2004), where the 
need for an explicit representation of a mapping 
between linguistic objects and data objects can 
be, at least partially, bypassed through the defini-
tion of semantic inferences at a textual level. In 
this framework, a text (T) is said to entail a hy-
pothesis (H) if the meaning of H can be derived 
from the meaning of T. 
On the basis of the TE framework, the QA 
problem can be recast as an entailment problem, 
where the text (T) is the question (or its affirma-
tive version) and the hypothesis (H) is a rela-
tional answer pattern, which is associated to in-
structions for retrieving the answer to the input 
question. In this framework, given a question Q 
and a set of relational answer patterns P, a QA 
system needs to select those patterns in P that are 
entailed by Q. Instructions associated to answer 
patterns may be viewed as high precision proce-
dures for answer extraction, which are dependent 
on the specific source which is asked for. In case 
of QA over structured data, instructions could be 
queries to a database; whilst in case of QA on the 
Web, an instruction could be the URL of a Web 
page containing the answer to a question or some 
form of IR query to a search engine. 
Therefore, the underlying idea of an entail-
ment-based QA system is to match the user?s re-
quest to a set of predefined question patterns in 
order to get some kind of analysis for the request. 
173
As an example consider the question ?Where 
can I watch the movie ?Dreamgirls? next Satur-
day?? and the predefined question patterns: 
? Which movies are currently running in 
[CINEMA]?  EAT = [MOVIE] 
? Where can I watch the movie [MOVIE] 
on [WEEKDAY]?  EAT = [CINEMA] 
? Where can I see [MOVIE]? 
 EAT = [CINEMA] 
In the example, each of the patterns contains 
placeholders for relevant named entities and has 
an expected answer type (EAT) associated with 
it. The entailment-based QA system should re-
turn that pattern (2) is entailed by the question 
and as a result the retrieval instructions associ-
ated to it will be used to answer the question. 
3 Description of system 
Our question answering system implements the 
concept of entailment-based question answering 
described in the previous section. The overall 
structure of our system is presented in Figure 1.  
Given a question asked by a user of the system 
in a known location, the QA planner forwards it 
to the Instance Annotator in order to find any 
concepts that might be related to the targeted 
domain (i.e. cinema, city, movie). The result is 
then analyzed by the Relation Matcher, which on 
the basis of entailment can either select the most 
appropriate interpretation of the question and im-
plicitly its associated procedure of answering the 
question, or decide that the user request is out-of-
coverage if no such interpretation is available. 
The cross-linguality of our system and, to a 
certain extent, the interaction between its compo-
nents is ensured by a domain ontology which is 
used for all four languages involved in the pro-
ject: English, German, Italian and Spanish, and 
its modules (Ou et al, 2008). Concepts from the 
ontology are used to annotate the user questions 
as well as data from which the answer is ex-
tracted. In the current stage of the project, the 
answers are contained in databases obtained from 
content provides or built from structured web 
pages. As a result, the information in the database 
tables was annotated with concepts from the on-
tology and then converted into an RDF graph to 
Figure 1. System Architecture 
174
facilitate retrieval using SPARQL query lan-
guage (Prud'hommeaux and Seaborne, 2006). 
Question patterns corresponding to one or several 
ontological relations were produced after ques-
tions for users were collected and used in the en-
tailment module. The question patterns used by 
the system are very similar to those presented in 
the previous section and contain placeholders for 
the actual entities that are expected to appear in a 
question. 
The SPARQL query associated with a pattern 
selected for a user question is used to retrieve the 
answers from the knowledge base and prepare for 
presentation. Given that our system is not limited 
to returning only textual information, further 
processing can be applied to the retrieved data. 
For example, for proximity questions the list of 
answers consists of cinema names and their GPS-
coordinates, which are used by the Answer Sort-
ing component to reorder the list of answers on 
the basis of their distance to the user?s location. 
Besides presenting the possible answers to a 
given question, the system can offer additional 
information based on the answer?s type: 
? a map for answers that are location 
names, 
? a route description for answers that are 
cinema names, 
? a video-trailer for answers that are movie 
names and 
? an image for answers that are person 
names. 
Due to the fact that a common semantics is 
shared by all four languages by way of a domain 
ontology, the system can be used not only in a 
monolingual setting, but also in a cross-language 
setting. This corresponds to a user-scenario 
where a tourist asks for information in their own 
language in a foreign location (i.e. English 
speaker in Italy). The only difference between 
monolingual and cross-language scenarios is that 
in the cross-language setting, the QA Core sub-
system (Figure 1) selects a Find Entailed Rela-
tion component according to the user input?s lan-
guage. This is due to the entailment algorithms 
that tend to use language specific resources in 
order to attain high accuracy results of matching 
the user request with one of the lexicalized rela-
tions (patterns). It is only the entailment compo-
nent that has to be provided in order to adapt the 
system to new input languages, once the lexical-
ized relations have been translated either manual 
or automatically. 
Both the Instance Annotator and the Answer 
Retriever are language independent, but location 
dependent (Figure 2). The Answer Retriever de-
pends on the location since it is querying data 
found at that place (i.e. Italy), while the Instance 
Annotator looks up instances of the data in the 
user?s question (i.e. annotates an English ques-
tion). They are language independent since they 
are working with data abstractions like SPARQL 
queries (Answer Retriever) or work at character 
level and do not consider language specific as-
pects, like words, in their look-up process (In-
stance Annotator). 
The current version of the system1 is designed 
according to the SOA (Service Oriented Archi-
tecture) and is implemented as point-to-point in-
tegrated web services. Any of the system?s com-
ponents can be substituted by alternative imple-
mentations with no need for further changes as 
long as the functionality remains the same. 
                                                 
1
 http://attila.dfki.uni-sb.de:8282/ QallMe_Proto-
type_WEB_Update/faces/Page6.jsp 
Figure 2. Cross-language Setting 
175
4 Evaluation 
A preliminary evaluation of the first prototype 
was carried out on randomly selected questions 
from a benchmark specifically designed for the 
project. This benchmark was developed to con-
tain questions about various aspects from the 
domain of tourism and for this reason we filtered 
out questions not relevant to cinema or movies. 
The evaluation of the system did not assess 
whether it can extract the correct answer. Instead, 
it measured to what extent the system can select 
the right SPARQL pattern. The explanation for 
this can be found in the fact that once a correct 
question pattern is selected, the extraction of the 
answer requires only retrieval of the answer from 
the database. Moreover, it should be pointed out 
that the main purpose of this preliminary evalua-
tion was to test the interaction between compo-
nents and indicate potential problems, and it was 
less about their performances.  
Table 1 summarises the results of the evalua-
tion. The number of questions used in the evalua-
tion is different from one language to another. 
This can be explained by the fact that for each 
language a number of questions (in general 500) 
was randomly selected from the benchmark and 
only the ones which referred to cinema or movies 
were selected. The column Questions indicates 
the number of questions assessed. The Correct 
column indicates for how many questions a cor-
rect SPARQL was generated. The Wrong column 
corresponds to the number of questions where a 
wrong or incomplete SPARQL was generated. 
This number also includes cases where no 
SPARQL was generated due to lack of corre-
sponding answer pattern. 
 
 Questions Correct Wrong 
English 167 74 (44.31%) 93 (55.68%) 
German 214 120 (56.04%) 94 (43.92%) 
Spanish 58 50 (86.20%) 8 (13.79%) 
Italian 99 46 (46.46%) 53 (53.53%) 
Table 1: Evaluation results 
As can be seen, the results are very different 
from one language to another. This can be ex-
plained by the fact that different entailment en-
gines are used for each language. In addition, 
even though the benchmark was built using a 
common set of guidelines, the complexity of 
questions varies from one language to another. 
For this reason, for some questions it is more dif-
ficult to find the correct pattern than for others.  
Analysis of the results revealed that one of the 
easiest ways to improve the performance of the 
system is to increase the number of patterns. Cur-
rently the average number of patterns per lan-
guage is 42. Improvement of the entailment en-
gines is another direction which needs to be pur-
sued. Most of the partners involved in the project 
have more powerful entailment engines than 
those integrated in the prototype which were 
ranked highly in RTE competitions. Unfortu-
nately, many of these engines cannot be used di-
rectly in our system due to their slow speed. Our 
system is supposed to give users results in real 
time which imposes some constraints on the 
amount of processing that can be done. 
5 Conclusions 
This paper presented the first prototype of an 
entailment-based QA system, which can answer 
questions about movies and cinema. The use of a 
domain ontology ensures that the system is cross-
language and can be extended to new languages 
with slight adjustments at the entailment engine. 
The system is implemented as a set of web ser-
vices and along a Service Oriented Architecture. 
6 Acknowledgements 
This work is supported by the EU-funded pro-
ject QALL-ME (FP6 IST-033860).  
References 
Androutsopoulos, I. and G.D. Ritchie and P. Thanisch. 
1995. Natural Language Interfaces to Databases -- 
An Introduction, Journal of Natural Language En-
gineering, vol.1, no.1, Cambridge University Press. 
Popescu Ana-Marie, Oren Etzioni, and Henry Kautz. 
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the confer-
ence on Intelligent User Interfaces. 
Dagan Ido and Oren Glickman. 2004. Probabilistic 
textual entailment: Generic applied modeling of 
language variability. In PASCAL Workshop on 
Learning Methods for Text Understanding and 
Mining, Grenoble. 
Ou Shiyan, Viktor Pekar, Constantin Orasan, Chris-
tian Spurk, Matteo Negri. 2008. Development and 
alignment of a domain-specific ontology for ques-
tion answering. In Proceedings of the 6th Edition of 
the Language Resources and Evaluation Confer-
ence (LREC-08).  
Prud'hommeaux Eric, Andy Seaborne (eds.). 2006. 
SPARQL Query Language for RDF. RDF Data Ac-
cess Working Group. 
176
231
232
233
234
Cross-Cutting Aspects  
of 
Cross-Language Question Answering Systems 
 
Bogdan Sacaleanu 
Language Technology Group 
DFKI GmbH 
Saarbr?cken, Germany 
bogdan@dfki.de 
G?nter Neumann 
Language Technology Group 
DFKI GmbH 
Saarbr?cken, Germany 
neumann@dfki.de 
 
 
Abstract 
We describe on-going work in the devel-
opment of a cross-language question-
answering framework for the open do-
main. An overview of the framework is 
being provided, some details on the im-
portant concepts of a flexible framework 
are presented and two cross-cutting as-
pects (cross-linguality and credibility) for 
question-answering systems are up for 
discussion. 
1 Introduction 
Different projects, different evaluation forums, 
different tasks, different languages, different 
document collections, different question types, 
different answer processing strategies ? Anyone 
familiar with all these concepts knows the com-
plexity and what a daunting prospect of develop-
ing a QA-System easily adaptable to ever chang-
ing requirements this might be. We have started 
off with a ?prototype-and-go? approach, trying to 
keep pace with the emergence of new tasks and 
managing the scarcity of your time and of your 
resources, to realize later on that what we had is 
a bunch of prototypes very tuned to their task 
requirements. Trying to adapt them to new re-
quirements seemed often more difficult then 
starting off with a new one. Therefore we started 
looking for an alternative, which should be more 
flexible and should allow us to cover much more 
requirements? variations; in other words we were 
considering putting together a Question Answer-
ing framework. 
In the rest of the paper we will shortly over-
view the components of such a framework and 
will describe the relevant aspects of the solution 
offered for each of them, aspects that should ac-
count for a large variety of question types, 
document collections and answer processing 
techniques, as well as for several languages. We 
will continue with a discussion of two issues that 
cut across several components of the framework, 
namely: cross-linguality and answer credibility, 
and will conclude by shortly naming the domains 
of usage for the framework and future work. 
2 Framework Overview 
Based on an existing set of cross-language 
Question Answering prototypes developed for 
different requirements, we began by looking for 
the commonalities among them. Following is a 
list of reusable components that might be 
considered as a starting point in defining a QA 
framework (see Figure 1). 
Several components along the work-flow of a 
typical QA system were identified: a Unit 
Alignment component in cross-language envi-
ronments and a Query Expansion compo-
nent for the Question Analysis task; a Unit 
Processor and a Query Generator com-
ponent for the Information Retrieval task; a 
Mention Chain component for the Answer 
Extraction task and a Scoring Strategy for 
the Answer Selection task. (see Figure 1) 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
15
Beside the reusability aspect in context of a 
QA framework, extensibility is a further 
important issue. We have approached it by con-
centrating on a flexible representation format of 
the data being passed around in the framework, 
namely the representation of the Question-
Object, the InformationUnit and the An-
swerObject. 
2.1 Reusability 
?Reusability is the likelihood a segment of 
structured code can be used again to add new 
functionalities with slight or no modification?.  
(Wikipedia) 
 
In a cross-language setting of a QA system, 
which is crossing the language barrier at the user 
end rather then at the document end, there are 
two approaches for getting the formalized user 
information need (QuestionObject) to the 
documents? language: either creating it based on 
the question translation or analyzing the original 
question and translating the formalized result. 
This last approach is convenient when machine 
readable dictionaries, part-of-speech taggers and 
other bilingual lexical resources (i.e. WordNets) 
for both languages are available. For this purpose 
a Unit Alignment Component was designed 
that produces an alignment of simple (words) 
and complex (phrases) syntactic units from the 
source to target language. 
Query Expansion is another component 
present among the support structures of our 
framework. Backed by lexico-semantic resources 
as EuroWordNet [V98] it can be used for all lan-
guages supported by these resource. For a given 
input word, it can return synonyms, hyponyms 
and hypernyms according to the following algo-
rithm: 
if (trgtWord_is_not_ambig) 
 return Information; 
else if (trgtWord_is_ambig) 
{ 
TRANS: 
1. translate Question 
2. do Unit Alignment 
3. if (transTrgtWord_is_EWN_aligned) 
a. if (alignment_is_not_ambig) 
return Information; 
b. else if (alignment_is_ambig) 
save Alignments; 
goto TRANS; 
} 
 
intersection = intersect_saved_alignments(); 
if (intersection.size == N)         // strict N=1 
 return Information_for_intersection; 
return NULL; 
An example for the above algorithm up to the 
stage 3.a is given in Section 3.1. This represents 
the ideal case, when our input can be 
disambiguated using the alignments of a question 
translation. But more often it is the case of 
advancing over this stage to 3.b, when the 
ambiguous EuroWordNet algnments are being 
saved and a new translation of the question 
through other online translation services is 
attempted. The idea behind this expansion 
method is that lexical diversity of different 
translations could narrow down the ambiguity of 
a word to a desired level (N). 
Figure 1: Framework Architecture 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
16
To select an initial set of information units 
relevant to the question, traditional search en-
gines are being used to scale down the search 
space. Two important aspects in this process are: 
the level of detail at which the indexation units 
are chosen and the granularity of the information 
units (be it document, paragraph, sentence or 
phrase). Two subcomponents are important at 
this stage: the Unit Processor and the 
Query Generator. 
The Unit Processor covers the above-
mentioned aspects: it takes as input an 
InformationUnit (i.e., a raw text document) 
and it either
 
reduces it to a set of new units (i.e., 
sentences), or it annotates the unit at different 
levels (i.e., named entities, grammatical rela-
tions). This way, by chaining different Unit 
Processors, you can both reduce the informa-
tion unit size and generate new indexing units.  
The Query Generator relies on an ab-
stract description of the processing method to 
accordingly generate the IRQuery to make use 
of the advanced indexation units. For example, 
when named entities were annotated during the 
Unit Processor chaining and used as index-
ing units, the Query Generator will adapt 
the IRQuery so as to search for an additional 
field (neType) that might reflect the expected 
answer type for the question considered. We 
consider the Query Generator as being the 
mediator between the question analysis result 
QuestionObject (answer type, constraints) 
and the search engine serving the retrieval com-
ponent with information units (documents, sen-
tences). Even more, the Query Generator 
relies on an abstract description of the search 
engine, too, and can adapt the IRQuery accord-
ing to either a boolean or a ranked search engine. 
A Mention Chain component in the An-
swer Extraction task provides an ease of burden 
for the Selection task by computing answer ref-
erence chains. This is very helpful for factoid 
questions on the Web, and not only, where re-
dundancy of an answer candidate is a good hint 
for its potential selection and credibility. A men-
tion chain contains all answers sharing a com-
mon normalized representation, determined ei-
ther through the string similarity of the answers 
only or by additionally employing context en-
tailment measures.  
The Scoring Strategy component builds 
on the mathematical graph theory and reduces 
the answer candidate scoring issue to a shortest 
path problem on lexical graphs. In most of the 
cases the answer ?suitability? could be scaled 
down to computing a distance metric for the an-
swer and some information in the question (i.e., 
keywords, focus). Both a simple textual distance 
measure and another one based on dependency 
structures were implemented on these graph 
structures, with slight variations making use of 
weight and cost properties for graph edges. 
Based on available web search API (i.e., 
Google, Yahoo) the Answer Validation 
component computes a total frequency count of 
co-occurrence for pairs of question and answer, 
assuming that the right answer shares more con-
texts with the question than any other candidate 
and that the considered answers are semantic 
independent and insensitive with respect to the 
timeline preferred by the search engines. 
2.2 Extensibility 
 ?Extensibility is a system design principle where 
the implementation takes into consideration fu-
ture growth. ? The central theme is to provide 
for change while minimizing impact to existing 
system functions.? (Wikipedia) 
Extensibility can be approached by two meth-
ods during framework design: through software 
design patterns and through a common extensible 
data representation format. While we have used 
some patterns through the development of reus-
able components like the chaining aspect in the 
Unit Processor, the normalization function in the 
Mention Chain and the graph design in the Scor-
ing Strategy, we have concentrated more on an 
extensible representation format for data being 
passed around through the framework: the 
QuestionObject, the InformationUnit 
and the AnswerObject. For this purpose we 
have used XML, XML Schema and data binding 
methods (JAXB) to guarantee component life on 
an evolving data format. The primary benefit of 
allowing extensibility in a format is that it en-
ables a format to evolve without requiring central 
control of the format. A secondary benefit is that 
it allows the format to stay focused and simple 
by pushing specialized cases and complex solu-
tions into optionally supported extensions. W3C 
XML Schema provides two features that promote 
extensibility in XML vocabularies: the wildcards 
xs:any and xs:anyAttribute are used to 
allow the occurrence of elements and attributes 
from specified namespaces into a given format, 
and the xsi:type attribute that can be placed 
on an element in an XML instance document to 
change its type to a more refined subtype. That 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
17
is, according to user?s need, the data exchanged 
among the framework?s components can be ex-
tended, without changing the framework func-
tionality. 
3 QA Cross-Cutting Aspects 
In a question answering framework there are as-
pects that does not directly relate to the core con-
cerns, but are needed for a proper design. More-
over, they can hardly be pinned down to a com-
ponent, as they cross several core components of 
the system.  We are talking about concepts like 
cross-linguality and credibility ? system credibil-
ity reflected in answer credibility. 
3.1 Cross-Linguality 
There are three traditional approaches that count 
for cross?linguality in context of information 
management systems: 
? translating the queries into the target 
language, 
? translating the document collection into 
the source language or 
? translating the queries and the docu-
ments into an intermediate representation 
(inter?lingua). 
Two types of translation services are well 
known within this context which are based on  
? lexical resources (e.g., dictionaries, 
aligned wordnets), or 
? machine translation (e.g., example?based 
translation). 
The only feasible approach when dealing with 
huge amounts of data, as is the case for question 
answering systems, is translating the question 
into the language of the document collection and 
the related issue of back-translating the answers 
into the language of the user. 
We are using two different methods for re-
sponding questions asked in a language different 
from the one of the answer-bearing documents. 
Both employ online translation services (Alta-
vista, FreeTranslation, etc.) for crossing the lan-
guage barrier, but at different processing steps: 
before and after formalizing the user information 
need into a QuestionObject. 
The before?method translates the question 
string in an earlier step, resulting in several 
automatic translated strings, of which the best 
one is analyzed by the Question Analysis com-
ponent and passed on to the Information Re-
trieval component. This is the strategy we use in 
an English?German cross-lingual setting. To be 
more precise: the English source question is 
translated into several alternative German ques-
tions using online MT services. Each German 
question is then parsed with SMES [NP02], our 
German parser. The resulting query object is then 
weighted according to its linguistic well?
formedness and its completeness wrt. query in-
formation (question type, question focus, an-
swer?type). 
The assumption behind this weighting scheme 
is that ?a translated string is of greater utility for 
subsequent processes than another one, if its lin-
guistic analysis is more complete or appropri-
ate.? 
The after?method translates the formal-
ized result of the Query Analysis Component by 
using the question translations, a language mod-
eling tool and a word alignment tool for creating 
a mapping of the formal information need from 
the source language into the target language. We 
illustrate this strategy in a German?English set-
ting along two lines (using the following German 
question as example: In welchem Jahr-
zehnt investierten japanische 
Autohersteller sehr stark?): 
- translations as returned by the on-line MT 
systems are being ranked according to a 
language model 
In which decade did Japanese 
automakers invest very 
strongly? (0.7) 
In which decade did Japanese 
car manufacturers invest 
very strongly? (0.8) 
- translations with a satisfactory degree of 
resemblance to a natural language utterance (i.e. 
linguistically well-formedness), given by a 
threshold on the language model ranking, are 
aligned based on several filters: back-
propagation dictionary filter - based on MRD 
(machine readable dictionaries), PoS filter - 
based on statistical part-of-speech taggers, and 
cognates filter - based on string similarity 
measures (dice coefficient and LCSR (lowest 
common substring ratio)). 
In: [in:1] 
welchem: [which:0.5] 
Jahrzehnt: [decade:1] 
investierten: [invest:1] 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
18
japanische: [Japanese:0.5] 
Autohersteller:  
[car manufacturers:0.8,  
auto makers:0.1] 
sehr: [very:1] 
stark: [strongly:0.5] 
The evaluation gives evidence that both 
strategies are comparable in results, whereby the 
last one is slightly better, due to the fact of not 
being forced to choose a best translation, but 
working with and combining all the translations 
available. That is, considering and combining 
several, possible different, translations of the 
same question, the chance of detecting a 
translation error in an earlier phase of the work?
flow becomes higher and avoids error 
propagations through the whole system. 
The related issue of back-translating is ex-
plored by looking for parallel data to the an-
swer?s context or metadata, and extracting trans-
lation candidates based on their context and 
string surface similarity. For example, in a CLEF 
setting, for a German question having as English 
answer  ?Yasser Arafat? we have extracted the 
time-stamp of the answer?s context (19.07.1994), 
collected all the data with a time-stamp of 
07.1994 in the source language, extracted the 
named entities of type PERSON and then aligned 
?Jassir Arafat? based on its string surface simi-
larity to the initial answer. 
The translations and their alignment to the 
original question, according to the above-
mentioned after-method, have also a posi-
tive side-effect, namely: some of the aligned 
words may have several ranked translations. As 
it is the case of the ?Autohersteller?, a word 
might consider the best ranked alignment (?car 
manufacturers?) as its direct translation and the 
remaining ones as its expanded words. As such, 
given a reliable alignment method, cross-
linguality can prove supportive even for Query 
Expansion. Moreover, another method of us-
age can confirm the added value of cross-
linguality for Query Expansion, as de-
scribed below. 
For this task we are using the German and the 
English wordnets aligned within the EuroWord-
Net [V98] lexical resource. Our goal is to extend 
the formalized information need Question 
Object with synonyms for the words that are 
present in the wordnet. 
Considering the ambiguity of words, a WSD 
module is required as part of the expansion task. 
For this purpose we are using both the original 
question and its translations, leveraging the re-
duction in ambiguity gained through translation. 
Our devised pseudo-WSD algorithm works 
as following: 
1. look up every word from the word-
translation alignment (see example above) in the 
lexical resource; 
2. if the word is not ambiguous (which is, for 
example, the case for Japanese) then extend 
the Question Object with its synonyms (e.g., 
[Japanese, Nipponese]); 
3. if the word is ambiguous (e.g., invest) then  
(3a) for every possible reading of it, get its 
aligned German correspondent reading (if it 
exists) and look up that reading in the German 
original question , e.g., 
1351398: adorn-clothe-invest (EN) 
1351223: invest-vest (EN) 
 1400771: empower-endow-endue-gift-
indue-invest (EN) 
1350325: induct-invest-seat (EN) 
1293271:  
? commit-invest-place-put (EN) 
? anlegen-investieren (DE) 
(3b) if an aligned reading is found (e.g., Read-
ing-1293271) retain it and add the English 
synonyms of it to the Question Object, i.e., 
expand it with: 
 commit, place, put 
Following the question expansion task, the 
Question Object has been enriched with new 
words that are synonyms of the un?ambiguous 
English words and by synonyms of those 
ambiguous words, whose meaning(s) have been 
found in the original German question. Thus our 
expanded example has gained several more 
expanded words as follows: 
{Nipponese,commit,place,put} 
3.2 Answer Credibility 
In the ideal case, a Questions Answering System 
(QAS) will deliver correct answers and knows 
that they are correct, i.e., it can deliver a proof of 
the correctness of the answers. However, at least 
for the case of open-domain textual QA 
applications, this is out of reach with current 
technology. Thus, current QAS can only deliver 
answers with certain trustworthyness. Since, a 
receiver of an answer usually assumes, that a 
QAS tries to identify the best answer possible (at 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
19
least for cooperative language games), the QAS 
should assign a credibility measure to each 
selected answer. The underlying decisions made, 
can then also be used for explaining, how the 
answer?s credibility was determined.  
We view answer credibility as an additional 
process to answer extraction and selection that 
determines the quality of identified answer 
candidates by checking the plausibility of the 
answer sources and context on basis of meta 
information. For example, useful document and 
web page information might be:1 
? The name of the author of this activity; 
? Textual fingerprints of authority, e.g., 
?official web page of US government?; 
? E-mail address of the contact person for 
this activity; 
? When was this webpage last updated and 
links were checked (also: is there an regu-
lar update); 
? The name of the host school or organiza-
tion; 
? The link structure of the document, e.g., 
link IN/OUT density, links to relevant 
people, other authorities, clusters / hierar-
chies of authorities;  
? Text structure, e.g., textual coherence or 
style; 
Another important source of meta information 
relates to ontological knowledge, e.g., the consis-
tency of contextual information with respect to a 
domain ontology that defines the scope of the 
answer candidates. By this we mean the follow-
ing:  
? Given an answer candidate A of a ques-
tion Q (e.g., an instance of a concept in 
question) determined by a web-based 
QAS. 
? Check the textual context of A concern-
ing the mentioning of other relevant 
facts/concepts, that can be determined via 
access to an external (domain) ontology 
using relevant terms from Q and A.  
                                                
1
 For information on the topic of Web Credibility, cf. 
http://credibility.stanford.edu/ . This URL links to the 
Web Credibility Project of the Stanford Persuasive 
Technology Lab. Although they do not consider 
credibility under a strict QA perspective as we do, 
their work and results are a rich source of inspiration. 
For example, if the following request is sent to a 
web-based QAS: Name IE-systems that are 
based on statistical methods. Assume that for 
this question, the QAS identifies a list of names 
of IE-systems from textual sources as answer 
candidates. Assume further, that the QAS has 
access to an ontology-based meta-store about 
Language Technology terms, cf. [JU05]. Then, 
answer credibility checking can use the query 
terms IE-system and statistical method for ex-
tracting relevant facts from this LT-store, e.g., 
names of statistical methods, IE experts, IE sub-
tasks or properties of the concept information 
extraction. Next, for each answer candidate, 
check the textual context of the answer for the 
mentioning of these terms and their mutual rela-
tionship. Then a possible credibility heuristics 
might decide that the more relevant domain-
knowledge can be identified in the context of the 
answer the higher is the credibility of this an-
swer. 
These examples demonstrate that a wide vari-
ety of metadata from different levels can be and 
should be exploited for determining the credibil-
ity of answers. Since answer credibility is a new 
research area, it is still unclear which level of 
information is best suited for which kind of ques-
tions and answers. In order to be able to investi-
gate many possible credibility scenarios, we con-
sider answer credibility as a complex abstract 
data type or QAS credibility model, QAS-CM 
for short. The QAS-CM allows the definition of 
different kinds of credibility parameters, which 
are related to corresponding meta data and are 
orthogonal to the component-oriented view of 
the QAS. The values of the parameters might be 
computed and evaluated by different compo-
nents, even in parallel. Thus, the credibility of an 
answer is a complex value determined through 
composition of component-related credibility 
values. We are distinguishing two types of meta-
data: 
? Static metadata: are available directly 
through the textual source of an answer 
candidate and are represented in form of 
annotations, e.g. HTML/XML tags. 
? Dynamic metadata: are computed 
online via the components of the QAS, 
e.g., linguistic entities, semantic relations, 
textual entailment, text structure and co-
herence, topic linkage.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
20
Following this perspective, we propose the 
following structure for the QAS-CM (see Figure 
2). We distinguish two major subsystems: the 
QAS-CM itself and the QAS runtime instance. 
The QAS-CM is further subdivided into three 
modules: 
1. CM-description 
2. Data-Model 
3. Process-Model 
The CM-description module defines the 
credibility properties in a declarative way 
independently from the specific QAS instance 
used. It defines the name space of the credibility 
properties that are to be implemented and the 
way they are composed. We assume that each 
individual credibility property returns a real 
number, i.e., we consider it as a utility function, 
cf. [RN03]. Thus, the composition rules define 
how a complex value is computed, e.g., by 
summation or multiplication. The CM-
description also contains specification about the 
actual QAS instance, e.g., the type of document 
source or natural language to be processed. The 
CM-description serves as a blueprint for the 
other two modules, the Data-Model and the 
Process-Model. 
The Data-Model implements credibility prop-
erties as decision rules over instantiated meta 
information taking into account either static or 
dynamic meta data. For example a rule like  
entails(<SOURCE>,?official 
web page of <LOCATION>?)  
authority:=?I 
assigns the weight ?I to the credibility property 
with name authority if the document source 
of the answer candidate has a tag <source> 
which contains a substring ?official web 
page <LOCATION>?, where <LOCATION> 
contains a location name which corresponds to 
one mentioned in the Wh-question. For a 
question like ?Name all German chancellors after 
World War II.?, then an answer with a web page 
source, that contains something like <SOURCE> 
? official web page of Germany ? 
</SOURCE> will receive a credibility value ?I. 
The Process-Model is related to the compo-
nents of the QAS that have been used for the 
processing of a question/answer pair. We assume 
that each component can assign a utility measure 
for its determined output. In doing so, a compo-
nent should also take into account the size and 
utility measure of its input and the ambiguity 
degree of its output etc. For example, assume 
that the question analysis component (cf. sec. 3) 
can recognize the syntactic quality of a parse tree 
of a Wh-question, as being complete, par-
tial or null, cf. [NS05a]. Then a possible 
decision rule for the Process-Model might be 
syntactic_quality(<QUESTION> 
, COMPLETE)  QueryAna-
lyser:=?I=1 
in case, the parser of the question analysis 
component was able to completely parse the Wh-
question <QUESTION>. In a similar way, the 
web-validator mentioned in sec. 2.1 can also be 
integrated as a credibility function into our QAS-
CM using the following decision rule: 
 
 
Figure 2. Credibility Model 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
21
validate(Web, <QUESTION>, 
<CurrAnsw>)  WebValua-
tor:=?I 
 
Note that in case statistical components are used 
in QAS, e.g., a stochastic NE-recognizer, then 
the probabilities of the those components can be 
used for defining the utility values for the 
representative decision rules. Furthermore note, 
that each decision rule of the Process-Model 
corresponds to a single component. The 
compositon of several such rules are defined in 
the CM-description module. 
We are currently implementing QAS-CM 
following the same object-oriented framework as 
described in sec. 3, which eases integration of the 
components of the QAS. The data and the 
process model are implemented as a set of 
production rules using the RuleML language, cf. 
http://www.ruleml.org/. Currently, we define the 
values of ?I manually. This is more easier to 
implement but more tricky to maintain because 
of potential mutual interdependencies between 
individual values. Therefore, in the next 
development cycle of our QAS-CM, we will use 
a statistical approach for automatically acquiring 
optimal parameter settings. Starting point will be 
a question/answer/document corpus. This corpus 
can directly be used for training the Data-Model. 
In the case of the Process-Model, a Bayesian 
Network will be dynamically trained following 
ideas from IE-research, cf. [PP03]. Since in this 
case we also need output from all major QAS 
components, we are integrating an exhaustive 
tracing mechanism into our QA framework as a 
basis for automatically determining initial 
training material. 
4 Results 
The presented QA framework has been used 
both in mono-lingual and cross-lingual scenarios 
for closed document collections (CLEF collec-
tion) and open document collections (World 
Wide Web). In terms of reusability and extensi-
bility, the framework allowed for up to two 
weeks of work for building fully functional QA 
systems for different use scenarios. In terms of 
time performance for systems build upon the 
framework, the following figures apply: for sys-
tems used to query the Web a response time of 
up to 20 seconds in mono-lingual settings could 
be measured; those querying the CLEF document 
collection in a mono-lingual setting (German 
only) registered a latency of up to 3 seconds and 
for a cross-lingual setting of up to 15 seconds. 
The qualitative performance has been measured 
only on closed document collection and the best 
results for 200 questions of the CLEF 2005 
evaluation campaign in different use scenarios, 
according to [NS05b], were as follows: 
 
 Right Wrong Inexact 
DeDe 87 43.5% 100 13 
DeEn 51 25.5% 141 8 
EnDe 46 23% 141 12 
Reference 
[JU05] B. J?rg and  H. Uszkoreit. The Ontology-
based Architecture of LT World, a Comprehensive 
Web Information System for a Science and Tech-
nology Discipline. Leitbild Informations-
kompetenz: Positionen - Praxis - Perspektiven im 
europ?ischen Wissensmarkt. 27. Online Tagung, 
2005. 
[NP02]
 G. Neumann and J. Piskorski. A shallow text 
processing core engine. Computational Intelli-
gence, 18(3):451?476, 2002. 
[NS05a]
 G. Neumann and S. Sacaleanu. Experiments 
on robust NL-question interpretation and multi-
layered document annotation for a cross-language 
question/answering system. In Clef 2004, volume 
3491. Springer-Verlag LNCS, 2005. 
[NS05b] G. Neumann and B. Sacaleanu. DFKI's LT-
lab at the CLEF 2005 Multiple Language Question 
AnsweringTrack. In Working Notes for the CLEF 
2005 Workshop, 21-23 September, Vienna, Aus-
tria, 2005. 
[PP03] L. Peshkin and A. Pfefer. Bayesian Informa-
tion Extraction Network. In proceedings of IJCAI, 
2003. 
[RN03] S. Russell and P. Norvig. Artificial Intelli-
gence: A Modern Approach. Prentice-Hall, Engle-
wood Cliffs, NJ, 2nd edition, 2003. 
Vossen, P. (eds) 1998 EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks, 
Kluwer Academic Publishers, Dordrecht.
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
22
