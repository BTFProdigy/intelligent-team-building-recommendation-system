Learning Chinese Bracketing Knowledge Based on  
a Bilingual Language Model 
Yajuan L?, Sheng Li, Tiejun Zhao, Muyun Yang  
School of Computer Science & Engineering, Harbin Institute of Technology 
Harbin, China, 150001 
Email: {lyj,lish,tjzhao,ymy}@mtlab.hit.edu.cn 
 
Abstract  
This paper proposes a new method for 
automatic acquisition of Chinese bracketing 
knowledge from English-Chinese sentence- 
aligned bilingual corpora. Bilingual sentence 
pairs are first aligned in syntactic structure by 
combining English parse trees with a 
statistical bilingual language model. Chinese 
bracketing knowledge is then extracted 
automatically. The preliminary experiments 
show automatically learned knowledge 
accords well with manually annotated 
brackets. The proposed method is 
particularly useful to acquire bracketing 
knowledge for a less studied language that 
lacks tools and resources found in a second 
language more studied. Although this paper 
discusses experiments with Chinese and 
English, the method is also applicable to 
other language pairs. 
Introduction 
The past few years have seen a great success in 
automatic acquisition of monolingual parsing 
knowledge and grammars. The availability of 
large tagged and syntactically bracketed corpora, 
such as Penn Tree bank, makes it possible to 
extract syntactic structure and grammar rules 
automatically (Marcus 1993). Substantial 
improvements have been made to parse western 
language such as English, and many powerful 
models have been proposed (Brill 1993, Collins 
1997). However, very limited progress has been 
achieved in Chinese. 
      Knowledge acquisition is a bottleneck for 
real appication of Chinese parsing. While some 
methods have been proposed to learn syntactic 
knowledge from annotated Chinese corpus, most 
of the methods depended on the annotated or 
partial annotated data(Zhou 1997, Streiter 2000). 
Due to the limited availbility of Chinese 
annotated corpus, tests of these methods are still 
small in scale. Although some institutions and 
universities currently are engaged in building 
Chinese tree bank, no large scale annotated 
corpus has been published until now because the 
complexity in Chinese syntatic sturcture and the 
difficulty in corpus annotation (Chen 1996).  
This paper proposes a novel method to 
facilitate the Chinese tree bank construction. 
Based on English-Chinese bilingual corpora and 
better English parsing, this method obtains 
Chinese bracketing information automatically via 
a bilingual model and word alignment results. 
The main idea of the method is that we may 
acquire knowledge for a language lacking a rich 
collection of resources and tools from a second 
language that is full of them.  
The rest of this paper is organized as 
follows : In the next section, a bilingual language 
model is introduced. Then, a bilingual parsing 
method supervised by English parsing is 
proposed in section 2. Based on the bilingual 
parsing, Chinese bracketing knowlege is 
extracted in section 3. The evaluation and 
discussion are given in section 4. We conclude 
with discussion of future work. 
1 A bilingual language model ? ITG 
Wu (1997) has proposed a bilingual language 
model called Inversion Transduction Grammar 
(ITG), which can be used to parse bilingual 
sentence pairs simultaneously. We will give a 
brief description here. For details please refer to 
(Wu 1995, Wu 1997).  
The Inversion Transduction Grammar is a 
bilingual context-free grammar that generates 
two matched output languages (referred to as L1 
and L2). It also differs from standard context-free 
grammars in that the ITG allows right-hand side 
production in two directions: straight or inverted. 
The following examples are two ITG 
productions: 
C -> [A B] 
C -> <A B> 
Each nonterminal symbol stands for a pair of 
matched strings. For example, the nonterminal A 
stands for the string-pair (A1, A2). A1 is a 
sub-string in L1, and A2 is A1?s corresponding 
translation in L2. Similarly, (B1, B2) denotes the 
string-pair generated by B. The operator [ ] 
performs the usual concatenation, so that C -> [A 
B] yields the string-pair (C1, C2), where C1=A1B1 
and C2=A2B2. On the other hand, the operator <> 
performs the straight concatenation for language 
1 but the reversing concatenation for language 2, 
so that C -> <A B> yields C1=A1B1, but C2=B2A2. 
The inverted concatenation operator permits the 
extra flexibility needed to accommodate many 
kinds of word-order variation between source 
and target languages (Wu 1995). 
There are also lexical productions of the 
following form in ITG: 
A -> x/y 
This means that a symbol x in language L1 is 
translated by the symbol y in language L2.  x or y 
may be a null symbol e, which means there may 
be no counterpart string on other side of the 
bitext.  
ITG based parsing matches constituents for 
an input sentence-pair. For example, Figure 1 
shows an ITG parsing tree for an 
English-Chinese sentence-pair. The inverted 
production is indicated by a horizontal line in the 
parsing tree. The English text is read in the usual 
depth-first left to right order, but for the Chinese 
text, a horizontal line means the right sub-tree is 
traversed before the left. The generated parsing 
results are: 
(1) [[[Mr. Wu]BNP [[plays basketball]VP [on 
Sunday ]PP ]VP ]S . ]S  
(2) [[[ ] [ [ 	]]] 
] 
We can also represent the common structure 
of the two sentences more clearly and compactly 
with the aid of <> notation: 
(3)  [[<Mr./ Wu/>BNP < [plays/ basketball/	]VP 
[on/e Sunday/]PP >VP ]S ./
]S 
where the horizontal line from Figure 1 
corresponds to the <> level of bracketing. 
. 
S 
BNP 
VP 
PP 
VP 
Mr./ Wu/ 
plays/ basketball/

 
on/e Sunday/	 
S 
./ 
Figure 1  Inversion transduction Grammar parsing
      Any ITG can be converted to a normal form, 
where all productions are either lexical 
productions or binary-fanout nonterminal 
productions(Wu 1997). If probability is 
associated with each production, the ITG is 
called the Stochastic Inversion Transduction 
Grammar (SITG). 
2 English parsing supervised bilingual 
bracketing 
Because of the difficulty in finding a suitable 
bilingual syntactic grammar for Chinese and 
English, a practical ITG is the generic Bracketing 
Inversion Transduction Grammar (BTG)(Wu 
1995). BTG is a simplified ITG that has only one 
nonterminal and does not use any syntactic 
grammar. A Statistical BTG (SBTG) grammar is 
as follows: 
j
b
i
b
ji
baa
veAeuA
vuAAAAAAA
ejie
ij
/    ;/ 
   ; /    ;    ];[ 
??????
???><??????
       SBTG employs only one nonterminal 
symbol A that can be used recursively. Here, ?a? 
denotes the probability of syntactic rules. 
However, since those constituent categories are 
not differentiated in BTG, it has no practical 
effect here and can be set to an arbitrary constant. 
The remaining productions are all lexical. bij is 
the translation probability that source word ui 
translates into target word vj. bij can be obtained 
using a statistical word-translation model 
(Melamed 2000) or word alignment(L? 2001a). 
The last two productions denote that the word in 
one language has no counterpart on other side of 
the bitext. A small constant can be chosen for the 
probabilities bie and bej.   
In BTG, no language specific syntactic 
grammar is used. The maximum-likelihood 
parser selects the parse tree that best satisfies the 
combined lexical translation preferences, as 
expressed by the bij probabilities. Because the 
expressiveness characteristics of ITG naturally 
constrain the space of possible matching in a 
highly appropriate fashion, BTG achieves 
encouraging results for bilingual bracketing 
using a word-translation lexicon alone (Wu 
1997). 
Since no syntactic knowledge is used in 
SBTG, output grammaticality can not be 
guaranteed. In particular, if the corresponding 
constituents appear in the same order in both 
languages, both straight and inverted, then lexical 
matching does not provide the discriminative 
leverage needed to identify the sub-constituent 
boundaries. For example, consider an 
English-Chinese sentence pair: 
(4) English: That old teacher is our adviser. 
Chinese: 	
 
Using SBTG, the bilingual bracketing result is : 
(5) [[[[[[The/ old/] teacher/] is/] our/	] 
adviser/
] ./] 
The result is not consistent with the 
expected syntactic structure. In this case, 
grammatical information about one or both of the 
languages can be very helpful. For example, if we 
know the English parsing result shown in (6), 
then the bilingual bracketing can be determined 
easily; the result should be (7).  
(6) [[That old teacher]BNP [is [our adviser]BNP ]VP .]S 
(7) [[That/ old/ teacher/] [is/ [our/	 
adviser/
] ] ./] 
From the example, we can see that if one 
language parser is available, the induced 
bilingual bracketing result would be more 
accurate. English parsing methods have been 
well studied and many powerful models have 
been proposed. It will be helpful to make use of 
English parsing results. In the following, we will 
propose a method of bilingual bracketing 
supervised by English parsing.  
Here, English parsing supervised BTG 
means using an English parser?s bracketing 
information as a boundary restriction in the BTG 
language model. But this does not necessitate 
parsing Chinese completely according to the 
same parsing boundary of English. If the English 
parsing structure is totally fixed, it is possible that 
the structure is not linguistically valid for 
Chinese under the formalism of Inversion 
Transduction Grammar. To illustrate this, see the 
example shown in Figure 2.  
If you want to lose weight, you had better eat less bread . 
    	
 
 

 
  
 
eat 
less bread 
VP 
BNP 

   

     

 
 (a) 
VP 
eat/ less/ 
bread/

 
X 
 (b) 
Figure 2  A example of mismatch subtree 
VP 
eat less bread 
 (c) 
 
 
 
        The sub-tree for blacked underlined part of 
English and corresponding Chinese are shown in 
Figure 2(a). We can see that the Chinese 
constituents do not match the English 
counterparts in the English structure. In this case, 
our solution is that: the whole English constituent 
of ?VP? is aligned with the whole Chinese 
correspondence; i.e., ?eat less bread? is matched 
with ?? shown in Figure 2(b). At the 
same time, we give the inner structure matching 
according to ITG regardless of the English 
parsing constraint. An ?X? tag is introduced to 
indicate that the sub-bilingual-parsing-tree is not 
consistent with the given English sub-tree. Our 
result can also be understood as a flattened 
bilingual parsing tree as shown in Figure 2(c). 
This means that when the bilingual constituents 
couldn?t match in the small syntactic structure, 
we will match them in a larger structure. 
        The main idea is that the given English 
parser is only used as a boundary constraint for 
bilingual parsing. When the constraint is 
incompatible with the bilingual model ITG, we 
use ITG as the default result. This process 
enables parsing to go on regardless of some 
failures in matching. 
We heuristically define a constraint function 
Fe(s, t) to denote the English boundary constraint, 
where s is the beginning position and t is the end. 
There are three cases of structure matching: 
violate match, exact match and inside match. 
Violate match means the bilingual parsing 
conflicts with the given English bracketing 
boundary. For example, given the following 
English bracketing result (8), (1,2), (1,3), (2,3), 
(2,4) etc. are Violate matches. We assign a 
minimum Fe(s, t) (0.0001 at present) to prevent 
the structure match from being chosen when an 
alternative match is available. Exact match 
means the match falls exactly on the English 
parsing boundary, and we assign a high Fe(s, t) 
value (10 at present) to emphasize it. (1,6), (2,5), 
(3,5) are examples. (3,4), (4,5) are examples of 
inside match, and the value 1 is assigned to these 
Fe(s, t) functions. 
(8) [She/1 [is/2 [a/3 lovely/4 girl/5] ] ./6]    
Let the input English and Chinese sentences 
be Tee ,...1  and Vcc ,...1 . As an abbreviation we 
write tse ...  for the sequence of words 
tss eee ..., ,21 ++ , and similarly write vuc ... . The local 
optimization function =),,,( vuts?  
]/[max
.... vuts ceP denotes the maximum probability 
of sub-parsing-tree of node q and that both the 
sub-string tse ...  and vuc ...  derive from node q. 
Thus, the best parser has the 
probability ),0,,0( VT? . ),,,( vuts? is calculated as 
the maximum probability combination of all 
possible sub-tree combinations(Wu 1995). To 
insert English parsing constraints in bilingual 
parsing, we integrate the constraint function Fe(s, 
t) into the local optimization function.   
Computation of the local optimization function is 
then modified as given below:  
.),,,(),,,(),(max),,,(
,),,,(),,,(),(max),,,(
,)],,,(),,,,(max[),,,(
0))(())((
0))(())((
[]
[]
UutSvUSstsFvuts
vUtSUuSstsFvuts
vutsvutsvuts
e
UvuUStsS
vUu
tSs
e
UvuUStsS
vUu
tSs
???
???
???
???+??
??
??
<>
???+??
??
??
<>
=
=
=
 
    Initialization is as follows : 
V1,1),/(
V1,1),/(
V1,1),/(
,1,,
,,,1
,1,,1
????=
????=
????=
?
?
??
vTtceb
vTteeb
vTtceb
vvvtt
tvvtt
vtvvtt
?
?
?
     
where, T ,V is the length of English and Chinese 
sentence respectively. )/( vt ceb is the probability 
of translating English word te  into Chinese word 
vc . A minimal probability can be assigned to 
empty word alignment b( eet / ) and b( vce / ). 
The optimal bilingual parsing tree for a 
given sentence-pair can be computed using  
dynamic programming (DP) algorithm(Wu 1997). 
Using the standard SBTG local optimization 
fuction, the obtained bilingual parsing result for 
the given sentence-pair(4) is shown as example 
(5); when using the above modified local 
optimization function, the parsing result is that 
shown as example (7). Comparing the two results, 
we can see that by intergrating English parsing 
constraints into BTG, the bilingual parsing 
becomes more grammatical. Our experiments 
showed that this English parsing supervised BTG 
would improve the accuracy of bilingual 
bracketing by nearly 20% (L? 2001b). 
The obtained bilingual parsing tree is in the 
normal form of ITG, that is each node in the tree 
is either a lexical node or a binary-fanout 
nonterminal node. We can combine the subtree to 
restore the fanout flexibility using the production 
characters [[AA]A]=[A[AA]]=[AAA] and 
<<AA>A>= <A<AA>>=<AAA>. The combining 
operation could not cross the given English 
parisng boundary.  
3 Chinese bracketing knowledge extraction 
Table 1 shows some bilingual bracketing 
examples obtained using the above method. To 
understand easily, we give the tree form of the 
first example in Figure 3(a). The leaf node is the 
aligned words of the two languages and their 
POS tag categories. These POS tags are 
generated from an English and a Chinese POS 
tagger respectively. The English POS tag and 
phrase tag set are the same as those of the Penn 
Tree Bank (Marcus 1993) and the Chinse POS 
tag set please refer to the web site: 
http://mtlab.hit.edu.cn. The nonterminal node are 
labeled using English sub-tree tags. 
Based on the bilingual parsing result, it is 
easy to extract the Chinese bracketing structure 
according to the Inversion Transduction 
Grammar. For the normal node, the Chinese text 
is traversed in depth-first left to right order, but 
for an inverted node (indicated by a horizontal 
line in the parsing tree or indicated by a <> 
notation in bracketing expression), the right 
sub-tree is traversed before the left. Thus, the 
Chinese parsing tree corresponding to Figure 3(a) 
is shown in Figure 3(b). The nonterminal labels 
are derived from the English sub-tree. The 
extracted Chinese bracketing results from Table1  
Table 1  Bilingual bracketing examples 
1. [<Mr.(NNP)/(nc) Chen(NNP)/(nx) >BNP [is (VBZ) /(vx) < [the(ART)/e representative(NN)/(ng)]BNP 
<of (IN) /(usde) [our (PRP$)/	(r) company(NN)/
(ng)]BNP >PP >NP ]VP .(.)/(wj) ]S 
2. [Spring(NN)/(t) [is(VBZ)/(vx) <[the(ART)/e first(JJ)/(m) e/(q) season(NN)/(ng) ]BNP <in(IN)/
(f) [a(ART)/(m) year(NN)/(q) ]BNP >PP >X ]VP .(.)/(wj) ]S 
3. [[The(ART)/e window(NN)/(ng)]BNP [is/e/VBZ <[e/(d) narrower(JJR)/Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 125?128,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Statistical Machine Translation Model Based on a Synthetic
Synchronous Grammar
Hongfei Jiang, Muyun Yang, Tiejun Zhao, Sheng Li and Bo Wang
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,ymy,tjzhao,lisheng,bowang}@mtlab.hit.edu.cn
Abstract
Recently, various synchronous grammars
are proposed for syntax-based machine
translation, e.g. synchronous context-free
grammar and synchronous tree (sequence)
substitution grammar, either purely for-
mal or linguistically motivated. Aim-
ing at combining the strengths of differ-
ent grammars, we describes a synthetic
synchronous grammar (SSG), which ten-
tatively in this paper, integrates a syn-
chronous context-free grammar (SCFG)
and a synchronous tree sequence substitu-
tion grammar (STSSG) for statistical ma-
chine translation. The experimental re-
sults on NIST MT05 Chinese-to-English
test set show that the SSG based transla-
tion system achieves significant improve-
ment over three baseline systems.
1 Introduction
The use of various synchronous grammar based
formalisms has been a trend for statistical ma-
chine translation (SMT) (Wu, 1997; Eisner, 2003;
Galley et al, 2006; Chiang, 2007; Zhang et al,
2008). The grammar formalism determines the in-
trinsic capacities and computational efficiency of
the SMT systems.
To evaluate the capacity of a grammar formal-
ism, two factors, i.e. generative power and expres-
sive power are usually considered (Su and Chang,
1990). The generative power refers to the abil-
ity to generate the strings of the language, and
the expressive power to the ability to describe the
same language with fewer or no extra ambigui-
ties. For the current synchronous grammars based
SMT, to some extent, the generalization ability of
the grammar rules (the usability of the rules for the
new sentences) can be considered as a kind of the
generative power of the grammar and the disam-
biguition ability to the rule candidates can be con-
sidered as an embodiment of expressive power.
However, the generalization ability and the dis-
ambiguition ability often contradict each other in
practice such that various grammar formalisms
in SMT are actually different trade-off be-
tween them. For instance, in our investiga-
tions for SMT (Section 3.1), the Formally SCFG
based hierarchical phrase-based model (here-
inafter FSCFG) (Chiang, 2007) has a better gen-
eralization capability than a Linguistically moti-
vated STSSG based model (hereinafter LSTSSG)
(Zhang et al, 2008), with 5% rules of the former
matched by NIST05 test set while only 3.5% rules
of the latter matched by the same test set. How-
ever, from expressiveness point of view, the for-
mer usually results in more ambiguities than the
latter.
To combine the strengths of different syn-
chronous grammars, this paper proposes a statisti-
cal machine translation model based on a synthetic
synchronous grammar (SSG) which syncretizes
FSCFG and LSTSSG. Moreover, it is noteworthy
that, from the combination point of view, our pro-
posed scheme can be considered as a novel system
combination method which goes beyond the ex-
isting post-decoding style combination of N -best
hypotheses from different systems.
2 The Translation Model Based on the
Synthetic Synchronous Grammar
2.1 The Synthetic Synchronous Grammar
Formally, the proposed Synthetic Synchronous
Grammar (SSG) is a tuple
G = ??
s
,?
t
, N
s
, N
t
, X,P?
where ?
s
(?
t
) is the alphabet set of source (target)
terminals, namely the vocabulary; N
s
(N
t
) is the
alphabet set of source (target) non-terminals, such
125
? ? ???
Figure 1: A syntax tree pair example. Dotted lines
stands for the word alignments.
as the POS tags and the syntax labels; X repre-
sents the special nonterminal label in FSCFG; and
P is the grammar rule set which is the core part of
a grammar. Every rule r in P is as:
r = ??, ?,A
NT
, A
T
, ???
where ? ? [{X}, N
s
,?
s
]
+
is a sequence of one or
more source words in ?
s
and nonterminals sym-
bols in [{X}, N
s
];? ? [{X}, N
t
,?
t
]
+
is a se-
quence of one or more target words in ?
t
and non-
terminals symbols in [{X}, N
t
]; A
T
is a many-to-
many corresponding set which includes the align-
ments between the terminal leaf nodes from source
and target side, and A
NT
is a one-to-one corre-
sponding set which includes the synchronizing re-
lations between the non-terminal leaf nodes from
source and target side; ?? contains feature values
associated with each rule.
Through this formalization, we can see that
FSCFG rules and LSTSSG rules are both in-
cluded. However, we should point out that the
rules with mixture of X non-terminals and syn-
tactic non-terminals are not included in our cur-
rent implementation despite that they are legal
under the proposed formalism. The rule extrac-
tion in current implementation can be considered
as a combination of the ones in (Chiang, 2007)
and (Zhang et al, 2008). Given the sentence pair
in Figure 1, some SSG rules can be extracted as
illustrated in Figure 2.
2.2 The SSG-based Translation Model
The translation in our SSG-based translation
model can be treated as a SSG derivation. A
derivation consists of a sequence of grammar rule
applications. To model the derivations as a latent
variable, we define the conditional probability dis-
tribution over the target translation e and the cor-
Input: A source parse tree T (f
J
1
)
Output: A target translation e?
for u := 0 to J ? 1 do
for v := 1 to J ? u do
foreach rule r = ??, ?,A
NT
, A
T
, ??? spanning
[v, v + u] do
if A
NT
of r is empty then
Add r into H[v, v + u];
end
else
Substitute the non-terminal leaf node pair
(N
src
, N
tgt
) with the hypotheses in the
hypotheses stack corresponding with N
src
?s
span iteratively.
end
end
end
end
Output the 1-best hypothesis in H[1, J] as the final translation.
Figure 3: The pseudocode for the decoding.
responding derivation d of a given source sentence
f as
(1) p
?
(d, e|f) =
exp
?
k
?
k
H
k
(d, e, f)
?
?
(f)
where H
k
is a feature function ,?
k
is the corre-
sponding feature weight and ?
?
(f) is a normal-
ization factor for each derivation of f. The main
challenge of SSG-based model is how to distin-
guish and weight the different kinds of derivations
. For a simple illustration, using the rules listed in
Figure 2, three derivations can be produced for the
sentence pair in Figure 1 by the proposed model:
d
1
= (R
4
, R
1
, R
2
)
d
2
= (R
6
, R
7
, R
8
)
d
3
= (R
4
, R
7
, R
2
)
All of them are SSG derivations while d
1
is also a
FSCFG derivation, d
2
is also a LSTSSG deriva-
tion. Ideally, the model is supposed to be able
to weight them differently and to prefer the better
derivation, which deserves intensive study. Some
sophisticated features can be designed for this is-
sue. For example, some features related with
structure richness and grammar consistency
1
of a
derivation should be designed to distinguish the
derivations involved various heterogeneous rule
applications. For the page limit and the fair com-
parison, we only adopt the conventional features
as in (Zhang et al, 2008) in our current implemen-
tation.
1
This relates with reviewers? questions: ?can a rule ex-
pecting an NN accept an X?? and ?. . . the interaction between
the two typed of rules . . . ?. In our study in progress, we
would design some features to distinguish the derivation steps
which fulfill the expectation or not, to measure how much
heterogeneous rules are applied in a derivation and so on.
126
R6
1?
BA
VV[2]NN[1]
1
VB[2] NP[1]?
PN
to me
TO PRP
PP
1
R7
penthe
DT NN
NP
??
NN
1
R4 Give 1? 1 X[1] X[2]X[2]? X[1] R5 X[1]X[1] ? 2 the pen 1 to 2me1??
R1 penthe 1?? 1 R3 theGive 2 pen 1? 2?? 1R2 to me 1? 1
R8
?
VV
Give
VB
11
Figure 2: Some synthetic synchronous grammar rules can be extracted from the sentence pair in Figure
1. R
1
-R
3
are bilingual phrase rules, R
4
-R
5
are FSCFG rules and R
6
-R
8
are LSTSSG rules.
2.3 Decoding
For efficiency, our model approximately search for
the single ?best? derivation using beam search as
(2) (
?
e,
?
d) = argmax
e,d
{
?
k
?
k
h
k
(d, e, f)
}
.
The major challenge for such a SSG-based de-
coder is how to apply the heterogeneous rules in a
derivation. For example, (Chiang, 2007) adopts a
CKY style span-based decoding while (Liu et al,
2006) applies a linguistically syntax node based
bottom-up decoding, which are difficult to inte-
grate. Fortunately, our current SSG syncretizes
FSCFG and LSTSSG. And the conventional de-
codings of both FSCFG and LSTSSG are span-
based expansion. Thus, it would be a natural way
for our SSG-based decoder to conduct a span-
based beam search. The search procedure is given
by the pseudocode in Figure 3. A hypotheses
stack H[i, j] (similar to the ?chart cell? in CKY
parsing) is arranged for each span [i, j] for stor-
ing the translation hypotheses. The hypotheses
stacks are ordered such that every span is trans-
lated after its possible antecedents: smaller spans
before larger spans. For translating each span
[i, j], the decoder traverses each usable rule r =
??, ?,A
NT
, A
T
, ???. If there is no nonterminal
leaf node in r, the target side ? will be added into
H[i, j] as the candidate hypothesis. Otherwise, the
nonterminal leaf nodes in r should be substituted
iteratively by the corresponding hypotheses until
all nonterminal leaf nodes are processed. The key
feature of our decoder is that the derivations are
based on synthetic grammar, so that one derivation
may consist of applications of heterogeneous rules
(Please see d
3
in Section 2.2 as a simple demon-
stration).
3 Experiments and Discussions
Our system, named HITREE, is implemented in
standard C++ and STL. In this section we report
Extracted(k) Scored(k)(S/E%) Filtered(k)(F/S%)
BP 11,137 4,613(41.4%) 323(0.5%)
LSTSSG 45,580 28,497(62.5%) 984(3.5%)
FSCFG 59,339 25,520(43.0%) 1,266(5.0%)
HITREE 93,782 49,404(52.7%) 1,927(3.9%)
Table 1: The statistics of the counts of the rules in
different phases. ?k? means one thousand.
on experiments with Chinese-to-English transla-
tion base on it. We used FBIS Chinese-to-English
parallel corpora (7.2M+9.2M words) as the train-
ing data. We also used SRI Language Model-
ing Toolkit to train a 4-gram language model on
the Xinhua portion of the English Gigaword cor-
pus(181M words). NIST MT2002 test set is used
as the development set. The NIST MT2005 test
set is used as the test set. The evaluation met-
ric is case-sensitive BLEU4. For significant test,
we used Zhang?s implementation (Zhang et al,
2004)(confidence level of 95%). For comparisons,
we used the following three baseline systems:
LSTSSG An in-house implementation of linguis-
tically motivated STSSG based model similar
to (Zhang et al, 2008).
FSCFG An in-house implementation of purely
formally SCFG based model similar to (Chiang,
2007).
MBR We use an in-house combination system
which is an implementation of a classic sentence
level combination method based on the Minimum
Bayes Risk (MBR) decoding (Kumar and Byrne,
2004).
3.1 Statistics of Rule Numbers in Different
Phases
Table 1 summarizes the statistics of the rules for
different models in three phases: after extrac-
tion (Extracted), after scoring(Scored), and af-
ter filtering (Filtered) (filtered by NIST05 test
set just, similar to the filtering step in phrase-
based SMT system). In Extracted phase, FSCFG
127
ID System BLEU4 #of used rules(k)
1 LSTSSG 0.2659?0.0043 984
2 FSCFG 0.2613?0.0045 1,266
3 HITREE 0.2730?0.0045 1,927
4 MBR(1,2) 0.2685?0.0044 ?
Table 2: The Comparison of LSTSSG, FSCFG
,HITREE and the MBR.
has obvious more rules than LSTSSG. However,
in Scored phase, this situation reverses. Inter-
estingly, the situation reverses again in Filtered
phase. The reasons for these phenomenons are
that FSCFG abstract rules involves high-degree
generalization. Each FSCFG abstract rule aver-
agely have several duplicates
2
in the extracted rule
set. Then, the duplicates will be discarded dur-
ing scoring. However, due to the high-degree gen-
eralization , the FSCFG abstract rules are more
likely to be matched by the test sentences. Con-
trastively, LSTSSG rules have more diversified
structures and thus weaker generalization capabil-
ity than FSCFG rules. From the ratios of two tran-
sition states, Table 1 indicates that HITREE can
be considered as compromise of FSCFG between
LSTSSG.
3.2 Overall Performances
The performance comparison results are presented
in Table 2. The experimental results show that
the SSG-based model (HITREE) achieves signifi-
cant improvements over the models based on the
two isolated grammars: FSCFG and LSTSSG
(both p < 0.001). From combination point of
view, the newly proposed model can be consid-
ered as a novel method going beyond the con-
ventional post-decoding style combination meth-
ods. The baseline Minimum Bayes Risk com-
bination of LSTSSG based model and FSCFG
based model (MBR(1, 2)) obtains significant im-
provements over both candidate models (both p <
0.001). Meanwhile, the experimental results show
that the proposed model outperforms MBR(1, 2)
significantly (p < 0.001). These preliminary re-
sults indicate that the proposed SSG-based model
is rather promising and it may serve as an alterna-
tive, if not superior, to current combination meth-
ods.
4 Conclusions
To combine the strengths of different gram-
mars, this paper proposes a statistical machine
2
Rules with identical source side and target side are du-
plicated.
translation model based on a synthetic syn-
chronous grammar (SSG) which syncretizes a
purely formal synchronous context-free gram-
mar (FSCFG) and a linguistically motivated syn-
chronous tree sequence substitution grammar
(LSTSSG). Experimental results show that SSG-
based model achieves significant improvements
over the FSCFG-based model and LSTSSG-based
model.
In the future work, we would like to verify
the effectiveness of the proposed model on vari-
ous datasets and to design more sophisticated fea-
tures. Furthermore, the integrations of more dif-
ferent kinds of synchronous grammars for statisti-
cal machine translation will be investigated.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In computational linguistics, 33(2).
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL 2003.
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-
rich syntactic translation models In Proceedings of
ACL-COLING.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In HLT-
04.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine transla-
tion. In Proceedings of ACL-COLING.
Keh-Yin Su and Jing-Shin Chang. 1990. Some key
Issues in Designing Machine Translation Systems.
Machine Translation, 5(4):265-300.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of LREC 2004, pages 2051-2054.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li,
Chew Lim Tan and Sheng Li. 2008. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT.
128
Statistics Based Hybrid Approach to Chinese Base Phrase Identification 
Tie-jun ZHAO, Mu-yun YANG~ Fang LIU, Jian-min YAO, Hao YU 
Department of Computer Science and Engineering, Harbin Institute of Technology 
{tjzaho, )may, flu.fang, james, yu} @mtlab.hit.edu.en 
ABSTRACT 
This paper extends the base noun 
phrase(BNP) identification i to a research 
on Chinese base phrase identification. ARer 
briefly introducing some basic concepts on 
Chinese base phrase, this paper presents a
statistics based hybrid model for identifying 
7 types of Chinese base phrases in view. 
Experiments how the efficiency of the 
proposed method in simplifying sentence 
structure. Significance of the research es in 
it provides a solid foundation for the 
Chinese parser. 
Keywords: Chinese base phrase 
identification, parsing, statistical model 
1 Introduction 
Decomposing syntactic analysis into 
several phases o as to decrease its difficulty 
is a new stream in NIP research. The 
successful POS tagging has encouraged 
researchers to explore further possibility for 
resolving sub-problems in parsing(Zhou, et 
al, 1999). The typical examples are the 
recognition of BaseNP in English and 
Chinese. 
In English BNP (base noun phrase) is 
defined as simple and non-nesting noun 
phrases, i.e. noun phrases that do not contain 
other noun phrase descendants (Church, 
1988). After that researches on BNP 
identification reports promising results for 
such task in English. Observing that the 
Chinese BNP is different form English, 
(Zhao & Huang, 1999) puts forward the 
definition of Chinese BNP in terms of 
combination of determinative modifier and 
head noun. According to them a BNP in 
Chinese can be recursively defined as: 
BaseNP ::= Determinative modifier + 
Noun I Nominalized verb(NIO 
Determinative modifier ::= Adjective I
Differentiable Adjective(DA) I Verb I Noun I 
Location I String l Numeral + Classifier 
Inspired by these researches, we extend 
the concept of BNP to Base Phrase in 
Chinese. It is based on such knowledge that 
there are many structures, not only NP, in 
which the trivial components closely attach 
to their central words and constitute a basic 
phrase in a Chinese sentence. Obviously, 
resolving all these base phrases will greatly 
benefit Chinese parser by reliving it from 
some pre-processing (though non-trivial) 
and enable it focus on the most subtle 
syntactic structures. 
Since the whole system of Chinese base 
phrase is still under discussing, this paper 
just presents some tentative research 
achievements on statistics based hybrid 
model to Chinese base phrase identification. 
For the 7 types we considered at present, our 
algorithm turns out promising results and 
smoothes the way for a better Chinese 
parser. 
2 Statistics Based Hybrid Approach to 
Chinese Base Phrase Identification 
2.1 Concepts and Defmitions 
In addition to BNP, constituents of 
many local structure in Chinese centers 
around a core word with certain fixed POS 
sequences. Therefore their identification is
slightly different from parsing in that it 
bears relatively simple phenomenon. Like 
BNP identification, identification of these 
phenomena before parsing will provide a 
simpler sequence for parser, and thus 
deserves a separate r search. 
CutTenfly, we are considering 7 Chinese 
base phrases in our research, namely base 
adjective phrase(BADJP), base adverbial 
phrase (BADVP), base noun phrase (BNP), 
73 
base temporal phrase (BTN), base location 
phrase (BNS), base verb phrase (BVP) and 
base quantity phrase (BMP) Though 
theoretically definitions for these base 
phrases are still unavailable, Appendix I lists 
the preliminary illustrations for them in 
BNF format (necessary account for POS 
annotation can also be found).. 
To frame the identification of Chinese 
base phrases, we fm'ther develop the 
following concepts: 
Definition 1: Chinese based phrases are 
recognized as atomic parts of a sentence 
beyond words that posses certain functions 
and meanings. A base phrase may consist of 
words or other base phrases, but its 
constituents, in turn, should not contain any 
base phrases. 
Definition 2: Base phrase tag is the 
token representing the syntactic function of 
the phrase. At present, base tag either falls in 
one of the 7 Chinese base phrases we are 
considering or not: 
Phrase-Tag ::= BADJP I BADVP I BNP I 
Br  r l Bm I BrP I BMP I lVULL 
Definition 3: Boundary tag denotes the 
possible relative position of a word to a base 
phrase. A boundary tag for a gfven word is 
either L( left boundary of a base phrase), 
R( right boundary of a ), I(inside a base 
phrase) or O(outside the base phrase). 
2.2 Duple Based HMM Parser 
Based on above definitions, we could, 
in view of Wojciech's proposal \[Wojeieeh and 
Thorsten, 1998\], interpret the parsing of 
Chinese base phrases as the following: 
Suppose the input as a sequence of POS 
annotations T= (to, ....... t , , ) .  The task is to 
find RC, a most possible sequence of duples 
formed by base phrase tags and boundary 
tags, among the POS sequence T.
RC = (<ro, co > ........ <rn, Cn>), 
in whil~h ri ( l  <i< =n )indicates the boundary 
tags, ci represents he base phrase tags. 
To go along with the POS tagger 
developed previously by us, we first think of 
preserving HMM (hidden Markov Model) 
for parsing Chinese base phrases. Thus the 
following formula is usually?at hand: 
RC = arg max p(RC I T) 
= arg max p(RC)*  p (T IRC)  
p(T) 
For a given sequence of T, this formula 
can be transformed into: 
RC = arg max p(RC IT) 
= arg max p(RC)*p(T  \ [RC)  
Essentially this model could be 
established through bigram or tri-gram 
statistical training by a annotated corpus. In 
practice, we just build our model from 
l O, O00 manual annotated sentences with 
common bi-gram training: 
p(RC p(RC , IRC  ,_,) 
i=1 
p(T  I RC ) = 1FI p (T i  I RC i) 
i= l  
In realization, a Viterbi algorithm is 
adopted to search the best path. An open test 
on additional 1000 sentences i  performed to 
check its accuracy. Results are shown in 
Tablel(note precision is calculated by 
word'k 
Precision 
for R 
Close 85.7% Test 
Open 82.4% 
Test 
Precision Precision for Both for .C RandC 
87.5% 79.0% 
85.1% 74.7% 
Table 1. Results for Duple Based HMM 
2.3 Triple Based MM Exploiting 
Linguistic Information 
Although results shown in Table 1 i s  
encouraging enough for research purposes, 
it is still lies a long way for practical 
Chinese parser we are aiming at. Reasons 
for errors may be account by too 
coarse-grained information provided by RC. 
Observing the fact that the Chinese base 
phrase occurs more frequently with some 
fixed patterns, i.e. some frozen POS chains, 
we decide to improved our previous model 
by emphasizing the contribution given by 
POS information. 
Adding t denoting POS in the duple (r, 
74 
c), we develop a triple in the form of (t,r,e) 
for the calculation of a node. Naturally, the 
new model is changed into a MM (Markov 
model) as: 
TRC = arg max p(TRC ) 
= arg max I~  p(TRC i I TRC i - 1) 
To train this model, we still using a 
bi-gram model. Applying the same corpus 
and tests described above, we got the 
performance of triple based MM identifier 
for Chinese base phrases (see Table 2). 
Precision Precision Precision 
for R ~rC 
89.2% 91 .5% 84.6% 
88.4% 89.9% 83% 
Close 
Open 
for Both 
R and C 
Table 2. Result for Triple Based MM 
2.4 Further  Improvement Through TBED 
Learning 
Like other statistical models, the above 
model, whether duple based or triple based, 
both seem to reach an accuracy ceiling after 
enlarging training set to 12, 000 or so. To 
cover the remaining accuracy, we apply the 
transformation-based error driven (TBED) 
learning strategy described in \[Brill, 1992\] 
to acquired esired rules. 
In our module, some initial rules are 
first designed as compensation of statistical 
model. Applying these rules will cause new 
mistakes as well as make correct 
identifications. Then the module will 
compare the processed texts with training 
sentences, generate new rules according to 
pre-defmed actions and update its rule bank 
after evaluation (see Fig 1.). 
I I Compare and Rules Passing 
Generate New Rules - - - - !~ Evaluation I 
Tt 
TextTraining \] TextPr?eessed \]
Identifier 
'T 
Input Text 
Figure 1. TBED Learning Module 
The dotted line in fig 2. will stop 
functioning if pre-set accuracy is reached by 
the identifier for the Chinese base phrase. 
Evaluation of new rules is based on an 
greedy algorithm: only rule with max 
contribution (max correction and rain error) 
will be added. Design of rule generation 
(pre-defined actions) is similar to those 
described in \[Brill, 1992\]. 
Table 3 shows a significant 
improvement after applying rules obtained 
through TBED learner. It is also the final 
performance of the proposed Chinese base 
phrase identification model. 
Precision Precision Precision for Both for R for C Rand C 
91.2% 92.8% 89% 
90.4% 91.1% 87.1% 
Close 
open 
Table 3. Results after TBED Module 
3 Conclusions and Discussions 
We have accomplished preliminary 
expedments on identification of various 
types of base phrases defined in this paper. 
The data shown in last seetion prove that our 
method generates atisfactory results for 
75 
Chinese base phrase identification. The 
overall process of our method is outlined the 
following figure. 
Input Chinese Sentences 
after Sengmentation and 
POS tagging 
~ Converted into Nodes 
to Be Parsed 
Triple Based Bi-gram 
MM with Viterbi 
Algorithm 
TBED Based 
Correction 
T" ' Output 
\ 
Fig 2. Processing ofChinese Based Phrase Identification 
However, the 7 types Chinese base 
phrases we have proposed are far l~om 
perfection. Even what we have proposed for 
the 7 phrases is still under test. Further 
improvement will focus on two aspects: one 
is to discuss and add new base phrase for a 
broader coverage; the other is to define, 
theoretically or empirically, the Chinese 
base phrases with more strict constraints. Of 
course, new techniques to improved the 
accuracy of statistical model are the constant 
aim of our research. 
To sum up, Chinese base phrase 
identification will reduce complexity of a 
Chinese parser. The successful idemifieation 
of the 7 base phrases clearly simplifies the 
structure of the sentence. We expect hat the 
research described in this paper will lay a 
solid foundation for a high-accuracy 
Chinese parser. 
22(2): pp141-146 
\[Zhou, et al 1999\] Zhou Qiang, Sun 
Mao-Song, Huang Chang-Ning, Chunk 
parsing scheme for Chinese sentences, 
Chinese J. Computer, 22(11): pp1159-1165 
Reference 
\[Church, 1988\] K. Church, A stochastic 
parts program and noun phrase parser for 
unrestricted text, In: Proc. of Second 
Conference on Applied Natural Language 
Processing, 1988 
\[Wojciech and Thorsten, 1998\] Wojciech Skut and 
Thorsten Brants, Chunk Tagger, Statistical 
Recongnition of Noun Phrases, In ESSLLI-98 
Workshop on Automated Acquisition of Syntax 
and Parsing, Saarbrvcken, 1998. 
\[Zhao & Huang, 1999\] Zhao Jun and Huang 
Chang-Ning, The model for Chinese baseNP 
structure analysis, Chinese J. Computer, 
76 
Appendix Illustration of 7 Chinese Base 
Phrases in BNF 
The patterns listed here are far from 
complete (even for the 7 phrases 
themselves). Theoretical definition is 
beyond this paper and what we provide here 
is actually stage results of expert 
observation and linguistic abstraction. 
BADJP ::= d++a \[ d+BADJP \] a + I a+BADJP 
\[ BADVP+a I BADVP+BADJP 
BADVP ::= a+usdi(:~) I d+usdi I vg+usdi I 
BADJP+usdi IBADVP+usdi IBMP+usdi 
BMP ::= m + \[ m*+q* \[ m+q+m \[ d+m+q \] 
f+m+q \[ r+m+q I BMP ? 
BNP ::= a+n I a+usde(~)+n I a+usde+BNP I 
a+BNP \]b+n \] b+usde+n I b+usde+BNP I 
b+BNP I d+usde+n I f+n I f+usde+n I f+BNP 
1 m+n I m+BNP I n+ I n+usde+n I
n+usde+BNP I n+usde+BMP I n+BNP I q+n 
I q+BNP I r+a+n I r+m+n I r+n I r+usde+n I 
r+usde+BNP \[ r+BNP I s+n I s+usde+n \[
s+usde+BNP I t+nl t+usde+n \[ t+usde+BNP 
I vg +usde+n I vg+usde+BNP I BADJP+n 
BADJP+usde+n \] BADJP+usde+BNP 
BADJP+BNP \[ BMP+n \[ BMP+usde+n 
BMP+usde+BNP \[ BMP+BNP \[ BNP+n 
BNP+usde+n \[ BNP+usde+BNP 
BNP+usde+BMP \[ BNP+BNP 
BNS+usde+n \[ BNS+usde+BNP 
BNS+BNP I BTN+usde+n 
BTN+usde+BNP \[ BVP+usde+n 
BVP+usde+BNP 
BNS ::= a+nd I m+nd I n+s I r+nd I 
n+usde+f I n+usde+nd I n+usde+s I 
n+usde+BNS I nd + I r+usde+nd \[ r+usde+s I 
s+usde+nd I s+usde+BNS I BNP BNS I 
BNS + 
BTN ::= a+t I m+t I r+t I t+ I t+usd~t  I 
BMP+t I BTN+t I BNP+usde+t 
BVP ::= a+vg I d+vg I vg+d+a I vg+d+vq I 
vg+d+vb I vg+usdf(~)+a I vg+usdf+d I 
vg+usdf+vq \[ vg+usdf+u I vg+usdf+BADJP I 
vg+ut I vg+vb I vg+ut+vq I vq+vg I vq+BVP 
\] vz+vg I vz+BVP I BADJP+vg I 
BADVP+vg \[ BADVP+BVP I BVP+ut I 
BVP+vq I BVP+BVP 
Symbol 
a 
d 
Part-Of-Speech 
Adjective 
Adverb 
TemporaYspacial 
position word 
Examples 
~(beaut i fu l ) ,  ~( romant ic )  
~(very), ~(s t i l l )  
~(in), _k(on), ~N(between) 
m numeral --(one), ~(two), -~(three) 
n noun ~ ~ (people), ~ ~I~  (tomato), 
"bl-~JL(computer) 
nd Name of place ~(Be i j ing) ,  I I~(Harb in ) ,  
~.\]t~(New York) 
q classifier \]\]~(flock), +(NULL) 
r pronoun '~'~(you), ~(I, me), ~(he, him) 
s location oun I~.(around), ~:gb(outside) 
t time noun ~;~(yesterday), --L~ (July) 
ut tense auxiliary ~,T,~c_(NULL) 
vb Complemental verb ~,~_t(NULL) 
vg common verb ~ll~(know), ~( long  for) 
vq directional verb ~,T  ~i~(NULL) 
vz modal verb ~I ~(can), )~(shou ld )  
Table for POS symbols used in Appendix 
77 
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 169?172,
Prague, June 2007. c?2007 Association for Computational Linguistics
HIT-WSD: Using Search Engine for Multilingual Chinese-English 
Lexical Sample Task 
PengYuan Liu, TieJun Zhao, MuYun Yang 
MOE-MS Key Laboratory of NLP & Speech, HIT, School of Computer Science and 
Technology, Harbin Institute of Technology, Harbin, Heilongjiang, China 
{pyliu,tjzhao,ymy}@mtlab.hit.edu.cn 
 
 
Abstract 
We have participated in the Multilingual 
Chinese-English Lexical Sample Task of 
SemEval-2007. Our system disambiguates 
senses of Chinese words and finds the 
correct translation in English by using the 
web as WSD knowledge source. Since all 
the statistic data is obtained from search 
engine, the method is considered to be 
unsupervised and does not require any 
sense-tagged corpus. 
1 Introduction 
Due to the lack of sense tagged corpora (and the 
difficulty of manually creating them), the 
unsupervised method tries to avoid, or at least to 
reduce, the knowledge acquisition problem, which 
the supervised methods have to deal with. In order 
to tackle the problem of the knowledge acquisition 
bottleneck, we adopted an unsupervised approach 
based on search engine, which does not require any 
sense tagged corpus. 
The majority of methods using the Web often try 
to automatically generate sense tagged corpora 
(Agirre and Martinez 2000;Agirre and Martinez 
2004;Gonzalo et al 2003; Mihalcea and Moldovan 
1999;Santamaria et al 2003). In this paper, we 
experiment with our initial attempt on another 
research trend that uses the Web not for extracting 
training samples but helping disambiguate directly 
during the translation selection process. 
The approach we present here is inspired by 
(Mihalcea and Moldovan 1999;Brill 2003; Rosso 
et al 2005; Dagan et al 2006; McCarthy 2002). 
Suppose that source ambiguous words are apt to 
appear with its target translation on bilingual web 
pages either parallel or non-parallel. Instead of 
searching the source language or target language 
respectively on web, we try to let the search engine 
think in a bilingual style. First, our system gets the 
co-occurrence information of Chinese context and 
its corresponding English context. Then it computes 
association measurements of Chinese context and 
English context in 4 kinds of way. Finally, it 
selects the correct English translation by 
computing the association measurements. 
In view that this is the first international standard 
evaluation to predict the correct English translation 
for ambiguous Chinese word, we built HIT-WSD 
system as our first attempt on disambiguation by using 
bilingual web search and just want to testify validity 
of our method. 
2 HIT-WSD System 
2.1 Disambiguation Process 
HIT-WSD system disambiguates senses of Chinese 
target ambiguous word and finds the correct 
translation in English by searching bilingual 
information on the web. Figure 1 gives the 
flowchart of our proposed approach. Given an 
ambiguous word with a Chinese sentence, we 
easily create its Chinese context. English context 
can be acquired from a Chinese-English dictionary and 
the translation mapping set(offered by the 
Multilingual Chinese-English Lexical Sample 
Task). System puts Chinese context and English 
context as queries on search engine individually 
and collectively. After this step, frequency and co-
occurrence frequency of Chinese context and English 
 
169
 
 
 
 
 
 
 
 
 
 
 
 
 
 
context will be found. Finally, our system selects the 
most probable English translation by computing 
association measurements. 
Figure 2 gives an example of how the proposed 
approach selects English translations of the 
Chinese ambiguous word ???/dongyao? given 
the sentence and its translation mapping set. This 
instance comes from the training data of Multilin-
gual Chinese-English Lexical Sample Task of Sem-
eval2007. According to the translation mapping set,  
Chinese target word ??? /dongyao? has two 
English Translations: shake and vacillate.  
English Context Candidates set is the 
translations set of the Chinese context. System uses 
translation mapping set to translate Chinese target 
ambiguous word and uses an Chinese-English 
dictionary to translate other words in Chinese 
context. English Context Candidates set could be 
any combination of translations and each 
combination could be selected as the English context. 
 After getting the Chinese context and English 
context, we put them as queries to search engine 
and extract page counts (which can be considered 
as frequency) which search engine returned.  We 
not only search Chinese context and English 
context individually, but also put them together to 
search engine.  
Association measurements: the Dice coefficient, 
point-wise mutual information, Log Likelihood 
score and? P2 P score are computed in the third phase 
while we got all kinds of statistic results from 
search engine. Finally, we determine the 
translation by simply computing the association 
measurements 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Example of the Chinese ambiguous word 
???/dongyao? selection process 
Figure 1: Flowchart of HIT-WSD System
Chinese sentence,
bilingual mapping
and C-E dictionary 
English 
context 
Search 
Engine
Comparing 
association 
measurements 
English 
Translation
Frequency and Co-
occurrence 
Frequency of 
Chinese context 
and English context Chinese 
context 
 
Chinese Context(CC): ???????? 
English Context Candidates set:  
Shake, shake is, not shake, line shake?/vacillate, 
not vacillate, vacillate is, line vacillate? 
English Context(EC): shake/vacillate 
Putting on Search Engine and getting counts:  
( ) 1880000, ( ) 5450
( ) 113000, ( , ) 77, ( , ) 12CC CC CC
c shake c vacillate
c c shake c vacillate
= =
= = =
Computing association measurements: 
( , )
2 ( , )
(( ( , ) ( )) ( ( , ) ( )
Dice CC shake
c CC shake
c CC shake c shake c CC shake c CC
=
?
+ ? +
2 77
(77 1880000) (77 113000))
7.24 10e
?=
+ ? +
?=  
2 ( , )
(( ( , ) ( )) ( ( , ) ( ))
( , )
CC CC CC
c CC vacillate
c vacillate c vacillate c vacillate c
Dice CC vacillate
?
+ ? +
=
2 12
(12 5450) (12 11300)
3.89 8e
?
+ ? +
= ?=  
Compare and Determine a Translation: 
3.89e-8>7.24e-10, So the answer is vacillate. 
Instance: ???????????????????
??????????<head>??</head>????
?????????? 
Chinese Ambiguous Word: ?? 
Translation Mapping Set: ??-shake/??-vacillate 
Translations of Chinese context in Chinese-English 
dictionary:?/not,?/is,??/line,??/ actualize 
170
2.2  Experiment Settings 
Although the Chinese context can be represented 
with local features, topic features, parts of speech 
and so on, we use sentence segment as Chinese 
context in our experiment system. The sentence 
segment is a window size ? n segment of the 
sentence including the ambiguous words. 
English Context Candidates set could be any 
combination of the translation of words appearing 
in Chinese context. In our experiment system, we 
just choose the translation of the Chinese target 
ambiguous words in the translation mapping set as 
English context. 
We choose googleTP 1 PT and baiduTP 2 PT as our search 
engine, for they are both most widely used for 
English and Chinese language respectively.  
Putting Chinese context and English context as 
queries to the search engine, we will get 
corresponding page counts it returned as figure 2 
shows. 
Four statistical measurements were used in order 
to measure the degree of association of Chinese 
Context (CC) and English Context (EC). CC and 
EC can be seen as two random events occuring in 
the web pages: 
 
1. Point-wise mutual information: 
2( , ) log ( ) ( )
n aMI CC EC
a b a c
?= + ? +  (1) 
2. DICE coefficient: 
2( , )
( ) ( )
aDICE CC EC
a b a c
?= + ? +   (2) 
3. ? P2 P score: 
2
2 ( )X ( , )
( ) ( ) ( ) ( )
n a d b cCC EC
a b a c b d c d
? ? ? ?= + ? + ? + ? +  (3) 
4. Log Likelihood score: 
( , ) 2 ( log
( ) ( )
log log
( ) ( ) ( ) ( )
log )
( ) ( )
n aLL CC EC a
a b a c
n b n cb c
a b b d c d a c
n dd
c d b d
?= ? ? + ? +
? ?+ ? + ?+ ? + + ? +
?+ ? + ? +
Here is the meaning of a, b, c, d and n. 
                                                 
TP
1
PT www.google.com. 
TP
2
PT www.baidu.com. 
Table 1:Training data results of Multilingual Chinese-
English Lexical Sample Task 
 Micro-average Macro-average
Our result 0.336898           0.395993 
Baseline (MFS) 0.4053 0.4618 
Table 2:Official results: Multilingual Chinese-English 
Lexical Sample Task 
a: all counts of the web pages which include 
Both CC and EC. 
b: all counts of the web pages which include CC, 
do not include EC. 
c: all counts of the web pages which include EC, 
do not include CC. 
d: all counts of the web pages which include 
neither CC and EC. 
n= a+ b+ c + d 
We applied our method to the training data of 
Multilingual Chinese-English Lexical Sample Task. 
The results are as showed in Table 1. 
Since only one test result can be uploaded for 
one system, our system selects the settings of one 
of the best results. The final settings of our system 
is: window size is [-1, +2], the search engine is 
baidu and the association measurement is Point-
wise mutual information. 
3 Official Results 
In multilingual Chinese-English lexical sample 
task of SemEval-2007, there are 2686 instances in 
training data for 40 Chinese ambiguous words. All 
these ambiguous words are either nouns or verbs. 
Test data consist of 935 untagged instances of the 
same target words. 
The official result of our system in multilingual 
Chinese-English lexical sample task is reported as 
in Table 2. 
 
 
Precision( Micro-average) 
Context Window Size 
Association 
-1,+1 -1,+2 -2,+2 
MI(Baidu) 0.349 0.349 0.339 
XX(Baidu) 0.338 0.344 0.314 
LL(Baidu) 0.315 0.320 0.293 
DICE(Baidu) 0.285 0.295 0.295 
MI(google) 0.334 0.334 0.339 
XX(google) 0.322 0.316 0.316 
LL(google) 0.295 0.306 0.299 
DICE(google) 0.281 0.278 0.272 
(4)
Measurements
171
4 Conclusions 
In SemEval-2007, we participated in Multilingual 
Chinese-English Lexical Sample Task with a fully 
unsupervised system based on bilingual web search. 
Our initial experiment result shows that our system 
fails to reach MFS (Most Familiar Sense) baseline 
due to our method is too simple where search 
queries are formed (just uses simple context 
window and English target translation). Our 
approach is the first attempt so far as we know on 
using bilingual web search for translation selection 
directly. The system is very simple but seemed to 
achieve a not bad performance when considered 
the performance of fully unsupervised systems in 
SENSEVAL-2, SENSEVAL -3 English tasks. 
For future research, we will investigate the 
dependency of bilingual documents, optimize the 
search queries, filter out potential noises and 
combine the different results in order to devise an 
improved method that can utilize bilingual web 
search better. 
 
References 
Agirre, E.and Martinez, D. 2000. Exploring Automatic 
Word Sense Disambiguation with Decision Lists and 
the Web. Proc. of the COLING-2000.  
Agirre, E.and Martinez, D. 2004. Unsupervised word 
sense disambiguation based on automatically 
retrieved examples: The important of bias. Proc. of 
the EMNLP 2004(Barcelona, Spain, July 2004).  
Brill, E. 2003. Processing Natural Language 
Processing without Natural Language Processing. 
Lecture Notes in Computer Science, Vol. 2588. 
Springer-Verlag (2003) 360?369. 
Dagan, I., Glickman, O., Gliozzo, A., Marmorshtein, E. 
and Strapparava, C. 2006. Direct Word Sense 
Matching for lexical substitution. Proceedings of 
ACL/COLING 2006. 
Gonzalo, J., Verdejo, F. and Chugar, I. 2003. The Web 
as a Resource for WSD. 1PstP MEANING Workshop, 
Spain. 
McCarthy, D. 2002. Lexical Substitution as a Task for 
WSD Evaluation. In Proceedings of the ACL 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, Philadelphia, USA. 
Mihalcea, R. and Moldovan, D.I. 1999. An Automatic 
Method for Generating Sense Tagged Corpora. Proc. 
of the 16th National Conf. on Artificial Intelligence. 
AAAI Press. 
Rosso, P., Montes, M., Buscaldi, D., Pancardo, A., and 
Villase, A., 2005. Two Web-based Approaches for 
Noun Sense Disambiguation. Int. Conf. on Comput. 
Linguistics and Intelligent Text Processing, 
CICLing-2005, Springer Verlag, LNCS (3406), 
Mexico D.F., Mexico, pp. 261-273 
Santamaria, C., Gonzalo, J. and Verdejo, F. 2003. 
Automatic Association of WWW Directories to Word 
Senses. Computational Linguistics (2003), Vol. 3, 
Issue 3 ? Special Issue on the Web as Corpus, 485?
502. 
172
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
References Extension for the Automatic Evaluation of MT by             Syntactic Hybridization   Bo Wang, Tiejun Zhao, Muyun Yang, Sheng Li School of Computer Science and Technology Harbin Institute of Technology Harbin, China {bowang,tjzhao,ymy,sl}@mtlab.hit.edu.cn       Abstract Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the au-tomatic evaluation only based on multiple manual references and their syntactic struc-tures. In our approach, the syntactic equiva-lents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long sub-segments of reference sentences. The experi-mental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments. 1 Introduction While human evaluation of machine translation output remains the most reliable method to assess translation quality, it is a costly and time consum-ing process. The development of automatic ma-chine translation evaluation metrics enables the rapid assessment of system output. By providing immediate feedback on the effectiveness of various techniques, these metrics have guided machine translation research and have facilitated rapid ad-vances in the state of the art. In addition, automatic evaluation metrics are useful in comparing the per-formance of multiple machine translation systems 
on a given translation task. Since automatic evalua-tion metrics are meant to serve as a surrogate for human judgments, their quality is determined by how well they correlate with assessors? preferences and how accurately they predicts human judg-ments. Although current methods for automatically evaluating machine translation output do not re-quire humans to assess individual system output, humans are nevertheless needed to generate a number of reference translations. The quality of machine-generated translations is determined by automatically comparing system output with these references. All current automatic evaluation met-rics are based on the various measures of the gen-eral similarity between the system translation and manual references. This kind of method has an ob-vious drawback: it does not account for combina-tions of lexical and syntactic differences that might occur between a perfectly fluent and accurately-translated machine output and a human reference translation (beyond variations already captured by the different reference translations themselves). Moreover, the set of human reference translations is unlikely to be an exhaustive inventory of ?good translations? for any given foreign language sen-tence. Therefore, it would be highly desirable to extend the coverage of the references for the simi-larity based evaluation methods. To match the system translation with various presentation of the same meaning, many work ha-ven been proposed to extend the references by generating lexical variations. The first strategy fo-cuses on the extension based on paraphrase identi-
37
fication (Lepage and Denoual, 2005; Lassner et al 2005; Zhou et al 2006; Kauchak and Barzilay, 2006; Owczarzak et al 2006; Owczarzak et al 2007). In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of sub-strings they may share. In short, all paraphrases of human-generated references should be considered ?good? translations. The second strategy extends the references with the synonymy (Banerjee and Lavie, 2005; Lassner et al 2005). This is an alter-nation to obtain lexical variations with synonymy dictionaries instead of the paraphrase. In this kind of method, the reference is matched against to the system translation with the pack of the synonymies of the reference words instead of the exact match-ing. Both two strategies can successfully capture the lexical variations and greatly extend the coverage of the references. But they still have two common deficiencies. The first is the demand of the external knowledge. Paraphrase based method need a mass of external corpus to extract paraphrases and syn-onymy based method need manually constructed semantic dictionaries. These demands seriously limit the application on various languages for which the external knowledge is absent. Another deficiency is that the two strategies cannot capture the equivalents of long sub-segments such as a clause. Synonymy based me-thod can only capture the equivalents of single words. Paraphrase based method can capture the equivalents of longer units but the length is still very narrow. In many cases, some long sub-segments can be varied with an entirely different presentation which cannot be decomposed into the variations of words or phrases. To address these problems we propose a novel strategy to generate variations presentation only using existing multiple manual references without any external knowledge. We identify the syntactic components on different level as the replaceable units and determine the syntactic equivalents of the components in the corresponding references. Then the equivalents of the syntactic components are hybridized into new references. The rest of the paper is organized as follows. Section 2 introduces the concept and identification of the syntactic equivalents. Section 3 proposes a process to hybridize the syntactic equivalents effi-
ciently. Experimental results are illustrated in sec-tion 4. We also include some related discussion in Section 5. Finally this work is concluded in Sec-tion 6. 2 Syntactic Equivalents  In our approach, we propose a novel method to obtain the equivalents of the sub-segments from the corresponding references to a single source sentence. A sub-segment can be a word, a phrase or longer unit such as a clause. As we know, the variations of the sentences to the same meaning can be distinguished into two categories. The first is the structural variations. In this case, presenta-tions employ the same words but arrange them in different structure. The second is lexical variations. In this case, presentations have the same structure but employ the different words. In practice, one reference sentence often has both of the two kinds of variations comparing with other corresponding reference sentences. As the previous works, we also focus on the lexical variations. The approach is that the equiva-lents of the words are not obtained by external knowledge. In our strategy, generally speaking, the equivalents of a sub-segment S in a reference sen-tence are identified as the sub-segments which play the same syntactic role in the same structure in the other corresponding references. The equivalents obtained in this way are called syntactic equiva-lents.  Suppose R1 and R2 is a corresponding reference sentence pair. T1 and T2 are the consecutive syntac-tic trees of R1 and R2 respectively. We formally define a syntactic equivalent pair between R1 and R2 with a 4-tuple:  <N1, N2, S1, S2>  where Ni is a non-terminal node in Ti and Si is the sub-segment which is covered by Ni. Then, all the syntactic equivalent pair R1 and R2 can be recur-sively identified using following process:  ?  The first syntactic equivalent pair <N1, N2, S1, S2> is identified where Ni is the root of Ti and Si= Ri. ?  Suppose <N1, N2, S1, S2> is a syntactic equivalent pair. {N11, N12, ?N1m} and { N21, N22, ?N2n} are the child nodes sequences of 
38
N1 and N2 respectively. If n=m and N1i= N2i (i.e. the child nodes sequence of N1 and N2 are exactly the same), for each node pair N1i and N2i a syntactic equivalent pair is identi-fied as < N1i, N2i, S1i, S2i>.  With this process, all equivalent pairs on differ-ent syntactic level can be identified by synchro-nously traveling the two trees from top to bottom. The following is an example of the identification of the equivalent pairs. Figure 1 gives out a refer-ence sentence pair and their syntactic trees. The nodes which are included in certain equivalent pair are surrounded by a rectangle.   (a)   (b)  Figure 1 An example of the identification of the syn-tactic equivalent pairs.    In this example, five equivalent pairs can be identified:  ?  <S, S, ?Machine translation develops con-stantly?, ?MT progresses persistently?> ?  <NP, NP, ?Machine translation?, ?MT?> ?  <VP, VP, ?develops constantly?, ?progresses persistently?> ?  <VV, VV, ?develops?, ?progresses?> ?  <ADV, ADV, ?constantly?, ?persistently?> 3 Hybridization of Syntactic Equivalents  The indentified syntactic equivalents pairs include the sub-segments which sharing the same role in the same syntactic structure. Because of this, we 
can obtain a variation of a reference sentence by switching the two sub-segments of an equivalent pair in this sentence. This operation did not change the structure of the sentence but only replace a sub-segment in the structure with its equivalent.  Consequently, two new references can be gener-ated by switching the two sub-segments of an equivalent pair between two reference sentences. Furthermore when we switch the sub-segments of all equivalent pairs between the two references, multiple new references are generated with various combinations of the switches. This operation is called the syntactic hybridization of the references which can be illustrated by following steps: Suppose R={ri}i=1?n is a reference set containing n reference sentences to a single source sentence. R? is the new reference set containing the original reference sentences and the hybridized reference sentences. R? can be obtained by formula (1):    where rooti is the root node of the syntactic tree of ri. Equ(nt) returns the set of all equivalent of the sub-segments covered by the tree node nt. The de-tailed process of Equ(nt) is:  Equ(nt):  Define set  equ = ? Add Seg(nt) to equ If nt is included in an equivalent pair <nt, nt?, s, s?> Add p? to equ Define childi=1?m is the m children of nt Define hybr = Equ(child1)?Equ(child2)??Equ(childm) Merge hybr into equ Return equ  where Seg(nt) is the sub-segment covered by the tree node nt. Operation S1? S2 generates the Carte-sian product of the sub-segment set S1 and S2, i.e. for each arbitrary sub-segment pair s1 and s2 se-lected from S1 and S respectively, we concatenate s1 and s2. Finally, the reduplicate references in R? are removed.   For the example in Section 2, eight hybridized references can be generated including the original two sentences:  
39
?  Machine Translation develops constantly ?  Machine Translation develops persistently ?  Machine Translation progresses constantly ?  Machine Translation progresses persistently  ?  MT develops constantly ?  MT develops persistently ?  MT progresses constantly ?  MT progresses persistently 4 Experiments  We will show experimental results in this section to verify the effectiveness of the extended set of hybridized reference sentences. In the experiments, multiple translations of the source language sen-tences are evaluated with several popular auto-matic evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson?s correlations between the human assessments and evaluation scores using two reference set are calculated and compared.   The multiple translations and human assess-ments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 & 2 describes the detail of the two datasets. The popular automatic evaluation metrics in-clude BLEU (Papieni et al, 2002), GTM (Me-lamed et al, 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are ob-tained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43.  Table 3 & 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-bigram with a window of nine words. And METEOR is run in ?exact? mode.   Release Year 2006 Genre Newswire Number of segments 919 Source Language Chinese 
Target Language English Number of system transla-tions 7 Number of reference trans-lations 4 Human assessment scores Score 1-5, ade-quacy & fluency Table 1 Description of LDC2006T04  Release Year 2008 Genre Newswire Number of segments 249 Source Language Arabic Target Language English Number of system transla-tions 8 Number of reference trans-lations 4 Human assessment scores Score 1-7, ade-quacy  Table 2 Description of LDC2008E43  After the hybridization, each source sentence in LDC2006T04 has 31 corresponding reference sen-tences in average and each source sentence in LDC2008E43 has 66 corresponding reference sen-tences in average. The number of the references is greatly increased. And as shown in the results, the usage of the extended reference set improves the correlations with human assessments for all the metrics in most cases except the ROUGE on LDC 2008E43.  Metric Original Extended BLEU 0.3488 0.3564 GTM 0.3671 0.3681 ROUGE 0.4252 0.4325 METEOR 0.4686         0.4723 Table 3 Pearson?s correlations with human assess-ments on sentence level on LDC2006T04  Metric Original Extended BLEU 0.6092 0.6109 GTM 0.5434 0.5438 ROUGE 0.6628 0.6582 METEOR 0.7053         0.7089 Table 4 Pearson?s correlations with human assess-ments on sentence level on LDC2008E43  The following is a real instance in the experi-ments from LDC2008E43:  Four original references: 
40
 ?  Ten churches burned down in 10 days in the American state of Alabama ?  Burning of ten churches in ten days in the American state of Alabama ?  Ten churches set on fire in ten days in American state of Alabama ?  Torching of ten churches within ten days in American state of Alabama  Six additional references:  ?  Torching of ten churches in ten days in the American state of Alabama ?  Torching of ten churches within ten days in the American state of Alabama ?  Torching of ten churches in ten days in American state of Alabama ?  Burning of ten churches within ten days in American state of Alabama ?  Burning of ten churches within ten days in the American state of Alabama ?  Burning of ten churches in ten days in American state of Alabama  The syntactic structure of the original references:  ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VBN Burned) (PP (IN Down) (PP (IN in) (NP (NPB (CD 10) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Ala-bama))))))))))) ?  (TOP (NP (NPB (NN Burning)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (DT the) (NNP American) (NNP State)) (PP (IN of) (NPB (NNP Alabama))))))))))) ?  (TOP (S (NPB (CD Ten) (NNS Churches)) (VP (VB Set) (PP (IN on) (NPB (NN Fire))) (PP (IN in) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))) ?  (TOP (NP (NPB (NNP Torching)) (PP (IN of) (NP (NPB (CD Ten) (NNS Churches)) (PP (IN within) (NP (NPB (CD Ten) (NNS Days)) (PP (IN in) (NP (NPB (NNP Ameri-
can) (NNP State)) (PP (IN of) (NPB (NNP Alabama)))))))))))  To investigate the distribution of the equivalents we also perform several statistics about the count and the length of the syntactic nodes. In table 5, we list the information about the count of the nodes. The first row is the average words count per refer-ence sentence. The second and third row is the count of all tree nodes and equivalent nodes in all references respectively. The fourth and fifth row is the average count of tree nodes and equivalent nodes per reference sentence respectively.   2006T04 2008E43 Average length of  reference 31.52 34.43 Total tree nodes 211231 62569 Total equivalent nodes 21807 10073 Average tree nodes 57.46 62.82 Average equivalent nodes 5.93 10.11 Table 5 Counts of the tree nodes and equivalent nodes in references.  We also investigate the distribution of the length (count of covered words) of the nodes. First, we count the tree nodes and equivalent nodes whose length is from 1 word to 50 words. Then we calcu-late the pro-portion of equivalent nodes and tree nodes for each length. Figure 2 and 3 illustrate the distribution of absolute count of the equivalent nodes. The X-axis is the length of the nodes and the Y-axis is the count. Figure 4 and 5 illustrate the distribution of the proportions on two datasets re-spectively. The X-axis is the length of the nodes and the Y-axis is the proportion. The investigation reveals four main messages. First, the absolute counts of the short equivalents are much more than those of long equivalents as expected. Second, the proportion of the long equivalents is greater than those of short equiva-lents, this clarify that the reason of large amount of short equivalents is the large amount of short tree nodes. Third, also from the proportion of view we can see that the new method comparably bias to the long equivalents. This happens because the method adopts a top-down survey of the tree. Forth, the multiple references in Arabic-English data seem to match each other better than the references 
41
in Chinese-English data. Arabic-English references have much more equivalents than Chinese-English data and bias to long equivalents more significant.  
 Figure 2 Distribution of absolute length of equivalent node on LDC2006T04  
  Figure 3 Distribution of absolute length of equivalent node on LDC2008E43  
  Figure 4 Distribution of length proportion of equiva-lent nodes on LDC2006T04 
 Figure 5 Distribution of length proportion of equiva-lent nodes on LDC2008E43 5 Discussion  The experimental results verify the positive effect of the hybridized reference for the automatic eval-
evaluation in most cases. Though the improvement of the correlations is not very significant it is stable across the metrics in various styles. Compared with the previous works based on pa-raphrase and synonym the new method has three important advantages. The first is that the hybrid-ized reference can switch the long span sub-segments beyond the words and phrases.  The second is that the switch can be per-formed in multiple levels, i.e. a sub-segment can not only be replaced as a single unit but also can be varied by replacing some child sub-segments of it. It?s noticeable that the multiple level switches also make it possible to present some structural varia-tions by means of the lexical variations. In hybridi-zation, we can realize some structural variation between syntactic nodes by switch their parent node instead of reordering them directly.  The third advantage is that the new method needs no external knowledge which greatly facili-tates the application. But this advantage also re-sults in the main deficiency of this approach: the hybridization references cannot adopt any novel equivalents which are absent in existing references. This deficiency can be overcome by introducing the paraphrase and synonym into the syntactic hy-bridization. It should be indicated that though the hybridiza-tion process generate many new references not all of the new references are reasonable.  In table 6 we compare the effect of hybridized references and manual references with more details on LDC2006T04. In the table, the first column is the contents of the references for each source sen-tence. ?Manual? means the manual references and the number in front of it indicates how many man-ual references are provided. ?Hybr? means the hy-bridized references generated from the manual references in front of the ?+?. The second column is the Pearson?s correlations between human as-sessments and the BLEU scores using the corre-sponding reference set. Besides the set containing 4 references the other correlations are the average of the correlations based on all possible subset con-taining certain number of references. For example correlation of ?2 Manual? is the average of the cor-relations based on 6 possible subset containing 2 references.  Reference Set Correlation 1 Manual 0.2565 
42
2 Manual 0.3057 2 Manual+ Hybr 0.3082 3 Manual 0.3316 3 Manual + Hybr 0.3369 4 Manual 0.3488 4 Manual+ Hybr 0.3564 Table 6 Pearson?s correlations based on incremental reference set  As shown in the Table 6 hybridized references can improve the correlations with human assess-ments on different sizes of manual references set. But it also indicated that though hybridization can generate a mass of novel references the new refer-ences is always not more effective than even one additional manual references. This tells us that the quality of the hybridized references still need to be further refined. Another message revealed by the table is that with the increase of the number of manual refer-ences the improvement of correlation made by ad-ditional manual references is decreasing. However, the improvement made by the hybridized is in-creasing. This happens because the number of hy-bridized references increases much faster than the number of manual references. There are still several noticeable deficiencies of this work. First, it only works when there are more than two existing references. This make it cannot be used to extend the single reference in mass bi-lingual corpus. Second, which is also the most im-portant one is that this method strongly focuses on the precision at the cost of recall. Though we have recognized many equivalents for each sentence but there are still many equivalents that share different context cannot be recognized. This will be our main future work. The last deficiency is the bias to the long equivalents. This problem is caused by the same reason with the second deficiency: this method define the equivalent with the same syntac-tic context. If two sub-nodes do not share the same parent it often have different brothers. 6 Conclusions and Future Work  In this work we present a novel method to extend the coverage of the reference set for the automatic evaluation of machine translation. The new method decomposes the existing references into sub-segments according to the syntactic structure. And then generate new reference sentences by hybridiz-
ing the equivalents of the segments which play the same syntactic role in corresponding references. In this way the new method can not only capture the equivalents of words and phrases like the other methods but also capture the equivalents of long sub-segments which are out of the capability of the other methods. Another important advantage of the new method is the no use of the external knowl-edge which greatly facilitates the application. Experimental results show that with the ex-tended reference set the state-of-the-arts automatic evaluation metrics achieve better correlation with the human assessments. In the future work, we will relax the restriction of the equivalent definition and try to recognize more equivalents. We will also introduce the para-phrase and synonyms into our method to see fur-ther improvement. Another interesting challenge is to hybridize the equivalents in the different order and present the structural variations directly. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grant No. 60773066 and 60736014, the National High Tech-nology Development 863 Program of China under Grant No. 2006AA010108. References  Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Trans-lation and/or Summarization. M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, Uni-versity of Pennsylvania. I. Dan Melamed, Ryan Green, Joseph P. Turian, 2003, Precision and recall of machine translation, In Pro-ceedings of HLT/NAACL 2003. David Kauchak, Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation, In Proceedings of the NAACL 2006. Dan Klein, Christopher Manning. 2003. Accurate Un-lexicalized Parsing. In Proceedings of the 41th Meet-ing of the ACL, pp. 423-430. Yves Lepage, Etienne Denoual. 2005. Automatic gen-eration of paraphrases to be used as translation refer-
43
ences in objective evaluation measures of ma-chine translation, In Proceedings of the IWP 2005. Karolina Owczarzak, Declan Groves, Josef Van Ge-nabith ,Andy Way. 2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation, In Pro-ceedings of the Workshop on Statistical Ma-chine Translation. Karolina Owczarzak, Josef Van Genabith, Andy Way. 2007. Dependency-Based Automatic Evaluation for Machine Translation, In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation. Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a method for automatic evalua-tion of machine translation, In Proceedings of the 40th Meeting of the ACL. Grazia Russo-Lassner, Jimmy Lin, Philip Resnik. 2005. Re-evaluating Machine Translation Results with Pa-raphrase Support, Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, MD. Chin-Yew Lin, Franz Josef Och. 2004. Automatic evaluation of machine translation quality using long-est common subsequence and skip-bigram sta-tistics. In Proceedings of the 42th  Meeting of the ACL. Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-evaluating Machine Translation Results with Pa-raphrase Support, In Proceedings of the EMNLP 2006. 
44
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 45?50,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study of Translation Rule Classification for Syntax-based Statistical
Machine Translation
Hongfei Jiang, Sheng Li, Muyun Yang and Tiejun Zhao
School of Computer Science and Technology
Harbin Institute of Technology
{hfjiang,lisheng,ymy,tjzhao}@mtlab.hit.edu.cn
Abstract
Recently, numerous statistical machine trans-
lation models which can utilize various kinds
of translation rules are proposed. In these
models, not only the conventional syntactic
rules but also the non-syntactic rules can be
applied. Even the pure phrase rules are in-
cludes in some of these models. Although the
better performances are reported over the con-
ventional phrase model and syntax model, the
mixture of diversified rules still leaves much
room for study. In this paper, we present a
refined rule classification system. Based on
this classification system, the rules are classi-
fied according to different standards, such as
lexicalization level and generalization. Espe-
cially, we refresh the concepts of the structure
reordering rules and the discontiguous phrase
rules. This novel classification system may
supports the SMT research community with
some helpful references.
1 Introduction
Phrase-based statistical machine translation mod-
els (Marcu and Wong, 2002; Koehn et al, 2003; Och
and Ney, 2004; Koehn, 2004; Koehn et al, 2007)
have achieved significant improvements in trans-
lation accuracy over the original IBM word-based
model. However, there are still many limitations in
phrase based models. The most frequently pointed
limitation is its inefficacy to modeling the struc-
ture reordering and the discontiguous correspond-
ing. To overcome these limitations, many syntax-
based SMT models have been proposed (Wu, 1997;
Chiang, 2007; Ding et al, 2005; Eisner, 2003; Quirk
et al, 2005; Liu et al, 2007; Zhang et al, 2007;
Zhang et al, 2008a; Zhang et al, 2008b; Gildea,
2003; Galley et al, 2004; Marcu et al, 2006; Bod,
2007). The basic motivation behind syntax-based
model is that the syntax information has the poten-
tial to model the structure reordering and discontigu-
ous corresponding by the intrinsic structural gener-
alization ability. Although remarkable progresses
have been reported, the strict syntactic constraint
(the both sides of the rules should strictly be a sub-
tree of the whole syntax parse) greatly hinders the
utilization of the non-syntactic translation equiva-
lents. To alleviate this constraint, a few works have
attempted to make full use of the non-syntactic rules
by extending their syntax-based models to more
general frameworks. For example, forest-to-string
transformation rules have been integrated into the
tree-to-string translation framework by (Liu et al,
2006; Liu et al, 2007). Zhang et al (2008a) made
it possible to utilize the non-syntactic rules and even
the phrases which are used in phrase based model
by advancing a general tree sequence to tree se-
quence framework based on the tree-to-tree model
presented in (Zhang et al, 2007). In these mod-
els, various kinds of rules can be employed. For
example, as shown in Figure 1 and Figure 2, Fig-
ure 1 shows a Chinese-to-English sentence pair with
syntax parses on both sides and the word alignments
(dotted lines). Figure 2 lists some of the rules which
can be extracted from the sentence pair in Figure 1
by the system used in (Zhang et al, 2008a). These
rules includes not only conventional syntax rules but
also the tree sequence rules (the multi-headed syn-
tax rules ). Even the phrase rules are adopted by
45
the system. Although the better performances are
reported over the conventional phrase-based model
and syntax-based model, the mixture of diversified
rules still leaves much room for study. Given such a
hybrid rule set, we must want to know what kinds of
rules can make more important contributions to the
overall system performance and what kinds of rules
are redundant compared with the others. From en-
gineering point of view, the developers may concern
about which kinds of rules should be preferred and
which kinds of rules could be discard without too
much decline in translation quality. However, one of
the precondition for the investigations of these issues
is what are the ?rule categories?? In other words,
some comprehensive rule classifications are neces-
sary to make the rule analyses feasible. The motiva-
tion of this paper is to present such a rule classifica-
tion.
2 Related Works
A few researches have made some exploratory in-
vestigations towards the effects of different rules by
classifying the translation rules into different sub-
categories (Liu et al, 2007; Zhang et al, 2008a;
DeNeefe et al, 2007). Liu et al (2007) differenti-
ated the rules in their tree-to-string model which in-
tegrated with forest1-to-string into fully lexicalized
rules, non-lexicalized rules and partial lexicalized
rules according to the lexicalization levels. As an
extension, Zhang et al (2008a) proposed two more
categories: Structure Reordering Rules (SRR) and
Discontiguous Phrase Rules (DPR). The SRR stands
for the rules which have at least two non-terminal
leaf nodes with inverted order in the source and tar-
get side. And DPR refers to the rules having at
least one non-terminal leaf node between two termi-
nal leaf nodes. (DeNeefe et al, 2007) made an illu-
minating breakdown of the different kinds of rules.
Firstly, they classify all the GHKM2 rules (Galley et
al., 2004; Galley et al, 2006) into two categories:
lexical rules and non-lexical rules. The former are
the rules whose source side has no source words.
In other words, a non-lexical rule is a purely ab-
1A ?forest? means a sub-tree sequence derived from a given
parse tree
2One reviewer asked about the acronym GHKM. We guess
it is an acronym for the authors of (Galley et al, 2004): Michel
Galley, Mark Hopkins, Kevin Knight and Daniel Marcu.
? ? ???
Figure 1: A syntax tree pair example. Dotted lines stands
for the word alignments.
stract rule. The latter is the complementary set of
the former. And then lexical rules are classified fur-
ther into phrasal rules and non-phrasal rules. The
phrasal rules refer to the rules whose source side
and the yield of the target side contain exactly one
contiguous phrase each. And the one or more non-
terminals can be placed on either side of the phrase.
In other words, each phrasal rule can be simulated
by the conjunction of two more phrase rules. (De-
Neefe et al, 2007) classifies non-phrasal rules fur-
ther into structural rules, re-ordering rules, and non-
contiguous phrase rules. However, these categories
are not explicitly defined in (DeNeefe et al, 2007)
since out of its focus. Our proposed rule classifica-
tion is inspired by these works.
3 Rules Classifications
Currently, there have been several classifications
in SMT research community. Generally, the rules
can be classified into two main groups according to
whether syntax information is involved: bilingual
phrases (Phrase) and syntax rules (Syntax). Fur-
ther, the syntax rules can be divided into three cat-
egories according to the lexicalization levels (Liu et
al., 2007; Zhang et al, 2008a):
1) Fully lexicalized (FLex): all leaf nodes in both
the source and target sides are lexicons (termi-
nals)
2) Unlexicalized (ULex): all leaf nodes in both the
46
??
? ?
??? ?
???
? ?
Figure 2: Some rules can be extracted by the system used in (Zhang et al, 2008a) from the sentence pair in Figure 1.
source and target sides are non-lexicons (non-
terminals)
3) Partially lexicalized (PLex): otherwise.
In Figure 2, R1-R3 are FLex rules, and R5-R8 are
PLex rules.
Following (Zhang et al, 2008b), a syntax rule r
can be formalized into a tuple
< ?s, ?t, AT , ANT >
, where ?s and ?t are tree sequences of source side
and target side respectively, AT is a many-to-many
correspondence set which includes the alignments
between the terminal leaf nodes from source and tar-
get side, and ANT is a one-to-one correspondence
set which includes the synchronizing relations be-
tween the non-terminal leaf nodes from source and
target side.
Then, the syntax rules can also fall into two cat-
egories according to whether equipping with gen-
eralization capability (Chiang, 2007; Zhang et al,
2008a):
1) Initial rules (Initial): all leaf nodes of this rule are
terminals.
2) Abstract rules (Abstract): otherwise, i.e. at least
one leaf node is a non-terminal.
A non-terminal leaf node in a rule is named an ab-
stract node since it has the generalization capabil-
ity. Comparing these two classifications for syntax
rules, we can find that a FLex rule is a initial rule
when ULex rules and PLex rules belong to abstract
rules.
These classifications are clear and easy for un-
derstanding. However, we argue that they need
further refinement for in-depth study. Specially,
more refined differentiations are needed for the ab-
stract rules (ULex rules and PLex rules) since they
play important roles for the characteristic capabil-
ities which are deemed to be the advantages over
the phrase-based model. For instance, the potentials
to model the structure reordering and the discon-
tiguous correspondence. The Structure Reordering
Rules (SRR) and Discontiguous Phrase Rules (DPR)
mentioned by (Zhang et al, 2008a) can be regarded
as more in-depth classification of the syntax rules.
In (Zhang et al, 2008a), they are described as fol-
lows:
Definition 1: The Structure Reordering Rule
(SRR) refers to the structure reordering rule that has
at least two non-terminal leaf nodes with inverted
order in the source and target side.
Definition 2: The Discontiguous Phrase Rule
(DPR) refers to the rule having at least one non-
terminal leaf node between two lexicalized leaf
nodes.
47
Based on these descriptions, R7, R8 in Figure 2
belong to the category of SRR and R6, R7 fall into
the category of DPR. Although these two definitions
are easy implemented in practice, we argue that the
definition of SRR is not complete. The reordering
rules involving the reordering between content word
terminals and non-terminal (such as R5 in Figure
2) also can model the useful structure reorderings.
Moreover, it is not uncommon that a rule demon-
strates the reorderings between two non-terminals
as well as the reorderings between one non-terminal
and one content word terminal. The reason for our
emphasis of content word terminal is that the re-
orderings between the non-terminals and function
word are less meaningful.
One of the theoretical problems with phrase based
SMT models is that they can not effectively model
the discontiguous translations and numerous at-
tempts have been made on this issue (Simard et al,
2005; Quirk and Menezes, 2006; Wellington et al,
2006; Bod, 2007; Zhang et al, 2007). What seems
to be lacking, however, is a explicit definition to the
discontiguous translation. The definition of DPR
in (Zhang et al, 2008a) is explicit but somewhat
rough and not very accurate. For example, in Fig-
ure 3(a), non-terminal node pair ([0,???], [0,?love?]
) is surrounded by lexical terminals. According to
Definition 2, it is a DPR. However, obviously it is
not a discontiguous phrase actually. This rule can be
simulated by conjunctions of three phrases (???, ?I?;
???, ?love?; ???,?you?). In contrast, the translation
rule in Figure 3(b) is an actual discontiguous phrase
rule. The English correspondences of the Chinese
word ??? is dispersed in the English side in which
the correspondence of Chinese word ??? is inserted.
This rule can not be simulated by any conjunctions
of the sub phrases. It must be noted that the dis-
contiguous phrase (???-?switch . . . off?) can not
be abstracted under the existing synchronous gram-
mar frameworks. The fundamental reason is that
the corresponding parts should be abstracted in the
same time and lexicalized in the same time. In other
words, the discontiguous phrase can not be modeled
by the permutation between non-terminals (abstract
nodes). Another point to notice is that our focus in
this paper is the ability demonstrated by the abstract
rules. Thus, we do not pay much attentions to the re-
orderings and discontiguous phrases involved in the
? ?? ? ?
Figure 3: Examples for demonstrating the actual discon-
tiguous phrase. (a) is a negative example for the definition
of DPR in (Zhang et al, 2008a), (b) is a actual discon-
tiguous phrase rule.
2
Figure 4: The rule classifications used in this paper. (a)
shows that the rules can be divided into phrase rules and
syntax rules according to whether a rule includes the syn-
tactic information. (b) illustrates that the syntax rules can
be classified into three kinds according to the lexicaliza-
tion level. (c) shows that the abstract rules can be classi-
fied into more refined sub-categories.
phrase rules (e.g. ?? ??-?switch the light off?)
since they lack the generalization capability. There-
fore, the discontiguous phrase is limited to the rela-
tion between non-terminals and terminals.
On the basis of the above analyses, we present
a novel classification system for the abstract rules
based on the crossings between the leaf node
alignment links. Given an abstract rule r =<
?s, ?t, AT , ANT >, it is
1) a Structure Reordering Rule (SRR), if ? a link
l ? ANT is crossed with a link l? ? {AT ?ANT }
a) a SRR NT2 rule, if the link l? ? ANT
b) a SRR NT-T rule, if the link l? ? AT
2) not a Structure Reordering Rule (N-SRR), other-
wise.
48
??
?
Figure 5: The patterns to show the characteristics of dis-
contiguous phrase rules.
Note that the intersection of SRR NT2 and SRR NT-
T is not necessary an empty set, i.e. a rule can be
both SRR NT2 and SRR NT-T rule.
The basic characteristic of the discontiguous
translation is that the correspondence of one non-
terminal NT is inserted among the correspondences
of one phrase X . Figure 5 (a) illustrates this sit-
uation. However, this characteristic can not sup-
port necessary and sufficient condition. For exam-
ple, if the phrase X can be divided like Figure 5
(b), then the rule in Figure 5 (a) is actually a re-
ordering rule rather than a discontiguous phrase rule.
For sufficient condition, we constrain that the phrase
X = wi . . . wj need to satisfy the requirement: wi
should be connected with wj through word align-
ment links (A word is connected with itself). In Fig-
ure 5(c), f1 is connected with f2 when NT ? is in-
serted between e1 and e2. Thus, the rule in Figure
5(c) is a discontiguous phrase rule.
Definition 3: Given an abstract rule r =<
?s, ?t, AT , ANT >, it is a Discontiguous Phrase iff
? two links lt1, lt2 from AT and a link lnt from ANT ,
satisfy: lt1, lt2 are emitted from the same word and
lt1 is crossed with lnt when lt2 is not crossed with
lnt.
Through Definition 3, we know that the DPR is a
sub-set of the SRR NT-T.
4 Conclusions and Future Works
In this paper, we present a refined rule classifica-
tion system. Based on this classification system, the
rules are classified according to different standards,
such as lexicalization level and generalization. Es-
pecially, we refresh the concepts of the structure re-
ordering rules and the discontiguous phrase rules.
This novel classification system may supports the
SMT research community with some helpful refer-
ences.
In the future works, aiming to analyze the rule
contributions and the redundances issues using the
presented rule classification based on some real
translation systems, we plan to implement some syn-
chronous grammar based syntax translation models
such as the one presented in (Liu et al, 2007) or
in (Zhang et al, 2008a). Taking such a system as
the experimental platform, we can perform compre-
hensive statistics about distributions of different rule
categories. What is more important, the contribu-
tion of each rule category can be evaluated seriatim.
Furthermore, which kinds of rules are preferentially
applied in the 1-best decoding can be studied. All
these investigations could reveal very useful infor-
mation for the optimization of rule extraction and the
improvement of the computational models for syn-
chronous grammar based machine translation.
Acknowledgments
This work is supported by the Key Program of
National Natural Science Foundation of China
(60736014), and the Key Project of the National
High Technology Research and Development Pro-
gram of China (2006AA010108).
References
Rens Bod. 2007. Unsupervised syntax-based machine
translation: The contribution of discontiguous phrases.
In Proceedings of Machine Translation Summit XI
2007,Copenhagen, Denmark.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. In computational linguistics, 33(2).
Ding, Y. and Palmer, M. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars In Proceedings of ACL.
DeNeefe, S. and Knight, K. and Wang, W. and Marcu, D.
2007. What can syntax-based MT learn from phrase-
based MT? In Proceedings of EMNLP/CONLL.
Michel Galley, Mark Hopkins, Kevin Knight and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of NAACL-HLT 2004, pages 273-280.
49
Galley, M. and Graehl, J. and Knight, K. and Marcu,
D. and DeNeefe, S. and Wang, W. and Thayer, I.
2006. Scalable inference and training of context-rich
syntactic translation models In Proceedings of ACL-
COLING
Daniel Gildea 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of ACL 2003,
pages 80-87.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of ACL
2003.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL 2003, pages 127-133, Edmonton,
Canada, May.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115-124.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. ACL 2007,
demonstration session, Prague, Czech Republic, June
2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-string
alignment template for statistical machine translation.
In Proceedings of ACL-COLING.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin. 2007.
Forest-to-string statistical translation rules. In Pro-
ceedings of ACL 2007, pages 704-711.
Daniel Marcu and William Wong. 2002. A phrase based,
joint probability model for statistical machine transla-
tion. In Proceedings of EMNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine trans-
lation with syntactified target language Phrases. In
Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440-447.
Franz Josef Och and Herman Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417-449.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages 271-
279, Ann Arbor, Michigan, June.
Chris Quirk and Arul Menezes. 2006. Do we need
phrases? Challenging the conventional wisdom in
Statistical Machine Translation. In Proceedings of
HLT/NAACL
Simard, M. and Cancedda, N. and Cavestro, B. and
Dymetman, M. and Gaussier, E. and Goutte, C. and
Yamada, K. and Langlais, P. and Mauser, A. 2005.
Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP, volume 2, pages 901-904.
Benjamin Wellington, Sonjia Waxmonsky and I. Dan
Melamed. 2006. Empirical Lower Bounds on the
Complexity of Translational Equivalence. In Proceed-
ings of ACL-COLING 2006, pages 977-984.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora. In
Proceedings of ACL 1997. Computational Linguistics,
23(3):377-403.
Min Zhang, Hongfei Jiang, Ai Ti AW, Jun Sun, Sheng
Li, and Chew Lim Tan. 2007. A tree-to-tree
alignment-based model for statistical machine trans-
lation. In Proceedings of Machine Translation Summit
XI 2007,Copenhagen, Denmark.
Min Zhang, Hongfei Jiang, Ai Ti AW, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008a. A tree sequence
alignment-based tree-to-tree translation model. In
Proceedings of ACL-HLT
Min Zhang, Hongfei Jiang, Haizhou Li, Ai Ti AW,
and Sheng Li. 2008b. Grammar Comparison Study
for Translational Equivalence Modeling and Statistical
Machine Translation. In Proceedings of Coling
50
Coling 2010: Poster Volume, pages 701?709,
Beijing, August 2010
Reexamination on Potential for Personalization in Web Search 
Daren Li1  Muyun Yang1  Haoliang Qi2  Sheng Li1  Tiejun Zhao1 
 
1School of Computer Science 
Harbin Institute of Technology 
{drli|ymy|tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
 
2School of Computer Science 
Heilongjiang Institute of Technology 
haoliang.qi@gmail.com 
 
Abstract 
Various strategies have been proposed 
to enhance web search through utiliz-
ing individual user information. How-
ever, considering the well acknowl-
edged recurring queries and repetitive 
clicks among users, it is still an open 
issue whether using individual user in-
formation is a proper direction of ef-
forts in improving the web search. In 
this paper, we first quantitatively dem-
onstrate that individual user informa-
tion is more beneficial than common 
user information. Then we statistically 
compare the benefit of individual and 
common user information through 
Kappa statistic. Finally, we calculate 
potential for personalization to present 
an overview of what queries can bene-
fit more from individual user informa-
tion. All these analyses are conducted 
on both English AOL log and Chinese 
Sogou log, and a bilingual perspective 
statistics consistently confirms our 
findings. 
1 Introduction 
Most of traditional search engines are designed 
to return identical result to the same query 
even for different users. However, it has been 
found that majority of queries are quite ambi-
guous (Cronen-Townsend et al, 2002) as well 
as too short (Silverstein et al, 1999) to de-
scribe the exact informational needs of users. 
Different users may have completely different 
information needs under the same query (Jan-
sen et al, 2000). For example, when users is-
sue a query ?Java? to a search engine, their 
needs can be something ranging from a pro-
gramming language to a kind of coffee. 
In order to solve this problem, personalized 
search is proposed, which is a typical strategy 
of utilizing individual user information. Pitkow 
et al (2002) describe personalized search as 
the contextual computing approach which fo-
cuses on understanding the information con-
sumption patterns of each user, the various 
information foraging strategies and applica-
tions they employ, and the nature of the infor-
mation itself. After that, personalized search 
has gradually developed into one of the hot 
topics in information retrieval. As for various 
personalization models proposed recently, Dou 
et al (2007), however, reveal that they actually 
harms the results for certain queries while im-
proving others. This result based on a large-
scale experiment challenges not only the cur-
rent personalization methods but also the mo-
tivation to improve web search by the persona-
lized strategies. 
In addition, the studies on query logs rec-
orded by search engines consistently report the 
prevailing repeated query submissions by large 
number of users (Silverstein et al, 1999; Spink 
et al, 2001). It is reported that the 25 most fre-
quent queries from the AltaVista cover 1.5% 
of the total query submissions, despite being 
only 0.00000016% of unique queries (Silvers-
tein et al, 1999). As a result, the previous us-
ers? activities may serve as valuable informa-
tion, and technologies focusing on common 
701
user information, such as collaborative filter-
ing (or recommendation) may be a better reso-
lution to web search. Therefore, the justifica-
tion of utilizing individual user information 
deserves further discussion. 
To address this issue, this paper conducts a 
bilingual perspective of survey on two large-
scale query logs publically available: the AOL 
in English and the Sogou1 in Chinese. First we 
quantitatively investigate the evidences for 
exploiting common user information and indi-
vidual user information in these two logs. Af-
ter that we introduce Kappa statistic to meas-
ure the consistency of users? implicit relevance 
judgment inferred from clicks. It is tentatively 
revealed that using individual user information 
is what requires web search to face with after 
common user information is well exploited. 
Finally, we study the distribution of potential 
for personalization over the whole logs to gen-
erally disclose what kind of query deserves for 
individual user information. 
The remainder of this paper is structured as 
follows. Section 2 introduces previous me-
thods employing individual and common user 
information. In Section 3, we quantitatively 
compare the evidences for exploiting common 
user information and individual user informa-
tion. In Section 4, we introduce Kappa statistic 
to measure the consistency of users? clicks on 
the same query and try to statistically present 
the development direction of current web 
search. Section 5 figures out utilizing individu-
al user information as a research issue after 
well exploiting common user information. Sec-
tion 6 presents the potential for personalization 
curve, trying to outline which kind of queries 
benefit the most from individual user informa-
tion. Conclusions and future work are detailed 
in Section 7. 
2 Related Work 
With the rapid expansion of World Wide Web, 
it becomes more and more difficult to find re-
levant information through one-size-fits-all 
information retrieval service provided by clas-
sical search engines. Two kinds of user infor-
mation are mainly used to enhance search en-
                                                 
1 A famous Chinese search engine with a large number of 
Chinese web search users. 
gines: common user information and individu-
al user information. We separately review the 
previous works focusing on using these two 
kinds of information. 
Among various attempts to improve the per-
formance of search engine, collaborative web 
search is the one to take advantage of the repe-
tition of users? behaviors, which we call com-
mon user information. Since there is no unified 
definition on collaborative web search, in this 
paper, we believe that the collaborative web 
search assumes that community search activi-
ties can provide valuable search knowledge, 
and sharing this knowledge facilitates improv-
ing traditional search engine results (Smyth, 
2007). An important technique of collaborative 
web search is Collaborative Filtering (CF, also 
known as collaborative recommendation), in 
which, items are recommended to an active 
user based on historical co-occurrence data 
between users and items (Herlocker et al, 
1999). A number of researchers have explored 
algorithms for collaborative filtering and the 
algorithms can be categorized into two classes: 
memory-based CF and model-based CF. 
Memory-based CF methods apply a nearest-
neighbor-like scheme to predict a user?s rat-
ings based on the ratings given by like-minded 
users (Yu et al, 2004). The model-based ap-
proaches expand memory-based CF to build a 
descriptive model of group-based user prefe-
rences and use the model to predict the ratings. 
Examples of model-based approaches include 
clustering models (Kohrs et al, 1999) and as-
pect models (J. Canny, 2002). 
The other way to improve web search is per-
sonalized web search, focusing on learning the 
individual preferences instead of others? beha-
viors, which is called individual user informa-
tion. Early works learn user profiles from the 
explicit description of users to filter search re-
sults (Chirita et al, 2005). However, most of 
users are not willing to provide explicit feed-
back on search results and describe their inter-
ests (Carroll et al, 1987). Therefore, recent 
researches on the personalized search focus on 
modeling user preference from different types 
of implicit data, such as query history (Speretta 
et al, 2005), browsing history (Sugiyama et al, 
2004), clickthrough data (Sun et al, 2005), 
immediate search context (Shen et al, 2005) 
and other personal information (Teevan et al, 
702
2005). So far, there is still no proper compari-
son between the two solutions. It is still an 
open question which kind of information is 
more effective to build the web search model. 
Considering the difficulty in collecting pri-
vate information, using individual user infor-
mation seems less promising as the cost-
effective solution to web search. To address 
this issue, some researches about the value of 
personalization have been conducted. Teevan 
et al (2007) have done a ground breaking job 
to quantify the benefit for the search engines if 
search results were tailored to satisfy each user. 
The possible improvement by the personalized 
search, named potential for personalization, is 
measured by a gap between the relevance of 
individualized rankings and group ranking 
based on NDCG. However, it is less touched 
for the position of individual user information 
in contrast with common user information in 
large scale query log and how to balance the 
usage of common and individual information 
in information retrieval model. 
This paper tentatively examines individual 
user information against common user infor-
mation on two large-scale search engine logs 
in following aspects: the evidence from clicks 
on the same query, Kappa statistic for the 
whole queries, and overall distribution of que-
ries in terms of number of submissions and 
Kappa value. The bilingual statistics consis-
tently reveals the tendency of using individual 
user information as an equally important issue 
as (if not more than) using common user in-
formation) issue for researches on web search. 
3 Quantitative Evidences for Using 
Common or Individual User Infor-
mation 
To quantitatively investigate the value of 
common user information and individual user 
information in query log, we discriminate the 
evidence for using the two different types of 
user information as follows: 
(1) Evidence for using common user infor-
mation: if there were multiple users who have 
exactly the same click sets on one query, we 
suppose those clicks sets, together with the 
query, as the evidence for exploiting common 
user information. It is clear that such queries 
are able to be better responded with other?s 
search results. Note that common user infor-
mation is hard to be clearly defined, in order to 
simplify the quantitative statistics we give a 
strict definition. Further analysis will be shown 
in following sections. 
(2) Evidence for using individual user in-
formation: if a user?s click set on a query was 
not the same as any other?s, for that query, the 
search intent of the user who issue that query 
can be better inferred from his/her individual 
information than common user information. 
We suppose this kind of clicks, together with 
the related queries, as the evidence for exploit-
ing individual user information. 
Since users may have different search in-
tents when they issue the same query, a query 
can be an evidence for using both common and 
individual user information. In our statistics, if 
a query has both duplicate click sets and 
unique click set, the query is not only counted 
by the first category but also the second cate-
gory.  
The statistics of the two categories are con-
ducted in the query log of both English and 
Chinese search engines. We use a subset of 
AOL Query Log from March 1, 2006 to May 
31, 2006 and Sogou Query Log from March 1, 
2007 to March 31, 2007. The basic statistics of 
AOL and Sogou log are shown in Table 1. No-
tice that the queries in raw AOL and Sogou log 
without clicks are removed in this study. 
 
Item AOL Sogou 
#days 92 31 
#users 6,614,960 7,488,754 
#queries 7,840,348 8,019,229 
#unique queries 4,811,649 4,580,836 
#clicks 12,984,610 17,607,808 
Table 1: Basic statistics of AOL & Sogou log 
 
Table 2 summarizes the statistics of differ-
ent evidence categories over AOL and Sogou 
log. Note that click set refers to the set of 
clicks related to a query submission instead of 
a unique query. As for evidence for using 
common and individual user information, there 
is no clear distinction in terms of number of 
records, number of users in two logs. However, 
in terms of unique query and distinct click set, 
one can?t fail to find that evidence for using 
individual user information clearly exceeds  
703
Log 
The Condition Number 
Repeated queries Click Records User Unique Query 
Distinct 
Click Set 
AOL 
3,745,088 
(47.77% of total 
query submissions) 
Same 2,438,284 277,416 382,267 461,460 
Different 2,563,245 343,846 542,593 1,349,892 
Sogou 
4,252,167 
(53.02% of total 
query submissions) 
Same 2,469,363 1,380,951 228,315 358,346 
Different 5,481,832 1,545,817 752,047 2,171,872 
 
Table 2: Different click behaviors on repeated queries 
that for using common user information, espe-
cially in Sogou log. Therefore, though making 
use of common and individual user informa-
tion can address equally well for half users and 
half visits to the search engine, the fact that  
much more unique queries and click sets ac-
tually claims the significance of needing indi-
vidual user information to personalize web 
results. And methods exploiting individual us-
er information provide a much more challeng-
ing task in terms of problem space, though one 
may argue utilizing common user information 
is much easier to attack. 
4 Kappa Statistics for Individual and 
Common user information 
Section 3 has shown the evidence for using 
individual user information is prevailing than 
common user information in quantity for the 
unique queries in search engines. However, 
these counts deserve a further statistical cha-
racterization. In this section, we introduce 
Kappa statistic to depict the overall consisten-
cy of users? clicks in query logs. 
4.1 Kappa 
Kappa is a statistical measure introduced to 
access the agreement among different raters. 
There are two types of Kappa. One is Cohen?s 
Kappa (Cohen, 1960), which measures only 
the degree of agreement between two raters. 
The other is Fleiss?s Kappa (Fleiss, 1971), 
which generalizes Cohen?s Kappa to measure 
agreement among more than two raters, de-
noted as: 
e
e
P
PP
?
?=
1
?  
where, P is the probability that a randomly 
selected rater agree with another on a random-
ly selected subject. eP is the expected probabil-
ity of agreement if all raters made ratings by 
chance. If we use Kappa to measure the consis-
tency of relevance judgment by different raters, 
P can be interpreted as the probability that 
two random selected raters consistently rate a 
random selected search result as relevant or 
non-relevant one. Similarly, eP can also be 
construed as the expected probability of iden-
tical relevance judgment rated by different ra-
ters all by chance.  
Teevan et al (2008) used Fleiss?s Kappa to 
measure the inter-rater reliability of different 
raters? explicit relevance judgments. We ex-
pand their work and employ Fleiss?s Kappa to 
measure the consistency of implicit relevance 
judgments by users on the same query2. Here 
clicks are treated as a proxy for relevance: 
documents clicked by a user are judged as re-
levant and those not clicked as non-relevant 
(Teevan et al, 2008). As we all know that the 
result set of one query may change over time, 
so we select the longest time span to calculate 
Kappa value of a query, during which the re-
sult set of it preserves unchanged. From Kappa 
value of each query, we can statistically interp-
ret to which extent users share consistent intent 
on the same query according to Table 3 (Lan-
dis and Koch, 1977). Though the interpretation 
in Table 3 is not accepted with no doubt, it can 
give us an intuition about what extent of 
agreement consistency is. In other words, 
Kappa is a measure with statistical sense. 
Meanwhile, Kappa values of queries with  
                                                 
2 There may be more than two users who submitted the 
same query. 
704
  
                                       (a). AOL                                                                  (b). Sogou 
 
Figure 1: Number of unique queries and query submissions as a function of Kappa value. 
 
? Interpretation 
< 0 No agreement 
0.0 ? 0.20 Slight agreement 
0.21 ? 0.40 Fair agreement 
0.41 ? 0.60 Moderate agreement 
0.61 ? 0.80 Substantial agreement 
0.81 ? 1.00 Almost perfect agreement 
Table 3:  Kappa Interpretation 
 
various sizes of click sets are also comparable. 
That is also the reason we choose Kappa to 
measure consistency. 
4.2 Distribution of Kappa 
As introduced in Section 2, common user in-
formation is supposed to be the repetition of 
users? behaviors. We consider that the amount 
of repetition of users? clicks on one query is 
quantified by the consistency of its clicks. To 
statistically present the scale of repetition in 
current query log, we try to give an overview 
of consistency level of two commercial query 
logs. 
Figure 1 plots distribution of Kappa value of 
the two logs in the coordinate with logarithmic 
Y-axis. About 34.5% unique queries (44.0% 
query submissions) in AOL log and only 
13.9% unique queries (15.2% query submis-
sions) in Sogou log have high Kappa values 
above 0.6. According to Table 3, click sets of 
these queries can be regarded as somewhat 
consistent. These queries can be roughly re-
solved by using common user information. On 
the other hand, for the rest of queries which 
constitute majority of the logs, users? click sets 
are rather diversified, which are hard to be sa-
tisfied by returning the same result list to them. 
As a whole, the queries in both AOL and So-
gou can be characterized as less consistently in 
the clicks according to Kappa value, which is a 
statistical support for exploiting individual user 
information. 
5 Individual or Common user infor-
mation: A Tendency View 
The above analyses quantitative analyses have 
shown that the repetition of search is not the 
statistically dominant factor, with the impres-
sion that employing individual user informa-
tion is equally, if not more, important than 
common user information. This section tries to 
further reveal this issue so as to balance the 
position of individual user information and 
common user information from a research 
point. 
Intuitively, a query can be characterized by 
the number of people issuing it, i.e. query fre-
quency if we remove the resubmissions of one 
query by the same people. We try to depict the 
above mentioned query submissions and Kap-
pa values as a function of number of people 
who issue the queries in Figure 2. In Figure 2, 
different numbers of users who issue the same 
query are shown on the x-axis, and the y-axis 
represents the number of different entities (left 
scale) and the average Kappa value (right scale) 
of the queries. We find that the number of que-
ries becomes very small when the number of 
users in a group grows over 10, so we set a 
variant step length for them: with the length 
step of the group size falling between 2 and 10 
set as 1, between 11 and 100 as 10, between 
101 and 1000 as 100 and above 1000 as 1000. 
705
   
 
                                     (a). AOL                                                                      (b). Sogou 
 
Figure 2: Average Kappa value of queries as a function of number of people in a group who issue 
the same query (line) and the number of submissions of the queries issued by the same size of 
group (dark columns). 
 
According to Figure 2(a), Kappa values of 
the queries in AOL log with more than 20 us-
ers are above 0.6, which indicates rather con-
sistent clicks for them, accounting for about  
29.4% of all query submissions. While for 
those queries visited by less than 20 users, the 
Kappa value declines gradually from 0.6 with 
the drop of users. For these queries occupying 
majority of query submissions, exploiting in-
dividual user information is supposed to be a 
better solution since the clicks on them are ra-
ther individualized. 
According to Figure 2(b), though Kappa 
values of queries increase similarly with 
people submitting them in AOL, the overall 
consistency of the queries in Sogou log is 
much lower: with a Kappa value below 0.6 
even for the queries visited by a large number 
of users. This fact indicates that Chinese users 
may be less consistent in their search intents, 
or partially reflects that the Chinese as a non-
inflection language has more ambiguity, which 
can also be implied from Table 2. Therefore, 
individual user information may be more ef-
fective than common user information in So-
gou log. 
Summarized from Figure 2, it is sensible 
that common user information is appropriate 
for the queries in the right-most of X-axis. 
With most number of visiting people, such 
queries bear rather consistent clicks though 
covering only a small proportion of the distinct 
query set. Moving from the right to the left, we 
can find the majority of queries yield a less 
Kappa value, for which the individualized 
clicks require individual user information to 
meet the needs of each user. In this sense, how 
to exploit individual user information is pre-
destined as the next issue of information re-
trieval if common user information was to be 
well utilized. 
6 Queries for Personalization 
Since using individual user information is a 
non-negligible issue in IR research, a subse-
quent issue is what queries can benefit in what 
extent from individual user information. In this 
section, we try to give an overview for this 
issue via a measure named potential for perso-
nalization. 
6.1 Potential for Personalization 
Potential for personalization proposed by Tee-
van et al (2007) is used to measure the norma-
lized Discounted Cumulative Gain (NDCG) 
improvement between the best ranking of the 
results to a group and individuals. NDCG is a 
well-known measure of the quality of a search 
result (J?rvelin and Kek?l?inen, 2000). 
The best ranking of the results to a group is 
the ranking with highest NDCG based on re-
levance judgments of the users in the group. 
For the queries with explicit judgments, the 
best ranking can be generated as follows: re-
sults that all raters thought were relevant are 
ranked first, followed by those that most 
people thought were relevant but a few people 
thought were irrelevant, until the  results most 
people though were irrelevant. In other word,  
706
  
 
(a)  AOL                                                                    (b)   Sogou 
 
Figure 3: Number of unique queries and query submissions as a function of potential for 
personalization 
 
   
 
(a)  AOL                                                                       (b) Sogou 
 
Figure 4: The average NDCG of group best ranking as a function of number of people in group 
(solid line), combining with the distribution of  the number of unique queries issued by the same 
size of group (dark columns) 
 
the best ranking always tries to put the results 
that have the highest collective gain first to get 
the highest NDCG. 
The previous work has shown that the im-
plicit click-based potential for personalization 
is strongly related to variation in explicit 
judgments (J. Teevan et al, 2008). In this pa-
per, we continue using click-based potential 
for personalization to measure the variation. 
Assuming the clicked results as relevant, we 
can calculate the potential for personalization 
of each query over the web search query log to 
present what kind of query can benefit more 
from personalization. 
6.2 Potential for Personalization Distribu-
tion over Query Logs 
Teevan et al (2007) have depicted a potential 
for personalization curve based on explicit 
judgment to characterize the benefit that could 
be obtained by personalizing search results for 
each user. We continue using potential for per-
sonalization based on click-through to roughly 
reveal what kind of query can benefit more 
from personalization. 
First we investigate the number of unique 
queries with different potential for personaliza-
tion, which is shown in Figure 3. We find that 
there are about 53.9% unique queries in AOL 
log and 32.4% unique queries in Sogou log, 
whose potential for personalization is 0. For 
these queries, current web search is able to re-
turn perfect results to all users. However, for 
the rest of queries, even the best group ranking 
of results can?t satisfy everyone who issues the 
query. So these queries should be better served 
by individual user information, covering 
707
46.1% unique queries in AOL and 67.6% in 
Sogou. 
Then, in order to further interpret what kind 
of query individual user information is needed 
most, we further relate potential for personali-
zation to the number of users who submit the 
queries over AOL and Sogou query log as 
shown in Figure 4. For clarity?s sake, we also 
set the same step length as in Figure 2. 
According to Figure 4, the curve of potential 
for personalization is approximately U-shaped 
in both AOL log and Sogou Log. As the num-
ber of users in one group increases, perfor-
mance of the best non-personalized rankings 
first declines, then flattens out and finally 
promotes3. Note that the left part of the curve 
is very similar to what Teevan et al (2007) 
showed in their work. 
Again in Figure 4, the queries which have 
the most potential for personalization are the 
ones which are issued by more than 6 and less 
than 20 users in AOL log. While in Sogou log, 
the queries issued by more than 6 and less than 
4000 users have the most potential for persona-
lization. Such different findings are probably 
caused by the content of query. There are 
many recommended queries in the homepage 
of Sogou search engine, most of which are in-
formational query and clicked by a large num-
ber of users. Even when the size of group who 
issue the same query becomes very big, the 
query still has a wide variation of users? beha-
viors. So the consistency level of queries in 
Sogou log is much lower than the queries in 
AOL log at the same size of group.  
7 Conclusion and Future Work 
In this paper, we try to justify the position of 
individual user information comparing with 
common user information. It is shown that ex-
ploiting individual user information is a non-
trivial issue challenging the IR community 
through the analysis of both English and Chi-
nese large scale search logs. 
We first classify the repetitive queries into 2 
categories according to whether the corres-
ponding clicks are unique among different us-
ers. We find that quantitatively the queries and 
                                                 
3 Note that the different step length dims the actual U-
shape in the figure. 
clicks deserving for individual user informa-
tion is much bigger than those deserving for 
common user information. 
After that we use Kappa statistic to present 
that the overall consistency of query clicks re-
coded in search logs is pretty low, which statis-
tically reveals that the repetition is not the do-
minant factor and individual user information 
is more desired to enhance most queries in cur-
rent query log. 
We also explore the distribution of Kappa 
values over different numbers of users in the 
group who issue the same query, concluding 
that how to utilize individual user information 
to improve the performance of web search en-
gine is the next research issue confronted by 
the IR community when the repeated search of 
users are properly exploited.  
Finally, potential for personalization is cal-
culated over the two query logs to present an 
overview of what kind of queries that the op-
timal group-based retrieval model fails, which 
is supposed to benefit most from individual 
user information. 
One possible enrichment to this work may 
come from the employment of content analysis 
based on text processing techniques. The dif-
ferent clicks, which are the basis of our exami-
nation, may have similar or even exact content 
in their web pages. Though the manual check 
for a small scale sampling from the Sogou log 
yields less than 1% probability for such case, 
the content based examination will be definite-
ly more convincing than simple click counts. 
In addition, the queries for the two types of 
user information are not examined for their 
contents or the related information needs. Con-
tent analysis or linguistic view to these queries 
would be more informative. Both of these is-
sues are to be addressed in our future work. 
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project 
(Grant No.2006AA010108). The authors are 
grateful for the anonymous reviewers for their 
valuable comments. 
 
 
708
References 
Canny John. 2002. Collaborative filtering with pri-
vacy via factor analysis. In Proceedings of SI-
GIR? 02, pages 45-57. 
Carroll M. John and Mary B. Rosson. 1987. Para-
dox of the active user. Interfacing thought: cog-
nitive aspect of human-computer interaction, 
pages 80-111. 
Chirita A. Paul, Wofgang Nejdl, Raluca Paiu, and 
Christian Kohlschutter. 2005. Using odp metada-
ta to personalize search. In Proceedings of SI-
GIR ?05, pages 178-185. 
Cohen Jacob. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20: 37-46 
Dou Zhicheng, Ruihua Song, and Ju-Rong Wen. 
2007. A Large-scale Evaluation and Analysis of 
Personalized Search Strategies. In Proceedings 
of WWW ?07, pages 581-590. 
Fleiss L. Joseph. 1971. Measuring nominal scale 
agreement among many raters. Psychological 
Bulletin, 76(5): 378-382. 
Herlocker L. Jonathan, Joseph A. Konstan, Al 
Borchers, and John Riedl. 1999. An algorithmic 
framework for performing collaborative filtering. 
In Proceedings of SIGIR ?99, pages 230-237. 
Jansen J. Bernard, Amanda Spink, and Tefko Sara-
cevic. 2000. Real life, real users, and real needs: 
a study and analysis of user queries on the web. 
Information Processing and Management, pages 
207-227. 
J?rvelin Kalervo and Jaana Kek?l?inen. 2000. IR 
evaluation methods for retrieving highly relevant 
documents. In Proceedings of SIGIR ?00, pages 
41-48. 
Kohrs Arnd and Bernard Merialdo. 1999. Cluster-
ing for collaborative filtering applications. In 
Proceedings of CIMCA ?99, pages 199-204. 
Landis J. Richard and Gary. G. Koch. 1977. The 
mea-surement of observer agreement for cate-
gorical data. Biometrics 33: 159-174. 
Pitkow James, Hinrich Schutze, Todd Cass, Rob 
Cooley, Don Turnbull, Andy Edmonds, Eytan 
Adar and Thomas Breuel. 2002. Personalized 
search. ACM, 45(9):50-55. 
Shen Xuehua, Bin Tan and ChengXiang Zhai. 2005 
Implicit user modeling for personalized search. 
In Proceedings of CIKM ?05, pages 824-831. 
 
Silverstein Craig, Monika Henzinger, Hannes Ma-
rais and Michael Moricz. 1999. Analysis of a 
very large web search engine query log. SIGIR 
Forum, 33(1):6-12. 
Smyth Barry. 2007. A Community-Based Approach 
to Personalizing Web Search. IEEE Computer, 
40(8): 42-50. 
Speretta Mirco and Susan Gauch. Personalized 
Search based on user search histories. 2005. In 
Proceedings of WI ?05, pages 622-628. 
Spink Amanda, Dietmar Wolfram, Major Jansen, 
Tefko Saracevic. 2001. Searching the web: The 
public and their queries. Journal of the American 
Society for Information Science and Technology, 
52(3), 226-234 
Sugiyama Kazunari, Kenji Hatano, and Masatoshi 
Yoshikawa. 2004. Adaptive web search based on 
user profile constructed without any effort from 
users. In Proceedings of WWW ?04, pages 675-
684. 
Sun Jian-Tao, Hua-Jun Zeng, Huan Liu, Yuchang 
Lu and Zheng Chen. 2005. CubeSVD: a novel 
approach to personalized web search. In Pro-
ceedings of WWW?05, pages 382-390. 
Teevan Jaime, Susan T. Dumais, and Eric Horvitz. 
2005. Personalizing search via automated analy-
sis of interests and activities. In Proceedings of 
SIGIR ?05, pages 449-456. 
Teevan Jaime, Susan T. Dumais and Eric Horvitz. 
2007. Characterizing the value of personalizing 
search. In Proceedings of SIGIR ?07, pages 757-
758. 
Teevan Jaime, Susan T. Dumais and Daniel J. 
Liebling. 2008. To personalize or Not to Perso-
nalize: Modeling Queries with Variation in User 
Intent. In Proceedings of SIGIR ?08, pages 163-
170. 
Townsend Steve Cronen and W. Bruce Croft. 2002. 
Quantifying query ambiguity. In Proceedings of 
HLT ?02, pages 613-622. 
Yu Kai, Anton Schwaighofer, Volker Tresp, Xiao-
wei Xu, Hans-Peter Kriegel. 2004. Probabilistic 
Memory-based Collaborative Filtering. In IEEE 
Transactions on Knowledge and Data Engineer-
ing, pages 56-59. 
 
709
Coling 2010: Poster Volume, pages 1203?1210,
Beijing, August 2010
Utilizing Variability of Time and Term Content, within and across 
Users in Session Detection 
Shuqi Sun1, Sheng Li1, Muyun Yang1, Haoliang Qi2, Tiejun Zhao1 
1Harbin Institute of Technology, 2Heilongjiang Institute of Technology 
{sqsun, ymy, tjzhao}@mtlab.hit.edu.cn, lisheng@hit.edu.cn 
haoliang.qi@gmail.com 
Abstract 
In this paper, we describe a SVM classi-
fication framework of session detection 
task on both Chinese and English query 
logs. With eight features on the aspects 
of temporal and content information ex-
tracted from pairs of successive queries, 
the classification models achieve signifi-
cantly superior performance than the stat-
of-the-art method. Additionally, we find 
through ROC analysis that there exists 
great discrimination power variability 
among different features and within the 
same feature across different users. To 
fully utilize this variability, we build lo-
cal models for individual users and com-
bine their predictions with those from the 
global model. Experiments show that the 
local models do make significant im-
provements to the global model, although 
the amount is small. 
1 Introduction 
To provide users better experiences of search 
engines, inspecting users? activities and inferring 
users? interests are indispensible. Query logs rec-
orded by search engines serves well for these 
purposes. Query log conveys the user interest 
information in the form of slices of the query 
stream. Thus the task of session detection con-
sists in distinguishing slice that corresponds to a 
user interest from other ones, and thus this paper, 
we adopt the definition of a session following 
(Jansen et al, 2007): 
(A session is) a series of interactions by the us-
er toward addressing a single information need. 
This definition is equivalent to that of the 
?search goal? proposed by Jones and Klinkner 
(2008), which corresponds to an atomic infor-
mation need, resulting in one or more queries.  
This paper adopts a classification point of 
view to the task of session detection (Jones and 
Klinkner, 2008). Given a pair of successive que-
ries in a query log, we examine it in various 
viewpoints (i.e. features) such as time proximity 
and similarity of the content of the two queries to 
determine whether these two queries cross a bor-
der of a search session. In other words, we classi-
fy the gap between the two queries into two clas-
ses: session shift and session continuation. In 
practice, search goals in a search mission and 
different search missions could be intermingled, 
and increase the difficulty of correctly identify-
ing them. In this paper, we do not take this issue 
into account and simply treat all boundaries be-
tween intermingled search goals as session shifts. 
The chief advantage in this choice is that we will 
have the opportunity to make classification mod-
el working online without caching user?s queries 
that are pending to be assigned to a session. 
Various studies built accurate models in pre-
dicting session boundaries and in distinguishing 
intermingled sessions, and they are summarized 
in Section 2. However, none of these works ana-
lyzed the contribution of individual features from 
a user-oriented viewpoint, or evaluated a fea-
ture?s discrimination power in a general scenario 
independent of its usage, as this paper does by 
conducting ROC analyses. During these analyses, 
we found that the discrimination power of fea-
tures varies dramatically, and for different users, 
the discrimination power of a particular feature 
also does not remain constant.  
Thus, it is appealing to build local models for 
users with have sufficient size of training exam-
ples, and combine the local models? predictions 
with those made by the global model trained by 
the whole training data. However, few of previ-
1203
ous works build user-specific models for the sake 
of characterizing the variability in user?s search 
activities, except that of Murray et al (2006). To 
fully make use of these two aspects of variability, 
inspired by Murray et al, we build users? local 
models based on a much broader range of evi-
dences, and show that different local models vary 
to a great extent, and experiments show that the 
local models do make significant improvements 
to the global model, although the amount is small. 
The remainder of this paper is organized as 
follows: Section 2 summarizes the related work 
of the session detection task. In Section 3, we 
first describe our classification framework as 
well as the features utilized. Then we conduct 
various evaluations on both English and Chinese 
query logs. Section 4 introduces the approaches 
to building local models based on an analysis of 
the variability of the discrimination power of 
features, and combine predictions of local mod-
els with those of the global model. Section 5 dis-
cusses the experimental results and concludes 
this paper. 
2 Related Work 
The simplest method in session detection is 
defining a timeout threshold and marking any 
time gaps of successive queries that exceed the 
threshold as session shifts. The thresholds 
adopted in different studies were significantly 
different, ranging from 5 minutes to 30 minutes 
(Silverstein et al, 1999; He and G?ker, 2000; 
Radlinski and Joachims, 2005; Downey et al, 
2007). Other study suggested adopting a dynamic 
timeout threshold. Murray et al (2006) proposed 
a user-centered hierarchical agglomerative 
clustering algorithm to determine timeout 
threshold for each user dynamically, other than 
setting a fixed threshold. However, Jones and 
Klinkner (2008) pointed out that single timeout 
criterion is always of limited utility, whatever its 
length is, and incorporating timeout features with 
other various features achieved satisfactory 
classification accuracy.  
An effective approach to combining the time 
out features with various evidences for session 
detection is machine learning. He et al (2002) 
collected statistical information from human an-
notated query logs to predict the probability a 
?New? pattern indicates a session shift according 
to the time gap between successive queries. 
?zmutlu and colleagues re-examined He et al?s 
work, and explored other machine learning tech-
niques such as neural networks, multiple linear 
regression, Monte Carlo simulation, conditional 
probabilities (Gayo-Avello, 2009), and HMMs 
(?zmutlu, 2009). 
In recent studies, Jones and Klinkner (2008) 
built logistic regression models to identify search 
goals and missions, and tackled the intermingled 
search goal/mission issue by examining arbitrary 
pairs of queries in the query log. Another contri-
bution of Jones and Klinkner is that they made a 
thorough analysis of contributions of individual 
features. However, they explored the features? 
contributions from a feature selection point of 
view rather than from a user-oriented one, and 
thus failed to characterize the variability of the 
discrimination power of the features when ap-
plied to different users. 
3 Learning to Detect Session Shifts 
3.1 Feature Extraction 
We adopt eight features covering both the tem-
poral and the content aspect of pairs of succes-
sive queries. Most these features are commonly 
used by previous studies (He and G?ker, 2000; 
?zmutlu, 2006; Jones and Klinkner, 2008). 
However, in this paper, we will analyze their 
contributions to the resulted model in a quite dif-
ferent way from that in previous works. 
Let Q = (q1, q2, ? , qn) denote a query log.  
The features are extracted from every successive 
pair of queries (qi, qi+1). Table 1 summarizes the 
features we adopt. The normalization described 
in Table1 is done according to the type of the 
feature. Features describing characters are nor-
malized by the average length of the two queries, 
while those describing character-n-grams are 
normalized by the average size of the n-gram sets 
of the two queries. Character-n-grams (e.g. bi-
grams ?ca? and ?at? in ?cat?) are robust to dif-
ferent representations of the same topic (e.g. ?IR? 
as Information Retrieval) and typos (e.g. 
?speling? as ?spelling?), and serve as a simple 
stemming method. In practice, character-n-grams 
are accumulative, which means they consist of 
all m-grams with m ? n. 
The feature ?avg_ngram_distance?, a variant 
of the ?lexical distance? in (Gayo-Avello, 2009), 
is more complicated than to be described briefly. 
1204
Here we first define n-gram distance (ND) from 
qi to qj, which is formalized as follows: 
j
ji
ji n
n
ND
qin   gram--char. of #
qin occur   qin   gram--char. of #
1)qq( ?=?  
Note that character-n-grams are accumulative 
and there could be multiple occurrences of a 
character-n-gram in a query, so the number of a 
character-n-gram is the sum of that of all m-
grams with m ? n, and multiple occurrences are 
all considered. At last, the average of character-
n-gram distance (ACD) of the pair (qi, qi+1) is:  
2
)qq()qq(
)q,q( 111
iiii
ii
NDND
ACD
?+?
=
++
+
 
There are seven features describing the content 
aspect of a query pair, and they are more or less 
overlapped (e.g. edit_distance vs. common_char). 
However, we show in the next subsection that all 
these features are beneficial to the final perfor-
mance.  
Feature Description 
time_interval time interval between 
successive queries 
avg_ngram_ 
distance 
avg. of character-n-gram 
distances 
edit_disance normalized Levenshtein 
edit distance 
common_prefix normalized length of pre-
fix shared 
common_suffix normalized length of suf-
fix shared 
common_char normalized number of 
characters shared 
common_ngram normalized number of 
character-n-grams shared 
Jaccard_ngram Jaccard distance between 
character-n-gram sets 
Table 1. Features used in classification models 
3.2 Data Preparation 
The query logs we explored include an English 
search log tracked by AOL from Mar 1, 2006 to 
May, 31 2006 (Pass et al, 2006), and a Chinese 
search log tracked by Sogou.com, which is one 
of the major Chinese Search Engines, from Mar 
1, 2007 to Mar 31, 20071. We applied systematic 
sampling over the user space on the two logs, 
which yielded 223 users and 2809 users, corre-
sponding to 6407 and 6917 query instances re-
                                                 
1 http://www.sogou.com/labs/resources.html 
spectively2. Sampling over the user space instead 
of over the query space avoids the bias to the 
most active users who submit much more queries 
than average users. 
For each sampled dataset, we invited annota-
tors who are familiar with IR and search process 
to determine each pair of successive queries of 
interest is across the border of a session. We 
made trivial pre-split process under two rules: 
 Queries from different users are not in the 
same session. 
 Queries from different days are not in the 
same session.  
Table 2 shows some basic statistics of the an-
notated data set. During the annotation process, 
the annotators were guided to identify the user?s 
information need at the finest granularity ever 
possible, because we focus on the atomic infor-
mation needs as described in Section 1. Conse-
quently, the average numbers of queries in a ses-
sion in both query logs are lower than previous 
studies. 
 AOL log Sogou log 
Queries 6407 6917 
Sessions 4571 5726 
Queries per session 1.40 1.21 
Longest session 21 12 
Table 2. Summary of the annotation results in 
both query logs 
3.3 Learning Framework 
In this section we seek to build accurate global 
classification model based on the whole training 
data obtained in the previous sub-subsection for 
both the query logs. We built the models within 
SVM framework. The implementation of SVM 
we used is libSVM (Chang and Lin, 2001). For 
the sake of evaluations and of model integration 
in the next section, we set the prediction of SVM 
to be probability estimation of the test example 
being positive. All features were pre-scaled into 
[0, 1] interval. We adopted the polynomial kernel, 
and for both datasets, we exhaustively tried each 
of the subset of the eight features using 5-fold 
cross validation. We found that using all the 
eight features yielded the best classification ac-
curacy. Thus in the experiments in rest of this 
                                                 
2 The sampling schema and sample size was deter-
mined following (Gayo-Avello, 2009). 
1205
section and the next section, we adopt the entire 
feature set to build global classification models. 
There is one parameter to be determined for 
feature extraction: the length of character-n-
grams. The proper lengths on AOL log and 
Sogou log are different. We tried the length from 
1 to 9, and according to cross validation accuracy, 
we found the best lengths for the two logs as 6 
and 3 respectively. 
3.4 Experimental Results 
3.4.1 Baseline Methods 
We provide two base line methods for compari-
sons. The first method is the commonly used 
timeout methods. We tried different timeout 
thresholds from 5 minutes to 30 minutes with a 
step of 5 minutes, and found that for both query 
logs the 5 minutes? threshold yield the best over-
all performance.  
The second method achieved the best perfor-
mance on the AOL log (Gayo-Avello, 2009), 
which addresses the session detection problem 
using a geometric interpolation method, in com-
parison to previous studies on this query log. We 
re-implemented this method and evaluated it on 
both the datasets. Similarly, the best parameters 
for the two query logs are different, such as the 
length of a character-n-gram. We only report the 
performance with the best parameter settings. 
3.4.2 Analyzing the Performance  
We analyze the performance of the SVM models 
according to precision, recall, F1-mean and F1.5-
mean of predictions on session shift and continu-
ation against human annotation data. 
The F

-mean is defined as: 
RP
PR
+
+
=
2
2)1(
mean-F ?
?
?  
where P denotes precision and R denotes recall. 
He et al (2002) regards recall more important 
than precision, and set the value of   in F

-mean 
to 1.5. We also report performance under this 
measure. 
In addition to traditional precision / recall 
based measures, we also perform ROC (Receiver 
Operating Characteristic) analysis to determine 
the discrimination power of different methods. 
The best merit of ROC analysis is that given a 
reference set, which is usually the human annota-
tion results, it evaluates a set of indicator?s dis-
crimination power for arbitrary binary classifica-
tion problem independent of the critical value 
with which the class predictions are made.  
Specifically, in the context session detection, 
regardless of the critical value that splits the clas-
sifier outputs into positive ones and negative 
ones (e.g. the 5-minutes? timeout threshold and 
50% probability in SVM?s output), the ROC 
analysis provides the overall discrimination pow-
er evaluation of the output set of a certain meth-
od (by trying to set each output value as the criti-
cal value). For the baseline method by Gayo-
Avello, the core of the decision heuristics also 
had a critical value to be determined. For details, 
readers could refer to (Gayo-Avello, 2009).  
3.4.3 Precision, Recall, and F-means 
Before we examine the discrimination power of 
each session detection method?s output independ-
ent of the threshold value selected. In this sub-
subsection, we begin with a more traditional eval-
uation schema: setting a proper threshold to pro-
duce binary predictions. It is straightforward to set 
the threshold for SVM method to 50%, and as 
described in sub-subsection 3.1.1, the threshold 
for timeout method is 5 minutes. The threshold of 
Gayo-Avello?s method is implied in its heuristics. 
Table 3 and Table 4 show the experimental re-
sults on AOL log and Sogou log respectively. 
For each dataset, we performed 1000-times boot-
strap resampling, generating 1000 bootstrapped 
datasets with the same size as the original dataset. 
To test the statistical significance of performance 
differences, we adopted Wilcoxon signed-rank 
test on the performance measures computed from 
the 1000 bootstrapped dataset, and found com-
parisons between each pair of methods were all 
significant at 95% level. 
The results show that SVM method clearly 
outperforms the baseline methods, and timeout 
method performs poorly. It may be argued that 
the poor performance of timeout method is due 
to the improper threshold value chosen. In this 
case, the ROC analysis, which assesses the dis-
crimination power of a method?s output set inde-
pendent of the threshold value chosen, is more 
suitable for performance evaluation. 
Gayo-Avello method significantly outperforms 
the timeout method. But due to its heuristic na-
ture, it is less likely to do better than the super-
vised-learning methods, although it avoids the 
over fitting issue. The Gayo-Avello method?s 
unstable performance in predicting session con-
1206
tinuations implies that its heuristics did not gen-
eralize well to Chinese query logs. 
 Timeout Gayo-Avello SVM 
P 
shift 75.92 89.35 90.96 
cont. 63.05 85.32 92.06 
R 
shift 64.49 87.85 93.82 
cont. 74.77 87.08 88.50 
F1 
shift 69.74 88.60 92.37 
cont. 68.41 86.19 90.25 
F1.5 
shift 67.62 88.31 92.92 
cont. 70.72 86.53 89.57 
Table 3. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on AOL dataset.  
 Timeout Gayo-Avello SVM 
P 
shift 67.75 75.10 87.53 
cont. 52.82 83.51 81.62 
R 
shift 59.52 91.44 86.17 
cont. 61.53 58.84 83.33 
F1 
shift 63.37 82.47 86.85 
cont. 56.84 69.04 82.47 
F1.5 
shift 61.83 85.71 86.59 
cont. 58.56 64.72 82.80 
Table 4. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of SVM method and the two 
baseline methods on Sogou dataset. 
3.4.4 ROC Analysis 
By setting certain threshold value, we analyzed 
the three method?s performance using precision / 
recall based measures. In this sub-subsection, we 
try to set each value in an output set as the 
threshold value, and evaluate the discrimination 
power of methods by the area under the ROC 
curve. 
Figure 1 shows the ROC curves of the SVM 
method and the two baseline methods: timeout 
and Gayo-Avello, for predicting session shifts. 
ROC curves for predicting session continuations 
are symmetric with respect to the reference line, 
so we omit them in the rest of this paper for the 
sake of space limit.  
The results show that SVM method clearly 
outperforms the baseline methods in the prospec-
tive of discrimination power, with ROC area 
0.9562 on AOL dataset and 0.9154 on Sogou 
dataset. The curves of the two baseline methods 
are clearly under that of SVM method. This 
means baseline methods can never achieve accu-
racy as high as SVM method w.r.t. a fixed false 
alarm (classification error) rate, nor false alarm 
rate as low as SVM method w.r.t. a fixed accura-
cy rate. Again, Gayo-Avello method significantly 
outperforms timeout method, while underper-
forms the SVM method. For the question in the 
previous sub-subsection, coinciding with previ-
ous studies (Murray et al, 2006; Jones and 
Klinkner, 2008), applying single timeout thresh-
old always yields limited discrimination power, 
wherever the operating point on ROC curve (i.e. 
threshold value) is set. 
4 Making Use of the Variability of Dis-
crimination Power 
In this section, we first analyze the amount of 
contribution that each feature makes and show 
that the contribution, i.e. the discrimination pow-
er of each feature varies dramatically across dif-
ferent users. Then, we propose an approach to 
making use of this variability. Finally through 
experimental results, we show that the proposed 
approach makes small, yet significant improve-
ments to the SVM method in Section 3. 
4.1 Variability of Discrimination Power 
The ROC analysis of individual feature provides 
adequate characterizations of the discrimination 
power of the feature. Another advantage of 
adopting ROC analysis is that the results are in-
dependent not only of the critical value, but also 
of the scale of the feature values.  
Figure 2 shows the ROC curves of all the eight 
features in both datasets. Note that some features 
are with a higher value indicating session contin-
uation rather than session shift, so their ROC 
curves are below the reference line. The feature 
?time_interval? behaves exactly the same as the 
timeout method in Figure 1. For the rest of the 
features, ?avg_ngram_distance?, ?common_ngram? 
and ?Jaccard_ngram? achieve the best discrimi-
nation powers, showing the character-n-gram 
representation is effective. The feature ?com-
mon_char? performs significantly better in 
Sogou dataset than in AOL dataset, because Chi-
nese characters convey much more information 
than English characters do. ?common_suffix? 
performing worse than ?common_prefix? reflects 
the custom of users. Users tend to add terms at 
the end of the query in a searching iteration, thus 
predicting session continuations by examining 
the common suffixes is problematic. 
1207
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.7707
Gayo-Avello ROC area: 0.9130
SVM ROC area: 0.9562
Reference
AOL
    
0.
00
0.
25
0.
50
1.
00
0.
75
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
Timeout ROC area: 0.6365
Gayo-Avello ROC area: 0.8463
SVM ROC area: 0.9154
Reference
Sogou
 
Figure 1. ROC analysis of SVM method and two baseline methods for predicting session shifts on 
both AOL and Sogou dataset. All comparisons between ROC areas within the same dataset are at 
least 95% statistically significant, because the corresponding confidence intervals do not overlap. 
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.7707
avg_ngram_distance ROC area: 0.9560
edit_disance ROC area: 0.8848
common_prefix ROC area: 0.2177
common_suffix ROC area: 0.2985
common_char ROC area: 0.1360
common_ngram ROC area: 0.0480
Jaccard_ngram ROC area: 0.0464
Reference
AOL
  
0.
00
0.
25
0.
50
0.
75
1.
00
Se
ns
iti
vi
ty
0.00 0.25 0.50 0.75 1.00
1-Specificity
time_interval ROC area: 0.6365
avg_ngram_distance ROC area: 0.9108
edit_disance ROC area: 0.8333
common_prefix ROC area: 0.2449
common_suffix ROC area: 0.3745
common_char ROC area: 0.0922
common_ngram ROC area: 0.1018
Jaccard_ngram ROC area: 0.0965
Reference
Sogou
 
Figure 2. ROC analysis of individual features for predicting session shifts on both AOL and Sogou 
dataset. Note that some curves with similar ROC area values overlap each other. 
In spite of the discrimination power a feature 
has, its behavior on different users is worth-
while to be examined. For selecting users that 
have sufficient data to draw stable conclusions, 
we consider only users who issued more than 50 
queries in the datasets. Unfortunately, there are 
too few users (6 users) qualified in Sogou da-
taset, so we show only the statistics of ROC 
area values of each of the features in Table 5 
based on 37 users in AOL dataset. 
The statistics in Table 5 show that for differ-
ent users. Recall that in sub-subsection 3.3.2, a 
0.04 difference of ROC area make the perfor-
mance of the SVM method significantly better 
1208
than that of the Gayo-Avello?s method. Thus, 
the discrimination power of a feature is likely to 
vary significantly, because all the standard de-
viations are at 0.03 or even higher level. Espe-
cially, the minimum and maximum values show 
that for these users, some of the findings above 
from the whole dataset do not hold. This implies 
that it is likely more feasible to build specific 
local models for these users to make full use of 
the variability within the same feature. 
Feature avg. sdev. min. max. 
time_interval 0.780 0.088 0.476 0.912
avg_ngram_ 
distance 
0.954 0.034 0.861 1.000
edit_disance 0.883 0.056 0.733 0.990
common_prefix 0.224 0.069 0.099 0.327
common_suffix 0.299 0.113 0.064 0.578
common_char 0.143 0.082 0.037 0.493
common_ngram 0.051 0.037 0.000 0.187
Jaccard_ngram 0.049 0.036 0.000 0.173
Table 5. Average, standard deviation, minimum, 
and maximum ROC areas of individual features 
4.2 Building Local Models 
We built individual local models for each user 
that issued more than 50 queries in AOL dataset. 
We also performed 5-fold cross validations and 
set the prediction to be the probability estima-
tion of a test example being positive. The fea-
ture selection process showed again that all the 
eight features are beneficial, and none of them 
should be excluded. 
In each fold of cross validation, we per-
formed 90%-bagging on the training set 10 
times to get the variance estimations of the local 
model. For each example in the test set, we set 
the final output on it to be the average of the 10 
outputs, and recorded the standard deviation of 
the outputs on this example which is used dur-
ing the model combination. We also conducted 
the same process for the global model for the 
sake of combination process described below. 
4.3 Combing with the Global Model 
Since the predictions of both the local and the 
global models are probability estimations, it is 
reasonable to combine them using linear combi-
nation. For each example, there are two outputs 
Ol and Og coming from local and global models 
accordingly. For each example e of a user?s sub 
dataset U, we have the outputs Ol(e) and Og(e) 
as well as the normalized deviations Dl(e) and 
Dg(e) (by the largest deviation in U of the corre-
sponding models). The final output O(e) is de-
fined as: 
)()(
)()()()(
)(
eDeD
eOeDeOeD
eO
gl
glgl
+
?+?
=
 
 Global Local Combine 
P 
shift 90.48 88.53 90.43 
cont. 91.75 92.12 92.52 
R 
shift 93.94 94.44 94.56 
cont. 87.20 84.16 87.04 
F1 
shift 92.18 91.39 92.45 
cont. 89.41 87.96 89.69 
F1.5 
shift 92.85 92.54 93.25 
cont. 88.55 86.46 88.65 
Table 6. Precision (P), recall (R), F1-mean (F1), 
and F1.5-mean (F1.5) of global model (bagging), 
local model (bagging) and combined model  
This combination process is similar to (Osl et 
al., 2008). Note that the more the deviation of a 
model is, the less feasible the corresponding 
model is. We compared the performance of 
three models: global model, local model, and 
combined model. The results are summarized in 
Table 6. All comparisons between different 
models are statistically significant at 95% level, 
based on the same bootstrapping settings in sub-
subsection 3.4.3. The combined model shows 
slight (may due to the inferior performance of 
the local model), yet significant improvement to 
the global model. In spite of the amount of the 
improvement, the local model did correct some 
errors of the global model. It may be not ac-
ceptable to build such an expensive combined 
model for a limited improvement. Nevertheless, 
the results do show that the variability across 
different users is exploitable. 
5 Discussion and Conclusion 
In this paper, we built a learning framework of 
detecting sessions which corresponds to user?s 
interest in a query log. We considered two as-
pect of a pair of successive queries: temporal 
aspect and content aspect, and designed eight 
features based on these two aspects, and the 
SVM models built with these features achieved 
satisfactory performance (92.37% F1-mean on 
session shift, 90.25% F1-mean on session con-
tinuation), significantly better than the best-ever 
approach on AOL query log. 
1209
The analysis of the features? discrimination 
power was conducted not only among different 
features, but also within the same feature when 
applied to different users in the query log. By 
analyzing the statistics of ROC area values of 
each of the features based on 37 users in AOL 
dataset, experimental results showed that there 
is considerable variability in both these aspects. 
To make full use of this variability, we built 
local models for individual user and combine 
the yielded predictions with those yielded by the 
global model. Experiments showed that the lo-
cal model did make significant improvements to 
the global model, although the amount was 
small (92.45% vs. 92.18% F1-mean on session 
shift, 89.69% vs. 89.41% F1-mean on session 
continuation). 
In future studies, we will explore other learn-
ing frameworks which better integrate the local 
model and the global model, and will try to ac-
quire more data to build local models. We will 
also analyze more deeply the characteristics of 
ROC analysis in the feature selection process.  
Acknowledgement 
This work is supported by the Key Project of 
Natural Science Foundation of China (Grant 
No.60736044), and National 863 Project (Grant 
No.2006AA010108). The authors are grateful 
for the anonymous reviewers for their valuable 
comments. 
References 
Chang Chih-Chung and Chih-Jen Lin. 2001. 
LIBSVM : a library for support vector machines. 
Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Downey Doug, Susan Dumais, and Eric Horvitz. 
2007. Models of searching and brows-
ing: languages, studies, and applications. In Pro-
ceedings of the 20th international joint conference 
on Artificial intelligence, pages 2740-2747, Hy-
derabad, India. 
Gayo-Avello Daniel. 2009. A survey on session de-
tection methods in query logs and a proposal for 
future evaluation, Information Science 
179(12):1822-1843. 
He Daqing and Ayse G?ker. 2000. Detecting Session 
Boundaries from Web User Logs. In BCS/IRSG 
22nd Annual Colloqui-um on Information Re-
trieval Research, pages 57-66.  
He Daqing, Ayse G?ke, and David J. Harper. 2002. 
Combining evidence for automatic web session 
identification. Information Processing and Man-
agement: an International Journal, 38(5):727-742. 
Jansen Bernard J., Amanda Spink, Chris Blakely, 
and Sherry Koshman. 2007. Defining a session on 
Web search engines: Research Articles. Journal of 
the American Society for Information Science and 
Technology, 58(6):862-871 
Jones Rosie and Kristina Lisa Klinkner. 2008. Be-
yond the session timeout: automatic hierarchical 
segmentation of search topics in query logs. In 
Proceedings of the 17th ACM conference on In-
formation and knowledge management, pages 
699-708, Napa Valley, California, USA. 
Murray G. Craig, Jimmy Lin, and Abdur Chowdhury. 
2007. Identification of user sessions with hierar-
chical agglomerative clustering. American Society 
for Information Science and Technology, 43(1):1-
9. 
Osl Melanie, Christian Baumgartner, Bernhard Tilg, 
and Stephan Dreiseitl. 2008. On the combination 
of logistic regression and local probability esti-
mates. In Proceedings of Third International Con-
ference on Broadband Communications, Infor-
mation Technology & Biomedical Applications, 
pages 124-128. 
?zmutlu Seda. 2006. Automatic new topic identifi-
cation using multiple linear regression. Infor-
mation Processing and Management: an Interna-
tional Journal, 42(4):934-950. 
?zmutlu Huseyin C. 2009. Markovian analysis for 
automatic new topic identification in search en-
gine transaction logs. Applied Stochastic Models 
in Business and Industry, 25(6):737-768. 
Pass Greg, Abdur Chowdhury, and Cayley Torgeson. 
2006. A picture of search. In Proceedings of the 
1st international conference on Scalable infor-
mation systems, Hong Kong. 
Radlinski Filip and Thorsten Joachims. 2005. Query 
chains: learning to rank from implicit feedback. In 
Proceedings of the eleventh ACM SIGKDD inter-
national conference on Knowledge discovery in 
data mining, pages 239-248, Chicago, Illinois, 
USA. 
Silverstein Craig, Hannes Marais, Monika Henzinger, 
and Michael Moricz. 1999. Analysis of a very 
large web search engine query log. ACM SIGIR 
Forum, 33(1):6-12. 
1210
Coling 2010: Poster Volume, pages 1533?1540,
Beijing, August 2010
All in Strings: a Powerful String-based Automatic MT  
Evaluation Metric with Multiple Granularities 
 
Junguo Zhu1, Muyun Yang1, Bo Wang2, Sheng Li1, Tiejun Zhao1 
 
1 School of Computer Science and Technology, Harbin Institute of Technology 
{jgzhu; ymy; tjzhao; lish}@mtlab.hit.edu.cn 
2 School of Computer Science and Technology, Tianjin University  
bo.wang.1979@gmail.com 
 
 
Abstract 
String-based metrics of automatic ma-
chine translation (MT) evaluation are 
widely applied in MT research. Mean-
while, some linguistic motivated me-
trics have been suggested to improve 
the string-based metrics in sentence-
level evaluation. In this work, we at-
tempt to change their original calcula-
tion units (granularities) of string-based 
metrics to generate new features. We 
then propose a powerful string-based 
automatic MT evaluation metric, com-
bining all the features with various 
granularities based on SVM rank and 
regression models. The experimental 
results show that i) the new features 
with various granularities can contri-
bute to the automatic evaluation of 
translation quality; ii) our proposed 
string-based metrics with multiple gra-
nularities based on SVM regression 
model can achieve higher correlations 
with human assessments than the state-
of-art  automatic metrics. 
1 Introduction 
The automatic machine translation (MT) eval-
uation has aroused much attention from MT 
researchers in the recent years, since the auto-
matic MT evaluation metrics can be applied to 
optimize MT systems in place of the expensive 
and time-consuming human assessments. The 
state-of-art strategy to automatic MT evalua-
tion metrics estimates the system output quali-
ty according to its similarity to human refer-
ences. To capture the language variability ex-
hibited by different reference translations, a 
tendency is to include deeper linguistic infor-
mation into machine learning based automatic 
MT evaluation metrics, such as syntactic and 
semantic information (Amig? et al, 2005; Al-
brecht and Hwa, 2007; Gim?nez and M?rquez, 
2008). Generally, such efforts may achieve 
higher correlation with human assessments by 
including more linguistic features. Neverthe-
less, the complex and variously presented lin-
guistic features often prevents the wide appli-
cation of the linguistic motivated metrics. 
Essentially, linguistic motivated metrics in-
troduce additional restrictions for accepting the 
outputs of translations (Amig? et al, 2009).  
With more linguistic features attributed, the 
model is actually capturing the sentence simi-
larity in a finer granularity. In this sense, the 
practical effect of employing various linguistic 
knowledge is changing the calculation units of 
the matching in the process of the automatic 
evaluation. 
Similarly, the classical string-based metrics 
can be changed in their calculation units direct-
ly. For example, the calculation granularity in 
BLEU (Papineni et al, 2002) metric is word: 
n-grams are extracted on the basis of single 
word as well as adjacent multiple words. And 
the calculation granularity in PosBLEU 
(Popovi? and Ney, 2009) metric is Pos tag, 
which correlate well with the human assess-
ments. Therefore, it is straight forward to apply 
the popular string-based automatic evaluation 
metrics, such as BLEU, to compute the scores 
of the systems outputs in the surface or linguis-
1533
tic tag sequences on various granularities le-
vels. 
In this paper, we attempt to change the orig-
inal calculation units (granularities) of string-
based metrics to generate new features. After 
that, we propose a powerful string-based au-
tomatic MT evaluation metric, combining all 
the features with various granularities based on 
SVM rank (Joachims, 2002) and regression 
(Drucker et al, 1996) models. Our analysis 
indicates that: i) the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality; ii) our pro-
posed string-based metrics with multiple gra-
nularities based on SVM regression model can 
achieve higher correlations with human as-
sessments than the state-of-art automatic me-
trics . 
The remainder of this paper is organized as 
follows: Section 2 reviews the related re-
searches on automatic MT evaluation. Section 
3 describes some new calculation granularities 
of string-based metrics on sentence level. In 
Section 4, we propose string-based metrics 
with multiple granularities based on SVM rank 
and regression models. In Section 5, we 
present our experimental results on different 
sets of data. And conclusions are drawn in the 
Section 6. 
2 Related Work on Automatic Ma-
chine Translation Evaluation 
The research on automatic string-based ma-
chine translation (MT) evaluation is targeted at 
a widely applicable metric of high consistency 
to the human assessments. WER (Nie?en et al, 
2000), PER (Tillmann et al, 1997), and TER 
(Snover et al, 2006) focuses on word error rate 
of translation output. GTM (Melamed et al, 
2003) and the variants of ROUGE (Lin and 
Och, 2004) concentrate on matched longest 
common substring and discontinuous substring 
of translation output according to the human 
references. BLEU (Papineni et al, 2002) and 
NIST (Doddington, 2002) are both based on 
the number of common n-grams between the 
translation hypothesis and human reference 
translations of the same sentence. BLEU and 
NIST are widely adopted in the open MT eval-
uation campaigns; however, the NIST MT 
evaluation in 2005 indicates that they can even 
error in the system level (Le and Przybocki, 
2005). Callison-Burch et al (2006) detailed the 
deficits of the BLEU and other similar metrics, 
arguing that the simple surface similarity cal-
culation between the machines translations and 
the human translations suffers from morpho-
logical issues and fails to capture what are im-
portant for human assessments.  
In order to attack these problems, some me-
trics have been proposed to include more lin-
guistic information into the process of match-
ing, e.g., Meteor (Banerjee and Lavie, 2005) 
metric and MaxSim (Chan nad Ng, 2008) me-
trics, which improve the lexical level by the 
synonym dictionary or stemming technique. 
There are also substantial studies focusing on 
including deeper linguistic information in the 
metrics (Liu and Gildea, 2005; Owczarzak et 
al., 2006; Amig? et al, 2006; Mehay and Brew, 
2007; Gim?nez and M?rquez, 2007; Owczar-
zak et al, 2007; Popovic and Ney, 2007; 
Gim?nez and M?rquez, 2008b). 
A notable trend improving the string-based 
metric is to combine various deeper linguistic 
information via machine learning techniques in 
the metrics (Amig? et al, 2005; Albrecht and 
Hwa, 2007; Gim?nez and M?rquez, 2008). 
Such efforts are practically amount of intro-
ducing additional linguistic restrictions into the 
automatic evaluation metrics (Amig? et al 
2009), achiving a higher performance at the 
cost of lower adaptability to other languages 
owing to the language dependent linguistics 
features. 
Previous work shows that including the new 
features into the evaluation metrics may bene-
fit to describe nature language accurately. In 
this sense, the string-based metrics will be im-
proved, if the finer calculation granularities are 
introduced into the metrics.  
Our study analyzes the role of the calcula-
tion granularities in the performance of metrics. 
We find that the new features with various 
granularities can contribute to the automatic 
evaluation of translation quality. Also we pro-
pose a powerful string based automatic MT 
evaluation metric with multiple granularities 
combined by SVM. Finally, we seek a finer 
feature set of metrics with multiple calculation 
granularities. 
1534
3 The New Calculation Granularities 
of String-based Metrics on Sentence 
Level  
The string-based metrics of automatic machine 
translation evaluation on sentence level adopt a 
common strategy: taking the sentences of the 
documents as plain strings. Therefore, when 
changing the calculation granularities of the 
string-based metrics we can simplify the in-
formation of new granularity with plain strings.  
In this work, five kinds of available calculation 
granularities are defined: ?Lexicon?, ?Letter?, 
?Pos?, ?Constitute? and ?Dependency?.  
Lexicon: The calculation granularity is 
common word in the sentences of the docu-
ments, which is popular practice at present. 
Letter: Split the granularities of ?Lexical? 
into letters. Each letter is taken as a matching 
unit. 
Pos: The Pos tag of each ?Lexicon? is taken 
as a matching unit in this calculation granulari-
ty. 
Constitute: Syntactic Constitutes in a tree 
structure are available through the parser tools. 
We use Stanford Parser (Klein and Manning, 
2003a; Klein and Manning, 2003b) in this 
work.  The Constitute tree is changed into 
plain string, travelling by BFS (Breadth-first 
search traversal) 1.  
Dependency: Dependency relations in a de-
pendency structure are also available through 
the parser tools. The dependency structure can 
also be formed in a tree, and the same 
processing of being changed into plain string is 
adopted as ?Constitute?. 
The following serves as an example:  
Sentence:  
I have a dog 
Pos tag:  
I/PRON have/V a/ART dog/N 
Constitute tree:  
 
                                                 
1 We also attempt some other traversal algorithms, in-
cluding preorder, inorder and postorder traversal, the 
performance are proved to be similar.  
Dependency tree:  
 
Then, we can change the sentence into the 
plain string in multiple calculation granulari-
ties as follows: 
Lexicon string:  
I have a dog 
Letter string:  
I h a v e a d o g 
Pos string: 
PRON V ART N 
Constitute string:  
PRON V ART N NP NP VP S 
Dependency string: 
 a I dog have 
The translation hypothesis and human refer-
ence translations are both changed into those 
strings of various calculation granularities. The 
strings are taken as inputs of the string-based 
automatic MT evaluation metrics. The outputs 
of each metric are calculated on different 
matching units. 
4 String-based Metrics with Multiple 
Granularities Combined by SVM 
Introducing machine learning methods to es-
tablished MT evaluation metric is a popular 
trend. Our study chooses rank and regression 
support vector machine (SVM) as the learning 
model. Features are important for the SVM 
models. 
Plenty of scores can be generated from the 
proposed metrics. In fact, not all these features 
are needed. Therefore, feature selection should 
be a necessary step to find a proper feature set 
and alleviate the language dependency by us-
ing fewer linguistic features. 
Feature selection is an NP-Complete prob-
lem; therefore, we adopt a greedy selection 
algorithm called ?Best One In? to find a local 
optimal feature set. Firstly, we select the fea-
ture among all the features which best corre-
lates with the human assessments.  Secondly, a 
feature among the rest features is added in to 
the feature set, if the correlation with the hu-
man assessments of the metric using new set is 
1535
the highest among all new metrics and higher 
than the previous metric in cross training cor-
pus. The cross training corpus is prepared by 
dividing the training corpus into five parts. 
Each four parts of the five are for training and 
the rest one for testing; then, we integrate 
scores of the five tests as scores of cross train-
ing corpus.  The five-fold cross training can 
help to overcome the overfitting. At the end, 
the feature selection stops, if adding any of the 
rest features cannot lead to higher correlation 
with human assessments than the current me-
tric.  
5 Experiments 
5.1 The Impact of the Calculation Granu-
larities on String-based Metrics 
In this section, we use the data from NIST 
Open MT 2006 evaluation (LDC2008E43), 
which is described in Table 1.  It consists of 
249 source sentences that were translated by 
four human translators as well as 8 MT sys-
tems. Each machine translated sentence was 
evaluated by human judges for their adequacy 
on a 7-point scale. 
 
 NIST 2002  
NIST 
2003  
NIST 
Open 
MT 2006
LDC 
corpus 
LDC2003
T17 
LDC2006
T04 
LDC2008
E43 
Type Newswire Newswire Newswire
Source Chinese Chinese Arabic 
Target English English English 
# of  
sentences 878 919 249 
# of 
systems 3 7 8 
#  of 
references 4 4 4 
Score 
1-5, 
adequacy 
& fluency
1-5, 
adequacy 
& fluency 
1-7 
adequacy
Table 1: Description of LDC2006T04, 
LDC2003T17 and LDC2008E43 
 
To judge the quality of a metric, we com-
pute Spearman rank-correlation coefficient, 
which is a real number ranging from -1 (indi-
cating perfect negative correlations) to +1 (in-
dicating perfect positive correlations), between 
the metric?s scores and the averaged human 
assessments on test sentences. 
We select 21 features in ?lexicon? calcula-
tion granularity and 11?4 in the other calcula-
tion granularities. We analyze the correlation 
with human assessments of the metrics in mul-
tiple calculation granularities.  Table 2 lists the 
optimal calculation granularity of the multiple 
metrics on sentence level in the data 
(LDC2008E43).  
 
Metric Granularity 
BLEU-opt Letter 
NIST-opt Letter 
GTM(e=1) Dependency 
TER Letter 
PER Lexicon 
WER Dependency 
ROUGE-opt Letter 
Table 2 The optimal calculation granularity of the 
multiple metrics 
 
The most remarkable aspect is that not all 
the best metrics are based on the ?lexicon? cal-
culation granularities, such as the ?letter? and 
?dependency?. In other words, the granulari-
ties-shifted string-based metrics are promising 
to contribute to the automatic evaluation of 
translation quality. 
5.2 Correlation with Human Assessments 
of String-based Metrics with Multiple 
Granularities Based on SVM Frame 
We firstly train the SVM rank and regression 
models on LDC2008E43 using all the features 
(21+11? 4 species), without any selection. 
Secondly, the other two SVM rank and regres-
sion models are trained on the same data using 
the feature set via feature selection, which are 
described in Table 3. We have four string-
based evaluation metrics with multiple granu-
larities on rank and regression SVM frame 
?Rank_All, Regression_All, Rank_Select and 
Regression_Select?.  Then we apply the four 
metrics to evaluate the sentences of the test 
data (LDC2006T04 and LDC2003T17). The 
results of Spearman correlation with human 
assessments is summarized in Table 3. For 
comparison, the results from some state-of-art 
metrics (Papineni et al, 2002; Doddington, 
1536
2002; Melamed et al, 2003; Banerjee and La-
vie, 2005; Snover et al, 2006; Liu and Gildea, 
2005) and two machine learning methods (Al-
brecht and Hwa, 2007; Ding Liu and Gildea, 
2007) are also included in Table 3. Of the two 
machine learning methods, both trained on the 
data LDC2006T04. The ?Albrecht, 2007? 
score reported a result of Spearman correlation 
with human assessments on the data 
LDC2003T17 using 53 features, while the 
?Ding Liu, 2007? score reported that under 
five-fold cross validation on the data 
LDC2006T04 using 31 features. 
 
 Feature number 
LDC
2003
T17 
LDC
2006
T04 
Rank_All 65 0.323 0.495
Regression_All 65 0.345 0.507
Rank_Select 16 0.338 0.491
Regression_Select 8 0.341 0.510
Albrecht, 2007 53 0.309 -- 
Ding Liu, 2007 31 -- 0.369
BLEU-opt2 -- 0.301 0.453
NIST-opt -- 0.219 0.417
GTM(e=1) -- 0.270 0.375
METEOR3 -- 0.277 0.463
TER -- -0.250 -0.302
STM-opt -- 0.205 0.226
HWCM-opt -- 0.304 0.377
 
Table 3: Comparison of Spearman correlations with 
human assessments of our proposed metrics and 
some start-of-art metrics and two machine learning 
methods 
?-opt? stands for the optimum values of the pa-
rameters on the metrics 
 
Table 3 shows that the string-based meta-
evaluation metrics with multiple granularities 
based on SVM frame gains the much higher 
Spearman correlation than other start-of-art 
metrics on the two test data and, furthermore, 
our proposed metrics also are higher than the 
machine learning metrics (Albrecht and Hwa, 
2007; Ding Liu and Gildea, 2007).  
The underlining is that our proposed metrics 
are more robust than the aforementioned two 
                                                 
2 The result is computed by mteval11b.pl.  
3 The result is computed by meteor-v0.7. 
machine learning metrics. As shown in Table 1 
the heterogeneity between the training and test 
data in our method is much more significant 
than that of the other two machine learning 
based methods.  
In addition, the ?Regression_Select? metric 
using only 8 features can achieve a high corre-
lation rate which is close to the metric pro-
posed in ?Albrecht, 2007? using 53 features, 
?Ding Liu, 2007? using 31 features, ?Regres-
sion_All? and ?Rank_All? metrics using  65 
features and ?Rank_Select? metric using 16 
features. What is more, ?Regression_Select? 
metric is better than ?Albrecht, 2007?, and 
slightly lower than ?Regression_All? on the 
data LDC2003T17; and better than both ?Re-
gression_All? and ?Rank_All? metrics on the 
data LDC2006T04. That confirms that a small 
cardinal of feature set can also result in a me-
tric having a high correlation with human as-
sessments, since some of the features represent 
the redundant information in different forms. 
Eliminating the redundant information is bene-
fit to reduce complexity of the parameter 
searching and thus improve the metrics per-
formance based on SVM models. Meanwhile, 
fewer features can relieve the language depen-
dency of the machine learning metrics. At last, 
our experimental results show that regression 
models perform better than rank models in the 
string-based metrics with multiple granularities 
based on SVM frame, since ?Regres-
sion_Select? and ?Regression_All? achieve 
higher correlations with human assessments 
than the others. 
5.3 Reliability of Feature Selection  
The motivation of feature selection is keeping 
the validity of the feature set and alleviating 
the language dependency. We also look for-
ward to the higher Spearman correlation on the 
test data with a small and proper feature set.  
We use SVM-Light (Joachims, 1999) to 
train our learning models using NIST Open 
MT 2006 evaluation data (LDC2008E43), and 
test on the two sets of data, NIST?s 2002 and 
2003 Chinese MT evaluations. All the data are 
described in Table 1. To avoid the bias in the 
distributions of the two judges? assessments in 
NIST?s 2002 and 2003 Chinese MT evalua-
tions, we normalize the scores following (Blatz 
et al, 2003). 
1537
We trace the process of the feature selection. 
The selected feature set of the metric based on 
SVM rank includes 16 features and that of the 
metric based on SVM regression includes 8 
features. The selected features are listed in Ta-
ble 4. The values in Table 4 are absolute 
Spearman correlations with human assess-
ments of each single feature score.  The prefix-
es ?C_?, ?D_?, ?L_?, ?P_?, and ?W_? 
represent ?Constitute?, ?Dependency?, ?Let-
ter?, ?Pos? and ?Lexicon? respectively. 
 
Rank spear-man Regression
spear-
man
C_PER .331 C_PER .331
C_ROUGE-W .562 C_ROUGE-W .562
D_NIST9 .479 D_NIST9 .479
D_ROUGE-W .679 D_ROUGE-L .667
L_BLEU6 .702 L_BLEU6 .702
L_NIST9 .691 L_NIST9 .691
L_ROUGE-W .634 L_ROUGE-W .634
P_PER .370 P_ROUGE-W .683
P_ROUGE-W .616  
W_BLEU1_ind .551  
W_BLEU2 .659  
W_GTM .360  
W_METEOR .693  
W_NIST5 .468  
W_ROUGE1 .642  
W_ROUGE-W .683  
 
Table 4: Feature sets of SVM rank and regression 
 
Table 4 shows that 8 features are selected 
from 65 features in the process of feature se-
lection based on SVM regression while 16 fea-
tures based on SVM rank. Fewer features 
based on SVM regression are selected than 
SVM rank. Only one feature in feature set 
based on SVM regression does not occur in 
that based on SVM rank. The reason is that 
there are more complementary advantages be-
tween the common selected features.  
Next, we will verify the reliability of our 
feature selection algorithm. Figure 1 and Fig-
ure 2 show the Spearman correlation values 
between our SVM-based metrics (regression 
and rank) and the human assessments on both 
training data (LDC2008E43) and test data 
(LDC2006T04 and LDC2003T17).  
 
 
 
Figure 1: The Spearman correlation values between 
our SVM rank metrics and the human assessments 
on both training data and test data with the exten-
sion of the feature sets 
 
 
 
Figure 2: The Spearman correlation values between 
our SVM regression metrics and the human as-
sessments on both training data and test data with 
the extension of the feature sets 
 
From Figure 1 and Figure 2, with the exten-
sion of the feature sets, we can find that the 
tendency of correlation obtained by each me-
tric based on SVM rank or regression roughly 
the same on both the training data and test data. 
Therefore, the two feature sets of SVM rank 
and regression models are reliable. 
6 Conclusion 
In this paper we propose an integrated platform 
for automatic MT evaluation by improving the 
string based metrics with multiple granularities. 
Our proposed metrics construct a novel inte-
grated platform for automatic MT evaluation 
based on multiple features. Our  key contribu-
tion consists of two parts: i) we suggest a strat-
egy  of changing the various complex features 
into plain string form. According to the strate-
gy, the automatic MT evaluation frame are 
1538
much more clarified, and the computation of 
the similarity is much more simple, since the 
various linguistic features may express in the 
uniform strings with multiple calculation gra-
nularities. The new features have the same 
form and are dimensionally homogeneous; 
therefore, the consistency of the features is 
enhanced strongly. ii) We integrate the features 
with machine learning and proposed an effec-
tive approach of feature selection. As a result, 
we can use fewer features but obtain the better 
performance. 
In this framework, on the one hand, string-
based metrics with multiple granularities may 
introduce more potential features into automat-
ic evaluation, with no necessarily of new simi-
larity measuring method, compared with the 
other metrics. On the other hand, we succeed 
in finding a finer and small feature set among 
the combinations of plentiful features, keeping 
or improving the performance. Finally, we 
proposed a simple, effective and robust string-
based automatic MT evaluation metric with 
multiple granularities. 
Our proposed metrics improve the flexibility 
and performance of the metrics based on the 
multiple features; however, it still has some 
drawbacks: i) some potential features are not 
yet considered, e.g. the semantic roles; and ii) 
the loss of information exists in the process of 
changing linguistic information into plain 
strings. For example, the dependency label in 
the calculation granularity ?Dependency? is 
lost when changing information into string 
form. Though the final results obtain the better 
performance than the other linguistic metrics, 
the performance is promising to be further im-
proved if the loss of information can be prop-
erly dealt with. 
Acknowledgement 
This work is supported by Natural Science 
foundation China (Grant No.60773066 & 
60736014) and National Hi-tech Program 
(Project No.2006AA010108), and the Natural 
Scientific Reserach Innovation Foundation in 
Harbin Institute of Technology (Grant No. 
HIT.NSFIR.20009070). 
 
References 
Albrecht S. Joshua and Rebecca Hwa. 2007. A 
Reexamination of Machine Learning Approaches 
for Sentence-Level MT Evaluation. In Proceed-
ings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 880-
887. 
Amig? Enrique, Julio Gonzalo, Anselmo P?nas, 
and Felisa Verdejo. 2005. QARLA: a Framework 
for the Evaluation of Automatic Summarization. 
In Proceedings of the 43th Annual Meeting of 
the Association for Computational Linguistics. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo,  
Felisa Verdejo. 2009. The Contribution of Lin-
guistic Features to Automatic Machine Transla-
tion Evaluation. In proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL 
and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP. 
Amig? Enrique, Jes?s Gim?nez, Julio Gonzalo, and 
Llu?s M?rquez. 2006. MT Evaluation: Human- 
Like vs. Human Acceptable. In Proceedings of 
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Lin-
guistic, pages 17?24. 
Banerjee Satanjeev and Alon Lavie. 2005. ME-
TEOR: An automatic metric for MT evaluation 
with improved correlation with human judg-
ments. In Proceedings of the ACL Workshop on 
Intrinsic and Extrinsic Evaluation Measures. 
Blatz John, Erin Fitzgerald, George Foster, Simona 
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto 
Sanchis, and Nicola Ueffing. 2003. Confidence 
estimation for machine translation. In Technical 
Report Natural Language Engineering Workshop 
Final Report, pages 97-100. 
 Callison-Burch Chris, Miles Osborne, and Philipp 
Koehn. 2006. Re-evaluating the Role of BLEU in 
Machine Translation Research. In Proceedings 
of 11th Conference of the European Chapter of 
the Association for Computational Linguistics  
Chan S. Yee and Hwee T. Ng. 2008. MAXSIM: A 
maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL-08: HLT, 
pages 55?62. 
Doddington George. 2002. Automatic Evaluation of 
Machine Translation Quality Using N-gram Co- 
Occurrence Statistics. In Proceedings of the 2nd 
International Conference on Human Language 
Technology, pages 138?145. 
1539
Drucker Harris, Chris J. C. Burges, Linda Kaufman, 
Alex Smola, Vladimir Vapnik. 1996. Support 
vector regression machines.  In NIPS. 
Gim?nez Jes?s and Llu?s M?rquez. 2007. Linguistic 
Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL 
Workshop on Statistical Machine Translation. 
Gim?nez Jes?s and Llu?s M?rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through 
Non-Parametric Metric Combinations. In Pro-
ceedings of IJCNLP, pages 319?326. 
Gim?nez Jes?s and Llu?s M?rquez. 2008b. On the 
Robustness of Linguistic Features for Automatic 
MT Evaluation. 
Joachims Thorsten. 2002. Optimizing search en-
gines using clickthrough data. In KDD. 
Klein Dan and Christopher D. Manning. 2003a. 
Fast Exact Inference with a Factored Model for 
Natural Language Parsing. In Advances in 
Neural Information Processing Systems 15, pp. 
3-10.  
Klein Dan and Christopher D. Manning. 2003b. 
Accurate Unlexicalized Parsing. Proceedings of 
the 41st Meeting of the Association for Compu-
tational Linguistics, pp. 423-430. 
Le Audrey and Mark Przybocki. 2005. NIST 2005 
machine translation evaluation official results. 
In Official release of automatic evaluation scores 
for all submission. 
Lin Chin-Yew and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quali-
ty Using Longest Common Subsequence and 
Skip-Bigram Statistics. Proceedings of the 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics, pp. 605-612. 
Liu Ding and Daniel Gildea. 2005. Syntactic Fea-
tures for Evaluation of Machine Translation. In 
Proceedings of ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for MT and/or 
Summarization, pages 25?32. 
Liu Ding and Daniel Gildea. 2007. Source Lan-
guage Features and Maximum Correlation 
Training for Machine Translation Evaluation. In 
proceedings of NAACL HLT 2007, pages 41?48 
Mehay Dennis and Chris Brew. 2007. BLEUATRE: 
Flattening Syntactic Dependencies for MT Eval-
uation. In Proceedings of the 11th Conference on 
Theoretical and Methodological Issues in Ma-
chine Translation. 
Melamed Dan I., Ryan Green, and Joseph P. Turian. 
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on 
Human Language Technology and the North 
American Chapter of the Association for Com-
putational Linguistics. 
Nie?en Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. An Evaluation Tool for 
Machine  Translation: Fast Evaluation for MT 
Research. In Proceedings of the 2nd Internation-
al Conference on Language Resources and Eval-
uation . 
Owczarzak Karolina, Declan Groves, Josef Van 
Genabith, and Andy Way. 2006. Contextual Bi-
text- Derived Paraphrases in Automatic MT 
Evaluation. In Proceedings of the 7th Confe-
rence of the Association for Machine Translation 
in the Americas, pages 148?155. 
Owczarzak Karolina, Josef van Genabith, and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation. In Proceedings of the 
ACL Workshop on Statistical Machine Transla-
tion, pages 104?111. 
Papineni Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In 
Proceedings of 40th Annual Meeting of the As-
sociation for Computational Linguistics. 
Popovi? Maja and Hermann Ney. 2007. Word Er-
ror Rates: Decomposition over POS classes and 
Applications for Error Analysis. In Proceedings 
of the Second Workshop on Statistical Machine 
Translation, pages 48?55. 
Popovi? Maja and Hermann Ney. 2009. Syntax-
oriented evaluation measures for machine trans-
lation output. In Proceedings of the 4th EACL 
Workshop on Statistical Machine Translation, 
pages 29?32. 
Snover Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of AMTA, pag-
es 223?231. 
Tillmann Christoph, Stefan Vogel, Hermann Ney, 
A. Zubiaga, and H. Sawaf. 1997. Accelerated 
DP based Search for Statistical Translation. In 
Proceedings of European Conference on Speech 
Communication and Technology. 
1540
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133?143,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Topic Representation for SMT with Neural Networks
?
Lei Cui
1
, Dongdong Zhang
2
, Shujie Liu
2
, Qiming Chen
3
, Mu Li
2
, Ming Zhou
2
, and Muyun Yang
1
1
School of Computer Science and Technology, Harbin Institute of Technology, Harbin, P.R. China
leicui@hit.edu.cn, ymy@mtlab.hit.edu.cn
2
Microsoft Research, Beijing, P.R. China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
3
Shanghai Jiao Tong University, Shanghai, P.R. China
simoncqm@gmail.com
Abstract
Statistical Machine Translation (SMT)
usually utilizes contextual information
to disambiguate translation candidates.
However, it is often limited to contexts
within sentence boundaries, hence broader
topical information cannot be leveraged.
In this paper, we propose a novel approach
to learning topic representation for paral-
lel data using a neural network architec-
ture, where abundant topical contexts are
embedded via topic relevant monolingual
data. By associating each translation rule
with the topic representation, topic rele-
vant rules are selected according to the dis-
tributional similarity with the source text
during SMT decoding. Experimental re-
sults show that our method significantly
improves translation accuracy in the NIST
Chinese-to-English translation task com-
pared to a state-of-the-art baseline.
1 Introduction
Making translation decisions is a difficult task in
many Statistical Machine Translation (SMT) sys-
tems. Current translation modeling approaches
usually use context dependent information to dis-
ambiguate translation candidates. For exam-
ple, translation sense disambiguation approaches
(Carpuat and Wu, 2005; Carpuat and Wu,
2007) are proposed for phrase-based SMT sys-
tems. Meanwhile, for hierarchical phrase-based
or syntax-based SMT systems, there is also much
work involving rich contexts to guide rule selec-
tion (He et al, 2008; Liu et al, 2008; Marton
and Resnik, 2008; Xiong et al, 2009). Although
these methods are effective and proven successful
in many SMT systems, they only leverage within-
?
This work was done while the first and fourth authors
were visiting Microsoft Research.
sentence contexts which are insufficient in explor-
ing broader information. For example, the word
driver often means ?the operator of a motor ve-
hicle? in common texts. But in the sentence ?Fi-
nally, we write the user response to the buffer, i.e.,
pass it to our driver?, we understand that driver
means ?computer program?. In this case, people
understand the meaning because of the IT topical
context which goes beyond sentence-level analy-
sis and requires more relevant knowledge. There-
fore, it is important to leverage topic information
to learn smarter translation models and achieve
better translation performance.
Topic modeling is a useful mechanism for dis-
covering and characterizing various semantic con-
cepts embedded in a collection of documents. At-
tempts on topic-based translation modeling in-
clude topic-specific lexicon translation models
(Zhao and Xing, 2006; Zhao and Xing, 2007),
topic similarity models for synchronous rules
(Xiao et al, 2012), and document-level translation
with topic coherence (Xiong and Zhang, 2013). In
addition, topic-based approaches have been used
in domain adaptation for SMT (Tam et al, 2007;
Su et al, 2012), where they view different topics
as different domains. One typical property of these
approaches in common is that they only utilize
parallel data where document boundaries are ex-
plicitly given. In this way, the topic of a sentence
can be inferred with document-level information
using off-the-shelf topic modeling toolkits such
as Latent Dirichlet Allocation (LDA) (Blei et al,
2003) or Hidden Topic Markov Model (HTMM)
(Gruber et al, 2007). Most of them also assume
that the input must be in document level. However,
this situation does not always happen since there is
considerable amount of parallel data which does
not have document boundaries. In addition, con-
temporary SMT systems often works on sentence
level rather than document level due to the effi-
ciency. Although we can easily apply LDA at the
133
sentence level, it is quite difficult to infer the topic
accurately with only a few words in the sentence.
This makes previous approaches inefficient when
applied them in real-world commercial SMT sys-
tems. Therefore, we need to devise a systematical
approach to enriching the sentence and inferring
its topic more accurately.
In this paper, we propose a novel approach to
learning topic representations for sentences. Since
the information within the sentence is insufficient
for topic modeling, we first enrich sentence con-
texts via Information Retrieval (IR) methods using
content words in the sentence as queries, so that
topic-related monolingual documents can be col-
lected. These topic-related documents are utilized
to learn a specific topic representation for each
sentence using a neural network based approach.
Neural network is an effective technique for learn-
ing different levels of data representations. The
levels inferred from neural network correspond to
distinct levels of concepts, where high-level rep-
resentations are obtained from low-level bag-of-
words input. It is able to detect correlations among
any subset of input features through non-linear
transformations, which demonstrates the superior-
ity of eliminating the effect of noisy words which
are irrelevant to the topic. Our problem fits well
into the neural network framework and we expect
that it can further improve inferring the topic rep-
resentations for sentences.
To incorporate topic representations as trans-
lation knowledge into SMT, our neural network
based approach directly optimizes similarities be-
tween the source language and target language in a
compact topic space. This underlying topic space
is learned from sentence-level parallel data in or-
der to share topic information across the source
and target languages as much as possible. Addi-
tionally, our model can be discriminatively trained
with a large number of training instances, without
expensive sampling methods such as in LDA or
HTMM, thus it is more practicable and scalable.
Finally, we associate the learned representation to
each bilingual translation rule. Topic-related rules
are selected according to distributional similarity
with the source text, which helps hypotheses gen-
eration in SMT decoding. We integrate topic simi-
larity features in the log-linear model and evaluate
the performance on the NIST Chinese-to-English
translation task. Experimental results demonstrate
that our model significantly improves translation
accuracy over a state-of-the-art baseline.
2 Background: Deep Learning
Deep learning is an active topic in recent years
which has triumphed in many machine learning
research areas. This technique began raising pub-
lic awareness in the mid-2000s after researchers
showed how a multi-layer feed-forward neural
network can be effectively trained. The train-
ing procedure often involves two phases: a layer-
wise unsupervised pre-training phase and a su-
pervised fine-tuning phase. For pre-training, Re-
stricted Boltzmann Machine (RBM) (Hinton et
al., 2006), auto-encoding (Bengio et al, 2006)
and sparse coding (Lee et al, 2006) are most fre-
quently used. Unsupervised pre-training trains the
network one layer at a time and helps guide the pa-
rameters of the layer towards better regions in pa-
rameter space (Bengio, 2009). Followed by fine-
tuning in this parameter region, deep learning is
able to achieve state-of-the-art performance in var-
ious research areas, including breakthrough results
on the ImageNet dataset for objective recognition
(Krizhevsky et al, 2012), significant error reduc-
tion in speech recognition (Dahl et al, 2012), etc.
Deep learning has also been successfully ap-
plied in a variety of NLP tasks such as part-of-
speech tagging, chunking, named entity recog-
nition, semantic role labeling (Collobert et al,
2011), parsing (Socher et al, 2011a), sentiment
analysis (Socher et al, 2011b), etc. Most NLP
research converts a high-dimensional and sparse
binary representation into a low-dimensional and
real-valued representation. This low-dimensional
representation is usually learned from huge
amount of monolingual texts in the pre-training
phase, and then fine-tuned towards task-specific
criterion. Inspired by previous successful re-
search, we first learn sentence representations us-
ing topic-related monolingual texts in the pre-
training phase, and then optimize the bilingual
similarity by leveraging sentence-level parallel
data in the fine-tuning phase.
3 Topic Similarity Model with Neural
Network
In this section, we explain our neural network
based topic similarity model in detail, as well as
how to incorporate the topic similarity features
into SMT decoding procedure. Figure 1 sketches
the high-level overview which illustrates how to
134
?? = ?(?) ?? = ?(?) 
cos(?? , ??) 
???(?, ?) 
 ?  ? 
 English document collection 
 ??  ?? 
Parallel sentence  
IR IR 
? ? 
 Chinese document collection 
Neural Network Training 
Data Preprocessing 
Figure 1: Overview of neural network based topic
similarity model.
learn topic representations using sentence-level
parallel data. Given a parallel sentence pair ?f, e?,
the first step is to treat f and e as queries, and
use IR methods to retrieve relevant documents to
enrich contextual information for them. Specifi-
cally, the ranking model we used is a Vector Space
Model (VSM), where the query and document are
converted into tf-idf weighted vectors. The most
relevant N documents d
f
and d
e
are retrieved and
converted to a high-dimensional, bag-of-words in-
put f and e for the representation learning
1
.
There are two phases in our neural network
training process: pre-training and fine-tuning. In
the pre-training phase (Section 3.1), we build two
neural networks with the same structure but differ-
ent parameters to learn a low-dimensional repre-
sentation for sentences in two different languages.
Then, in the fine-tuning phase (Section 3.2), our
model directly optimizes the similarity of two low-
dimensional representations, so that it highly cor-
relates to SMT decoding. Finally, the learned rep-
resentation is used to calculate similarities which
are integrated as features in SMT decoding proce-
dure (Section 3.3).
3.1 Pre-training using denoising
auto-encoder
In the pre-training phase, we leverage neural
network structures to transform high-dimensional
sparse vectors to low-dimensional dense vectors.
The topic similarity is calculated on top of the
learned dense vectors. This dense representation
should preserve the information from the bag-of-
1
We use f and e to denote the n-of-V vector converted
from the retrieved documents.
words input, meanwhile alleviate data sparse prob-
lem. Therefore, we use a specially designed mech-
anism called auto-encoder to solve this problem.
Auto-encoder (Bengio et al, 2006) is one of the
basic building blocks of deep learning. Assum-
ing that the input is a n-of-V binary vector x rep-
resenting the bag-of-words (V is the vocabulary
size), an auto-encoder consists of an encoding pro-
cess g(x) and a decoding process h(g(x)). The
objective of the auto-encoder is to minimize the
reconstruction error L(h(g(x)), x). Our goal is to
learn a low-dimensional vector which can preserve
information from the original n-of-V vector.
One problem with auto-encoder is that it treats
all words in the same way, making no distinguish-
ment between function words and content words.
The representation learned by auto-encoders tends
to be influenced by the function words, thereby it
is not robust. To alleviate this problem, Vincent et
al. (2008) proposed the Denoising Auto-Encoder
(DAE), which aims to reconstruct a clean, ?re-
paired? input from a corrupted, partially destroyed
vector. This is done by corrupting the initial in-
put x to get a partially destroyed version
?
x. DAE
is capable of capturing the global structure of the
input while ignoring the noise. In our task, for
each sentence, we treat the retrieved N relevant
documents as a single large document and convert
it to a bag-of-words vector x in Figure 2. With
DAE, the input x is manually corrupted by apply-
ing masking noise (randomly mask 1 to 0) and get-
ting
?
x. Denoising training is considered as ?filling
in the blanks? (Vincent et al, 2010), which means
the masking components can be recovered from
the non-corrupted components. For example, in
IT related texts, if the word driver is masked, it
should be predicted through hidden units in neural
networks by active signals such as ?buffer?, ?user
response?, etc.
In our case, the encoding process transforms
the corrupted input
?
x into g(
?
x) with two layers:
a linear layer connected with a non-linear layer.
Assuming that the dimension of the g(
?
x) is L,
the linear layer forms a L ? V matrix W which
projects the n-of-V vector to a L-dimensional hid-
den layer. After the bag-of-words input has been
transformed, they are fed into a subsequent layer
to model the highly non-linear relations among
words:
z = f(W
?
x + b) (1)
where z is the output of the non-linear layer, b is a
135
? ?? 
?(??) 
(?? ??) 
?(?? ?? ,?) 
Figure 2: Denoising auto-encoder with a bag-of-
words input.
L-length bias vector. f(?) is a non-linear function,
where common choices include sigmoid function,
hyperbolic function, ?hard? hyperbolic function,
rectifier function, etc. In this work, we use the
rectifier function as our non-linear function due to
its efficiency and better performance (Glorot et al,
2011):
rec(x) =
{
x if x > 0
0 otherwise
(2)
The decoding process consists of a linear layer
and a non-linear layer with similar network struc-
tures, but different parameters. It transforms the
L-dimensional vector g(
?
x) to a V -dimensional
vector h(g(
?
x)). To minimize reconstruction error
with respect to
?
x, we define the loss function as
the L2-norm of the difference between the uncor-
rupted input and reconstructed input:
L(h(g(
?
x)), x) = ?h(g(
?
x))? x?
2
(3)
Multi-layer neural networks are trained with the
standard back-propagation algorithm (Rumelhart
et al, 1988). The gradient of the loss function
is calculated and back-propagated to the previous
layer to update its parameters. Training neural net-
works involves many factors such as the learning
rate and the length of hidden layers. We will dis-
cuss the optimization of these parameters in Sec-
tion 4.
3.2 Fine-tuning with parallel data
In the fine-tuning phase, we stack another layer on
top of the two low-dimensional vectors to maxi-
mize the similarity between source and target lan-
guages. The similarity scores are integrated into
the standard log-linear model for making transla-
tion decisions. Since the vectors from DAE are
trained using information from monolingual train-
ing data independently, these vectors may be in-
adequate to measure bilingual topic similarity due
to their different topic spaces. Therefore, in this
stage, parallel sentence pairs are used to help con-
necting the vectors from different languages be-
cause they express the same topic. In fact, the ob-
jective of fine-tuning is to discover a latent topic
space which is shared by both languages as much
as possible. This shared topic space is particularly
useful when the SMT decoder tries to match the
source texts and translation candidates in the tar-
get language.
Given a parallel sentence pair ?f, e?, the DAE
learns representations for f and e respectively, as
z
f
= g(f) and z
e
= g(e) in Figure 1. We then take
two vectors as the input to calculate their similar-
ity. Consequently, the whole neural network can
be fine-tuned towards the supervised criteria with
the help of parallel data. The similarity score of
the representation pair ?z
f
, z
e
? is defined as the co-
sine similarity of the two vectors:
sim(f, e) = cos(z
f
, z
e
)
=
z
f
? z
e
?z
f
??z
e
?
(4)
Since a parallel sentence pair should have the
same topic, our goal is to maximize the similar-
ity score between the source sentence and target
sentence. Inspired by the contrastive estimation
method (Smith and Eisner, 2005), for each paral-
lel sentence pair ?f, e? as a positive instance, we
select another sentence pair ?f
?
, e
?
? from the train-
ing data and treat ?f, e
?
? as a negative instance. To
make the similarity of the positive instance larger
than the negative instance by some margin ?, we
utilize the following pairwise ranking loss:
L(f, e) = max{0, ? ? sim(f, e) + sim(f, e
?
)}
(5)
where ? =
1
2
? sim(f, f
?
). The rationale behind
this criterion is, the smaller sim(f, f
?
) is, the more
we should penalize negative instances.
To effectively train the model in this task, neg-
ative instances must be selected carefully. Since
different sentences may have very similar topic
distributions, we select negative instances that are
dissimilar with the positive instances based on the
following criteria:
1. For each positive instance ?f, e?, we select e
?
which contains at least 30% different content
words from e.
136
2. If we cannot find such e
?
, remove ?f, e? from
the training instances for network learning.
The model minimizes the pairwise ranking loss
across all training instances:
L =
?
?f,e?
L(f, e) (6)
We used standard back-propagation algorithm
to further fine-tune the neural network parameters
W and b in Equation (1). The learned neural net-
works are used to obtain sentence topic representa-
tions, which will be further leveraged to infer topic
representations of bilingual translation rules.
3.3 Integration into SMT decoding
We incorporate the learned topic similarity scores
into the standard log-linear framework for SMT.
When a synchronous rule ??, ?? is extracted from
a sentence pair ?f, e?, a triple instance I =
(??, ??, ?f, e?, c) is collected for inferring the
topic representation of ??, ??, where c is the count
of rule occurrence. Following (Chiang, 2007), we
give a count of one for each phrase pair occurrence
and a fractional count for each hierarchical phrase
pair. The topic representation of ??, ?? is then cal-
culated as the weighted average:
z
?
=
?
(??,??,?f,e?,c)?T
{c? z
f
}
?
(??,??,?f,e?,c)?T
{c}
(7)
z
?
=
?
(??,??,?f,e?,c)?T
{c? z
e
}
?
(??,??,?f,e?,c)?T
{c}
(8)
where T denotes all instances for the rule ??, ??,
z
?
and z
?
are the source-side and target-side topic
vectors respectively.
By measuring the similarity between the source
texts and bilingual translation rules, the SMT de-
coder is able to encourage topic relevant transla-
tion candidates and penalize topic irrelevant candi-
dates. Therefore, it helps to train a smarter transla-
tion model with the embedded topic information.
Given a source sentence s to be translated, we de-
fine the similarity as follows:
Sim(z
s
, z
?
) = cos(z
s
, z
?
) (9)
Sim(z
s
, z
?
) = cos(z
s
, z
?
) (10)
where z
s
is the topic representation of s. The
similarity calculated against z
?
or z
?
denotes the
source-to-source or the source-to-target similarity.
We also consider the topic sensitivity estimation
since general rules have flatter distributions while
topic-specific rules have sharper distributions. A
standard entropy metric is used to measure the sen-
sitivity of the source-side of ??, ?? as:
Sen(?) = ?
|z
?
|
?
i=1
z
?i
? log z
?i
(11)
where z
?i
is a component in the vector z
?
. The
target-side sensitivity Sen(?) can be calculated in
a similar way. The larger the sensitivity is, the
more topic-specific the rule manifests.
In addition to traditional SMT features, we add
new topic-related features into the standard log-
linear framework. For the SMT system, the best
translation candidate e? is given by:
e? = argmax
e
P (e|f) (12)
where the translation probability is given by:
P (e|f) ?
?
i
w
i
? log ?
i
(f, e)
=
?
j
w
j
? log ?
j
(f, e)
? ?? ?
Standard
+
?
k
w
k
? log ?
k
(f, e)
? ?? ?
Topic related
(13)
where ?
j
(f, e) is the standard feature function and
w
j
is the corresponding feature weight. ?
k
(f, e)
is the topic-related feature function and w
k
is the
feature weight. The detailed feature description is
as follows:
Standard features: Translation model, includ-
ing translation probabilities and lexical weights
for both directions (4 features), 5-gram language
model (1 feature), word count (1 feature), phrase
count (1 feature), NULL penalty (1 feature), num-
ber of hierarchical rules used (1 feature).
Topic-related features: rule similarity scores
(2 features), rule sensitivity scores (2 features).
4 Experiments
4.1 Setup
We evaluate the performance of our neural net-
work based topic similarity model on a Chinese-
to-English machine translation task. In neural net-
work training, a large number of monolingual doc-
uments are collected in both source and target lan-
guages. The documents are mainly from two do-
mains: news and weblog. We use Chinese and
137
English Gigaword corpus (Version 5) which are
mainly from news domain. In addition, we also
collect weblog documents with a variety of top-
ics from the web. The total data statistics are
presented in Table 1. These documents are built
in the format of inverted index using Lucene
2
,
which can be efficiently retrieved by the paral-
lel sentence pairs. The most relevant N docu-
ments are collected, where we experiment with
N = {1, 5, 10, 20, 50}.
Domain
Chinese English
Docs Words Docs Words
News 5.7M 5.4B 9.9M 25.6B
Weblog 2.1M 8B 1.2M 2.9B
Total 7.8M 13.4B 11.1M 28.5B
Table 1: Statistics of monolingual data, in num-
bers of documents and words (main content). ?M?
refers to million and ?B? refers to billion.
We implement a distributed framework to speed
up the training process of neural networks. The
network is learned with mini-batch asynchronous
gradient descent with the adaptive learning rate
procedure called AdaGrad (Duchi et al, 2011).
We use 32 model replicas in each iteration during
the training. The model parameters are averaged
after each iteration and sent to each replica for the
next iteration. The vocabulary size for the input
layer is 100,000, and we choose different lengths
for the hidden layer as L = {100, 300, 600, 1000}
in the experiments. In the pre-training phase, all
parallel data is fed into two neural networks re-
spectively for DAE training, where network pa-
rameters W and b are randomly initialized. In
the fine-tuning phase, for each parallel sentence
pair, we randomly select other ten sentence pairs
which satisfy the criterion as negative instances.
These training instances are leveraged to optimize
the similarity of two vectors.
In SMT training, an in-house hierarchical
phrase-based SMT decoder is implemented for our
experiments. The CKY decoding algorithm is
used and cube pruning is performed with the same
default parameter settings as in Chiang (2007).
The parallel data we use is released by LDC
3
. In
total, the datasets contain nearly 1.1 million sen-
tence pairs. Translation models are trained over
the parallel data that is automatically word-aligned
2
http://lucene.apache.org/
3
LDC2003E14, LDC2002E18, LDC2003E07,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34,
LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09
using GIZA++ in both directions, and the diag-
grow-final heuristic is used to refine symmetric
word alignment. An in-house language modeling
toolkit is used to train the 5-gram language model
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). The English monolingual data used
for language modeling is the same as in Table
1. The NIST 2003 dataset is the development
data. The testing data consists of NIST 2004,
2005, 2006 and 2008 datasets. The evaluation
metric for the overall translation quality is case-
insensitive BLEU4 (Papineni et al, 2002). The
reported BLEU scores are averaged over 5 times
of running MERT (Och, 2003). A statistical sig-
nificance test is performed using the bootstrap re-
sampling method (Koehn, 2004).
4.2 Baseline
The baseline is a re-implementation of the Hiero
system (Chiang, 2007). The phrase pairs that ap-
pear only once in the parallel data are discarded
because most of them are noisy. We also use
the fix-discount method in Foster et al (2006)
for phrase table smoothing. This implementation
makes the system perform much better and the
translation model size is much smaller.
We compare our method with the LDA-based
approach proposed by Xiao et al (2012). In (Xiao
et al, 2012), the topic of each sentence pair is ex-
actly the same as the document it belongs to. Since
some of our parallel data does not have document-
level information, we rely on the IR method to
retrieve the most relevant document and simulate
this approach. The PLDA toolkit (Liu et al, 2011)
is used to infer topic distributions, which takes
34.5 hours to finish.
4.3 Effect of retrieved documents and length
of hidden layers
We illustrate the relationship among translation
accuracy (BLEU), the number of retrieved docu-
ments (N ) and the length of hidden layers (L) on
different testing datasets. The results are shown in
Figure 3. The best translation accuracy is achieved
when N=10 for most settings. This confirms that
enriching the source text with topic-related doc-
uments is very useful in determining topic repre-
sentations, thereby help to guide the synchronous
rule selection. However, we find that as N be-
comes larger in the experiments, e.g. N=50, the
translation accuracy drops drastically. As more
documents are retrieved, less relevant information
138
0 5 10 20 5042
42.2
42.4
42.6
42.8
43
Number of Retrieved Documents (N)
BLE
U
NIST 2004
 
 L=100L=300L=600L=1000
0 5 10 20 5041
41.2
41.4
41.6
41.8
42
Number of Retrieved Documents (N)
BLE
U
NIST 2005
 
 L=100L=300L=600L=1000
0 5 10 20 5037.8
38
38.2
38.4
38.6
38.8
39
39.2
Number of Retrieved Documents (N)
BLE
U
NIST 2006
 
 L=100L=300L=600L=1000
0 5 10 20 5031
31.2
31.4
31.6
31.8
32
Number of Retrieved Documents (N)
BLE
U
NIST 2008
 
 L=100L=300L=600L=1000
Figure 3: End-to-end translation results (BLEU%) using all standard and topic-related features, with
different settings on the number of retrieved documents N and the length of hidden layers L.
is also used to train the neural networks. Irrel-
evant documents bring so many unrelated topic
words hence degrade neural network learning per-
formance.
Another important factor is the length of hid-
den layers L in the network. In deep learning, this
parameter is often empirically tuned with human
efforts. As shown in Figure 3, the translation accu-
racy is better when L is relatively small. Actually,
there is no obvious distinction of the performance
when L is less than 600. However, when L equals
1,000, the translation accuracy is inferior to other
settings. The main reason is that parameters in
the neural networks are too many to be effectively
trained. As we know when L=1000, there are a
total of 100, 000? 1, 000 parameters between the
linear and non-linear layers in the network. Lim-
ited training data prevents the model from getting
close to the global optimum. Therefore, the model
is likely to fall in local optima and lead to unac-
ceptable representations.
4.4 Effect of topic related features
We evaluate the performance of adding new topic-
related features to the log-linear model and com-
pare the translation accuracy with the method in
(Xiao et al, 2012). To make different methods
comparable, we set the dimension of topic rep-
resentation as 100 for all settings. This takes 10
hours in pre-training phase and 22 hours in fine-
tuning phase. Table 2 shows how the accuracy is
improved with more features added. The results
confirm that topic information is indispensable for
SMT since both (Xiao et al, 2012) and our neural
network based method significantly outperforms
the baseline system. Our method improves 0.86
BLEU points at most and 0.76 BLEU points on
average over the baseline. We observe that source-
side similarity is more effective than target-side
similarity, but their contributions are cumulative.
This proves that bilingually induced topic repre-
sentation with neural network helps the SMT sys-
tem disambiguate translation candidates. Further-
more, rule sensitivity features improve SMT per-
formance compared with only using similarity fea-
tures. Because topic-specific rules usually have a
larger sensitivity score, they can beat general rules
when they obtain the same similarity score against
the input sentence. Finally, when all new fea-
tures are integrated, the performance is the best,
preforming substantially better than (Xiao et al,
2012) with 0.39 BLEU points on average.
It is worth mentioning that the performance
of (Xiao et al, 2012) is similar to the settings
with N=1 and L=100 in Figure 3. This is not
simply coincidence since we can interpret their
approach as a special case in our neural net-
work method: when a parallel sentence pair has
139
Settings NIST 2004 NIST 2005 NIST 2006 NIST 2008 Average
Baseline 42.25 41.21 38.05 31.16 38.17
(Xiao et al, 2012) 42.58 41.61 38.39 31.58 38.54
Sim(Src) 42.51 41.55 38.53 31.57 38.54
Sim(Trg) 42.43 41.48 38.4 31.49 38.45
Sim(Src+Trg) 42.7 41.66 38.66 31.66 38.67
Sim(Src+Trg)+Sen(Src) 42.77 41.81 38.85 31.73 38.79
Sim(Src+Trg)+Sen(Trg) 42.85 41.79 38.76 31.7 38.78
Sim(Src+Trg)+Sen(Src+Trg) 42.95 41.97 38.91 31.88 38.93
Table 2: Effectiveness of different features in BLEU% (p < 0.05), with N=10 and L=100. ?Sim?
denotes the rule similarity feature and ?Sen? denotes rule sensitivity feature. ?Src? and ?Trg? means
utilizing source-side/target-side rule topic vectors to calculate similarity or sensitivity, respectively. The
?Average? setting is the averaged result of four datasets.
document-level information, that document will
be retrieved for training; otherwise, the most rel-
evant document will be retrieved from the mono-
lingual data. Therefore, our method can be viewed
as a more general framework than previous LDA-
based approaches.
4.5 Discussion
In this section, we give a case study to explain
why our method works. An example of transla-
tion rule disambiguation for a sentence from the
NIST 2005 dataset is shown in Figure 4. We find
that the topic of this sentence is about ?rescue af-
ter a natural disaster?. Under this topic, the Chi-
nese rule ??? X? should be translated to ?de-
liver X? or ?distribute X?. However, the baseline
system prefers ?send X? rather than those two can-
didates. Although the translation probability of
?send X? is much higher, it is inappropriate in this
context since it is usually used in IT texts. For
example, ?????, send emails?, ?????,
send messages? and ?????, send data?. In
contrast, with our neural network based approach,
the learned topic distributions of ?deliver X? or
?distribute X? are more similar with the input sen-
tence than ?send X?, which is shown in Figure 4.
The similarity scores indicate that ?deliver X? and
?distribute X? are more appropriate to translate the
sentence. Therefore, adding topic-related features
is able to keep the topic consistency and substan-
tially improve the translation accuracy.
5 Related Work
Topic modeling was first leveraged to improve
SMT performance in (Zhao and Xing, 2006; Zhao
and Xing, 2007). They proposed a bilingual
topical admixture approach for word alignment
and assumed that each word-pair follows a topic-
specific model. They reported extensive empir-
ical analysis and improved word alignment ac-
curacy as well as translation quality. Follow-
ing this work, (Xiao et al, 2012) extended topic-
specific lexicon translation models to hierarchical
phrase-based translation models, where the topic
information of synchronous rules was directly in-
ferred with the help of document-level informa-
tion. Experiments show that their approach not
only achieved better translation performance but
also provided a faster decoding speed compared
with previous lexicon-based LDA methods.
Another direction of approaches leveraged topic
modeling techniques for domain adaptation. Tam
et al (2007) used bilingual LSA to learn latent
topic distributions across different languages and
enforce one-to-one topic correspondence during
model training. They incorporated the bilingual
topic information into language model adaptation
and lexicon translation model adaptation, achiev-
ing significant improvements in the large-scale
evaluation. (Su et al, 2012) investigated the rela-
tionship between out-of-domain bilingual data and
in-domain monolingual data via topic mapping
using HTMM methods. They estimated phrase-
topic distributions in translation model adaptation
and generated better translation quality. Recently,
Chen et al (2013) proposed using vector space
model for adaptation where genre resemblance is
leveraged to improve translation accuracy. We
also investigated multi-domain adaptation where
explicit topic information is used to train domain
specific models (Cui et al, 2013).
Generally, most previous research has leveraged
conventional topic modeling techniques such as
LDA or HTMM. In our work, a novel neural net-
work based approach is proposed to infer topic
representations for parallel data. The advantage of
140
S
rc
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
R
ef
(1
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
al
so
be
gu
n
de
li
ve
ri
ng
ba
si
c
m
ed
ic
al
ki
ts
(2
)
th
e
un
ic
ef
ha
s
al
so
st
ar
te
d
to
di
st
ri
bu
te
ba
si
c
m
ed
ic
al
ki
ts
(3
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
al
so
be
gu
n
di
st
ri
bu
ti
ng
ba
si
c
m
ed
ic
al
ki
ts
(4
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
be
gu
n
de
li
ve
ri
ng
ba
si
c
m
ed
ic
al
ki
ts
B
as
el
in
e
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
be
ga
n
to
se
nd
ba
si
c
m
ed
ic
al
ki
ts
O
ur
s
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
be
gu
n
to
di
st
ri
bu
te
ba
si
c
m
ed
ic
al
ki
ts
T
ab
le
4:
A
ck
n
ow
le
d
gm
en
ts
T
he
ac
kn
ow
le
dg
m
en
ts
sh
ou
ld
go
im
m
ed
ia
te
ly
be
-
fo
re
th
e
re
fe
re
nc
es
.
D
o
no
t
nu
m
be
r
th
e
ac
kn
ow
l-
ed
gm
en
ts
se
ct
io
n.
D
o
no
t
in
cl
ud
e
th
is
se
ct
io
n
w
he
n
su
bm
it
ti
ng
yo
ur
pa
pe
r
fo
r
re
vi
ew
.
R
ef
er
en
ce
s
Y
os
hu
a
B
en
gi
o,
P
as
ca
l
L
am
bl
in
,
D
an
P
op
ov
ic
i,
an
d
H
ug
o
L
ar
oc
he
ll
e.
20
06
.
G
re
ed
y
la
ye
r-
w
is
e
tr
ai
n-
in
g
of
de
ep
ne
tw
or
ks
.
In
B
.
S
ch
o?l
ko
pf
,
J.
P
la
tt
,
an
d
T
.
H
of
fm
an
,
ed
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
a-
ti
on
P
ro
ce
ss
in
g
Sy
st
em
s
19
,
pa
ge
s
15
3?
16
0.
M
IT
P
re
ss
,C
am
br
id
ge
,M
A
.
Y
os
hu
a
B
en
gi
o.
20
09
.
L
ea
rn
in
g
de
ep
ar
ch
it
ec
tu
re
s
fo
r
ai
.
Fo
un
d.
Tr
en
ds
M
ac
h.
L
ea
rn
.,
2(
1)
:1
?1
27
,
Ja
n-
ua
ry
.
D
av
id
M
.
B
le
i,
A
nd
re
w
Y
.
N
g,
an
d
M
ic
ha
el
I.
Jo
rd
an
.
20
03
.
L
at
en
t
di
ri
ch
le
t
al
lo
ca
ti
on
.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
3:
99
3?
10
22
,M
ar
ch
.
M
ar
in
e
C
ar
pu
at
an
d
D
ek
ai
W
u.
20
07
.
C
on
te
xt
-
de
pe
nd
en
t
ph
ra
sa
l
tr
an
sl
at
io
n
le
xi
co
ns
fo
r
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
P
ro
ce
ed
in
gs
of
M
ac
hi
ne
Tr
an
s-
la
ti
on
Su
m
m
it
X
I,
pa
ge
s
73
?8
0.
D
av
id
C
hi
an
g.
20
07
.
H
ie
ra
rc
hi
ca
l
ph
ra
se
-b
as
ed
tr
an
s-
la
ti
on
.
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
,3
3(
2)
:2
01
?2
28
.
R
on
an
C
ol
lo
be
rt
,J
as
on
W
es
to
n,
L
e?o
n
B
ot
to
u,
M
ic
ha
el
K
ar
le
n,
K
or
ay
K
av
uk
cu
og
lu
,
an
d
P
av
el
K
uk
sa
.
20
11
.
N
at
ur
al
la
ng
ua
ge
pr
oc
es
si
ng
(a
lm
os
t)
fr
om
sc
ra
tc
h.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
12
:2
49
3?
25
37
,
N
ov
em
be
r.
G
.
E
.
D
ah
l,
D
on
g
Y
u,
L
i
D
en
g,
an
d
A
.
A
ce
ro
.
20
12
.
C
on
te
xt
-d
ep
en
de
nt
pr
e-
tr
ai
ne
d
de
ep
ne
ur
al
ne
tw
or
ks
fo
r
la
rg
e-
vo
ca
bu
la
ry
sp
ee
ch
re
co
gn
it
io
n.
Tr
an
s.
A
u-
di
o,
Sp
ee
ch
an
d
L
an
g.
P
ro
c.
,2
0(
1)
:3
0?
42
,J
an
ua
ry
.
Jo
hn
D
uc
hi
,
E
la
d
H
az
an
,
an
d
Y
or
am
S
in
ge
r.
20
11
.
A
da
pt
iv
e
su
bg
ra
di
en
t
m
et
ho
ds
fo
r
on
li
ne
le
ar
ni
ng
an
d
st
oc
ha
st
ic
op
ti
m
iz
at
io
n.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
12
:2
12
1?
21
59
,J
ul
y.
G
eo
rg
e
F
os
te
r,
R
ol
an
d
K
uh
n,
an
d
H
ow
ar
d
Jo
hn
so
n.
20
06
.
P
hr
as
et
ab
le
sm
oo
th
in
g
fo
r
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
20
06
C
on
fe
re
nc
e
on
E
m
pi
ri
ca
l
M
et
ho
ds
in
N
at
ur
al
L
an
gu
ag
e
P
ro
-
ce
ss
in
g,
pa
ge
s
53
?6
1,
S
yd
ne
y,
A
us
tr
al
ia
,
Ju
ly
.
A
s-
so
ci
at
io
n
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
A
m
it
G
ru
be
r,
M
ic
ha
lR
os
en
-z
vi
,a
nd
Y
ai
r
W
ei
ss
.
20
07
.
H
id
de
n
to
pi
c
m
ar
ko
v
m
od
el
s.
In
In
P
ro
ce
ed
in
gs
of
A
rt
ifi
ci
al
In
te
ll
ig
en
ce
an
d
St
at
is
ti
cs
.
Z
ho
ng
ju
n
H
e,
Q
un
L
iu
,
an
d
S
ho
ux
un
L
in
.
20
08
.
Im
-
pr
ov
in
g
st
at
is
ti
ca
lm
ac
hi
ne
tr
an
sl
at
io
n
us
in
g
le
xi
ca
l-
iz
ed
ru
le
se
le
ct
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
22
nd
In
-
te
rn
at
io
na
l
C
on
fe
re
nc
e
on
C
om
pu
ta
ti
on
al
L
in
gu
is
-
ti
cs
(C
ol
in
g
20
08
),
pa
ge
s
32
1?
32
8,
M
an
ch
es
te
r,
U
K
,
A
ug
us
t.
C
ol
in
g
20
08
O
rg
an
iz
in
g
C
om
m
it
te
e.
G
eo
ff
re
y
E
.
H
in
to
n,
S
im
on
O
si
nd
er
o,
an
d
Y
ee
-W
hy
e
T
eh
.
20
06
.
A
fa
st
le
ar
ni
ng
al
go
ri
th
m
fo
r
de
ep
be
li
ef
ne
ts
.
N
eu
ra
l
C
om
pu
t.
,1
8(
7)
:1
52
7?
15
54
,J
ul
y.
R
ei
nh
ar
d
K
ne
se
r
an
d
H
er
m
an
n
N
ey
.
19
95
.
Im
-
pr
ov
ed
ba
ck
in
g-
of
f
fo
r
m
-g
ra
m
la
ng
ua
ge
m
od
el
in
g.
In
A
co
us
ti
cs
,
Sp
ee
ch
,
an
d
Si
gn
al
P
ro
ce
ss
in
g,
19
95
.
IC
A
SS
P
-9
5.
,1
99
5
In
te
rn
at
io
na
lC
on
fe
re
nc
e
on
,v
ol
-
um
e
1,
pa
ge
s
18
1?
18
4.
IE
E
E
.
P
hi
li
pp
K
oe
hn
.
20
04
.
S
ta
ti
st
ic
al
si
gn
ifi
ca
nc
e
te
st
s
fo
r
m
ac
hi
ne
tr
an
sl
at
io
n
ev
al
ua
ti
on
.
In
D
ek
an
g
L
in
an
d
D
ek
ai
W
u,
ed
it
or
s,
P
ro
ce
ed
in
gs
of
E
M
N
L
P
20
04
,
pa
ge
s
38
8?
39
5,
B
ar
ce
lo
na
,
S
pa
in
,
Ju
ly
.
A
ss
oc
ia
ti
on
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
A
le
x
K
ri
zh
ev
sk
y,
Il
ya
S
ut
sk
ev
er
,
an
d
G
eo
ff
H
in
to
n.
20
12
.
Im
ag
en
et
cl
as
si
fi
ca
ti
on
w
it
h
de
ep
co
nv
ol
u-
ti
on
al
ne
ur
al
ne
tw
or
ks
.
In
P.
B
ar
tl
et
t,
F.
C
.N
.P
er
ei
ra
,
C
.J
.C
.
B
ur
ge
s,
L
.
B
ot
to
u,
an
d
K
.Q
.
W
ei
nb
er
ge
r,
ed
-
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
at
io
n
P
ro
ce
ss
in
g
Sy
st
em
s
25
,p
ag
es
11
06
?1
11
4.
H
on
gl
ak
L
ee
,
A
le
xi
s
B
at
tl
e,
R
aj
at
R
ai
na
,
an
d
A
n-
dr
ew
Y
.
N
g.
20
06
.
E
ffi
ci
en
t
sp
ar
se
co
di
ng
al
go
-
ri
th
m
s.
In
B
.
S
ch
o?l
ko
pf
,
J.
P
la
tt
,
an
d
T
.
H
of
fm
an
,
ed
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
at
io
n
P
ro
ce
ss
in
g
Sy
st
em
s
19
,p
ag
es
80
1?
80
8.
M
IT
P
re
ss
,C
am
br
id
ge
,
M
A
.
Q
un
L
iu
,
Z
ho
ng
ju
n
H
e,
Y
an
g
L
iu
,
an
d
S
ho
ux
un
L
in
.
20
08
.
M
ax
im
um
en
tr
op
y
ba
se
d
ru
le
se
le
ct
io
n
m
od
el
fo
r
sy
nt
ax
-b
as
ed
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
20
08
C
on
fe
re
nc
e
on
E
m
pi
ri
ca
l
M
et
ho
ds
in
N
at
ur
al
L
an
gu
ag
e
P
ro
ce
ss
in
g,
pa
ge
s
89
?9
7,
H
on
ol
ul
u,
H
aw
ai
i,
O
ct
ob
er
.
A
ss
oc
ia
ti
on
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
Z
hi
yu
an
L
iu
,
Y
uz
ho
u
Z
ha
ng
,
E
dw
ar
d
Y
.
C
ha
ng
,
an
d
M
ao
so
ng
S
un
.
20
11
.
P
ld
a+
:
P
ar
al
le
l
la
te
nt
di
ri
ch
le
t
al
lo
ca
ti
on
w
it
h
da
ta
pl
ac
em
en
ta
nd
pi
pe
li
ne
pr
oc
es
s-
in
g.
A
C
M
Tr
an
sa
ct
io
ns
on
In
te
ll
ig
en
t
Sy
st
em
s
an
d
Te
ch
no
lo
gy
,
sp
ec
ia
l
is
su
e
on
L
ar
ge
Sc
al
e
M
ac
hi
ne
L
ea
rn
in
g.
S
of
tw
ar
e
av
ai
la
bl
e
at
h
t
t
p
:
/
/
c
o
d
e
.
g
o
o
g
l
e
.
c
o
m
/
p
/
p
l
d
a
.
0
20
40
60
80
100
00.020.040.060.080.1???
???
???
???
???
???
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , de
liver 
X>
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , di
stribu
te X>
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , se
nd X>
S
et
ti
n
gs
N
IS
T
20
04
N
IS
T
20
05
N
IS
T
20
06
N
IS
T
20
08
A
ve
ra
ge
B
as
el
in
e
42
.2
5
41
.2
1
38
.0
5
31
.1
6
38
.1
7
(X
ia
o
et
al
.,
20
12
)
42
.5
8
41
.6
1
38
.3
9
31
.5
8
38
.5
4
S
im
(S
rc
)
42
.5
1
41
.5
5
38
.5
3
31
.5
7
38
.5
4
S
im
(T
rg
)
42
.4
3
41
.4
8
38
.4
31
.4
9
38
.4
5
S
im
(S
rc
+
T
rg
)
42
.7
41
.6
6
38
.6
6
31
.6
6
38
.6
7
S
im
(S
rc
+
T
rg
)+
S
en
(S
rc
)
42
.7
7
41
.8
1
38
.8
5
31
.7
3
38
.7
9
S
im
(S
rc
+
T
rg
)+
S
en
(T
rg
)
42
.8
5
41
.7
9
38
.7
6
31
.7
38
.7
8
S
im
(S
rc
+
T
rg
)+
S
en
(S
rc
+
T
rg
)
42
.9
5
41
.9
7
38
.9
1
31
.8
8
38
.9
3
T
ab
le
2:
E
ff
ec
ti
ve
ne
ss
of
di
ff
er
en
t
fe
at
ur
es
in
B
L
E
U
%
(p
<
0.
05
),
w
it
h
N
=
10
an
d
L
=
10
0.
?S
im
?
de
no
te
s
th
e
ru
le
si
m
il
ar
it
y
fe
at
ur
e
an
d
?S
en
?
de
no
te
s
ru
le
se
ns
it
iv
it
y
fe
at
ur
e.
?S
rc
?
an
d
?T
rg
?
m
ea
ns
ut
il
iz
in
g
so
ur
ce
-s
id
e/
ta
rg
et
-s
id
e
ru
le
to
pi
c
ve
ct
or
s
to
ca
lc
ul
at
e
si
m
il
ar
it
y
or
se
ns
it
iv
it
y,
re
sp
ec
ti
ve
ly
.
T
he
?A
ve
ra
ge
?
se
tt
in
g
is
th
e
av
er
ag
ed
re
su
lt
s
of
fo
ur
da
ta
se
ts
.
pa
re
d
w
it
h
on
ly
us
in
g
si
m
il
ar
it
y
fe
at
ur
es
.
B
ec
au
se
to
pi
c-
sp
ec
ifi
c
ru
le
s
us
ua
ll
y
ha
ve
a
la
rg
er
se
ns
it
iv
-
it
y
sc
or
e,
th
ey
ca
n
be
at
ge
ne
ra
l
ru
le
s
w
he
n
th
ey
ob
ta
in
th
e
sa
m
e
si
m
il
ar
it
y
sc
or
e
ag
ai
ns
t
th
e
in
pu
t
se
nt
en
ce
.
F
in
al
ly
,
w
he
n
al
l
ne
w
fe
at
ur
es
ar
e
in
-
te
gr
at
ed
,
th
e
pe
rf
or
m
an
ce
is
th
e
be
st
,
pr
ef
or
m
in
g
su
bs
ta
nt
ia
ll
y
be
tt
er
th
an
(X
ia
o
et
al
.,
20
12
)
w
it
h
0.
39
B
L
E
U
po
in
ts
on
av
er
ag
e.
O
ne
in
te
re
st
in
g
ob
se
rv
at
io
n
is
,t
he
pe
rf
or
m
an
ce
of
(X
ia
o
et
al
.,
20
12
)
is
qu
it
e
si
m
il
ar
to
th
e
se
t-
ti
ng
s
w
it
h
N
=
1
an
d
L
=
10
0
in
F
ig
ur
e
3.
T
hi
s
is
no
t
si
m
pl
y
co
in
ci
de
nc
e
si
nc
e
w
e
ca
n
in
te
rp
re
t
th
ei
r
ap
pr
oa
ch
as
a
sp
ec
ia
l
ca
se
in
ou
r
ne
ur
al
ne
t-
w
or
k
m
et
ho
d.
W
he
n
a
pa
ra
ll
el
se
nt
en
ce
pa
ir
ha
s
do
cu
m
en
t-
le
ve
l
in
fo
rm
at
io
n,
th
at
do
cu
m
en
t
w
il
l
be
re
tr
ie
ve
d
fo
r
tr
ai
ni
ng
.
O
th
er
w
is
e,
th
e
m
os
ts
im
-
il
ar
do
cu
m
en
t
w
il
l
be
ob
ta
in
ed
fr
om
th
e
m
on
ol
in
-
gu
al
da
ta
.
O
ur
m
et
ho
d
ca
n
be
vi
ew
ed
as
a
m
or
e
ge
ne
ra
l
fr
am
ew
or
k
th
an
pr
ev
io
us
L
D
A
-b
as
ed
ap
-
pr
oa
ch
es
.
4.
5
D
is
cu
ss
io
n
In
ou
r
ex
pe
ri
m
en
ts
,
In
pr
ev
io
us
L
D
A
-b
as
ed
m
et
ho
d,
if
a
do
cu
m
en
t
D
oc
co
nt
ai
ns
M
se
nt
en
ce
s,
al
l
M
se
nt
en
ce
s
w
il
l
sh
ar
e
th
e
sa
m
e
to
pi
c
di
st
ri
bu
ti
on
of
D
oc
.
A
l-
th
ou
gh
di
ff
er
en
t
se
nt
en
ce
s
m
ay
ex
pr
es
s
sl
ig
ht
ly
di
ff
er
en
t
im
pl
ic
at
io
ns
an
d
th
e
to
pi
c
w
il
l
ch
an
ge
,
th
e
co
nv
en
ti
on
al
L
D
A
-b
as
ed
ap
pr
oa
ch
do
es
no
t
ta
ke
th
e
to
pi
c
tr
an
si
ti
on
in
to
co
ns
id
er
at
io
n.
In
co
n-
tr
as
t,
ou
r
ap
pr
oa
ch
di
re
ct
ly
le
ar
ns
th
e
to
pi
c
re
p-
re
se
nt
at
io
n
w
it
h
an
ab
un
da
nc
y
of
re
la
te
d
do
cu
-
m
en
ts
.
In
ad
di
ti
on
al
to
th
e
or
ig
in
al
do
cu
m
en
tf
ro
m
w
hi
ch
th
e
se
nt
en
ce
is
ex
tr
ac
te
d,
th
e
IR
m
et
ho
d
al
so
re
tr
ie
ve
s
ot
he
r
re
le
va
nt
do
cu
m
en
ts
w
hi
ch
pr
o-
vi
de
co
m
pl
em
en
ta
ry
to
pi
c
in
fo
rm
at
io
n.
T
he
re
fo
re
,
th
e
to
pi
c
re
pr
es
en
ta
ti
on
s
le
ar
ne
d
ar
e
m
or
e
fi
ne
-
gr
ai
ne
d
an
d
th
us
m
or
e
ac
cu
ra
te
.
R
u
le
s
P
(?
|?
)
S
im
(z
s
,z
?
)
??
?
X
,d
el
iv
er
X
?
0.
02
37
0.
84
69
??
?
X
,d
is
tr
ib
ut
e
X
?
0.
05
46
0.
82
68
??
?
X
,s
en
d
X
?
0.
24
64
0.
61
19
T
ab
le
3:
D
ev
el
op
m
en
ta
nd
te
st
in
g
da
ta
us
ed
in
th
e
ex
pe
ri
m
en
ts
.
5
R
el
at
ed
W
or
k
T
op
ic
m
od
el
in
g
w
as
fi
rs
t
le
ve
ra
ge
d
to
im
pr
ov
e
S
M
T
pe
rf
or
m
an
ce
in
(Z
ha
o
an
d
X
in
g,
20
06
;Z
ha
o
an
d
X
in
g,
20
07
).
T
he
y
pr
op
os
ed
a
bi
li
ng
ua
l
to
pi
ca
l
ad
m
ix
tu
re
ap
pr
oa
ch
fo
r
w
or
d
al
ig
nm
en
t
an
d
as
su
m
ed
th
at
ea
ch
w
or
d-
pa
ir
fo
ll
ow
s
a
to
pi
c-
sp
ec
ifi
c
m
od
el
.
T
he
y
re
po
rt
ed
ex
te
ns
iv
e
em
pi
r-
ic
al
an
al
ys
is
an
d
im
pr
ov
ed
w
or
d
al
ig
nm
en
t
ac
-
cu
ra
cy
as
w
el
l
as
tr
an
sl
at
io
n
qu
al
it
y.
F
ol
lo
w
-
in
g
th
is
w
or
k,
(X
ia
o
et
al
.,
20
12
)
ex
te
nd
ed
to
pi
c-
sp
ec
ifi
c
le
xi
co
n
tr
an
sl
at
io
n
m
od
el
s
to
hi
er
ar
ch
ic
al
ph
ra
se
-b
as
ed
tr
an
sl
at
io
n
m
od
el
s,
w
he
re
th
e
to
pi
c
in
fo
rm
at
io
n
of
sy
nc
hr
on
ou
s
ru
le
s
w
as
di
re
ct
ly
in
-
fe
rr
ed
w
it
h
th
e
he
lp
of
do
cu
m
en
t-
le
ve
l
in
fo
rm
a-
ti
on
.
E
xp
er
im
en
ts
sh
ow
th
at
th
ei
r
ap
pr
oa
ch
no
t
on
ly
ac
hi
ev
ed
be
tt
er
tr
an
sl
at
io
n
pe
rf
or
m
an
ce
bu
t
al
so
pr
ov
id
ed
a
fa
st
er
de
co
di
ng
sp
ee
d
co
m
pa
re
d
w
it
h
pr
ev
io
us
le
xi
co
n-
ba
se
d
m
et
ho
ds
.
A
no
th
er
di
re
ct
io
n
of
ap
pr
oa
ch
es
le
ve
ra
ge
d
to
pi
c
m
od
el
in
g
te
ch
ni
qu
es
fo
r
do
m
ai
n
ad
ap
ta
ti
on
.
T
am
et
al
.
(2
00
7)
us
ed
bi
li
ng
ua
l
L
S
A
to
le
ar
n
la
te
nt
to
pi
c
di
st
ri
bu
ti
on
s
ac
ro
ss
di
ff
er
en
t
la
ng
ua
ge
s
an
d
en
fo
rc
e
on
e-
to
-o
ne
to
pi
c
co
rr
es
po
nd
en
ce
du
ri
ng
m
od
el
tr
ai
ni
ng
.
T
he
y
in
co
rp
or
at
ed
th
e
bi
li
ng
ua
l
to
pi
c
in
fo
rm
at
io
n
in
to
la
ng
ua
ge
m
od
el
ad
ap
ta
ti
on
an
d
le
xi
co
n
tr
an
sl
at
io
n
m
od
el
ad
ap
ta
ti
on
,
ac
hi
ev
-
in
g
si
gn
ifi
ca
nt
im
pr
ov
em
en
ts
in
th
e
la
rg
e-
sc
al
e
ev
al
ua
ti
on
.
(S
u
et
al
.,
20
12
)
in
ve
st
ig
at
ed
th
e
re
la
-
ti
on
sh
ip
be
tw
ee
n
ou
t-
of
-d
om
ai
n
bi
li
ng
ua
ld
at
a
an
d
in
-d
om
ai
n
m
on
ol
in
gu
al
da
ta
vi
a
to
pi
c
m
ap
pi
ng
us
-
Figure 4: An exampl from the NIST 2005 dataset. We ill strate the normalized topic repres ntations of
the source sentence and three ambiguous synchronous rules. Details are explained in Section 4.5.
our method is that it is applicable to both sentence-
level and doc ment-level SMT, since we do not
place any restricti ns on the input. In addition, our
method directly maximizes the similarity between
parallel sentence pairs, which is ideal for SMT de-
coding. Compared to document-level topic mod-
eling which uses the topic of a document for all
sentences within the document (Xiao et al, 2012),
our contributions are:
? We proposed a more general approach to
leveraging topic information for SMT by us-
ing IR methods to get a collection of related
documents, regardless of whether or not doc-
ument boundaries are explicitly given.
? We used neural networks to learn topic repre-
sentations more accurately, with more practi-
cable and scalable modeling techniques.
? We directly optimized bilingual topic simi-
larity in the deep learning framework with
the help of sentence-level parallel data, so
that the learned representation could be easily
used in SMT decoding procedure.
6 Conclusion and Future Work
In this paper, we propose a neural network based
approach to learning bilingual topic representa-
tion for SMT. We enrich contexts of parallel sen-
tence pairs with topic related monolingual data
and obtain a set of documents to represent sen-
tences. These documents are converted to a bag-
of-words input and fed into neural networks. The
learned low-dimensional vector is used to obtain
the topic representations of synchronous rules. In
SMT decoding, appropriate rules a e selected to
best match source texts according to their similar-
ity in the topic space. Experimental results show
that our approach is promising for SMT systems to
learn a better translation model. It is a significant
improvement over the state-of-the-art Hiero sys-
tem, as well as a conventional LDA-based method.
In the future research, we will extend our neural
network methods to address document-level trans-
lation, where topic transition between sentences is
a crucial problem to be solved. Since the transla-
tion of the current sentence is usually influenced
by the topic of previous sentences, we plan to
leverage recurrent neural networks to model this
phenomenon, where the history translation infor-
mation is naturally combined in the model.
Acknowledgments
We are grateful to the anonymous reviewers for
their insightful comments. We also thank Fei
Huang (BBN), Nan Yang, Yajuan Duan, Hong Sun
and Duyu Tang for the helpful discussions. This
work is supported by the National Natural Science
Foundation of China (Granted No. 61272384)
141
References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise train-
ing of deep networks. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153?160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Found. Trends Mach. Learn., 2(1):1?127, Jan-
uary.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
pages 387?394, Ann Arbor, Michigan, June. Asso-
ciation for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. Proceedings of Machine Trans-
lation Summit XI, pages 73?80.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1285?
1293, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,
Mu Li, and Ming Zhou. 2013. Multi-domain adap-
tation for SMT using multi-task learning. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1055?
1065, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech and Language
Processing, 20(1):30?42, January.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53?61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Amit Gruber, Michal Rosen-zvi, and Yair Weiss. 2007.
Hidden topic markov models. In In Proceedings of
Artificial Intelligence and Statistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527?1554, July.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In P. Bartlett, F.C.N. Pereira,
C.J.C. Burges, L. Bottou, and K.Q. Weinberger, ed-
itors, Advances in Neural Information Processing
Systems 25, pages 1106?1114.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y. Ng. 2006. Efficient sparse coding algo-
rithms. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 801?808. MIT Press, Cambridge,
MA.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
89?97, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
142
Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based transla-
tion. In Proceedings of ACL-08: HLT, pages 1003?
1011, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696?699. MIT
Press, Cambridge, MA, USA.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 354?362, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011a. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 151?161, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 459?468, Jeju Island, Korea,
July. Association for Computational Linguistics.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, December.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th International
Conference on Machine Learning, ICML ?08, pages
1096?1103, New York, NY, USA. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res., 11:3371?
3408, December.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for
hierarchical phrase-based translation. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 750?758, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
AAAI.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Bing Zhao and Eric P. Xing. 2007. Hm-bitam: Bilin-
gual topic exploration, word alignment, and trans-
lation. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1689?1696. MIT
Press, Cambridge, MA.
143
