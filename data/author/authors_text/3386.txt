Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 85?90,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Question Difficulty Estimation in Community Question Answering Services?
Jing Liu? Quan Wang? Chin-Yew Lin? Hsiao-Wuen Hon?
?Harbin Institute of Technology, Harbin 150001, P.R.China
?Peking University, Beijing 100871, P.R.China
?Microsoft Research Asia, Beijing 100080, P.R.China
jliu@ir.hit.edu.cn quanwang1012@gmail.com {cyl,hon}@microsoft.com
Abstract
In this paper, we address the problem of
estimating question difficulty in community
question answering services. We propose a
competition-based model for estimating ques-
tion difficulty by leveraging pairwise compar-
isons between questions and users. Our ex-
perimental results show that our model sig-
nificantly outperforms a PageRank-based ap-
proach. Most importantly, our analysis shows
that the text of question descriptions reflects
the question difficulty. This implies the pos-
sibility of predicting question difficulty from
the text of question descriptions.
1 Introduction
In recent years, community question answering (C-
QA) services such as Stackoverflow1 and Yahoo!
Answers2 have seen rapid growth. A great deal
of research effort has been conducted on CQA, in-
cluding: (1) question search (Xue et al, 2008; Du-
an et al, 2008; Suryanto et al, 2009; Zhou et al,
2011; Cao et al, 2010; Zhang et al, 2012; Ji et
al., 2012); (2) answer quality estimation (Jeon et al,
2006; Agichtein et al, 2008; Bian et al, 2009; Liu
et al, 2008); (3) user expertise estimation (Jurczyk
and Agichtein, 2007; Zhang et al, 2007; Bouguessa
et al, 2008; Pal and Konstan, 2010; Liu et al, 2011);
and (4) question routing (Zhou et al, 2009; Li and
King, 2010; Li et al, 2011).
?This work was done when Jing Liu and Quan Wang were
visiting students at Microsoft Research Asia. Quan Wang is
currently affiliated with Institute of Information Engineering,
Chinese Academy of Sciences.
1http://stackoverflow.com
2http://answers.yahoo.com
However, less attention has been paid to question
difficulty estimation in CQA. Question difficulty es-
timation can benefit many applications: (1) Experts
are usually under time constraints. We do not want
to bore experts by routing every question (including
both easy and hard ones) to them. Assigning ques-
tions to experts by matching question difficulty with
expertise level, not just question topic, will make
better use of the experts? time and expertise (Ack-
erman and McDonald, 1996). (2) Nam et al (2009)
found that winning the point awards offered by the
reputation system is a driving factor in user partici-
pation in CQA. Question difficulty estimation would
be helpful in designing a better incentive mechanis-
m by assigning higher point awards to more diffi-
cult questions. (3) Question difficulty estimation can
help analyze user behavior in CQA, since users may
make strategic choices when encountering questions
of different difficulty levels.
To the best of our knowledge, not much research
has been conducted on the problem of estimating
question difficulty in CQA. The most relevant work
is a PageRank-based approach proposed by Yang et
al. (2008) to estimate task difficulty in crowdsourc-
ing contest services. Their key idea is to construct
a graph of tasks: creating an edge from a task t1 to
a task t2 when a user u wins task t1 but loses task
t2, implying that task t2 is likely to be more diffi-
cult than task t1. Then the standard PageRank al-
gorithm is employed on the task graph to estimate
PageRank score (i.e., difficulty score) of each task.
This approach implicitly assumes that task difficulty
is the only factor affecting the outcomes of competi-
tions (i.e. the best answer). However, the outcomes
of competitions depend on both the difficulty levels
of tasks and the expertise levels of competitors (i.e.
85
other answerers).
Inspired by Liu et al (2011), we propose a
competition-based approach which jointly models
question difficulty and user expertise level. Our ap-
proach is based on two intuitive assumptions: (1)
given a question answering thread, the difficulty s-
core of the question is higher than the expertise score
of the asker, but lower than that of the best answerer;
(2) the expertise score of the best answerer is higher
than that of the asker as well as all other answer-
ers. Given the two assumptions, we can determine
the question difficulty score and user expertise score
through pairwise comparisons between (1) a ques-
tion and an asker, (2) a question and a best answerer,
(3) a best answerer and an asker, and (4) a best an-
swerer and all other non-best answerers.
The main contributions of this paper are:
?We propose a competition-based approach to es-
timate question difficulty (Sec. 2). Our model signif-
icantly outperforms the PageRank-based approach
(Yang et al, 2008) for estimating question difficulty
on the data of Stack Overflow (Sec. 3.2).
?Additionally, we calibrate question difficulty s-
cores across two CQA services to verify the effec-
tiveness of our model (Sec. 3.3).
?Most importantly, we demonstrate that different
words or tags in the question descriptions indicate
question difficulty levels. This implies the possibil-
ity of predicting question difficulty purely from the
text of question descriptions (Sec. 3.4).
2 Competition based Question Difficulty
Estimation
CQA is a virtual community where people can ask
questions and seek opinions from others. Formally,
when an asker ua posts a question q, there will be
several answerers to answer her question. One an-
swer among the received ones will be selected as the
best answer by the asker ua or voted by the com-
munity. The user who provides the best answer is
called the best answerer ub, and we denote the set of
all non-best answerers as S = {uo1 , ? ? ? , uoM}. As-
suming that question difficulty scores and user ex-
pertise scores are expressed on the same scale, we
make the following two assumptions:
?The difficulty score of question q is higher than
the expertise score of asker ua, but lower than that
of the best answerer ub. This is intuitive since the
best answer ub correctly responds to question q that
asker ua does not know.
?The expertise score of the best answerer ub is
higher than that of asker ua and all answerers in S.
This is straightforward since the best answerer ub
solves question q better than asker ua and all non-
best answerers in S.
Let?s view question q as a pseudo user uq. Tak-
ing a competitive viewpoint, each pairwise compar-
ison can be viewed as a two-player competition with
one winner and one loser, including (1) one compe-
tition between pseudo user uq and asker ua, (2) one
competition between pseudo user uq and the best
answerer ub, (3) one competition between the best
answerer ub and asker ua, and (4) |S| competitions
between the best answerer ub and all non-best an-
swers in S. Additionally, pseudo user uq wins the
first competition and the best answerer ub wins all
remaining (|S| + 2) competitions.
Hence, the problem of estimating the question d-
ifficulty score (and the user expertise score) is cast
as a problem of learning the relative skills of play-
ers from the win-loss results of the generated two-
player competitions. Formally, let Q denote the set
of all questions in one category (or topic), andRq de-
note the set of all two-player competitions generated
from question q ? Q, i.e., Rq = {(ua ? uq), (uq ?
ub), (ua ? ub), (uo1 ? ub), ? ? ? , (uo|S| ? ub)},
where j ? i means that user i beats user j in the
competition. Define
R =
?
q?Q
Rq (1)
as the set of all two-player competitions. Our prob-
lem is then to learn the relative skills of players from
R. The learned skills of the pseudo question users
are question difficulty scores, and the learned skills
of all other users are their expertise scores.
TrueSkill In this paper, we follow (Liu et al,
2011) and apply TrueSkill to learn the relative skill-
s of players from the set of generated competitions
R (Equ. 1). TrueSkill (Herbrich et al, 2007) is a
Bayesian skill rating model that is developed for es-
timating the relative skill levels of players in games.
In this paper, we present a two-player version of
TrueSkill with no-draw.
TrueSkill assumes that the practical performance
of each player in a game follows a normal distribu-
86
tion N(?, ?2), where ? means the skill level of the
player and ? means the uncertainty of the estimated
skill level. Basically, TrueSkill learns the skill lev-
els of players by leveraging Bayes? theorem. Giv-
en the current estimated skill levels of two players
(priori probability) and the outcome of a new game
between them (likelihood), TrueSkill model updates
its estimation of player skill levels (posterior prob-
ability). TrueSkill updates the skill level ? and the
uncertainty ? intuitively: (a) if the outcome of a new
competition is expected, i.e. the player with higher
skill level wins the game, it will cause small updates
in skill level ? and uncertainty ?; (b) if the outcome
of a new competition is unexpected, i.e. the player
with lower skill level wins the game, it will cause
large updates in skill level ? and uncertainty ?. Ac-
cording to these intuitions, the equations to update
the skill level ? and uncertainty ? are as follows:
?winner = ?winner +
?2winner
c ? v
(
t
c ,
?
c
)
, (2)
?loser = ?loser ?
?2loser
c
? v
(
t
c
, ?
c
)
, (3)
?2winner = ?2winner ?
[
1 ? ?
2
winner
c2
? w
(
t
c
, ?
c
)]
,
(4)
?2loser = ?2loser ?
[
1 ?
?2loser
c2
? w
(
t
c
, ?
c
)]
, (5)
where t = ?winner ? ?loser and c2 = 2?2 +
?2winner+?2loser. Here, ? is a parameter representing
the probability of a draw in one game, and v(t, ?)
and w(t, ?) are weighting factors for skill level ?
and standard deviation ? respectively. Please refer
to (Herbrich et al, 2007) for more details. In this
paper, we set the initial values of the skill level ?
and the standard deviation ? of each player the same
as the default values used in (Herbrich et al, 2007).
3 Experiments
3.1 Data Set
In this paper, we use Stack Overflow (SO) for our
experiments. We obtained a publicly available da-
ta set3 of SO between July 31, 2008 and August 1,
2012. SO contains questions with various topics,
such as programming, mathematics, and English. In
this paper, we use SO C++ programming (SO/CPP)
3http://blog.stackoverflow.com/category/
cc-wiki-dump/
and mathematics4 (SO/Math) questions for our main
experiments. Additionally, we use the data of Math
Overflow5 (MO) for calibrating question difficulty
scores across communities (Sec. 3.3). The statistics
of these data sets are shown in Table 1.
SO/CPP SO/Math MO
# of questions 122, 012 51, 174 27, 333
# of answers 357, 632 94, 488 65, 966
# of users 67, 819 16, 961 12, 064
Table 1: The statistics of the data sets.
To evaluate the effectiveness of our proposed
model for estimating question difficulty scores, we
randomly sampled 300 question pairs from both
SO/CPP and SO/Math, and we asked experts to
compare the difficulty of every pair. We had two
graduate students majoring in computer science an-
notate the SO/CPP question pairs, and two gradu-
ate students majoring in mathematics annotate the
SO/Math question pairs. When annotating each
question pair, only the titles, descriptions, and tags
of the questions were shown, and other information
(e.g. users, answers, etc.) was excluded. Given each
pair of questions (q1 and q2), the annotators were
asked to give one of four labels: (1) q1 ? q2, which
means that the difficulty of q1 was higher than q2;
(2) q1 ? q2, which means that the difficulty of q1
was lower than q2; (3) q1 = q2, which means that
the difficulty of q1 was equal to q2; (4) Unknown,
which means that the annotator could not make a
decision. The agreements between annotators on
both SO/CPP (kappa value = 0.741) and SO/Math
(kappa value = 0.873) were substantial. When eval-
uating models, we only kept the pairs that annotators
had given the same labels. There were 260 SO/CPP
question pairs and 280 SO/Math question pairs re-
maining.
3.2 Accuracy of Question Difficulty Estimation
We employ a standard evaluation metric for infor-
mation retrieval: accuracy (Acc), defined as follows:
Acc = the number of correct pairwise comparisons
the total number of pairwise comparisons
.
We use the PageRank-based approach proposed
by Yang et al (2008) as a baseline. As described in
4http://math.stackexchange.com
5http://mathoverflow.net
87



	


 	     	     	 	 	 
  	






