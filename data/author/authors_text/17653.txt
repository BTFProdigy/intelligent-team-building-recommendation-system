Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 365?372,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Quality Estimation for Machine Translation Using the Joint Method 
of Evaluation Criteria and Statistical Modeling 
 
 
Aaron Li-Feng Han 
hanlifengaaron@gmail.com 
Yi Lu 
mb25435@umac.mo 
Derek F. Wong 
derekfw@umac.mo 
   
Lidia S. Chao 
lidiasc@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Junwen Xing 
mb15470@umac.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to introduce our participation in 
the WMT13 shared tasks on Quality Estima-
tion for machine translation without using ref-
erence translations. We submitted the results 
for Task 1.1 (sentence-level quality estima-
tion), Task 1.2 (system selection) and Task 2 
(word-level quality estimation). In Task 1.1, 
we used an enhanced version of BLEU metric 
without using reference translations to evalu-
ate the translation quality. In Task 1.2, we uti-
lized a probability model Na?ve Bayes (NB) as 
a classification algorithm with the features 
borrowed from the traditional evaluation met-
rics. In Task 2, to take the contextual infor-
mation into account, we employed a discrimi-
native undirected probabilistic graphical mod-
el Conditional random field (CRF), in addition 
to the NB algorithm. The training experiments 
on the past WMT corpora showed that the de-
signed methods of this paper yielded promis-
ing results especially the statistical models of 
CRF and NB. The official results show that 
our CRF model achieved the highest F-score 
0.8297 in binary classification of Task 2. 
 
1 Introduction 
Due to the fast development of Machine transla-
tion, different automatic evaluation methods for 
the translation quality have been proposed in re-
cent years. One of the categories is the lexical 
similarity based metric. This kind of metrics in-
cludes the edit distance based method, such as 
WER (Su et al, 1992), Multi-reference WER 
(Nie?en et al, 2000), PER (Tillmann et al, 
1997), the works of (Akiba, et al, 2001), 
(Leusch et al, 2006) and (Wang and Manning, 
2012); the precision based method, such as 
BLEU (Papineni et al, 2002), NIST (Doddington, 
2002), and SIA (Liu and Gildea, 2006); recall 
based method, such as ROUGE (Lin and Hovy 
2003); and the combination of precision and re-
call, such as GTM (Turian et al, 2003), METE-
OR (Lavie and Agarwal, 2007), BLANC (Lita et 
al., 2005), AMBER (Chen and Kuhn, 2011), 
PORT (Chen et al, 2012b), and LEPOR (Han et 
al., 2012). 
Another category is the using of linguistic fea-
tures. This kind of metrics includes the syntactic 
similarity, such as the POS information used by 
TESLA (Dahlmeier et al, 2011), (Liu et al, 
2010) and (Han et al, 2013), phrase information 
used by (Povlsen, et al, 1998) and (Echizen-ya 
and Araki, 2010), sentence structure used by 
(Owczarzak et al, 2007); the semantic similarity, 
such as textual entailment used by (Mirkin et al, 
2009) and (Castillo and Estrella, 2012), Syno-
nyms used by METEOR (Lavie and Agarwal, 
2007), (Wong and Kit, 2012), (Chan and Ng, 
2008); paraphrase used by (Snover et al, 2009). 
The traditional evaluation metrics tend to 
evaluate the hypothesis translation as compared 
to the reference translations that are usually of-
fered by human efforts. However, in the practice, 
there is usually no golden reference for the trans-
lated documents, especially on the internet works. 
How to evaluate the quality of automatically 
translated documents or sentences without using 
the reference translations becomes a new chal-
lenge in front of the NLP researchers. 
365
ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X . 
ADJ PREP, 
PREP/DEL 
ADV, 
NEG 
CC, 
CCAD, 
CCNEG, 
CQUE, 
CSUBF, 
CSUBI, 
CSUBX 
ART NC, 
NMEA, 
NMON, 
NP, 
PERCT,  
UMMX 
CARD, 
CODE, 
QU 
DM, 
INT, 
PPC, 
PPO, 
PPX, 
REL 
SE VCLIger, 
VCLIinf, 
VCLIfin, 
VEadj, 
VEfin, 
VEger, 
VEinf, 
VHadj, 
VHfin, 
VHger, 
VHinf, 
VLadj, 
VLfin, 
VLger, 
VLinf, 
VMadj, 
VMfin, 
VMger, 
VMinf, 
VSadj, 
VSfin, 
VSger, 
VSinf 
ACRNM, 
ALFP, 
ALFS, 
FO, ITJN, 
ORD, 
PAL, 
PDEL, 
PE, PNC, 
SYM 
BACKSLASH, 
CM, COLON, 
DASH, DOTS, 
FS, LP, QT, 
RP, SEMICO-
LON, SLASH 
Table 1: Developed POS mapping for Spanish and universal tagset 
 
2 Related Works 
Gamon et al (2005) perform a research about 
reference-free SMT evaluation method on sen-
tence level. This work uses both linear and non-
linear combinations of language model and SVM 
classifier to find the badly translated sentences. 
Albrecht and Hwa (2007) conduct the sentence-
level MT evaluation utilizing the regression 
learning and based on a set of weaker indicators 
of fluency and adequacy as pseudo references. 
Specia and Gimenez (2010) use the Confidence 
Estimation features and a learning mechanism 
trained on human annotations. They show that 
the developed models are highly biased by diffi-
culty level of the input segment, therefore they 
are not appropriate for comparing multiple sys-
tems that translate the same input segments. Spe-
cia et al (2010) discussed the issues between the 
traditional machine translation evaluation and the 
quality estimation tasks recently proposed. The 
traditional MT evaluation metrics require refer-
ence translations in order to measure a score re-
flecting some aspects of its quality, e.g. the 
BLEU and NIST. The quality estimation ad-
dresses this problem by evaluating the quality of 
translations as a prediction task and the features 
are usually extracted from the source sentences 
and target (translated) sentences. They also show 
that the developed methods correlate better with 
human judgments at segment level as compared 
to traditional metrics. Popovi? et al (2011) per-
form the MT evaluation using the IBM model 
one with the information of morphemes, 4-gram 
POS and lexicon probabilities. Mehdad et al 
(2012) use the cross-lingual textual entailment to 
push semantics into the MT evaluation without 
using reference translations. This evaluation 
work mainly focuses on the adequacy estimation. 
Avramidis (2012) performs an automatic sen-
tence-level ranking of multiple machine transla-
tions using the features of verbs, nouns, sentenc-
es, subordinate clauses and punctuation occur-
rences to derive the adequacy information. Other 
descriptions of the MT Quality Estimation tasks 
can be gained in the works of (Callison-Burch et 
al., 2012) and (Felice and Specia, 2012). 
3 Tasks Information  
This section introduces the different sub-tasks we 
participated in the Quality Estimation task of 
WMT 13 and the methods we used.  
3.1 Task 1-1 Sentence-level QE 
Task 1.1 is to score and rank the post-editing 
effort of the automatically translated English-
Spanish sentences without offering the reference 
translation. 
Firstly, we develop the English and Spanish 
POS tagset mapping as shown in Table 1. The 75 
Spanish POS tags yielded by the Treetagger 
(Schmid, 1994) are mapped to the 12 universal 
tags developed in (Petrov et al, 2012). The Eng-
lish POS tags are extracted from the parsed sen-
tences using the Berkeley parser (Petrov et al, 
2006). 
Secondly, the enhanced version of BLEU 
(EBLEU) formula is designed with the factors of 
modified length penalty (   ), precision, and 
recall, the   and   representing the lengths of 
hypothesis (target) sentence and source sentence 
respectively. We use the harmonic mean of pre-
cision and recall, i.e.  (       ). We assign 
the weight values     and    , i.e. higher 
weight value is assigned to precision, which is 
different with METEOR (the inverse values). 
 
       
          (?      ( (       ))) (1) 
 
     {
   
 
            
   
 
            
 (2) 
 
    
                    
                                
 (3) 
 
    
                    
                                
 (4) 
366
Lastly, the scoring for the post-editing effort 
of the automatically translated sentences is per-
formed on the extracted POS sequences of the 
source and target languages. The evaluated per-
formance of EBLEU on WMT 12 corpus is 
shown in Table 2 using the Mean-Average-Error 
(MAE), Root-Mean-Squared-Error (RMSE).  
 
 Precision Recall MLP EBLEU 
MAE 0.17 0.19 0.25 0.16 
RMSE 0.22 0.24 0.30 0.21 
Table 2: Performance on the WMT12 corpus 
The official evaluation scores of the testing re-
sults on WMT 13 corpus are shown in Table 3. 
The testing results show similar scores as com-
pared to the training scores (the MAE score is 
around 0.16 and the RMSE score is around 0.22), 
which shows a stable performance of the devel-
oped model EBLEU. However, the performance 
of EBLEU is not satisfactory currently as shown 
in the Table 2 and Table 3. This is due to the fact 
that we only used the POS information as lin-
guistic feature. This could be further improved 
by the combination of lexical information and 
other linguistic features such as the sentence 
structure, phrase similarity, and text entailment. 
 
 MAE RMSE DeltaAvg 
Spearman 
Corr 
EBLEU 16.97 21.94 2.74 0.11 
Baseline 
SVM 
14.81 18.22 8.52 0.46 
Table 3: Performance on the WMT13 corpus 
3.2 Task 1-2 System Selection 
Task 1.2 is the system selection task on EN-ES 
and DE-EN language pairs. Participants are re-
quired to rank up to five alternative translations 
for the same source sentence produced by multi-
ple translation systems.  
Firstly, we describe the two variants of 
EBLEU method for this task. We score the five 
alternative translation sentences as compared to 
the source sentence according to the closeness of 
their POS sequences. The German POS is also 
extracted using Berkeley parser (Petrov et al, 
2006). The mapping of German POS to universal 
POS tagset is using the developed one in the 
work of (Petrov et al, 2012). When we convert 
the absolute scores into the corresponding rank 
values, the variant EBLEU-I means that we use 
five fixed intervals (with the span from 0 to 1) to 
achieve the alignment as shown in Table 4. 
[1,0.4) [0.4, 0.3) [0.3, 0.25) [0.25, 0.2) [0.2, 0] 
5 4 3 2 1 
Table 4: Convert absolute scores into ranks 
 
The alignment work from absolute scores to 
rank values shown in Table 4 is empirically de-
termined. We have made a statistical work on the 
absolute scores yielded by our metrics, and each 
of the intervals shown in Table 4 covers the simi-
lar number of sentence scores. 
On the other hand, in the metric EBLEU-A, 
?A? means average. The absolute sentence edit 
scores are converted into the five rank values 
with the same number (average number). For 
instance, if there are 1000 sentence scores in to-
tal then each rank level (from 1 to 5) will gain 
200 scores from the best to the worst. 
Secondly, we introduce the NB-LPR model 
used in this task. NB-LPR means the Na?ve 
Bayes classification algorithm using the features 
of Length penalty (introduced in previous sec-
tion), Precision, Recall and Rank values. NB-
LPR considers each of its features independently. 
Let?s see the conditional probability that is also 
known as Bayes? rule. If the  ( | )  is given, 
then the  ( | ) can be calculated as follows: 
 
  ( | )  
 ( | ) ( )
 ( )
 (5) 
 
Given a data point identified as 
 (          ) and the classifications 
 (          ), Bayes? rule can be applied to 
this statement: 
 
  (  |          )  
 (         |  ) (  )
 (         )
 (6) 
 
As in many practical applications, parameter 
estimation for NB-LPR model uses the method 
of maximum likelihood. For details of Na?ve 
Bayes algorithm, see the works of (Zhang, 2004) 
and (Harrington, 2012). 
Thirdly, the SVM-LPR model means the sup-
port vector machine classification algorithm us-
ing the features of Length penalty, Precision, 
Recall and Rank values, i.e. the same features as 
in NB-LPR. SVM solves the nonlinear classifica-
tion problem by mapping the data from a low 
dimensional space to a high dimensional space 
using the Kernel methods. In the projected high 
dimensional space, the problem usually becomes 
a linear one, which is easier to solve. SVM is 
also called maximum interval classifier because 
it tries to find the optimized hyper plane that 
367
separates different classes with the largest mar-
gin, which is usually a quadratic optimization 
problem. Let?s see the formula below, we should 
find the points with the smallest margin to the 
hyper plane and then maximize this margin. 
 
          {    (      ( 
    ))  
 
? ?
}
 (7) 
 
where   is normal to the hyper plane, || || is 
the Euclidean norm of  , and | | || ||  is the 
perpendicular distance from the hyper plane to 
the origin. For details of SVM, see the works of 
(Cortes and Vapnik, 1995) and (Burges, 1998). 
 
EN-ES 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.315 .399 .40s .304 .551 60.67s 
DE-EN 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.318 .401 .79s .312 .559 111.7s 
Table 5: NB-LPR and SVM-LPR training 
In the training stage, we used all the officially 
released data of WMT 09, 10, 11 and 12 for the 
EN-ES and DE-EN language pairs. We used the 
WEKA (Hall et al, 2009) data mining software 
to implement the NB and SVM algorithms. The 
training scores are shown in Table 5. The NB-
LPR performs lower scores than the SVM-LPR 
but faster than SVM-LPR. 
 
 DE-EN EN-ES 
Methods 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
EBLEU-I -0.38 -0.03 -0.35 0.02 
EBLEU-A N/A N/A -0.27 N/A 
NB-LPR -0.49 0.01 N/A 0.07 
Baseline  -0.12 0.08 -0.23 0.03 
Table 6: QE Task 1.2 testing scores 
The official testing scores are shown in Table 
6. Each task is allowed to submit up to two sys-
tems and we submitted the results using the 
methods of EBLEU and NB-LPR. The perfor-
mance of NB-LPR on EN-ES language pair 
shows higher Tau score (0.07) than the baseline 
system score (0.03) when the ties are ignored. 
Because of the number limitation of submitted 
systems for each task, we did not submit the 
SVM-LPR results. However, the training exper-
iments prove that the SVM-LPR model performs 
better than the NB-LPR model though SVM-
LPR takes more time to run. 
3.3 Task 2 Word-level QE 
Task 2 is the word-level quality estimation of 
automatically translated news sentences from 
English to Spanish without given reference trans-
lations. Participants are required to judge each 
translated word by assigning a two- or multi-
class labels. In the binary classification, a good 
or a bad label should be judged, where ?bad? 
indicates the need for editing the token. In the 
multi-class classification, the labels include 
?keep?, ?delete? and ?substitute?. In addition to 
the NB method, in this task, we utilized a dis-
criminative undirected probabilistic graphical 
model, i.e. Conditional Random Field (CRF). 
CRF is early employed by Lefferty (Lefferty 
et al, 2001) to deal with the labeling problems of 
sequence data, and is widely used later by other 
researchers. As the preparation for CRF defini-
tion, we assume that   is a variable representing 
the input sequence, and   is another variable rep-
resenting the corresponding labels to be attached 
to  . The two variables interact as conditional 
probability  ( | )  mathematically. Then the 
definition of CRF: Let a graph model   (   ) 
comprise a set   of vertices or nodes together 
with a set   of edges or lines and      |  
  , such that   is indexed by the vertices of  , 
then (   ) shapes a CRF model. This set meets 
the following form:  
 
   ( | )      
(?     (   |   )       ?     (   |   )     )
 (8) 
 
where   and   represent the data sequence and 
label sequence respectively;    and    are the 
features to be defined;    and    are the parame-
ters trained from the datasets. We used the tool 
CRF++1 to implement the CRF algorithm. The 
features we used for the NB and CRF are shown 
in Table 7. We firstly trained the CRF and NB 
models on the officially released training corpus 
(produced by Moses and annotated by computing 
TER with some tweaks). Then we removed the 
truth labels in the training corpus (we call it 
pseudo test corpus) and labeled each word using 
the derived training models. The test results on 
the pseudo test corpus are shown in Table 8, 
                                                 
1 https://code.google.com/p/crfpp/ 
368
which specifies CRF performs better than NB 
algorithm. 
 
     (    ) 
Unigram, from antecedent 4th 
to subsequent 3rd token 
       
 (    ) 
Bigram, from antecedent 2nd 
to subsequent 2nd token 
      
Jump bigram, antecedent and 
subsequent token 
          
 (    ) 
Trigram, from antecedent 2nd 
to subsequent 2nd token 
Table 7: Developed features 
 
Binary 
CRF NB 
Training Accuracy Training Accuracy 
Itera=108 
Time=2.48s 
0.944 Time=0.59s 0.941 
Multi-classes 
CRF NB 
Training Accuracy Training Accuracy 
Itera=106 
Time=3.67s 
0.933 Time=0.55s 0.929 
Table 8: Performance on pseudo test corpus 
The official testing scores of Task 2 are shown 
in Table 9. We include also the results of other 
participants (CNGL and LIG) and their ap-
proaches. 
 
 Binary Multiclass 
Methods Pre Recall F1 Acc 
CNGL-
dMEMM 
0.7392 0.9261 0.8222 0.7162 
CNGL-
MEMM 
0.7554 0.8581 0.8035 0.7116 
LIG-All N/A N/A N/A 0.7192 
LIG-FS 0.7885 0.8644 0.8247 0.7207 
LIG-
BOOSTING 
0.7779 0.8843 0.8276 N/A 
NB 0.8181 0.4937 0.6158 0.5174 
CRF 0.7169 0.9846 0.8297 0.7114 
Table 9: QE Task 2 official testing scores 
The results show that our method CRF yields 
a higher recall score than other systems in binary 
judgments task, and this leads to the highest F1 
score (harmonic mean of precision and recall). 
The recall value reflects the loyalty to the truth 
data. The augmented feature set designed in this 
paper allows the CRF to take the contextual in-
formation into account, and this contributes 
much to the recall score. On the other hand, the 
accuracy score of CRF in multiclass evaluation is 
lower than LIG-FS method. 
4 Conclusions 
This paper describes the algorithms and features 
we used in the WMT 13 Quality Estimation tasks. 
In the sentence-level QE task (Task 1.1), we de-
velop an enhanced version of BLEU metric, and 
this shows a potential usage for the traditional 
evaluation criteria. In the newly proposed system 
selection task (Task 1.2) and word-level QE task 
(Task 2), we explore the performances of several 
statistical models including NB, SVM, and CRF, 
of which the CRF performs best, the NB per-
forms lower than SVM but much faster than 
SVM. The official results show that the CRF 
model yields the highest F-score 0.8297 in binary 
classification judgment of word-level QE task. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
RG060/09-10S/CS/FST. The authors also wish to 
thank the anonymous reviewers for many helpful 
comments. 
References  
Akiba, Yasuhiro, Kenji Imamura, and Eiichiro Sumita. 
2001. Using Multiple Edit Distances to Automati-
cally Rank Machine Translation Output. In Pro-
ceedings of the MT Summit VIII, Santiago de 
Compostela, Spain. 
Albrecht, Joshua, and Rebecca Hwa. 2007. Regres-
sion for sentence-level MT evaluation with pseudo 
references. ACL. Vol. 45. No. 1. 
Avramidis, Eleftherios. 2012. Comparative quality 
estimation: Automatic sentence-level ranking of 
multiple machine translation outputs. In Proceed-
ings of 24th International Conference on 
Computational Linguistics (COLING), pages 
115?132, Mumbai, India. 
Burges, Christopher J. C. 1998. A Tutorial on Support 
Vector Machines for Pattern Recognition. J. Data 
Min. Knowl. Discov. Volume 2 Issue 2, June 
1998, 121-167. Kluwer Academic Publishers 
Hingham, MA, USA. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh 
369
Workshop on Statistical Machine Translation, 
pages 10?51, Montr?al, Canada, June. 
Castillo, Julio and Paula Estrella. 2012. Semantic 
Textual Similarity for MT evaluation, Proceed-
ings of the 7th Workshop on Statistical Ma-
chine Translation (WMT2012), pages 52?58, 
Montre a?l, Canada, June 7-8. Association for 
Computational Linguistics. 
Chan, Yee Seng and Hwee Tou Ng. 2008. MAXSIM: 
A maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL 2008: 
HLT, pages 55?62. Association for Computational 
Linguistics. 
Chen, Boxing and Roland Kuhn. 2011. Amber: A 
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical 
Machine translation of the Association for 
Computational Linguistics(ACL-WMT), pages 
71-77, Edinburgh, Scotland, UK. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July. 
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
Vector Networks, J. Machine Learning, Volume 
20, issue 3, pp 273-297. Kluwer Academic Pub-
lishers, Boston. Manufactured in The Netherlands. 
Dahlmeier, Daniel, Chang Liu, and Hwee Tou Ng. 
2011. TESLA at WMT2011: Translation evalua-
tion and tunable metric. In Proceedings of the 
Sixth Workshop on Statistical Machine Trans-
lation, Association for Computational Linguis-
tics (ACL-WMT), pages 78-84, Edinburgh, Scot-
land, UK. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Echizen-ya, Hiroshi and Kenji Araki. 2010. Automat-
ic evaluation method for machine translation using 
noun-phrase chunking. In Proceedings of ACL 
2010, pages 108?117. Association for Computa-
tional Linguistics. 
Gamon, Michael, Anthony Aue, and Martine Smets. 
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. 
Proceedings of EAMT. 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten. 2009. 
The WEKA data mining software: An update. 
SIGKDD Explorations, 11. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Han, Aaron Li-Feng, Derek F. Wong, Lidia S. Chao, 
Liangye He, Yi Lu, Junwen Xing and Xiaodong 
Zeng. 2013. Language-independent Model for Ma-
chine Translation Evaluation with Reinforced Fac-
tors. Proceedings of the 14th International 
Conference of Machine Translation Summit 
(MT Summit 2013), Nice, France. 
Harrington, Peter. 2012. Classifying with probability 
theory: na?ve bayes. Machine Learning in Ac-
tion, Part 1 Classification. Page 61-82. Publisher: 
Manning Publications. April. 
Lafferty, John, McCallum Andrew, and Pereira C.N. 
Ferando. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of 18th Internation-
al Conference on Machine Learning. 282-289. 
Lavie, Alon and Abhaya Agarwal. 2007. METEOR: 
An Automatic Metric for MT Evaluation with High 
Levels of Correlation with Human Judgments, 
Proceedings of the ACL Second Workshop on 
Statistical Machine Translation, pages 228-231, 
Prague, June. 
Leusch, Gregor, Nicola Ueffing, and Hermann Ney. 
2006. CDer: Efficient MT Evaluation Using Block 
Movements. In Proceedings of the 13th Confer-
ence of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-06), 
241-248. 
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003 
Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada, May 27 - June 
1. 
Lita, Lucian Vlad, Monica Rogati and Alon Lavie. 
2005. BLANC: Learning Evaluation Metrics for 
MT, Proceedings of Human Language Tech-
nology Conference and Conference on Empir-
ical Methods in Natural Language Processing 
(HLT/EMNLP), pages 740?747, Vancouver, Oc-
tober. Association for Computational Linguistics. 
Liu, Chang, Daniel Dahlmeier and Hwee Tou Ng. 
2010. TESLA: Translation evaluation of sentences 
370
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR. 
Liu, Ding and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. 
Sydney. ACL06. 
Mariano, Felice and Lucia Specia. 2012. Linguistic 
Features for Quality Estimation. Proceedings of 
the 7th Workshop on Statistical Machine 
Translation, pages 96?103. 
Mehdad, Yashar, Matteo Negri, and Marcello Federi-
co. 2012. Match without a referee: evaluating MT 
adequacy without reference translations. Proceed-
ings of the Seventh Workshop on Statistical 
Machine Translation. Association for Compu-
tational Linguistics. 
Mirkin, Shachar, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, and Idan Szpektor. 2009. 
Source-Language Entailment Modeling for Trans-
lating Unknown Terms, Proceedings of the Joint 
Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the 
AFNLP, pages 791?799, Suntec, Singapore, 2-7. 
ACL and AFNLP. 
Nie?en, Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International 
Conference on Language Resources and Eval-
uation (LREC-2000). 
Owczarzak, Karolina, Josef van Genabith and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation, Proceedings of the ACL 
Second Workshop on Statistical Machine 
Translation, pages 104-111, Prague. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja, David Vilar, Eleftherios Avramidis, 
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In 
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, Association for 
Computational Linguistics (ACL-WMT), pages 
99-103, Edinburgh, Scotland, UK. 
Povlsen, Claus, Nancy Underwood, Bradley Music, 
and Anne Neville. 1998. Evaluating Text-Type 
Suitability for Machine Translation a Case Study 
on an English-Danish System. Proceedings of the 
First Language Resources and Evaluation 
Conference, LREC-98, Volume I. 27-31. Grana-
da, Spain. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew G., Nitin Madnani, Bonnie Dorr, 
and Richard Schwartz. 2009. TER-Plus: paraphrase, 
semantic, and alignment enhancements to Transla-
tion Edit Rate. J. Machine Tranlslation, 23: 117-
127. 
Specia, Lucia and Gimenez, J. 2010. Combining Con-
fidence Estimation and Reference-based Metrics 
for Segment-level MT Evaluation. The Ninth 
Conference of the Association for Machine 
Translation in the Americas (AMTA). 
Specia, Lucia, Dhwaj Raj, and Marco Turchi. 2010. 
Machine Translation Evaluation Versus Quality 
Estimation. Machine Translation, 24:39?50. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France, July. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Turian, Joseph P., Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and its 
Evaluation. In Machine Translation Summit IX, 
pages 386?393. International Association for Ma-
chine Translation. 
Wang, Mengqiu and Christopher D. Manning. 2012. 
SPEDE: Probabilistic Edit Distance Metrics for 
MT Evaluation, WMT2012, 76-83. 
Wong, Billy T. M. and Chunyu Kit. 2012. Extending 
Machine Translation Evaluation Metrics with Lex-
ical Cohesion to Document Level. Proceedings of 
the 2012 Joint Conference on Empirical 
371
Methods in Natural Language Processing and 
Computational Natural Language Learning, 
pages 1060?1068, Jeju Island, Korea, 12?14 July. 
Association for Computational Linguistics. 
Zhang, Harry. 2004. The Optimality of Naive Bayes. 
Proceedings of the Seventeenth International 
Florida Artificial Intelligence Research Socie-
ty Conference, Miami Beach, Florida, USA. 
AAAI Press. 
372
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
UM-Checker: A Hybrid System for English Grammatical Error Cor-
rection 
 
 
Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng 
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory,  
Department of Computer and Information Science, 
University of Macau, Macau S.A.R., China 
nlp2ct.{vincent, anson}@gmail.com,  
{derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.com 
 
  
 
Abstract 
This paper describes the NLP2CT Grammati-
cal Error Detection and Correction system for 
the CoNLL 2013 shared task, with a focus on 
the errors of article or determiner (ArtOrDet), 
noun number (Nn), preposition (Prep), verb 
form (Vform) and subject-verb agreement 
(SVA). A hybrid model is adopted for this spe-
cial task. The process starts with spell-
checking as a preprocessing step to correct any 
possible erroneous word. We used a Maxi-
mum Entropy classifier together with manual-
ly rule-based filters to detect the grammatical 
errors in English. A language model based on 
the Google N-gram corpus was employed to 
select the best correction candidate from a 
confusion matrix. We also explored a graph-
based label propagation approach to overcome 
the sparsity problem in training the model. Fi-
nally, a number of deterministic rules were 
used to increase the precision and recall. The 
proposed model was evaluated on the test set 
consisting of 50 essays and with about 500 
words in each essay. Our system achieves the 
5
th
 and 3
rd
 F1 scores on official test set among 
all 17 participating teams based on gold-
standard edits before and after revision, re-
spectively.  
1 Introduction 
With the increasing number of people all over 
the world who study English as their second lan-
guage1, grammatical errors in writing often oc-
curs due to cultural diversity, language habits, 
education background, etc. Thus, there is a sub-
stantial and increasing need of using computer 
                                                 
    1 A well-known fact is that the most popular language 
chosen as a first foreign language is English. 
techniques to improve the writing ability for sec-
ond language learners. Grammatical error correc-
tion is the task of automatically detecting and 
correction erroneous word usage and ill-formed 
grammatical constructions in text (Dahlmeier et 
al., 2012). 
In recent decades, this special task has gained 
more attention by some organizations such as the 
Helping Our Own (HOO) challenge (Dale and 
Kilgarriff, 2010; Dale et al, 2012). Although the 
performance of grammatical error correction sys-
tems has been improved, it is still mostly limited 
to dealing with the determiner and preposition 
error types with a very low recall and precision. 
This year, the CoNLL-2013 shared task extends 
to include a more comprehensive list of error 
types, as shown in Table 1. 
To take on this challenge, this paper proposes 
pipe-line architecture in combination with sever-
al error detection and correction models based on 
a hybrid approach. As a preprocessing step we 
firstly employ a spelling correction to correct the 
misspelled words. To correct the grammatical 
errors, a hybrid system is designed that integrat-
ed with Maximum Entropy (ME) classifier, de-
terministic filter and N-gram language model 
scorer, each of which is constructed as an indi-
vidual model. According to the phenomena of 
the problems, we use different combinations of 
the models trained on specific data to tackle the 
corresponding types of errors. For instance, Prep 
and Nn have a strong inter-relation with the 
words (surface) that are preceding and following 
the active word. This can be detected and recov-
ered by using a language model. On the other 
hand, SVA is more complicated and it is more 
effective to determine the mistakes by using the 
linguistic and grammatical rules. The correction
34
Error Type Description Example 
Vform 
Replacement The solution can be obtain (obtained) by using technology. 
Insertion 
However, the world has always beyond our imagination and ? (has) 
never let us down. 
Deletion It also indicates that the economy has been (?) dramatically grown. 
SVA 
Subject-verb-
Agreement 
My brothers is (are) nutritionists. 
ArtOrDet 
Replacement 
The leakage of these (this) confidential information can be a sensitive 
issue to personal, violation of freedom and breakdown of safety. 
Insertion The survey was done by ? (the) United Nations. 
Deletion 
The air cargo of the (?) Valujet plane was on fire after the plane had 
taken off. 
Nn Noun number He receives two letter (letters). 
Prep 
Replacement They work under (in) a conductive environment. 
Insertion 
Definitely, there are point of view that agree ? (with) the technology 
but also the voices of objection. 
Deletion 
Today, the surveillance technology has become almost manifest to (?) 
wherever we go. 
 
Table 1: The error types with descriptions and examples. 
 
components are combined into a pipeline of cor-
rection steps to form an end-to-end correction 
system. Different types of corrections may inter-
act with each other. Therefore, only for each fo-
cus word in a sentence will pass the filter and 
predict by the system. 
Take the sentence for example, ?The patent 
applications do not need to be censored.?, if the 
word ?applications? is changed to ?application? 
(Nn error) by a correction module, then the fol-
lowing auxiliary verb ?do? should be revised to 
?does? (SVA error) accordingly. That is, if a mis-
take is introduced by a component in the prior 
step, subsequent analyses are most likely affect-
ed negatively. To avoid the errors propagated 
into further components, we proposed to deploy 
the analytical (pipelined) components in the or-
der of Nn, ArtOrDet, Vform, SVA and Prep. 
For non-native language learners, over 90% 
usage of prepositions and articles are correctly 
used, which makes the errors very sparse (Ro-
zovskaya and Roth, 2010c) in a text, and about 
10% error is not ?sparse? by the way. This factor 
severely restricts the improvement of data-driven 
systems. Different from the previous methods to 
overcome error sparsity, we explored a graph-
based label propagation method that makes use 
of the prediction on large amount of unlabeled 
data. The predicted data are then used to 
resample our training data. This semi-supervised 
method may fix a skewed label distribution in the 
training set and is helpful to enhance the models.  
The paper is organized as follows. We firstly 
review and discuss the related work. The data 
used to construct the models is described in Sec-
tion 3. Section 4 discusses the proposed model 
based on semi-supervised learning, and the over-
all hybrid system is given in Section 5. The 
methods of grammatical error detection and cor-
rection are detailed in Section 6, followed by an 
evaluation, discussion and a conclusion to end 
the paper. 
2 Related Work 
The issues of grammatical error correction have 
been discussed from different perspectives for 
several decades. In this section, we briefly re-
view some related methods. 
The use of machine learning methods to tackle 
this problem has shown a promising perfor-
mance. These methods are normally created 
based on a large corpus of well-formed native 
English texts (Tetreault and Chodorow 2008; 
Tetreault et al, 2010) or annotated non-native 
data (Gamon, 2010; Han et al, 2010). Although 
the manually error-tagged text is much more ex-
pensive, it has shown improvements over the 
models trained solely on well-formed native text 
(Kochmar et al, 2012). Additionally, both gener-
ative and discriminative classifiers were widely 
used. Among them, Maximum Entropy was gen-
erally used (Rozovskaya and Roth, 2011; 
Sakaguchi et al, 2012; Quan et al, 2012) and 
obtained a good result for preposition and article 
correction using a large feature set. Naive Bayes 
35
were also applied to recognize or correct the er-
rors in speech or texts (Lynch et al, 2012). How-
ever, only using classifiers always cannot give a 
satisfied performance. Thus, grammar rules and 
probabilistic language model can be used as a 
simple but effective assistant for correction of 
spelling (Kantrowitz et al 2003) and grammati-
cal errors (Dahlmeier et al, 2012; Lynch et al, 
2012; Quan et al, 2012; Rozovskaya et al, 
2012). 
3 Data Set 
The training data is the NUS Corpus of Learner 
English (NUCLE) that provided by the National 
University of Singapore (Dahlmeier et al, 2013). 
The NUCLE contains more than one million 
words (1,400 essays) and has been annotated 
with error-tags and correction-labels. There are 
27 categories of errors, with 45,106 errors in to-
tal. In this CoNLL-2013 shared task, five types 
of errors (around 32% of the total errors) are 
concerned. Figure 1 shows the statistics infor-
mation of error types. 
 
 
 
Figure 1. The distribution of different error types in 
the training set. 
 
As the distribution of different errors respects 
the real environment, there is a serious problem 
hidden in it. Roughly estimated, the ratio be-
tween the correct and error classes in NUCLE is 
around 100:1, or even more. The imbalance 
problem may be heavily harmful to machine 
learning methods. Therefore, researchers (Ro-
zovskaya et al, 2012; Dahlmeier et al, 2012) 
provided several approaches such as reducing 
correct instances to deal with error sparsity. In-
stead of downsampling the data, we try to up-
sample error instances. Different from UI system 
(Rozovskaya et al, 2012) which simulates learn-
ers to make mistakes artificially, we propose a 
semi-supervised learning method that makes use 
of a large amount of unlabeled data which is easy 
to collect. In practice, semi-supervised learning 
requires less human effort and gives higher accu-
racy in creating a model.  
4 Error Examples Expansion Using 
Graph-Based Label Propagation  
As mentioned before, the corpus contains a low 
amount of error examples, which results in a 
high sparsity in the label distribution. In reality, 
the balance between the error and correct data is 
crucial for training a robust grammar detection 
models. Our experiment results demonstrate that 
too many correct data lead to unfavorable error 
detection rate. In order to resolve this obstacle, 
this paper introduces to using external data 
sources, i.e., a large amount of easily accessible 
raw texts, to automatically achieve more labeled 
example for training a stronger model. This pa-
per employs transductive graph-based semi-
supervised learning approach. 
4.1 Graph-Based Label Propagation 
Graph-based label propagation is one of the criti-
cal subclasses of SSL. Graph-based label propa-
gation methods have recently shown they can 
outperform the state-of-the-art in several natural 
language processing (NLP) tasks, e.g., POS tag-
ging (Subramanya et al, 2010), knowledge ac-
quisition (Talukdar et al, 2008), shallow seman-
tic parsing for unknown predicate (Das and 
Smith, 2011).  This study uses graph SSL to en-
rich training data, mainly the examples with in-
correct tag, from raw texts.  
This approach constructs a k nearest-neighbor 
(k-nn) similarity graph over the labeled and un-
labeled data in the first step. The vertices in the 
constructed graph consist of all instances (feature 
vector) that occur in labeled and unlabeled text, 
and edge weights between vertices are computed 
using their Euclidean distance. Pairs of vertices 
are connected by weighted edges which encode 
the degree to which they are expected to have the 
same label (Zhu, 2003). In the second step, label 
propagation operates on the constructed graph. 
The primary objective is to propagate labels from 
a few labeled vertices to the unlabeled ones by 
optimizing a loss function based on the con-
straints or properties derived from the graph, e.g. 
smoothness (Zhu et al, 2003; Subramanya and 
Bilmes, 2008; Talukdar et al, 2009), or sparsity 
(Das and Smith, 2012). This paper uses propaga-
tion method (MAD) in (Talukdar et al, 2009).  
Vform
9%
SVA
10%
ArtOrDet
42%
Nn
24%
P ep
15%
36
  
 
Figure 2. Workflow of our proposed system. 
4.2 Implementation 
In this paper, the labeled data is taken from NU-
CLE corpus. They are regarded as the ?seed? 
data, including 93,000 correct and 1,200 incor-
rect instances. The unlabeled data is collected 
from the English side of news magazine corpus 
(LDC2005T10). Based on that, a 5-NN similarity 
graph is constructed. With the graph and the 
properties of the labeled data derived from the 
NUCLE, the MAD algorithm is used to propa-
gate the error-tag (label) from labeled vertices to 
the unlabeled vertices. Afterwards, the unlabeled 
examples with incorrect tag are added into the 
original training data for training. 
5 System Description 
This section describes the details of our system, 
including preprocessing of training set, confusion 
set generating, classifier training and language 
models building. The grammatical error correc-
tion procedure is shown in Figure 2. 
5.1 Preprocessing 
As mentioned in Section 3, there is a large 
amount (68%) of other error types which may 
result in new errors or confuse the system with 
wrong information in correction. In order to 
make the best use of the corpus, it needs to filter 
all errors not covered by the CoNLL 2013 shared 
task, and then generate a separate corpus for each 
error type. Therefore, we recovered other irrele-
vant errors accordingly. For each error type, we 
also recover other 4 types of errors, and then we 
got a pure training data set which only includes 
one error type.  
For the misspelled problem, we used an open 
source toolkit (JMySpell2) which allows us to 
use the dictionaries form OpenOffice. JMySpell 
                                                 
    2 Available at https://kenai.com/projects/jmyspell. 
gives a list of suggestion candidate words, and 
we select the first one to replace the original 
word.  
5.2 Confusion Set Generating 
Confusion sets include the correction candidates 
which are used to modify the wrong places of a 
sentence. We generated a confusion set for each 
type of error correction component.  
The confusion set for Nn, Vform and SVA was 
built on Penn Treebank3. The format can be de-
scribed as that each prototype word follows all 
possible combinations with Part-Of-Speech (POS) 
and variants. For instance, the format of the word 
?look? in confusion set should looks like ?look 
look#VB look#VBP looking#VBG looks#VBZ 
looked#VBN look#NN looks#NNS?. The proto-
type ?look? and POS are the constraints for 
choosing the correct candidate. In order to quick-
ly find the candidates according to each detected 
error place, we indexed the confusion set in Lu-
cene4 which is another open source toolkit with a 
high-performance, full-featured text search en-
gine library. 
For ArtOrDet and Prep, the confusion sets are 
manually created because the possible modifica-
tions are not so many which are discussed in 
Section 6.1 and 6.2. 
5.3 Maximum Entropy Classifier 
The machine learning algorithm we used to train 
the detection models is Maximum Entropy (ME), 
which can classify the data by giving a probabil-
ity distribution. It is similar to multiclass logistic 
regression models, but much more profitable 
with sparse explanatory feature vectors. For ME 
classifier, the feature of text data is suitable for 
training the model, so we choose it as our detec-
tion classifier.  
                                                 
    3 Available at http://www.cis.upenn.edu/~treebank/. 
    4 Available at http://lucene.apache.org/. 
Source
Text
Rule-based 
Filter
ArtOrDet
LM Scorer
Nn
ME
classifier
LM Scorer
Vform
SVA
ME
classifier
Rule-based 
Filter
Rule-based 
Filter
Rule-based 
Filter
Prep
LM Scorer
Hybrid System
Correct
Text
37
We employed Stanford Classifier5 which is a 
Java implementation of maximum entropy 
(Manning & Klein, 2003).  
5.4 N-gram Language Model 
The probabilistic language model is constructed 
on Google Web 1T 5-gram corpus (Brants and 
Franz, 2006) by using the SRILM toolkit 
(Stolcke, 2002). All generated modification can-
didates are scored by it and only candidates that 
strictly increase than a threshold can be kept.  
The normalized language model score is de-
fined as 
1 log Pr( )lmscore ss?
                (1) 
in which s is the corrected sentence and |s| is the 
sentence length in tokens (Dahlmeier et al, 
2012). 
6 Grammatical Error Correction 
6.1 Article and Determiner 
The component for ArtOrDet task integrates with 
the language model and rule-based techniques. 
Language models are constructed to select the 
best candidate from a confusion set of possible 
article choices {a, the, an, ?}, given the pre-
corrected sentence. Each Noun Phrase (NP) in 
the test sentence will be pre-corrected as correc-
tion candidates. However, only using a language 
model to determine the best correction will often 
result in a low precision, because a certain 
amount of correct usages of ArtOrDet are mis-
judged. 
In order to avoid this problem, we proposed a 
voting method based on multiple language mod-
els. We integrated two separate language models: 
one was converted from the large Google corpus 
(general LM) and the other one was constructed 
from a small in-domain corpus (in-domain LM). 
Additionally, the in-domain corpus involves two 
parts. One is the training data which has been 
totally corrected according to the gold answer. 
The other one includes the sentences which are 
similar to the test set. We extracted them from 
some well-formed native English corpora such as 
English News Magazine of LDC2005T106 using 
term frequency-inverse document frequency (TF-
IDF) as the similarity score. Each document Di is 
                                                 
    5 Available at 
http://nlp.stanford.edu/software/classifier.shtml. 
    
6
 Available at http://www.ldc.upenn.edu/Catalog/catalog 
Entry.jsp?catalogId=LDC2005T10. 
represented as a vector (wi1, wi2,?, win), and n is 
the size of the vocabulary. So wij is calculated as 
follows: 
 )log( jijij idftfw ??  (2) 
where tfij is term frequency (TF) of the j-th word 
in the vocabulary in the document Di, and idfj is 
the is the inverse document frequency (IDF) of 
the j-th word calculated. The similarity between 
two sentences is then defined as the cosine of the 
angle between two vectors.  
Each candidate sentence will be scored by 
these two LMs and compared with a threshold. 
Only if both of the LMs agree, the modification 
will be kept. We believe this method could filter 
a lot of wrong modification and improve the pre-
cision. 
6.2 Preposition 
For Prep error type, we used the same method as 
ArtOrDet. The only difference is confusion ma-
trix. Our system corrects the unnecessary, miss-
ing and unwanted errors for the five most fre-
quently prepositions which are in, for, to, of and 
on. While developing our system, we found that 
adding more prepositions did not increase per-
formance in our experiments. Thus the confusion 
set is {in, for, to, of, on, ?}. 
6.3 Noun Number 
A single noun in the sentence that is hard to dis-
tinguish whether it is singular or plural, so we 
treat a noun phrase as a observe subject. Our 
strategy of correcting noun number error is to use 
a filter contains rule-based and machine learning 
method. It can filter a part of nouns that absolute-
ly right, and the rest of nouns will be detected by 
the language model generated by SRILM7. 
The rule-based filter of our system contains 
several criteria. It can detect the noun phrase by 
article, i.e. it can simply find out that the noun is 
singular which with an article of ?a? or ?an?. 
The determiner and cardinal number also will be 
taken into consider by the rule-based model such 
as ?I have three apple.?, then system can find out 
the ?apple? should be ?apples?. The correct noun 
will keep the original one, and the incorrect noun 
will be replaced with a new candidate. 
After the first level filtering by the rules, the 
rest of noun phrases are indeterminacy by system. 
Therefore, we use a ME classifier for further fil-
tering. We use lexical, POS and dependency 
                                                 
    7 http://www.speech.sri.com/projects/srilm/. 
38
parse information as features. The features are 
listed in Table 2.  
In previous steps, most of the error can be de-
tected, but also it may give a lot of wrong sug-
gests, in order to reduce this situation, we use N-
gram language model scorer to evaluate on the 
candidates and choose the highest probability 
one. 
 
Feature Example 
Observer word 
Word (w0) resource 
POS (p0) NN 
First word in NP 
Word (wNP-1st) a 
POS (pNP-1st) DT 
Dependency Relation det 
Previous word before observed word 
Word (w-1) good 
POS (p-1) JJ 
Word after observed word 
Word (w1) and 
POS (p1) CC 
Head word of observed word 
Word (whead) water 
POS (phead) NN 
Dependency relation rcomd 
Word Combination 
w0 + wNP-1st resource + a 
w0 + w-1 resource + good 
w0 + w1 resource + and 
w0 + whead resource + water 
wNP-1st + whead a + water 
POS Combination 
p0 + pNP-1st NN + DT 
p0 + p-1 NN + JJ 
p0 + p1 NN + CC 
p0 + phead NN + NN 
pNP-1st + phead DT + NN 
 
Table 2: Features for Nn and the example: ?An exam-
ple is water which is a good resource and is plentiful.? 
6.4 Verb Form 
Determining the correct form of a verb in Eng-
lish is complex, involving a relatively wide range 
of choices. A verb can have many forms, such as 
base, gerund, preterite, past participle and so on. 
To detect the tense of verb error is much more 
related to the semantics level than syntax level. 
Therefore, it is hard to extract a common feature 
for training model. We chose to separate it into 
several problems and use rule-based model to do 
the Vform correction. 
For auxiliary verbs, there are three categories, 
one is modal verbs (do, can, may, will, might, 
should, must, need and dare), the other is the 
form of ?be? and ?have?. In a verb phrase, nor-
mally modals precede ?have? and ?be?, and 
?have? proceed ?be?, then we can get the order-
ing like this: Modal, Have, Be. Auxiliary verbs 
can incorporate with other verbs, and have dif-
ferent combination. Based on the previous study 
of the core language engine (Alshawi, 1992), we 
define the rules that contain the type of verb, 
which tense of verbs can be used with, and their 
entries in the lexicon. For example: 
 
(can (aux (modal) (vform pres)  (COMPFORM bare)) 
 
This means ?can? is a modal verb, it can be 
used with a verb that in the present tense, when 
?can? used alone with the main verb should as 
complement the base (bare) form. In here, the 
COMPFORM attribute is the entry condition in 
the grammar.  
6.5 Subject-Verb Agreement 
The basic principle of Subject-Verb Agreement 
is singular subjects need singular verbs; plural 
subjects need plural verbs, such as following sen-
tences: 
My brother is a nutritionist. 
My sisters are dancers. 
Therefore, the subject of the sentence is the 
key point. To decide whether the verb is singular 
or plural should look into the context and find 
out the POS of the subject. We utilize the exist-
ing information given by NUCLE to extract the 
subject of the verb. For example, the sentence 
?Statistics show that the number are continuing 
to grow with the existing population explosion.? 
Figure 3 shows the parse tree of this sentence. 
 
Figure 3. Parse tree of the example sentence. 
Root
S1
NP1
VP1
VBP1NNPS
SBAR
IN1
S2
NP2 VP2
?DT2 NN2 VBP2
arenumberthe
that
showStatistics
.
.
?
39
Through Figure 3, the observed words are 
?show? and ?are?, the subjects are ?statistics? 
and ?number? respectively that we can conclude 
?statistics? should use plural verb and ?number? 
should use singular verb ?is? instead of ?are?. 
The other features extracted for training are 
listed in Table 3. 
 
Feature Example 
Observer word 
Word (w0) are 
POS (p0) VBP 
Subject NP 
First word (wNP-1st) the 
POS of first word (pNP-1st) DT 
Head word (wNP-head) number 
POS of head word (pNP-head) NN 
Previous word before observed word 
Word (w-1) number 
POS (p-1) NN 
NP after observed word 
First word (wNPa-1st) the 
POS of first word (pNPa-1st) DT 
Head word (wNPa-head) explosion 
POS of head word (pNPa-head) NN 
Word combination 
w0 + wNP-1st are + the 
w0 + wNP-head are + number 
w0 + w-1 are + number 
w0 + wNPa-1st are + the 
w0 + wNPa-head are + explosion 
POS combination 
p0 + pNP-1st VBP + DT 
p0 + pNP-head VBP + NN 
p0 + p-1 VBP + NN 
p0 + pNPa-1st VBP + DT 
p0 + pNPa-head VBP + NN 
 
Table 3: Features for SVA and the example: ?Statis-
tics show that the number are continuing to grow with 
the existing population explosion.? 
 
The purpose of extracting the noun phrase af-
ter the observed word is in the situation of the 
subject is after the verb, such as ?Where are my 
scissors??, ?scissors? is the subject of this sen-
tence. 
7 Evaluation and Discussion 
The evaluation is provided by the organizer and 
generated by M2 scorer (Dahlmeier & Ng, 2012). 
The result consists of precision, recall and F-
score. Our grammatical error correction system 
has proposed 1,011 edits. The evaluation result 
of our system output for the CoNLL-2013 test 
data is shown in Table 4. 
 
Results Precision Recall F-score 
Before 
Revision 
0.2849 0.1753 0.2170 
After  
Revision 
0.3712 0.2366 0.2890 
 
Table 4: Evaluation result of Precision, Recall and F-
score. 
 
Error Type Error # Correct # % 
ArtOrDet 690 145 21.01 
Nn 396 92 23.23 
Vform 122 8 6.55 
SVA 124 37 29.83 
Prep 311 6 1.93 
 
Table 5: Detail information of evaluation result (Be-
fore Revision). 
 
Error Type Error # Correct # % 
ArtOrDet 725 177 24.42 
Nn 484 132 27.27 
Vform 151 16 10.60 
SVA 138 47 34.06 
Prep 325 9 2.77 
 
Table 6: Detail information of evaluation result (After 
Revision). 
 
The data in table 5 and 6 are the detailed in-
formation for each error type which was calcu-
lated by us, the table 5 is the data before revision, 
and the table 6 is that after revision. Second col-
umn is the amount of the gold edits, and the third 
column is the amount of our correct edits, and 
the last column is the percentage of correct edits. 
We analyzed the results in detail, and found sev-
eral critical reasons of causing low recall. Firstly, 
the five error types are associated relatively, if 
one is modified, it may cause a chain reaction, 
such as the article will affect the noun number, 
and the noun number will cause the SVA errors. 
Some Nn errors still cannot be detected or given 
a wrong correction by our system, which de-
creases the precision and recall of SVA. Another 
reason is our system does not perform well in 
Vform and Prep error correction. In our output, 
just a few errors have been revised. This means 
the quantity of correction rules is not enough that 
cannot cover all the linguistic phenomena. For 
40
instance, the situation of missing verb or unnec-
essary verb cannot be detected. On the other 
hand, the hybrid method of our system has fil-
tered some wrong suggestion candidates that im-
prove the precision. 
8 Conclusion 
We have presented the hybrid system for English 
grammatical error correction. It achieves a 28.9% 
F1-score on the official test set. We believe that if 
we find more appropriate features, our system 
can still be improved and achieve a better per-
formance. 
 
Acknowledgments 
 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
MYRG076(Y1-L2)-FST13-WF. The authors also 
wish to thank the anonymous reviewers for many 
helpful comments as well as Liangye He, Yuchu 
Lin and Jiaji Zhou who give us a lot of help. 
References  
Hiyan Alshawi. 1992. The core language engine. The 
MIT Press. 
Jon Louis Bentley. 1980. Multidimensional divide-
and-conquer. Communications of the ACM, 
23:214?229. 
Alina Beygelzimer, Sham Kakade, and John Lang-
ford. 2006. Cover trees for nearest neighbor. In: 
Proceedings of the 23rd International Confer-
ence on Machine Learning, pp. 97?104. 
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1. Linguistic Data Consortium, 
Philadelohia, PA. 
Olivier Chapelle, Bernhard Sch?lkopf, Alexander 
Zien, and others. 2006. Semi-supervised learning. 
MIT press Cambridge. 
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng 
Ng. 2012. NUS at the HOO 2012 Shared Task. In: 
Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 216?224. 
Daniel Dahlmeier & Hwee Tou Ng, and Siew Mei 
Wu (2013). Building a Large Annotated Corpus of 
Learner English: The NUS Corpus of Learner Eng-
lish. To appear in Proceedings of the 8th Work-
shop on Innovative Use of NLP for Building 
Educational Applications (BEA 2013). Atlanta, 
Georgia, USA. 
Daniel Dahlmeier, and Hwee Tou Ng (2012). Better 
Evaluation for Grammatical Error Correction. 
Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics (NAACL 2012), 
pp. 568 ? 572. 
Robert Dale, Ilya Anisimoff, and George Narroway. 
2012. HOO 2012: A report on the preposition and 
determiner error correction shared task. In: Pro-
ceedings of the Seventh Workshop on Building 
Educational Applications Using NLP, pp. 54?
62. 
Robert Dale and Adam Kilgarriff. 2011. Helping our 
own: The HOO 2011 pilot shared task. In: 
Proceedings of the 13th European Workshop 
on Natural Language Generation, pp. 242?249. 
Dipanjan Das and Noah A. Smith 2012. Graph-based 
lexicon expansion with sparsity-inducing penalties. 
In: Proceedings of the 2012 Conference of the 
North American Chapter of the Association 
for Computational Linguistics: Human Lan-
guage Technologies, pp. 677?687. 
Michael Gamon. 2010. Using mostly native data to 
correct errors in learners? writing: a meta-classifier 
approach. In: Human Language Technologies: 
The 2010 Annual Conference of the North 
American Chapter of the Association for 
Computational Linguistics, pp. 163?171. 
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing 
stars when there aren?t many stars: graph-based 
semi-supervised learning for sentiment categoriza-
tion. In: Proceedings of the First Workshop on 
Graph Based Methods for Natural Language 
Processing, pp. 45?52. 
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner 
corpus to develop an ESL/EFL error correction 
system. In: Proceedings of LREC, pp. 763?770. 
Mark Kantrowitz. 2003. Method and apparatus for 
analyzing affect and emotion in text. U.S. Patent 
No. 6,622,140. 
Ekaterina Kochmar. 2011. Identification of a writer?s 
native language by error analysis. Master?s thesis, 
University of Cambridge. 
Gerard Lynch, Erwan Moreau, and Carl Vogel. 2012. 
A Naive Bayes classifier for automatic correction 
of preposition and determiner errors in ESL text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 257?262. 
41
Christopher Manning and Dan Klein. 2003. Optimiza-
tion, Maxent Models, and Conditional Estimation 
without Magic. Tutorial at HLT-NAACL 2003 
and ACL 2003. 
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian 
Hadiwinoto, and Joel Tetreault (2013). The 
CoNLL-2013 Shared Task on Grammatical Error 
Correction. To appear in Proceedings of the Sev-
enteenth Conference on Computational Natu-
ral Language Learning. 
Li Quan, Oleksandr Kolomiyets, and Marie-Francine 
Moens. 2012. KU Leuven at HOO-2012: a hybrid 
approach to detection and correction of determiner 
and preposition errors in non-native English text. 
In: Proceedings of the Seventh Workshop on 
Building Educational Applications Using 
NLP, pp. 263?271. 
Juan Ramos. 2003. Using tf-idf to determine word 
relevance in document queries. In: Proceedings of 
the First Instructional Conference on Machine 
Learning. 
Alla Rozovskaya and Dan Roth. 2010. Training para-
digms for correcting errors in grammar and usage. 
In: Human Language Technologies: The 2010 
Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics, pp. 154?162. 
Alla Rozovskaya, Mark Sammons, and Dan Roth. 
2012. The UI system in the HOO 2012 shared task 
on error correction. In: Proceedings of the Sev-
enth Workshop on Building Educational Ap-
plications Using NLP, pp. 272?280. 
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, 
Lis Kanashiro, Tomoya Mizumoto, Mamoru Ko-
machi, and Yuji Matsumoto. 2012. NAIST at the 
HOO 2012 Shared Task. In: Proceedings of the 
Seventh Workshop on Building Educational 
Applications Using NLP, pp. 281?288. 
Andreas Stolcke and others. 2002. SRILM-an exten-
sible language modeling toolkit. In: Proceedings 
of the International Conference on Spoken 
Language Processing, pp. 901?904. 
Partha Pratim Talukdar and Koby Crammer. 2009. 
New regularized algorithms for transductive learn-
ing. In: Machine Learning and Knowledge 
Discovery in Databases. Springer, pp. 442?457. 
Joel Tetreault, Jennifer Foster, and Martin Chodorow. 
2010. Using parse features for preposition selection 
and error detection. In: Proceedings of the Acl 
2010 Conference Short Papers, pp. 353?358. 
Joel R. Tetreault and Martin Chodorow. 2008. The 
ups and downs of preposition error detection in 
ESL writing. In: Proceedings of the 22nd Inter-
national Conference on Computational Lin-
guistics Volume 1, pp. 865?872. 
 
42
