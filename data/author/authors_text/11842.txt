Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 13?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploring Topic Continuation Follow-up Questions using Machine Learning
Manuel Kirschner
KRDB Center
Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
kirschner@inf.unibz.it
Raffaella Bernardi
KRDB Center
Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
bernardi@inf.unibz.it
Abstract
Some of the Follow-Up Questions (FU Q) that
an Interactive Question Answering (IQA) sys-
tem receives are not topic shifts, but rather
continuations of the previous topic. In this pa-
per, we propose an empirical framework to ex-
plore such questions, with two related goals in
mind: (1) modeling the different relations that
hold between the FU Q?s answer and either the
FU Q or the preceding dialogue, and (2) show-
ing how this model can be used to identify the
correct answer among several answer candi-
dates. For both cases, we use Logistic Regres-
sion Models that we learn from real IQA data
collected through a live system. We show that
by adding dialogue context features and fea-
tures based on sequences of domain-specific
actions that represent the questions and an-
swers, we obtain important additional predic-
tors for the model, and improve the accuracy
with which our system finds correct answers.
1 Introduction
Interactive Question Answering (IQA) can be de-
scribed as a fusion of the QA paradigm with di-
alogue system capabilities. While classical QA is
concerned with questions posed in isolation, its in-
teractive variant is intended to support the user in
finding the correct answer via natural-language dia-
logue. In an IQA setting, both the system and the
user can pose Follow-Up Questions (FU Q). In the
second case, whenever an IQA system receives an
additional user question (note that this is what we
call a Follow-Up Question throughout this work), it
can either interpret it as being thematically related to
a previous dialogue segment (topic continuation), or
as a shift to some new, unrelated topic (topic shift).
A definition of thematic relatedness of FU Qs might
rely on the elements of the attentional state, i.e., on
the objects, properties and relations that are salient
before and after processing the user question. Topic
continuation FU Qs should be interpreted within the
context, whereas topic shift FU Qs have to be treated
as first questions and can thus be processed with
standard QA technologies. Therefore, a first task
in IQA is to detect whether a FU Q is a topic shift or
a topic continuation (Yang et al, 2006).
To help answering topic continuation FU Qs, an
IQA system would need to fuse the FU Q with cer-
tain information from the dialogue context (cf. (van
Schooten et al, 2009)). Thus, a second task in IQA
is to understand which turns in the dialogue context
are possible locations of such information, and ex-
actly what kind of information should be considered.
Knowing that a FU Q concerns the same topic as the
previous question or answer, we thus want to study
in more detail the way the informational content of
questions and answers evolves before/after the FU Q
is asked. A model of these so-called informational
transitions would provide insights into what a user is
likely to ask about next in natural coherent human-
machine dialogue.
In order to tackle any of the two IQA tasks men-
tioned above we need IQA dialogues. Most current
work on IQA uses the TREC QA data; the TREC
QA tracks in 2001 and 2004 included series of con-
text questions, where FU Qs always depended on the
context set by an earlier question from the same se-
ries. However, these data were constructed artifi-
cially and are not representative of actual dialogues
from an IQA system (for instance, system answers
are not considered at all). Real IQA data yield chal-
13
lenges for an automatic processing approach (Yang
et al, 2006). Our work is based on collecting and
analyzing IQA dialogues from users of a deployed
system.
In this paper, we address the second task intro-
duced above, namely the study of common relations
between the answer to a topic continuation FU Q and
other turns in the dialogue context. Our collected di-
alogue data are from the ?library help desk? domain.
In many of the dialogues, library users request in-
formation about a specific library-related action; we
are thus dealing with task-oriented dialogues. This
work is based on two hypotheses regarding relations
holding between the FU Q?s answer and the dialogue
context. For studying such relations, we want to ex-
plore the usefulness of (1) a representation of the
library-related action underlying questions and an-
swers, and (2) a representation of the dialogue con-
text of the FU Q.
2 Background
In order to understand what part of the history of
the dialogue is important for processing FU Qs,
significant results come from Wizard-of-Oz stud-
ies, like (Dahlba?ck and Jo?nsson, 1989; Bertomeu
et al, 2006; Kirschner and Bernardi, 2007), from
which it seems that the immediate linguistic context
(i.e., the last user initiative plus the last system re-
sponse) provides the most information for resolving
any context-dependency of the FU Qs. These studies
analyzed one particular case of topic continuation
FU Q, namely those questions containing reference-
related discourse phenomena (ellipsis, definite de-
scription or anaphoric pronoun); we assume that the
results could be extended to fully specified ques-
tions, too.
Insights about the informational transitions within
a dialogue come from Natural Language Genera-
tion research. (McCoy and Cheng, 1991) provide
a list of informational transitions (they call them fo-
cus shifts) that we can interpret as transitions based
on certain thematic relations. Depending on the con-
versation?s current focus type, they list specific focus
shift candidates, i.e., the items that should get focus
as a coherent conversation moves along. Since we
are interested in methods for interpreting FU Qs au-
tomatically, we decided to restrict ourselves to use
Node type Informational transition targets
Action Actor, object, etc., of the action ?
any participant (Fillmore) role; pur-
pose (goal) of action, next action in
some sequence, subactions, special-
izations of the action
Table 1: Possible informational transition targets for ?ac-
tion? node type (McCoy and Cheng, 1991)
only the ?action? focus type to represent the focus
of questions and answers in IQA dialogues. We con-
jecture that actions form a suitable and robust basis
for describing the (informational) meaning of utter-
ances in our class of task-based ?help desk? IQA di-
alogues. Table 1 shows the focus shift candidates
for a current focus of type ?action?. In this work
we concentrate on the informational transitions in-
volving two actions (i.e., including one of the focus
targets listed in bold face in the table).
3 Exploring topic continuation FU Qs
using Machine Learning
We base our study of topic continuation FU Qs on
the two main results described in Section 2: We
study snippets of dialogues consisting of four turns,
viz. a user question (Q?1), the corresponding sys-
tem answer (A?1), the FU Q and its system answer
(A0); we use Logistic Regression Models to learn
from these snippets (1) which informational (action-
action) transitions hold between A0 and the FU Q
or the preceding dialogue, and (2) how to predict
whether a specific answer candidate A0 is correct for
a given dialogue snippet.
3.1 Machine learning framework: Logistic
Regression
Logistic regression models (Agresti, 2002) are gen-
eralized linear models that describe the relationship
between features (predictors) and a binary outcome
(in our case: answer correctness). We estimate the
model parameters (the beta coefficients ?1, . . . , ?k)
that represent the contribution of each feature to the
total answer correctness score using maximum like-
lihood estimation. Note that there is a close rela-
tionship to Maximum Entropy models, which have
performed well in many tasks. A major advantage
of using logistic regression as a supervised machine
14
learning framework (as opposed to other, possibly
better performing approaches) is that the learned co-
efficients are easy to interpret. The logistic regres-
sion equation which predicts the probability for a
particular answer candidate A0 being correct, de-
pending on the learned intercept ?0, the other beta
coefficients and the feature values x1, . . . , xk (which
themselves depend on a combination of Q?1, A?1,
FU Q or A0) is:
Prob{answerCorrect} = 11 + exp(?X??) , where
X?? = ?0 + (?1x1 + . . .+ ?kxk)
3.2 Dialogue data collection
We have been collecting English human-computer
dialogues using BoB, an IQA system which is pub-
licly accessible on the Library?s web-site of our
university1. We see the availability of dialogue
data from genuinely motivated visitors of the library
web-site as an interesting detail of our approach; our
data are less constrained and potentially more dif-
ficult to interpret than synthesized dialogues (e.g.,
TREC context track data), but should on the other
hand provide insights into the structure of actual
IQA dialogues that IQA systems might encounter.
We designed BoB as a simple chatbot-inspired ap-
plication that robustly matches user questions using
regular expression-based question patterns, and re-
turns an associated canned-text answer from a repos-
itory of 529. The question patterns and answers
have been developed by a team of librarians, and
cover a wide range of library information topics,
e.g., opening time, lending procedures and different
library services. In the context of this work, we use
BoB merely as a device for collecting real human-
computer IQA dialogues.
As a preliminary step towards automatically mod-
eling action-based informational transitions trig-
gered by FU Qs, we annotated each of the 529 an-
swers in our IQA system?s repository with the ?li-
brary action? that we considered to best represent
its (informational) meaning. For this, we had de-
vised a (flat) list of 25 library-related actions by an-
alyzing the answer repository (e.g.: access, borrow,
change, deliver). We also added synonymous verbs
1www.unibz.it/library
to our action list, like ?obtain? for ?borrow?. If we
did not find any action to represent a system an-
swer, we assigned it a special ?generic-information?
tag, e.g. for answers to questions like ?What are the
opening times??.
We base our current study on the dialogues col-
lected during the first four months of the IQA sys-
tem being accessible via the Library?s web site. Af-
ter a first pass of manually filtering out dialogues
that consisted only of a single question, or where the
question topics were only non-library-related, the
collected corpus consists of 948 user questions (first
or FU Qs) in 262 dialogue sessions (i.e., from differ-
ent web sessions). We hand-annotated the user FU
Qs in these dialogues as either ?topic continuation?
(248 questions), or ?topic shift? (150 questions).
The remaining FU Qs are user replies to system-
initiative clarification questions, which we do not
consider here. For each user question, we marked
whether the answer given by the IQA system was
correct; in the case of wrong answers, we asked our
library domain experts to provide the correct answer
that BoB should have returned. However, we only
corrected the system answer in those cases where
the user did not ask a further FU Q afterwards, as
we must not change on-going dialogues.
To get the actual training/test data, we had to fur-
ther constrain the set of 248 topic continuation FU
Qs. We removed all FU Qs that immediately follow
a system answer that we considered incorrect; this is
because any further FU Q is then uttered in a situa-
tion where the user is trying to react to the problem-
atic answer, which clearly influences the topic of the
FU Q. Of the then remaining 76 FU Qs, we keep the
following representation of the dialogue context: the
previous user question Q?1 and the previous system
answer A?1. We also keep the FU Q itself, and its
corresponding correct answer A0.
Finally, we automatically annotated each question
with one or more action tags. This was done by sim-
ply searching the stemmed question string for any
verb stem from our list of 25 actions (or one of their
synonyms); if no action stem is found, we assigned
the ?generic-information? tag to the question. Note
that this simple action detection algorithm for ques-
tions fails in case of context-dependent questions
where the verb is elided or if the question contains
still unknown action synonyms.
15
3.3 Features
In the machine learning framework introduced
above, the model is intended to predict the correct-
ness of a given system answer candidate, harnessing
information from the local dialogue context: Q?1,
A?1, FU Q and the particular answer candidate A0.
We now introduce different features that relate A0 to
either the FU Q or some other preceding turn of the
dialogue. The features describe specific aspects of
how the answer candidate relates to the current dia-
logue. Note that we do not list features relating Q?1
and A0, since our experiments showed no evidence
for including them in our models.
tfIdfSimilarityQA, tfIdfSimilarityAA: TF/IDF-
based proximity scores (ranging from 0 to 1) be-
tween two strings, namely FU Q and A0, or A?1
and A0, respectively. Based on vector similarity (us-
ing the cosine measure of angular similarity) over
dampened and discriminatively weighted term fre-
quencies. Definition of the TF/IDF distance: two
strings are more similar if they contain many of the
same tokens with the same relative number of occur-
rences of each. Tokens are weighted more heavily if
they occur in few documents2, hence we used a sub-
set of the UK English version of the Web-as-Corpus
data3 to train the IDF scores.
Features based on action sequences. To describe
the action-related informational transitions we ob-
serve between the FU Q and A0 and between A?1
and A0, we use two sets of features, both of which
are based on hand-annotated actions for answers
and automatically assigned actions for questions.
actionContinuityQA, actionContinuityAA: sim-
ple binary features indicating whether the same li-
brary action (or one of its synonyms) was identi-
fied between the FU Q and A0, or A?1 and A0, re-
spectively. lmProbQA, lmProbAA: encode Statis-
tical Language Model probabilities for action tag se-
quences, i.e., the probability for A0 having a certain
action, given the action associated with FU Q, or the
action of A?1, respectively. The underlying Statis-
tical Language Models are probability distributions
2Cf. Alias-i?s LingPipe documentation http:
//alias-i.com/lingpipe/demos/tutorial/
stringCompare/read-me.html
3http://wacky.sslmit.unibo.it
over action-action sequences that reflect how likely
certain action sequences occur in our IQA dialogues,
thus capturing properties of salient action sequences.
More technically, we use Witten-Bell smoothed 2-
gram statistical language models, which we trained
on our action-tagged FU Q data.
4 Results
For the evaluation of the logistic regression model,
we proceed as follows. Applying a cross-validation
scheme, we split our 76 FU Q training examples
randomly into five non-intersecting partitions of 15
(or 16) FU Q (with corresponding Q?1, A?1, and
correct A0) each. To train the logistic regression
model, we need training data consisting of a vec-
tor of independent variables (the various feature val-
ues), along with the binary dependent variable, i.e.,
?answer correct? or ?answer false?. We generate
these training data by ?multiplying out? each train-
ing partition?s 61 FU Qs (76 minus the held-out test
set of 15) with all 529 answer candidates; for each
FU Q dialogue snippet used for training, this results
in one positive training example (where A0 is the 1
correct out 529 answer candidates), and 528 nega-
tive training examples (for all other answer candi-
dates).
For each of the five training/test partitions, we
train a different model. We then evaluate each of
these models on their corresponding held-out test
set. Following the cross-validation idea through, we
also train separate Statistical Language Models on
sequences of action tags for each of the five training
splits; this ensures that the language model proba-
bilities were never trained on test data. We perform
the evaluation in terms of the mean rank that the cor-
rect answer A0 is assigned after ranking all 529 an-
swer candidates (by evaluating the logistic regres-
sion equation to yield answer scores).
In the following, we give details of different lo-
gistic regression models we experimented with. Ini-
tially, we chose a subset from the list of features
introduced above. Our goal was to retain as few
features as needed to explore our two hypotheses,
i.e., whether we can make use of (1) a representa-
tion of the FU Q?s underlying library action, and/or
(2) a representation of the immediate dialogue con-
text. By dropping uninformative features, the result-
16
ing models become simpler and easier to interpret.
With this goal in mind, we applied a fast backwards
elimination routine that drops uninformative predic-
tors (cf. (Baayen, 2008, p.204)) on the five training
data splits. In all five splits, both TF/IDF features
turned out to be important predictors; in four of the
splits, also lmProbQA was retained. lmProbAA was
dropped as superfluous in all but two splits, and ac-
tionSimilarityAA was retained only in one. With
these results, the set of features we retain for our
modeling experiments is: tfIdfSimilarityQA, tfIdf-
SimilarityAA and lmProbQA.
?Complete? model: tfIdfSimilarityQA, tfIdfSim-
ilarityAA and lmProbQA We estimated logistic
regression models on the five cross evaluation train-
ing sets using all three features as predictors. Table 2
shows the mean ranks of the correct answer for the
five evaluation runs, and an overall mean rank with
the average across the five splits.
To illustrate the contribution of each of the three
predictors towards the score of an answer candi-
date, we provide the (relevant linear part of) the
learned logistic regression equation for the ?com-
plete? model (trained on split 1 of the data). Note
that the ?answer ranker? evaluates this equation to
get a score for an answer candidate A0.
X?? = ?8.4 + (9.5 ? tfIdfSimilarityQA +
4.6 ? tfIdfSimilarityAA +
1.7 ? lmProbQA)
Reduced model 1: No representation of dialogue
context Only the features concerning the FU Q
and the answer A0 (tfIdfSimilarityQA, lmProbQA)
are used as predictors in building the logistic re-
gression model. The result is a model that treats
every FU Q as a stand-alone question. Across the
five models, the coefficient for tfIdfSimilarityQA is
roughly five times the size of that for lmProbQA.
Reduced model 2: No action sequences We
keep only the two TF/IDF features (tfIdfSimilari-
tyQA, tfIdfSimilarityAA). This model thus does not
use any features that depend on human annotation,
but only fully automatic features. The coefficient
learned for tfIdfSimilarityQA is generally twice as
large as that for tfIdfSimilarityAA.
Reduced model 3: No dialogue context, no action
sequences Considered as a baseline, this model
uses a single feature (tfIdfSimilarityQA) to predict
answer correctness, favoring those answer candi-
dates that have the highest lexical similarity wrt. the
FU Q.
5 Discussion
In order to better understand the relatively high
mean ranks of the correct answer candidates across
Table 2, we scrutinized the results of the answer
ranker (based on all tests on the ?complete? model).
The distribution of the ranks of correct answers is
clearly skewed; in around half of the 76 cases, the
correct answer was actually ranked among the top
20 of the 529 answer candidates. However, the mean
correct rank deteriorates badly due to the lowest-
ranking third of cases. Analyzing these lowest-
ranking cases, it appears that they are often instances
of two sub-classes of topic continuation FU Qs: (i)
the FU Q is context-dependent, i.e., underspecified
or exhibiting reference-related discourse phenom-
ena; (ii) the FU Q is a slight variation of the pre-
vious question (e.g. only the wh-phrase changes, or
only the object changes). This error analysis seems
to suggest that it should be worthwhile to distin-
guish between sub-classes of topic-continuation FU
Qs, and to improve specifically how answers for the
?difficult? sub-classes are ranked.
The relatively high mean ranks are also due to the
fact that in our approach of acquiring dialogue data,
for each FU Q we marked only one answer from the
whole repository as ?correct?. Again for the ?com-
plete? model, we checked the top 20 answer can-
didates that ranked higher than the actual ?correct?
one. We found that in over half of the cases an an-
swer that could be considered correct was among the
top 20.
Looking at the ranking results across the differ-
ent models in Table 2, the fact that the ?complete?
model seems to outperform each of the three re-
duced models (although no statistical significance
could be attained from comparing the rank num-
bers) confirms our two hypotheses proposed earlier.
Firstly, identifying the underlying actions of ques-
tions/answers and modeling action-based sequences
yield important information for identifying correct
17
Reduced m. 3 Reduced m. 2 Reduced m. 1 Complete model
Predictors tfIdfSimilarityQA tfIdfSimilarityQA, tfIdfSimilarityQA, tfIdfSimilarityQA,
in model tfIdfSimilarityAA tfIdfSimilarityAA,
lmProbQA lmProbQA
Split 1 141.2 108.4 112.5 96.2
Split 2 102.7 97.4 53.8 57.7
Split 3 56.7 63.7 50.5 52.7
Split 4 40.5 26.2 37.9 35.7
Split 5 153.1 105.3 129.6 89.1
Mean 98.8 80.2 76.7 66.3
Table 2: Mean ranks of correct A0 out of 529 answer candidates, across models and training/test splits
answers to topic continuation FU Qs. Secondly, as
for the role of the immediate dialogue context for
providing additional clues for identifying good an-
swers to FU Qs, our data show that a high lexical
similarity score between A?1 and A0 indicates a cor-
rect answer candidate. While (Yang et al, 2006)
point out the importance of Q?1 to provide context
information, in our experiments it was generally su-
perseded by A?1.
As for the two features relating the underlying
actions of A?1 and A0 (actionContinuityAA, lm-
ProbAA), the picture seems less clear; in our current
modeling experiments, we had not enough evidence
to keep these features. However, we plan to explore
the underlying idea of action-action sequences in the
future, and conjecture that such information should
come into its own for context-dependent FU Qs.
6 Future work
Besides annotating and using more dialogue data as
more people talk to our IQA system, we plan to
implement a state-of-the-art topic-shift detection al-
gorithm as proposed in (Yang et al, 2006), train-
ing and testing it on our own FU Q data. We will
attempt to improve this system by adding action-
based features, and then extend it to distinguish
three classes: topic shifts, (topic continuation) FU
Qs that are fully specified, and (topic continuation)
context-dependent FU Qs. We then plan to build
dedicated logistic regression models for the differ-
ent sub-classes of topic continuation FU Qs. If each
model uses a specific set of predictors, we hope to
improve the overall rank of correct answers across
the different classes of FU Qs. Also, from compar-
ing the different models, we are interested in study-
ing the specific properties of different FU Q types.
References
[Agresti2002] Alan Agresti. 2002. Categorical Data
Analysis. Wiley-Interscience, New York.
[Baayen2008] R. Harald Baayen. 2008. Analyzing Lin-
guistic Data. Cambridge University Press.
[Bertomeu et al2006] Nu?ria Bertomeu, Hans Uszkoreit,
Anette Frank, Hans-Ulrich Krieger, and Brigitte Jo?rg.
2006. Contextual phenomena and thematic relations
in database QA dialogues: results from a wizard-of-oz
experiment. In Proc. of the Interactive Question An-
swering Workshop at HLT-NAACL 2006, pages 1?8,
New York, NY.
[Dahlba?ck and Jo?nsson1989] Nils Dahlba?ck and Arne
Jo?nsson. 1989. Empirical studies of discourse repre-
sentations for natural language interfaces. In Proc. of
the 4th Conference of the European Chapter of the
ACL (EACL?89), pages 291?298, Manchester, UK.
[Kirschner and Bernardi2007] Manuel Kirschner and
Raffaella Bernardi. 2007. An empirical view on
iqa follow-up questions. In Proc. of the 8th SIGdial
Workshop on Discourse and Dialogue, Antwerp,
Belgium.
[McCoy and Cheng1991] Kathleen F. McCoy and Jean-
nette Cheng. 1991. Focus of attention: Constraining
what can be said next. In Cecile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 103?124. Kluwer Aca-
demic Publishers, Norwell, MA.
[van Schooten et al2009] Boris van Schooten, R. op den
Akker, R. Rosset, O. Galibert, A. Max, and G. Illouz.
2009. Follow-up question handling in the IMIX and
Ritel systems: A comparative study. Journal of Natu-
ral Language Engineering, 15(1):97?118.
[Yang et al2006] Fan Yang, Junlan Feng, and Giuseppe
Di Fabbrizio. 2006. A data driven approach to rele-
vancy recognition for contextual question answering.
In Proc. of the Interactive Question Answering Work-
shop at HLT-NAACL 2006, pages 33?40, New York
City, NY.
18
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 25?32
Manchester, August 2008
Context Modeling for IQA: The Role of Tasks and Entities
Raffaella Bernardi and Manuel Kirschner
KRDB, Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
{bernardi, kirschner}@inf.unibz.it
Abstract
In a realistic Interactive Question Answer-
ing (IQA) setting, users frequently ask
follow-up questions. By modeling how the
questions? focus evolves in IQA dialogues,
we want to describe what makes a partic-
ular follow-up question salient. We intro-
duce a new focus model, and describe an
implementation of an IQA system that we
use for exploring our theory. To learn prop-
erties of salient focus transitions from data,
we use logistic regression models that we
validate on the basis of predicted answer
correctness.
1 Questions within a Context
Question Answering (QA) systems have reached a
high level of performance within the scenario orig-
inally described in the TREC competitions, and
are ready to tackle new challenges as shown by
the new tracks proposed in recent instantiations
(Voorhees, 2004). To answer these challenges, at-
tention is moving towards adding semantic infor-
mation at different levels. Our work is about con-
text modeling for Interactive Question Answering
(IQA) systems. Our research hypothesis is that a)
knowledge about the dialogue history, and b) lexi-
cal knowledge about semantic arguments improve
an IQA system?s ability to answer follow-up ques-
tions. In this paper we use logistic regression mod-
eling to verify our claims and evaluate how the per-
formance of our Q?A mapping algorithm varies
based on whether such knowledge is taken into ac-
count.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Actual IQA dialogues often exhibit ?context-
dependent? follow-up questions (FU Qs) contain-
ing anaphoric devices, like Q2 below. Such ques-
tions are potentially difficult to process by means
of standard QA techniques, and it is for these cases
that we claim that predicting the FU question?s fo-
cus (here, the entity ?library card?) will help a sys-
tem find the correct answer (cf. Sec. 6 for empirical
backup).
Q1: Can high-school students use the library?
A1: Yes, if they got a library card.
Q2: So, how do I get it?
Following (Stede and Schlangen, 2004), we re-
fer to the type of IQA dialogues we are studying
as ?information-seeking chat?, and conjecture that
this kind of dialogue can be handled by means of a
simple model of discourse structure. Our assump-
tion is that in general the user engages in a coherent
dialogue with the system. As proposed in (Ahren-
berg et al, 1995), we model the dialogues in terms
of pairs of initiatives (questions) and responses
(answers), ignoring other intentional acts.
The approach we adopt aims at answering the
following questions: (a) In what way does infor-
mation about the previous user questions and pre-
vious system answers help in predicting the next
FU Q? (b) Does the performance of an IQA sys-
tem improve if it has structure/history-based infor-
mation? (c) Which is the role that each part of this
information plays for determining the correct an-
swer to a FU Q?
This paper is structured as follows. Section 2
gives an overview of some theories of focus used in
dialogue and IQA. Section 3 then gives a detailed
account of our theory, explaining what a question
can focus on, and what patterns of focus change
we expect a FU Q will trigger. Hence, this first
25
part answers our question (a) above. We then move
to more applied issues in Sec. 4, where we show
how questions and answers were annotated with
focus information. The next Section 5 explains the
Q?A algorithm we use to test our theory so as to
answer (b), while Section 6 covers the logistic re-
gression models with which we learn optimal val-
ues for the algorithm from data, addressing ques-
tion (c).
2 Coherence in IQA dialogues
In the area of Discourse processing, much work
has been devoted to formulating rules that account
for the coherence of dialogues. This coherence
can often be defined in terms of focus and focus
shifts. In the following, we adopt the definition
from (Lec?uche et al, 1999): focus stands for the
?set of all the things to which participants in a di-
alogue are attending to at a certain point in a dia-
logue?.
1
In general, all theories of dialogue focus
considered by Lec?uche et al claim that the focus
changes according to some specific and well de-
fined patterns, following the rules proposed by the
respective theory. The main difference between
these theories lies in how these rules are formu-
lated.
A major distinguishing feature of different fo-
cus theories has been the question whether they ad-
dress global or local focus. While the latter explain
coherence between consecutive sentences, the for-
mer are concerned with how larger parts of the di-
alogue can be coherent. We claim that in ?infor-
mation seeking dialogue? this distinction is moot,
and the two kinds of foci collapse into one. Fur-
thermore, our empirical investigation shows that it
suffices to consider a rather short history of the di-
alogue, i.e. the previous user question and previous
system answer, when looking for relations between
previous dialogue and a FU Q.
Salient transitions between two consecutive
questions are defined in (Chai and Jin, 2004) un-
der the name of ?informational transitions?. The
authors aim to describe how the topic within a di-
1
This definition is in line with how focus has been used in
Computational Linguistics and Artificial Intelligence (hence,
?AI focus?), originating in the work of Grosz and Sidner on
discourse entity salience. We follow Lec?uche et al in that
focused elements could also be actions/tasks. We see the most
salient focused element (corresponding to the ?Backward-
looking center? in Centering Theory) as the topic of the ut-
terance. Accordingly, in the following we will use the terms
focus and topic interchangeably; cf. (Vallduvi, 1990) for a sur-
vey of these rather overloaded terms.
alogue evolves. They take ?entities? and ?activi-
ties? as the main possible focus of a dialogue. A
FU Q can be used to ask (i) a similar question as
the previous one but with different constraints or
different participants (topic extension); (ii) a ques-
tion concerning a different aspect of the same topic
(topic exploration); (iii) a question concerning a
related activity or a related entity (topic shift). We
take this analysis as our starting point, extend it
and propose an algorithm to automatically detect
the kind of focus transition a user performs when
asking a FU Q, and evaluate our extended theory
with real dialogue data. Following (Bertomeu et
al., 2006) we consider also the role of the system
answer, and we analyze the thematic relations be-
tween the current question and previous question,
and the current question and previous answer. Un-
like (Bertomeu et al, 2006), we attempt to learn a
model of naturally occurring thematic relations in
relatively unconstrained IQA dialogues.
3 Preliminary Observations
3.1 What ?things? do users focus on?
For all forthcoming examples of dialogues, ques-
tions and answers, we will base our discussion
on an actual prototype IQA system we have been
developing; this system is supposed to provide
library-related information in a university library
setting.
In the dialogues collected via an earlier Wizard-
of-Oz (WoZ) experiment (Kirschner and Bernardi,
2007), we observed that users either seem to
have some specific library-related task (action, e.g.
?search?) in mind that they want to ask the system
about, or they want to retrieve information on some
specific entity (e.g., ?guided tour?). People tend
to use FU Qs to ?zoom into? (i.e., find out more
about) either of the two. In line with this analysis,
the focus of a FU Q might move from the task (ac-
tion/verb) to the entities that are possible fillers of
the verb?s semantic argument slots.
Based on these simple observations, we pro-
pose a task/entity-based model for describing the
focus of questions and answers in our IQA set-
ting. Our theory of focus structure is related to the
task-based theory of (Grosz, 1977). Tasks corre-
spond to verbs, which are inherently connected to
an argument structure defining the verb?s semantic
roles. By consulting lexical resources like Prop-
Bank (Palmer et al, 2005), we can use existing
knowledge about possible semantic arguments of
26
the tasks we have identified.
We claim that actions/verbs form a suitable
and robust basis for describing the (informational)
meaning of utterances in IQA. Taking the main
verb along with its semantic arguments to repre-
sent the core meaning of user questions seems to
be a more feasible alternative to deep semantic ap-
proaches that still lack the robustness for dealing
with unconstrained user input.
Further, we claim that analyzing user questions
on the basis of their task/entity structure provides a
useful level of abstraction and granularity for em-
pirically studying informational transitions in IQA
dialogues. We back up this claim in Section 6.
Along the lines of (Kirschner and Bernardi, 2007),
we aim for a precise definition of focus structure
for IQA questions. Our approach is similar in spirit
to (Chai and Jin, 2004), whereas we need to re-
duce the complexity of their discourse representa-
tion (i.e., their number of possible question ?top-
ics?) so that we arrive at a representation of focus
structure that lends itself to implementation in a
practical IQA system.
3.2 How focus evolves in IQA
We try to formulate our original question, ?Given
a user question and a system response, what does
a salient FU Q focus on?? more precisely. We
want to know whether the FU Q initiates one of
the following three transitions:
2
Topic zoom asking about a different aspect of
what was previously focused
1. asking about the same task and same ar-
gument, but different question type (e.g.,
search for books: Q: where, FU Q: how)
2. asking about the same entity (e.g.,
guided tour: Q: when, FU Q: where)
3. asking about the same task but different
argument (e.g., Q: search for books, FU
Q: search for journals)
4. asking about an entity introduced in the
previous system answer
Coherent shift to a ?related? (semantically, or:
verb?its semantic argument) focus
1. from task to semantically related task
2. from task to related entity: entity is a se-
mantic argument of the task
2
Comparing our points to (Chai and Jin, 2004), Topic
zoom: 1. and 2. are cases of topic exploration, 3. of topic
extension, and 4. is new. Coherent shift: 1. and 2. are cases of
topic shift, and 3. and 4. are new.
3. from entity to semantically related entity
4. from entity to related task: entity is a se-
mantic argument of the task
Shift to an unrelated focus
From the analysis of our WoZ data we get cer-
tain intuitions about salient focus flow between
some preceding dialogue and a FU Q. First of all,
we learn that a dialogue context of just one previ-
ous user question and one previous system answer
generally provides enough information to resolve
context-dependent FU Qs. In the remainder of this
section, we describe the other intuitions by propos-
ing alternative ways of detecting the focus of a FU
Q that follows a salient relation (?Topic zoom? or
?Coherent shift?). Later in this paper we show how
we implement these intuitions as features, and how
we use a regression model to learn the importance
of these features from data.
Exploiting task/entity structure Knowing
which entities are possible semantic arguments
of a library-related task can help in detecting the
focused task. Even if the task is not expressed
explicitly in the question, the fact that a number of
participant entities are found in the question could
help identify the task at hand.
Exploiting (immediate) dialogue context: pre-
vious user question It might prove useful to
know the things that the immediately preceding
user question focused on. If users tend to con-
tinue focusing on the same task, entity or question
type, this focus information can help in ?complet-
ing? context-dependent FU Qs where the focused
things cannot be detected easily since they are not
mentioned explicitly. This way of using dialogue
context has been used in previous IQA systems,
e.g., the Ritel system (van Schooten et al, forth-
coming).
Exploiting (immediate) dialogue context: pre-
vious system answer Whereas the role of the
system answer has been ignored in some pre-
vious accounts of FU Qs (e.g., (Chai and Jin,
2004) and even in the highly influential TREC task
(Voorhees, 2004)), our data suggest that the system
answer does play a role for predicting what a FU
Q will focus on: it seems that the system answer
can introduce entities that a salient FU Q will ask
more information about. (van Schooten and op den
Akker, 2005) and (Bertomeu et al, 2006) describe
IQA systems that also consider the previous sys-
tem answer.
27
Exploiting task/entity structure combined with
dialogue context It might be useful to com-
bine knowledge about the task/entity structure with
knowledge about the previously focused task or
entity. E.g., a previously focused task might make
a ?coherent shift? to a participant entity likely;
likewise, a previously focused entity might enable
a coherent shift to a task in which that entity could
play a semantic role.
The questions to be addressed in the remain-
der of the paper now are the following. Does the
performance of an IQA system improve if it has
structure/history-based information as mentioned
above? Which is the role that each part of this in-
formation plays for determining the correct answer
to a FU Q?
4 Tagging focus on three levels
Following the discussion in Section 3.1, and hav-
ing studied the user dialogues from our WoZ data,
we propose to represent the (informational) mean-
ing of a user question by identifying the task
and/or entity that the question is about (focuses
on). Besides task and entity, we have Question
Type (QType) as a third level on which to describe
a question?s focus. The question type relates to
what type of information the user asks about the
focused task/entity, and equivalently describes the
exact type of answer (e.g., why, when, how) that
the user hopes to get about the focused task/entity.
Thus, we can identify the focus of a question with
the triple <Task, Entity, QType>.
We have been manually building a small
domain-dependent lexical resource that in the fol-
lowing we will call ?task/entity structure?. We
see it as a miniature version of the PropBank, re-
stricted to the small number of verbs/tasks that we
have identified to be relevant in our domain, but
extended with some additional semantic argument
slots if required. Most importantly, the argument
slots have been assigned to possible filler entities,
each of which can be described with a number of
synonymous names.
Tasks By analyzing a previously acquired exten-
sive list of answers to frequently-asked library-
related questions, we identified a list of 11 tasks
that library users might ask about (e.g. search, re-
serve, pick up, browse, read, borrow, etc.). Our
underlying assumption is that the focus (as identi-
fied by the focus triple) of a question is identical to
that of the corresponding answer. Thus, we assume
the focus triple describing a user question also de-
scribes its correct answer. For example, in Table 1,
A1 would share the same focus triple as Q1.
We think of the tasks as abstract descriptions of
actions that users can perform in the library con-
text. A user question focuses on a specific task if it
either explicitly contains that verb (or a synonym),
or implicitly refers to the same ?action frame? that
the verb instantiates.
Entities Starting from the information about se-
mantic arguments of these verbs available in
PropBank, and extending it when necessary for
domain-specific use of the verbs, for each task we
determined its argument slots. Again by inspect-
ing our list of FAQ answers, we started assign-
ing library-related entities to these argument slots,
when we found that the answer focuses on both
the task and the semantic argument entity. We
found that many answers focus on some library-
related entity without referring to any task. Thus,
we explicitly provide for the possibility of a ques-
tion/answer being about just an entity, e.g.: ?What
are the opening times??. A user question focuses
on a specific entity if it refers to it explicitly or
via some reference phenomenon (anaphora, ellip-
sis, etc.) linked to the dialogue history.
Question Types We compiled a list of question
(or answer) types by inspecting our FAQ answers
list, and thinking about the types of questions that
could have given rise to these answers. We aimed
for a compromise between potentially more fine-
grained distinctions of question semantics, and
better distinguishability of the resulting set of la-
bels (for a human annotator or a computer pro-
gram).
We defined each question type by providing a
typical question template, e.g.: ?where: where
can I find $Entity??, ?whatis: what is $Entity??,
?yesno: can I $Task $Entity??, ?howto: how do I
$Task $Entity??. Note how some question types
capture questions that focus on some task along
with some participant entity, while others focus on
just an entity. We also devised some question types
for questions focusing on just a task, where we as-
sume an implicit semantic argument which is not
expressed, e.g., ?how can I borrow?? (where in the
specific context of our application we can imply a
semantic argument like ?item?). A question has a
specific question type if it can be paraphrased with
the corresponding question template. An answer
28
has a specific type if it is the correct answer to that
question template.
4.1 A repository of annotated answers
From our original collection of answers to library
FAQs, we have annotated around 200 with focus
triples. The triples we selected include all poten-
tial answers to the FU Qs from the free FU Q elic-
itation experiment described in the next section.
Some of the actual answers were annotated with
more than one focus triple, e.g., often the answer
corresponded to more than one question type. The
total of 207 focus triples include all 11 tasks and
23 different question types (where the 4 most fre-
quent types were the ones mentioned as examples
above, accounting for just over 50% of all focus
triples).
For instance, the answer: ?You can restrict your
query in the OPAC on individual Library locations.
The search will then be restricted e.g. to the Li-
brary of Bressanone-Brixen or the library of the
?Museion?.? is marked by: <Task: search, Entity:
specific library location, QType: yesno>.
The algorithm we introduce in Section 5 uses
this answer repository as the setA of potential can-
didates from which it chooses the answer to a new
user question. Again, we assume that if we can de-
termine the correct focus triple of a user question,
the answer from our collection that has been an-
notated with that same triple will correctly answer
the question.
4.2 Annotated user questions
Having created an answer repository annotated
with focus triples, we need user questions anno-
tated on the same three levels, which we can then
use for training and evaluating the Q?A algorithm
that we introduce in Section 5. We acquired these
data in two steps: 1. eliciting free FU Qs from sub-
jects in a web-based experiment, 2. annotating the
questions with focus triples.
Dialogue Collection Experiment We set up a
web-based experiment to collect genuine FU Qs.
We adopted the experimental setup proposed in
(van Schooten and op den Akker, 2005)), in that
we presented to our subjects short dialogues con-
sisting of a first library-related question, and a cor-
responding correct answer, as exemplified by ?Q1?
and ?A1? in Table 1.
We asked the subjects to provide a FU Q ?Q2?
such that it will help further serve their information
need in the situation defined by the given previous
question-answer exchange. In this way, we col-
lected 88 FU Qs from 8 subjects and 11 contexts
(first questions and answers).
3
Annotating the questions We annotated these
88 FU Qs, along with the 11 first questions that
were presented to the subjects, with focus triples.
By (informally) analyzing the differences between
different annotators? results, we continuously tried
to disambiguate and improve the annotation in-
structions. As a result, we present a pre-compiled
list of entities from which the annotator selects the
one they consider to be in focus, and that of all
possible candidates is the one least ?implied? by
the context. Table 1 shows one example annota-
tion of one of the 11 first user questions and two of
the 8 corresponding FU Qs.
5 A feature-based Q?A algorithm
We now present an algorithm for mapping a user
question to a canned-text answer from our answer
repository. The decision about which answer to se-
lect is based on a score that the algorithm assigns to
each answer, which in turn depends on the values
of the features we have introduced in the previous
section. Thus, the purpose of the algorithm is to
select the best answer focus triple from the repos-
itory, based on feature values. In this way, we can
use the algorithm as a test bed for identifying fea-
tures that are good indicators for a correct answer.
Our goal is to evaluate the algorithm based on its
accuracy in finding correct focus triples (which are
the ?keys? to the actual system answers) for user
questions (see Section 5.2).
For each new user question q that is entered, the
algorithm iterates through all focus triples a in the
annotated answer repository A (cf. Section 4.1).
For each combination of q and a, all 10 features
x
1,q,a
. . . x
10,q,a
are evaluated. Each feature that
evaluates to true (? = 1) or some positive value,
contributes with this score ? towards the overall
score of a. The algorithm then returns the highest-
scoring answer a?.
a? = argmax
a?A
(?
1
x
1,q,a
+ ? ? ?+ ?
10
x
10,q,a
)
3
In the future, we plan to collect real FU Qs from users of
our online IQA system, which will solve the potential prob-
lem of these questions being somewhat artificial due to the
experimental setting. However, we still expect our current
data to be highly relevant for studying what users would ask
about next.
29
ID Q/A Task Entity QType
Q1 Can I get search results for a specific search specific library location yesno
library location?
A1 You can restrict your query in the OPAC
on individual Library locations. (...)
Q2a How can I do that? search specific library location howto
Q2b How long is my book reserved there if I reserve my book howlong
want to get it?
Table 1: Example annotation of one first question and two corresponding FU Qs
5.1 Features
Based on the intuitions presented in Section 3.2,
we now describe the 10 features x
1,q,a
, . . . , x
10,q,a
that our algorithm uses as predictors for answer
correctness. All Task and Entity matching is done
using string matching over word stems. QType
matching uses regular expression matching with
a set of simple regex patterns we devised for our
QTypes.
3 surface-based features x
1,q,a
, . . . , x
3,q,a
:
whether {Task
a
,Entity
a
,QType
a
} are
matched in q. Entity feature returns the
length in tokens of the matched entity.
1 task/entity structure-based feature x
4,q,a
:
how many of the participant entities of Task
a
(as encoded in our task/entity structure) are
matched in q.
4 focus continuity features x
5,q,a
, . . . , x
8,q,a
:
whether {Task
a
,Entity
a
,QType
a
} are con-
tinued in q, wrt. previous dialogue as fol-
lows:
4
? Task, Entity, QType continuity wrt. pre-
vious user question.
? Entity continuity wrt. previous system
answer.
2 task/entity structure + focus continuity fea-
tures x
9,q,a
, x
10,q,a
:
? Focused Task of previous user question
has Entity
a
as a participant.
? Task
a
has focused Entity of previous
question as a participant.
5.2 First Evaluation
Table 2 shows manually set feature scores
?
1
, . . . , ?
10
we used for a first evaluation of the al-
4
Both entity continuity features evaluate to ?2? when ex-
actly the same entity is used again, but to ?1? when a synonym
of the first entity is used.
k x
k,q,a
range(x
k,q,a
) ?
k
1 qTypeMatch 0,1 4
2 taskMatch 0,1 3
3 lenEntityMatch n 2
4 nEntitiesInTask n 1
5 taskContinuity 0,1 1
6 entityContinuity 0,1,2 1
7 qTypeContinuity 0,1 1
8 entityInPrevAnsw 0,1,2 2
9 entityInPrevTask 0,1 1
10 prevEntityInTask 0,1 1
Table 2: Manually set feature scores
gorithm; we chose these particular scores after in-
specting our WoZ data. With these scores, we ran
the Q?A algorithm on the annotated questions of
annotator 1, who had provided a ?gold standard?
annotation for 78 of the 99 user questions (the re-
mainder of the questions are omitted because the
annotator did not know how to assign a focus triple
to them). For 24 out of 78 questions, the algorithm
found the exact focus triple (from a total of 207
focus triples in the answer repository), yielding an
accuracy of 30.8%.
6 Logistic Regression Model
To improve the accuracy of the Q?A algorithm
and to learn about the importance of the single
features for predicting whether an answer from
A is correct, we want to learn optimal scores
?
1
, . . . , ?
10
from data. We use a logistic regression
model (cf. (Agresti, 2002)). Logistic regression
models describe the relationship between some
predictors (i.e., our features) and an outcome (an-
swer correctness).
We use the logit ? coefficients ?
1
, . . . , ?
k
that
the logistic regression model estimates (from train-
ing data, using maximum likelihood estimation)
30
Coeff. 95% C.I.
lenEntityMatch 6.76 5.26?8.26
qTypeMatch 2.54 2.02?3.06
taskContinuity 2.17 1.39?2.94
entityInPrevAnsw 1.78 1.06?2.49
taskMatch 1.37 0.80?1.94
prevEntityInTask -1.24 -2.06? -0.43
Table 3: Model M
2
: Magnitudes of significant ef-
fects
for the predictors as empirically motivated scores.
In contrast to other supervised machine learn-
ing techniques, regression models yield human-
readable coefficients that show the individual ef-
fect of each predictor on the outcome variable.
6.1 Generating Training data
We generate the training data for learning the lo-
gistic regression model from our annotated answer
repository A (Sec. 4.1) and annotated questions
(Sec. 4.2) as follows. For each human-annotated
question q and each candidate answer focus triple
from our repository (a ? A), we evaluate our fea-
tures x
1,q,a
, . . . , x
10,q,a
. If the focus triples of q
and a are identical, we take the particular feature
values as a training instance for a correct answer; if
the focus triples differ, we have a training instance
for a wrong answer.
5
6.2 Results and interpretation
We fit model M
1
based on the annotation of anno-
tator 2 using all 10 features.
6
We then fit a second
model M
2
, this time including only the 6 features
that correspond to coefficients from modelM
1
that
are significantly different from zero. Table 3 shows
the resulting logit ? coefficients with their 95%
confidence intervals. Using these coefficients as
new scores in our Q?A algorithm (and setting all
non-significant coefficients? feature scores to 0), it
finds the correct focus triple for 47 out of 78 test
questions (as before, annotated by annotator 1);
answer accuracy now reaches 60.3%.
We interpret the results in Table 3 as follows.
All three surface-based features are significant pre-
dictors of a correct answer. The length of the
5
Although in this way we get imbalanced data sets with
|A| ? 1 negative training instances for each positive one, we
have not yet explored this issue further.
6
We use annotator 2?s data for training, and annotator 1?s
for testing throughout this paper.
matched entity contributes more than the other
two; we attribute this to the fact that there are
more cases where our simple implementations of
qTypeMatch and taskMatch fail to detect the cor-
rect QType or task. While the task/entity structure-
based nEntitiesInTask clearly misses to reach sig-
nificance, the history-based features taskContinu-
ity and entityInPrevAnsw are useful indicators for
a correct answer. The first is evidence for ?Topic
zoom?, with the FU Q asking about a different as-
pect of the previously focused task, while the sec-
ond shows the influence of the previous answer in
shaping the entity focus of the FU Q. From the two
?task/entity structure + focus continuity? features,
we find that if a FU Q focuses on a task that in
our task/entity structure has an argument slot filled
with the previously focused entity, it actually indi-
cates a false answer; the implications of this find-
ing will have to be explored in future work.
Finally, to pinpoint the important contributions
of structure- and/or focus continuity features, we
fit a new model M
3
, this time including only the 3
(significant) surface-based features. Evaluating the
resulting coefficients in the same way as above, we
get only 24 out of 78 correct answer focus triples,
an accuracy of 30.8%. This result supports our ini-
tial claim that an IQA system improves if it has a
way of predicting the focus of a FU Q.
7 Conclusion
Our original hypothesis was that a) knowledge
about the dialogue history, and b) lexical knowl-
edge about semantic arguments could improve an
IQA system?s ability to answer FU Qs. We opera-
tionalized these notions by formulating a set of 10
features that evaluate whether a candidate answer
is the correct one given a new (FU) user question.
We then used regression modeling to investigate
the usefulness of each individual feature by learn-
ing from annotated IQA dialogue data, showing
that certain knowledge about the dialogue history
(the previously focused task, and the entities men-
tioned in the previous system answer) and about
semantic arguments are useful for distinguishing
correct from wrong answers to a FU Q. Finally,
we evaluated these results by showing how our
Q?A mapping algorithm?s answer accuracy im-
proved by using the empirically learned scores for
all statistically significant predictors/features. The
features and the Q?A algorithm as a whole are
based on a simple way to describe IQA questions
31
in terms of focus triples. By showing how we
have improved an actual system with learned fea-
ture scores, we demonstrated this representation?s
viability for implementation and for empirically
studying informational transitions in IQA.
Although the IQA system used in our project is
in several ways limited, our findings about how
focus evolves in real IQA dialogues should scale
up to any new or existing IQA system that allows
users to ask context-dependent FU Qs in a type of
?information seeking? paradigm. It would be in-
teresting to see how this type of knowledge could
be added to other IQA or dialogue systems in gen-
eral.
We see several directions for future work. Re-
garding coherent focus transitions, we have to look
into which transitions to different tasks/entities are
more coherent than others, possibly based on se-
mantic similarity. A major desideratum for show-
ing the scaleability of our work is to explore the
influence of the subjects on our data annotation.
We are currently working on getting an objective
inter-annotator agreement measure, using external
annotators. Finally, we plan to collect a large cor-
pus of IQA dialogues via a publicly accessible IQA
system, and have these dialogues annotated. With
more data, coming from genuinely interested users
instead of experimental subjects, and having these
data annotated by external annotators, we expect
to have more power to find significant and gener-
ally valid patterns of how focus evolves in IQA di-
alogues.
Acknowledgments
We thank Marco Baroni, Oliver Lemon, Massimo
Poesio and Bonnie Webber for helpful discussions.
References
Agresti, Alan. 2002. Categorical Data Analysis.
Wiley-Interscience, New York.
Ahrenberg, L., N. Dahlb?ack, and A. J?onsson. 1995.
Coding schemes for studies of natural language dia-
logue. In Working Notes from AAAI Spring Sympo-
sium, Stanford.
Bertomeu, N?uria, Hans Uszkoreit, Anette Frank, Hans-
Ulrich Krieger, and Brigitte J?org. 2006. Contex-
tual phenomena and thematic relations in database
QA dialogues: results from a wizard-of-oz experi-
ment. In Proc. of the Interactive Question Answer-
ing Workshop at HLT-NAACL 2006, pages 1?8, New
York, NY.
Chai, Joyce Y. and Rong Jin. 2004. Discourse structure
for context question answering. In Proc. of the HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering, Boston, MA.
Grosz, Barbara Jean. 1977. The representation and
use of focus in dialogue understanding. Ph.D. thesis,
University of California, Berkeley.
Kirschner, Manuel and Raffaella Bernardi. 2007. An
empirical view on iqa follow-up questions. In Proc.
of the 8th SIGdial Workshop on Discourse and Dia-
logue, Antwerp, Belgium.
Lec?uche, Renaud, Chris Mellish, Catherine Barry,
and Dave Robertson. 1999. User-system dia-
logues and the notion of focus. Knowl. Eng. Rev.,
13(4):381?408.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Comput. Linguist., 31(1):71?106.
Stede, Manfred and David Schlangen. 2004.
Information-seeking chat: Dialogue management by
topic structure. In Proc. of SemDial?04 (Catalog),
Barcelona, Spain.
Vallduvi, Enric. 1990. The Informational Component.
Ph.D. thesis, University of Pennsylvania, Philadel-
phia, PA.
van Schooten, Boris and Rieks op den Akker. 2005.
Follow-up utterances in QA dialogue. Traitement
Automatique des Langues, 46(3):181?206.
van Schooten, Boris, R. op den Akker, R. Rosset,
O. Galibert, A. Max, and G. Illouz. forthcoming.
Follow-up question handling in the IMIX and Ritel
systems: A comparative study. Journal of Natural
Language Engineering.
Voorhees, Ellen M. 2004. Overview of the TREC 2004
question answering track. In Proc. of the 13th Text
REtrieval Conference.
32
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 322?331,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Towards an Empirically Motivated Typology of Follow-Up Questions:
The Role of Dialogue Context
Manuel Kirschner and Raffaella Bernardi
KRDB Centre, Faculty of Computer Science
Free University of Bozen-Bolzano, Italy
{kirschner,bernardi}@inf.unibz.it
Abstract
A central problem in Interactive Ques-
tion Answering (IQA) is how to answer
Follow-Up Questions (FU Qs), possibly
by taking advantage of information from
the dialogue context. We assume that FU
Qs can be classified into specific types
which determine if and how the correct
answer relates to the preceding dialogue.
The main goal of this paper is to propose
an empirically motivated typology of FU
Qs, which we then apply in a practical
IQA setting. We adopt a supervised ma-
chine learning framework that ranks an-
swer candidates to FU Qs. Both the an-
swer ranking and the classification of FU
Qs is done in this framework, based on a
host of measures that include shallow and
deep inter-utterance relations, automati-
cally collected dialogue management meta
information, and human annotation. We
use Principal Component Analysis (PCA)
to integrate these measures. As a result,
we confirm earlier findings about the ben-
efit of distinguishing between topic shift
and topic continuation FU Qs. We then
present a typology of FU Qs that is more
fine-grained, extracted from the PCA and
based on real dialogue data. Since all our
measures are automatically computable,
our results are relevant for IQA systems
dealing with naturally occurring FU Qs.
1 Introduction
When real users engage in written conversations
with an Interactive Question Answering (IQA)
system, they typically do so in a sort of dia-
logue rather than asking single shot questions.
The questions? context, i.e., the preceding interac-
tions, should be useful for understanding Follow-
Up Questions (FU Qs) and helping the system
pinpoint the correct answer. In previous work
(Kirschner et al, 2009; Bernardi et al, 2010;
Kirschner, 2010), we studied how dialogue con-
text should be considered to answer FU Qs. We
have used Logistic Regression Models (LRMs),
both for learning which aspects of dialogue struc-
ture are relevant to answering FU Qs, and for com-
paring the accuracy with which the resulting IQA
systems can correctly answer these questions. Un-
like much of the related research in IQA, which
used artificial collections of user questions, our
work has been based on real user-system dialogues
we collected via a chatbot-inspired help-desk IQA
system deployed on the web site of our University
Library.
Previously, our experiments used a selection
of shallow (Kirschner et al, 2009) and deep
(Bernardi et al, 2010) features, all of which de-
scribe specific relations holding between two ut-
terances (i.e., user questions or system answers).
In this paper we present additional features derived
from automatically collected dialogue meta-data
from our chatbot?s dialogue management com-
ponent. We use Principal Component Analysis
(PCA) to combine the benefits of all these infor-
mation sources, as opposed to using only certain
hand-selected features as in our previous work.
The main goal of this paper is to learn from data
a new typology of FU Qs; we then compare it to an
existing typology based on hand-annotated FU Q
types, as proposed in the literature. We show how
this new typology is effective for finding the cor-
rect answer to a FU Q. We produce this typology
by analyzing the main components of the PCA.
This paper presents two main results. A new,
empirically motivated typology of FU Qs confirms
earlier results about the practical benefit of dis-
tinguishing between topic continuation and topic
shift FU Qs, which are typically based on hand
annotation. We then show that we can do without
such hand annotations, in that our fully automatic,
322
on-line measures ? which include automatically
collected dialogue meta-data from our chatbot?s
dialogue manager ? lead to better performance in
identifying correct answers to FU Qs.
In the remainder of this paper, we first review
relevant previous work concerning FU Q typolo-
gies in IQA. Section 3 then introduces our col-
lection of realistic IQA dialogues which we will
use in all our experiments; the section includes
descriptions of meta information in the form of
dialogue management features and post-hoc hu-
man annotations. In Section 4 we introduce our
experimental framework, based on inter-utterance
features and LRMs. Our experimental results are
presented in Section 5, which is followed by our
conclusions.
2 Related work
Much of previous work on dialogue processing in
the domain of contextual or interactive Question
Answering (QA) (Bertomeu, 2008; van Schooten
et al, 2009; Chai and Jin, 2004; Yang et al, 2006)
has been based on (semi-)artificially devised sets
of context questions. However, the importance of
evaluating IQA against real user questions and the
need to consider preceding system answers has al-
ready been emphasized (Bernardi and Kirschner,
2010). The corpus of dialogues we deal with con-
sists of real logs in which actual library users were
conversing (by typing) with a chat-bot to obtain
information in a help-desk scenario.
(Yang et al, 2006) showed that shallow simi-
larity features between a FU Q and the preceding
utterances are useful to determine whether the FU
Q is a continuation of the on-going topic (?topic
continuation?), or it is a ?topic shift?. The authors
showed that recognizing these two basic types of
FU Qs is important for deciding which context
fusion strategies to employ for retrieving the an-
swer to the FU Q. (Kirschner et al, 2009) showed
how shallow measures of lexical similarity be-
tween questions and answers in IQA dialogues are
as effective as manual annotations for distinguish-
ing between these basic FU Q types. However,
that earlier work was based on a much smaller set
of dialogue data than we use in this paper, mak-
ing for statistically weaker results. (Bernardi et
al., 2010) improved on this approach by increas-
ing the data set, and adding ?deep? features that
quantify text coherence based on different theories
of dialogue and discourse structure. However, FU
Q classification was performed using either single,
hand-selected shallow or deep features, or a hand-
selected combination of one shallow and one deep
feature. In this paper, we adopt the most promising
measures of similarity and coherence from the two
aforementioned papers, add new features based
on automatically collected dialogue management
meta-data, and combine all this information via
Principal Component Analysis (PCA). By using
PCA, we circumvent the theoretical problem that
potentially multicollinear features pose to our sta-
tistical models, and at the same time we have a
convenient means for inducing a new typology of
FU Qs from our data, by analyzing the composi-
tion of the principal components of the PCA.
More fine-grained typologies of FU Qs have
been suggested, and different processing strategies
have been proposed for the identified types. In
this paper, we start from our own manual annota-
tion of FU Qs into four basic classes, as suggested
by the aforementioned literature (Bertomeu, 2008;
van Schooten et al, 2009; Sun and Chai, 2007).
We then compare it to our new PCA-based FU Q
typology.
3 Data
We now introduce the set of IQA dialogue data
which we will use in our experiments. For the pur-
pose of calculating inter-utterance features within
these user-system interactions ? as described in
Section 4.4 ? we propose to represent utterances
in terms of dialogue snippets. A dialogue snip-
pet, or snippet for short, contains a FU Q, along
with a 2-utterance window of the preceding dia-
logue context. In this paper we use a supervised
machine learning approach for evaluating the cor-
rectness of a particular answer to a FU Q; we thus
represent also the answer candidate as part of the
snippet. Introducing the naming convention we
use throughout this paper, a snippet consists of the
following four successive utterances: Q1, A1, Q2,
and A2. The FU Q is thus referred to as Q2.
The data consists of 1,522 snippets of 4-turn
human-machine interactions in English: users ask
questions and the system answers them. The data
set was collected via the Bolzano Bot (BoB) web
application that has been working as an on-line
virtual help desk for the users of our University
Library since October 2008.1 The snippets were
1www.unibz.it/library. More information on the
BoB dialogue corpus: bob.iqa-dialogues.net.
323
extracted from 916 users? interactions.
Table 3 shows three example dialogue snippets
with correct A1 and A2; these examples are meant
to give an idea of the general shape of the BoB
dialogue data. In the third example snippet, A1
and A2 actually contain clickable hyperlinks that
open an external web-site. We represent them here
as dots in parentheses.
Our library domain experts manually checked
that each FU Q was either correctly answered in
the first place by BoB, or they corrected BoB?s an-
swer by hand, by assigning to it the correct answer
from BoB?s answer repository. In this way, the di-
alogue data contain 1,522 FU Qs, along with their
respective contexts (Q1 and A1) and their correct
answers (A2). The resulting set of correct A2s
contains 306 unique answers.2
The BoB dialogue data also contain two levels
of meta information that we will use in this paper.
On the one hand, we have automatically collected
dialogue meta-data from BoB?s dialogue manager
that describe the internal state of the BoB system
when a FU Q was asked; this information is de-
scribed in Section 4.2. On the other hand, 417 of
the 1,522 FU Qs were hand-annotated regarding
FU Q type, as described in Section 4.3.
4 Model
Our goal is, given a FU Q (Q2 in our dialogue
snippets), to pick the best answer from the fixed
candidate set of 306 A2s, by assigning a score to
each candidate, and ranking them by this score.
Different FU Q types might require different an-
swer picking strategies. Thus, we specify both
A2 (identification) features, aiming at selecting the
correct A2 among candidates, and context (iden-
tification) features, that aim at characterizing the
context. The A2 identification features measure
the similarity or coherence between an utterance
in the context (e.g., Q2) and a candidate A2. Con-
text features measure the similarity or coherence
between pairs of utterances in the context (e.g.,
Q1 and Q2). They do not provide direct infor-
mation about A2, but might cue a special context
(say, an instance of topic shift) where we should
pay more attention to different A2 identification
features (say, less attention to the relation between
2Many of the 306 answer candidates overlap semantically.
This is problematic, given that our evaluation approach as-
sumes exactly one candidate to be correct, while all other 305
answers to be wrong. In this paper, we shall accept this fact,
for the merit of simplicity.
Q2 and A2, and more to the one between A1 and
A2).
We implement these ideas by estimating a gen-
eralized linear model from training data to predict
the probability that a certain A2 is correct given
the context. In this model, we enter A2 features as
main effects, and context features in interactions
with the former, allowing for differential weight
assignment to the same A2 features depending on
the values of the context features.
4.1 Logistic Regression
Logistic regression models (LRMs) are general-
ized linear models that describe the relationship
between features (independent variables) and a bi-
nary outcome (Agresti, 2002). LRMs are closely
related to Maximum Entropy models, which have
performed well in many NLP tasks. A major ad-
vantage of using logistic regression as a super-
vised machine learning framework (as opposed to
other, possibly better performing approaches) is
that the learned coefficients are easy to interpret
and assess in terms of their statistical significance.
The logistic regression equations specify the prob-
ability for a particular answer candidate A2 being
correct, depending on the ? coefficients (repre-
senting the contribution of each feature to the total
answer correctness score), and the feature values
x1, . . . , xk. In our setting, we are only interested
in the rank of each A2 among all answer candi-
dates, which can be easily and efficiently calcu-
lated through the linear part of the LRM: score
= ?1x1 + . . .+ ?kxk.
FU Q typology is implicitly modeled by inter-
action terms, given by the product of an A2 fea-
ture and a context feature. An interaction term
provides an extra ? to assign a differential weight
to an A2 feature depending on the value(s) of a
context feature. In the simplest case of interaction
with a binary 0-1 feature, the interaction ? weight
is only added when the binary feature has the 1-
value.
As described in (Kirschner, 2010), we esti-
mate the model parameters (the beta coefficients
?1, . . . , ?k) using maximum likelihood estima-
tion. Moreover, we put each model we construct
under trial by using an iterative backward elimina-
tion procedure that keeps removing the least sig-
nificant predictor from the model until a specific
stopping criterion that takes into account the sta-
tistical goodness of fit is satisfied. All the results
324
we report below are obtained with models that un-
derwent this trimming procedure.
There is a potential pitfall when using multi-
ple regression models such as LRMs with multi-
collinear predictors, i.e., predictors that are inter-
correlated, such as our alternative implementa-
tions of inter-utterance string similarity. In such
situations, the model may not give valid results
about the importance of the individual predictors.
In this paper, we use PCA to circumvent the prob-
lem by combining potentially multicollinear pre-
dictors to completely uncorrelated PC-based pre-
dictors.
In the following three sections, we describe the
different types of information that are the basis for
our features.
4.2 BoB dialogue management meta-data
When BoB interacts with a user, it keeps log files
of the IQA dialogue. First of all, these logs in-
clude a timed protocol of user input and BoB?s
responses: the user and system utterances are the
literal part of the information. On the other hand,
BoB also logs two dimensions of meta informa-
tion, both of which are based on BoB?s internal
status of its dialogue management routine. This
routine is based on a main initiative-response loop,
mapping user input to some canned-text answer,
where the user input should be matched by (at
least) one of a set of hand-devised regular expres-
sion question patterns.
Sub-dialogues Whenever BoB asks a system-
initiated question, the main loop is suspended, and
the system goes into a sub-dialogue state, where it
waits for a specific response from the user ? typ-
ically a short answer indicating the user?s choice
about one of the options suggested by BoB. The
next user input is then matched against a small
number of regular expression patterns specifically
designed for the particular system-intiative ques-
tion at hand. Depending on this user input, the
sub-dialogue can:
Continue: the user input matched one of the
regular expression patterns intended to capture
possible user choices
Break: the user broke the sub-dialogue by en-
tering something unforeseen, e.g., a new question
The first two parts of Table 4 give an overview
of the statistics of BoB?s dialogue management-
based meta information concerned with sub-
dialogue status. Besides continue and break, for
Q1 we consider also a third, very common case
that a user question was not uttered in a sub-
dialogue setting at all. Note that we excluded from
our data collection all those cases where Q2 con-
tinues a sub-dialogue from our collection of IQA
dialogues, since we do not consider such Q2s as
FU Qs, as they are highly constrained by the pre-
vious dialogue.
Apology responses The third part of Table 4
gives statistics of whether a particular system re-
sponse A1 was an apology message stating that
BoB did not understand the user?s input, i.e., none
of BoB?s question patterns matched the user ques-
tion.
4.3 Manual dialogue annotation
We now turn to the meta information in BoB dia-
logue data that stems from post-hoc human anno-
tation. For a portion of BoB?s log files, we added
up to two additional levels of meta information, by
annotating the log files after they were collected.3
The following paragraphs explain the individ-
ual levels of annotation by giving the correspond-
ing annotator instructions; Table 5 contains an
overview of the corresponding features. First of
all, we annotated FU Qs with their FU Q type.
Our choice of the particular four levels of the
FUQtype feature was influenced by the following
literature literature: from (De Boni and Manand-
har, 2005) and (Yang et al, 2006) we adopted the
distinction between topic shift and topic continua-
tion, while from (Bertomeu et al, 2006) we took
the notions of rephrases and context dependency.
Our annotation scheme is described in Figure 1;
note that topic continuations have three sub-types,
which are spelled out below.
FUQtype = isTopicShift: marks a FU Q
as a topic shift based on an intuitive notion of
whether the FU Q ?switches to something com-
pletely different?.
FUQtype = isRephrase: marks whether
the FU Q is an attempt to re-formulate the same
question. The FU Q could be a literal repetition of
the previous question, or it could be a rephrasing.
FUQtype = isContextDepentFUQ:
marks whether the FU Q needs to be consid-
ered along with some information provided by
3All annotations were performed by either one of the au-
thors.
325
the dialogue context in order to be correctly
understood.
FUQtype = isFullySpecifiedFUQ:
marks whether the FU Q does not need any
information from the dialogue context in order to
be correctly understood.
The second level of hand-annotation concerns a
manual check of the correctness of A1. It is avail-
able for 1,179 of our 1,522 snippets.
A1.isAnswer.correct: marks whether the
system response is correct for the given question.
A1.isApology.correct: marks whether
BoB?s apology message is correct for the given
question.
4.4 Shallow/deep inter-utterance relations
We exploit shallow features, which measure the
similarity between two utterances within a snip-
pet, and deep features, which encode coherence
between two utterances based on linguistic the-
ory. For each feature we will use names encoding
the utterances involved; e.g., distsim.A1.Q2
stands for the Distributional Similarity feature cal-
culated between A1 and Q2.
Shallow features The detailed description of all
the shallow features we used in our experiments
can be found in (Kirschner et al, 2009). The in-
tuition is that a high similarity between Q and A
tends to indicate a correct answer, while in the
case of high similarity between the dialogue con-
text and the FU Q, it indicates a ?topic continua-
tion? FU Q (as opposed to a ?topic shift? FU Q),
and thus helps discriminating these two classes of
FU Qs.
Lexical Similarity (lexsim): If
two utterances share some terms, they are simi-
lar; the more discriminative the terms they share,
the more similar the utterances. Implements a TF-
IDF-based similarity metric. Distributional
Similarity (distsim.svd): Two utter-
ances are similar not only if they share the same
terms, but also if they share similar terms (e.g.,
book and journal). Term similarity is estimated
on a corpus, by representing each content word
(noun, verb, adjective) as a vector that records
its corpus co-occurrence with other content words
within a 5-word span. Action sequence
(action): Based on the notion that in our help-
desk setting we are dealing with task-based dia-
logues, which revolve around library-related ac-
tions (e.g., ?borrow?, ?search?). The action fea-
ture indicates whether two utterances contain the
same action.
Deep features These features encode different
theories of discourse and dialogue coherence. Re-
fer to (Bernardi et al, 2010) for a full description
of all deep features we used experimentally, along
with more details on the underlying linguistic the-
ories, and our implementation choices for these
features.
We introduce a four-level feature, center,
that encodes the four transitions holding between
adjacent utterances that Centering Theory de-
scribes (Brennan et al, 1987; Grosz et al, 1995).
Somewhat differently from that classic theory,
(Sun and Chai, 2007) define the transitions de-
pending on whether both the head and the modi-
fier of the Noun Phrases (NP) representing the pre-
ferred centers4 are continued (cont) or switched
(rough shift: roughSh) betweenQ1 andQ2. The
remaining two transitions are defined in similar
terms.
4.5 PCA-based context classification features
Principal Component Analysis (PCA) (Manly,
2004) is a statistical technique for finding patterns
in high-dimensional data, or for reducing their di-
mensionality. Intuitively, PCA rotates the axes of
the original data dimensions in such a way that
few of the new axes already cover a large portion
of the variation in the data. These few new axes
are represented by the so-called principal compo-
nents (PCs). We employ this technique as a tool
for combining a multitude of potentially multi-
collinear predictors for context classification, i.e.,
all predictors that involve Q2 and some preceding
utterance. In our experiments we will also want to
look at the correlations of each of the top PCs with
the original context classification features; these
correlations are called loadings in PCA. We exper-
iment with the following three versions of PCA:
PCAA: without BoB dialogue management
meta-data features PCA performed over all
context classification features of the shallow and
deep types described in Section 4.4.
4Centers are noun phrases. The syntactic structure of a
noun phrase comprises a head noun, and possibly a modi-
fier, e.g., an adjective. We use a related approach, described
in (Ratkovic, 2009), to identify the preferred center of each
question.
326
PCAB: with BoB dialogue management meta-
data features PCAA plus BoB?s dialogue-
management meta-data features (Section 4.2).
PCAC: with BoB dialogue management meta-
data features and manual A1 correctness check
PCAB plus additional manual annotation of A1
correctness (Section 4.3).
5 Evaluation
We employ a standard 10-fold cross-validation
scheme for splitting training and prediction data.
We assess our LRMs by comparing the ranks that
the models assign to the gold-standard correct A2
candidate (i.e., the single A2 that our library do-
main experts had marked as correct for each of the
1,522 FU Qs). To determine whether differences
inA2 ranking performance are significant, we con-
sult both the paired t-test and the Wilcoxon signed
rank test about the difference of the 1,522 ranks.
5.1 Approximating hand-annotated FU Q
types with PCA-based features
We begin the evaluation of our approach by ex-
ploring the value of the hand-annotation-based FU
Q type as cues for expressing the relevance and
topical relatedness of that particular FU Q?s dia-
logue context.
For this purpose, we use the subset of 417
dialogue snippets which we annotated with the
FUQtype feature described in the first half of Ta-
ble 5. Figure 1 depicts our FU Q type taxonomy,
and the distribution of the four types in our data.
First of all, for this hand-annotated subset of di-
alogue snippets, we try to improve the A2 ranking
results of a ?main effects only? baseline LRM,
i.e., a model which does not distinguish between
different FU Q types. This baseline model was
proposed in earlier work (Kirschner et al, 2009).
We tried the following features as interaction
term(s) in our models, one after the other: whether
the hand-annotated FUQType feature indicates a
topic shift or not; the full four levels of FUQType;
a linear combination of the top five PCs of each of
the three PCA feature sets introduced in Section
4.5. After applying our automatic predictor elimi-
nation routine described in Section 4.1 and evalu-
ating the A2 ranking results of each of these mod-
els, none of the interactive models significantly
outperform our baseline. PCA-based context clas-
sification using only fully automatic BoB meta in-
formation features (PCAB in Section 4.5) results
in the largest improvement over baseline; however,
this improvement does not reach statistical signifi-
cance, most likely due to the small data set of only
417 cases. Still, using the hand-annotated FU Q
type feature FUQType, we can visualize how the
top PCs cluster the 417 FU Qs, and how this clus-
tering mirrors some of the distinctions of manually
assigned FU Q types: see Figure 2. E.g., plotting
the FU Qs along their PC1 and PC2 values seems
to mimic the annotator?s distinction between topic
shift FU Qs and the other three FU Q types. The
other pairs of PCs also appear to show certain clus-
ters. Overall, the automatic context classification
features that served as input to the PCA are useful
for describing different context-related behaviors
of different FU Qs.
5.2 Optimizing A2 ranking scores using
PCA-based features
Having shown the usefulness (in terms of assign-
ing high ranks to the gold-standard correct A2) of
FU Q classification via a PCA-based combination
of purely automatic context classification features,
we can now consider the full sample of 1,522 di-
alogue snippets described in Section 3, for which
we do not in general possess manual FU Q type
annotations.
The first row of Table 1 shows the A2 ranking
results of our baseline LRM. In the remainder of
the table, we compare this baseline model to three
different models which use a linear combination
of different versions of the top five PCs as interac-
tion terms. The three versions (A, B and C) were
introduced in Section 4.5.
5.3 Analysis of PC-based context features
The main goal of this paper is to devise an empiri-
cally motivated typology of FU Qs, under consid-
eration of automatically collected dialogue man-
agement meta information. We then want to show
how this new typology is effective for finding the
correct answer to a specific FU Q, in that for the
given FU Q it indicates the relevance and top-
ical relatedness of the question?s particular dia-
logue context. In Section 5.2 we have seen how
all PCA-based context classification features per-
form clearly better than a non-interactive baseline
model; more specifically, the top five PCs from
the PCAB scheme yield significantly better A2
ranking results than the PCAA scheme which does
not consider BoB dialogue management meta-data
features. Based on these results, we now look in
327
Model ID Interaction terms Mean rank Median rank Standard p (Paired p (Wilcoxon
correct A2 correct A2 dev. t-test) signed rank)
baseline none 48.72 14 69.35
PCAA PC1 + . . .+ PC5 44.25 12 64.58 < 0.0001 < 0.0001
PCAB PC1 + . . .+ PC5 42.72 12 62.53 0.0006 0.0087
PCAC PC1 + . . .+ PC5 42.87 12 62.94 not sig. not sig.
Table 1: Improving ranking of correct A2 (out of 306 answer candidates) with different PCA-based
interaction terms. Significance tests of rank differences wrt. result in preceding row.
more detail at the relevance of the top five PC fea-
tures in PCAB , and at their most important load-
ings, i.e., the original context classification fea-
tures that are most highly correlated with the value
of each particular PC. After running our predic-
tor elimination routine, the corresponding LRM
has kept three of these five top PCs as interaction
terms: PC1, PC2 and PC5. Table 2 describes the
top three positive and top three negative loadings
of these PCs. The table also shows how in model
PCAB , each of the interaction terms correspond-
ing to the three PCs influences the score that is cal-
culated for everyA2 candidate, either positively or
negatively.
Interpreting the results of Table 2 on a high,
dialogue-specific level, we draw the following
conclusions:
PC1 seems to capture a rather general distinc-
tion of topic shift versus topic continuation. A
FU Q with high lexical similarity to the preced-
ing utterances (i.e., a ?topic continuation?) should
preferably get an A2 with higher lexical similar-
ity with respect to both A1 and Q2. In this con-
text, ?topic shift? is partly described by a feature
from Centering Theory, and two of BoB?s dia-
logue management meta-data features.
PC2 shows relatively weak positive correlations
with any context classification features. On the
negative end, PC2 seems to describe a class of FU
Qs that are uttered after a Q1 that did neither con-
tinue nor exit a sub-dialogue. Also,A1 was a regu-
lar system answer (as opposed to an apology mes-
sage by BoB). Such FU Qs can thus be interpreted
as ?single shot? questions that a user poses after
their previous question was already dealt with in
A1. Because of the negative loadings, the value of
PC2 becomes negative, resulting in the avoidance
of any A2 that is highly similar to the preceding
A1.
PC5 distinguishes FU Qs that are mostly related
to the previous answer from those that are more
related to the previous question. Depending on
whether PC5 turns positive or negative, A2s are
preferred that are more similar to A1 or Q2, re-
spectively. Q1.Q2 similarity is determined by both
lexical similarity and Centering Theory features.
6 Conclusion
In this paper we have experimentally explored the
problem of FU Q types and their corresponding
answer identification strategies. The first result is
that our hand-annotated FU Q types did not sig-
nificantly improveQ2 answering performance (for
the annotated sub-set of 417 snippets). We at-
tribute this negative result in part to the difficulty
of the 4-level FU Q type annotation task. On the
other hand, we believe it is encouraging that with
purely automatic features for context classifica-
tion, combined through PCA, we significantly out-
performed our baseline. Adding BoB?s dialogue
management meta information ? which is also au-
tomatically available when using our dialogue col-
lection scheme ? for context classification helped
improve the scores even further. We analyzed the
top loadings of three PCs that our best-performing
LRM uses for FU Q type classification. We used
PCA both for circumventing the problem of mul-
ticollinear predictors in LRM, and as a diagnostic
tool to analyze the most important components of
automatically combined FU Q classification fea-
tures. Finally, a potentially difficult and cumber-
some manual annotation of the correctness of the
previous system answer A1 did not improve A2
ranking performance.
References
Alan Agresti. 2002. Categorical Data Analysis.
Wiley-Interscience, New York.
Raffaella Bernardi and Manuel Kirschner. 2010. From
328
LOADINGS
PC1 PC2 PC5
0.33 distsim.Q1.Q2 0.05 Q1.bob.contSubdial 0.45 distsim.A1.Q2
0.26 distsim.A1.Q2 0.04 Q2.center.roughSh 0.31 A1.bob.isApology
0.26 action.Q1.Q2 0.02 Q2.bob.breakSubdial 0.29 lexsim.A1.Q2
...
...
...
?0.13 A1.bob.isApology ?0.22 A1.bob.isAnswer ?0.18 lexsim.Q1.Q2
?0.15 Q2.bob.noSubdial ?0.30 Q2.bob.noSubdial ?0.23 Q2.center.cont
?0.22 Q2.center.roughSh ?0.31 Q1.bob.noSubdial ?0.26 A1.bob.isAnswer
INFLUENCE ON A2 SELECTION IN MODEL PCAB
pos for each A2 similar to Q2 pos for each A2 similar to A1 pos for each A2 similar to A1
pos for each A2 similar to A1 neg for each A2 similar to Q2
Table 2: Strongest loadings for the three PCs retained as interaction terms in Model PCAB , and indication
of each PC?s positive/negative influence on lexical similarity-based A2 selection features
artificial questions to real user interaction logs: Real
challenges for interactive question answering sys-
tems. In Proc. of Workshop on Web Logs and Ques-
tion Answering (WLQA?10), Valletta, Malta.
Raffaella Bernardi, Manuel Kirschner, and Zorana
Ratkovic. 2010. Context fusion: The role of dis-
course structure and centering theory. In Proceed-
ings of the Seventh conference on International Lan-
guage Resources and Evaluation (LREC?10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
Nu?ria Bertomeu, Hans Uszkoreit, Anette Frank, Hans-
Ulrich Krieger, and Brigitte Jo?rg. 2006. Contextual
phenomena and thematic relations in database QA
dialogues. In Proc. of the Interactive Question An-
swering Workshop at HLT-NAACL 2006, pages 1?8,
New York, NY.
Nuria Bertomeu. 2008. A Memory and Attention-
Based Approach to Fragment Resolution and its Ap-
plication in a Question Answering System. Ph.D.
thesis, Department of Computational Linguistics,
Saarland University.
Susan E. Brennan, Marilyn W. Friedman, and Carl J.
Pollard. 1987. A centering approach to pronouns.
In Proceedings of the 25th annual meeting on Asso-
ciation for Computational Linguistics, pages 155?
162, Stanford, California.
Joyce Y. Chai and Rong Jin. 2004. Discourse structure
for context question answering. In Proc. of the HLT-
NAACL 2004 Workshop on Pragmatics in Question
Answering, Boston, MA.
Marco De Boni and Suresh Manandhar. 2005. Im-
plementing clarification dialogues in open domain
question answering. Journal of Natural Language
Engineering, 11(4):343?361.
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein-
stein. 1995. Centering: A framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
Manuel Kirschner, Raffaella Bernardi, Marco Baroni,
and Le Thanh Dinh. 2009. Analyzing Interactive
QA dialogues using Logistic Regression Models. In
Proc. of XIth International Conference of the Italian
Association for Artificial Intelligence (AI*IA), Reg-
gio Emilia, Italy.
Manuel Kirschner. 2010. The Structure of Real
User-System Dialogues in Interactive Question An-
swering. Ph.D. thesis, Free University of Bozen-
Bolzano, Italy.
Bryan F. J. Manly. 2004. Multivariate Statistical
Methods: A Primer. Chapman & Hall/CRC, 3rd
edition.
Zorana Ratkovic. 2009. Deep analysis in iqa: evalua-
tion on real users? dialogues. Master?s thesis, Euro-
pean Masters Program in Language and Communi-
cation Technologies.
Mingyu Sun and Joyce Y. Chai. 2007. Discourse pro-
cessing for context question answering based on lin-
guistic knowledge. Know.-Based Syst., 20(6):511?
526.
Boris van Schooten, R. op den Akker, R. Rosset,
O. Galibert, A. Max, and G. Illouz. 2009. Follow-
up question handling in the IMIX and Ritel systems:
A comparative study. Journal of Natural Language
Engineering, 15(1):97?118.
Fan Yang, Junlan Feng, and Giuseppe Di Fabbrizio.
2006. A data driven approach to relevancy recogni-
tion for contextual question answering. In Proc. of
the Interactive Question Answering Workshop at
HLT-NAACL 2006, pages 33?40, New York City,
NY.
329
Q1 where in the library can i find books
A1 Most of our holdings are accessible in the open-shelves area. In the repository we only keep
some older volumes of journals or the outsize stuff.
Q2 shelf mark
A2 Shelf marks are a combination of letters and numbers, which exactly define the location of a
book in a Library. If you find a book or a journal in the catalogue, you also see the shelf mark.
The shelf marks of the University Library consist of an alphanumeric code (e.g. ES 560 K19).
Would you like to know how you search them?
Q1 When can I return a book?
A1 Books and other media can be consigned at the information desk during the regular opening
hours of the Library. You can also leave them at the book drop located in front of the main
entrance of the university building - 24 hours a day.
Q2 I want to read the newspaper on the computer
A2 If you haven?t got a Library Card you can visit the Library to read books, journals or newspapers.
If you want to borrow something or use the other services we offer you should get a Library Card.
Do you want to know how it works?
Q1 i need the opac
A1 Here (. . . ) is the page with our OPAC for you. You can search the complete holdings of the
Library.
Q2 where is the advisory service
A2 The University?s Advisory Service provides an information service for prospective students who
would like to know more about which degree course to study. It also provides a support service
for enrolled students during their entire time at the University. You can get further information
from this (. . . ) site.
Table 3: Example dialogue snippets with correctly identified A2
Feature name Freq. Description
Q1.bob.contSubdial 7.6% Q1 continues system-initiated sub-dialogue
Q1.bob.breakSubdial 9.6% Q1 breaks out of system-initiated sub-dialogue
Q1.bob.noSubdial 82.9% BoB not in sub-dialogue mode when Q1 was uttered
Q2.bob.breakSubdial 13.6% Q2 breaks out of system-initiated sub-dialogue
Q2.bob.noSubdial 86.4% BoB not in sub-dialogue mode when Q2 was uttered
A1.bob.isAnswer 75.6% A1 is regular answer retrieved by BoB
A1.bob.isApology 24.4% A1 is apology message: BoB did not understand
Table 4: BoB dialogue management meta information. Proportions out of those 1,441 of total 1,522
snippets for which this information was logged.
Feature name Freq. Description
FUQtype=isTopicShift 40.0% (of 417) Q2 is topic shift
FUQtype=isRephrase 19.2% (of 417) Q2 is rephrasing of Q1
FUQtype=isContextDepentFUQ 6.5% (of 417) Q2 is context dependent
FUQtype=isFullySpecifiedFUQ 34.3% (of 417) Q2 is not context dependent
A1.isAnswer.correct 66.5% (of 1,179) BoB?s regular answer A1 is correct
A1.isAnswer.false 19.0% (of 1,179) BoB?s regular answer A1 is false
A1.isApology.correct 1.3% (of 1,179) BoB?s apology message A1 is correct
A1.isApology.false 13.2% (of 1,179) BoB?s apology message A1 is false
Table 5: Manual annotation meta information. Proportions out of those sub-sets of total 1,522 snippets
with available annotation.
330
Topic 
continuation
Topic shift
Related/
salient 
transition 
FU Q       
Rephrase
Context-
dependent
Fully 
specified
143
80
250
27
170
417
167
Figure 1: Manual FU Q type annotation scheme, with counts of FU Q types
Scatter Plot Matrix
PC1
2
4
6
2 4 6
-4
-2
0
-4 -2 0
PC2
2
4
6
2 4 6
-4
-2
0
-4 -2 0
PC3
0
2
4
0 2 4
-6
-4
-2
-6 -4 -2
PC4
2
4
6
2 4 6
-2
0
2
-2 0 2
PC5
0
2
4
0 2 4
-4
-2
0
-4 -2 0
FU Q types in 'context classification features' space
isContextDependent
isFullySpecified
isRephrase
isTopicShift
Figure 2: Distribution of hand-annotated FU Q types in PC-based feature space (PCAB)
331
