Learning Dependency Translation Models 
as Collections of Finite-State Head 
Transducers 
Hiyan Alshawi* 
Shannon Laboratory, AT&T Labs 
Shona Douglas* 
Shannon Laboratory, AT&T Labs 
Srinivas Bangalore* 
Shannon Laboratory, AT&T Labs 
The paper defines weighted head transducers,finite-state machines that perform middle-out string 
transduction. These transducers are strictly more expressive than the special case of standard left- 
to-right finite-state transducers. Dependency transduction models are then defined as collections 
of weighted head transducers that are applied hierarchically. A dynamic programming search 
algorithm is described for finding the optimal transduction of an input string with respect to a 
dependency transduction model. A method for automatically training a dependency transduc- 
tion model from a set of input-output example strings is presented. The method first searches 
for hierarchical alignments of the training examples guided by correlation statistics, and then 
constructs the transitions of head transducers that are consistent with these alignments. Experi- 
mental results are given for applying the training method to translation from English to Spanish 
and Japanese. 
1. Introduction 
We will define a dependency transduction model in terms of a collection of weighted 
head transducers. Each head transducer is a finite-state machine that differs from 
"standard" finite-state transducers in that, instead of consuming the input string left 
to right, it consumes it "middle out" from a symbol in the string. Similarly, the output 
of a head transducer is built up middle out at positions relative to a symbol in the 
output string. The resulting finite-state machines are more expressive than standard 
left-to-right transducers. In particular, they allow long-distance movement with fewer 
states than a traditional finite-state ransducer, a useful property for the translation task 
to which we apply them in this paper. (In fact, finite-state head transducers are capable 
of unbounded movement with a finite number of states.) In Section 2, we introduce 
head transducers and explain how input-output positions on state transitions result 
in middle-out transduction. 
When applied to the problem of translation, the head transducers forming the de- 
pendency transduction model operate on input and output strings that are sequences 
of dependents of corresponding headwords in the source and target languages. The 
dependency transduction model produces ynchronized dependency trees in which 
each local tree is produced by a head transducer. In other words, the dependency 
* 180 Park Avenue, Florham Park, NJ 07932 
t 180 Park Avenue, Florham Park, NJ 07932 
180 Park Avenue, Florham Park, NJ 07932 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 1 
model applies the head transducers ecursively, imposing a recursive decomposition 
of the source and target strings. A dynamic programming search algorithm finds op- 
timal (lowest total weight) derivations of target strings from input strings or word 
lattices produced by a speech recognizer. Section 3 defines dependency transduction 
models and describes the search algorithm. 
We construct the dependency transduction models for translation automatically 
from a set of unannotated examples, each example comprising a source string and a 
corresponding target string. The recursive decomposition of the training examples 
results from an algorithm for computing hierarchical alignments of the examples, 
described in Section 4.2. This alignment algorithm uses dynamic programming search 
guided by source-target word correlation statistics as described in Section 4.1. 
Having constructed a hierarchical alignment for the training examples, a set of 
head transducer t ansitions are constructed from each example as described in Sec- 
tion 4.3. Finally, the dependency transduction model is constructed by aggregating the 
resulting head transducers and assigning transition weights, which are log probabili- 
ties computed from the training counts by simple maximum likelihood estimation. 
We have applied this method of training statistical dependency transduction mod- 
els in experiments on English-to-Spanish and English-to-Japanese translations of tran- 
scribed spoken utterances. The results of these experiments are described in Section 5; 
our concluding remarks are in Section 6. 
2. Head Transducers 
2.1 Weighted Finite-State Head Transducers 
In this section we describe the basic structure and operation of a weighted head trans- 
ducer. In some respects, this description is simpler than earlier presentations (e.g., 
Alshawi 1996); for example, here final states are simply a subset of the transducer 
states whereas in other work we have described the more general case in which final 
states are specified by a probability distribution. The simplified escription is adequate 
for the purposes of this paper. 
Formally, aweighted head transducer is a 5-tuple: an alphabet W of input symbols; 
an alphabet V of output symbols; a finite set Q of states q0 . . . . .  qs; a set of final states 
F c Q; and a finite set T of state transitions. A transition from state q to state q' has 
the form 
(q,q',w,v,o~,fl, cl
where w is a member of W or is the empty string c; v is a member of V or ?; the integer 
o~ is the input position; the integer fl is the output position; and the real number c is 
the weight or cost of the transition. A transition in which oz = 0 and fl = 0 is called a 
head transition. 
The interpretation f q, q', w, and v in transitions i similar to left-to-right transduc- 
ers, i.e., in transitioning from state q to state qt, the transducer "reads" input symbol 
w and "writes" output symbol v, and as usual if w (or v) is e then no read (respec- 
tively write) takes place for the transition. The difference lies in the interpretation f 
the read position c~ and the write position ft. To interpret the transition positions as 
transducer actions, we consider notional input and output apes divided into squares. 
On such a tape, one square is numbered 0,and the other squares are numbered 1,2 . . . .  
rightwards from square 0, and -1 , -2  . . . .  leftwards from square 0 (Figure 1). 
A transition with input position ~ and output position fl is interpreted as reading 
w from square c~ on the input tape and writing v to square fl of the output tape; if 
square fl is already occupied, then v is written to the next empty square to the left of 
46 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
<q, q' ,  w, v, a, fl,, c> 
@ o p -@ 
C 
I lw l  w0 I I 
-4 -3a,=--2-1 0 1 2 3 4 
-4 -3 -2 -1 0 1 2 ,0=3 4 
Figure 1 
Transition symbols and positions. 
fl if fl < 0, or to the right of fl if fl > 0, and similarly, if input was already read from 
position a, w is taken from the next unread square to the left of a if a < 0 or to the 
right of c~ if a ~ 0. 
The operation of a head transducer is nondeterministic. It starts by taking a head 
transition 
{q, q', w0, v0, 0, 0, c} 
where w0 is one of the symbols (not necessarily the leftmost) in the input string. (The 
valid initial states are therefore implicitly defined as those with an outgoing head 
transition.) w0 is considered to be at square 0 of the input tape and v0 is output at 
square 0 of the output tape. Further state transitions may then be taken until a final 
state in F is reached. For a derivation to be valid, it must read each symbol in the 
input string exactly once. At the end of a derivation, the output string is formed by 
taking the sequence of symbols on the target ape, ignoring any empty squares on this 
tape. 
The cost of a derivation of an input string to an output string by a weighted 
head transducer is the sum of the costs of transitions taken in the derivation. We can 
now define the string-to-string transduction function for a head transducer to be the 
function that maps an input string to the output string produced by the lowest-cost 
valid derivation taken over all initial states and initial symbols. (Formally, the function 
is partial in that it is not defined on an input when there are no derivations or when 
there are multiple outputs with the same minimal cost.) 
In the transducers produced by the training method described in this paper, the 
source and target positions are in the set {-1,0,1},  though we have also used hand- 
coded transducers (Alshawi and Xia 1997) and automatically trained transducers (A1- 
shawl and Douglas 2000) with a larger range of positions. 
2.2 Relationship to Standard FSTs 
The operation of a traditional eft-to-right ransducer can be simulated by a head 
transducer by starting at the leftmost input symbol and setting the positions of the 
first transition taken to a = 0 and fl = 0, and the positions for subsequent transitions 
to o~ = 1 and fl = 1. However, we can illustrate the fact that head transducers are more 
47 
Computational Linguistics Volume 26, Number 1 
a:a  
a:a ~ b:b 
0:0 
Figure 2 
Head transducer to reverse an input string of arbitrary length in the alphabet {a, b}. 
expressive than left-to-right transducers by the case of a finite-state head transducer 
that reverses a string of arbitrary length. (This cannot be performed by a traditional 
transducer with a finite number of states.) 
For example, the head transducer described below (and shown in Figure 2) with 
input alphabet {a, b} will reverse an input string of arbitrary length in that alphabet. 
The states of the example transducer are Q = {ql, q2} and F = {q2}, and it has the 
following transitions (costs are ignored here): 
{ql, q2,a,a,O,O} 
<ql, q2, b, b, 0, 0> 
<q2,q2,a,a,-1,1} 
(q2, q2, b, b, -1,1} 
The only possible complete derivations of the transducer read the input string right 
to left, but write it left to right, thus reversing the string. 
Another similar example is using a finite-state head transducer to convert a palin- 
drome of arbitrary length into one of its component halves. This clearly requires the 
use of an empty string on some of the output transitions. 
3. Dependency Transduction Models 
3.1 Dependency Transduction using Head Transducers 
In this section we describe dependency transduction models, which can be used for 
machine translation and other transduction tasks. These models consist of a collection 
of head transducers that are applied hierarchically. Applying the machines hierarchi- 
cally means that a nonhead transition is interpreted not simply as reading an input- 
output pair (w, v), but instead as reading and writing a pair of strings headed by (w, v) 
according to the derivation of a subnetwork. 
For example, the head transducer shown in Figure 3 can be applied recursively in 
order to convert an arithmetic expression from infix to prefix (Polish) notation (as noted 
by Lewis and Stearns \[1968\], this transduction cannot be performed by a pushdown 
transducer). 
In the case of machine translation, the transducers derive pairs of dependency 
trees, a source language dependency tree and a target dependency tree. A dependency 
tree for a sentence, in the sense of dependency grammar (for example Hays \[1964\] and 
Hudson \[1984\]), is a tree in which the words of the sentence appear as nodes (we do 
not have terminal symbols of the kind used in phrase structure grammar). In such a 
tree, the parent of a node is its head and the child of a node is the node's dependent. 
The source and target dependency trees derived by a dependency transduction 
model are ordered, i.e., there is an ordering on the nodes of each local tree. This 
48 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
b b ~C~ b:b b:b 
Figure 3 
Dependency transduction network mapping bracketed arithmetic expressions from infix to 
prefix notation. 
I I want to make a collect call I 
? , 
\[ quiero hac~ una llamada de cobr~ I 
Figure 4 
Synchronized dependency trees derived for transducing I want to make a collect call into quiero 
hacer una llamada de cobrar. 
means, in particular, that the target sentence can be constructed directly by a simple 
recursive traversal of the target dependency tree. Each pair of source and target rees 
generated is synchronized in the sense to be formalized in Section 4.2. An example is 
given in Figure 4. 
Head transducers and dependency transduction models are thus related as fol- 
lows: Each pair of local trees produced by a dependency transduction derivation is the 
result of a head transducer derivation. Specifically, the input to such a head transducer 
is the string corresponding to the flattened local source dependency tree. Similarly, the 
output of the head transducer derivation is the string corresponding to the flattened 
local target dependency tree. In other words, the head transducer is used to convert 
a sequence consisting of a headword w and its left and right dependent words to a 
sequence consisting of a target word v and its left and right dependent words (Fig- 
ure 5). Since the empty string may appear in a transition in place of a source or target 
symbol, the number of source and target dependents can be different. 
The cost of a derivation produced by a dependency transduction model is the 
sum of all the weights of the head transducer derivations involved. When applying a 
dependency transduction model to language translation, we choose the target string 
obtained by flattening the target ree of the lowest-cost dependency derivation that 
also generates the source string. 
We have not yet indicated what weights to use for head transducer t ansitions. 
The definition of head transducers as such does not constrain these. However, for a 
dependency transduction model to be a statistical model for generating pairs of strings, 
we assign transition weights that are derived from conditional probabilities. Several 
49 
Computational Linguistics Volume 26, Number 1 
Iw1 ..- wk.ll ~?1 ..-'~nl 
Iv ,  
Figure 5 
Head transducer converts the sequences of left and right dependents (wl ... wk-l/ and 
(wk+i ? ? ? w,) of w into left and right dependents (vl... vj-1) and {Vj+I... Vp) of v. 
probabilistic parameterizations can be used for this purpose including the following 
for a transition with headwords w and v and dependent words w' and v': 
P(q', w', v', fllw, v, q). 
Here q and q' are the from-state and to-state for the transition and a and fl are the 
source and target positions, as before. We also need parameters P(q0, ql\]w, v) for the 
probability of choosing a head transition 
(qo, ql, w,v,O,O) 
given this pair of headwords. To start the derivation, we need parameters 
P(roots(wo, vo)) for the probability of choosing w0,v0 as the root nodes of the two 
trees. 
These model parameters can be used to generate pairs of synchronized epen- 
dency trees starting with the topmost nodes of the two trees and proceeding recur- 
sively to the leaves. The probability of such a derivation can be expressed as: 
P( oots(wo, vo) )P(Dwo,vo) 
where P(Dw,v) is the probability of a subderivation headed by w and v, that is 
P(Dw,v) = P(qo, qllw, v) H P(qi+l, Wi, Vi,~i, fli\]w,v, qi)P(Dwi,vl) 
1K i ln  
for a derivation in which the dependents of w and v are generated by n transitions. 
3.2 Transduction Algorithm 
To carry out translation with a dependency transduction model, we apply a dynamic 
programming search to find the optimal derivation. This algorithm can take as input 
either word strings, or word lattices produced by a speech recognizer. The algorithm 
is similar to those for context-free parsing such as chart parsing (Earley 1970) and 
the CKY algorithm (Younger 1967). Since word string input is a special case of word 
lattice input, we need only describe the case of lattices. 
We now present a sketch of the transduction algorithm. The algorithm works 
bottom-up, maintaining a set of configurations. A configuration has the form 
In1, n2, w, v, q, c, t\] 
corresponding to a bottom-up artial derivation currently in state q covering an input 
sequence between nodes nl and n2 of the input lattice, w and v are the topmost 
50 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
nodes in the source and target derivation trees. Only the target ree t is stored in the 
configuration. 
The algorithm first initializes configurations for the input words, and then per- 
forms transitions and optimizations to develop the set of configurations bottom-up: 
Initialization: For each word edge between odes n and n ~ in the lattice 
with source word w0, an initial configuration is constructed for any head 
transition of the form 
(q, q', w0, v0, 0, 0, c} 
Such an initial configuration has the form: 
\[n, n t, w0, v0, q~, c, v0\] 
Transition: We show the case of a transition in which a new configuration 
results from consuming a source dependent wl to the left of a headword 
w and adding the corresponding target dependent Vl to the right of the 
target head v. Other cases are similar. The transition applied is: 
(q, q~, Wl, Vl, -1,1, c'} 
It is applicable when there are the following head and dependent 
configurations: 
\[n2,n3,w,v,q,c,t\] 
\[nl, n2, Wl, Vl, qf, Cl, tl\] 
where the dependent configuration is in a final state qf. The result of 
applying the transition is to add the following to the set of 
configurations: 
In1, n3, w, v, q', c + Cl q- C', t'\] 
where Y is the target dependency tree formed by adding tl as the 
rightmost dependent of t. 
Optimization: We also require a dynamic programming condition to 
remove suboptimal (sub)derivations. Whenever there are two 
configurations 
\[n, n', w, v, q, Cl, tl\] 
\[n, n', w, v, q, C2, t2\] 
and c2 > Cl, the second configuration is removed from the set of 
configurations. 
If, after all applicable transitions have been taken, there are configurations span- 
ning the entire input lattice, then the one with the lowest cost is the optimal derivation. 
When there are no such configurations, we take a pragmatic approach in the trans- 
lation application and simply concatenate the lowest costing of the minimal length 
sequences of partial derivations that span the entire lattice. A Viterbi-like search of 
the graph formed by configurations i used to find the optimal sequence of deriva- 
tions. One of the advantages ofmiddle-out transduction is that robustness i improved 
through such use of partial derivations when no complete derivations are available. 
51 
Computational Linguistics Volume 26, Number 1 
4. Training Method 
Our training method for head transducer models only requires a set of training exam- 
ples. Each example, or bitext, consists of a source language string paired with a target 
language string. In our experiments, the bitexts are transcriptions of spoken English 
utterances paired with their translations into Spanish or Japanese. 
It is worth emphasizing that we do not necessarily expect he dependency repre- 
sentations produced by the training method to be traditional dependency structures 
for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see 
below) dependency representations that are appropriate to performing the translation 
task for a specific language pair or specific bilingual corpus. For example, headwords 
in both languages are chosen to force a synchronized alignment (for better or worse) 
in order to simplify cases involving so-called head-switching. This contrasts with one 
of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the transla- 
tion problem, i.e., the approach in which translation problems are seen in terms of 
bridging the gap between the most natural monolingual representations underlying 
the sentences of each language. 
The training method has four stages: (i) Compute co-occurrence statistics from the 
training data. (ii) Search for an optimal synchronized hierarchical alignment for each 
bitext. (iii) Construct a set of head transducers that can generate these alignments with 
transition weights derived from maximum likelihood estimation. 
4.1 Computing Pairing Costs 
For each source word w in the data set, assign a cost, the translation pairing cost 
c(w, v) for all possible translations v into the target language. These translations of the 
source word may be zero, one, or several target language words (see Section 4.4 for 
discussion of the multiword case). The assignment of translation pairing costs (effec- 
tively a statistical bilingual dictionary) may be done using various statistical measures. 
For this purpose, a suitable statistical function needs to indicate the strength of co- 
occurrence correlation between source and target words, which we assume is indicative 
of carrying the same semantic ontent. Our preferred choice of statistical measure for 
assigning the costs is the ~ correlation measure (Gale and Church 1991). We apply 
this statistic to co-occurrence of the source word with all its possible translations in 
the data set examples. We have found that, at least for our data, this measure leads to 
better performance than the use of the log probabilities of target words given source 
words (cf. Brown et al 1993). 
In addition to the correlation measure, the cost for a pairing includes a distance 
measure component that penalizes pairings proportionately to the difference between 
the (normalized) positions of the source and target words in their respective sentences. 
4.2 Computing Hierarchical Alignments 
As noted earlier, dependency transduction models are generative probabilistic models; 
each derivation generates a pair of dependency trees. Such a pair can be represented 
as a synchronized hierarchical alignment of two strings. A hierarchical alignment 
consists of four functions. The first two functions are an alignment mapping f from 
source words w to target words f(w) (which may be the empty string ~), and an 
inverse alignment mapping from target words v to source words fr(v). The inverse 
mapping is needed to handle mapping of target words to ~; it coincides wi thf  for pairs 
without source ~. The other two functions are a source head-map g mapping source 
dependent words w to their heads g(w) in the source string, and a target head-map 
h mapping target dependent words v to their headwords h(v) in the target string. An 
52 
Alshawi, Bangalore, and Douglas Leaning Dependency Translation Models 
g 
show me nonstop flights to boston 
muestreme los vuelos sin escalas a boston 
g 
show me z ' \  
muestr~me 
nonstop flights to boston 
los vuelos sin escalas a boston 
Figure 6 
A hierarchical alignment: alignment mappings f and f', and head-maps g and h. 
example hierarchical alignment is shown in Figure 6 (f and f '  are shown separately 
for clarity). 
A hierarchical alignment is synchronized (i.e., it corresponds to synchronized e- 
pendency trees) if these conditions hold: 
Nonover lap :  If wl # w2, thenf(wl) f(w2), and similarly, if Vl  V2, thenf'(vl) # 
d'(v2). 
Synchron izat ion :  if f (w) = v and v # e, then f(g(w)) = h(v), and f'(v) = w. 
Similarly, ifd'(v) = w and w # e, thend'(h(v)) = g(w), andf(w) = v. 
Phrase  cont igu i ty :  The image under f of the maximal substring dominated by a 
headword w is a contiguous egment of the target string. 
(Here w and v refer to word tokens not symbols (types). We hope that the context of 
discussion will make the type-token distinction clear in the rest of this article.) The 
hierarchical alignment in Figure 6 is synchronized. 
Of course, translations of phrases are not always transparently related by a hier- 
archical alignment. In cases where the mapping between a source and target phrase is 
unclear (for example, one of the phrases might be an idiom), then the most reasonable 
choice of hierarchical alignment may be for f and f '  to link the heads of the phrases 
only, all the other words being mapped to e, with no constraints on the monolingual 
head mappings h and g. (This is the approach we take to compound lexical pairings, 
discussed in Section 4.4.) 
In the hierarchical alignments produced by the training method described here, 
the source and target strings of a bitext are decomposed into three aligned regions, 
as shown in Figure 7: a head region consisting of headword w in the source and its 
corresponding targetf(w) in the target string, a left substring region consisting of the 
source substring to the left of w and its projection under f on the target string, and 
a right substring region consisting of the source substring to the right of w and its 
projection under f  on the target string. The decomposition is recursive in that the left 
substring region is decomposed around a left headword wl, and the right substring 
53 
Computational Linguistics Volume 26, Number 1 
\[ 
Figure 7 
Decomposing source and target strings around heads w and f(w). 
region is decomposed around a right headword Wr. This process of decomposition 
continues for each left and right substring until it only contains a single word. 
For each bitext there are, in general, multiple such recursive decompositions that 
satisfy the synchronization constraints for hierarchical alignments. We wish to find 
such an alignment hat respects the co-occurrence statistics of bitexts as well as the 
phrasal structure implicit in the source and target strings. For this purpose we define 
a cost function on hierarchical alignments. The cost function is the sum of three terms. 
The first term is the total of all the translation pairing costs c(w,f(w)) of each source 
word w and its translation f(w) in the alignment; the second term is proportional to 
the distance in the source string between dependents wd and their heads g(wa); and the 
third term is proportional to the distance in the target string between target dependent 
words va and their heads h(va). 
The hierarchical alignment hat minimizes this cost function is computed using 
a dynamic programming procedure. In this procedure, the pairing costs are first re- 
trieved for each possible source-target pair allowed by the example. Adjacent source 
substrings are then combined to determine the lowest-cost subalignments for suc- 
cessively larger substrings of the bitext satisfying the constraints tated above. The 
successively larger substrings eventually span the entire source string, yielding the 
optimal hierarchical alignment for the bitext. This procedure has O(n 6) complexity 
in the number of words in the source (or target) sentence. In Alshawi and Douglas 
(2000) we describe a version of the alignment algorithm in which heads may have 
an arbitrary number of dependents, and in which the hierarchical alignments for the 
training corpus are refined by iterative reestimation. 
4.3 Constructing Transducers 
Building a head transducer involves creating appropriate head transducer states and 
tracing hypothesized head transducer transitions between them that are consistent 
with the hierarchical alignment of a bitext. 
The main transitions that are traced in our construction are those that map heads, 
wl and Wr, of the right and left dependent phrases of w to their translations as indi- 
cated by the alignment function f in the hierarchical alignment. The positions of the 
dependents in the target string are computed by comparing the positions off(wt) and 
f(Wr) to the position of v = f(w). 
In order to generalize from instances in the training data, some model states aris- 
ing from different raining instances are shared. In particular, in the construction de- 
scribed here, for a given pair (w, v) there is only one final state. (We have also tried 
using automatic word-clustering techniques to merge states further, but for the lim- 
ited domain corpora we have used so far, the results are inconclusive.) To specify 
54 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
? \oo 
Figure 8 
+1 :+1 -1 :+ 1 
States and transitions constructed for the "swapping" decomposition shown in Figure 7. 
the sharing of states we make use of a one-to-one state-naming function ? from se- 
quences of strings to transducer states. The same state-naming function is used for 
all examples in the data set, ensuring that the transducer fragments recorded for 
the entire data set will form a complete collection of head transducer transition et- 
works. 
Figure 7 shows a decomposition i which w has a dependent to either side, v 
has both dependents to the right, and the alignment is "swapping" (f(wl) is to the 
right off(wr)). The construction for this decomposition case is illustrated in Figure 8 
as part of a finite-state transition diagram, and described in more detail below. (The 
other transition arrows shown in the diagram will arise from other bitext alignments 
containing (w,f(w)) pairings.) Other cases covered by our algorithm (e.g., a single left 
source dependent but no right source dependent, or target dependents on either side 
of the target head) are simple variants. 
The detailed construction is as follows: 
1. Construct a transition from sl = ?(initial) to S 2 = O ' (w, f (w) ,  head) mapping 
the source headword w to the target head f(w) at position 0 in source 
and target. (In our training construction there is only one initial state sl.) 
2. Since the target dependentf(wr) is to the left of target dependentf(wl) 
(and we are restricting positions to {-1, 0, +1}) the Wr transition is 
constructed first in order that the target dependent nearest he head is 
output first. 
Construct a transition from s2 to s3 = c~(w,f(w), swapping, Wr,f(Wr) 
mapping the source dependent Wr at position +1 to the target dependent 
f(Wr) at position +1. 
3. Construct a transition from s3 to s4 = cr(w,f(w),final) mapping the source 
dependent wl at position -1 to the target dependentf(wl) at position +1. 
If instead the alignment had been as in Figure 9, in which the source dependents 
are mapped to target dependents in a parallel rather than swapping configuration 
(the configuration of sin escalas and Boston around flights:los vuelos in Figure 6), the 
construction is the same, except for the following differences: 
. 
. 
Since the target dependentf(wl) is to the left of target dependentf(Wr), 
the wl transition is constructed first in order that the target dependent 
nearest he head is output first. 
The source and target positions are as shown in Figure 10. Instead of 
s ta te  s3, we use a different state ss = ?(w,f(w),parallel, wl,f(wl)). 
55 
Computational Linguistics Volume 26, Number 1 
\[ "'" l \ [ \ ] \ [  ..-4..- \] 
j J  
Figure 9 
Decomposing source and target strings around heads w and f(w)--"parallel'. 
w :f(w) ?\oo 
Figure 10 
-1 :+1 
w / f (w , ) 
+1 :+ 1 
States and transitions constructed for the "parallel" decomposition shown in Figure 9. 
Other states are the same as for the first case. The resulting states and transitions are 
shown in Figure 10. 
After the construction described above is applied to the entire set of aligned bi- 
texts in the training set, the counts for transitions are treated as event observation 
counts of a statistical dependency transduction model with the parameters described 
in Section 3.1. More specifically, the negated logs of these parameters are used as the 
weights for transducer t ansitions. 
4.4 Mult iword Pairings 
In the translation application, source word w and target word v are generalized so 
they can be short substrings (compounds) of the source and target strings. Exam- 
ples of such multiword pairs are show me:muestrdme and nonstop:sin escalas in Fig- 
ure 6. The cost for such pairings still uses the same ~ statistic, now taking the ob- 
servations to be the co-occurrences of the substrings in the training bitexts. How- 
ever, in order that these costs can be comparable to the costs for simple pairings, 
they are multiplied by the number of words in the source substring of the pair- 
ing. 
The use of compounds in pairings does not require any fundamental changes to 
the hierarchical lignment dynamic programming algorithm, which simply produces 
dependency trees with nodes that may be compounds. In the transducer construction 
phase of the training method, one of the words of a compound is taken to be the pri- 
mary or "real" headword. (In fact, we take the least common word of a compound to 
be its head.) An extra chain of transitions i constructed to transduce the other words 
of compounds, if necessary using transitions with epsilon strings. This compilation 
means that the transduction algorithm is unaffected by the use of compounds when 
aligning training data, and there is no need for a separate compound identification 
phase when the transduction algorithm is applied to test data. Some results for dif- 
ferent choices of substring lengths can be found in Alshawi, Bangalore, and Douglas 
(1998). 
56 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
5. Experiments 
5.1 Evaluation Method 
In order to reduce the time required to carry out training evaluation experiments, 
we have chosen two simple, string-based evaluation metrics that can be calculated 
automatically. These metrics, simple accuracy and translation accuracy, are used to 
compare the target string produced by the system against a reference human transla- 
tion from held-out data. 
Simple accuracy is computed by first finding a transformation f one string into 
another that minimizes the total weight of insertions, deletions, and substitutions. (We 
use the same weights for these operations as in the NIST ASR evaluation software 
\[National Institute of Standards and Technology 1997\].) Translation accuracy includes 
transpositions (i.e., movement) of words as well as insertions, deletions, and substi- 
tutions. We regard the latter metric as more appropriate for evaluation of translation 
systems because the simple metric would count a transposition as two errors: an in- 
sertion plus a deletion. (This issue does not arise for speech recognizers because these 
systems do not normally make transposition errors.) 
For the lowest edit-distance transformation between the reference translation and 
system output, if we write I for the number of insertions, D for deletions, S for substi- 
tutions, and R for number of words in the reference translation string, we can express 
simple accuracy as 
simple accuracy = 1 - ( I  + D + S) /R .  
Similarly, if T is the number of transpositions in the lowest weight transformation 
including transpositions, we can express translation accuracy as 
translation accuracy = 1 - ( I  ~ + D ~ + S + T) /R .  
Since a transposition corresponds toan insertion and a deletion, the values of I ~ and D ~ 
for translation accuracy will, in general, be different from I and D in the computation of
simple accuracy. For Spanish, the units for string operations in the evaluation metrics 
are words, whereas for Japanese they are Japanese characters. 
5.2 English-to-Spanish 
The training and test data for the English-to-Spanish experiments were taken from 
a set of transcribed utterances from the Air Travel Information System (ATIS) corpus 
together with a translation of each utterance to Spanish. An utterance is typically asin- 
gle sentence but is sometimes more than one sentence spoken in sequence. Alignment 
search and transduction training was carried out only on bitexts with sentences up 
to length 20, a total of 13,966 training bitexts. The test set consisted of 1,185 held-out 
bitexts at all lengths. Table 1 shows the word accuracy percentages ( ee Section 5.1) 
for the trained model, e2s, against he original held-out ranslations at various source 
sentence l ngths. Scores are also given for a "word-for-word" baseline, sww, in which 
each English word is translated by the most highly correlated Spanish word. 
5.3 English-to-Japanese 
The training and test data for the English-to-Japanese experiments was a set of tran- 
scribed utterances of telephone service customers talking to AT&T operators. These 
utterances, collected from real customer-operator interactions, tend to include frag- 
mented language, restarts, etc. Both training and test partitions were restricted to bi- 
texts with at most 20 English words, giving 12,226 training bitexts and 3,253 held-out 
test bitexts. In the Japanese text, we introduce "word" boundaries that are convenient 
57 
Computational Linguistics Volume 26, Number 1 
Table 1 
Simple accuracy/translation accuracy (percent) for the trained 
English-to-Spanish model (e2s) against he word-for-word baseline 
(sww). 
Length < 5 < 10 G 15 < 20 All 
sww 45.1/45.8 46.7/48.6 46.5/48.2 45.5/47.1 45.2/46.9 
e2s 75.4/75.8 76.3/78.0 75.4/77.0 74.4/76.0 73.3/75.0 
Table 2 
Simple accuracy/translation accuracy as percentages of Japanese 
characters, for the trained English-to-Japanese model (e2j) and the 
word-for-word baseline (jww). 
Length G 5 < 10 G 15 ~ 20 All 
jww 75.8/78.0 45.2/50.4 40.0/45.4 37.2/42.8 37.2/42.8 
e2j 89.2/89.7 74.0/76.6 68.6/72.2 66.4/70.1 66.4/70.1 
for the training process. These word boundaries are parasitic on the word boundaries 
in the English transcriptions: the translators are asked to insert such a word boundary 
between any two Japanese characters that are taken to have arisen from the translation 
of distinct English words. This results in bitexts in which the number of multichar- 
acter Japanese "words" is at most the number of English words. However, as noted 
above, evaluation of the Japanese output is done with Japanese characters, i.e., with 
the Japanese text in its natural format. Table 2 shows the Japanese character accuracy 
percentages for the trained English-to-Japanese model, e2j, and a baseline model, jww, 
which gives each English word its most highly correlated translation. 
5.4 Note on Experimental Setting 
The vocabularies in these English-Spanish and English-Japanese experiments are only 
a few thousand words; the utterances are fairly short (an average of 7.3 words per utter- 
ance) and often contain errors typical of spoken language. So while the domains may 
be representative of task-oriented ialogue settings, further experimentation would 
be needed to assess the effectiveness of our method in situations uch as translat- 
ing newspaper articles. In terms of the training data required, Tsukada et al (1999) 
provide indirect empirical evidence suggesting accuracy can be further improved by 
increasing the size of our training sets, though also suggesting that the learning curve 
is relatively shallow beyond the current size of corpus. 
6. Concluding Remarks 
Formalisms for finite-state and context-free transduction have a long history (e.g., 
Lewis and Stearns 1968; Aho and Ullman 1972), and such formalisms have been ap- 
plied to the machine translation problem, both in the finite-state case (e.g., Vilar et al 
1996) and the context-free case (e.g., Wu 1997). In this paper we have added to this 
line of research by providing a method for automatically constructing fully lexicalized 
statistical dependency transduction models from training examples. 
Automatically training a translation system brings important benefits in terms of 
maintainability, robustness, and reducing expert coding effort as compared with tra- 
58 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
ditional rule-based translation systems (a number of which are described in Hutchins 
and Somers \[1992\]). The reduction of effort results, in large part, from being able 
to do without artificial intermediate representations of meaning; we do not require 
the development of semantic mapping rules (or indeed any rules) or the creation of 
a corpus including semantic annotations. Compared with left-to-right ransduction, 
middle-out ransduction also aids robustness because, when complete derivations are 
not available, partial derivations tend to have meaningful headwords. 
At the same time, we believe our method has advantages over the approach de- 
veloped initially at IBM (Brown et al 1990; Brown et al 1993) for training translation 
systems automatically. One advantage is that our method attempts to model the nat- 
ural decomposition of sentences into phrases. Another is that the compilation of this 
decomposition i to lexically anchored finite-state head transducers produces imple- 
mentations that are much more efficient han those for the IBM model. In particular, 
our search algorithm finds optimal transductions of test sentences in less than "real 
time" on a 300MHz processor, that is, the time to translate an utterance is less than 
the time taken to speak it, an important consideration for our speech translation ap- 
plication. 
References 
Aho, Alfred V. and Jeffrey D. Ullman. 1972. 
The Theory o/Parsing, Translation, and 
Compiling. Prentice-Hall, Englewood 
Cliffs, NJ. 
Alshawi, H. 1996. Head automata for 
speech translation. In Proceedings ofthe 
International Conference on Spoken Language 
Processing, pages 2360-2364, Philadelphia, 
PA. 
Alshawi, H., S. Bangalore, and S. Douglas. 
1998. Learning phrase-based head 
transduction models for translation of 
spoken utterances. In Proceedings ofthe 
International Conference on Spoken Language 
Processing, pages 2767-2770, Sydney, 
Australia. 
Alshawi, H. and S. Douglas. 2000. Learning 
dependency transduction models from 
unannotated examples. Philosophical 
Transactions ofthe Royal Society (Series A: 
Mathematical, Physical and Engineering 
Sciences). To appear. 
Alshawi, Hiyan and Fei Xia. 1997. 
English-to-Mandarin speech translation 
with head transducers. In Proceedings of
the Workshop on Spoken Language 
Translation, Madrid, Spain. 
Brown, P. J., J. Cocke, S. A. Della Pietra, 
V. J. Della Pietra, J. Lafferty, R. L. Mercer, 
and P. Rossin. 1990. A statistical approach 
to machine translation. Computational 
Linguistics, 16(2):79-85. 
Brown, P. J., S. A. Della Pietra, V. J. Della 
Pietra, and R. L. Mercer. 1993. The 
mathematics ofmachine translation: 
Parameter estimation. Computational 
Linguistics, 16(2):263-312. 
Dorr, B. J. 1994. Machine translation 
divergences: A formal description and 
proposed solution. Computational 
Linguistics, 20(4):597-634. 
Earley, J. 1970. An efficient context-free 
parsing algorithm. Communications of the 
ACM, 13(2):94-102. 
Gale, W. A. and K. W. Church. 1991. 
Identifying word correspondences in 
parallel texts. In Proceedings ofthe Fourth 
DARPA Speech and Natural Language 
Processing Workshop, ages 152-157, Pacific 
Grove, CA. 
Hays, D. G. 1964. Dependency theory: A 
formalism and some observations. 
Language, 40:511-525. 
Hudson, R. A. 1984. Word Grammar. 
Blackwell, Oxford. 
Hutchins, W. J. and H. L. Somers. 1992. An 
Introduction to Machine Translation. 
Academic Press, New York. 
Lewis, P. M. and R. E. Stearns. 1968. 
Syntax-directed transduction. Journal of the 
Association for Computing Machinery, 
15(3):465-488. 
National Institute of Standards and 
Technology. 1997. Spoken Natural 
Language Processing Group Web page. 
http://www.itl.nist.gov/div894. 
Tsukada, Hajime, Hiyan Alshawi, Shona 
Douglas, and Srinivas Bangalore. 1999. 
Evaluation of machine translation system 
based on a statistical method by using 
spontaneous speech transcription. In
Proceedings ofthe Fall Meeting of the 
Acoustical Society of Japan, pages 115-116, 
September. 
Vilar, J. M., V. M. Jim~nez, J. C. Amengual, 
A. Castellanos, D. Llorens, and E. Vidal. 
1996. Text and speech translation by 
59 
Computational Linguistics Volume 26, Number 1 
means of subsequential transducers. 
Natural Language Engineering, 2(4):351-354. 
Watanabe, Hideo. 1995. A model of a 
bi-directional transfer mechanism using 
rule combination. Machine Translation, 
10(4):269-291. 
Wu, Dekai.1997. Stochastic inversion 
transduction grammars and bilingual 
parsing of parallel corpora. Computational 
Linguistics, 23(3):377-404. 
Younger, D. 1967. Recognition and Parsing 
of Context-Free Languages in Time n 3. 
Information and Control, 10:189-208. 
60 
Active Learning for Classifying Phone Sequences from Unsupervised
Phonotactic Models
Shona Douglas
AT&T Labs - Research
Florham Park, NJ 07932, USA
shona@research.att.com
Abstract
This paper describes an application of active
learning methods to the classification of phone
strings recognized using unsupervised phono-
tactic models. The only training data required
for classification using these recognition meth-
ods is assigning class labels to the audio files.
The work described here demonstrates that
substantial savings in this effort can be ob-
tained by actively selecting examples to be la-
beled using confidence scores from the Boos-
Texter classifier. The saving in class label-
ing effort is evaluated on two different spo-
ken language system domains in terms both of
the number of utterances to be labeled and the
length of the labeled utterances in phones. We
show that savings in labeling effort of around
30% can be obtained using active selection of
examples.
1 Introduction
A major barrier to the rapid and cost-effective develop-
ment of spoken language processing applications is the
need for time-consuming and expensive human transcrip-
tion and annotation of collected data. Extensive transcrip-
tion of audio is generally undertaken to provide word-
level labeling to train recognition models. Applications
that use statistically trained classification as a component
of an understanding system also require this transcribed
text to train on, plus an assignment of class labels to each
utterance.
In recent work by Alshawi (2003) reported in this con-
ference, new methods for unsupervised training of phone
string recognizers have been developed, removing the
need for word-level transcription. The phone-string out-
put of such recognizers has been used in classification
tasks using the BoosTexter text classification algorithm,
giving utterance classfication accuracy that is surprisingly
close to that obtained using conventionally trained word
trigram models requiring transcription. The only train-
ing data required for classification using these recogni-
tion methods is assigning class labels to the audio files.
The aim of the work described in this paper is to amplify
this advantage by reducing the amount of effort required
to train classifiers for phone-based systems by actively
selecting which utterances to assign class labels. Active
learning has been applied to classification problems be-
fore (McCallum and Nigam, 1998; Tur et al, 2003), but
not to classifiying phone strings.
2 Unsupervised Phone Recognition
Unsupervised recognition of phone sequences is car-
ried out according to the method described by
Alshawi (2003). In this method, the training inputs to
recognition model training are simply the set of audio
files that have been recorded from the application.
The recognition training phase is an iterative procedure
in which a phone n-gram model is refined successively:
The phone strings resulting from the current pass over the
speech files are used to construct the phone n-gram model
for the next iteration. We currently only re-estimate the n-
gram model, so the same general-purpose HMM acoustic
model is used for ASR decoding in all iterations.
Recognition training can be briefly described as fol-
lows. First, set the phone sequence model to an initial
phone string model. This initial model used can be an
unweighted phone loop or a general purpose phonotac-
tic model for the language being recognized. Then, for
successively larger n-grams, produce the output set of
phone sequences from recognizing the training speech
files with the current phone sequence model, and train the
next larger n-gram phone sequence model on this output
corpus.
3 Training phone sequence classifiers with
active selection of examples
The method we use for training the phone sequence clas-
sifier is as follows.
1. Choose an initial subset S of training recordings at
random; assign class label(s) to each example.
2. Recognize these recordings using the phone recog-
nizer described in section 2.
3. Train an initial classifier C on the pairs (phone
string, class label) of S.
4. Run the classifier on the recognized phone strings of
the training corpus, obtaining confidence scores for
each classification.
5. While labeling effort is available, or until per-
formance on a development corpus reaches some
threshold,
(a) Choose the next subset S? of examples from of
the training corpus, on the basis of the confi-
dence scores or other indicators. (Selection cri-
teria are discussed later.)
(b) Assign class label(s) to each selected example.
(c) Train classifier C ? on all the data labeled so far.
(d) Run C ? on the whole training corpus, obtaining
confidence scores for each classification.
(e) Optionally test C ? on a separate test corpus.
4 Experimental Setup
The datasets tested on and the classifier used are the same
as those in the experiments on phone sequence classifica-
tion reported by Alshawi (2003). The details are briefly
restated here.
4.1 Data
Two collections of utterances from two domains were
used in the experiments:
1. Customer care utterances (HMIHY). These utter-
ances are the customer side of live English conversations
between AT&T residential customers and an automated
customer care system. This system is open to the public
so the number of speakers is large (several thousand).
The total number of training utterances was 40,106.
All tests use 9724 test utterances. Average utterance
length was 11.19 words; there were 56 classes, with an
average of 1.09 classes per utterance.
2. Text-to-Speech Help Desk utterances (TTSHD).
This is a smaller database of utterances in which cus-
tomers called an automated information system primar-
ily to find out about AT&T Natural Voices text-to-speech
synthesis products.
The total number of possible training utterances was
10,470. All tests use 5005 test utterances. Average utter-
ance length was 3.95 words; there were 54 classes, with
an average of 1.23 classes per utterance.
4.2 Phone sequences
The phone sequences used for testing and training are
those obtained using the phone recognizer described in
section 2. Since the phone recognizer is trained with-
out labeling of any sort, we can use all available train-
ing utterances to train it, that is, 40,106 in the HMIHY
domain and 10,470 in the TTSHD domain. The initial
model used to start the iteration is, as in (Alshawi, 2003),
an unweighted phone loop.
4.3 Classifier
For the experiments reported here we use the BoosT-
exter classifier (Schapire and Singer, 2000). The fea-
tures used were identifiers corresponding to prompts, and
phone n-grams up to length 4. Following Schapire and
Singer (2000), the confidence level for a given prediction
is taken to be the difference between the scores assigned
by BoosTexter to the highest ranked action (the predicted
action) and the next highest ranked action.
4.4 Selection criteria
Subsets of the recognized phone sequences were selected
to be assigned class labels and used in training the clas-
sifiers. Examples were selected in order of BoosTex-
ter confidence score, least confident first. Further selec-
tion by utterance length was also used in some experi-
ments such that only recognized utterances with less than
a given number of phones were selected.
5 Experiments
5.1 Evaluation metrics
We are interested in comparing the performance for a
given amount of labeling effort of classifiers trained on
random selection of examples with that of classifiers
trained on examples chosen according to the confidence-
based method described in section 3.
The basic measurements are:
A(e): the classification accuracy at a given labeling
effort level e of the classifier trained on actively selected
labeling examples.
R(e): the classification accuracy at a given labeling
effort level e of the classifier trained on randomly selected
labeling examples.
A?1(R(e)): the effort required to achieve the perfor-
mance of random selection at effort e, using active learn-
ing.
Derived from these is the main comparison we are in-
terested in:
Effort A R A?1(R) Effort
(utt) (%) (%) (utt) Ratio
2000 67.4 66.0 1128 0.56
4000 69.6 68.0 2678 0.67
Table 1: HMIHY, no length limit, effort is number of ut-
terances
Effort A R A?1(R) Effort
(phn) (%) (%) (phn) Ratio
68032 67.0 66.1 52940 0.78
128636 69.3 67.9 91057 0.71
Table 2: HMIHY, length limited, effort is number of
phones
EffortRatio(e) = A?1(R(e))/e: the proportion of the
effort that would be required to achieve the performance
of random selection at effort e, actually required using
active learning: that is, low is good.
We use two metrics for labeling effort: the number
of utterances to be labeled and the number of phones in
those utterances. The number of phones is indicative of
the length of the audio file that must be listened to in order
to make the class label assignment, so this is relevant to
assessing just how much real effort is saved by any active
learning technique.
5.2 Results
Table 1 gives the results for selected levels of labeling ef-
fort in the HMIHY domain, calculated in terms of number
of utterances labeled.
These results suggest that we can achieve the same
accuracy as random labeling with around 60% of the
effort by active selection of examples according to the
confidence-based method described in section 3.
However, a closer inspection of the chosen examples
reveals that, on average, the actively selected utterances
are nearly 1.5 times longer than the random selection in
terms of number of phones. (This is not suprising given
that the classification method performs much worse on
longer utterances, and the confidence levels reflect this.)
In order to overcome this we introduce as part of the se-
lection criteria a length limit of 50 phones. This allows us
to retain appreciable effort savings as shown in table 2.
The TTSHD application is considerably less complex
than HMIHY, and this may be reflected in the greater sav-
ings obtained using active learning. Tables 3 and 4 show
the corresponding results for this domain.
There is also a smaller variation in utterance length be-
tween actively and randomly selected training examples
(more like 110% than the 150% for HMIHY); table 4
shows that defining effort in terms of number of phones
still results in appreciable savings for active learning. (In-
Effort A R A?1(R) Effort
(utt) (%) (%) (utt) Ratio
2000 78.9 77.5 1327 0.66
4000 80.3 78.8 1971 0.49
Table 3: TTSHD, effort is number of utterances
Effort A R A?1(R) Effort
(phn) (%) (%) (phn) Ratio
35877 78.9 77.9 27019 0.75
71338 80.3 79.1 48267 0.68
Table 4: TTSHD, effort is number of phones
corporating a length limit gave little additional benefit
here.)
6 Discussion
By actively choosing the examples with the lowest con-
fidence scores first, we can get the same classification
results with around 60-70% of the utterances labeled in
HMIHY and TTSHD. But we want to optimize labeling
effort, which is presumably some combination of a fixed
amount of effort per utterance plus a ?listening effort?
proportional to utterance length. We therefore augmented
our active learning selection to include a constraint on the
length of the utterances, measured in recognized phones.
If we simply take effort to be proportional to the number
of phones in the utterances selected (likely to result in a
conservative estimate of savings), the effort reduction at
4,000 utterances is around 30% even for the more com-
plex HMIHY domain. Further investigation is needed
into the best way to measure overall labeling effort, and
into refinements of the active learning process to optimize
that labeling effort.
References
H. Alshawi. 2003. Effective utterance classification with
unsupervised phonotactic models. In HLT-NAACL
2003, Edmonton, Canada.
A. K. McCallum and K. Nigam. 1998. Employing EM
in pool-based active learning for text classification. In
Proceedings of the 15th International Conference on
Machine Learning, pages 350?358.
R. E. Schapire and Y. Singer. 2000. BoosTexter: A
boosting-based system for text categorization. Ma-
chine Learning, 39(2/3):135?168.
Gokhan Tur, Robert E. Schapire, , and Dilek Hakkani-
Tur. 2003. Active learning for spoken language un-
derstanding. In Proceedings of International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP?03), Hong Kong, April. (to appear).
Variant Transduction: A Method for Rapid Development of
Interactive Spoken Interfaces
Hiyan Alshawi and Shona Douglas
AT&T Labs Research
180 Park Avenue
Florham Park, NJ 07932, USA
fhiyan,shonag@research.att.com
Abstract
We describe an approach (\vari-
ant transduction") aimed at reduc-
ing the eort and skill involved
in building spoken language inter-
faces. Applications are created
by specifying a relatively small set
of example utterance-action pairs
grouped into contexts. No interme-
diate semantic representations are
involved in the specication, and
the conrmation requests used in
the dialog are constructed automat-
ically. These properties of vari-
ant transduction arise from combin-
ing techniques for paraphrase gen-
eration, classication, and example-
matching. We describe how a spo-
ken dialog system is constructed
with this approach and also provide
some experimental results on vary-
ing the number of examples used to
build a particular application.
1 Introduction
Developing non-trivial interactive spoken lan-
guage applications currently requires signi-
cant eort, often several person-months. A
major part of this eort is aimed at coping
with variation in the spoken language input
by users. One approach to handling varia-
tion is to write a large natural language gram-
mar manually and hope that its coverage is
sucient for multiple applications (Dowding
et al, 1994). Another approach is to cre-
ate a simulation of the intended system (typ-
ically with a human in the loop) and then
record users interacting with the simulation.
The recordings are then transcribed and an-
notated with semantic information relating to
the domain; the transcriptions and annota-
tions can then be used to create a statistical
understanding model (Miller et al, 1998) or
used as guidance for manual grammar devel-
opment (Aust et al, 1995).
Building mixed initiative spoken language
systems currently usually involves the design
of semantic representations specic to the ap-
plication domain. These representations are
used to pass data between the language pro-
cessing components: understanding, dialog,
conrmation generation, and response gener-
ation. However, such representations tend to
be domain-specic, and this makes it dicult
to port to new domains or to use machine
learning techniques without extensive hand-
labeling of data with the semantic represen-
tations. Furthermore, the use of intermediate
semantic representations still requires a nal
transduction step from the intermediate rep-
resentation to the action format expected by
the application back-end (e.g. SQL database
query or procedure call).
For situations when the eort and exper-
tise available to build an application is small,
the methods mentioned above are impracti-
cal, and highly directed dialog systems with
little allowance for language variability are
constructed.
In this paper, we describe an approach to
constructing interactive spoken language ap-
plications aimed at alleviating these prob-
lems. We rst outline the characteristics of
the method (section 2) and what needs to
be provided by the application builder (sec-
tion 3). In section 4 and section 5 we ex-
plain variant expansion and the operation of
the system at runtime, and in section 6 we
describe how conrmation requests are pro-
duced by the system. In section 7 we give
some initial experimental results on varying
the number of examples used to construct a
call-routing application.
2 Characteristics of our approach
The goal of the approach discussed in this pa-
per (which we refer to as \variant transduc-
tion") is to avoid the eort and specialized
expertise used to build current research pro-
totypes, while allowing more natural spoken
input than is handled by spoken dialog sys-
tems built using current commercial practice.
This led us to adopt the following constraints:
 Applications are constructed using a rel-
atively small number of example inputs
(no grammar development or extensive
data collection).
 No intermediate semantic representa-
tions are needed. Instead, manipulations
are performed on word strings and on ac-
tion strings that are nal (back-end) ap-
plication calls.
 Conrmation queries posed by the sys-
tem to the user are constructed automat-
ically from the examples, without the use
of a separate generation component.
 Dialog control should be simple to spec-
ify for simple applications, while allowing
the exibility of delegating this control
to another module (e.g. an \intelligent"
back-end agent) for more complex appli-
cations.
We have constructed two telephone-based
applications using this method, an applica-
tion to access email and a call-routing appli-
cation. These two applications were chosen
to gain experience with the method because
they have dierent usage characteristics and
back-end complexity. For the e-mail access
system, usage is typically habitual, and the
system's mapping of user utterances to back-
end actions needs to take into account dy-
namic aspects of the current email session.
For the call-routing application, the back-end
calls executed by the system are relatively
simple, but users may only encounter the sys-
tem once, and the system's initial prompt is
not intended to constrain the rst input spo-
ken by the user.
3 Constructing an application with
example-action contexts
An interactive spoken language application
constructed with the variant transduction
method consists of a set of contexts. Each
context provides the mapping between user
inputs and application actions that are mean-
ingful in a particular stage of interaction be-
tween the user and system. For example the
e-mail reader application includes contexts for
logging in and for navigating a mail folder.
The actual contexts that are used at run-
time are created through a four step process:
1. The application developer species (a
small number of) triples he; a; ci where
e is a natural language string (a typical
user input), a is an application action
(back-end application API call). For in-
stance, the string read the message from
John might be paired with the API call
mailAgent.getWithSender("jsmith@att.com").
The third element of a triple, c, is an
expression identifying another (or the
same) context, specically, the context
the system will transition to if e is the
closest match to the user's input.
2. The set of triples for each context is ex-
panded by the system into a larger set
of triples. The additional triples are of
the form hv; a
0
; ci where v is a \variant"
of example e (as explained in section 4
below), and a
0
is an \adapted" version of
the action a.
3. During an actual user session, the set of
triples for a context may optionally be
expanded further to take into account
the dynamic aspects of a particular ses-
sion. For example, in the mail access ap-
plication, the set of names available for
recognition is increased to include those
present as senders in the user's current
mail folder.
4. A speech recognition language model is
compiled from the expanded set of ex-
amples. We currently use a language
model that accepts any sequence of sub-
strings of the examples, optionally sepa-
rated by ller words, as well as sequences
of digits. (For a small number of exam-
ples, a statistical N-gram model is inef-
fective because of low N-gram counts.) A
detailed account of the recognition lan-
guage model techniques used in the sys-
tem is beyond the scope of this paper.
In the current implementation, actions are
sequences of statements in the Java language.
Constructors can be called to create new ob-
jects (e.g. a mail session object) which can be
assigned to variables and referenced in other
actions. The context interpreter loads the re-
quired classes and evaluates methods dynam-
ically as needed. It is thus possible for an
application developer to build a spoken inter-
face to their target API without introducing
any new Java classes. The system could eas-
ily be adapted to use action strings from other
interpreted languages.
A key property of the process described
above is that the application developer needs
to know only the back-end API and English
(or some other natural language).
4 Variant compilation
Dierent expansion methods can be used in
the second step to produce variants v of an
example e. In the simplest case, v may be
a paraphrase of e. Such paraphrase vari-
ants are used in the experiments in section 7,
where domain-independent \carrier" phrases
are used to create variants. For example, the
phrase I'd like to (among others) is used as a
possible alternative for the phrase I want to.
The context compiler includes an English-to-
English paraphrase generator, so the applica-
tion developer is not involved in the expan-
sion process, relieving her of the burden of
handling this type of language variation. We
are also experimenting with other forms of
variation, including those arising from lexical-
semantic relations, user-specic customiza-
tion, and those variants uttered by users dur-
ing eld trials of a system.
When v is a paraphrase of e, the adapted
action a
0
is the same string as a. In the more
general case, the meaning of variant v is dif-
ferent from that of e, and the system attempts
(not always correctly) to construct a
0
so that
it reects this dierence in meaning. For ex-
ample, including the variant show the message
from Bill Wilson of an example read the mes-
sage from John, involves modifying the ac-
tion mailAgent.getWithSender("jsmith@att.com")
to mailAgent.getWithSender("wwilson@att.com").
We currently adopt a simple approach to
the process of mapping language string vari-
ants to their corresponding target action
string variants. The process requires the
availability of a \token mapping" t between
these two string domains, or data or heuristics
fromwhich such a mapping can be learned au-
tomatically. Examples of the token mapping
are names to email addresses as illustrated in
the example above, name to identier pairs in
a database system, \soundex" phonetic string
spelling in directory applications, and a bilin-
gual dictionary in a translation application.
The process proceeds as follows:
1. Compute a set of lexical mappings be-
tween the variant v and example e. This
is currently performed by aligning the
two string in such a way as that the align-
ment minimizes the (weighted) edit dis-
tance between them (Wagner and Fis-
cher, 1974).
2. The token mapping t is used to map
substitution pairs identied by the align-
ment (hread; showi and hJohn, Bill Wil-
soni in the example above) to corre-
sponding substitution pairs in the action
string. In general this will result in a
smaller set of substitution strings since
not all word strings will be present in
the domain of t. (In the example, this re-
sults in the single pair hjsmith@att.com,
wwilson@att.comi.)
3. The action substitution pairs are applied
to a to produce a
0
.
4. The resulting action a
0
is checked for
(syntactic) well-formedness in the action
string domain; the variant v is rejected if
a
0
is ill-formed.
5 Input interpretation
When an example-action context is active
during an interaction with a user, two com-
ponents (in addition to the speech recognition
language model) are compiled from the con-
text in order to map the user inputs into the
appropriate (possibly adapted) action:
Classier A classier is built with training
pairs hv; ai where v is a variant of an
example e for which the example action
pair he; ai is a member of the unexpanded
pairs in the context. Note that the clas-
sier is not trained on pairs with adapted
examples a
0
since the set of adapted
actions may be too large for accurate
classication (with standard classica-
tion techniques). The classiers typically
use text features such as N-grams ap-
pearing in the training data. In our ex-
periments, we have used dierent classi-
ers, including BoosTexter (Schapire and
Singer, 2000), and a classier based on
Phi-correlation statistics for the text fea-
tures (see Alshawi and Douglas (2000)
for our earlier application of Phi statis-
tics in learning machine translation mod-
els from examples). Other classiers
such as decision trees (Quinlan, 1993) or
support vector machines (Vapnik, 1995)
could be used instead.
Matcher The matcher can compute a dis-
tortion mapping and associated distance
between the output s of the speech rec-
ognizer and a variant v. Various match-
ers can be used such as those suggested
in example-based approaches to machine
translation (Sumita and Iida, 1995). So
far we have used a weighted string edit
distance matcher and experimented with
dierent substitution weights including
ones based on measures of statistical sim-
ilarity between words such as the one
described by Pereira et al (1993). The
output of the matcher is a real number
(the distance) and a distortion mapping
represented as a sequence of edit opera-
tions (Wagner and Fischer, 1974).
Using these two components, the method
for mapping the user's utterance to an exe-
cutable action is as follows:
1. The language model derived from con-
text c is activated in the speech recog-
nizer.
2. The speech recognizer produces a string
s from the user's utterance.
3. The classier for c is applied to s to pro-
duce an unadapted action a.
4. The matcher is applied pairwise to com-
pare s with each variant v
a
derived from
a triple he; a; c
0
i in the unexpanded ver-
sion of c.
5. The triple hv; a
0
; c
0
i for which v pro-
duces the smallest distance is selected
and passed along with e to the dialog con-
troller.
The relationship between the input s, vari-
ant v, example e, and actions a and a
0
is
depicted in Figure 1. In the gure, f is
the mapping between examples and actions
in the unexpanded context; r is the relation
between examples and variants; and g is the
search mapping implemented by the classier-
matcher. The role of e
0
is related to conrma-
tions as explained in the following section.
6 Conrmation and dialog control
Dialog control is straightforward as the reader
might expect, except for two aspects de-
scribed in this section: (i) evaluation of next-
context expressions, and (ii) generation of
p (prompt): say a mailreader command
s (words spoken): now show me messages from Bill
v (variant): show the message from Bill Wilson
e (example): read the message from John
a (associated action): mailAgent.getWithSender("jsmith@att.com")
a
0
(adapted action): mailAgent.getWithSender("wwilson@att.com")
e
0
(adapted example): read the message from Bill Wilson
Figure 2: Example
Figure 1: Variant Transduction mappings
conrmation requests based on the examples
in the context and the user's input.
As noted in section 3 the third element c
of each triple he; a; ci in a context is an ex-
pression that evaluates to the name of the
next context (dialog state) that the system
will transition to if the triple is selected. For
simple applications, c can simply always be
an identier for a context, i.e. the dialog state
transition network is specied explicitly in ad-
vance in the triples by the application devel-
oper.
For more complex applications, next con-
text expressions c may be calls that evalu-
ate to context identiers. In our implemen-
tation, these calls can be Java methods ex-
ecuted on objects known to the action in-
terpreter. They may thus be calls on the
back-end application system, which is appro-
priate for cases when the back-end has state
information relevant to what should happen
next (e.g. if it is an \intelligent agent"). It
might also be a call to component that imple-
ments a dialog strategy learning method (e.g.
Levin and Pieraccini (1997)), though we have
not yet tried such methods in conjunction
with the present system.
A conrmation request of the form do you
mean e
0
is constructed for each variant-action
pair (v; a
0
) of an example-action pair (e; a).
The string e
0
is constructed by rst comput-
ing a submapping h
0
of the mapping h rep-
resenting the distortion between e and v. h
0
is derived from h by removing those edit op-
erations which were not involved in mapping
the action a to the adapted action a
0
. (The
matcher is used to compute h except when
the process of deriving (v; a
0
) from (e; a) al-
ready includes an explicit representation of h
and t(h).)
The restricted mapping h
0
is used instead of
h to construct e
0
in order to avoid misleading
the user about the extent to which the ap-
plication action is being adapted. Thus if h
includes the substitution w ! w
0
but t(w) is
not a substring of a then this edit operation is
not included in h
0
. This way, e
0
includes w un-
changed, so that the conrmation asked of the
user does not carry the implication that the
change w ! w
0
is taken into account in the
action a
0
to be executed by the system. For
instance, in the example in Figure 2, the word
\now" in the user's input does not correspond
to any part of the adapted action, and is not
included in the conrmation string. In prac-
tice, the conrmation string e
0
is computed
at the same time that the variant-action pair
(v; a
0
) is derived from the original example
pair (e; a).
The dialog ow of control proceeds as fol-
lows:
1. The active context c is set to a distin-
guished initial context c
0
indicated by
the application developer.
2. A prompt associated with the current ac-
tive context c is played to the user using
a speech synthesiser or by playing an au-
dio le. For this purpose the application
developer provides a text string (or audio
le) for each context in the application.
3. The user's utterance is interpreted as ex-
plained in the previous section to pro-
duce the triple hv; a
0
; c
0
i.
4. A match distance d is computed as the
sum of the distance computed for the
matcher between s and v and the dis-
tance computed by the matcher between
v and e (where e is the example from
which v was derived).
5. If d is smaller than a preset threshold, it
is assumed that no conrmation is neces-
sary and the next three steps are skipped.
6. The system asks the user do you mean:
e
0
. If the user responds positively then
proceed to the next step, otherwise re-
turn to step 2.
7. The action a
0
is executed, and any string
output it produces is read to the user
with the speech synthesizer.
8. The active context is set to the result of
evaluating the expression c
0
.
9. Return to step 2.
Figure 2 gives an example showing the
strings involved in a dialog turn. Handling
the user's verbal response to the conrmation
is done with a built-in yes-no context.
The generation of conrmation requests
requires no work by the application de-
veloper. Our approach thus provides
an even more extreme version of auto-
matic conrmation generation than that used
by Chu-Carroll and Carpenter (1999) where
only a small eort is required by the devel-
oper. In both cases, the benets of care-
fully crafted conrmation requests are being
traded for rapid application development.
7 Experiments
An important question relating to our method
is the eect of the number of examples on
system interpretation accuracy. To measure
this eect, we chose the operator services call
routing task described by Gorin et al (1997).
We chose this task because a reasonably large
data set was available in the form of actual
recordings of thousands of real customers call-
ing AT&T's operators, together with tran-
scriptions and manual labeling of the de-
sired call destination. More specically, we
measure the call routing accuracy for uncon-
strained caller responses to the initial context
prompt AT&T. How may I help you?. An-
other advantage of this task was that bench-
mark call routing accuracy gures were avail-
able for systems built with the full data set
(Gorin et al, 1997; Schapire and Singer,
2000). We have not yet measured interpreta-
tion accuracy for the structurally more com-
plex e-mail access application.
In this experiment, the responses to How
may I help you? are \routed" to fteen des-
tinations, where routing means handing o
the call to another system or human operator,
or moving to another example-action context
that will interact further with the user to elicit
further information so that a subtask (such as
making a collect call) can be completed. Thus
the actions in the initial context are simply
the destinations, i.e. a = a
0
, and the matcher
is only used to compute e
0
.
The fteen destinations include a destina-
tion \other" which is treated specially in that
it is also taken to be the destination when the
system rejects the user's input, for example
because the condence in the output of the
speech recognizer is too low. Following previ-
ous work on this task, cited above, we present
the results for each experimental condition as
an ROC curve plotting the routing accuracy
(on non-rejected utterances) as a function of
the false rejection rate (the percentage of the
samples incorrectly rejected); a classication
by the system of \other" is considered equiv-
alent to rejection.
The dataset consists of 8,844 utterances of
which 1000 were held out for testing. We refer
to the remaining 7,884 utterances as the \full
training dataset".
In the experiments, we vary two conditions:
Input uncertainty The input string to the
interpretation component is either a hu-
man transcription of the spoken utter-
ance or the output of a speech recog-
nizer. The acoustic model used for au-
tomatic speech recognition was a gen-
eral telephone speech HHM model in all
cases. (For the full dataset, better re-
sults can be achieved by an application-
specic acoustic model, as presented by
Gorin et al (1997) and conrmed by our
results below.)
Size of example set We select progres-
sively larger subsets of examples from
the full training set, as well as showing
results for the full training set itself. We
wish to approximate the situation where
an application developer uses typical
examples for the initial context without
knowing the distribution of call types.
We therefore select k utterances for each
destination, with k set to 3, 5, and 10,
respectively. This selection is random,
except for the provision that utterances
appearing more than once are preferred,
to approximate the notion of a typical
utterance. The selected examples are
expanded by the addition of variants, as
described earlier. For each value of k,
the results shown are for the median of
three runs.
Figure 3 shows the routing accuracy ROC
curves for transcribed input for k = 3; 5; 10
and for the full training dataset. These re-
sults for transcribed input were obtained with
BoosTexter (Schapire and Singer, 2000) as the
classier module in our system because we
have observed that BoosTexter generally out-
performs our Phi classier (mentioned earlier)
for text input.
Figure 4 shows the corresponding four ROC
curves for recognition output, and an ad-
ditional fth graph (the top one) showing
the improvement that is obtained with a do-
main specic acoustic model coupled with a
trigram language model. These results for
recognition output were obtained with the
Phi classier module rather than BoosTex-
ter; the Phi classier performance is generally
the same as, or slightly better than, Boos-
Texter when applied to recognition output.
The language models used in the experiments
for Figure 4 are derived from the example
sets for k = 3; 5; 10 (lower three graphs) and
for the full training set (upper two graphs),
respectively. As described earlier, the lan-
guage model for restricted numbers of exam-
ples is an unweighted one that recognizes se-
quences of substrings of the examples. For the
full training set, statistical N-gram language
models are used (N=3 for the top graph and
N=2 for the second to top) since there is suf-
cient data in the full training set for such
language models to be eective.
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
90
100
False rejection %
%
 C
or
re
ct
 a
ct
io
ns
full training set            
10 examples/action + variants
5 examples/action + variants 
3 examples/action + variants 
Figure 3: Routing accuracy for transcribed
utterances
Comparing the two gures, it can be seen
that the performance shortfall from using
small numbers of examples compared to the
full training set is greater when speech recog-
0 10 20 30 40 50 60 70 80
0
10
20
30
40
50
60
70
80
90
100
False rejection %
%
 C
or
re
ct
 a
ct
io
ns
full training set,  trigrams, domain acoustics
full training set, bigrams                    
10 examples/action + variants, subsequences   
5 examples/action + variants, subsequences    
3 examples/action + variants, subsequences    
Figure 4: Routing accuracy for speech recog-
nition output
nition errors are included. This suggests that
it might be advantageous to use the examples
to adapt a general statistical language model.
There also seem to be diminishing returns as
k is increased from 3 to 5 to 10. A likely
explanation is that expansion of examples by
variants is progressively less eective as the
size of the unexpanded set is increased. This
is to be expected since additional real exam-
ples presumably are more faithful to the task
than articially generated variants.
8 Concluding remarks
We have described an approach to construct-
ing interactive spoken interfaces. The ap-
proach is aimed at shifting the burden of han-
dling linguistic variation for new applications
from the application developer (or data col-
lection lab) to the underlying spoken language
understanding technology itself. Applications
are specied in terms of a relatively small
number of examples, while the mapping be-
tween the inputs that users speak, variants
of the examples, and application actions, are
handled by the system. In this approach, we
avoid the use of intermediate semantic rep-
resentations, making it possible to develop
general approaches to linguistic variation and
dialog responses in terms of word-string to
word-string transformations. Conrmation
requests used in the dialog are computed au-
tomatically from variants in a way intended to
minimize misleading the user about the appli-
cation actions to be executed by the system.
The quantitative results we have pre-
sented indicate that a surprisingly small num-
ber of training examples can provide use-
ful performance in a call routing application.
These results suggest that, even at its cur-
rent early stage of development, the vari-
ant transduction approach is a viable option
for constructing spoken language applications
rapidly without specialized expertise. This
may be appropriate, for example, for boot-
strapping data collection, as well as for situa-
tions (e.g. small businesses) for which devel-
opment of a full-blown system would be too
costly. When a full dataset is available, the
method can provide similar performance to
current techniques while reducing the level of
skill necessary to build new applications.
References
H. Alshawi and S. Douglas. 2000. Learning
dependency transduction models from unan-
notated examples. Philosophical Transactions
of the Royal Society (Series A: Mathematical,
Physical and Engineering Sciences), 358:1357{
1372, April.
H. Aust, M. Oerder, F. Seide, and V. Steinbiss.
1995. The Philips automatic train timetable
information system. Speech Communication,
17:249{262.
Jennifer Chu-Carroll and Bob Carpenter. 1999.
Vector-based natural language call routing.
Computational Linguistic, 25(3):361{388.
J. Dowding, J. M. Gawron, D. Appelt, J. Bear,
L. Cherny, R. Moore, and D. Moran. 1994.
Gemini: A Natural Language System For
Spoken-Language Understanding. In Proc.
ARPA Human Language Technology Workshop
'93, pages 43{48, Princeton, NJ.
A.L. Gorin, G. Riccardi, and J.H. Wright. 1997.
How may I help you? Speech Communication,
23(1-2):113{127.
E. Levin and R. Pieraccini. 1997. A stochas-
tic model of computer-human interaction for
learning dialogue strategies. In Proceedings of
EUROSPEECH97, pages 1883{1886, Rhodes,
Greece.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion { BBN: description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
Fairfax, VA. Morgan Kaufmann.
F. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of english words. In Proceed-
ings of the 31st meeting of the Association for
Computational Linguistics, pages 183{190.
J.R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Robert E. Schapire and Yoram Singer. 2000.
BoosTexter: A Boosting-based System for
Text Categorization. Machine Learning,
39(2/3):135{168.
Eiichiro Sumita and Hitoshi Iida. 1995. Het-
erogeneous computing for example-based trans-
lation of spoken language. In Proceedings of
the 6
th
International Conference on Theoretical
and Methodological Issues in Machine Transla-
tion, pages 273{286, Leuven, Belgium.
V.N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer, New York.
Robert A. Wagner and Michael J. Fischer.
1974. The String-to-String Correction Prob-
lem. Journal of the Association for Computing
Machinery, 21(1):168{173, January.
Speech Translation Performance of Statistical Dependency Transduction
and Semantic Similarity Transduction
Hiyan Alshawi and Shona Douglas
AT&T Labs - Research
Florham Park, NJ 07932, USA
 
hiyan,shona  @research.att.com
Abstract
In this paper we compare the performance
of two methods for speech translation.
One is a statistical dependency transduc-
tion model using head transducers, the
other a case-based transduction model in-
volving a lexical similarity measure. Ex-
amples of translated utterance transcrip-
tions are used in training both models,
though the case-based model also uses se-
mantic labels classifying the source utter-
ances. The main conclusion is that while
the two methods provide similar transla-
tion accuracy under the experimental con-
ditions and accuracy metric used, the sta-
tistical dependency transduction method
is significantly faster at computing trans-
lations.
1 Introduction
Machine translation, natural language processing,
and more generally other computational problems
that are not amenable to closed form solutions,
have typically been tackled by one of three broad
approaches: rule-based systems, statistical mod-
els (including generative models), and case-based
systems. Hybrid solutions combining these ap-
proaches have also been used in language pro-
cessing generally (Klavans and Resnik, 1996) and
more specifically in machine translation (for exam-
ple Frederking et al (1994)).
In this paper we compare the performance of two
methods for speech translation. One is the statistical
dependency transduction model (Alshawi and Dou-
glas, 2000; Alshawi et al, 2000b), a trainable gener-
ative statistical translation model using head trans-
ducers (Alshawi, 1996). The other is a case-based
transduction model which makes use of a semantic
similarity measure between words. Both models are
trained automatically using examples of translated
utterances (the transcription of a spoken utterance
and a translation of that transcription). The case-
based model makes use of additional information in
the form of labels associated with source language
utterances, typically one or two labels per utterance.
This additional information, which was originally
provided for a separate monolingual task, is used to
construct the lexical similarity measure.
In training these translation methods, as well as
their runtime application, no pre-existing bilingual
lexicon is needed. Instead, in both cases, the initial
phase of training from the translation data is a sta-
tistical hierarchical alignment search applied to the
set of bilingual examples. This training phase pro-
duces a bilingual lexicon, used by both methods, as
well as synchronized hierarchical alignments used to
build the dependency transduction model.
In the experiments comparing the performance
of the models we look at accuracy as well as the
time taken to translate sentences from English to
Japanese. The source language inputs used in these
experiments are naturally spoken utterances from
large numbers of real customers calling telephone
operator services.
In section 2 we describe the hierarchical align-
ment algorithm followed by descriptions of the
translation methods in sections 3 and 4. We present
the experiments in section 5 and provide concluding
remarks in section 6.
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 31-38.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
Figure 1: Alignment mapping  , source head-map

, and target head-map 
2 Hierarchical alignments
Both the translation systems described in this pa-
per make use of automatically created hierarchical
alignments of the source and target strings of the
training corpus bitexts. As will be described in sec-
tion 3, we estimate the parameters of a dependency
transduction model from such alignments. In the
case-based method described in section 4, the align-
ments are the basis for the translation lexicon used
to compute substitutions and word-for-word transla-
tions.
A hierarchical alignment consists of four func-
tions. The first two functions are an alignment
mapping  from source words  to target words

	 (which may be the empty word  ), and an in-
verse alignment mapping from target words  to
source words 	 . (The inverse mapping is needed
to handle mapping of target words to  ; it coincides
with  for pairs without  .) The other two functions
are a source head-map  mapping source dependent
words  to their heads  	 in the source string,
and a target head-map  mapping target dependent
words  to their head words 	 in the target string.
An example hierarchical alignment is shown in Fig-
ure 1.
A hierarchical alignment is synchronized (i.e.
corresponds to synchronized dependency trees) if,
roughly speaking,  induces an isomorphism be-
tween the dependency functions  and  (see
Alshawi and Douglas (2000) for a more formal def-
inition). The hierarchical alignment in Figure 1 is
synchronized.
In some previous work (Alshawi et al, 1998; Al-
shawi et al, 2000a; Alshawi et al, 2000b) the train-
ing method constructs synchronized alignments in
which each head word has at most two dependent
phrases. Here we use the technique described by
Alshawi and Douglas (2000) where the models have
greater freedom to vary the granularity of phrase lo-
cality.
Constructing synchronized hierarchical align-
ments for a corpus has two stages: (a) computing
co-occurrence statistics from the training data; (b)
searching for an optimal synchronized hierarchical
alignment for each bitext.
2.1 Word correlation statistics
For each source word in the dataset, a translation
pairing cost 	 is assigned for all possible
translations in the context of a bitext  . Here  and 
are usually words, but may also be the empty word 
or compounds formed from contiguous words; here
we restrict compounds to a maximum length of two
words.
The assignment of these lexical translation pair-
ing costs may be done using various statistical mea-
sures. The main component of  is the so-called
