Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 346?350,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Detection and Correction of  Errors in Dependency Tree-
banks
Alexander Volokh
DFKI
Stuhlsatzenhausweg 3
66123 Saarbr?cken, Germany
alexander.volokh@dfki.de
G?nter Neumann
DFKI
Stuhlsatzenhausweg 3
66123 Saarbr?cken, Germany
neumann@dfki.de
Abstract
Annotated corpora are essential for almost all 
NLP applications. Whereas they are expected 
to be of a very high quality because of their 
importance  for  the  followup  developments, 
they still contain a considerable number of er-
rors. With this work we want to draw attention 
to this fact.  Additionally, we try to estimate 
the amount of errors and propose a method for 
their  automatic  correction.  Whereas  our  ap-
proach is able to find only a portion of the er-
rors that we suppose are contained in almost 
any annotated corpus due to the nature of the 
process of its creation, it has a very high pre-
cision, and thus is in any case beneficial for 
the quality of the corpus it  is  applied to. At 
last, we compare it to a different method for 
error detection in treebanks and find out that 
the errors that we are able to detect are mostly 
different and that our approaches are comple-
mentary.
1 Introduction
Treebanks and other annotated corpora  have be-
come essential for almost all NLP applications. Pa-
pers about corpora like the Penn Treebank [1] have 
thousands of citations, since most of the algorithms 
profit from annotated data during the development 
and testing and thus are widely used in the field. 
Treebanks are therefore expected to be of a very 
high  quality  in  order  to  guarantee  reliability  for 
their theoretical and practical uses. The construc-
tion of an annotated corpus involves a lot of work 
performed by large groups. However, despite the 
fact that a lot of human post-editing and automatic 
quality  assurance  is  done,  errors  can  not  be 
avoided completely [5]. 
In this paper we propose an approach for find-
ing and correcting errors in dependency treebanks. 
We apply our method to the English dependency 
corpus ? conversion of the Penn Treebank to the 
dependency format done by Richard Johansson and 
Mihai  Surdeanu [2]  for  the  CoNLL shared tasks 
[3].  This  is  probably  the  most  used  dependency 
corpus, since English is the most popular language 
among the researchers. Still we are able to find a 
considerable amount of errors in it. Additionally, 
we  compare  our  method  with  an  interesting  ap-
proach developed by a different group of research-
ers (see section 2). They are able to find a similar 
number of errors in different corpora, however, as 
our investigation shows, the overlap between our 
results is quite small and the approaches are rather 
complementary.
2 Related Work
Surprisingly, we were not able to find a lot of work 
on the topic of error detection in treebanks. Some 
organisers of shared tasks usually try to guarantee 
a certain quality of the used data, but the quality 
control is usually performed manually. E.g. in the 
already mentioned CoNLL task the organisers ana-
lysed a large amount of dependency treebanks for 
different  languages  [4],  described  problems  they 
have encountered and forwarded them to the de-
velopers  of  the  corresponding corpora.  The  only 
work,  that  we were able to find,  which involved 
automatic quality control, was done by the already 
mentioned  group  around  Detmar  Meurers.  This 
work  includes  numerous  publications  concerning 
finding errors in phrase structures [5] as well as in 
dependency treebanks [6]. The approach is based 
on the concept of ?variation detection?, first intro-
duced  in  [7].  Additionally,  [5]  presents  a  good 
346
method  for  evaluating the automatic  error  detec-
tion. We will perform a similar evaluation for the 
precision of our approach. 
3 Variation Detection
We will  compare  our  outcomes  with  the  results 
that can be found with the approach of ?variation 
detection? proposed by Meurers  et  al.  For  space 
reasons, we will not be able to elaborately present 
this method and advise to read the referred work, 
However, we think that we should at least briefly 
explain its idea.
The idea behind ?variation detection? is to find 
strings, which occur multiple times in the corpus, 
but which have varying annotations. This can obvi-
ously have only two reasons: either the strings are 
ambiguous and can have different  structures,  de-
pending on the meaning, or the annotation is erro-
neous in at least one of the cases. The idea can be 
adapted to dependency structures as well, by ana-
lysing the possible dependency relations between 
same words. Again different dependencies can be 
either the result of ambiguity or errors. 
4 Automatic Detection of Errors
We propose a different approach. We take the Eng-
lish  dependency  treebank  and  train  models  with 
two different  state  of  the  art  parsers:  the  graph-
based  MSTParser  [9]  and  the  transition-based 
MaltParser [10]. We then parse the data, which we 
have used for training, with both parsers. The idea 
behind this step is that we basically try to repro-
duce the gold standard, since parsing the data seen 
during the training is very easy (a similar idea in 
the area of POS tagging is very broadly described 
in  [8]).  Indeed  both  parsers  achieve  accuracies 
between 98% and 99% UAS (Unlabeled Attach-
ment Score), which is defined as the proportion of 
correctly identified dependency relations. The reas-
on why the parsers are not able to achieve 100% is 
on the one hand the fact that some of the phenom-
ena are too rare and are not captured by their mod-
els. On the other hand, in many other cases parsers 
do make correct predictions, but the gold standard 
they are evaluated against is wrong.
We  have  investigated  the  latter  case,  namely 
when both parsers  predict  dependencies  different 
from the gold standard (we do not consider the cor-
rectness of the dependency label). Since MSTPars-
er and MaltParser are based on completely differ-
ent parsing approaches they also tend to make dif-
ferent mistakes [11]. Additionally, considering the 
accuracies of 98-99% the chance that both parsers, 
which  have  different  foundations,  make  an  erro-
neous  decision  simultaneously is  very small  and 
therefore these cases are the most likely candidates 
when looking for errors.
5 Automatic Correction of Errors
In this section we propose our algorithm for auto-
matic  correction of  errors,  which consists  out  of 
the following steps:
1. Automatic  detection  of  error  candidates, 
i.e. cases where two parsers deliver results 
different to gold-standard.
2. Substitution of the annotation of the error 
candidates by the annotation proposed by 
one  of  the  parsers  (in  our  case 
MSTParser).
3. Parse of the modified corpus with a third 
parser (MDParser).
4. Evaluation of the results.
5. The modifications are only kept for those 
cases  when  the  modified  annotation  is 
identical  with  the  one  predicted  by  the 
third parser and undone in other cases. 
For the English dependency treebank we have 
identified  6743  error  candidates,  which  is  about 
0.7% of all tokens in the corpus.
The third dependency parser, which is used is 
MDParser1 - a fast transition-based parser. We sub-
situte  the  gold  standard  by  MSTParser  and  not 
MaltParser in order not to give an advantage to a 
parser  with  similar  basics  (both  MDParser  and 
MDParser are transition-based). 
During this experiment we have found out that 
the result of MDParser significantly improves: it is 
able to correctly recgonize 3535 more dependen-
cies than before the substitution of the gold stand-
ard. 2077 annotations remain wrong independently 
of the changes in the gold standard. 1131 of the re-
lations  become  wrong  with  the  changed  gold 
standard,  whereas they were correct  with the old 
unchanged version. We then undo the changes to 
the gold standard when the wrong cases remained 
wrong and when the correct cases became wrong. 
We suggest that the 3535 dependencies which be-
came correct after the change in gold standard are 
1 http://mdparser.sb.dfki.de/  
347
errors, since a) two state of the art parsers deliver a 
result which differs from the gold standard and b) a 
third parser confirms that by delivering exactly the 
same result as the proposed change. However, the 
exact  precision of  the  approach can probably be 
computed only by manual investigation of all cor-
rected dependencies.
6 Estimating the Overall Number Of Er-
rors
The previous section tries to evaluate the precision 
of the approach for the identified error candidates. 
However, it remains unclear how many of the er-
rors are found and how many errors can be still ex-
pected in the corpus. Therefore in this section we 
will describe our attempt to evaluate the recall of 
the proposed method.
In  order  to  estimate  the  percentage  of  errors, 
which can be found with our method, we have de-
signed the following experiment.  We have taken 
sentences of different lengths from the corpus and 
provided them with a ?gold standard? annotation 
which  was  completely  (=100%)  erroneous.  We 
have achieved that by substituting the original an-
notation by the annotation of a different sentence 
of the same length from the corpus, which did not 
contain  dependency  edges  which  would  overlap 
with the original annotation. E.g consider the fol-
lowing sentence in the (slightly simplified) CoNLL 
format:
1 Not RB 6 SBJ
2 all PDT 1 NMOD
3 those DT 1 NMOD
4 who WP 5 SBJ
5 wrote VBD 1 NMOD
6 oppose VBP 0 ROOT
7 the DT 8 NMOD
8 changes NNS 6 OBJ
9 . . 6 P
We would substitute its annotation by an annota-
tion chosen from a different sentence of the same 
length:
1 Not RB 3 SBJ
2 all PDT 3 NMOD
3 those DT 0 NMOD
4 who WP 3 SBJ
5 wrote VBD 4 NMOD
6 oppose VBP 5 ROOT
7 the DT 6 NMOD
8 changes NNS 7 OBJ
9 . . 3 P
This way we know that we have introduced a 
well-formed dependency tree (since its annotation 
belonged to a different tree before) to the corpus 
and  the  exact  number  of  errors  (since  randomly 
correct  dependencies  are  impossible).  In  case  of 
our example 9 errors are introduced to the corpus.
In  our  experiment  we  have  introduced  sen-
tences  of  different  lengths  with  overall  1350 
tokens.  We  have  then  retrained  the  models  for 
MSTParser and MaltParser and have applied our 
methodology  to  the  data  with  these  errors.  We 
have then counted how many of these 1350 errors 
could  be  found.  Our  result  is  that  619  tokens 
(45.9%)  were different  from the  erroneous gold-
standard. That means that despite the fact that the 
training data contained some incorrectly annotated 
tokens, the parsers were able to annotate them dif-
ferently. Therefore we suggest that the recall of our 
method is close to the value of 0.459. However, of 
course we do not know whether the randomly in-
troduced errors  in  our  experiment  are  similar  to 
those which occur in real treebanks.
7 Comparison with Variation Detection
The interesting question which naturally arises at 
this  point  is  whether  the  errors  we  find  are  the 
same as those found by the method of variation de-
tection. Therefore we have performed the follow-
ing experiment: We have counted the numbers of 
occurrences  for   the  dependencies  B? A (the 
word B is the head of the word A) and C? A
(the  word  C is  the  head  of  the  word  A),  where 
B? A is the dependency proposed by the pars-
ers and  C? A is the dependency proposed by 
the gold standard. In order for variation detection 
to be applicable the frequency counts for both rela-
tions must be available and the counts for the de-
pendency proposed by the parsers should ideally 
greatly outweigh the frequency of the gold stand-
ard, which would be a great indication of an error. 
For the 3535 dependencies that we classify as er-
rors the variation detection method works only 934 
times (39.5%). These are the cases when the gold 
standard is obviously wrong and occurs only few 
times, most often - once, whereas the parsers pro-
348
pose much more frequent dependencies. In all oth-
er cases the counts suggest that the variation detec-
tion would not work, since both dependencies have 
frequent counts or the correct dependency is even 
outweighed by the incorrect one.
8 Examples 
We will provide some of the example errors, which 
we are able to find with our approach. Therefore 
we  will  provide  the  sentence  strings  and briefly 
compare the gold standard dependency annotation 
of a certain dependency within these sentences.
Together, the two stocks wreaked havoc among  
takeover stock traders, and caused a 7.3% drop in  
the DOW Jones Transportation Average, second in  
size  only  to  the  stock-market  crash of  Oct.  19  
1987.
In this sentence the gold standard suggests the 
dependency  relation  market? the ,  whereas 
the  parsers  correctly  recognise  the  dependency 
crash? the .  Both  dependencies  have  very 
high counts  and therefore  the  variation detection 
would not work well in this scenario.
Actually, it  was down only a few points at the 
time.
In  this  sentence  the  gold  standard  suggests 
points?at ,  whereas  the  parsers  predict 
was? at . The gold standard suggestion occurs 
only  once  whereas  the  temporal  dependency 
was? at occurs 11 times in the corpus. This is 
an example of an error which could be found with 
the variation detection as well.
Last October, Mr. Paul paid out $12 million of  
CenTrust's cash ? plus a $1.2 million commission 
? for ?Portrait of a Man as Mars?.
In this sentence the gold standard suggests the 
dependency relation $? a , whereas the parsers 
correctly  recognise  the  dependency 
commission?a .  The  interesting  fact  is  that 
the  relation  $? a is  actually  much  more  fre-
quent than commission?a , e.g. as in the sen-
tence he cought up an additional $1 billion or so. 
( $? an )  So  the  variation  detection  alone 
would not suffice in this case.
9 Conclusion
The quality of treebanks is of an extreme import-
ance for the community.  Nevertheless, errors can 
be found even in the most popular and widely-used 
resources. In this paper we have presented an ap-
proach for  automatic  detection and correction  of 
errors and compared it to the only other work we 
have found in this field. Our results show that both 
approaches are rather complementary and find dif-
ferent types of errors. 
We have only analysed the errors in the head-
modifier annotation of the dependency relations in 
the  English  dependency  treebank.  However,  the 
same methodology can easily be applied to detect 
irregularities in any kind of annotations, e.g. labels, 
POS tags etc. In fact, in the area of POS tagging a 
similar strategy of using the same data for training 
and testing in order to detect  inconsistencies has 
proven to be very efficient [8]. However, the meth-
od lacked means  for  automatic  correction of  the 
possibly inconsistent annotations. Additionally, the 
method off course can as well be applied to differ-
ent corpora in different languages. 
Our  method  has  a  very  high  precision,  even 
though  we  could  not  compute  the  exact  value, 
since it  would require an expert  to go through a 
large number of cases. It is even more difficult to 
estimate the recall of our method, since the overall 
number of errors in a corpus is unknown. We have 
described an experiment  which to  our  mind is  a 
good  attempt  to  evaluate  the  recall  of  our  ap-
proach.  On  the  one  hand  the  recall  we  have 
achieved in this experiment is rather low (0.459), 
which means that our method would definitely not 
guarantee to find all errors in a corpus. On the oth-
er hand it has a very high precision and thus is in 
any case beneficial,  since the quality of the tree-
banks increases with the removal of errors. Addi-
tionally, the low recall suggests that treebanks con-
tain an even larger number of errors, which could 
not  be found.  The overall  number  of errors  thus 
seems to be over 1% of the total size of a corpus, 
which is expected to be of a very high quality. A 
fact that one has to be aware of when working with 
annotated resources and which we would like to 
emphasize with our paper.
10 Acknowledgements
The presented work was partially supported by a 
grant from the German Federal  Ministry of Eco-
nomics  and  Technology  (BMWi)  to  the  DFKI 
Theseus  project  TechWatch?Ordo  (FKZ:  01M-
Q07016). 
349
References
[1]  Mitchell  P.  Marcus,  Beatrice  Santorini  and  Mary 
Ann Marcinkiewicz , 1993. Building a Large Annot-
ated Corpus of English: The Penn Treebank. In Com-
putational Lingustics, vol. 19, pp. 313-330.
[2] Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Lluis Marquez and Joakim Nivre.  The CoNLL-2008 
Shared Task on Joint  Parsing of Syntactic and Se-
mantic  Dependencies.  In  Proceedings  of  the  12th 
Conference  on  Computational  Natural  Language 
Learning (CoNLL-2008), 2008 
[3] Sabine Buchholz and Erwin Marsi, 2006. CoNLL-X 
shared task on multilingual dependency parsing.  In 
Proceedings  of  CONLL-X,  pages  149?164,  New 
York.
[4] Sabine Buchholz and Darren Green, 2006.  Quality 
control  of  treebanks:  documenting,  converting,  
patching. In LREC 2006 workshop on Quality assur-
ance  and  quality  measurement  for  language  and 
speech resources.
[5] Markus Dickinson and W. Detmar Meurers,  2005. 
Prune  Diseased  Branches  to  Get  Healthy  Trees!  
How to Find Erroneous Local Trees in a Treebank  
and Why  It  Matters.  In  Proceedings  of  the  Fourth 
Workshop on Treebanks and Linguistic Theories, pp. 
41?52
[6] Adriane Boyd, Markus Dickinson and Detmar Meur-
ers, 2008. On Detecting Errors in Dependency Tree-
banks.  In  Research on Language and Computation, 
vol. 6, pp. 113-137.
[7] Markus Dickinson and Detmar Meurers, 2003.  De-
tecting inconsistencies in treebanks.  In  Proceedings 
of TLT 2003
[8] van Halteren, H. (2000). The detection of inconsist-
ency  in  manually  tagged  text.  In  A.  Abeill?,  T. 
Brants, and H. Uszkoreit (Eds.), Proceedings of the 
Second Workshop on Linguistically Interpreted Cor-
pora (LINC-00), Luxembourg. 
[9 R. McDonald, F. Pereira, K. Ribarov, and J. Haji?c . 
2005. Non-projective  Dependency  Parsing  using  
Spanning Tree Algorithms. In Proc. of HLT/EMNLP 
2005.
[10]  Joakim Nivre,  Johan  Hall,  Jens  Nilsson,  Atanas 
Chanev,  Gulsen  Eryigit,  Sandra  Kubler,  Svetoslav 
Marinov  and  Erwin  Marsi.  2007.   MaltParser:  A  
Language-Independent  System for Data-Driven De-
pendency  Parsing,  Natural  Language  Engineering 
Journal, 13, pp. 99-135.
[11] Joakim Nivre and Ryan McDonald, 2008. Integrat-
ing GraphBased and Transition-Based Dependency  
Parsers. In Proceedings of the 46th Annual Meeting 
of  the  Association  for  Computational  Linguistics: 
Human Language Technologies.
350
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 308?312,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
372:Comparing the Benefit of Different Dependency Parsers for Textu-
al Entailment Using Syntactic Constraints Only 
Alexander Volokh G?nter Neumann
alexander.volokh@dfki.de neumann@dfki.de
DFKI DFKI
Stuhlsatzenhausweg 3 Stuhlsatzenhausweg 3 
66123 Saarbr?cken, Germany 66123 Saarbr?cken, Germany 
Abstract
We compare several  state of the art dependency 
parsers  with  our  own  parser  based  on  a  linear 
classification  technique.  Our  primary  goal  is 
therefore to use syntactic information only, in or-
der to keep the comparison of the parsers as fair 
as possible. We demonstrate, that despite the in-
ferior result using the standard evaluation metrics 
for  parsers  like  UAS  or  LAS  on  standard  test 
data,  our  system  achieves  comparable  results 
when used in an application, such as the SemEv-
al-2 #12 evaluation exercise PETE. Our submis-
sion achieved the 4th position out of 19 participat-
ing systems. However, since it only uses a linear 
classifier  it  works 17-20 times faster  than other 
state of the parsers, as for instance MaltParser or 
Stanford Parser.
1 Introduction
Parsing is the process of mapping sentences to 
their syntactic representations. These representa-
tions  can be used by computers  for  performing 
many  interesting  natural  language  processing 
tasks, such as question answering or information 
extraction.  In recent years  a lot of  parsers have 
been developed for this purpose.
A very interesting and important  issue is  the 
comparison between a large number of such pars-
ing systems.  The most  widespread method is to 
evaluate the number of correctly recognized units 
according to a certain gold standard. For depend-
ency-based units unlabeled or labeled attachment 
scores (percentage of correctly classified depend-
ency relations, either with or without the depend-
ency relation type) are usually used (cf. Buchholz 
and Marsi, 2006).
However, parsing is very rarely a goal in itself. 
In most cases it is a necessary preprocessing step 
for  a certain application.  Therefore it  is  usually 
not the best option to decide which parser suits 
one's goals best by purely looking on its perform-
ance on some standard test data set.  It is rather 
more  sensible  to  analyse  whether  the  parser  is 
able  to  recognise  those  syntactic  units  or  rela-
tions, which are most relevant for one's applica-
tion.
The  shared  task  #12  PETE in  the  SemEval-
2010  Evaluation  Exercises  on  Semantic  Evalu-
ation (Yuret, Han and Turgut, 2010) involved re-
cognizing  textual  entailments  (RTE).  RTE  is  a 
binary  classification  task,  whose  goal  is  to  de-
termine, whether for a pair of texts T and H the 
meaning  of  H is  contained  in  T  (Dagan et  al., 
2006). This task can be very complex depending 
on the properties of these texts. However, for the 
data, released by the organisers of PETE, only the 
syntactic information should be sufficient to reli-
ably perform this task. Thus it offers an ideal set-
ting for  evaluating the performance of different 
parsers.
To our mind evaluation of parsers via RTE is a 
very good additional possibility, besides the usual 
evaluation metrics, since in most cases the main 
thing in real-word applications is to recognize the 
primary units, such as the subject, the predicate, 
308
the objects, as well as their modifiers, rather than 
the other subordinate relations.
We have been developing our own a multilin-
gual dependency parser (called MDParser), which 
is  based  on  linear  classification1.  Whereas  the 
system is quite fast because the classification is 
linear,  it  usually achieves inferior  results  (using 
UAS/LAS evaluation metrics)  in  comparison  to 
other parsers, which for example use kernel-based 
classification  or  other  more  sophisticated  meth-
ods. 
Therefore the PETE shared task was a perfect 
opportunity for us to investigate whether the in-
ferior result of our parser is also relevant for its 
applicability  in  a  concrete  task.  We have  com-
pared our system with three state of the art pars-
ers made available on the PETE web page: Malt-
Parser,  MiniPar  and  StandfordParser.  We  have 
achieved the total score of 0.6545 (200/301 cor-
rect  answers  on  the  test  data),  which  is  the  4th 
rank out of 19 submissions. 
2 MDParser
MDParser stands for multilingual dependency 
parser and is a data-driven system, which can be 
used  to  parse  text  of  an  arbitrary  language  for 
which training data is available. It is a transition-
based parser and uses a deterministic version of 
the Covington's algorithm (Covington, 2000).
The models of the system are based on various 
features, which are extracted from the words of 
the  sentence,  including word forms  and part  of 
speech tags. No additional morphological features 
or lemmas are currently used in our models, even 
if they are available in the training data, since the 
system is especially designed for processing plain 
text in different languages, and such components 
are not available for every language.
The  preprocessing  components  of  MDParser 
include a.)  a  sentence splitter2,  since  the  parser 
constructs a dependency structure for  individual 
sentences,  b.)  a  tokenizer,  in order to recognise 
the elements between which the dependency rela-
tions will be built3, and c.) a part of speech tagger, 
1http://www.dfki.de/~avolokh/mdparser.pdf
2http://morphadorner.northwestern.edu/morphadorner/sen-
tencesplitter/
3http://morphadorner.northwestern.edu/morphadorner/word-
tokenizer/
in  order  to  determine  the  part  of  speech  tags, 
which are intensively used in the feature models4.
MDParser is an especially fast system because 
it  uses  a  linear  classification  algorithm  L1R-
LR(L1  regularised  logistic  regression)  from the 
machine learning package LibLinear (Lin et al, 
2008) for constructing its dependency structures 
and  therefore  it  is  particularly  suitable  for  pro-
cessing very large amounts of data. Thus it can be 
used as a part of larger applications in which de-
pendency structures are desired. 
Additionally,  significant efforts were made in 
order to make the gap between our linear classi-
fication and more advanced methods as small as 
possible,  e.g.  by  introducing  features  conjunc-
tions, which are complex features built out of or-
dinary features, as well as methods for automatic-
ally measuring feature usefulness in order to auto-
mate and optimise feature engineering.
3 Triple Representation
Every parser  usually produces  its  own some-
how special  representation  of  the  sentence.  We 
have created such a representation, which we will 
call  triple representation and have implemented 
an  automatic  transformation  of  the  results  of 
Minipar,  MaltParser,  Stanford  Parser  and  of 
course MDParser into it (cf. Wang and Neumann, 
2007).
The triple representation of a sentence is a set 
of  triple  elements  of  the  form  <parent,  label, 
child>, where child and parent elements stand for 
the head and the modifier words and their parts of 
speech, and label stands for the relation between 
them.  E.g.  <have:VBZ,  SBJ,  Somebody:NN>. 
This information is extractable from the results of 
any dependency parser.
4 Predicting Entailment
Whereas the first part of the PETE shared task 
was to construct syntactic representations for all 
T-H-pairs,  the  second important  subtask was  to 
determine whether the structure of H is entailed 
by the structure of T. The PETE guide5 states that 
the following three phenomena were particularly 
important to recognise the entailment relation:
4The part of speech tagger was trained with the SVMTool 
http://www.lsi.upc.edu/~nlp/SVMTool/
5http://pete.yuret.com/guide
309
1. subject-verb  dependency  (John  kissed 
Mary. ? John kissed somebody.)
2. verb-object  dependency  (John  kissed 
Mary ? Mary was kissed.)
3. noun-modifier  dependency (The big red  
boat sank. ? The boat was big.)
Thus we have manually formulated the follow-
ing generic decision rule for determining the en-
tailment relation between T and H:
1. identify  the  root  triple  of  H  <null:null, 
ROOT, x>
2. check whether the subject and the com-
plements(objects, verb complements) of the root 
word in H are present in T. Formally: all triples of 
H of the form <x, z, y>  should be contained in 
T(x in 1 and 2 is thus the same word).
3. if 2 returns false we have to check wheth-
er H is a structure in passive and T contains the 
same content in active voice(a) or the other way 
around(b). Formally:
3a. For triples of the form <be:VBZ, SBJ, s> 
and <be:VBZ, VC, t> in H check whether there is 
a  triple of the form <s, NMOD, t> in T.
3b. For triples of the form <u, OBJ,v> in H 
check whether there is  a triple  of  the  form <v, 
NMOD, u> in T.
It turned out that few additional modifications 
to  the  base  rule  were  necessary  for  some  sen-
tences: 1.) For sentences containing conjunctions: 
If we were looking for a subject of a certain verb 
and could not find it, we investigated whether this 
verb is connected via a conjunction with another 
one. If true, we compared the subject in H with 
the subject of the conjunct verb. 2.) For sentences 
containing special verbs, e.g. modal verbs may or 
can or auxiliary verbs like to have it turned out to 
be important to go one level deeper into the de-
pendency structure  and to  check whether  all  of 
their  arguments  in  H are  also present  in T,  the 
same way as in 3.
A triple <x,z,y> is contained in a set of triples 
S, when there exists at least one of the triples in S 
<u,w,v>, such that x=u, w=z and y=v. This is also 
true  if  the  words  somebody,  someone or  some-
thing are  used  on  one  of  the  equation  sides. 
Moreover, we use an English lemmatizer for all 
word forms, so when checking the equality of two 
words we actually check their lemmas, e.g., is and 
are are also treated equally.
5 Results
We have parsed the 66 pairs  of  the develop-
ment  data  with  4  parsers:6 MiniPar,  Stanford 
Parser, MaltParser and MDParser. After applying 
our rule we have achieved the following result:
Accuracy Parsing Speed
MiniPar 45/66 1233 ms
Stanford Parser 50/66 32889 ms
MaltParser 51/66 37149 ms
MDParser 50/66 1785 ms
We used  the  latest  versions  of  MiniPar7 and 
Stanford Parser8. We did not re-test the perform-
ance of these parsers on standard data, since we 
were sure that these versions provide the best pos-
sible results of these systems. 
As far as the MaltParser is concerned we had to 
train our own model. We have trained the model 
with the following LibSVM options: ?-s_0_-t_1_-
d_2_-g_0.18_-c_0.4_-r_0.4_-e_1.0?.  We  were 
able  to  achieve  a  result  of  83.86%  LAS  and 
87.25% UAS on the standard CoNLL English test 
data,  a  result  which is  only slightly worse  than 
those reported in the literature, where the options 
are probably better tuned for the data. The train-
ing  data  used  for  training  was  the  same  as  for 
MDParser. 
The application of our rule for MDParser and 
MaltParser  was fully automated,  since both use 
the  same  training  data  and  thus  work  over  the 
same tag sets. For MiniPar and Stanford Parser, 
which  construct  different  dependency structures 
with  different  relation  types,  we  had  to  go 
through all pairs manually in order to investigate 
how the rule should be adopted to their tag sets 
and structures. However, since we have already 
counted the  number  of  structures,  for  which an 
adoptation of the rule would work during this in-
vestigation, we did not implement it in the end. 
Therefore  these  results  might  be  taken  with  a 
pinch of salt, despite the fact that we have tried to 
stay as fair as possible and treated some pairs as 
correct, even if a quite large modification of the 
6For all results reported in this section a desktop PC with 
an Intel Core 2 Duo E8400 3.00 GHz processor and 4.00 GB 
RAM was used.
7http://webdocs.cs.ualberta.ca/~lindek/minipar  
8http://nlp.stanford.edu/downloads/lex-parser.shtml  
310
rule was necessary in order to adopt it to the dif-
ferent tag set and/or dependency structure.
For test  data we were only able to apply our 
rule for the results of MDParser and MaltParser, 
since for such a large number of pairs (301) only 
the fully automated version of our mechanism for 
predicting entailment could be applied. For Mini-
Par and Stanford Parser it was too tedious to ap-
ply it to them manually or to develop a mapping 
between  their  dependency  annotations  and  the 
ones used in MDParser or MaltParser.  Here are 
the official results of our submissions for Malt-
Parser and MDParser:
Accuracy Parsing Speed
MDParser 197/301 8704 ms
MaltParser 196/301 147938 ms
6 Discussion
We were able to show that our parser based on 
a linear classification technique is especially fast 
compared to other state of the art parsers. Further-
more, despite the fact, that it achieves an inferior 
result,  when using usual  evaluation metrics like 
UAS or LAS, it is absolutely suitable for being 
used in applications, since the most important de-
pendency relations are recognized correctly even 
with  a  less  sophisticated  linear  classifier  as  the 
one being used in MDParser.
As  far  as  the  overall  score  is  concerned  we 
think a much better result  could be achieved, if 
we would put more effort into our mechanism for 
recognizing  entailment  using  triple  representa-
tions. However, many of the pairs required more 
than only syntactical information. In many cases 
one would need to extend one's mechanism with 
logic,  semantics  and  the  possibility  to  resolve 
anaphoric  expressions,  which  to  our  mind  goes 
beyond the idea behind the PETE task. Since we 
were  primarly  interested  in  the  comparison 
between MaltParser and MDParser, we have not 
tried to include solutions for such cases. Here are 
some of the pairs we think require more than only 
syntax:
(4069  entailment="YES")  <t>Mr.  Sherwood 
speculated  that  the  leeway  that  Sea  Containers 
has means that Temple would have to "substan-
tially  increase  their  bid  if  they're  going  to  top 
us."</t>
<h>Someone  would  have  to  increase  the 
bid.</h>
(7003 entailment="YES") <t>After all, if  you 
were going to set up a workshop you had to have 
the proper equipment and that was that.</t>
<h>Somebody  had  to  have  the  equip-
ment.</h>
(3132.N entailment="YES")  <t>The first  was 
that America had become -- or was in danger of 
becoming  --  a  second-rate  military  power.</t>
<h>America was in danger.</h>
? 4069,  7003 and 3132.N are  examples  for 
sentences were beyond syntactical information lo-
gic  is  required.  Moreover  we are  surprised that 
sentences of the form ?if A, then B? entail B and 
a sentence of the form ?A or  B? entails  B, since 
?or? in this case means uncertainty.
(4071.N  entailment="NO")  <t>Interpublic 
Group said its television programming operations 
-- which it expanded earlier this year -- agreed to 
supply  more  than  4,000  hours  of  original  pro-
gramming across Europe in 1990.</t>
<h>Interpublic Group expanded.</h>
(6034  entailment="YES")  <t>"Oh,"  said  the 
woman, "I've seen that picture already."</t>
<h>The woman has seen something.</h>
? In 4071.N one has to resolve ?it? in ?it ex-
panded? to Interpublic Group. In 6034 one has to 
resolve ?I? in ?I've seen? to ?the woman?. Both 
cases are examples for the necessity of anaphora 
resolution, which goes beyond syntax as well.
(2055) <t>The Big Board also added computer 
capacity  to  handle  huge  surges  in  trading 
volume.</t>
<h>Surges were handled.</h>
? If something is added in order to do some-
thing it does not entail that this something is thus 
automatically done. Anyways pure syntax is not 
sufficient,  since  the  entailment  depends  on  the 
verb used in such a construction.
(3151.N) <t>Most of them are Democrats and 
nearly all consider themselves, and are viewed as, 
liberals.</t>
<h>Some consider themselves liberal.</h>
? One has to know that the semantics of ?con-
sider themselves as liberals? and ?consider them-
selves liberal? is the same.
Acknowledgements
311
The work presented here was partially suppor-
ted by a research grant from the German Federal 
Ministry of Economics and Technology (BMWi) 
to  the  DFKI  project  Theseus  Ordo  TechWatch 
(FKZ: 01MQ07016). We thank Joakim Nivre and 
Johan Hall for their support and tips when train-
ing models with MaltParser. Additionally, we are 
very grateful to Sven Schmeier for providing us 
with a trained part of speech tagger for English 
and for his support when using this tool. 
References 
Michael  A.  Covington,  2000.  A  Fundamental  Al-
gorithm for  Dependency  Parsing.  In  Proceedings  of 
the 39th Annual ACM Southeast Conference. 
Dan Klein and Christopher D. Manning, 2003. Accur-
ate  Unlexicalized  Parsing.  Proceedings  of  the  41st 
Meeting  of  the  Association  for  Computational  Lin-
guistics, pp. 423-430. 
Lin D, 2003.  Dependency-Based Evaluation Of Mini-
par. In  Building and using Parsed Corpora Edited by: 
Abeill? A. Dordrecht: Kluwer; 2003.
Sabine  Buchholz  and  Erwin  Marsi.  2006.  CoNLL-X 
shared  task  on  multilingual  dependency  parsing.  In 
Proceedings of CONLL-X, pages 149?164, New York.
Ido  Dagan,  Oren  Glickman  and  Bernardo  Magnini. 
The  PASCAL Recognising  Textual  Entailment  Chal-
lenge.  In  Quinonero-Candela, J.;  Dagan, I.;  Magnini, 
B.;  d'Alche-Buc,  F.  (Eds.),  Machine  Learning  Chal-
lenges. Lecture Notes in Computer Science, Vol. 3944, 
pp. 177-190, Springer, 2006.
Nivre, J., J. Hall and J. Nilsson, 2006. MaltParser: A 
Data-Driven Parser-Generator for Dependency Pars-
ing.  In  Proceedings  of  the  fifth  international  confer-
ence  on  Language  Resources  and  Evaluation 
(LREC2006),  May  24-26,  2006,  Genoa,  Italy,  pp. 
2216-2219. 
Rui  Wang and G?nter  Neumann,  2007.  Recognizing 
Textual  Entailment  Using  a  Subsequence  Kernel  
Method. In Proceedings of AAAI 2007. 
R.  Fan,  K.  Chang,  C.  Hsieh,  X.  Wang,  and C.  Lin, 
2008. LIBLINEAR: A Library for Large Linear Classi-
fication.  Journal of Machine Learning Research, 9(4): 
1871?1874.
Deniz Yuret,  Ayd?n Han and Zehra Turgut, 2010. Se-
mEval-2010 Task 12: Parser Evaluation using Textual  
Entailments. In  Proceedings  of  the  SemEval-2010 
Evaluation Exercises on Semantic Evaluation. 
The  Stanford  Parser:  A  Statistical  Parser.  
http://nlp.stanford.edu/downloads/lex-parser.shtml
Maltparser. http://maltparser.org/
Minipar. http://webdocs.cs.ualberta.ca/~lindek/mini-
par.htm
MDParser:  Multilingual  Dependency  Parser. 
http://mdparser.sb.dfki.de/
312
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 213?217
Manchester, August 2008
A Puristic Approach for Joint Dependency Parsing and Semantic 
Role Labeling 
Alexander Volokh 
LT-lab, DFKI 
66123 Saarbr?cken, Germany 
Alexander.Volokh@dfki.de 
G?nter Neumann 
LT-lab, DFKI 
66123 Saarbr?cken, Germany 
neumann@dfki.de 
 
Abstract 
We present a puristic approach for com-
bining dependency parsing and semantic 
role labeling. In a first step, a data-driven 
strict incremental deterministic parser is 
used to compute a single syntactic de-
pendency structure using a MEM trained 
on the syntactic part of the CoNLL 2008 
training corpus. In a second step, a cas-
cade of MEMs is used to identify predi-
cates, and, for each found predicate, to 
identify its arguments and their types. All 
the MEMs used here are trained only 
with labeled data from the CoNLL 2008 
corpus. We participated in the closed 
challenge, and obtained a labeled macro 
F1 for WSJ+Brown of 19.93 (20.13 on 
WSJ only, 18.14 on Brown). For the syn-
tactic dependencies we got similar bad 
results (WSJ+Brown=16.25, WSJ= 16.22, 
Brown=16.47), as well as for the seman-
tic dependencies (WSJ+Brown=22.36, 
WSJ=22.86, Brown=17.94). The current 
results of the experiments suggest that 
our risky puristic approach of following a 
strict incremental parsing approach to-
gether with the closed data-driven per-
spective of a joined syntactic and seman-
tic labeling was actually too optimistic 
and eventually too puristic. 
The CoNLL 2008 shared task on joint parsing of 
syntactic and semantic dependencies (cf. Sur-
deanu, 2008) offered to us an opportunity to ini-
tiate, implement and test new ideas on large-
scale data-driven incremental dependency pars-
ing. The topic and papers of the ACL-2004 
workshop ?Incremental Parsing: Bringing Engi-
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
neering and Cognition Together? (accessible at 
http://aclweb.org/anthology-new/W/W04/#0300) 
present a good recent overview into the field of 
incremental processing from both an engineering 
and cognitive point of view. 
Our particular interest is the exploration and 
development of strict incremental deterministic 
strategies as a means for fast data-driven depend-
ency parsing of large-scale online natural lan-
guage processing. By strict incremental process-
ing we mean, that the parser receives a stream of 
words w1 to wn word by word in left to right or-
der, and that the parser only has information 
about the current word wi, and the previous 
words w1 to wi-1.1 By deterministic processing we 
mean that the parser has to decide immediately 
and uniquely whether and how to integrate the 
newly observed word wi with the already con-
structed (partial) dependency structure without 
the possibility of revising its decision at later 
stages. The strategy is data-driven in the sense 
that the parsing decisions are made on basis of a 
statistical language model, which is trained on 
the syntactic part of the CoNLL 2008 training 
corpus. The whole parsing strategy is based on 
Nivre (2007), but modifies it in several ways, see 
sec. 2 for details. 
Note that there are other approaches of incre-
mental deterministic dependency parsing that 
assume that the complete input string of a sen-
tence is already given before parsing starts and 
that this additional right contextual information 
is also used as a feature source for language 
modeling, e.g., Nivre (2007). 
In light of the CoNLL 2008 shared task, this 
actually means that, e.g., part-of-speech tagging 
and lemmatization has already been performed 
                                                 
1
 Note that in a truly strict incremental processing 
regime the input to the NLP system is actually a 
stream of signals where even the sentence segmenta-
tion is not known in advance. Since in our current 
system, the parser receives a sentence as given input, 
we are less strict as we could be. 
213
for the complete sentence before incremental 
parsing starts, so that this richer source of infor-
mation is available for defining the feature space. 
Since, important word-based information espe-
cially for a dependency analysis is already 
known for the whole sentence before parsing 
starts, and actually heavily used during parsing, 
one might wonder, what the benefit of such a 
weak incremental parsing approach is compared 
to a non-incremental approach. Since, we 
thought that such an incremental processing per-
spective is a bit too wide (especially when con-
sidering the rich input of the CoNLL 2008 shared 
task), we wanted to explore a strict incremental 
strategy. 
Semantic role labeling is considered as a post-
process that is applied on the output of the syn-
tactic parser. Following Hacioglu (2004), we 
consider the labeling of semantic roles as a clas-
sification problem of dependency relations into 
one of several semantic roles. However, instead 
of post-processing a dependency tree firstly into 
a sequence of relations, as done by Hacioglu 
(2004), we apply a cascade of statistical models 
on the unmodified dependency tree in order to 
identify predicates, and, for each found predicate, 
to identify its arguments and their types. All the 
language models used here are trained only with 
labeled data from the CoNLL 2008 corpus; cf. 
sec. 3 for more details. 
Both, the syntactic parser and the semantic 
classifier are language independent in the sense 
that only information contained in the given 
training corpus is used (e.g., PoS tags, depend-
ency labels, information about direction etc.), but 
no language specific features, e.g., no PropBank 
frames nor any other external language and 
knowledge specific sources. 
The complete system has been designed and 
implemented from scratch after the announce-
ment of the CoNLL 2008 shared task. The main 
goal of our participation was therefore actually 
on being able to create some initial software im-
plementation and baseline experimentations as a 
starting point for further research in the area of 
data-driven incremental deterministic parsing. 
In the rest of this brief report, we will describe 
some more details of the syntactic and semantic 
component in the next two sections, followed by 
a description and discussion of the achieved re-
sults. 
1 Syntactic Parsing 
Our syntactic dependency parser is a variant of 
the incremental non-projective dependency 
parser described in Nivre (2007). Nivres? parser 
is incremental in the sense, that although the 
complete list of words of a sentence is known, 
construction of the dependency tree is performed 
strictly from left to right. It uses Treebank-
induced classifiers to deterministically predict 
the actions of the parser. The classifiers are 
trained using support vector machines (SVM). A 
further interesting property of the parser is its 
capability to derive (a subset of) non-projective 
structures directly. The core idea here is to ex-
ploit a function permissible(i, j, d) that returns 
true if and only if the dependency links i ? j and 
j ? i have a degree less than or equal to d given 
the dependency graph built so far. A degree d=0 
gives strictly projective parsing, while setting 
d=? gives unrestricted non-projective parsing; cf. 
Nivre (2007) for more details. The goal of this 
function is to restrict the call of a function link(i, 
j) which is a nondeterministic operation that adds 
the arc i ? j, the arc j ? i, or does nothing at all. 
Thus the smaller the value of d is the fewer links 
can be drawn. 
The function link(i, j) is directed by a trained 
SVM classifier that takes as input the feature rep-
resentation of the dependency tree built so far 
and the (complete) input x = w1, ?, wn and out-
puts a decision for choosing exactly one of the 
three possible operations.  
We have modified Nivres algorithm as follows: 
1. Instead of using classifiers learned by 
SVM, we are using classifiers based on 
Maximum Entropy Models (MEMs), cf. 
(Manning and Sch?tze, 1999).2 
2. Instead of using the complete input x, we 
only use the prefix from w1 up to the cur-
rent word wi. In this way, we are able to 
model a stricter incremental processing 
regime. 
3. We are using a subset of feature set de-
scribed in Nivre (2007). 3  In particular, 
we had to discard all features from 
Nivre?s set that refer to a word right to 
the current word in order to retain our 
                                                 
2
 We are using the opennlp.maxent package available 
via http://maxent.sourceforge.net/. 
3
 We mean here all features that are explicitly de-
scribed in Nivre (2007). He also mentions the use of 
some additional language specific features, but they 
are not further described, and, hence not known to us. 
214
strict incremental behavior. Additionally, 
we added the following features: 
a. Has j more children in the current 
dependency graph compared with 
the average number of children of 
element of same POS. 
b. Analogously for node i 
c. Distance between i and j 
Although some results ? for example Wang et 
al. (2006) ? suggest that SVMs are actually more 
suitable for deterministic parsing strategies than 
MEMs, we used MEMs instead of SVM basi-
cally for practical reasons: 1) we already had 
hands-on experience with MEMs, 2) training 
time was much faster than SVM, and 3) the theo-
retical basis of MEMs should give us enough 
flexibility for testing with different sets of fea-
tures. 
Initial experiments applied on the same cor-
pora as used by Nivre (2007), soon showed that 
our initial prototype is certainly not competitive 
in its current form. For example, our best result 
on the TIGER Treebank of German (Brants et al, 
2002) is 53.6% (labeled accuracy), where Nivre 
reports 85.90%; cf. Volokh (2008) and sec. 4 for 
more details 
Anyway, we decided to use it as a basis for the 
CoNLL 2008 shared task and to combine it with 
a component for semantic role labeling at least to 
get some indication of ?what went wrong?. 
2 Semantic Role Labeling 
On the one hand, it is clear that we should expect 
that our current version of the strict incremental 
deterministic parsing regime still returns too er-
roneous dependency analysis. On the other hand, 
we decided to apply semantic role labeling on the 
parser?s output. Hence, the focus was set towards 
a robust strictly data-driven approach. 
Semantic role labeling is modeled as a se-
quence of classifiers that follow the structure of 
predicates, i.e., firstly candidate predicates are 
identified and then the arguments are looked up. 
Predicate and argument identification both 
proceed in two steps: first determine whether a 
word can be a predicate or argument (or not), and 
then, each found predicate (argument) is typed. 
More precisely, semantic role labeling receives 
the output of the syntactic parser and performs 
the following steps in that order: 
1. Classify each word as being a predicate 
or not using a MEM-based classifier. 
2. Assign to each predicate its reading. Cur-
rently, this is done on basis of the fre-
quency readings as determined from the 
corpus (for unknown words, we simply 
assign the reading .01 to the lemma if the 
whole word was classified as a predicate). 
3. For each predicate identified in a sen-
tence, classify each word as argument for 
this predicate or not using a MEM-based 
classifier. 
4. For each argument identified for each 
predicate, assign its semantic role using a 
MEM-based classifier. 
For step 1 the following features are used for 
word wi: 1) word form, 2) word lemma, 3) POS, 
4) dependency type, 5) number of dependent 
elements in subtree of wi, 6) POS of parent, 7) 
dependency type of parent, 8) children or parent 
of word belong to prepositions, and 9) parent is 
predicate. 
For step 3 the same features are used as in step 
1, but 5) (for arguments the number of children is 
not important) and two additional features are 
used: 10) left/right of predicate (arguments are 
often to the right of its predicate), and 11) dis-
tance to predicate (arguments are not far from the 
predicate). Finally, for step 4 the same features 
are used as in step 1, but 5) and 9). 
3 Experiments 
As mentioned above, we started the develop-
ment of the system from scratch with a very 
small team (actually only one programmer). 
Therefore we wanted to focus on certain aspects, 
totally abandoning our claims for achieving de-
cent results for the others. One of our major 
goals was the construction of correct syntactic 
trees and the recognition of the predicate-
argument structure - a subtask which mainly cor-
responds to the unlabeled accuracy. For that rea-
son we reduced the scale of our experiments 
concerning such steps as dependency relation 
labeling, determining the correct reading for the 
predicates or the proper type of the arguments. 
Unfortunately only the labeled accuracy was 
evaluated at this year?s task, which was very 
frustrating in the end. 
3.1 Syntactic Dependencies 
For testing the strict incremental dependency 
parser we used the CoNLL 2008 shared task 
training and development set. Our best syntactic 
score that we could achieve on the development 
data was merely unlabeled attachment score 
(UAL) of 45.31%. However, as mentioned in sec. 
2, we used a set of features proposed by Nivre, 
215
which contains 5 features relying on the depend-
ency types. Since we couldn?t develop a good 
working module for this part of the task due to 
the lack of time, we couldn?t exploit these fea-
tures.  
Note that for this experiment and all others re-
ported below, we used the default settings of the 
opennlp MEM trainer. In particular this means 
that 100 iterations were used in all training runs 
and that for all experiments no tuning of parame-
ters and smoothing was done, basically because 
we had no time left to exploit it in a sensible way. 
These parts will surely be revised and improved 
in the future. 
3.2 Semantic Dependencies 
As we describe in the sec. 3 our semantic module 
consists of 4 steps. For the first step we achieve 
the F-score of 76.9%. Whereas the verb predi-
cates are recognized very well (average score for 
every verb category is almost 90%), we do badly 
with the noun categories. Since our semantic 
module depends on the input produced by the 
syntactic parser, and is influenced by its errors, 
we also did a test assuming a 100% correct parse. 
In this scenario we could achieve the F-score of 
79.4%. 
We have completely neglected the second step 
of the semantic task. We didn?t even try to do the 
feature engineering and to train a model for this 
assignment, basically because of time con-
straints. Neither did we try to include some in-
formation about the predicate-argument structure 
in order to do better on this part of the task. The 
simple assignment of the statistically most fre-
quent reading for each predicate reduced the ac-
curacy from 76.9% down to 69.3%. In case of 
perfect syntactic parse the result went down from 
79.4% to 71.5%. 
Unfortunately the evaluation software doesn?t 
provide the differentiation between the unlabeled 
and labeled argument recognition, which corre-
sponds to our third and fourth steps respectively. 
Whereas we put some effort on identifying the 
arguments, we didn?t focus on their classifica-
tion. Therefore the overall best labeled attach-
ment score for our system is 29.38%, whereas 
the unlabeled score is 50.74%. Assuming the 
perfect parse the labeled score is 32.67% and the 
unlabeled score is 66.73%. In our further work 
we will try to reduce this great deviation between 
both results. 
3.3 Runtime performance 
One of the main strong sides of the strict incre-
mental approach is its runtime performance. 
Since we are restricted in our feature selection 
to the already seen space to the left of the current 
word, both the training and the application of our 
strategy are done fast.  
The training of our MEMs for the syntactic 
part requires 62 minutes. The training of the 
models for our semantic components needs 31 
minutes. The test run of our system for the test 
data from the Brown corpus (425 sentences with 
7207 tokens) lasted 1 minute and 18 seconds. 
The application on the WSJ test data (2399 sen-
tences with 57676 tokens) took 20 minutes and 
42 seconds. The experiments have been per-
formed on a computer with one Intel Pentium 
1,86 Ghz processor and 1GB memory. 
4 Results and Discussion 
The results of running our current version on the 
CoNLL 2008 shared task test data were actually 
a knockdown blow. We participated in the closed 
challenge, and obtained for the complete problem 
a labeled macro F1 for WSJ+Brown of 19.93 
(20.13 on WSJ only, 18.14 on Brown). For the 
syntactic dependencies we got similar bad results 
(WSJ+Brown = 16.25, WSJ = 16.22, Brown = 
16.47), as well as for the semantic dependencies 
(WSJ+Brown = 22.36, WSJ = 22.86, Brown = 
17.94).  
We see at least the following two reasons for 
this disastrous result: On the one hand we fo-
cused on the construction of correct syntactic 
trees and the recognition of the predicate-
argument structure which were only parts of the 
task. On the other hand we stuck to our strict in-
cremental approach, which greatly restricted the 
scope of development of our system. 
Whereas the labeling part, which was so far 
considerably neglected, will surely be improved 
in the future, the strict incremental strategy in its 
current form will probably have to be revised. 
4.1 Post-evaluation experiments 
We have already started beginning the im-
provement of our parsing system, and we briefly 
discuss our current findings. On the technical 
level we already found a software bug that at 
least partially might explain the unexpected high 
difference in performance between the results 
obtained for the development set and the test set. 
Correcting this error now yields an UAL of 
53.45% and an LAL of 26.95% on the syntactic 
216
part of the Brown test data which is a LAL-
improvement of about 10%. 
On the methodological level we are studying 
the effects of relaxing some of the assumptions 
of our strict incremental parsing strategy. In or-
der to do so, we developed a separate model for 
predicting the unlabeled edges and a separate 
model for labeling them. In both cases we used 
the same features as described in sec. 2, but 
added features that used a right-context in order 
to take into account the PoS-tag of the N-next 
words viz. N=5 for the syntactic parser and N=3 
for the labeling case. Using both models during 
parsing interleaved, we obtained UAL=65.17% 
and LAL=28.47% on the development set.  
We assumed that the low LAL might have 
been caused by a too narrow syntactic context. In 
order to test this assumption, we decoupled the 
prediction of the unlabeled edges and their label-
ing, such that the determination of the edge la-
bels is performed after the complete unlabeled 
dependency tree is computed. Labeling of the 
dependency edges is then simply performed by 
running through the constructed parse trees as-
signing each edge the most probable dependency 
type. This two-phase strategy achieved an LAL 
of 60.44% on the development set, which means 
an improvement of about 43%. Applying the 
two-phase parser on the WSJ test data resulted in 
UAL=65.22% and LAL=62.83%; applying it on 
the Brown test data resulted in UAL=66.50% and 
LAL=61.11%, respectively. 
Of course, these results are far from being op-
timal. Thus, beside testing and improving our 
parser on the technical level, we will run further 
experiments for different context sizes, exploit-
ing different settings of parameters of the classi-
fier and feature values, and eventually testing 
other ML approaches. The focus here will be on 
the development of unlabeled edge models, be-
cause it seems that an improvement here is sub-
stantial for an overall improvement. For exam-
ple, applying the decoupled edge labeling model 
directly on the given unlabeled dependency trees 
of the development set (i.e. we assume an UAL 
of 100%) gave as an LAL of 92.88%. 
Beside this, we will also re-investigate inter-
leaved strategies of unlabeled edge and edge la-
beling prediction as a basis for (mildly-) strict 
incremental parsing. Here, it might be useful to 
relax the strict linear control regime by exploring 
beam search strategies, e.g. along the lines of 
Collins and Roark (2004). 
5 Conclusion 
We have presented a puristic approach for 
joint dependency parsing and semantic role la-
beling. Since, the development of our approach 
has been started from scratch, we didn?t manage 
to deal with all problems. Our focus was on set-
ting up a workable backbone, and then on trying 
to do as much feature engineering as possible. 
Our bad results on the CoNLL 2008 suggest that 
our current strategy was a bit too optimistic and 
risky, and that the strict incremental deterministic 
parsing regime seemed to have failed in its cur-
rent form. We are now in the process of analysis 
of ?what went wrong?, and have already indi-
cated some issues in the paper. 
Acknowledgement 
The work presented here was partially supported 
by a research grant from the German Federal 
Ministry of Education, Science, Research and 
Technology (BMBF) to the DFKI project HyLaP, 
(FKZ: 01 IW F02). We thank the developers of 
the Opennlp.maxent software package. 
References 
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER 
Treebank in Proceedings of the Workshop on 
Treebanks and Linguistic Theories Sozopol.  
Collins, Michael, and Brian Roark. (2004). Incre-
mental Parsing with the Perceptron Algorithm. 
ACL 2004. 
Hacioglu, Kadri. 2004. Semantic Role Labeling Using 
Dependency Trees. Coling 2004. 
Nivre, Joakim. 2007. Incremental Non-Projective De-
pendency Parsing. NAACL-HLT 200). 
Manning, Christopher, and Hinrich Schutze. 1999. 
Foundations of statistical natural language process-
ing. Cambridge, Mass.: MIT Press. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008). 
Volokh, Alexander. 2008. Datenbasiertes De-
pendenzparsing. Bachelor Thesis, Saarland Uni-
versity. 
Wang, Mengqui, Kenji Sagae, and Teruko Mitamura. 
2006. A Fast, Accurate Deterministic Parser for 
Chinese. ACL 2006. 
217
