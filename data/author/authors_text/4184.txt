Period Disambiguation with Maxent Model
Chunyu Kit and Xiaoyue Liu
Department of Chinese, Translation and Linguistics,
City University of Hong Kong, 83 Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0}@cityu.edu.hk
Abstract. This paper presents our recent work on period disambigua-
tion, the kernel problem in sentence boundary identification, with the
maximum entropy (Maxent) model. A number of experiments are con-
ducted on PTB-II WSJ corpus for the investigation of how context
window, feature space and lexical information such as abbreviated and
sentence-initial words affect the learning performance. Such lexical in-
formation can be automatically acquired from a training corpus by a
learner. Our experimental results show that extending the feature space
to integrate these two kinds of lexical information can eliminate 93.52%
of the remaining errors from the baseline Maxent model, achieving an
F-score of 99.8227%.
1 Introduction
Sentence identification is an important issue in practical natural language pro-
cessing. It looks simple at first glance since there are a very small number of
punctuations, namely, period (?.?), question mark (???), and exclamation (?!?),
to mark sentence ends in written texts. However, not all of them are consistently
used as sentence ends. In particular, the use of the dot ?.? is highly ambiguous
in English texts. It can be a full stop, a decimal point, or a dot in an abbreviated
word, a numbering item, an email address or a ULR. It may be used for other
purposes too. Below are a number of examples from PTB-II WSJ Corpus to
illustrate its ambiguities.
(1) Pierre Vinken, 61 years old, will join the board as a nonexecutive
director Nov. 29.
(2) The spinoff also will compete with International Business Machines
Corp. and Japan?s Big Three -- Hitachi Ltd., NEC Corp. and Fujitsu
Ltd.
(3) The government?s construction spending figures contrast with a report
issued earlier in the week by McGraw-Hill Inc.?s F.W. Dodge Group.
Frequently, an abbreviation dot coincides with a full stop, as exemplified by
?Ltd.? in (2) above. A number followed by a dot can be a numbering item, or
simply a normal number at sentence end.
In contrast to ?.?, ?!? and ??? are rarely ambiguous. They are seldom used
for other purposes than exclamation and question marks. Thus, the focus of
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 223?232, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
224 C. Kit and X. Liu
sentence identification is on period disambiguation to resolve the ambiguity of
?.?: Whenever a dot shows up in a text token, we need to determine whether or
not it is a true period. It is a yes-no classification problem that is suitable for
various kinds of machine learning technology to tackle.
Several approaches were developed for sentence splitting. These approaches
can be categorized into three classes: (1) rule-based models consisting of man-
ually constructed rules (e.g., in the form of regular expression), supplemented
with abbreviation lists, proper names and other relevant lexical resources, as
illustrated in [1]; (2) machine learning algorithms, e.g., decision tree classifiers
[11], maximum entropy (Maxent) modelling [10] and neural networks [8], among
many others; and (3) syntactic methods that utilize syntactic information, e.g.,
[6] is based on a POS tagger. The machine learning approaches are popular, for
period disambiguation is a typical classification problem for machine learning,
and the training data is easily available.
Our research reported in this paper explores how context length and feature
space affects the performance of the Maxent model for period disambiguation.
The technical details involved in this research are introduced in Section 2, with a
focus on feature selection and training algorithm. Section 3 presents experiments
to show the effectiveness of context length and feature selection on learning
performance. Section 4 concludes the paper with our findings: putting frequent
abbreviated words or sentence-initial words into the feature space significantly
enhances the learning performance, and using a three-word window context gives
better performance than others in terms of the F-score. The best combination of
the two kinds of lexical information achieves an F-score of 99.8227%, eliminating
93.5% remaining errors from the baseline Maxent model.
2 Feature Selection
The problem of period disambiguation can be formulated as a statistical classi-
fication problem. Our research is aimed at exploring the effectiveness of Maxent
model [2,12] tackling this problem when trained with various context length and
feature sets.
Maxent model is intended to achieve the most unbiased probabilistic distri-
bution on the data set for training. It is also a nice framework for integrating
heterogeneous information into a model for classification purpose. It has been
popular in NLP community for various language processing tasks since Berger
et al [2] and Della Pietra et al [3] presenting its theoretical basis and basic
training techniques. Ratnaparkhi [9] applied it to tackle several NL ambiguity
problems, including sentence boundary detection. Wallach [14] and Malouf [4]
compared the effectiveness of several training algorithms for Maxent model.
There are a number of full-fledged implementations of Maxent models avail-
able from the Web. Using the OpenNLP MAXENT package from http://
maxent.sourceforge.net/, acknowledged here with gratitude, we are released
from the technical details of its implementation and can concentrate on exam-
ining the effectiveness of context length and feature space on period disam-
Period Disambiguation with Maxent Model 225
biguation. Basically, our exploration is carried out along the following working
procedure: (1) prepare a set of training data in terms of the feature space we
choose; (2) train the Maxent model, and test its performance with a set of testing
data; (3) examine the errors in the test outcomes and adjust the feature space
for the next round of training and testing towards possible improvement.
2.1 Context and Features
To identify sentence boundaries, a machine learner needs to learn from the train-
ing data the knowledge whether or not a dot is a period in a given context .
Classification decision is based on the available contextual information. A con-
text is the few tokens next to the target. By ?target? we refer to the ?.? to
be determined whether or not it is a period, and by ?target word? (or ?dotted
word?) we refer to the token that carries the dot in question. The dot divides
the target word into prefix and suffix, both of which can be empty. Each dot has
a true or false answer for whether it is a true period in a particular context, as
illustrated by the following general format.
[ preceding-words prefix .suffix following-words ] ? Answer: true/false . (1)
Contextual information comes from all context words surrounding the target
dot, including its prefix and suffix. However, instead of feeding the above con-
textual items to a machine learner as a number of strings for training and
testing, extracting special and specific features from them for the training is
expected to achieve more effective results. To achieve a learning model as unbi-
ased as possible, we try to extract as many features as possible from the con-
text words, and let the training algorithm to determine their significance. The
main cost of using a large feature set is the increase of training time. However,
this may be paid off by giving the learner a better chance to achieve a better
model.
Table 1. Features for a context word
Feature Description Example
IsCap Starting with a capital letter On
IsRpunct Ending with a punctuation Calgary,
IsLpunct Starting with a punctuation ??We
IsRdot Ending with a dot billions.
IsRcomma Ending with a comma Moreover,
IsEword An English word street
IsDigit An numeric item 25%, 36
IsAllCap Consisting of only capital letters (& dots) WASHINGTON
The feature set for a normal context word that we have developed through sev-
eral rounds of experiments along the above working procedure are presented in
Table 1. Basically, we extract from a word all features that we can observe from its
226 C. Kit and X. Liu
Table 2. Features for a target word
Feature Description Example
IsHiphenated Containing a dash non-U.S.
IsAllCap Consisting of only capital letters (& dots) D.C.
IsMultiDot Containing more than one dot N.Y.,
prefixIsNull A null prefix .270
prefixIsRdigit Ending with a digit 45.6
prefixIsRpunct Ending with a punctuation 0.2%.
prefixIsEword An English word slightly.
prefixIsCap Starting with a capital letter Co.
suffixIsNull A null suffix Mr.
suffixIsLdigit Starting with a digit 78.99
suffixIsLpunct Starting with a punctuation Co.?s
suffixIsRword Ending with a word Calif.-based
suffixIsCap Starting with a capital letter B.A.T
text form. For feature extraction, this set is applied equally, in a principled way, to
all context words. The feature set for both parts of a target word is highly similar
to that for a context word, except for a few specific to prefix and/or suffix, as given
in Table 2, of 13 features in total. The data entry for a given dot, for either training
or testing, consists of all such features from its target word and each of its context
words. Given a context window of three tokens, among which one is target word,
there are 2?8+13=29 features, plus an answer, in each data entry for training.
After feature extraction, each data entry originally in the form of (1) is turned
into a more general form for machine learning, as shown in (2) below, consisting
of a feature value vector and an answer.
f : [f1 =v1, f2 =v2, f3 =v3, ? ? ? , fn =vn] ? a: true/false . (2)
Accordingly, the Maxent model used in our experiments has the following
distribution in the exponential form:
p(a|f) = 1
Z(f)
exp(
?
i
?i?(fi, a)) , (3)
where ?i is a parameter to be estimated for each i through training, the fea-
ture function ?i(fi, a) = vi for the feature fi in a data entry f ? a, and the
normalization factor
Z(f) =
?
a
exp(
?
i
?i?(fi, a)) . (4)
2.2 Abbreviation List and Sentence-Initial Words
In addition to the above features, other types of contextual information can
be helpful too. For example, abbreviated words like ?Dr.?, ?Mr.? and ?Prof.?
Period Disambiguation with Maxent Model 227
may give a strong indication that the dot they carry is very unlikely to be a
period. They may play the role of counter-examples. Another kind of useful
lexical resource is sentence-initial words, e.g., ?The?, ?That? and ?But?, which
give a strong indication that a preceding dot is very likely to be a true period.
In order to integrate these two kinds of lexical resource into the Maxent
model, we introduce two multi-valued features, namely, isAbbr and isSentInit,
for the target word and its following word, respectively. They are both multi-
valued feature function. A list of abbreviated words and a list of sentence-initial
words can be easily compiled from a training corpus. Theoretically, the larger
the lists are, the better the learning performance could be. Our experiments, to
be reported in the next section, show, however, that this is not true, although
using the most frequent words in the two lists up to a certain number does lead
to a significant improvement.
3 Experiments and Results
3.1 Corpus
The corpus used for our experiments is the PTB-II WSJ corpus, a refined version
of PTB [5]. It is particularly suitable for our research purpose. In contrast to
BNC and Brown corpus, the WSJ corpus indeed contains many more dots used
in different ways for various purposes. Sentence ends are clearly marked in its
POS tagged version, although a few mistakes need manual correction. Among
53K sentences from the corpus, 49K end with ?.?. This set of data is divided
into two for training and testing by the ratio of 2:1. The baseline performance
by brute-force guess of any dot as a period is 65.02% over the entire set of data.
3.2 Baseline Learning Performance
Our first experiment is to train a Maxent model on the training set with a
three-word context window in terms of the features in Tables 1 and 2 above.
The performance on the open test is presented in Table 3. It is the baseline
performance of the Maxent model.
Table 3. Baseline learning performance of Maxent model
Precision (%) Recall (%) F-score (%)
97.55 96.97 97.26
3.3 Effectiveness of Context Window
To examine how context words affect the learning performance, we carry out a
number of experiments with context windows of various size. The experimental
results are presented in Fig. 1, where x stands for the position of target word and
228 C. Kit and X. Liu
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0  2  4  6  8  10
F-
s c
o r
e
Context Type
00x00
01x00 11x00
00x10 00x11
01x10
11x10
01x11
11x11
Fig. 1. Effectiveness of context window
1 for a context word in use. For example, 01x10 represents a context window con-
sisting of a target word, its preceding and following words. Each such window is
itself a context type.
We can observe from the results that (1) the features extracted from the
target word itself already lead the Maxent model to an F-score beyond 92%,
(2) the context words preceding the target word are less effective, in general,
than those following the target, and (3) combining context words on both sides
outperforms those on only one side. The best three context types and the cor-
respondent performance are presented in Table 4. Since they are more effective
than others, the experiments to test the effectiveness of abbreviated words and
sentence-initial words are based on them.
Table 4. Outperforming context types and their performance
Context Type 01x10 11x10 11x11
F-score (%) 97.2623 97.6949 97.6909
3.4 Effectiveness of Abbreviated Words
Information about whether a target word is an abbreviation plays a critical role in
determining whether a dot is truly a period. To examine the significance of such
information, an abbreviation list is acquired from the training data by dotted word
collection, and sorted in terms of the difference of each item?s occurrences in the
middle and at the end of a sentence. It is assumed that the greater this difference is,
the more significant a dotted word would be as a counter-example. In total, 469
such words are acquired, among which many are not really abbreviated words.
A series of experiments are then conducted by adding the next 50 most frequent
dotted words to the abbreviation list for model training each time. To utilize such
Period Disambiguation with Maxent Model 229
 0.994
 0.9945
 0.995
 0.9955
 0.996
 0.9965
 0.997
 0.9975
 0.998
 0  100  200  300  400  500
F-
s c
o r
e
Abbreviated Word Number
01x10
11x10
11x11
Fig. 2. Effectiveness of abbreviation list
Table 5. Effectiveness of abbreviation list
Context Type 01x10 11x10 11x11
F-score (%) 99.6908 99.6908 99.6815
Increase +2.4285 +1.9959 +1.9906
lexical resource, a multi-valued feature isAbbr is introduced to the feature set to
indicate whether a target word is in the abbreviation list and what it is. That is,
all words in the list actually play a role equivalent to individual bi-valued features,
under the umbrella of this new feature.
The outcomes from the experiments are presented in Fig. 2, showing that
performance enhancement reaches rapidly to the top around 150. The perfor-
mance of the three best context types at this point is given in Table 5, indi-
cating that an abbreviation list of 150 words leads to an enhancement of 1.99?
2.43 percentage points, in comparison to Table 4. This enhancement is very
significant at this performance level. Beyond this point, the performance goes
down slightly.
3.5 Effectiveness of Sentence-Initial Words
In a similar way, we carry out a series of experiments to test the effectiveness
of sentence-initial words. In total, 4190 such words (word types) are collected
from the beginning of all sentences in the training corpus. Every time the next
200 most frequent words are added to the sentence-initial word list for training,
with the aid of another multi-valued feature isSentInit for the context word
immediately following the target word.
Experimental outcomes are presented in Fig. 3, showing that the performance
maintains roughly at the same level when the list grows. Until the very end,
230 C. Kit and X. Liu
 0.94
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0  500  1000  1500  2000  2500  3000  3500  4000
F-
s c
o r
e
Sentence Initial Word Number
01x10
11x10
11x11
Fig. 3. Effectiveness of sentence-initial words
Table 6. Performance enhancement by sentence-initial words
Context Type 01x10 11x10 11x11
List size 1200 1000 1200
F-score (%) 98.4307 98.4868 98.5463
Increase +1.1784 +0.7919 +0.8554
when those most infrequent (or untypical) sentence-initial words are added, the
performance drops rapidly. The numbers of sentence-initial words leading to the
best performance with various context types are presented in Table 6. This list
of words lead to a significant performance enhancement of 0.79?1.18 percentage
points, in comparison to Table 4.
3.6 Combination of Two Lists
Through the experiments reported above we find the optimal size of abbreviation
list and sentence-initial words, both in the order of their frequency ranks, in
each context type of our interests. The straightforward combination of these two
lists in terms of these optimal sizes leads to almost no difference from using
abbreviation list only, as presented in Table 7.
Table 7. Performance from simple combination of the two lists
Context Type 01x10 11x10 11x11
Sentence-initial words 1200 1000 1200
Abbreviation list 150 150 150
F-score (%) 99.7064 99.7156 99.6912
Period Disambiguation with Maxent Model 231
Table 8. Performance from various size combination of the two lists
Sentence-initial Abbreviation F-score
words list 01x10 11x10 11x11
100 200 99.7646% 99.7738% 99.7707%
100 400 99.7125% 99.7033% 99.7002%
100 600 99.7033% 99.6971% 99.6971%
100 800 99.6788% 99.6941% 99.6911%
100 1000 99.6696% 99.6818% 99.6696%
100 1200 99.6635% 99.6574% 99.6544%
150 200 99.8013% 99.7890% 99.7921%
150 400 99.7431% 99.7339% 99.7369%
150 600 99.7431% 99.7370% 99.7370%
150 800 99.7401% 99.7309% 99.7278%
150 1000 99.7156% 99.7156% 99.7064%
150 1200 99.7064% 99.7034% 99.6912%
200 200 99.8227% 99.7890% 99.7921%
200 400 99.7584% 99.7461% 99.7339%
200 600 99.7523% 99.7431% 99.7339%
200 800 99.7462% 99.7370% 99.7340%
200 1000 99.7309% 99.7125% 99.7064%
200 1200 99.7095% 99.6973% 99.6911%
To explore the optimal combination of the two lists, a series of experi-
ments are carried out near each list?s optimal size. The results are presented in
Table 8, showing that the best combination is around 200 words from each list
and any deviation from this point would lead to observable performance declina-
tion. The best performance at this optimal point is 99.8227% F-score, achieved
with the 01x10 context type, which is significantly better than the best perfor-
mance using any single list of the two.
Comparing to the baseline performance of the Maxent model in Table 4,
we can see that this improvement increases only 99.8227 - 97.2623 = 2.5604
percentage points. Notice, however, that it is achieved near the ceiling level. Its
particular significance lies in the fact that 99.8227?97.2623100?97.2623 = 93.52% remaining
errors from the baseline model are further eliminated by this combination of the
two lists, both of which are of a relatively small size.
4 Conclusions
We have presented in the above sections our recent investigation into how con-
text window, feature space and simple lexical resources like abbreviation list and
sentence-initial words affect the performance of the Maxent model on period dis-
ambiguation, the kernel problem in sentence identification. Our experiments on
PTB-II WSJ corpus suggest the following findings: (1) the target word itself pro-
vides most useful information for identifying whether or not the dot it carries is a
232 C. Kit and X. Liu
true period, achieving an F-score beyond 92%; (2) unsurprisingly, the most useful
context words are the two words next to the target word, and the context words to
its right is more informative in general than those to its left; and (3) extending the
feature space to utilize lexical information from the most frequent 200 abbreviated
words and sentence-initial words, all of which can be straightforwardly collected
from the training corpus, can eliminate 93.52% remaining errors from the baseline
model in the open test, achieving an F-score of 99.8227%.
Acknowledgements
The work described in this paper was supported by the Research Grants Council
of HKSAR, China, through the CERG grant 9040861 (CityU 1318/03H). We
wish to thank Alex Fang for his help.
References
1. Aberdeen, J., Burger, J., Day, D., Hirschman, L., Robinson, P., and Vilain, M.:
Mitre: Description of the alembic system used for muc-6. In Proceedings of the
Sixth Message Understanding Conference (MUC-6), Columbia, Maryland. Morgan
Kaufmann (1995)
2. Berger, A., Pietra, S.D., and Pietra, V.D.: A maximum entropy approach to natural
language processing. Computational linguistics. (1996) 22(1):39?71
3. Della Pietra, S., Della Pietra, V., and Lafferty, J.: Inducing features of ran-
dom fields. Transactions Pattern Analysis and Machine Intelligence. (1997) 19(4):
380?393
4. Malouf, R.: A comparison of algorithms for maximum entropy parameter estima-
tion. In Proceedings of CoNLL-2002, Taipei, Taiwan (2002) 49?55
5. Marcus, M.P., Santorini, B., and Marcinkiewicz, M.A.: Building a large annotated
corpus of english: The penn treebank. Computational Linguistics. (1993) 19(2):
313?329
6. Mikheev, A.: Tagging sentence boundaries. In Proceedings of the First Meeting
of the North American Chapter of the Association for Computational Linguistics
(NAACL?2000). (2000)
7. Mitchell, T.: Machine Learning. McGraw Hill, New York (1997)
8. Palmer, D.D. and Hearst, M.A.: Adaptive Multilingual Sentence Boundary Disam-
biguation. Computational Linguistics. (1997) 23(2):241?267
9. Ratnaparkhi, A.: Maximum entropy models for natural language ambiguity resolu-
tion. Ph.D. dissertation, University of Pennsylvania (1998)
10. Reynar, J.C. and Ratnaparkhi, A.: A maximum entropy approach to identifying
sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural
Language Processing, Washington, D.C. (1997)
11. Riley, M.D.: Some applications of tree-based modelling to speech and language
indexing. In Proceedings of the DARPA Speech and Natural Language Workshop.
Morgan Kaufmann (1989) 339?352
12. Rosenfeld, R.: Adaptive statistical language modeling: A Maximum Entropy Ap-
proach. PhD thesis CMU-CS-94. (1994)
13. Van Rijsbergen, C.J.: Information Retrieval. Butterworths, London (1979)
14. Wallach, H.M.: Efficient training of conditional random fields. Master?s thesis, Uni-
versity of Edinburgh (2002)
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30?39,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Semantic Dependency Parsing of NomBank and PropBank:
An Efficient Integrated Approach via a Large-scale Feature Selection
?
Hai Zhao(??)?, Wenliang Chen(???)?, Chunyu Kit?(???)
?
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Avenue, Kowloon, Hong Kong, China
?
Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
We present an integrated dependency-
based semantic role labeling system for
English from both NomBank and Prop-
Bank. By introducing assistant argument
labels and considering much more fea-
ture templates, two optimal feature tem-
plate sets are obtained through an effec-
tive feature selection procedure and help
construct a high performance single SRL
system. From the evaluations on the date
set of CoNLL-2008 shared task, the per-
formance of our system is quite close to
the state of the art. As to our knowl-
edge, this is the first integrated SRL sys-
tem that achieves a competitive perfor-
mance against previous pipeline systems.
1 Introduction
We investigate the possibility to construct an effec-
tive integrated system for dependency-based se-
mantic role labeling (SRL) task. This means in
this work that a single system handles all these
sub-tasks, predicate identification/disambiguation
and argument identification/classification, regard-
less of whether the predicate is verbal or nominal.
Traditionally, a SRL task, either dependency
or constituent based, is implemented as two sub-
tasks, namely, argument identification and clas-
sification. If the predicate is unknown, then a
predicate identification or disambiguation subtask
should be additionally considered. A pipeline
framework is usually adopted to handle all these
sub-tasks. The reason to divide the whole task
?
This study is partially supported by CERG grant
9040861 (CityU 1318/03H), CityU Strategic Research Grant
7002037.
into multiple stages is two-fold, one is each sub-
task asks for its favorable features, the other is
at the consideration of computational efficiency.
Generally speaking, a joint system is slower than
a pipeline system in training. (Xue and Palmer,
2004) fount out that different features suited for
different sub-tasks of SRL, i.e. argument identifi-
cation and classification. The results from CoNLL
shared tasks in 2005 and 2008 (Carreras and Mar-
quez, 2005; Koomen et al, 2005; Surdeanu et al,
2008; Johansson and Nugues, 2008), further show
that SRL pipeline may be one of the standard to
achieve a state-of-the-art performance in practice.
In the recent years, most works on SRL, includ-
ing two CoNLL shared task in 2004 and 2005,
focus on verbal predicates with the availability
of PropBank (Palmer et al, 2005). As a com-
plement to PropBank, NomBank (Meyers et al,
2004) annotates nominal predicates and their cor-
responding semantic roles using similar semantic
framework as PropBank. Though SRL for nomi-
nal predicates offers more challenge, it draws rel-
atively little attention (Jiang and Ng, 2006).
(Pustejovsky et al, 2005) discussed the issue of
merging various treebanks, including PropBank,
NomBank, and others. The idea of merging these
two different treebanks was implemented in the
CoNLL-2008 shared task (Surdeanu et al, 2008).
However, few empirical studies support the ne-
cessity of an integrated learning strategy from
NomBank and PropBank. Though aiming at Chi-
nese SRL, (Xue, 2006) reported that their exper-
iments show that simply adding the verb data to
the training set of NomBank and extracting the
same features from the verb and noun instances
will hurt the overall performance. From the re-
sults of CoNLL-2008 shared task, the top system
by (Johansson and Nugues, 2008) also used two
30
different subsystems to handle verbal and nominal
predicates, respectively.
Despite all the above facts, an integrated SRL
system still holds some sort of merits, being eas-
ier to implement, a single-stage feature selection
benefiting the whole system, an all-in-one model
outputting all required semantic role information
and so on.
The shared tasks at the CoNLL 2008 and 2009
are devoted to the joint learning of syntactic and
semantic dependencies, which show that SRL can
be well performed using only dependency syn-
tax input. Using data and evaluation settings
of the CoNLL-2008 shared task, this work will
only focus on semantic dependency parsing and
compares the best-performing SRL system in the
CoNLL-2009 shared Task (Zhao et al, 2009b)
with those in the CoNLL-2008 shared task (Sur-
deanu et al, 2008; Haji?c et al, 2009)
1
.
Aiming at main drawbacks of an integrated ap-
proach, two key techniques will be applied. 1)
Assistant argument labels are introduced for the
further improvement of argument pruning. This
helps the development of a fast and lightweight
SRL system. 2) Using a greedy feature selec-
tion algorithm, a large-scale feature engineering is
performed on a much larger feature template set
than that in previous work. This helps us find fea-
tures that may be of benefit to all SRL sub-tasks as
long as possible. As two optimal feature template
sets have been proven available, for the first time
we report that an integrated SRL system may pro-
vide a result close to the state-of-the-art achieved
by those SRL pipelines or individual systems for
some specific predicates.
2 Adaptive Argument Pruning
A word-pair classification is used to formulate se-
mantic dependency parsing as in (Zhao and Kit,
2008). As for predicate identification or disam-
biguation, the first word is set as a virtual root
(which is virtually set before the beginning of the
sentence.) and the second as a predicate candi-
date. As for argument identification/classification,
the first word in a word pair is specified as a predi-
1
CoNLL-2008 is an English-only task, while CoNLL-
2009 is a multilingual one. Though the English corpus in
CoNLL-2009 is almost identical to the corpus in the CoNLL-
2008 shared task evaluation, the latter holds more sophisti-
cated input structure as in (Surdeanu et al, 2008). The most
difference for these two tasks is that the identification of se-
mantic predicates is required in the task of CoNLL-2008 but
not in CoNLL-2009.
cate candidate and the second as an argument can-
didate. In either of case, the first word is called a
semantic head, and noted as p in our feature rep-
resentation, the second is called a semantic depen-
dent and noted as a.
Word pairs are collected for the classifier in
such order. The first word of the pair is set to the
virtual root at first, the second word is then spec-
ified as a predicate candidate. According to the
result that the predicate candidate is classified or
proven to be non-predicate, 1) the second word is
reset to next predicate candidate if the answer is
non-predicate, otherwise, 2) the first word of the
pair is reset to the predicate that is just determined,
and the second is set to every argument candidates
one by one. The classifier will scan the input sen-
tence from left to right to check if each word is a
true predicate.
Without any constraint, all word pairs in an in-
put sequence must be considered by the classifier,
leading to poor computational efficiency and un-
necessary performance loss. Thus, the training
sample for SRL task needs to be pruned properly.
We use a simple strategy to prune predicate can-
didates, namely, only verbs and nouns are chosen
in this case.
There are two paths to collect argument candi-
dates over the sequence. One is based on an input
syntactic dependency tree, the other is based on
a linear path of the sentence. As for the former
(hereafter it is referred to synPth), we continue to
use a dependency version of the pruning algorithm
of (Xue and Palmer, 2004). The pruning algorithm
is readdressed as the following.
Initialization: Set the given predicate as the
current node;
(1) The current node and all of its syntactic
children are selected as argument candidates
(children are traversed from left to right.).
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
Note that this pruning algorithm is slightly dif-
ferent from that of (Xue and Palmer, 2004), the
predicate itself is also included in the argument
candidate list as the nominal predicate sometimes
takes itself as its argument.
The above pruning algorithm has been shown
effective. However, it is still inefficient for a SRL
31
system that needs to tackle argument identifica-
tion/classification in a single stage. Assuming that
arguments trend to surround their predicate, an as-
sistant argument label ? NoMoreArgument? is in-
troduced for further pruning. If an argument can-
didate in the above algorithm is assigned to such
a label, then the pruning algorithm will end im-
mediately. In training, this assistant label means
no more samples will be generated for the current
predicate, while in test, the decoder will not search
arguments any more. It will be seen that this adap-
tive technique more effectively prunes argument
candidates without missing more true arguments.
Along the linear path (hereafter referred to
linPth), the classifier will search all words before
and after the predicate. Similar to the pruning
algorithm for synPth, we also introduce two as-
sistant argument labels ? noLeft? and ? noRight?
to adaptively prune words too far away from the
predicate.
To show how assistant argument labels actually
work, we give an example for linP th. Suppose an
input sequence with argument labels for a predi-
cate is
a b c d e f g h .
A1 A0
Note that c and g are two boundary words as no
more arguments appear before or after them. After
two assistant argument labels are added, it will be
a b c d e f g h .
noLeft A1 A0 noRight
Training samples will generated from c to g ac-
cording to the above sequence.
We use a Maximum Entropy classifier with a
tunable Gaussian prior as usual. Our implemen-
tation of the model adopts L-BFGS algorithm for
parameter optimization.
3 Feature Templates
3.1 Elements for Feature Generation
Motivated by previous works, we carefully con-
sider those factors from a wide range of features
that can help semantic role labeling for both predi-
cate disambiguation, argument?s identification and
classification as the predicate is either verbal or
nominal. These works include (Gildea and Juraf-
sky, 2002; Carreras and Marquez, 2005; Koomen
et al, 2005; Marquez et al, 2005; Dang and
Palmer, 2005; Pradhan et al, 2005; Toutanova et
al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007;
Surdeanu et al, 2007; Johansson and Nugues,
2008; Che et al, 2008). Most feature templates
that we will adopt for this work will come from
various combinations or integrations of the follow-
ing basic elements.
Word Property. This type of elements include
word form (form and its split form, spForm)
2
,
lemma (lemma,spLemma), and part-of-speech tag
(pos, spPos), syntactic dependency label (dprel),
and semantic dependency label (semdprel)
3
.
Syntactic Connection. This includes syn-
tactic head (h), left(right) farthest(nearest) child
(lm, ln, rm, and rn), and high(low) support
verb or noun. We explain the last item, sup-
port verb(noun). From a given word to the
syntactic root along the syntactic tree, the first
verb/noun/preposition that is met is called as its
low support verb/noun/preposition, and the near-
est one to the root is called as its high support
verb/noun/preposition. The concept of support
verb was broadly used (Toutanova et al, 2005;
Xue, 2006; Jiang and Ng, 2006)
4
, we here extend
it to nouns and prepositions. In addition, we intro-
duce a slightly modified syntactic head, pphead,
it returns the left most sibling of a given word if
the word is headed by a preposition, otherwise it
returns the original head.
Path. There are two basic types of path between
the predicate and the argument candidates. One
is the linear path (linePath) in the sequence, the
other is the path in the syntactic parsing tree (dp-
Path). For the latter, we further divide it into four
sub-types with respect to the syntactic root, dp-
Path is the full path in the syntactic tree. Leading
two paths to the root from the predicate and the
argument, respectively, the common part of these
two paths will be dpPathShare. Assume that dp-
PathShare starts from a node r
?
, then dpPathPred
is from the predicate to r
?
, and dpPathArgu is from
the argument to r
?
.
Family. Two types of children sets for the pred-
icate or argument candidate are considered, the
2
In CoNLL-2008, Treebank tokens are split at the position
that a hyphen (-) or a forward slash (/) occurs. This leads to
two types of feature columns, non-split and split.
3
Lemma and pos for either training or test are from auto-
matically pre-analyzed columns in the input files.
4
Note that the meaning of support verb is slightly different
between (Toutanova et al, 2005) and (Xue, 2006; Jiang and
Ng, 2006)
32
first includes all syntactic children (children), the
second also includes all but excludes the left most
and the right most children (noFarChildren).
Concatenation of Elements. For all collected
elements according to linePath, children and so
on, we use three strategies to concatenate all those
strings to produce the feature value. The first is
seq, which concatenates all collected strings with-
out doing anything. The second is bag, which
removes all duplicated strings and sort the rest.
The third is noDup, which removes all duplicated
neighbored strings.
We address some other elements that are not in-
cluded by the above description as the following.
dpTreeRelation. It returns the relationship of a
and p in the input syntactic tree. The possible val-
ues for this feature include parent, sibling
etc.
isCurPred. It judges if a given word is the cur-
rent predicate. If the word is the predicate, then it
returns the predicate itself, otherwise it returns a
default value.
existCross. It judges if a forthcoming depen-
dency relation that is between a given word pair
may cause any cross with all existing dependency
relations.
distance. It counts the number of words along a
given path, either dpPath or linePath.
existSemdprel. It checks if the given argument
label for other predicates has been assigned to a
given word.
voice. This feature returns Active or Passive for
verbs, and a default value for nouns.
baseline. Two types of semantic role baseline
outputs are used for features from (Carreras and
Marquez, 2005)
5
. baseline Ax tags the head of
the first NP before the predicate as A0 and the
head of the first NP after the predicate as A1.
baseline Mod tags the dependant of the predicate
as AM-MOD as it is a modal verb.
We show some feature template examples de-
rived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
p
?1
.pos+p.pos pos of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas
5
These baseline rules were developed by Erik Tjong Kim
Sang, from the University of Antwerp, Belgium.
along the syntactic tree path from the argument
to the predicate, then removed all duplicated
ones and sort the rest, finally concatenate all as a
feature string.
a:p.highSupportNoun|linePath.dprel.seq Col-
lect all dependant labels along with the line path
from the argument to the high support noun of the
predicate, then concatenate all as a feature string.
3.2 Feature Template Selection
Based on the above mentioned elements, 781 fea-
ture templates (hereafter the set of these templates
is referred to FT )
6
are initially considered. Fea-
ture templates in this initial set are constructed in
a generalized way. For example, if we find that
a feature template a.lm.lemma was once used in
some existing work, then such three templates,
a.rm.lemma, a.rn.lemma, a.ln.lemma will be also
added into the set.
As an optimal feature template subset cannot be
expected to be extracted from so large a set by
hand, a greedy feature selection similar to that in
(Jiang and Ng, 2006; Ding and Chang, 2008) is ap-
plied. The detailed algorithm is described in Algo-
rithm 1. Assuming that the number of feature tem-
plates in a given set is n, the algorithm of (Ding
and Chang, 2008) requires O(n
2
) times of train-
ing/test routines, it cannot handle a set that con-
sists of hundreds of templates. As the time com-
plexity of Algorithm 1 is only O(n), it permits a
large scale feature selection accomplished by pay-
ing a reasonable time cost. Though the time com-
plexity of the algorithm given by (Jiang and Ng,
2006) is also linear, it should assume all feature
templates in the initial selected set ?good? enough
and handles other feature template candidates in a
strict incremental way. However, these two con-
straints are not easily satisfied in our case, while
Algorithm 1 may release these two constraints.
Choosing the first 1/10 templates in FT as
the initial selected set S, the feature selection is
performed for two argument candidate traverse
schemes, synPth and linP th, respectively. 4686
machine learning routines run for the former,
while 6248 routines for the latter. Two feature
template sets, FT
syn
and FT
lin
, are obtained at
last. These two sets are given in Table 1-3. We see
that two sets share 30 identical feature templates
as in Table 1. FT
syn
holds 51 different templates
6
This set with detailed explanation will be available at our
website.
33
p.lm.dprel
p.rm.dprel
p.spForm
p
?1
.spLemma
p.spLemma
p
?1
.spLemma+p.spLemma
p.spLemma + p
1
.spLemma
p.spLemma + p.h.spForm
p.spLemma + p.currentSense
p.lemma
p.lemma + p
1
.lemma
p
?1
.pos+p.pos
a.isCurPred.lemma
a
?2
.isCurPred.lemma + a
?1
.isCurPred.lemma
a.isCurPred.spLemma
a
?1
.isCurPred.spLemma + a.isCurPred.spLemma
a.isCurPred.spLemma + a
1
.isCurPred.spLemma
a.children.dprel.bag
a
?1
.spLemma + a.spLemma
a
?1
.spLemma + a.dprel
a
?1
.spLemma + a.dprel + a.h.spLemma
a.lm
?1
.spLemma
a.rm
?1
.dprel + a.spPos
a
?1
.lemma + a.dprel + a.h.lemma
a.lemma + p.lemma
a.pos + p.pos
a.spLemma + p.spLemma
a:p|dpPath.dprel
a:p|dpPathArgu.dprel
a:p|dpPathPred.spPos
Table 1: Feature templates for both synPth and
linP th
as in Table 2 and FT
lin
holds 57 different tem-
plates as in Table 3. In these tables, the subscripts -
2(or -1) and 1(or 2) stand for the previous and next
words, respectively. For example, a.lm
?1
.lemma
returns the lemma of the previous word of the ar-
gument?s left most child.
4 Decoding
After the predicate sense is disambiguated, an op-
timal argument structure for each predicate is de-
termined by the following maximal probability.
S
p
= argmax
?
i
P (a
i
|a
i?1
, a
i?2
, ...), (1)
where S
p
is the argument structure, P (a
i
|a
i?1
...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. A beam
search algorithm is used to find the optimal argu-
ment structure.
5 Evaluation Results
Our evaluation is performed on the standard
training/development/test corpus of CoNLL-2008
shared task. The data is derived by merging a de-
pendency version of the Penn Treebank with Prop-
Bank and NomBank. More details on the data are
Algorithm 1 Greedy Feature Selection
Input:
The set of all feature templates: FT
The set of selected feature templates: S
0
Output:
The set of selected feature templates: S
Procedure:
Let the counter i = 1
Let S
i
= S
0
and C = FT ? S
i
while do
Train a model with features according to S
i
,
test on development set and the result is p
i
.
Let C
r
= null.
for each feature template f
j
in set S
i
do
Let S
?
= S
i
? f
j
.
Train a model with features according to
S
?
, test on development set and the result
is p
?
.
if p
?
> p
i
then
C
r
= C
r
+ f
j
.
end if
end for
C = C + C
r
S
i
= S
i
? C
r
Let S
?
i
= S
i
Train a model with features according to S
?
i
,
test on development set and the result is q
i
.
Let C
r
= null
for each feature template f
j
in set C do
Let C
?
= S
?
i
+ f
j
.
Train a model with features according to
C
?
, test on development set and the result
is p
?
.
if p
?
> q
i
then
C
r
= C
r
+ f
j
.
end if
end for
C = C ? C
r
S
?
i
= S
?
i
+ C
r
if S
i
= S
i?1
(No feature templates are added
or removed) or, neither p
i
nor q
i
is larger than
p
i?1
and q
i?1
then
Output S = argmax
p
i
,q
i
{S
i
, S
?
i
} and the
algorithm ends.
else
Let i = i+ 1, S
i
=S
i?1
and C = FT ? S
i
end if
end while
34
p?1
.lemma + p.lemma
p
?2
.pos
p.pos
p
?2
.spForm + p
?1
.spForm
p
1
.spForm
p.spForm + p.children.dprel.noDup
p.lm.spPos
p.spForm + p.lm.spPos
+ p.noFarChildren.spPos.bag + p.rm.spPos
p.dprel
p.children.dprel.bag
p.children.pos.seq
p.dprel = OBJ ?
a
a.dprel
a
?1
.lemma + a
1
.lemma
a
1
.lemma
a
?1
.pos
a
1
.spPos
a.h.lemma
a.h.spLemma
a.pphead.lemma
a.pphead.spLemma
a.lm.dprel + a.spPos
a.rm
?1
.pos
a.spLemma + a.h.spPos
a.existSemdprel A1
a.dprel = OBJ ?
a.form + a.children.pos.seq
a.children.adv.bag
b
a:p|linePath.distance
a:p|dpPath.distance
a:p|existCross
a:p|dpPath.dprel.bag
a:p|dpPathPred.dprel.bag
a:p|dpPath.spForm.seq
a:p|dpPathArgu.spForm.seq
a:p|dpPathPred.spForm.bag
a:p|dpPath.spLemma.seq
a:p|dpPathArgu.spLemma.seq
a:p|dpPathArgu.spLemma.bag
a:p|dpPathPred.spLemma.bag
a:p|dpPath.spPos.bag
a:p|dpPathPred.spPos.bag
(a:p|dpPath.dprel.seq) + p.spPos
(a:p|dpTreeRelation) + a.spPos
(a:p|dpTreeRelation) + p.spPos
(a.highSupportVerb:p|dpTreeRelation) + a.spPos
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
a:p|linePath.spForm.bag
a:p|linePath.spLemma.bag
a:p|linePath.spLemma.seq
a
This feature checks if the dependant type is OBJ.
b
adv means all adverbs.
Table 2: Feature templates only for synPth
p.currentSense + a.spLemma
p.currentSense + a.spPos
p.voice + (a:p|direction)
p.rm.dprel
p.children.dprel.noDup
p.rm.form
p.lowSupportNoun.spForm
p.lowSupportProp:p|dpTreeRelation
p
?2
.form + p
?1
.form
p.voice
p.form + p.children.dprel.noDup
p.pos + p.dprel
p.spForm + p.children.dprel.bag
a.voice + (a:p|direction)
a
?1
.isCurPred.lemma
a
1
.isCurPred.lemma
a
?1
.isCurPred.lemma + a.isCurPred.lemma
a.isCurPred.lemma + a
1
.isCurPred.lemma
a
1
.isCurPred.spLemma
a
?2
.isCurPred.spLemma + a
?1
.isCurPred.spLemma
a.baseline Ax + a.voice + (a:p|direction)
a.baseline Mod
a.h.children.dprel.bag
a.lm.dprel + a.dprel
a.lm.dprel + a.pos
a.lm
?1
.lemma
a.lm.lemma
a.lm
1
.lemma
a.lm.pos + a.pos
a.lm.spForm
a.lm
?1
.spPos
a.lm.spPos
a.ln.dprel + a.pos
a.noFarChildren.spPos.bag + a.rm.spPos
a.children.spPos.seq + p.children.spPos.seq
a.rm.dprel + a.pos
a.rm
?1
.spPos
a.rm.spPos
a.rm
1
.spPos
a.rn.dprel + a.spPos
a.form
a.form + a
1
.form
a.form + a.pos
a
?1
.lemma
a
?1
.lemma + a.lemma
a
?2
.pos
a.spForm + a
1
.spForm
a.spForm + a.spPos
a.spLemma + a
1
.spLemma
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
a.spLemma + a.h.spForm
a.spLemma + a.pphead.spForm
a.existSemdprel A2
a:p|dpPathArgu.pos.seq
a:p|dpPathPred.dprel.seq
a:p|dpTreeRelation
Table 3: Feature templates only for linPth
35
in (Surdeanu et al, 2008). Note that CoNLL-2008
shared task is essentially a joint learning task for
both syntactic and semantic dependencies, how-
ever, we will focus on semantic part of this task.
The main semantic measure that we adopt is se-
mantic labeled F
1
score (Sem-F
1
). In addition, the
macro labeled F
1
scores (Macro-F
1
), which was
used for the ranking of the participating systems of
CoNLL-2008, the ratio between labeled F
1
score
for semantic dependencies and the LAS for syn-
tactic dependencies (Sem-F
1
/LAS), are also given
for reference.
5.1 Syntactic Dependency Parsers
We consider three types of syntactic information
to feed the SRL task. One is gold-standard syn-
tactic input, and other two are based on automati-
cally parsing results of two parsers, the state-of-
the-art syntactic parser described in (Johansson
and Nugues, 2008)
7
(it is referred to Johansson)
and an integrated parser described as the follow-
ing (referred to MST
ME
).
The parser is basically based on the MSTParser
8
using all the features presented by (McDonald et
al., 2006) with projective parsing. Moreover, we
exploit three types of additional features to im-
prove the parser. 1) Chen et al (2008) used fea-
tures derived from short dependency pairs based
on large-scale auto-parsed data to enhance depen-
dency parsing. Here, the same features are used,
though all dependency pairs rather than short de-
pendency pairs are extracted along with the de-
pendency direction from training data rather than
auto-parsed data. 2) Koo et al (2008) presented
new features based on word clusters obtained from
large-scale unlabeled data and achieved large im-
provement for English and Czech. Here, the same
features are also used as word clusters are gen-
erated only from the training data. 3) Nivre and
McDonald (2008) presented an integrating method
to provide additional information for graph-based
and transition-based parsers. Here, we represent
features based on dependency relations predicted
by transition-based parsers for the MSTParer. For
the sake of efficiency, we use a fast transition-
7
It is a 2-order maximum spanning tree parser with
pseudo-projective techniques. A syntactic-semantic rerank-
ing was performed to output the final results according to (Jo-
hansson and Nugues, 2008). However, only 1-best outputs of
the parser before reranking are used for our evaluation. Note
that the reranking may slightly improve the syntactic perfor-
mance according to (Johansson and Nugues, 2008).
8
It?s freely available at http://mstparser.sourceforge.net.
Parser Path Adaptive Pruning Coverage
/wo /w Rate
Gold synPth 2.13M 1.05M 98.4%
(49.30%)
linP th 5.29M 1.57M 100.0%
(29.68%)
Johansson synPth 2.15M 1.06M 95.4%
(49.30%)
linP th 5.28M 1.57M 100.0%
(29.73%)
MST
ME
synPth 2.15M 1.06M 95.0%
(49.30%)
linP th 5.29M 1.57M 100.0%
(29.68%)
Table 4: The number of training samples on argu-
ment candidates
synPth+FT
syn
linPth+FT
lin
Syn-Parser LAS Sem Sem-F
1
Sem Sem-F
1
F
1
/LAS F
1
/LAS
MST
ME
88.39 80.53 91.10 79.83 90.31
Johansson 89.28 80.94 90.66 79.84 89.43
Gold 100.00 84.57 84.57 83.34 83.34
Table 5: Semantic Labeled F
1
based parser based on maximum entropy as in
Zhao and Kit (2008). We still use the similar fea-
ture notations of that work.
5.2 The Results
At first, we report the effectiveness of the proposed
adaptive argument pruning. The numbers of argu-
ment candidates are in Table 4. The statistics is
conducted on three different syntactic inputs. The
coverage rate in the table means the ratio of how
many true arguments are covered by the selected
pruning scheme. Note that the adaptive pruning
of argument candidates using assistant labels does
not change this rate. This ratio only depends on
which path, either synPth or linP th, is chosen,
and how good the syntactic input is (if synPth
is the case). From the results, we see that more
than a half of argument candidates can be effec-
tively pruned for synPth and even 2/3 for linP th.
As mentioned by (Pradhan et al, 2004), argument
identification plays a bottleneck role in improving
the performance of a SRL system. The effective-
ness of the proposed additional pruning techniques
may be seen as a significant improvement over the
original algorithm of (Xue and Palmer, 2004). The
results also indicate that such an assumption holds
that arguments trend to close with their predicate,
at either type of distance, syntactic or linear.
Based on different syntactic inputs, we obtain
different results on semantic dependency parsing
36
as shown in Table 5. These results on differ-
ent syntactic inputs also give us a chance to ob-
serve how semantic performance varies according
to syntactic performance. The fact from the re-
sults is that the ratio Sem-F
1
/LAS becomes rela-
tively smaller as the syntactic input becomes bet-
ter. Though not so surprised, the results do show
that the argument traverse scheme synPth always
outperforms the other linP th. The result of this
comparison partially shows that an integrated se-
mantic role labeler is sensitive to the order of how
argument candidates are traversed to some extent.
The performance given by synPth is com-
pared to some other systems that participated in
the CoNLL-2008 shared task. They were cho-
sen among the 20 participating systems either be-
cause they held better results (the first four partic-
ipants) or because they used some joint learning
techniques (Henderson et al, 2008). The results of
(Titov et al, 2009) that use the similar joint learn-
ing technique as (Henderson et al, 2008) are also
included
9
. Results of these evaluations on the test
set are in Table 6. Top three systems of CoNLL-
2008, (Johansson and Nugues, 2008; Ciaramita et
al., 2008; Che et al, 2008), used SRL pipelines.
In this work, we partially use the similar
techniques (synPth) for our participation in the
shared tasks of CoNLL-2008 and 2009 (Zhao and
Kit, 2008; Zhao et al, 2009b; Zhao et al, 2009a).
Here we report that all SRL sub-tasks are tackled
in one integrated model, while the predicate dis-
ambiguation sub-task was performed individually
in both of our previous systems. Therefore, this is
our first attempt at a full integrated SRL system.
(Titov et al, 2009) reported the best result by
using joint learning technique up to now. The
comparison indicates that our integrated system
outputs a result quite close to the state-of-the-art
by the pipeline system of (Johansson and Nugues,
2008) as the same syntactic structure input is
adopted. It is worth noting that our system actu-
ally competes with two independent sub-systems
of (Johansson and Nugues, 2008), one for verbal
predicates, the other for nominal predicates. In ad-
dition, the results of our system is obtained with-
out using additional joint learning technique like
syntactic-semantic reranking. It indicates that our
system is expected to obtain some further perfor-
mance improvement by using such techniques.
9
In addition, the work of (Henderson et al, 2008) and
(Titov et al, 2009) jointly considered syntactic and semantic
dependencies, that is significantly different from the others.
6 Conclusion
We have described a dependency-based semantic
role labeling system for English from NomBank
and PropBank. From the evaluations, the result of
our system is quite close to the state of the art. As
to our knowledge, it is the first integrated SRL sys-
tem that achieves such a competitive performance
against previous pipeline systems.
According to the path that the word-pair classi-
fier traverses argument candidates, two integration
schemes are presented. Argument candidate prun-
ing and feature selection are performed on them,
respectively. These two schemes are more than
providing a trivial comparison. As assistant la-
beled are introduced to help further argument can-
didate pruning, and this techniques work well for
both schemes, it support the assumption that argu-
ments trend to surround their predicate. The pro-
posed feature selection procedure also work for
both schemes and output quite different two fea-
ture template sets, and either of the sets helps the
system obtain a competitive performance, this fact
suggests that the feature selection procedure is ro-
bust and effective, too.
Either of the presented integrated systems can
provide a competitive performance. This conclu-
sion about basic learning scheme for SRL is some
different from previous literatures. However, ac-
cording to our results, there does exist a ?harmony?
feature template set that is helpful to both predi-
cate and argument identification/classification, or
SRL for both verbal and nominal predicates. We
attribute this different conclusion to two main fac-
tors, 1) much more feature templates (for example,
ten times more than those used by Xue et al) than
previous that are considered for a successful fea-
ture engineering, 2) a maximum entropy classifier
makes it possible to accept so many various fea-
tures in one model. Note that maximum entropy is
not so sensitive to those (partially) overlapped fea-
tures, while SVM and other margin-based learners
are not so.
Acknowledgements
Our thanks give to Dr. Richard Johansson, who
kindly provided the syntactic output for his partic-
ipation in the CoNLL-2008 shared task.
37
Systems
a
LAS Sem-F
1
Macro Sem-F
1
pred-F
1
b
argu-F
1
c
Verb-F
1
d
Nomi-F
1
e
F
1
/LAS
Johansson:2008*
f
89.32 81.65 85.49 91.41 87.22 79.04 84.78 77.12
Ours:Johansson 89.28 80.94 85.12 90.66 86.57 78.30 83.66 76.93
Ours:MST
ME
88.39 80.53 84.93 91.10 86.80 77.60 82.77 77.23
Johansson:2008 89.32 80.37 84.86 89.98 85.40 78.02 84.45 74.32
Ciaramita:2008* 87.37 78.00 82.69 89.28 83.46 75.35 80.93 73.80
Che:2008 86.75 78.52 82.66 90.51 85.31 75.27 80.46 75.18
Zhao:2008* 87.68 76.75 82.24 87.53 78.52 75.93 78.81 73.59
Ciaramita:2008 86.60 77.50 82.06 89.49 83.46 74.56 80.15 73.17
Titov:2009 87.50 76.10 81.80 86.97 ? ? ? ?
Zhao:2008 86.66 76.16 81.44 87.88 78.26 75.18 77.67 73.28
Henderson:2008* 87.64 73.09 80.48 83.40 81.42 69.10 75.84 68.90
Henderson:2008 86.91 70.97 79.11 81.66 79.60 66.83 73.80 66.26
Ours:Gold 100.0 84.57 92.20 84.57 87.67 83.15 88.71 78.39
a
Ranking according to Sem-F
1
b
Labeled F
1
for predicate identification and classification
c
Labeled F
1
for argument identification and classification
d
Labeled F
1
for verbal predicates
e
Labeled F
1
for nominal predicates
f
* means post-evaluation results, which are available at the official website of CoNLL-2008 shared task,
http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.
Table 6: Comparison of the best existing systems
References
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling. In Proceedings of CoNLL-2005, pages 152?
164, Ann Arbor, Michigan, USA.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang
Li, Bing Qin, Ting Liu, and Sheng Li. 2008. A
cascaded syntactic and semantic dependency pars-
ing system. In Proceedings of CoNLL-2008, pages
238?242, Manchester, England, August.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Massimiliano Ciaramita, Giuseppe Attardi, Felice
Dell?Orletta, and Mihai Surdeanu. 2008. Desrl: A
linear-time semantic role labeling system. In Pro-
ceedings of CoNLL-2008, pages 258?262, Manch-
ester, England, August.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL-2005, pages 42?49, Ann Arbor,
USA.
Weiwei Ding and Baobao Chang. 2008. Improving
chinese semantic role classification with hierarchi-
cal feature selection strategy. In Proceedings of
EMNLP-2008, pages 324?323, Honolulu, USA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jan Haji?c, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, pages 1?
18, Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic depen-
dencies. In Proceedings of CoNLL-2008, pages
178?182, Manchester, England, August.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Seman-
tic role labeling of nombank: A maximum entropy
approach. In Proceedings of EMNLP-2006, pages
138?145, Sydney, Australia.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic?semantic analysis
with propbank and nombank. In Proceedings of
CoNLL-2008, page 183?187, Manchester, UK.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen
tau Yih. 2005. Generalized inference with multi-
ple semantic role labeling systems. In Proceedings
of CoNLL-2005, pages 181?184, Ann Arbor, Michi-
gan, USA.
Chang Liu and Hwee Tou Ng. 2007. Learning pre-
dictive structures for semantic role labeling of nom-
bank. In Proceedings of ACL-2007, pages 208?215,
Prague, Czech.
38
Lluis Marquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A robust combination strat-
egy for semantic role labeling. In Proceedings
of HLT/EMNLP-2005, page 644?651, Vancouver,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a
two-stage discriminative parser. In Proceedings of
CoNLL-X, New York City, June.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24?31, Boston, Massachusetts, USA, May 6.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proceedings of ACL-08: HLT, pages
950?958, Columbus, Ohio, June.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of HLT/NAACL-2004, pages 233?240,
Boston, Massachusetts, USA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role labeling using different syntactic views.
In Proceedings of ACL-2005, pages 581?588, Ann
Arbor, USA.
James Pustejovsky, Adam Meyers, Martha Palmer, and
Massimo Poesio. 2005. Merging propbank, nom-
bank, timebank, penn discourse treebank and coref-
erence. In Proceedings of the Workshop on Frontiers
in Corpus Annotations II: Pie in the Sky, pages 5?12,
Ann Arbor, USA.
Mihai Surdeanu, Lluis Marquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29:105?151.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
CoNLL-2008, pages 159?177, Manchester, UK.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisation
for synchronous parsing of semantic and syntactic
dependencies. In IJCAI-2009, Pasadena, California,
USA.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of ACL-2005, pages
589?596, Ann Arbor, USA.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of EMNLP-2004, pages 88?94, Barcelona, Spain,
July 25-26.
Nianwen Xue. 2006. Semantic role labeling of nom-
inalized predicates in chinese. In Proceedings of
NAACL-2006, pages 431?438, New York City, USA,
June.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203?207, Manchester, UK.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009a. Multilin-
gual dependency learning: Exploiting rich features
for tagging syntactic and semantic dependencies. In
Proceedings of the 13th Conference on Computa-
tional Natural Language Learning (CoNLL-2009),
June 4-5, pages 61?66, Boulder, Colorado, USA.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009b. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
pages 55?60, Boulder, Colorado, USA.
39
An Example-Based Chinese Word Segmentation System for CWSB-2
Chunyu Kit Xiaoyue Liu
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0}@cityu.edu.hk
Abstract
This paper reports the example-based
segmentation system for our participa-
tion in the second Chinese Word Seg-
mentation Bakeoff (CWSB-2), present-
ing its basic ideas, technical details and
evaluation. It is a preliminary imple-
mentation. CWSB-2 valuation shows
that it performs very well in identify-
ing known words. Its unknown word
detection module also illustrates great
potential. However, proper facilities for
identifying time expressions, numbers
and other types of unknown words are
needed for improvement.
1 Introduction
Word segmentation is to identify lexical items,
especially individual word forms, in a text. It
involves two fundamental tasks, both aiming at
minimizing segmentation errors: one is to in-
fer out-of-vocabulary (OOV) words, also known
as unknown (or unseen) word detection, and the
other to identify in-vocabulary (IV) words, with
an emphasis on disambiguation. OOV words and
ambiguities are the two major causes to segmen-
tation errors.
Accordingly?word segmentation approaches
can be divided into the categories summarized in
Table 1 in terms of the resources in use to tackle
these two causes. The closed and open tracks in
CWSB correspond, respectively, to the last two
categories, both involving inferring OOV words
Category Resource in use Major Task
Lexicon Tr. Corpus OOV Disamb.
WDa - (-)b +
WS/CLc + - - +
WS/ILd + - + +
WS/TCe (+)f + + +
WS/TC+Lg + + + +
aWord discovery, or unsupervised lexical acquisition
bInput data is used for unsupervised training
cWord segmentation with a complete lexicon
dWord segmentation with an incomplete lexicon
eWord segmentation with a pre-segmented training corpus
fIt can be extracted from the given training corpus.
gWord segmentation with a pre-segmented training corpus
and an extra lexicon
Table 1: Categories of segmentation approach
beyond disambiguating IV words. Word discov-
ery and OOV word detection pursue a similar tar-
get, i.e., inferring new words. The continuum
connecting them is the size of the lexicon in use:
the former assumes few words known and the lat-
ter an existing lexicon to some scale. Inferring
new words is an essential task in word segmen-
tation, for a complete lexicon is rarely a realistic
assumption in practice.
This paper presents our segmentation system
for participation in CWSB-2. It takes an example-
based approach to recognize IV words and fol-
lows description length gain (DLG) to infer OOV
words in terms of their text compression effect.
Sections 2 and 3 below introduce the example-
based and DLG-based segmentation respectively.
Section 4 presents a strategy to combine their
strength and Section 5 reports our system?s per-
formance in CWSB-2. Following error analysis
in Section 6, Section 7 concludes the paper.
146
2 Example-based segmentation
How to utilize as much information as possi-
ble from the training corpus to adapt a segmen-
tation system towards a segmentation standard
has been a critical issue. Kit et al (2002) and
Kit et al (2003) attempt to integrate case-based
learning with statistical models (e.g., n-gram) by
extracting transformation rules from the train-
ing corpus for disambiguation via error correc-
tion; Gao et al (2004) adopt a similar strategy
for adaptive segmentation, with transformation
templates (instead of case-based rules) to modify
word boundaries (instead of individual words).
The basic idea of example-based segmentation
is very simple: existing pre-segmented strings in
training corpus provide reliable examples for seg-
menting similar strings in input texts. In contrast
to dictionary checking for locating possible words
in an input sentence to facilitate later segmenta-
tion operations, pre-segmented examples give ex-
act segmentation to copy.
The example-based segmentation can be im-
plemented in the following steps.
1. Find all exemplar pre-segmented fragments,
with regards to a training corpus, and all
possible words, with regards to a lexicon,
from each character in an input sentence;
2. Identify the optimal sequence, among all
possibilities, of the above items over the sen-
tence following some optimization criterion.
If adopting the minimal number of fragments or
words in a sequence as optimization criterion, we
have a maximal matching approach to word seg-
mentation. However, it differs remarkably from
the previous maximal matching approaches: it
matches pre-segmented fragments, instead of dic-
tionary words, against an input sentence. It can be
carried out by a best-first strategy: repeatedly se-
lect the next longest example or word until the en-
tire sentence is properly covered. Unfortunately,
the best-first approach does not guarantee to give
the best answer. For CWSB-2, we implemented
a program following the Viterbi algorithm to per-
form a complete search in terms of the number of
fragments, and then words, in a sequence.
However, a serious problem with this example-
based approach is the sparse data problem. Long
exemplar fragments are more reliable but small
in number, whereas short ones are large in num-
ber but less reliable. In the case of no exemplar
fragment available for an input sentence, this ap-
proach draws back to the maximal match segmen-
tation with a dictionary. How to incorporate sta-
tistical inference into example-based segmenta-
tion to infer more reliable optimal segmentation
beyond string matching remains a critical issue
for us to tackle.
3 DLG-based segmentation
DLG is formulated in Kit and Wilks (1999) and
Kit (2000) as an empirical measure for the com-
pression effect of extracting a substring from a
given corpus as a lexical item. DLG optimization
is applied to detect OOV words for our participa-
tion in CWSB-2. It works as follows in two steps.
1. Calculate the DLG for all known words
and all new word candidate (i.e., substrings
with frequency ? 2, preferably, in the test
corpus), based on frequency information in
the training and the test corpora;
2. Find the optimal sequence of such items over
an input sentence with the greatest sum of
DLG.
Step 2 above in our system re-implements only
the first round of DLG-based lexical learning in
Kit (2000). It is implemented by the same algo-
rithm as the one for example-based segmentation,
with DLG as optimization criterion. Evaluation
results show that this learning-via-compression
approach discovers many OOV words success-
fully, in particular, person names.
4 Integration
The example-based segmentation is good at iden-
tifying IV words but incapable of recognizing any
new words. In contrast, the DLG-based segmen-
tation performs slightly worse but has potential to
detect new words. It is expected that the strength
of the two could be combined together for perfor-
mance enhancement.
However, because of inadequate time we had
to take a shortcut in order to catch the CWSB-
2 deadline: DLG segmentation is only applied
to recognize new words among the sequences of
mono-character items in the example-based seg-
mentation output.
147
Track P R F OOV ROOV RIV
ASc .944 .902 .923 .043 .234 .976
PKUc .929 .904 .916 .058 .252 .971
MSRc .965 .935 .950 .026 .189 .986
Table 2: System performance in CWSB-2
5 Performance
Our group took part in three closed tracks in
CWSB-2, namely, ASc, PKUc and MSRc, with a
preliminary implementation of the example-based
word segmentation presented above. Our sys-
tem?s performance in terms of CWSB-2?s offi-
cial scores is presented in Table 2. Its ROOV
scores look undesirable, showing that applying
the first round of DLG-based segmentation to se-
quences of mono-character items is inadequate
for the OOV word discovery task. Nevertheless,
its RIV scores are, in general, quite close to the
top systems in CWSB-2, although it does not have
a disambiguation module to polish its maximal
matching output.
However, this is not to say that the DLG-based
segmentation deserves no credit in unknown word
detection. It does recognize many OOV words,
as shown in Table 3. The low ROOV rate has to
do with our system?s incapability in handling time
expressions, numbers, and foreign words.
6 Error analysis
Most errors made by our system are due to
the following causes: (1) no knowledge, overt
or implicit, in use for recognizing time expres-
sions, numbers and foreign words, as restricted by
CWSB-2 rules, (2) a premature module for OOV
word detection, (3) no further disambiguation be-
sides example application, and (4) significant in-
consistency in the training and test data.
The inconsistency exists not only between the
training and test corpora for each track but, more
surprisingly, also within individual training cor-
pora. Some suspected cases are illustrated in Ta-
bles 4, 5 and 6. They are observed to be in a large
number in the CWSB-2 corpora. Scoring with a
golden standard involving so many of them ap-
pears to be problematic, for it penalizes the sys-
tems for handling such cases right and rewards
the others for producing ?correct? answers. What
ASc: ??(106) ???(45) ??(31) 29)??) ??(21) ??(20) ?
?(18) ???(17) ???(16) ???(15) ???(13) ??(12) ?
??(11) ??(11) ????(11) ??10)?) ??(9) ???(8) ?
?(8) ???(7) ??(7) ???(7) ???(6) ????(6) ?
?(5) ???(5) ???(5) ???(5) ???(5) ???(5) ??(5) ?
?5)?) ???(4) ??(4) ??(4) ????(4) ???(3) ?
?(3) ????(3) ???(2) ??(2) ????2)?) ??(2) ? ? ? ? ? ?
PKUc: ??(38) ??(23) ??(21) ???20)???) ?
?(19) 17)???) ?16)?) ??(15) ???(12) ??(11) ??
?(10) ??(10) ??9)?) ??(9) ?8)??) ??(8) ???(8) ?
?(7) ???(7) ??(6) ???(6) ??6)?) ??(6) ?6)?) ??
?(5) ????(5) 5)??) ???(5) ???(5) ??(5) ??(5) ?
?(5) ???(4) ???(4) ???(4) ?4)?) ???(4) ??(4) ?
?(4) ??(4) ?4)?) ???(4) ???(4) ??(4) ??3)?) ?
??(3) ?????(3) ??(3) ???(3) ??(3) ??(3) ?
?(3) ???(3) ???(3) ? ? ? ? ? ?
MSRc: ?26)?) ??(19) ???(19) ???(17) ?15)?) ?
14)?) ??(14) ???(13) ??(13) ????????(12) ?
?(12) ??(11) ???(10) ??(10) ?10)???) ???(10) ?
?(10) ???(9) ??9)?) ???(8) ?8)?) ??(8) ???(7) ?
???(7) ???(7) ??(6) ???(6) ??(6) ?6)?) ??(6) ?
?(5) ???(5) ??(5) ??(5) ???(5) ???(5) ???(4) ?
?(4) ?4)?) ??(4) ???(4) ???(4) ???(3) ???(3) ?
?(3) ???(3) ???(3) ???(3) ??3)?) ??(3) ???(3) ?
??(3) ???(3) ???(3) ??(3) ???(3) ????(2) ? ? ? ? ? ?
Table 3: Illustration of new words successfully
detected, with frequency in parentheses
is even more worth noting is that (1) an inconsis-
tent case involves more than one word, and (2)
the difference between a correct and an erroneous
judgment of a word is 1, in a sense, but the differ-
ence between one system that loses it for doing
right and another that earns it by doing wrong is
surely greater.
7 Conclusions
In the above sections we have reported the
example-based word segmentation system for
our participation in CWSB-2, including its ba-
sic ideas, technical details and evaluation results.
It has illustrated an excellent performance in IV
word identification and nice potential in OOV
word discovery. However, its weakness in han-
dling time expressions, numbers and other types
of unknown words has hindered it from perform-
ing better. We are expecting to implement a full-
fledged version of the system for improvement.
Acknowledgements
The work described in this paper was supported
by the RGC of HKSAR, China, through the
CERG grant 9040861. We wish to thank Alex
Fang and Robert Neather for their help.
148
Training & Answer fT/fA Golden Standard fT/fA
?? ? 4/8 ??? 0/0
? ? 28/7 ?? 0/0
?? ? 5/7 ??? 0/0
? ?? 11/6 ??? 0/0
? ? 186/5 ?? 0/0
?? ? 41/4 ??? 0/0
? ? ? 29/4 ??? 0/0
? ? ? 129/4 ??? 0/0
? ? ? 23/3 ??? 0/0
? ?? 47/3 ??? 0/0
?? ?? 33/2 ???? 0/0
?? ?? 32/2 ???? 0/0
?? ?? 85/2 ???? 0/0
???? 10/2 ?? ?? 0/0
?? ? 62/2 ??? 0/0
? ?? 23/2 ??? 0/0
???? 192/1 ? ?? ? 0/0
??? 149/1 ? ?? 0/0
?? ?? 66/1 ???? 0/0
??? 31/1 ?? ? 0/0
? ?? 80/1 ??? 0/0
?? ? 68/1 ??? 0/0
? ? ?? 13/1 ???? 0/0
?? ?? 13/1 ???? 0/0
? ?? 20/1 ??? 0/0
? ? ? ? 6/1 ???? 0/0
? ? ? 29/1 ??? 0/0
?? ? ? ? 4/1 ????? 0/0
???? 24/7 ?? ?? 25/0
???? 17/3 ?? ?? 53/0
? ? ? 1201/2 ??? 2/0
Table 4: Some inconsistent cases in AS corpus
Training & Answer fT/fA Golden Standard fT/fA
??? 14/26 ? ?? 0/0
??? 6/1 ? ?? 0/0
??? 5/21 ? ?? 0/0
???? 24/19 ?? ? ? 0/0
? ? 23/18 ?? 0/0
???? 66/15 ?? ?? 0/0
????? 10/9 ?? ??? 0/0
????? 10/5 ? ?? ?? 0/0
???? 45/5 ?? ?? 0/0
???? 42/5 ?? ?? 0/0
???? 27/4 ?? ?? 0/0
???? 21/4 ?? ?? 0/0
???? 126/4 ?? ?? 0/0
???? 20/4 ?? ?? 0/0
???? 15/4 ?? ?? 0/0
???? 25/4 ? ??? 0/0
???? 25/3 ?? ?? 0/0
????? 13/3 ?? ??? 0/0
??? 32/3 ?? ? 0/0
???? 30/3 ?? ?? 0/0
??? 11/3 ? ?? 0/0
???? 15/3 ?? ?? 0/0
???? 22/3 ?? ?? 0/0
?? ? 11/2 ? ?? 0/0
?? 25/2 ? ? 0/0
??????? 3/1 ???? ? ? ? 0/0
??? 13/1 ? ? ? 0/0
??? 24/5 ? ?? 1/0
??? 49/4 ?? ? 1/0
?? 112/3 ? ? 14/0
???? 48/1 ?? ?? 1/0
Table 5: Some inconsistent cases in PKU corpus
Training & Answer fT/fA Golden Standard fT/fA
?? ? 12/7 ??? 0/0
??? 16/6 ? ?? 0/0
o ?? 29/5 o?? 0/0
?? ???? 6/3 ???? ?? 0/0
?? ? 3/3 ? ?? 0/0
???? 1/2 ?? ?? 0/0
????? 4/2 ?? ?? 0/0
? ? 10/2 ?? 0/0
?? 3/2 ? ? 0/0
???? ? ? 7/1 ?????? 0/0
??? 2/1 ?? ? 0/0
??? ? 1/1 ? ? ?? 0/0
??a?a?? 4/1 ?? a ? a ? 0/0
????? 1/1 ??? ?? 0/0
????? ? 1/1 ?? ???? 0/0
??? ???? 1/1 ????? ?? 0/0
? ??? 10/1 ?? ? 0/0
??? 16/1 ? ? 0/0
??? 4/1 ? ? 0/0
???? 16/1 ? ? ? 0/0
???? 122/1 ?? ?? 0/0
???? ?? 3/1 ?? ???? 0/0
Table 6: Some inconsistent cases in MSR corpus
References
E. Brill. 1993. A Corpus-Based Approach to Lan-
guage Learning. PhD thesis, University of Pennsyl-
vania, Philadelphia.
J. Gao, A. Wu, M. Li, C. Huang, H. Li, X. Xia and H.
Qin. 2004. Adaptive Chinese word segmentation.
In ACL-04. Barcelona, July 21-26.
C. Kit and Y. Wilks. 1999. Unsupervised learning of
word boundary with description length gain. In M.
Osborne and E. T. K. Sang (eds.), CoNLL-99, pp.1-
6. Bergen, Norway, June 12.
C. Kit 2000. Unsupervised Lexical Learning as
Inductive Inference. PhD thesis, University of
Sheffield.
C. Kit, H. Pan and H. Chen. 2002. Learning case-
based knowledge for disambiguating Chinese word
segmentation: A preliminary study. SIGHAN-1,
pp.33?39. Taipei, Sept. 1, 2002.
C. Kit, Z. Xu and J. J. Webster. 2003. Integrating
n-gram model and case-based learning for Chinese
word segmentation. In Q. Ma and F. Xia (eds.),
SIGHAN-2, pp.160-163. Sapporo, 11 July, 2003.
D. Palmer. A trainable rule-based algorithm for word
segmentation. In ACL-97, pp.321-328. Madrid.
149
Harvesting the Bitexts of the Laws of Hong Kong From the Web
Chunyu Kit Xiaoyue Liu KingKui Sin Jonathan J. Webster
Department of Chinese, Translation and Linguistics
City University of Hong Kong, Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0, ctsinkk, ctjjw}@cityu.edu.hk
Abstract
In this paper we present our recent work
on harvesting English-Chinese bitexts
of the laws of Hong Kong from the
Web and aligning them to the subpara-
graph level via utilizing the number-
ing system in the legal text hierarchy.
Basic methodology and practical tech-
niques are reported in detail. The re-
sultant bilingual corpus, 10.4M English
words and 18.3M Chinese characters,
is an authoritative and comprehensive
text collection covering the specific and
special domain of HK laws. It is par-
ticularly valuable to empirical MT re-
search. This piece of work has also laid
a foundation for exploring and harvest-
ing English-Chinese bitexts in a larger
volume from the Web.
1 Introduction
Bitexts, also referred to as parallel texts or bilin-
gual corpora, collections of bilingual text pairs
aligned at various levels of granularity, have been
playing a critical role in the current development
of machine translation technology. It is such
large data sets that give rise to the plausibility
of empirical approaches to machine translation,
most of which involve the application of a variety
of machine learning techniques to infer various
types of translation knowledge from bitext data
to facilitate automatic translation and enhance
translation quality. Large volumes of training
data of this kind are indispensable for construct-
ing statistical translation models (Brown et al,
1993; Melamed, 2000), acquiring bilingual lex-
icon (Gale and Church, 1991; Melamed, 1997),
and building example-based machine translation
(EBMT) systems (Nagao, 1984; Carl and Way,
2003; Way and Gough, 2003). They also provide
a basis for inferring lexical connection between
vocabularies in cross-languages information re-
trieval (Davis and Dunning, 1995).
Existing parallel corpora have illustrated their
particular value in empirical NLP research, e.g.,
Canadian Hansard Corpus (Gale and Church,
1991b), HK Hansard (Wu, 1994), INTERSECT
(Salkie, 1995), ENPC (Ebeling, 1998), the Bible
parallel corpus (Resnik et al, 1999) and many
others. The Web is being explored not only as a
super corpus for NLP and linguistic research (Kil-
garriff and Grefenstette, 2003) but also, more im-
portantly to MT research, as a treasure for mining
bitexts of various language pairs (Resnik, 1999;
Chen and Nie, 2000; Nie and Cai, 2001; Nie
and Chen, 2002; Resnik and Smith, 2003; Way
and Gough, 2003). The Web has been the play-
ground for many NLPers. More and more Web
sites are found to have cloned their Web pages in
several languages, aiming at conveying informa-
tion to audience in different languages. This gives
rise to a huge volume of wonderful bilingual or
multi-lingual resources freely available from the
Web for research. What we need to do is to har-
vest the right resources for the right applications.
In this paper we present our recent work on
harvesting English-Chinese parallel texts of the
laws of Hong Kong from the Web and construct-
71
ing a subparagraph-aligned bilingual corpus of
about 20 million words. The bilingual texts of the
laws is introduced in Section 2, with an emphasis
on HK?s legislation text hierarchy and its num-
bering system that can be utilized for text align-
ment to subparagraph level. Section 3 presents
basic methodology and technical details for har-
vesting and aligning bilingual Web page pairs, ex-
tracting content texts from the pages, and align-
ing text structures in terms of the text hierarchy
via utilizing consistent intrinsic features in the
Web pages and content texts. Section 4 presents
XML schema for encoding the alignment results
and illustrates the display mode for browsing the
aligned bilingual corpus. Section 5 concludes
the paper, highlighting the value of the corpus in
term of its volume, translation quality, specificity
and comprehensiveness, and alignment granular-
ity. Our future work to explore the Web for har-
vesting more quantities of parallel bitexts is also
briefly outlined.
2 Bilingual Texts of the Laws of HK
The laws of Hong Kong (HK) before 1987 were
exclusively enacted in English. They were trans-
lated into Chinese in the run-up to the handover
in 1997. Since then all HK laws have been en-
acted in both English and Chinese, both versions
being equally authentic. This gives rise to a valu-
able set of bitexts in large quantity and high qual-
ity that can be utilized to facilitate empirical MT
research.
2.1 BLIS Corpus
The bilingual texts of the laws of Hong Kong
have been made available to the public in re-
cent years by the Justice Department of the HK-
SAR through the bilingual laws information sys-
tem (BLIS). All these texts are freely accessible
from http://www.justice.gov.hk/.
BLIS provides the most comprehensive docu-
mentation of HK legislation. It contains all statute
laws of Hong Kong currently in operation, includ-
ing all ordinances and subsidiary legislation of
HK (and some of their past versions dating back
to 60 June 1997), the Basic Law and the Sino-
British Joint Declaration, the constitution of PRC
and national laws that apply in HK, and other rel-
evant instruments. The entire bilingual corpus of
Figure 1: Illustration of BLIS hierarchy
BLIS legal texts contains approximately 10 mil-
lion English words and 18 million Chinese char-
acters. Lexical resources of this kind are particu-
larly useful in bilingual legal terminology studies
and text alignment work.
2.2 Text Hierarchy
BLIS organizes the legal texts in terms of the
hierarchy of the Loose-Leaf Edition of the Laws
of Hong Kong. At the top level, the ordinances
are arranged by chapters, each of which is identi-
fied by an assigned number and a short title, e.g.,
Chapter 5 OFFICIAL LANGUAGES ORDINANCE /
?5? ??????. The assigned number for a
subsidiary legislation chapter consists of a chap-
ter number and a following uppercase letter, e.g.,
CAP 5C HIGH COURT CIVIL PROCEDURE (USE
OF LANGUAGE) RULES / ?5C? ???????
?(????)??.
The content of an ordinance, exclusive of its
long title, is divided and identified according to a
very rigid numbering system which encodes the
hierarchy of the texts of the laws. Both the Chi-
nese and English versions of an ordinance fol-
low exactly the same hierarchical structures such
as chapters (?), parts (?), sections (?), sub-
sections (?), paragraphs (?) and subparagraphs
(?). This allows us to align the bitexts along
72
Figure 2: BLIS texts in pair
this hierarchical structure, once they are down-
loaded from the BLIS official site. To our knowl-
edge, a well-aligned bilingual corpus of this size
covering a special domain so comprehensively is
seldom readily available for the Chinese-English
language pair.
Excerpts from the BLIS corpus are illustrated
in Figure 1 and 2, one illustrating its hierarchy and
the other a pair of BLIS bitexts. From the excerpts
we can see that not everything has an exact match
between a pair of BLIS Web pages. For example,
the Chinese side has a gazette number ?25 of 1998
s. 2? and a piece of ?remarks? at the beginning of
content text, whereas its English counterpart has
none of them.
3 Harvesting Bitexts from the Web
Basically two phases are involved in construct-
ing the bilingual corpus of the laws of HK. The
first phase is to harvest the monolingual texts of
HK laws from the BLIS site and align them into
pairs. It involves the following steps: (1) down-
loading Web pages one by one with the aid of a
Web crawler, (2) extracting the texts from them
by filtering out the HTML markup, and (3) align-
ing the extracted monolingual texts into bilingual
Figure 3: BLIS web pages connected as two dou-
ble linked lists
pairs. The second phase is to align finer-grained
text structures within each text pair.
3.1 Downloading BLIS Web Pages
A BLIS Web page does not necessarily corre-
spond to any particular text structure such as a
chapter, a part, a section, a subsection, or a para-
graph in the BLIS hierarchy. A chapter, espe-
cially a short one, may be organized into a few
sections in a Web page or in several contiguous
pages. Some sections, e.g., the long ones, are di-
vided into several pages. In general, BLIS does
not maintain any reliable match between its Web
pages and any particular text hierarchical struc-
tures.
Fortunately, in most cases a BLIS page always
has a counterpart in the other language. There is
a ?switch language? button on each page to link
to the counterpart page. Such linkage allows us
to download the Web pages in pairs and, conse-
quently, harvest a list of page-to-page aligned bi-
texts.
In addition to the pair link, each BLIS page also
carries links for the ?next? and the ?previous sec-
tion of enactment?. These two kinds of linkage
turn the pages into two double linked lists, each
in a language, as illustrated in Figure 3, with each
page as a node. Nodes in pairs are also double
linked between the two lists.
However, the pairwise linkage is not reliable
in the BLIS site, because there are missing Web
pages in one of the two languages in question
(see Table 3 below for more details). In order to
download all bitexts of legislation from the site,
we need to go through one linked list and down-
load each page and its counterpart, if there is one,
in the other language. Such scanning gives a list
of text pairs, where some pages may have a null
73
Total time Downloaded files
English 17 hours 50,638 (429MB)
Chinese 18 hours 50,510 (460MB)
Table 1: File downloading
File name
BLIS HTML page title Chinese English
Cap 5A ... 5A c.txt 5A e.txt
Cap 5A s 1 ... 5A-1 c.txt 5A-1 e.txt
Cap 5A s 2 ... 5A-2 c.txt 5A-2 e.txt
Cap 5A s 3 ... 5A-3 c.txt 5A-3 e.txt
Table 2: Naming downloaded files in terms of
BLIS numbering
counterpart. An alternative strategy is to down-
load each list separately, and then match the pages
into pairs sequentially with the aid of numbering
information in the header of each page ? see 3.2
below. These two strategies verify one another,
making sure that all pages are downloaded and
put in the right pairs.
The downloading is carried out by a Web
crawler implemented in Java. In order to accom-
plish the above strategies, it also has to handle a
number of technical issues.
? It sleeps for a while (e.g., 10 seconds) when
it finishes downloading a certain number of
pages (e.g., 50 pages), because the BLIS site
refuses continuous access from one site for a
too long time.
? When an error occurs, it remembers the cur-
rent URL. Then it re-starts from where it
stops.
The data about the file downloading from BLIS
site is given in Table 1. One can conceive that
if the time intervals for sleep and downloading
could be automatically tuned by the crawler to
maximize the downloading efficiency, it would
get the job done significantly more quickly. Our
option for 10 seconds sleep between every 50 files
is based on error records of a number of test runs.
3.2 Aligning Web Pages
Every BLIS Web page is identified by a subti-
tle that carries numbering information about the
page, as illustrated in Figure 1. Such a subtitle
is exactly retained in the page as its HTML title.
Files English Chinese
Aligned 50,506 (62.3MB)a 50,506 (38.5MB)
Missing 132 4
Total 50,638 50,510
Sizeb 10.4M words 18.3M char.s
aThe size of extracted texts.
bExclusive of punctuation marks.
Table 3: The number of aligned and missing files
This feature is utilized to align BLIS pages: all
downloaded files are named in terms of the num-
bering information extracted from their HTML ti-
tles, as illustrated in Table 2. Consequently, all
files are naturally aligned in pairs by their names.
Any file names not in a pair indicate the missing
counterparts in the other language. The statistics
of file alignment are given in Table 3.
3.3 Text Extraction
Basically, this task involves two aspects, namely,
filtering HTML markup and extracting content
text. A straightforward strategy is that we first
clean up HTML tags in each page and then the
non-legal content. The tags are in brackets, and
non-legal content in a consistent pattern through-
out all BLIS pages. However, a more convenient
way to do it is to make use of a reliable feature
in the BLIS pages: legal content is placed in be-
tween two ? the only two ? horizontal bars in each
page. Accordingly, we implement a strategy to
first extract every thing in between the two bars
and then clean up remaining HTML tags. The
output from this procedure includes
? a header as a fixed set of items, including
chapter number, title, heading, etc., and
? a piece of content text as a list of numbered
items each in a line. (See the header and con-
tent text in Figure 2.)
The text in a BLIS page is displayed as a sequence
of hierarchically numbered items, such as subsec-
tions, paragraphs and subparagraphs.
3.4 Text Alignment within Text Pairs
After page (or file) alignment, each page finds its
counterpart in the other language. After text ex-
traction, a page gives a content text consisting of
a list of numbered items, each in a line. A such
74
Remarks:
Adaptation amendments retroactively made - see
26 of 1999 s.3//a
(1) All Ordinances shall be enacted and published
in both official languages.//
(2) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in
both official languages where that Ordinance
amends another Ordinance and-//
(a) that other Ordinance was enacted in the
English language only; and//
(b) no authentic text of that Ordinance has been
published in the Chinese language under
section 4B(1).//
(3) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in both
official languages where the Chief Executive
in Council- (Amended 26 of 1999 s.3)//
aIndicating a text line break.
Table 4: Anchors in a sample text
item can be divided into a numbering item and the
remaining content text in the line, as illustrated in
Table 4. The Chinese counterpart of this text car-
ries similar lines, if no missing line in any page of
the pair.
Unfortunately, missing lines are found in some
BLIS pages, as exemplified in Figure 2. There is
no guarantee that matching text lines one by one
in sequence would carry out the expected align-
ment within a page pair. However, the numbering
items at the beginning of each line can be utilized
as anchors to facilitate the alignment. The strat-
egy along this line is given as follows.
1. Anchor identification: numbering items at
the beginning of each line are recognized
as anchors, with the beginning and the end
of the whole content text as two special an-
chors, resulting in a list of anchors for each
page;
2. Anchor alignment: match the two lists of an-
chors sequentially. If a pair of anchors does
not match, give up the smaller one (in terms
of the BLIS numbering hierarchy) and move
on to the next possible pair, working in ex-
actly the same procedure as matching iden-
tical anchor pairs between two sorted lists of
anchors.
3. Text line alignment: a pair of matched an-
chors give a pair of matched lines; an un-
matched anchor indicates a missing line in
the other language.
4 XML Markup for the Aligned Corpus
XML is applied to encode the text alignment
outcomes output from the above alignment pro-
cedure. It has been a standard for data repre-
sentation and exchange on the Web, and also
accepted by the NLP community as a standard
for linguistic data annotation and representation
(Ide et al, 2000; Mengel and Lezius, 2000;
Kim et al, 2001). There are a series of yearly
NLPXML workshops for it since 2001. It pro-
vides a platform-independent flexible and sophis-
ticated plain text format for data encoding and
manipulation. It is particularly suitable for hier-
archical linguistic data such as the hierarchically-
aligned bilingual corpus that we have produced.
What?s more, converting data to XML format not
only significantly reduces the complexity of data
exchange among different computer systems but
also enhances data transmission reliability and
eases Web browsing.
There have been many corpora that are anno-
tated with XML, e.g., HCRC Map Task Corpus
(Anderson et al, 1991), American National Cor-
pus (Ide and Macleod, 2001), the La Republica
corpus (Baroni et al, 2004). Below we present
the XML schema for our subparagraph-aligned
BLIS bitexts, with sample annotation, and nec-
essary Web browsing.
4.1 XML Schema
The current version of the XML schema for the
bilingual BLIS corpus, as given in Figure 4, fo-
cuses on encoding all text structures in the BLIS
hierarchy, including all elements in each BLIS
Web page. It is to be extended to cover finer-
grained structures such as clauses, phrases and
words, as we proceed to align the BLIS bitexts
at these linguistic levels. For simplicity, we al-
low para to subsume all types of text line, be
they a section, subsection, paragraph or subpara-
graph. The annotation of a sample bitext with this
schema is illustrated in Figure 5. Annotation of
this kind is carried out by a Java program auto-
matically for the entire bitext corpus.
4.2 Corpus Browsing
A number of display modes are designed for
browsing the subparagraph-aligned bitexts, in-
cluding bilingual modes and monolingual modes.
75
Figure 4: XML schema for aligned BLIS bitexts
In a bilingual mode, text line pairs are displayed
in sequence. Switch of language order or from
one mode to another is allowed any time during
browsing. The bilingual display mode is illus-
trated in Figure 6.
5 Conclusion
We have presented in the above sections our re-
cent work on harvesting and aligning the bitexts
of the laws of Hong Kong, including basic tech-
niques for downloading English-Chinese bilin-
gual legal texts from BLIS official site, sound
strategies for aligning the bitexts by utilizing the
numbering system in the legal texts, and neces-
sary XML annotation for the alignment results.
The value of the outcomes, i.e., the subparagraph-
aligned bilingual corpus, can be evaluated in
terms of the following aspects.
Corpus size The entire corpus is of 10.4M En-
glish words and 18.3M Chinese characters,
several times larger than the well-known
Penn Treebank Corpus in size.
Figure 5: Sample bitext in XML encoding
Translation quality All texts of the corpus are
prepared by the Law Drafting Division of
the Department of Justice, Hong Kong Gov-
ernment. Legal texts are known to be more
precise and less ambiguous than most other
types of text.
Specificity and comprehensiveness The corpus
covers specifically the domain of Hong Kong
legislation. It is the most authoritative and
complete text collection of the laws of Hong
Kong.
Alignment granularity The entire corpus is
aligned precisely to the subparagraph level.
Most subparagraphs in the legal texts are
phrases, fragments of a clause, or clauses; as
shown in Table 4.
76
Figure 6: Illustration of browsing modes
A bilingual corpus of this size and quality cov-
ering a specific domain so comprehensively is
particularly useful not only in empirical MT re-
search but also in computational studies of bilin-
gual terminology and legislation. Our future work
will focus on word alignment for inferring bilin-
gual lexical resources and on automatic recogni-
tion of legal terminology.
Also, our experience in constructing this bilin-
gual corpus has laid a foundation for us to con-
tinue to harvest more bilingual text materials from
the Web, e.g., from Hong Kong government?s
Web sites. We find that almost all Hong Kong
government web sites, which are in large num-
bers, maintain their Web pages consistently par-
allel in English and Chinese. We are not sure if
such bitexts in such pages are larger than that in
the BLIS site in volume. We do know they cover
a large number of distinct domains. This is partic-
ularly useful for MT. If we can harvest and align
the bitexts from such Web pages efficiently via
utilizing their intrinsic characteristics of URL cor-
respondence and text structure, it would not be a
dream any more to put an end to the time of hav-
ing too few existing translation materials for em-
pirical MT studies, at least, for the language pair
of Chinese and English.
Acknowledgements
The work described in this paper was supported
by the Research Grants Council of HKSAR,
China, through the CERG grants 9040861 and
9040482. We wish to thank our team members
for their help.
References
Anne H. Anderson, Miles Bader, Ellen G. Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllis-
ter, Jim Miller, Catherine Sotillo, Henry Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34(4):351?366.
Marco Baroni, Silvia Bernardini, Federica Comastri,
Lorenzo Piccioni, Alessandra Volpi, Guy Aston,
and Marco Mazzoleni. 2004. Introducing the La
Repubblica corpus: A large, annotated, TEI(XML)-
compliant corpus of newspaper Italian. In LREC
2004, pp. 1771-1774.
Simon P. Botley, Anthony M. McEnery, and Andrew
Wilson (eds.). 2000. Multilingual Corpora in
Teaching and Research. Amsterdam: Rodopi.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Michael Carl and Andy Way (eds.). 2003. Recent
Advances in Example-based Machine Translation.
Dordrecht: Kluwer.
Jiang Chen and Jian Y. Nie. 2000. Parallel Web text
mining for cross-language information retrieval. In
RIAO?2000, pp. 62?77. Paris.
Mark Davis and Ted Dunning. 1995. A TREC evalu-
ation of query translation methods for multi-lingual
text retrieval. In TREC-4, pp. 483?498. NIST.
Jarle Ebeling. 1998. Contrastive linguistics, transla-
tion, and parallel corpora. In Meta, 43(4):602?615.
William A. Gale and Kenneth W. Church. 1991. Iden-
tifying word correspondences in parallel texts. In
Fourth DARPA Workshop on Speech and Natural
Language, pp. 152?157. Asilomar, California.
William A. Gale and Kenneth W. Church. 1991b. A
Program for Aligning Sentences in Bilingual Cor-
pora. In ACL?91, pp. 177?184. Berkeley.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: an XML-based encoding standard
for linguistic corpora. In LREC2000, pp. 825?830.
Athens, Greece.
77
Nancy Ide and Catherine Macleod. 2001. The Amer-
ican National Corpus: A Standardized Resource of
American English. Proceedings of Corpus Linguis-
tics 2001, Lancaster UK.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the Special Issue on the Web as Cor-
pus. Computational Linguistics, 29(3):333?347.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, Hideki
Mima and Jun?ichi Tsujii. 2001. XML-based lin-
guistic annotation of corpus. In NLPXML-1, pp. 47?
54. Tokyo.
I. Dan Melamed. 1997. Automatic discovery of
non-compositional compounds in parallel data. In
EMNLP?97, pp. 97?108. Brown University, Au-
gust.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Andreas Mengel and Wolfgang Lezius. 2000. An
XML-based representation format for syntactically
annotated corpora. In LREC2000, Volume 1,
pp. 121?126. Athens, Greece.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and English by anal-
ogy principle. Artificial and Human Intelligence,
pp. 173?180. Amsterdam: North-Holland.
Jian Y. Nie and Jian Cai. 2001. Filtering noisy paral-
lel corpora of Web pages. In IEEE Symposium on
Natural Language Processing and Knowledge En-
gineering, pp. 453?458. Tucson, AZ.
Jian Y. Nie and Jiang Chen. 2002. Exploiting the
Web as Parallel Corpora for Cross-Language Infor-
mation Retrieval. Web Intelligence, pp. 218?239.
Philip Resnik, Mari B. Olse, and Mona Diab. 1999.
The Bible as a parallel corpus: Annotating the
?Book of 2000 Tongues?. Computers and the Hu-
manities, 33(1-2):129?153.
Philip Resnik. 1999b. Mining the Web for Bilingual
Text. In ACL?99, pp. 527?534. Maryland.
Philip Resnik and Noah A. Smith. 2003. The Web
as a Parallel Corpus. Computational Linguistics,
29(3):349?380.
Raphael Salkie. 1995. INTERSECT: a parallel cor-
pus project at Brighton University. Computers and
Texts 9 (May 1995), pp. 4?5.
Jean Veronis. 2000. Parallel Text Processing. Dor-
drecht: Kluwer.
Andy Way and Nano Gough. 2003. wEBMT:
Developing and validating an example-based ma-
chine translation system using the World Wide Web.
Computational Linguistics, 29(3):421?457.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In ACL?94,
pp. 80?87. Las Cruces, New Mexico, U.S.A.
78
An Empirical Comparison of Goodness Measures for
Unsupervised Chinese Word Segmentation with a Unified Framework
Hai Zhao?and Chunyu Kit
Department of Chinese, Translation and Linguistics,
City University of Hong Kong,
83 Tat Chee Avenue, Kowloon, Hong Kong, China
Email: haizhao@cityu.edu.hk, ctckit@cityu.edu.hk
Abstract
This paper reports our empirical evaluation
and comparison of several popular good-
ness measures for unsupervised segmenta-
tion of Chinese texts using Bakeoff-3 data
sets with a unified framework. Assuming no
prior knowledge about Chinese, this frame-
work relies on a goodness measure to iden-
tify word candidates from unlabeled texts
and then applies a generalized decoding al-
gorithm to find the optimal segmentation
of a sentence into such candidates with the
greatest sum of goodness scores. Exper-
iments show that description length gain
outperforms other measures because of its
strength for identifying short words. Further
performance improvement is also reported,
achieved by proper candidate pruning and
by assemble segmentation to integrate the
strengths of individual measures.
1 Introduction
Unsupervised Chinese word segmentation was ex-
plored in a number of previous works for various
purposes and by various methods (Ge et al, 1999;
Fu and Wang, 1999; Peng and Schuurmans, 2001;
?The research described in this paper was supported by the
Research Grants Council of Hong Kong S.A.R., China, through
the CERG grant 9040861 (CityU 1318/03H) and by City Uni-
versity of Hong Kong through the Strategic Research Grant
7002037. Dr. Hai Zhao was supported by a postdoctoral Re-
search Fellowship in the Department of Chinese, Translation
and Linguistics, City University of Hong Kong. Thanks four
anonymous reviewers for their insightful comments!
SUN et al, 2004; Jin and Tanaka-Ishii, 2006). How-
ever, various heuristic rules are often involved in
most existing works, and there has not been a com-
prehensive comparison of their performance in a
unified way with available large-scale ?gold stan-
dard? data sets, especially, multi-standard ones since
Bakeoff-1 1.
In this paper we will propose a unified frame-
work for unsupervised segmentation of Chinese text.
Four existing approaches to unsupervised segmenta-
tions or word extraction are considered as its special
cases, each with its own goodness measurement to
quantify word likelihood. The output by each ap-
proach will be evaluated using benchmark data sets
of Bakeoff-32 (Levow, 2006). Note that unsuper-
vised segmentation is different from, if not more
complex than, word extraction, in that the former
must carry out the segmentation task for a text, for
which a segmentation (decoding) algorithm is indis-
pensable, whereas the latter only acquires a word
candidate list as output (Chang and Su, 1997; Zhang
et al, 2000).
2 Generalized Framework
We propose a generalized framework to unify the
existing methods for unsupervised segmentation, as-
suming the availability of a list of word candidates
each associated with a goodness for how likely it is
to be a true word. Let W = {{wi, g(wi)}i=1,...,n} be
such a list, where wi is a word candidate and g(wi)
1First International Chinese Word Segmentation Bakeoff, at
http://www.sighan.org/bakeoff2003
2The Third International Chinese Language Processing
Bakeoff, at http://www.sighan.org/bakeoff2006.
9
its goodness function.
Two generalized decoding algorithms, (1) and (2),
are formulated for optimal segmentation of a given
plain text. The first one, decoding algorithm (1), is a
Viterbi-style one to search for the best segmentation
S? for a text T , as follows,
S? = argmax
w1???wi???wn =T
n?
i=1
g(wi), (1)
with all {wi, g(wi)} ? W .
Another algorithm, decoding algorithm (2), is a
maximal-matching one with respect to a goodness
score. It works on T to output the best current word
w? repeatedly with T=t? for the next round as fol-
lows,
{w?, t?} = argmax
wt=T
g(w) (2)
with each {w, g(w)} ? W . This algorithm will back
off to forward maximal matching algorithm if the
goodness function is set to word length. Thus the
former may be regarded as a generalization of the
latter. Symmetrically, it has an inverse version that
works the other way around.
3 Goodness Measurement
An unsupervised segmentation strategy has to rest
on some predefined criterion, e.g., mutual informa-
tion (MI), in order to recognize a substring in the text
as a word. Sproat and Shih (1990) is an early inves-
tigation in this direction. In this study, we examine
four types of goodness measurement for a candidate
substring3. In principle, the higher goodness score
for a candidate, the more possible it is to be a true
word.
Frequency of Substring with Reduction A lin-
ear algorithm was proposed in (Lu? et al, 2004) to
produce a list of such reduced substrings for a given
corpus. The basic idea is that if two partially over-
lapped n-grams have the same frequency in the input
corpus, then the shorter one is discarded as a redun-
dant word candidate. We take the logarithm of FSR
3Although there have been many existing works in this di-
rection (Lua and Gan, 1994; Chien, 1997; Sun et al, 1998;
Zhang et al, 2000; SUN et al, 2004), we have to skip the de-
tails of comparing MI due to the length limitation of this paper.
However, our experiments with MI provide no evidence against
the conclusions in this paper.
as the goodness for a word candidate, i.e.,
gFSR(w) = log(p?(w)) (3)
where p?(w) is w?s frequency in the corpus. This
allows the arithmetic addition in (1). According to
Zipf?s Law (Zipf, 1949), it approximates the use of
the rank of w as its goodness, which would give it
some statistical significance. For the sake of effi-
ciency, only those substrings that occur more than
once are considered qualified word candidates.
Description Length Gain (DLG) The goodness
measure is proposed in (Kit and Wilks, 1999) for
compression-based unsupervised segmentation. The
DLG from extracting all occurrences of xixi+1...xj
(also denoted as xi..j) from a corpus X= x1x2...xn
as a word is defined as
DLG(xi..j) = L(X)? L(X[r ? xi..j ]? xi..j) (4)
where X[r ? xi..j ] represents the resultant corpus
from replacing all instances of xi..j with a new sym-
bol r throughout X and ? denotes the concatenation
of two substrings. L(?) is the empirical description
length of a corpus in bits that can be estimated by the
Shannon-Fano code or Huffman code as below, fol-
lowing classic information theory (Shannon, 1948).
L(X) .= ?|X|
?
x?V
p?(x)log2p?(x) (5)
where | ? | denotes string length, V is the character
vocabulary of X and p?(x) x?s frequency in X . For
a given word candidate w, we define gDLG(w) =
DLG(w). In principle, a substring with a negative
DLG do not bring any positive compression effect
by itself. Thus only substrings with a positive DLG
value are added into our word candidate list.
Accessor Variety (AV) Feng et al (2004) propose
AV as a statistical criterion to measure how likely a
substring is a word. It is reported to handle low-
frequent words particularly well. The AV of a sub-
string xi..j is defined as
AV (xi..j) = min{Lav(xi..j), Rav(xi..j)} (6)
where the left and right accessor variety Lav(xi..j)
and Rav(xi..j) are, respectively, the number of dis-
tinct predecessor and successor characters. For a
similar reason as to FSR, the logarithm of AV is used
10
as goodness measure, and only substrings with AV
> 1 are considered word candidates. That is, we
have gAV (w) = logAV (w) for a word candidate w.
Boundary Entropy (Branching Entropy, BE) It
is proposed as a criterion for unsupervised segmen-
tation in some existing works (Tung and Lee, 1994;
Chang and Su, 1997; Huang and Powers, 2003; Jin
and Tanaka-Ishii, 2006). The local entropy for a
given xi..j , defined as
h(xi..j) = ?
?
x?V
p(x|xi..j)log p(x|xi..j), (7)
indicates the average uncertainty after (or before)
xi..j in the text, where p(x|xi..j) is the co-occurrence
probability for x and xi..j . Two types of h(xi..j),
namely hL(xi..j) and hR(xi..j), can be defined for
the two directions to extend xi..j (Tung and Lee,
1994). Also, we can define hmin = min{hR, hL} in
a similar way as in (6). In this study, only substrings
with BE > 0 are considered word candidates. For a
candidate w, we have gBE (w) = hmin(w)4.
4 Evaluation
The evaluation is conducted with all four corpora
from Bakeoff-3 (Levow, 2006), as summarized in
Table 1 with corpus size in number of characters.
For unsupervised segmentation, the annotation in
the training corpora is not used. Instead, they
are used for our evaluation, for they are large and
thus provide more reliable statistics than small ones.
Segmentation performance is evaluated by word F-
measure F = 2RP/(R + P ). The recall R and
precision P are, respectively, the proportions of the
correctly segmented words to all words in the gold-
standard and a segmenter?s output5.
Note that a decoding algorithm always requires
the goodness score of a single-character candidate
4Both AV and BE share a similar idea from Harris (1970):
If the uncertainty of successive token increases, then it is likely
to be at a boundary. In this sense, one may consider them the
discrete and continuous formulation of the same idea.
5All evaluations will be represented in terms of word
F-measure if not otherwise specified. A standard scoring
tool with this metric can be found in SIGHAN website,
http://www.sighan.org/bakeoff2003/score. However, to com-
pare with related work, we will also adopt boundary F-measure
Fb = 2RbPb/(Rb + Pb), where the boundary recall Rb and
boundary precision Pb are, respectively, the proportions of the
correctly recognized boundaries to all boundaries in the gold-
standard and a segmenter?s output (Ando and Lee, 2000).
Table 1: Bakeoff-3 Corpora
Corpus AS CityU CTB MSRA
Training(M) 8.42 2.71 0.83 2.17
Test(K) 146 364 256 173
Table 2: Performance with decoding algorithm (1)
M. Good- Training corpus
L.a ness AS CityU CTB MSRA
FSR .400 .454 .462 .432
2 DLG/d .592 .610 .604 .603AV .568 .595 .596 .577
BE .559 .587 .592 .572
FSR .193 .251 .268 .235
7 DLG/d .331 .397 .409 .379AV .399 .423 .430 .407
BE .390 .419 .428 .403
aM.L.: Maximal length allowable for word candidates.
for computation. There are two ways to get this
score: (1) computed by the goodness measure,
which is applicable only if the measure allows; (2)
set to zero as default value, which is always appli-
cable even to single-character candidates not in the
word candidate list in use. For example, all single-
character candidates given up by DLG because of
their negative DLG scores will have a default value
during decoding. We will use a ?/d? to indicate ex-
periments using such a default value.
4.1 Comparison
We apply the decoding algorithm (1) to segment all
Bakeoff-3 corpora with the above goodness mea-
sures. Both word candidates and goodness values
are derived from the raw text of each training cor-
pus. The performance of these measures is presented
in Table 2. From the table we can see that DLG
and FSR have the strongest and the weakest perfor-
mance, respectively, whereas AV and BE are highly
comparable to each other.
Decoding algorithm (2) runs the forward and
backward segmentation with the respective AV
and BE criteria, i.e., LAV /hL for backward and
RAV /hR forward, and the output is the union of two
segmentations 6. A performance comparison of AV
and BE with both algorithms (1) and (2) is presented
in Table 3. We can see that the former has a rela-
6That is, all segmented points by either segmentation will be
accounted into the final segmentation.
11
Table 3: Performance comparison: AV vs. BE
M. Good- Training corpus
L. ness AS CityU CTB MSRA
AV(1) .568 .595 .596 .577
AV(2)/d .485 .489 .508 .471
AV(2) .445 .366 .367 .3872 BE(1) .559 .587 .592 .572
BE(2)/d .485 .489 .508 .471
BE(2) .504 .428 .446 .446
AV(1) .399 .423 .430 .407
AV(2)/d .570 .581 .588 .572
AV(2) .445 .366 .368 .3877 BE(1) .390 .419 .428 .403
BE(2)/d .597 .604 .605 .593
BE(2) .508 .431 .449 .446
2 3 4 5 6 70.35
0.4
0.45
0.5
0.55
0.6
 The Range of Word Length
 
F?meas
ure
BE/(2): ASBE/(2): CityUBE/(2): CTBBE/(2): MSRADLG/(1): ASDLG/(1): CityUDLG/(1): CTBDLG/(1): MSRA
Figure 1: Performance vs. word length
tively better performance on shorter words and the
latter outperforms on longer ones.
How segmentation performance varies along with
word length is exemplified with DLG and BE as ex-
amples in Figure 1, with (1) and (2) indicating a re-
spective decoding algorithm in use. It shows that
DLG outperforms on two-character words and BE
on longer ones.
4.2 Word Candidate Pruning
Up to now, word candidates are determined by the
default goodness threshold 0. The number of them
for each of the four goodness measures is presented
in Table 4. We can see that FSR generates the largest
set of word candidates and DLG the smallest. More
interestingly or even surprising, AV and BE generate
exactly the same candidate list for all corpora.
In addition to word length, another crucial factor
to affect segmentation performance is the quality of
the word candidates as a whole. Since each candi-
date is associated with a goodness score to indicate
how good it is, a straightforward way to ensure, and
further enhance, the overall quality of a candidate
set is to prune off those with low goodness scores.
Table 4: Word candidate number by threshold 0
Good- Training Corpus
ness AS CityU CTB MSRA
FSR 2,009K 832K 294K 661K
DLG 543K 265K 96K 232K
AV 1,153K 443K 160K 337K
BE 1,153K 443K 160K 337K
2 3 4 5 6 70.4
0.45
0.5
0.55
0.6
0.65
 The Range of Word Length
 
F?meas
ure
100% size89% size79% size74% size70% size65% size62% size48% size38% size
Figure 2: Performance by candidate pruning: DLG
To examine how segmentation performance changes
along with word candidate pruning and decide the
optimal pruning rate, we conduct a series of experi-
ments with each goodness measurements. Figures 2
and 3 present, as an illustration, the outcomes of two
series of our experiments with DLG by decoding al-
gorithm (1) and BE by decoding algorithm (1) and
(2) on CityU training corpus. We find that appro-
priate pruning does lead to significant performance
improvement and that both DLG and BE keep their
superior performance respectively on two-character
words and others. We also observe that each good-
ness measure has a stable and similar performance
in a range of pruning rates around the optimal one,
e.g., 79-62% around 70% in Figure 2.
The optimal pruning rates found through our ex-
periments for the four goodness measures are given
in Table 5, and their correspondent segmentation
performance in Table 6. These results show a re-
markable performance improvement beyond the de-
2 3 4 5 6 70.4
0.45
0.5
0.55
0.6
0.65
 The Range of Word Length
 
F?meas
ure 100% size/(1)38% size/(1)32% size/(1)19% size/(1)10% size/(1)100% size/(2)27% size/(2)19% size/(2)16% size/(2)13.5% size/(2)11% size/(2)4.5% size/(2)
Figure 3: Performance by candidate pruning: BE
12
Table 5: Optimal rates for candidate pruning (%)
Decoding Goodness measure
algorithm FSR DLG AV BE
(1) 1.8 70 12.5 20
(2) ? ? 8 12.5
Table 6: Performance via optimal candidate pruning
M. Good- Training corpus
L. ness AS CityU CTB MSRA
FSR(1) .501 .525 .513 .522
DLG(1)/d .710 .650 .664 .638
2 AV(1) .616 .625 .609 .618BE(1) .613 .614 .605 .611
AV(2)/d .585 .602 .589 .599
BE(2)/d .591 .599 .596 .593
FSR(1) .444 .491 .486 .486
DLG(1)/d .420 .447 .460 .423
7 AV(1) .517 .568 .549 .544BE(1) .501 .539 .510 .519
AV(2)/d .623 .624 .604 .615
BE(2)/d .630 .631 .620 .622
fault threshold setting. What remains unchanged is
the advantage of DLG for two-character words and
that of AV/BE for longer words. However, DLG
achieves the best overall performance among the
four, although it uses only single- and two-character
word candidates. The overwhelming number of two-
character words in Chinese allows it to triumph.
4.3 Ensemble Segmentation
Although proper pruning of word candidates brings
amazing performance improvement, it is unlikely
for one to determine an optimal pruning rate in prac-
tice for an unlabeled corpus. Here we put forth a
parameter-free method to tackle this problem with
the aids of all available goodness measures.
The first step of this method to do is to derive an
optimal set of word candidates from the input. We
have shown above that quality candidates play a crit-
ical role in achieving quality segmentation. Without
any better goodness criterion available, the best we
can opt for is the intersection of all word candidate
lists generated by available goodness measures with
the default threshold. A good reason for this is that
the agreement of them can give a more reliable de-
cision than any individual one of them. In fact, we
only need DLG and AV/BE to get this intersection,
because AV and BE give the same word candidates
Table 7: Performances of ensemble segmentation
M. Good- Training corpus
L. ness AS CityU CTB MSRA
FSR(1) .629 .635 .624 .623
2 DLG(1)/d .664 .653 .643 .650AV(1) .641 .644 .631 .634
BE(1) .640 .643 .632 .634
7 AV(2)/d .595 .637 .624 .610BE(2)/d .593 .635 .620 .609
DLG(1)/d+AV(2)/d .672 .684 .663 .665
DLG(1)/d+BE(2)/d .660 .681 .656 .653
and DLG generates only a subset of what FSR does.
The next step is to use this intersection set of
word candidates to perform optimal segmentation
with each goodness measures, to see if any fur-
ther improvement can be achieved. The best re-
sults are given in Table 7, showing that decoding al-
gorithm (1) achieves marvelous improvement using
short word candidates with all other goodness mea-
sures than DLG. Interestingly, DLG still remains at
the top by performance despite of some slip-back.
To explore further improvement, we also try
to combine the strengths of DLG and AV/BE re-
spectively for recognizing two- and multi-character
word. Our strategy to combine them together is to
enforce the multi-character words in AV/BE seg-
mentation upon the correspondent parts of DLG seg-
mentation. This ensemble method gives a better
overall performance than all others that we have
tried so far, as presented at the bottom of Table 7.
4.4 Yet Another Decoding Algorithm
Jin and Tanaka-Ishii (2006) give an unsupervised
segmentation criterion, henceforth referred to as de-
coding algorithm (3), to work with BE. It works as
follows: if g(xi..j+1) > g(xi..j) for any two over-
lapped substrings xi..j and xi..j+1, then a segment-
ing point should be located right after xi..j+1. This
algorithm has a forward and a backward version.
The union of the segmentation outputs by both ver-
sions is taken as the final output of the algorithm,
in exactly the same way as how decoding algorithm
(2) works7. This algorithm is evaluated in (Jin and
Tanaka-Ishii, 2006) using Peking University (PKU)
7Three segmentation criteria are given in (Jin and Tanaka-
Ishii, 2006), among which the entropy increase criterion,
namely, decoding algorithm (3), proves to be the best. Here we
would like to thank JIN Zhihui and Prof. Kumiko Tanaka-Ishii
for presenting the details of their algorithms.
13
Table 8: Performance comparison by word and
boundary F-measure on PKU corpus (M. L. = 6)
Good- Decoding algorithm
ness (1)/d (1) (2)/d (2) (3)/d (3)
AV .313 .325 .588 .373 .376 .453
F AV? .372 .372 .663 .663 .445 .445
BE .309 .319 .624 .501 .376 .624
BE? .370 .370 .676 .676 .447 .447
AV .695 .700 .830 .762 .762 .728
Fb AV? .728 .728 .865 .865 .783 .783
BE .696 .699 .849 .810 .762 .837a
BE? .728 .728 .872 .872 .784 .784
aWith the same hyperparameters, (Jin and Tanaka-Ishii, 2006)
report their best result of boundary precision 0.88 and boundary
recall 0.79, equal to boundary F-measure 0.833.
Corpus of 1.1M words8 as gold standard with a word
candidate list extracted from the 200M Contempo-
rary Chinese Corpus that mostly consists of several
years of Peoples? Daily9. Here, we carry out evalu-
ation with similar data: we extract word candidates
from the unlabeled texts of People?s Daily (1993 -
1997), of 213M and about 100M characters, in terms
of the AV and BE criteria, yielding a list of 4.42 mil-
lion candidates up to 6-character long10 for each cri-
terion. Then, the evaluation of the three decoding
algorithms is performed on PKU corpus.
The evaluation results with both word and bound-
ary F-measure are presented for the same segmenta-
tion outputs in Table 8, with ?*? to indicate candi-
date pruning by DLG > 0 as reported before. Note
that boundary F-measure gives much more higher
score than word F-measure for the same segmenta-
tion output. However, in either of metric, we can
find no evidence in favor of decoding algorithm (3).
Undesirably, this algorithm does not guarantee a sta-
ble performance improvement with the BE measure
through candidate pruning.
4.5 Comparison against Supervised
Segmentation
Huang and Zhao (2007) provide empirical evidence
to estimate the degree to which the four segmenta-
tion standards involved in the Bakeoff-3 differ from
each other. As quoted in Table 9, a consistency rate
8http://icl.pku.edu.cn/icl groups/corpus/dwldform1.asp
9http://ccl.pku.edu.cn:8080/ccl corpus/jsearch/index.jsp
10This is to keep consistence with (Jin and Tanaka-Ishii,
2006), where 6 is set as the maximum n-gram length.
Table 9: Consistency rate among Bakeoff-3 segmen-
tation standards (Huang and Zhao, 2007)
Test Training corpus
corpus AS CityU CTB MSRA
AS 1.000 0.926 0.959 0.858
CityU 0.932 1.000 0.935 0.849
CTB 0.942 0.910 1.000 0.877
MSRA 0.857 0.848 0.887 1.000
beyond 84.8% is found among the four standards.
If we do not over-expect unsupervised segmentation
to achieve beyond what these standards agree with
each other, it is reasonable to take this figure as the
topline for evaluation. On the other hand, Zhao et al
(2006) show that the words of 1 to 2 characters long
account for 95% of all words in Chinese texts, and
single-character words alone for about 50%. Thus,
we can take the result of the brute-force guess of ev-
ery single character as a word as a baseline.
To compare to supervised segmentation, which
usually involves training using an annotated train-
ing corpus and, then, evaluation using test corpus,
we carry out unsupervised segmentation in a com-
parable manner. For each data track, we first ex-
tract word candidates from both the training and test
corpora, all unannotated, and then evaluate the un-
supervised segmentation with reference to the gold-
standard segmentation of the test corpus. The re-
sults are presented in Table 10, together with best
and worst official results of the Bakeoff closed test.
This comparison shows that unsupervised segmen-
tation cannot compete against supervised segmenta-
tion in terms of performance. However, the experi-
ments generate positive results that the best combi-
nation of the four goodness measures can achieve an
F-measure in the range of 0.65-0.7 on all test corpora
in use without using any prior knowledge, but ex-
tracting word candidates from the unlabeled training
and test corpora in terms of their goodness scores.
5 Discussion: How Things Happen
Note that DLG criterion is to perform segmentation
with the intension to maximize the compression ef-
fect, which is a global effect through the text. Thus
it works well incorporated with a probability maxi-
mization framework, where high frequent but inde-
pendent substrings are effectively extracted and re-
14
Table 10: Comparison of performances against su-
pervised segmentation
Type Test corpusAS CityU CTB MSRA
Baseline .389 .345 .337 .353
DLG(1)/d .597 .616 .601 .602
DLG?(1)/d .655 .659 .632 .655
2 AV(1) .577 .603 .597 .583
AV?(1) .630 .650 .618 .638
BE(1) .570 .598 .594 .580
BE?(1) .629 .649 .618 .638
AV(2)/d .512 .551 .543 .526
AV?(2)/d .591 .644 .618 .604
7 BE(2)/d .518 .554 .546 .533
BE?(2)/d .587 .641 .614 .605
DLG?(1)/d +AV?(2)/d .663 .692 .658 .667
DLG?(1)/d +BE?(2)/d .650 .689 .650 .656
Worst closed .710 .589 0.818 .819
Best closed .958 .972 0.933 .963
combined. We know that most unsupervised seg-
mentation criteria will bring up long word bias prob-
lem, so does DLG measure. This explains why it
gives the worse results as long candidates are added.
As for AV and BE measures, both of them give the
metric of the uncertainty before or after the current
substring. This means that they are more concerned
with local uncertainty information near the current
substring, instead of global information among the
whole text as DLG. Thus local greedy search in
maximal matching style is more suitable for these
two measures than Viterbi search.
Our empirical results about word candidate list
with default threshold 0, where the same list is from
AV and BE, give another proof that both AV and BE
reflect the same uncertainty. The only difference is
behind the fact that the former and the latter is in the
discrete and continuous formulation, respectively.
6 Conclusion and Future Work
This paper reported our empirical comparison of a
number of goodness measures for unsupervised seg-
mentation of Chinese texts with the aid two gener-
alized decoding algorithms. We learn no previous
work by others for a similar attempt. The compari-
son is carried out with Bakeoff-3 data sets, showing
that all goodness measures exhibit their strengths for
recognizing words of different lengths and achieve a
performance far beyond the baseline. Among them,
DLG with decoding algorithm (1) can achieve the
best segmentation performance for single- and two-
character words identification and the best overall
performance as well. Our experiments also show
that the quality of word candidates plays a criti-
cal role in ensuring segmentation performance 11.
Proper pruning of candidates with low goodness
scores to enhance this quality enhances the seg-
mentation performance significantly. Also, the suc-
cess of unsupervised segmentation depends strongly
on an appropriate decoding algorithm. Generally,
Viterbi-style decoding produces better results than
best-first maximal-matching. But the latter is not shy
from exhibiting its particular strength for identifying
multi-character words.
Finally, the ensemble segmentation we put forth
to combine the strengths of different goodness mea-
sures proves to be a remarkable success. It achieves
an impressive performance improvement on top of
individual goodness measures.
As for future work, it would be natural for re-
searchers to enhance supervised learning for Chi-
nese word segmentation with goodness measures in-
troduced here. There does be two successful exam-
ples in our existing work (Zhao and Kit, 2007). This
is still an ongoing work.
References
Rie Kubota Ando and Lillian Lee. 2000. Mostly-
unsupervised statistical segmentation of Japanese: Ap-
plications to kanji. In Proceedings of the first Confer-
ence on North American Chapter of the Association
for Computational Linguistics and the 6th Conference
on Applied Natural Language Processing, pages 241?
248, Seattle, Washington, April 30.
Jing-Shin Chang and Keh-Yih Su. 1997. An unsuper-
vised iterative method for Chinese new lexicon ex-
traction. Computational Linguistics and Chinese Lan-
guage Processing, 2(2):97?148.
Lee-Feng Chien. 1997. PAT-tree-based keyword extrac-
tion for Chinese information retrieval. In Proceedings
of the 20th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 50?58, Philadelphia.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
11This observation is shared by other researchers, e.g., (Peng
et al, 2002).
15
Guo-Hong Fu and Xiao-Long Wang. 1999. Unsu-
pervised Chinese word segmentation and unknown
word identification. In 5th Natural Language Process-
ing Pacific Rim Symposium 1999 (NLPRS?99), ?Clos-
ing the Millennium?, pages 32?37, Beijing, China,
November 5-7.
Xianping Ge, Wanda Pratt, and Padhraic Smyth. 1999.
Discovering Chinese words from unsegmented text. In
SIGIR ?99: Proceedings of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 271?272,
Berkeley, CA, USA, August 15-19. ACM.
Zellig Sabbetai Harris. 1970. Morpheme boundaries
within words. In Papers in Structural and Transfor-
mational Linguistics, page 68?77.
Jin Hu Huang and David Powers. 2003. Chinese
word segmentation based on contextual entropy. In
Dong Hong Ji and Kim-Ten Lua, editors, Proceedings
of the 17th Asian Pacific Conference on Language, In-
formation and Computation, pages 152?158, Sentosa,
Singapore, October, 1-3. COLIPS Publication.
Chang-Ning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21(3):8?20.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branch-
ing entropy. In COLING/ACL 2006, pages 428?435,
Sidney, Australia.
Chunyu Kit and Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. In M. Osborne and E. T. K. Sang, editors,
CoNLL-99, pages 1?6, Bergen, Norway.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In Proceedings of the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing, pages 108?117, Sydney, Australia, July.
Xueqiang Lu?, Le Zhang, and Junfeng Hu. 2004. Sta-
tistical substring reduction in linear time. In Keh-
Yih Su et al, editor, Proceeding of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP-2004), volume 3248 of Lecture Notes
in Computer Science, pages 320?327, Sanya City,
Hainan Island, China, March 22-24. Springer.
Kim-Teng Lua and Kok-Wee Gan. 1994. An applica-
tion of information theory in Chinese word segmenta-
tion. Computer Processing of Chinese and Oriental
Languages, 8(1):115?123.
Fuchun Peng and Dale Schuurmans. 2001. Self-
supervised Chinese word segmentation. In The Fourth
International Symposium on Intelligent Data Analysis,
pages 238?247, Lisbon, Portugal, September, 13-15.
Fuchun Peng, Xiangji Huang, Dale Schuurmans, Nick
Cercone, and Stephen Robertson. 2002. Using self-
supervised word segmentation in Chinese information
retrieval. In Proceedings of the 25th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 349?350,
Tampere, Finland, August, 11-15.
Claude E. Shannon. 1948. A mathematical theory of
communication. The Bell System Technical Journal,
27:379?423, 623?656, July, October.
Richard Sproat and Chilin Shih. 1990. A statistical
method for finding word boundaries in Chinese text.
Computer Processing of Chinese and Oriental Lan-
guages, 4(4):336?351.
Maosong Sun, Dayang Shen, and Benjamin K. Tsou.
1998. Chinese word segmentation without using lexi-
con and hand-crafted training data. In COLING-ACL
?98, 36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International Confer-
ence on Computational Linguistics, volume 2, pages
1265?1271, Montreal, Quebec, Canada.
Mao Song SUN, Ming XIAO, and Benjamin K. Tsou.
2004. Chinese word segmentation without using dic-
tionary based on unsupervised learning strategy (in
Chinese) (????????????????
???????). Chinese Journal of Computers,
27(6):736?742.
Cheng-Huang Tung and His-Jian Lee. 1994. Iden-
tification of unknown words from corpus. Compu-
tational Proceedings of Chinese and Oriental Lan-
guages, 8:131?145.
Jian Zhang, Jianfeng Gao, and Ming Zhou. 2000. Ex-
traction of Chinese compound words ? an experimen-
tal study on a very large corpus. In Proceedings of
the Second Chinese Language Processing Workshop,
pages 132?139, Hong Kong, China.
Hai Zhao and Chunyu Kit. 2007. Incorporating global
information into supervised learning for Chinese word
segmentation. In Proceedings of the 10th Conference
of the Pacific Association for Computational Linguis-
tics, pages 66?74, Melbourne, Australia, September
19-21.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese word
segmentation via conditional random field modeling.
In Proceedings of the 20th Asian Pacific Conference on
Language, Information and Computation, pages 87?
94, Wuhan, China, November 1-3.
George Kingsley Zipf. 1949. Human Behavior and
the Principle of Least Effort. Addison-Wesley, Cam-
bridge, MA.
16
Unsupervised Segmentation Helps Supervised Learning of
Character Tagging for Word Segmentation and Named Entity Recognition
Hai Zhao and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Ave., Kowloon, Hong Kong
Email: {haizhao, ctckit}@cityu.edu.hk
Abstract
This paper describes a novel character tag-
ging approach to Chinese word segmenta-
tion and named entity recognition (NER) for
our participation in Bakeoff-4.1 It integrates
unsupervised segmentation and conditional
random fields (CRFs) learning successfully,
using similar character tags and feature tem-
plates for both word segmentation and NER.
It ranks at the top in all closed tests of word
segmentation and gives promising results for
all closed and open NER tasks in the Bake-
off. Tag set selection and unsupervised seg-
mentation play a critical role in this success.
1 Introduction
A number of recent studies show that character se-
quence labeling is a simple but effective formula-
tion of Chinese word segmentation and name en-
tity recognition for machine learning (Xue, 2003;
Low et al, 2005; Zhao et al, 2006a; Chen et al,
2006). Character tagging becomes a prevailing tech-
nique for this kind of labeling task for Chinese lan-
guage processing, following the current trend of ap-
plying machine learning as a core technology in the
field of natural language processing. In particular,
when a full-fledged general-purpose sequence learn-
ing model such as CRFs is involved, the only work
to do for a given application is to identify an ideal
set of features and hyperparameters for the purpose
1The Fourth International Chinese Language Processing
Bakeoff & the First CIPS Chinese Language Processing Evalu-
ation, at http://www.china-language.gov.cn/bakeoff08/bakeoff-
08 basic.html.
of achieving the best learning model that we can
with available training data. Our work in this aspect
provides a solid foundation for applying an unsuper-
vised segmentation criterion to enrich the supervised
CRFs learning for further performance enhancement
on both word segmentation and NER.
This paper is intended to present the research for
our participation in Bakeoff-4, with a highlight on
our strategy to select character tags and feature tem-
plates for CRFs learning. Particularly worth men-
tioning is the simplicity of our system in contrast to
its success. The rest of the paper is organized as fol-
lows. The next section presents the technical details
of the system and Section 3 its evaluation results.
Section 4 looks into a few issues concerning charac-
ter tag set, unsupervised segmentation, and available
name entities (NEs) as features for open NER test.
Section 5 concludes the paper.
2 System Description
Following our previous work (Zhao et al, 2006a;
Zhao et al, 2006b; Zhao and Kit, 2007), we con-
tinue to apply the order-1 linear chain CRFs (Laf-
ferty et al, 2001) as our learning model for Bakeoff-
4. Specifically, we use its implementation CRF++
by Taku Kudo2 freely available for research purpose.
We opt for a similar set of character tags and feature
templates for both word segmentation and NER.
In addition, two key techniques that we have ex-
plored in our previous work are applied. One is to
introduce more tags in the hope of utilizing more
precise contextual information to achieve more pre-
2http://crfpp.sourceforge.net/
106
Sixth SIGHAN Workshop on Chinese Language Processing
107
Sixth SIGHAN Workshop on Chinese Language Processing
Table 3: Training corpora for assistant learners
Track CityU NER MSRA NER
Ass. Seg. CityU (Bakeoff-1 to 4) MSRA (Bakeoff-2)
ANER-1 CityU(Bakeoff-3) CityU(Bakeoff-3)
ANER-2 MSRA(Bakeoff-3) CityU(Bakeoff-4)
Table 4: NE lists from Chinese Wikipedia
Category Number
Place name suffix 85
Chinese place name 6,367
Foreign place name 1,626
Chinese family name 573
Most common Chinese family name 109
Foreign name 2,591
Chinese university 515
didate s with a score AV (s) is defined as
fn(s) = t, if 2t ? AV (s) < 2t+1,
where t is an integer to logarithmize the score. This
is to alleviate the sparse data problem by narrowing
down the feature representation involved. Note that
t is used as a feature value rather than a parameter
for the CRFs training in our system. For an over-
lap character of several word candidates, we only
choose the one with the greatest AV score to activate
the above feature function for that character. It is
in this way that the unsupervised segmentation out-
comes are fit into the CRFs learning.
2.3 Features for Open NER
Three extra groups of feature template are used for
the open NER beyond those for the closed.
The first group includes three segmentation fea-
ture templates. One is character type feature tem-
plate T (C?1)T (C0)T (C1), where T (C) is the type
of character C. For this, five character types are de-
fined, namdely, number, foreign letter, punctuation,
date and time, and others. The other two are gener-
ated respectively by two assistant segmenters (Zhao
et al, 2006a), a maximal matching segmenter based
on a dictionary from Peking University3 and a CRFs
segmenter using the 6-tag set and the six n-gram fea-
ture templates for training.
3It consists of about 108K words of one to four character-
slong, available at http://ccl.pku.edu.cn/doubtfire/Course/Chi
nese%20Information%20Processing/Source Code/Chapter 8/
Lexicon full.zip.
Table 5: Segmentation results for previous Bakeoffs
Bakeoff-1 AS CityU CTB PKU
?AV
F .9727 .9473 .8720 .9558
ROOVa .7907 .7576 .7022 .7078
+AV
F .9725 .9554 .9023 .9612
ROOV .7597 .7616 .7502 .7208
Bakeoff-2 AS CityU MSRA PKU
?AV
F .9534 .9476 .9735 .9515
ROOV .6812 .6920 .7496 .6720
+AV
F .9570 .9610 .9758 .9540
ROOV .6993 .7540 .7446 .6765
Bakeoff-3 AS CityU CTB MSRA
?AV
F .9538 .9691 .9322 .9608
ROOV .6699 .7815 .7095 .6658
+AV
F .9586 .9747 .9431 .9660
ROOV .6935 .8005 .7608 .6620
aRecall of out-of-vocabulary (OOV) words.
The second group comes from the outputs of
two assistant NE recognizers (ANERs), both trained
with a corresponding 6-tag set and the same six n-
gram feature templates. They share a similar feature
representation as the assistant segmenter. Table 3
lists the training corpora for the assistant CRFs seg-
menter and the ANERs for various open NER tests.
The third group consists of feature templates gen-
erated from seven NE lists acquired from Chinese
Wikipedia.4 The categories and numbers of these
NE items are summarized in Table 4.
3 Evaluation Results
The performance of both word segmentation and
NER is measured in terms of the F-measure F =
2RP/(R + P ), where R and P are the recall and
precision of segmentation or NER.
We tested the techniques described above with
the previous Bakeoffs? data5 (Sproat and Emerson,
2003; Emerson, 2005; Levow, 2006). The evalua-
tion results for the closed tests of word segmentation
are reported in Table 5 and those for the NER on two
corpora of Bakeoff-3 are in the upper part of Table 7.
?+/?AV? indicates whether AV features are applied.
For Bakeoff-4, we participated in all five closed
tracks of word segmentation, namely, CityU, CKIP,
CTB, NCC, and SXU, and in all closed and open
NER tracks of CityU and MSRA.6 The evaluation
4http://zh.wikipedia.org/wiki/??
5http://www.sighan.org
6We declare that our team has never been exposed to the
108
Sixth SIGHAN Workshop on Chinese Language Processing
Table 6: Evaluation results of word segmentation on Bakeoff-4 data sets
Feature Data F P R FIVa PIV RIV FOOV POOV ROOV
CityU .9426 .9410 .9441 .9640 .9636 .9645 .7063 .6960 .7168
CKIP .9421 .9387 .9454 .9607 .9581 .9633 .7113 .7013 .7216
?AV CTB .9634 .9641 .9627 .9738 .9761 .9715 .7924 .7719 .8141
(n-gram) NCC .9333 .9356 .9311 .9536 .9612 .9461 .5678 .5182 .6280
SXU .9552 .9559 .9544 .9721 .9767 .9675 .6640 .6223 .7116
CityU .9510 .9493 .9526 .9667 .9626 .9708 .7698 .7912 .7495
CKIP .9470 .9440 .9501 .9623 .9577 .9669 .7524 .7649 .7404
+AV*b CTB .9589 .9596 .9583 .9697 .9704 .9691 .7745 .7761 .7730
NCC .9405 .9407 .9402 .9573 .9583 .9562 .6080 .5984 .6179
SXU .9623 .9625 .9622 .9752 .9764 .9740 .7292 .7159 .7429
aF-score for in-vocabulary (IV) words.
bHenceforth the official evaluation results in Bakeoff-4 are marked with ?*?.
Table 7: NER evaluation results
Track Setting FPER FLOC FORG FNE
Bakeoff-3
CityU ?AV .8849 .9219 .7905 .8807+AV .9063 .9281 .7981 .8918
MSRA ?AV .7851 .9072 .8242 .8525+AV .8171 .9139 .8164 .8630
Bakeoff-4
?AV .8222 .8682 .6801 .8092
CityU +AV* .8362 .8677 .6852 .8152Open1* .9125 .9216 .7862 .8869
Open2 .9137 .9214 .7853 .8870
?AV .9221 .9193 .8367 .8968
+AV* .9319 .9219 .8414 .9020
MSRA Open* 1.000 .9960 .9920 .9958
Open1a .9710 .9601 .9352 .9558
Open2b .9699 .9581 .9359 .9548
aFor our official submission to Bakeoff-4, we also used
an ANER trained on the MSRA NER training corpus of
Bakeoff-3. This makes our official evaluation results ex-
tremely high but trivial, for a part of this corpus is used as
the MSRA NER test corpus for Bakeoff-4. Presented here
are the results without using this ANER.
bOpen2 is the result of Open1 using no NE list feature.
results of word segmentation and NER for our sys-
tem are presented in Tables 6 and 7, respectively.
For the purpose of comparison, the word segmen-
tation performance of our system on Bakeoff-4 data
using the 2- and 4-tag sets and the best correspond-
ing n-gram feature templates as in (Tsai et al, 2006;
Low et al, 2005) are presented in Table 8.7 This
comparison reconfirms the conclusion in (Zhao et
CityU data sets in any other situation than the Bakeoff.
7The templates for the 2-tag set, adopted from (Tsai et al,
2006), include C?2, C?1, C0, C1, C?3C?1, C?2C0, C?2C?1,
C?1C0, C?1C1 and C0C1. Those for the 4-tag set, adopted
from (Xue, 2003) and (Low et al, 2005), include C?2, C?1,
C0, C1, C2, C?2C?1, C?1C0, C?1C1, C0C1and C1C2.
al., 2006b) about tag set selection for character tag-
ging for word segmentation that the 6-tag set is more
effective than others, each with its own best corre-
sponding feature template set.
Table 8: Segmentation F-scores by different tag sets
AV Tags CityU CKIP CTB NCC SXU
2 .9303 .9277 .9434 .9198 .9454
? 4 .9370 .9348 .9481 .9280 .9512
6 .9426 .9421 .9634 .9333 .9552
2 .9382 .9319 .9451 .9239 .9485
+ 4 .9482 .9423 .9527 .9356 .9593
6 .9510 .9470 .9589 .9405 .9623
4 Discussion
4.1 Tag Set and Computational Cost
Using more labels in CRFs learning is expected to
bring in performance enhancement. Inevitably, how-
ever, it also leads to a huge rise of computational
cost for model training. We conducted a series of ex-
periments to study the computational cost of CRFs
training with different tag sets using Bakeoff-3 data.
The experimental results are given in Table 9, show-
ing that the 6-tag set costs nearly twice as much time
as the 4-tag set and about three times as the 2-tag
set. Fortunately, its memory cost with the six n-gram
feature templates remains very close to that of the 2-
and 4-tag sets with the n-gram feature template sets
from (Tsai et al, 2006; Xue, 2003).
However, a 2-tag set is popular in use for word
segmentation and NER for the reason that CRFs
training is very computationally expensive and a
large tag set would make the situation worse. Cer-
109
Sixth SIGHAN Workshop on Chinese Language Processing
Table 9: Comparison of computational cost
Tags Templates AS CityU CTB MSRA
Training time (Minutes)
2 Tsai 112 52 16 35
4 Xue 206 79 28 73
6 Zhao 402 146 47 117
Feature numbers (?106)
2 Tsai 13.2 7.3 3.1 5.5
4 Xue 16.1 9.0 3.9 6.8
6 Zhao 15.6 8.8 3.8 6.6
Memory cost (Giga bytes)
2 Tsai 5.4 2.4 0.9 1.8
4 Xue 6.6 2.8 1.1 2.2
6 Zhao 6.4 2.7 1.0 2.1
tainly, a possible way out of this problem is the
computer hardware advancement, which is predicted
by Moore?s Law (Moore, 1965) to be improving at
an exponential rate in general, including processing
speed and memory capacity. Specifically, CPU can
be made twice faster every other year or even 18
months. It is predictable that computational cost will
not be a problem for CRFs training soon, and the ad-
vantages of using a larger tag set as in our approach
will be shared by more others.
4.2 Unsupervised Segmentation Features
Our evaluation results show that the unsupervised
segmentation features bring in performance im-
provement on both word segmentation and NER for
all tracks except CTB segmentation, as highlighted
in Table 6. We are unable explain this yet, and can
only attribute it to some unique text characteristics
of the CTB segmented corpus. An unsupervised seg-
mentation criterion provides a kind of global infor-
mation over the whole text of a corpus (Zhao and
Kit, 2007). Its effectiveness is certainly sensitive to
text characteristics.
Quite a number of other unsupervised segmen-
tation criteria are available for word discovery in
unlabeled texts, e.g., boundary entropy (Tung and
Lee, 1994; Chang and Su, 1997; Huang and Powers,
2003; Jin and Tanaka-Ishii, 2006) and description-
length-gain (DLG) (Kit and Wilks, 1999). We found
that among them AV could help the CRFs model to
achieve a better performance than others, although
the overall unsupervised segmentation by DLG was
slightly better than that by AV. Combining any two
of these criteria did not give any further performance
improvement. This is why we have opted for AV for
Bakeoff-4.
4.3 NE List Features for Open NER
We realize that the NE lists available to us are far
from sufficient for coping with all NEs in Bakeoff-
4. It is reasonable that using richer external NE
lists gives a better NER performance in many cases
(Zhang et al, 2006). Surprisingly, however, the NE
list features used in our NER do not lead to any sig-
nificant performance improvement, according to the
evaluation results in Table 7. This is certainly an-
other issue for our further inspection.
5 Conclusion
Without doubt our achievements in Bakeoff-4 owes
not only to the careful selection of character tag set
and feature templates for exerting the strength of
CRFs learning but also to the effectiveness of our un-
supervised segmentation approach. It is for the sake
of simplicity that similar sets of character tags and
feature templates are applied to two distinctive label-
ing tasks, word segmentation and NER. Relying on
little preprocessing and postprocessing, our system
simply follows the plain training and test routines
of machine learning practice with the CRFs model
and achieves the best or nearly the best results for all
tracks of Bakeoff-4 in which we participated. Sim-
ple is beautiful, as Albert Einstein said, ?Everything
should be made as simple as possible, but not one
bit simpler.? Our evaluation results also provide evi-
dence that simple can be powerful too.
Acknowledgements
The research described in this paper was sup-
ported by the Research Grants Council of Hong
Kong S.A.R., China, through the CERG grant
9040861 (CityU 1318/03H) and by City University
of Hong Kong through the Strategic Research Grant
7002037. Dr. Hai Zhao was supported by a Post-
doctoral Research Fellowship in the Department of
Chinese, Translation and Linguistics, City Univer-
sity of Hong Kong.
References
Jing-Shin Chang and Keh-Yih Su. 1997. An unsuper-
vised iterative method for Chinese new lexicon ex-
110
Sixth SIGHAN Workshop on Chinese Language Processing
traction. Computational Linguistics and Chinese Lan-
guage Processing, 2(2):97?148.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006.
Chinese named entity recognition with conditional
random fields. In SIGHAN-5, pages 118?121, Sydney,
Australia, July 22-23.
Thomas Emerson. 2005. The second international Chi-
nese word segmentation bakeoff. In SIGHAN-4, pages
123?133, Jeju Island, Korea, October 14-15.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004a. Accessor variety criteria for Chi-
nese word extraction. Computational Linguistics,
30(1):75?93.
Haodi Feng, Kang Chen, Chunyu Kit, and Xiaotie Deng.
2004b. Unsupervised segmentation of Chinese cor-
pus using accessor variety. In First International
Joint Conference on Natural Language Processing
(IJCNLP-04), pages 255?261, Sanya, Hainan Island,
China, March 22-24. Also in K. Y. Su, J. Tsujii, J.
H. Lee & O. Y. Kwong (eds.), Natural Language Pro-
cessing - IJCNLP 2004, LNAI 3248, pages 694-703.
Springer.
Zellig Sabbetai Harris. 1955. From phoneme to mor-
pheme. Language, 31(2):190?222.
Zellig Sabbetai Harris. 1970. Morpheme boundaries
within words. In Papers in Structural and Transfor-
mational Linguistics, page 68?77.
Jin Hu Huang and David Powers. 2003. Chinese
word segmentation based on contextual entropy. In
Dong Hong Ji and Kim-Ten Lua, editors, PACLIC -
17, pages 152?158, Sentosa, Singapore, October, 1-3.
COLIPS Publication.
Zhihui Jin and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised segmentation of Chinese text by use of branch-
ing entropy. In COLING/ACL?2006, pages 428?435,
Sidney, Australia, July 17-21.
Chunyu Kit and Yorick Wilks. 1998. The virtual corpus
approach to deriving n-gram statistics from large scale
corpora. In Changning Huang, editor, Proceedings of
1998 International Conference on Chinese Informa-
tion Processing Conference, pages 223?229, Beijing,
Nov. 18-20.
Chunyu Kit and Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. In M. Osborne and E. T. K. Sang, editors,
CoNLL-99, pages 1?6, Bergen, Norway.
Chunyu Kit and Hai Zhao. 2007. Improving Chi-
nese word segmentation with description length gain.
In 2007 International Conference on Artificial Intelli-
gence (ICAI?07), Las Vegas, June 25-28.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML?2001, pages 282?289, San Francisco, CA.
Gina-Anne Levow. 2006. The third international Chi-
nese language processing bakeoff: Word segmentation
and named entity recognition. In SIGHAN-5, pages
108?117, Sydney, Australia, July 22-23.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to Chinese word seg-
mentation. In SIGHAN-4, pages 161?164, Jeju Island,
Korea, October 14-15.
Udi Manber and Gene Myers. 1993. Suffix arrays: A
new method for on-line string searches. SIAM Journal
on Computing, 22(5):935?948.
Gordon E. Moore. 1965. Cramming more components
onto integrated circuits. Electronics, 3(8), April 19.
Richard Sproat and Thomas Emerson. 2003. The first
international Chinese word segmentation bakeoff. In
SIGHAN-2, pages 133?143, Sapporo, Japan.
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-
Lung Sung, Hong-Jie Dai, and Wen-Lian Hsu. 2006.
On closed task of Chinese word segmentation: An im-
proved CRF model coupled with character clustering
and automatically generated template matching. In
SIGHAN-5, pages 108?117, Sydney, Australia, July
22-23.
Cheng-Huang Tung and His-Jian Lee. 1994. Iden-
tification of unknown words from corpus. Compu-
tational Proceedings of Chinese and Oriental Lan-
guages, 8:131?145.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29?48.
Suxiang Zhang, Ying Qin, Juan Wen, and Xiaojie Wang.
2006. Word segmentation and named entity recog-
nition for SIGHAN Bakeoff3. In SIGHAN-5, pages
158?161, Sydney, Australia, July 22-23.
Hai Zhao and Chunyu Kit. 2007. Incorporating global
information into supervised learning for Chinese word
segmentation. In PACLING-2007, pages 66?74, Mel-
bourne, Australia, September 19-21.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a.
An improved Chinese word segmentation system with
conditional random field. In SIGHAN-5, pages 162?
165, Sydney, Australia, July 22-23.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006b. Effective tag set selection in Chinese word
segmentation via conditional random field modeling.
In PACLIC-20, pages 87?94, Wuhan, China, Novem-
ber 1-3.
111
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 55?63,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Cross Language Dependency Parsing using a Bilingual Lexicon?
Hai Zhao(??)??, Yan Song(??)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
{haizhao,yansong,ctckit}@cityu.edu.hk, gdzhou@suda.edu.cn
Abstract
This paper proposes an approach to en-
hance dependency parsing in a language
by using a translated treebank from an-
other language. A simple statistical ma-
chine translation method, word-by-word
decoding, where not a parallel corpus but
a bilingual lexicon is necessary, is adopted
for the treebank translation. Using an en-
semble method, the key information ex-
tracted from word pairs with dependency
relations in the translated text is effectively
integrated into the parser for the target lan-
guage. The proposed method is evaluated
in English and Chinese treebanks. It is
shown that a translated English treebank
helps a Chinese parser obtain a state-of-
the-art result.
1 Introduction
Although supervised learning methods bring state-
of-the-art outcome for dependency parser infer-
ring (McDonald et al, 2005; Hall et al, 2007), a
large enough data set is often required for specific
parsing accuracy according to this type of meth-
ods. However, to annotate syntactic structure, ei-
ther phrase- or dependency-based, is a costly job.
Until now, the largest treebanks1 in various lan-
guages for syntax learning are with around one
million words (or some other similar units). Lim-
ited data stand in the way of further performance
enhancement. This is the case for each individual
language at least. But, this is not the case as we
observe all treebanks in different languages as a
whole. For example, of ten treebanks for CoNLL-
2007 shared task, none includes more than 500K
?The study is partially supported by City University of
Hong Kong through the Strategic Research Grant 7002037
and 7002388. The first author is sponsored by a research fel-
lowship from CTL, City University of Hong Kong.
1It is a tradition to call an annotated syntactic corpus as
treebank in parsing community.
tokens, while the sum of tokens from all treebanks
is about two million (Nivre et al, 2007).
As different human languages or treebanks
should share something common, this makes it
possible to let dependency parsing in multiple lan-
guages be beneficial with each other. In this pa-
per, we study how to improve dependency parsing
by using (automatically) translated texts attached
with transformed dependency information. As a
case study, we consider how to enhance a Chinese
dependency parser by using a translated English
treebank. What our method relies on is not the
close relation of the chosen language pair but the
similarity of two treebanks, this is the most differ-
ent from the previous work.
Two main obstacles are supposed to confront in
a cross-language dependency parsing task. The
first is the cost of translation. Machine translation
has been shown one of the most expensive lan-
guage processing tasks, as a great deal of time and
space is required to perform this task. In addition,
a standard statistical machine translation method
based on a parallel corpus will not work effec-
tively if it is not able to find a parallel corpus that
right covers source and target treebanks. How-
ever, dependency parsing focuses on the relations
of word pairs, this allows us to use a dictionary-
based translation without assuming a parallel cor-
pus available, and the training stage of translation
may be ignored and the decoding will be quite fast
in this case. The second difficulty is that the out-
puts of translation are hardly qualified for the pars-
ing purpose. The most challenge in this aspect is
morphological preprocessing. We regard that the
morphological issue should be handled aiming at
the specific language, our solution here is to use
character-level features for a target language like
Chinese.
The rest of the paper is organized as follows.
The next section presents some related existing
work. Section 3 describes the procedure on tree-
55
bank translation and dependency transformation.
Section 4 describes a dependency parser for Chi-
nese as a baseline. Section 5 describes how a
parser can be strengthened from the translated
treebank. The experimental results are reported in
Section 6. Section 7 looks into a few issues con-
cerning the conditions that the proposed approach
is suitable for. Section 8 concludes the paper.
2 The Related Work
As this work is about exploiting extra resources to
enhance an existing parser, it is related to domain
adaption for parsing that has been draw some in-
terests in recent years. Typical domain adaptation
tasks often assume annotated data in new domain
absent or insufficient and a large scale unlabeled
data available. As unlabeled data are concerned,
semi-supervised or unsupervised methods will be
naturally adopted. In previous works, two basic
types of methods can be identified to enhance an
existing parser from additional resources. The first
is usually focus on exploiting automatic generated
labeled data from the unlabeled data (Steedman
et al, 2003; McClosky et al, 2006; Reichart and
Rappoport, 2007; Sagae and Tsujii, 2007; Chen
et al, 2008), the second is on combining super-
vised and unsupervised methods, and only unla-
beled data are considered (Smith and Eisner, 2006;
Wang and Schuurmans, 2008; Koo et al, 2008).
Our purpose in this study is to obtain a further
performance enhancement by exploiting treebanks
in other languages. This is similar to the above
first type of methods, some assistant data should
be automatically generated for the subsequent pro-
cessing. The differences are what type of data are
concerned with and how they are produced. In our
method, a machine translation method is applied
to tackle golden-standard treebank, while all the
previous works focus on the unlabeled data.
Although cross-language technique has been
used in other natural language processing tasks,
it is basically new for syntactic parsing as few
works were concerned with this issue. The rea-
son is straightforward, syntactic structure is too
complicated to be properly translated and the cost
of translation cannot be afforded in many cases.
However, we empirically find this difficulty may
be dramatically alleviated as dependencies rather
than phrases are used for syntactic structure repre-
sentation. Even the translation outputs are not so
good as the expected, a dependency parser for the
target language can effectively make use of them
by only considering the most related information
extracted from the translated text.
The basic idea to support this work is to make
use of the semantic connection between different
languages. In this sense, it is related to the work of
(Merlo et al, 2002) and (Burkett and Klein, 2008).
The former showed that complementary informa-
tion about English verbs can be extracted from
their translations in a second language (Chinese)
and the use of multilingual features improves clas-
sification performance of the English verbs. The
latter iteratively trained a model to maximize the
marginal likelihood of tree pairs, with alignments
treated as latent variables, and then jointly parsing
bilingual sentences in a translation pair. The pro-
posed parser using features from monolingual and
mutual constraints helped its log-linear model to
achieve better performance for both monolingual
parsers and machine translation system. In this
work, cross-language features will be also adopted
as the latter work. However, although it is not es-
sentially different, we only focus on dependency
parsing itself, while the parsing scheme in (Bur-
kett and Klein, 2008) based on a constituent rep-
resentation.
Among of existing works that we are aware of,
we regard that the most similar one to ours is (Ze-
man and Resnik, 2008), who adapted a parser to a
new language that is much poorer in linguistic re-
sources than the source language. However, there
are two main differences between their work and
ours. The first is that they considered a pair of suf-
ficiently related languages, Danish and Swedish,
and made full use of the similar characteristics of
two languages. Here we consider two quite dif-
ferent languages, English and Chinese. As fewer
language properties are concerned, our approach
holds the more possibility to be extended to other
language pairs than theirs. The second is that a
parallel corpus is required for their work and a
strict statistical machine translation procedure was
performed, while our approach holds a merit of
simplicity as only a bilingual lexicon is required.
3 Treebank Translation and Dependency
Transformation
3.1 Data
As a case study, this work will be conducted be-
tween the source language, English, and the tar-
get language, Chinese, namely, we will investigate
56
how a translated English treebank enhances a Chi-
nese dependency parser.
For English data, the Penn Treebank (PTB) 3
is used. The constituency structures is converted
to dependency trees by using the same rules as
(Yamada and Matsumoto, 2003) and the standard
training/development/test split is used. However,
only training corpus (sections 2-21) is used for
this study. For Chinese data, the Chinese Treebank
(CTB) version 4.0 is used in our experiments. The
same rules for conversion and the same data split
is adopted as (Wang et al, 2007): files 1-270 and
400-931 as training, 271-300 as testing and files
301-325 as development. We use the gold stan-
dard segmentation and part-of-speech (POS) tags
in both treebanks.
As a bilingual lexicon is required for our task
and none of existing lexicons are suitable for trans-
lating PTB, two lexicons, LDC Chinese-English
Translation Lexicon Version 2.0 (LDC2002L27),
and an English to Chinese lexicon in StarDict2,
are conflated, with some necessary manual exten-
sions, to cover 99% words appearing in the PTB
(the most part of the untranslated words are named
entities.). This lexicon includes 123K entries.
3.2 Translation
A word-by-word statistical machine translation
strategy is adopted to translate words attached
with the respective dependency information from
the source language to the target one. In detail, a
word-based decoding is used, which adopts a log-
linear framework as in (Och and Ney, 2002) with
only two features, translation model and language
model,
P (c|e) = exp[
?2
i=1 ?ihi(c, e)]?
c exp[
?2
i=1 ?ihi(c, e)]
Where
h1(c, e) = log(p?(c|e))
is the translation model, which is converted from
the bilingual lexicon, and
h2(c, e) = log(p?(c))
is the language model, a word trigram model
trained from the CTB. In our experiment, we set
two weights ?1 = ?2 = 1.
2StarDict is an open source dictionary software, available
at http://stardict.sourceforge.net/.
The conversion process of the source treebank
is completed by three steps as the following:
1. Bind POS tag and dependency relation of a
word with itself;
2. Translate the PTB text into Chinese word by
word. Since we use a lexicon rather than a parallel
corpus to estimate the translation probabilities, we
simply assign uniform probabilities to all transla-
tion options. Thus the decoding process is actu-
ally only determined by the language model. Sim-
ilar to the ?bag translation? experiment in (Brown
et al, 1990), the candidate target sentences made
up by a sequence of the optional target words are
ranked by the trigram language model. The output
sentence will be generated only if it is with maxi-
mum probability as follows,
c = argmax{p?(c)p?(c|e)}
= argmax p?(c)
= argmax
?
p?(wc)
A beam search algorithm is used for this process
to find the best path from all the translation op-
tions; As the training stage, especially, the most
time-consuming alignment sub-stage, is skipped,
the translation only includes a decoding procedure
that takes about 4.5 hours for about one million
words of the PTB in a 2.8GHz PC.
3. After the target sentence is generated, the at-
tached POS tags and dependency information of
each English word will also be transferred to each
corresponding Chinese word. As word order is of-
ten changed after translation, the pointer of each
dependency relationship, represented by a serial
number, should be re-calculated.
Although we try to perform an exact word-by-
word translation, this aim cannot be fully reached
in fact, as the following case is frequently encoun-
tered, multiple English words have to be translated
into one Chinese word. To solve this problem,
we use a policy that lets the output Chinese word
only inherits the attached information of the high-
est syntactic head in the original multiple English
words.
4 Dependency Parsing: Baseline
4.1 Learning Model and Features
According to (McDonald and Nivre, 2007), all
data-driven models for dependency parsing that
have been proposed in recent years can be de-
scribed as either graph-based or transition-based.
57
Table 1: Feature Notations
Notation Meaning
s The word in the top of stack
s? The first word below the top of stack.
s?1,s1... The first word before(after) the word
in the top of stack.
i, i+1,... The first (second) word in the
unprocessed sequence, etc.
dir Dependent direction
h Head
lm Leftmost child
rm Rightmost child
rn Right nearest child
form word form
pos POS tag of word
cpos1 coarse POS: the first letter of POS tag of word
cpos2 coarse POS: the first two POS tags of word
lnverb the left nearest verb
char1 The first character of a word
char2 The first two characters of a word
char?1 The last character of a word
char?2 The last two characters of a word
. ?s, i.e., ?s.dprel? means dependent label
of character in the top of stack
+ Feature combination, i.e., ?s.char+i.char?
means both s.char and i.char work as a
feature function.
Although the former will be also used as compari-
son, the latter is chosen as the main parsing frame-
work by this study for the sake of efficiency. In de-
tail, a shift-reduce method is adopted as in (Nivre,
2003), where a classifier is used to make a parsing
decision step by step. In each step, the classifier
checks a word pair, namely, s, the top of a stack
that consists of the processed words, and, i, the
first word in the (input) unprocessed sequence, to
determine if a dependent relation should be estab-
lished between them. Besides two dependency arc
building actions, a shift action and a reduce ac-
tion are also defined to maintain the stack and the
unprocessed sequence. In this work, we adopt a
left-to-right arc-eager parsing model, that means
that the parser scans the input sequence from left
to right and right dependents are attached to their
heads as soon as possible (Hall et al, 2007).
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, we
apply maximum entropy model as the learning
model for efficient training and adopting over-
lapped features as our work in (Zhao and Kit,
2008), especially, those character-level ones for
Chinese parsing. Our implementation of maxi-
mum entropy adopts L-BFGS algorithm for pa-
rameter optimization as usual.
With notations defined in Table 1, a feature set
as shown in Table 2 is adopted. Here, we explain
some terms in Tables 1 and 2. We used a large
scale feature selection approach as in (Zhao et al,
2009) to obtain the feature set in Table 2. Some
feature notations in this paper are also borrowed
from that work.
The feature curroot returns the root of a par-
tial parsing tree that includes a specified node.
The feature charseq returns a character sequence
whose members are collected from all identified
children for a specified word.
In Table 2, as for concatenating multiple sub-
strings into a feature string, there are two ways,
seq and bag. The former is to concatenate all sub-
strings without do something special. The latter
will remove all duplicated substrings, sort the rest
and concatenate all at last.
Note that we systemically use a group of
character-level features. Surprisingly, as to our
best knowledge, this is the first report on using this
type of features in Chinese dependency parsing.
Although (McDonald et al, 2005) used the pre-
fix of each word form instead of word form itself
as features, character-level features here for Chi-
nese is essentially different from that. As Chinese
is basically a character-based written language.
Character plays an important role in many means,
most characters can be formed as single-character
words, and Chinese itself is character-order free
rather than word-order free to some extent. In ad-
dition, there is often a close connection between
the meaning of a Chinese word and its first or last
character.
4.2 Parsing using a Beam Search Algorithm
In Table 2, the feature preactn returns the previous
parsing action type, and the subscript n stands for
the action order before the current action. These
are a group of Markovian features. Without this
type of features, a shift-reduce parser may directly
scan through an input sequence in linear time.
Otherwise, following the work of (Duan et al,
2007) and (Zhao, 2009), the parsing algorithm is
to search a parsing action sequence with the max-
imal probability.
Sdi = argmax
?
i
p(di|di?1di?2...),
where Sdi is the object parsing action sequence,
p(di|di?1...) is the conditional probability, and di
58
Figure 1: A comparison before and after translation
Table 2: Features for Parsing
in.form, n = 0, 1
i.form + i1.form
in.char2 + in+1.char2, n = ?1, 0
i.char?1 + i1.char?1
in.char?2 n = 0, 3
i1.char?2 + i2.char?2 +i3.char?2
i.lnverb.char?2
i3.pos
in.pos + in+1.pos, n = 0, 1
i?2.cpos1 + i?1.cpos1
i1.cpos1 + i2.cpos1 + i3.cpos1
s?2.char1
s?.char?2 + s?1.char?2
s??2.cpos2
s??1.cpos2 + s?1.cpos2
s?.cpos2 + s?1.cpos2
s?.children.cpos2.seq
s?.children.dprel.seq
s?.subtree.depth
s?.h.form + s?.rm.cpos1
s?.lm.char2 + s?.char2
s.h.children.dprel.seq
s.lm.dprel
s.char?2 + i1.char?2
s.charn + i.charn, n = ?1, 1
s?1.pos + i1.pos
s.pos + in.pos, n = ?1, 0, 1
s : i|linePath.form.bag
s?.form + i.form
s?.char2 + in.char2, n = ?1, 0, 1
s.curroot.pos + i.pos
s.curroot.char2 + i.char2
s.children.cpos2.seq + i.children.cpos2.seq
s.children.cpos2.seq + i.children.cpos2.seq
+ s.cpos2 + i.cpos2
s?.children.dprel.seq + i.children.dprel.seq
preact?1
preact?2
preact?2+preact?1
is i-th parsing action. We use a beam search algo-
rithm to find the object parsing action sequence.
5 Exploiting the Translated Treebank
As we cannot expect too much for a word-by-word
translation, only word pairs with dependency rela-
tion in translated text are extracted as useful and
reliable information. Then some features based
on a query in these word pairs according to the
current parsing state (namely, words in the cur-
rent stack and input) will be derived to enhance
the Chinese parser.
A translation sample can be seen in Figure 1.
Although most words are satisfactorily translated,
to generate effective features, what we still have to
consider at first is the inconsistence between the
translated text and the target text.
In Chinese, word lemma is always its word form
itself, this is a convenient characteristic in com-
putational linguistics and makes lemma features
unnecessary for Chinese parsing at all. However,
Chinese has a special primary processing task, i.e.,
word segmentation. Unfortunately, word defini-
tions for Chinese are not consistent in various lin-
guistical views, for example, seven segmentation
conventions for computational purpose are for-
mally proposed since the first Bakeoff3.
Note that CTB or any other Chinese treebank
has its own word segmentation guideline. Chi-
nese word should be strictly segmented according
to the guideline before POS tags and dependency
relations are annotated. However, as we say the
3Bakeoff is a Chinese processing share task held by
SIGHAN.
59
English treebank is translated into Chinese word
by word, Chinese words in the translated text are
exactly some entries from the bilingual lexicon,
they are actually irregular phrases, short sentences
or something else rather than words that follows
any existing word segmentation convention. If the
bilingual lexicon is not carefully selected or re-
fined according to the treebank where the Chinese
parser is trained from, then there will be a serious
inconsistence on word segmentation conventions
between the translated and the target treebanks.
As all concerned feature values here are calcu-
lated from the searching result in the translated
word pair list according to the current parsing
state, and a complete and exact match cannot be
always expected, our solution to the above seg-
mentation issue is using a partial matching strat-
egy based on characters that the words include.
Above all, a translated word pair list, L, is ex-
tracted from the translated treebank. Each item in
the list consists of three elements, dependant word
(dp), head word (hd) and the frequency of this pair
in the translated treebank, f .
There are two basic strategies to organize the
features derived from the translated word pair list.
The first is to find the most matching word pair
in the list and extract some properties from it,
such as the matched length, part-of-speech tags
and so on, to generate features. Note that a
matching priority serial should be defined afore-
hand in this case. The second is to check every
matching models between the current parsing state
and the partially matched word pair. In an early
version of our approach, the former was imple-
mented. However, It is proven to be quite inef-
ficient in computation. Thus we adopt the sec-
ond strategy at last. Two matching model fea-
ture functions, ?(?) and ?(?), are correspondingly
defined as follows. The return value of ?(?) or
?(?) is the logarithmic frequency of the matched
item. There are four input parameters required
by the function ?(?). Two parameters of them
are about which part of the stack(input) words is
chosen, and other two are about which part of
each item in the translated word pair is chosen.
These parameters could be set to full or charn as
shown in Table 1, where n = ...,?2,?1, 1, 2, ....
For example, a possible feature could be
?(s.full, i.char1, dp.full, hd.char1), it tries to
find a match in L by comparing stack word and
dp word, and the first character of input word
Table 3: Features based on the translated treebank
?(i.char3, s?.full, dp.char3, hd.full)+i.char3
+s?.form
?(i.char3, s.char2, dp.char3, hd.char2)+s.char2
?(i.char3, s.full, dp.char3, hd.char2)+s.form
?(s?.char?2, hd.char?2, head)+i.pos+s?.pos
?(i.char3, s.full, dp.char3, hd.char2)+s.full
?(s?.full, i.char4, dp.full, hd.char4)+s?.pos+i.pos
?(i.full, hd.char2, root)+i.pos+s.pos
?(i.full, hd.char2, root)+i.pos+s?.pos
?(s.full, dp.full, dependant)+i.pos
pairscore(s?.pos, i.pos)+s?.form+i.form
rootscore(s?.pos)+s?.form+i.form
rootscore(s?.pos)+i.pos
and the first character of hd word. If such
a match item in L is found, then ?(?) returns
log(f). There are three input parameters required
by the function ?(?). One parameter is about
which part of the stack(input) words is chosen,
and the other is about which part of each item
in the translated word pair is chosen. The third
is about the matching type that may be set to
dependant, head, or root. For example, the
function ?(i.char1, hd.full, root) tries to find a
match in L by comparing the first character of in-
put word and the whole dp word. If such a match
item in L is found, then ?(?) returns log(f) as hd
occurs as ROOT f times.
As having observed that CTB and PTB share a
similar POS guideline. A POS pair list from PTB
is also extract. Two types of features, rootscore
and pairscore are used to make use of such infor-
mation. Both of them returns the logarithmic value
of the frequency for a given dependent event. The
difference is, rootscore counts for the given POS
tag occurring as ROOT, and pairscore counts for
two POS tag combination occurring for a depen-
dent relationship.
A full adapted feature list that is derived from
the translated word pairs is in Table 3.
6 Evaluation Results
The quality of the parser is measured by the pars-
ing accuracy or the unlabeled attachment score
(UAS), i.e., the percentage of tokens with correct
head. Two types of scores are reported for compar-
ison: ?UAS without p? is the UAS score without
all punctuation tokens and ?UAS with p? is the one
with all punctuation tokens.
The results with different feature sets are in Ta-
ble 4. As the features preactn are involved, a
60
beam search algorithm with width 5 is used for
parsing, otherwise, a simple shift-reduce decoding
is used. It is observed that the features derived
from the translated text bring a significant perfor-
mance improvement as high as 1.3%.
Table 4: The results with different feature sets
features with p without p
baseline -d 0.846 0.858
+da 0.848 0.860
+Tb -d 0.859 0.869
+d 0.861 0.870
a+d: using three Markovian features preact and
beam search decoding.
b+T: using features derived from the translated text
as in Table 3.
To compare our parser to the state-of-the-art
counterparts, we use the same testing data as
(Wang et al, 2005) did, selecting the sentences
length up to 40. Table 5 shows the results achieved
by other researchers and ours (UAS with p), which
indicates that our parser outperforms any other
ones 4. However, our results is only slightly better
than that of (Chen et al, 2008) as only sentences
whose lengths are less than 40 are considered. As
our full result is much better than the latter, this
comparison indicates that our approach improves
the performance for those longer sentences.
Table 5: Comparison against the state-of-the-art
full up to 40
(McDonald and Pereira, 2006)a - 0.825
(Wang et al, 2007) - 0.866
(Chen et al, 2008) 0.852 0.884
Ours 0.861 0.889
aThis results was reported in (Wang et al, 2007).
The experimental results in (McDonald and
Nivre, 2007) show a negative impact on the pars-
ing accuracy from too long dependency relation.
For the proposed method, the improvement rela-
tive to dependency length is shown in Figure 2.
From the figure, it is seen that our method gives
observable better performance when dependency
lengths are larger than 4. Although word order is
changed, the results here show that the useful in-
formation from the translated treebank still help
those long distance dependencies.
4There is a slight exception: using the same data splitting,
(Yu et al, 2008) reported UAS without p as 0.873 versus ours,
0.870.
1 4 7 10 13 16 19
0.4
0.5
0.6
0.7
0.8
0.9
1
 Dependency Length
 
F1
basline: +d
+T: +d
Figure 2: Performance vs. dependency length
7 Discussion
If a treebank in the source language can help im-
prove parsing in the target language, then there
must be something common between these two
languages, or more precisely, these two corre-
sponding treebanks. (Zeman and Resnik, 2008)
assumed that the morphology and syntax in the
language pair should be very similar, and that is
so for the language pair that they considered, Dan-
ish and Swedish, two very close north European
languages. Thus it is somewhat surprising that
we show a translated English treebank may help
Chinese parsing, as English and Chinese even be-
long to two different language systems. However,
it will not be so strange if we recognize that PTB
and CTB share very similar guidelines on POS and
syntactics annotation. Since it will be too abstract
in discussing the details of the annotation guide-
lines, we look into the similarities of two treebanks
from the matching degree of two word pair lists.
The reason is that the effectiveness of the proposed
method actually relies on how many word pairs at
every parsing states can find their full or partial
matched partners in the translated word pair list.
Table 6 shows such a statistics on the matching
degree distribution from all training samples for
Chinese parsing. The statistics in the table suggest
that most to-be-check word pairs during parsing
have a full or partial hitting in the translated word
pair list. The latter then obtains an opportunity to
provide a great deal of useful guideline informa-
tion to help determine how the former should be
tackled. Therefore we have cause for attributing
the effectiveness of the proposed method to the
similarity of these two treebanks. From Table 6,
61
we also find that the partial matching strategy de-
fined in Section 5 plays a very important role in
improving the whole matching degree. Note that
our approach is not too related to the characteris-
tics of two languages. Our discussion here brings
an interesting issue, which difference is more im-
portant in cross language processing, between two
languages themselves or the corresponding anno-
tated corpora? This may be extensively discussed
in the future work.
Table 6: Matching degree distribution
dependant-match head-match Percent (%)
None None 9.6
None Partial 16.2
None Full 9.9
Partial None 12.4
Partial Partial 42.6
Partial Full 7.3
Full None 3.7
Full Partial 7.0
Full Full 0.2
Note that only a bilingual lexicon is adopted in
our approach. We regard it one of the most mer-
its for our approach. A lexicon is much easier to
be obtained than an annotated corpus. One of the
remained question about this work is if the bilin-
gual lexicon should be very specific for this kind
of tasks. According to our experiences, actually, it
is not so sensitive to choose a highly refined lexi-
con or not. We once found many words, mostly
named entities, were outside the lexicon. Thus
we managed to collect a named entity translation
dictionary to enhance the original one. However,
this extra effort did not receive an observable per-
formance improvement in return. Finally we re-
alize that a lexicon that can guarantee two word
pair lists highly matched is sufficient for this work,
and this requirement may be conveniently satis-
fied only if the lexicon consists of adequate high-
frequent words from the source treebank.
8 Conclusion and Future Work
We propose a method to enhance dependency
parsing in one language by using a translated tree-
bank from another language. A simple statisti-
cal machine translation technique, word-by-word
decoding, where only a bilingual lexicon is nec-
essary, is used to translate the source treebank.
As dependency parsing is concerned with the re-
lations of word pairs, only those word pairs with
dependency relations in the translated treebank are
chosen to generate some additional features to en-
hance the parser for the target language. The ex-
perimental results in English and Chinese tree-
banks show the proposed method is effective and
helps the Chinese parser in this work achieve a
state-of-the-art result.
Note that our method is evaluated in two tree-
banks with a similar annotation style and it avoids
using too many linguistic properties. Thus the
method is in the hope of being used in other simi-
larly annotated treebanks 5. For an immediate ex-
ample, we may adopt a translated Chinese tree-
bank to improve English parsing. Although there
are still something to do, the remained key work
has been as simple as considering how to deter-
mine the matching strategy for searching the trans-
lated word pair list in English according to the
framework of our method. .
Acknowledgements
We?d like to give our thanks to three anonymous
reviewers for their insightful comments, Dr. Chen
Wenliang for for helpful discussions and Mr. Liu
Jun for helping us fix a bug in our scoring pro-
gram.
References
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Computational Linguistics, 16(2):79?85.
David Burkett and Dan Klein. 2008. Two lan-
guages are better than one (for syntactic parsing). In
EMNLP-2008, pages 877?886, Honolulu, Hawaii,
USA.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchi-
moto, Yujie Zhang, and Hitoshi Isahara. 2008. De-
pendency parsing with short dependency relations
in unlabeled data. In Proceedings of IJCNLP-2008,
Hyderabad, India, January 8-10.
Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Proba-
bilistic parsing action models for multi-lingual de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
940?946, Prague, Czech, June 28-30.
Johan Hall, Jens Nilsson, Joakim Nivre,
Gu?lsen Eryig?it, Bea?ta Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
5For example, Catalan and Spanish treebanks from the
AnCora(-Es/Ca) Multilevel Annotated Corpus that are an-
notated by the Universitat de Barcelona (CLiC-UB) and the
Universitat Polit?cnica de Catalunya (UPC).
62
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933?939,
Prague, Czech, June.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT, pages 595?603,
Columbus, Ohio, USA, June.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Reranking and self-training for parser
adaptation. In Proceedings of ACL-COLING 2006,
pages 337?344, Sydney, Australia, July.
Ryan McDonald and Joakim Nivre. 2007. Charac-
terizing the errors of data-driven dependency pars-
ing models. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 122?131,
Prague, Czech, June 28-30.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81?88,
Trento, Italy, April.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of ACL-2005,
pages 91?98, Ann Arbor, Michigan, USA, June 25-
30.
Paola Merlo, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In ACL-2002,
pages 207?214, Philadelphia, Pennsylvania, USA.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The conll 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, page
915?932, Prague, Czech, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of IWPT-
2003), pages 149?160, Nancy, France, April 23-25.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL-
2002, pages 295?302, Philadelphia, USA, July.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. In Proceedings of
ACL-2007, pages 616?623, Prague, Czech Republic,
June.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency
parsing and domain adaptation with lr models and
parser ensembles. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, page
1044?1050, Prague, Czech, June 28-30.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of ACL-COLING 2006,
page 569?576, Sydney, Australia, July.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proceedings of EACL-2003, page
331?338, Budapest, Hungary, April.
Qin Iris Wang and Dale Schuurmans. 2008. Semi-
supervised convex training for dependency parsing.
In Proceedings of ACL-08: HLT, pages 532?540,
Columbus, Ohio, USA, June.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005. Strictly lexical dependency parsing. In Pro-
ceedings of IWPT-2005, pages 152?159, Vancouver,
BC, Canada, October.
Qin Iris Wang, Dekang Lin, and Dale Schuurmans.
2007. Simple training of dependency parsers via
structured boosting. In Proceedings of IJCAI 2007,
pages 1756?1762, Hyderabad, India, January.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Sta-
tistical dependency analysis with support vector
machines. In Proceedings of IWPT-2003), page
195?206, Nancy, France, April.
Kun Yu, Daisuke Kawahara, and Sadao Kurohashi.
2008. Chinese dependency parsing with large
scale automatically constructed case structures. In
Proceedings of COLING-2008, pages 1049?1056,
Manchester, UK, August.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of IJCNLP 2008 Workshop
on NLP for Less Privileged Languages, pages 35?
42, Hyderabad, India, January.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage max-
imum entropy models. In Proceeding of CoNLL-
2008, pages 203?207, Manchester, UK.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning:
A huge feature engineering method to semantic de-
pendency parsing. In Proceedings of CoNLL-2009,
Boulder, Colorado, USA.
Hai Zhao. 2009. Character-level dependencies in
chinese: Usefulness and learning. In EACL-2009,
pages 879?887, Athens, Greece.
63
 	
	   	
	   Integrating Ngram Model and Case-based Learning
For Chinese Word Segmentation
Chunyu Kit Zhiming Xu Jonathan J. Webster
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Ave., Kowloon, Hong Kong
{ctckit, ctxuzm, ctjjw}@cityu.edu.hk
Abstract
This paper presents our recent work
for participation in the First Interna-
tional Chinese Word Segmentation Bake-
off (ICWSB-1). It is based on a general-
purpose ngram model for word segmen-
tation and a case-based learning approach
to disambiguation. This system excels
in identifying in-vocabulary (IV) words,
achieving a recall of around 96-98%.
Here we present our strategies for lan-
guage model training and disambiguation
rule learning, analyze the system?s perfor-
mance, and discuss areas for further im-
provement, e.g., out-of-vocabulary (OOV)
word discovery.
1 Introduction
After about two decades of studies of Chinese word
segmentation, ICWSB-1 (henceforth, the bakeoff)
is the first effort to put different approaches and
systems to the test and comparison on common
datasets. We participated in the bakeoff with a
segmentation system that is designed to integrate a
general-purpose ngram model for probabilistic seg-
mentation and a case- or example-based learning
approach (Kit et al, 2002) for disambiguation.
The ngram model, with words extracted from
training corpora, is trained with the EM algorithm
(Dempster et al, 1977) using unsegmented train-
ing corpora. Originally it was developed to en-
hance word segmentation accuracy so as to facili-
tate Chinese-English word alignment for our ongo-
ing EBMT project, where only unsegmented texts
are available for training. It is expected to be ro-
bust enough to handle novel texts, independent of
any segmented texts for training. To simplify the
EM training, we used the uni-gram model for the
bakeoff and relied on the Viterbi algorithm (Viterbi,
1967) for the most probable segmentation, instead of
attempting to exhaust all possible segmentations of
each sentence for a complicated full version of EM
training.
The case-based learning works in a straightfor-
ward way. It first extracts case-based knowledge,
as a set of context-dependent transformation rules,
from the segmented training corpus, and then ap-
plies them to ambiguous strings in a test corpus in
terms of the similarity of their contexts. The simi-
larity is empirically computed in terms of the length
of relevant common affixes of context strings.
The effectiveness of this integrated approach is
verified by its outstanding performance on IV word
identification. Its IV recall rate, ranging from 96%
to 98%, stands at the top or the next to the top in all
closed tests in which we have participated. Unfortu-
nately, its overall performance is not sustainable at
the same level, due to the lack of a module for OOV
word detection.
This paper is intended to present the implementa-
tion of the system and analyze its performance and
problems, aiming at exploration of directions for fur-
ther improvement. The remaining sections are or-
ganized as follows. Section 2 presents the ngram
model and its training with the EM algorithm, and
Section 3 presents the case-based learning for dis-
ambiguation. The overall architecture of our system
is given in Section 4, and its performance and prob-
lems are analyzed in Section 5. Section 6 concludes
the paper and previews future work.
2 Ngram model and training
An ngram model can be utilized to find the most
probable segmentation of a sentence. Given a Chi-
nese sentence s = c1c2 ? ? ? cm (also denoted as cn1 ),
its probabilistic segmentation into a word sequence
w1w2 ? ? ?wk (also denoted as wk1 ) with the aid of an
ngram model can be formulated as
seg(s) = arg max
s= w1?w2?????wk
k
?
i
p(wi|wi?1i?n+1) (1)
where ? denotes string concatenation, wi?1i?n+1 the
context (or history) of wi, and n is the order of the
ngram model in use. We have opted for uni-gram for
the sake of simplicity. Accordingly, p(wi|wi?1i?n+1)
in (1) becomes p(wi), which is commonly estimated
as follows, given a corpus C for training.
p(wi) .= f(wi)/
?
w?C
f(w) (2)
In order to estimate a reliable p(wi), the ngram
model needs to be trained with the EM algorithm
using the available training corpus. Each EM itera-
tion aims at approaching to a more reliable f(w) for
estimating p(w), as follows:
fk+1(w) =
?
s?C
?
s??S(s)
pk(s?) fk(w ? s?) (3)
where k denotes the current iteration, S(s) the set of
all possible segmentations for s, and f k(w ? s?) the
occurrences of w in a particular segmentation s?.
However, assuming that every sentence always
has a segmentation, the following equation holds:
?
s??S(s)
pk(s?) = 1 (4)
Accordingly, we can adjust (3) as (5) with a normal-
ization factor ? = ?s??S(s) pk(s?), to avoid favor-
ing words in shorter sentences too much. In general,
shorter sentences have higher probabilities.
fk+1(w) =
?
s?C
?
s??S(s)
pk(s?)
? f
k(w ? s?) (5)
Following the conventional idea to speed up the
EM training, we turned to the Viterbi algorithm. The
underlying philosophy is to distribute more prob-
ability to more probable events. The Viterbi seg-
mentation, by utilizing dynamic programming tech-
niques to go through the word trellis of a sentence
efficiently, finds the most probable segmentation un-
der the current parameter estimation of the language
model, fulfilling (1)). Accordingly, (6) becomes
fk+1(w) =
?
s?C
pk(seg(s)) fk(w ? seg(s)) (6)
and (5) becomes
fk+1(w) =
?
s?C
fk(w ? seg(s)) (7)
where the normalization factor is skipped, for
only the Viterbi segmentation is used for EM re-
estimation. Equation (7) makes the EM training
with the Viterbi algorithm very simple for the uni-
gram model: iterate word segmentation, as (1), and
word count updating, via (7), sentence by sentence
through the training corpus until there is a conver-
gence.
Since the EM algorithm converges to a local max-
ima only, it is critical to start the training with an
initial f 0(w) for each word not too far away from its
?true? value. Our strategy for initializing f 0(w) is
to assume all possible words in the training corpus
as equiprobable and count each of them as 1; and
then p0(w) is derived using (2). This strategy is sup-
posed to have a weaker bias to favor longer words
than maximal matching segmentation.
For the bakeoff, the ngram model is trained with
the unsegmented training corpora together with the
test sets. It is a kind of unsupervised training.
Adding the test set to the training data is reasonable,
to allow the model to have necessary adaptation to-
wards the test sets. Experiments show that the train-
ing converges very fast, and the segmentation per-
formance improves significantly from iteration to it-
eration. For the bakeoff experiments, we carried out
the training in 6 iterations, because more iterations
than this have not been observed to bring any signif-
icant improvement on segmentation accuracy to the
training sets.
3 Case-based learning for disambiguation
No matter how well the language model is trained,
probabilistic segmentation cannot avoid mistakes on
ambiguous strings, although it resolves most ambi-
guities by virtue of probability. For the remaining
unresolved ambiguities, however, we have to resort
to other strategies and/or resources. Our recent study
(Kit et al, 2002) shows that case-based learning is
an effective approach to disambiguation.
The basic idea behind the case-based learning is
to utilize existing resolutions for known ambiguous
strings to do disambiguation if similar ambiguities
occur again. This learning strategy can be imple-
mented in two straightforward steps:
1. Collection of correct answers from the train-
ing corpus for ambiguous strings together with
their contexts, resulting in a set of context-
dependent transformation rules;
2. Application of appropriate rules to ambiguous
strings.
A transformation rule of this type is actually an ex-
ample of segmentation, indicating how an ambigu-
ous string is segmented within a particular context.
It has the following general form:
C l? Cr : ? ? w1 w2 ? ? ?wk
where ? is the ambiguous string, C l and Cr its left
and right contexts, respectively, and w1 w2 ? ? ?wk
the correct segmentation of ? given the contexts.
In our implementation, we set the context length on
each side to two words.
For a particular ambiguity, the example with the
most similar context in the example (or, rule) base
is applied. The similarity is measured by the sum
of the length of the common suffix and prefix of,
respectively, the left and right contexts. The details
of computing this similarity can be found in (Kit et
al., 2002) . If no rule is applicable, its probabilistic
segmentation is retained.
For the bakeoff, we have based our approach to
ambiguity detection and disambiguation rule extrac-
tion on the assumption that only ambiguous strings
cause mistakes: we detect the discrepancies of our
probabilistic segmentation and the standard segmen-
tation of the training corpus, and turn them into
transformation rules. An advantage of this approach
is that the rules so derived carry out not only disam-
biguation but also error correction. This links our
disambiguation strategy to the application of Brill?s
(1993) transformation-based error-driven learning to
Chinese word segmentation (Palmer, 1997; Hocken-
maier and Brew, 1998).
4 System architecture
The overall architecture of our word segmentation
system is presented in Figure 1.
Figure 1: Overall architecture of the system
5 Performance and analysis
The performance of our system in the bakeoff is pre-
sented in Table 1 in terms of precision (P), recall
(R) and F score in percentages, where ?c? denotes
closed tests. Its IV word identification performance
is remarkable.
However, its overall performance is not in bal-
ance with this, due to the lack of a module for OOV
word discovery. It only gets a small number of OOV
words correct by chance. The higher OOV propor-
tion in the test set, the worse is its F score. The rel-
atively high Roov for PKc track is, mostly, the result
of number recognition with regular expressions.
Test P R F OOV Roov Riv
SAc 95.2 93.1 94.2 02.2 04.3 97.2
CTBc 80.0 67.4 73.2 18.1 07.6 95.9
PKc 92.3 86.7 89.4 06.9 15.9 98.0
Table 1: System performance, in percentages (%)
5.1 Error analysis
Most errors on IV words are due to the side-effect
of the context-dependent transformation rules. The
rules resolve most remaining ambiguities and cor-
rect many errors, but at the same time they also cor-
rupt some proper segmentations. This side-effect is
most likely to occur when there is inadequate con-
text information to decide which rules to apply.
There are two strategies to remedy, or at least al-
leviate, this side-effect: (1) retrain probabilistic seg-
mentation ? a conservative strategy; or, (2) incorpo-
rate Brill?s error-driven learning with several rounds
of transformation rule extraction and application, al-
lowing mistakes caused by some rules in previous
rounds to be corrected by other rules in later rounds.
However, even worse than the above side-effect is
a bug in our disambiguation module: it always ap-
plies the first available rule, leading to many unex-
pected errors, each of which may result in more than
one erroneous word. For instance, among 430 er-
rors made by the system in the SA closed test, some
70 are due to this bug. A number of representative
examples of these errors are presented in Table 2,
together with some false errors resulting from the
inconsistency in the standard segmentation.
Errors Standard False errors Standard
2? (8) 2 ? ???D ? ? ?D
? u (7) ?u tjs? tj s?
. ? (7) .? ???? ?? ??
_ A (5) _A P_,? P_ ,?
?  (4) ? 1w?? 1w? ?
n? (4) n ? .??? . ? ? ?
Table 2: Errors and false errors
6 Conclusion and future work
We have presented our recent work for partici-
pation in ICWSB-1 based on a general-purpose
ngram model for probabilistic word segmentation
and a case-based learning strategy for disambigua-
tion. The ngram model is trained using available
unsegmented texts with the EM algorithm with the
aid of Viterbi segmentation. The learning strategy
acquires a set of context-dependent transformation
rules to correct mistakes in the probabilistic segmen-
tation of ambiguous substrings. This integrated ap-
proach demonstrates an impressive effectiveness by
its outstanding performance on IV word identifica-
tion. With elimination of the bug and false errors, its
performance could be significantly better.
6.1 Future work
The above problem analysis points to two main di-
rections for improvement in our future work: (1)
OOV word detection; (2) a better strategy for learn-
ing and applying transformation rules to reduce the
side-effect. In addition, we are also interested in
studying the effectiveness of higher-order ngram
models and variants of EM training for Chinese
word segmentation.
Acknowledgements
The work is part of the CERG project ?EBMT for
HK Legal Texts? funded by HK UGC under the
grant #9040482, with Jonathan J. Webster as the
principal investigator and Chunyu Kit, Caesar S.
Lun, Haihua Pan, King Kuai Sin and Vincent Wong
as investigators. The authors wish to thank all team
members for their contribution to this paper.
References
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
A. P. Dempster, N. M. Laird, and D. B.Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 34:1?38.
J. Hockenmaier and C. Brew. 1998. Error-driven learn-
ing of Chinese word segmentation. In PACLIC-12,
pages 218?229, Singapore. Chinese and Oriental Lan-
guages Processing Society.
C. Kit, H. Pan, and H. Chen. 2002. Learning case-based
knowledge for disambiguating Chinese word segmen-
tation: A preliminary study. In COLING2002 work-
shop: SIGHAN-1, pages 33?39, Taipei.
D. Palmer. 1997. A trainable rule-based algorithm
for word segmentation. In ACL-97, pages 321?328,
Madrid.
A. J. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, IT-13:260?
267.
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 55?60,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Multilingual Dependency Learning:
A Huge Feature Engineering Method to Semantic Dependency Parsing ?
Hai Zhao(??)??, Wenliang Chen(???)?, Chunyu Kit?, Guodong Zhou?
?Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
?Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
?School of Computer Science and Technology
Soochow University, Suzhou, China 215006
haizhao@cityu.edu.hk, chenwl@nict.go.jp
Abstract
This paper describes our system about mul-
tilingual semantic dependency parsing (SR-
Lonly) for our participation in the shared task
of CoNLL-2009. We illustrate that semantic
dependency parsing can be transformed into
a word-pair classification problem and im-
plemented as a single-stage machine learning
system. For each input corpus, a large scale
feature engineering is conducted to select the
best fit feature template set incorporated with a
proper argument pruning strategy. The system
achieved the top average score in the closed
challenge: 80.47% semantic labeled F1 for the
average score.
1 Introduction
The syntactic and semantic dependency parsing in
multiple languages introduced by the shared task
of CoNLL-2009 is an extension of the CoNLL-
2008 shared task (Hajic? et al, 2009). Seven lan-
guages, English plus Catalan, Chinese, Czech, Ger-
man, Japanese and Spanish, are involved (Taule? et
al., 2008; Palmer and Xue, 2009; Hajic? et al, 2006;
Surdeanu et al, 2008; Burchardt et al, 2006; Kawa-
hara et al, 2002). This paper presents our research
for participation in the semantic-only (SRLonly)
challenge of the CoNLL-2009 shared task, with a
?This study is partially supported by CERG grant 9040861
(CityU 1318/03H), CityU Strategic Research Grant 7002037,
Projects 60673041 and 60873041 under the National Natural
Science Foundation of China and Project 2006AA01Z147 un-
der the ?863? National High-Tech Research and Development
of China.
highlight on our strategy to select features from a
large candidate set for maximum entropy learning.
2 System Survey
We opt for the maximum entropy model with Gaus-
sian prior as our learning model for all classification
subtasks in the shared task. Our implementation of
the model adopts L-BFGS algorithm for parameter
optimization as usual. No additional feature selec-
tion techniques are applied.
Our system is basically improved from its early
version for CoNLL-2008 (Zhao and Kit, 2008). By
introducing a virtual root for every predicates, The
job to determine both argument labels and predicate
senses is formulated as a word-pair classification
task in four languages, namely, Catalan, Spanish,
Czech and Japanese. In other three languages, Chi-
nese, English and German, a predicate sense clas-
sifier is individually trained before argument label
classification. Note that traditionally (or you may
say that most semantic parsing systems did so) ar-
gument identification and classification are handled
in a two-stage pipeline, while ours always tackles
them in one step, in addition, predicate sense classi-
fication are also included in this unique learning/test
step for four of all languages.
3 Pruning Argument Candidates
We keep using a word-pair classification procedure
to formulate semantic dependency parsing. Specif-
ically, we specify the first word in a word pair as a
predicate candidate (i.e., a semantic head, and noted
as p in our feature representation) and the next as an
argument candidate (i.e., a semantic dependent, and
55
noted as a). We do not differentiate between verbal
and non-verbal predicates and our system handles
them in the exactly same way.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly. As predicates overtly
known in the share task, we only consider how to
effectively prune argument candidates.
We adopt five types of argument pruning strate-
gies for seven languages. All of them assume that a
syntactic dependency parsing tree is available.
As for Chinese and English, we continue to use
a dependency version of the pruning algorithm of
(Xue and Palmer, 2004) as described in (Zhao and
Kit, 2008). The pruning algorithm is readdressed as
the following.
Initialization: Set the given predicate candidate
as the current node;
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head and
repeat step (1) until the root is reached.
Note that the given predicate candidate itself is
excluded from the argument candidate list for Chi-
nese, that is slightly different from English.
The above pruning algorithm has been shown ef-
fective. However, it is still inefficient for a single-
stage argument identification/classification classifi-
cation task. Thus we introduce an assistant argument
label ? NoMoreArgument? to alleviate this difficulty.
If an argument candidate in the above algorithm is
labeled as such a label, then the pruning algorithm
will end immediately. In training, this assistant label
means no more samples will be generated for the
current predicate, while in test, the decoder will not
search more argument candidates any more. This
adaptive technique more effectively prunes the ar-
gument candidates. In fact, our experiments show
1/3 training memory and time may be saved from it.
As for Catalan and Spanish, only syntactic chil-
dren of the predicate are considered as the argument
candidates.
As for Czech, only syntactic children, grandchil-
dren, great-grandchildren, parent and siblings of the
predicate are taken as the argument candidates.
As for German, only syntactic children, grand-
children, parent, siblings, siblings of parent and sib-
lings of grandparent of the predicate are taken as the
argument candidates.
The case is somewhat sophisticated for Japanese.
As we cannot identify a group of simple predicate-
argument relations from the syntactic tree. Thus
we consider top frequent 28 syntactic relations be-
tween the predicate and the argument. The parser
will search all words before and after the predicate,
and only those words that hold one of the 28 syn-
tactic relations to the predicate are considered as
the argument candidate. Similar to the pruning al-
gorithm for Chinese/English/German, we also in-
troduce two assistant labels ? leftNoMoreArgument?
and ? rightNoMoreArgument? to adaptively prune
words too far away from the predicate.
4 Feature Templates
As we don?t think that we can benefit from know-
ing seven languages, an automatic feature template
selection is conducted for each language.
About 1000 feature templates (hereafter this tem-
plate set is referred to FT ) are initially considered.
These feature templates are from various combina-
tions or integrations of the following basic elements.
Word Property. This type of elements include
word form, lemma, part-of-speech tag (PoS), FEAT
(additional morphological features), syntactic de-
pendency label (dprel), semantic dependency label
(semdprel) and characters (char) in the word form
(only suitable for Chinese and Japanese)1.
Syntactic Connection. This includes syntactic
head (h), left(right) farthest(nearest) child (lm, ln,
rm, and rn), and high(low) support verb or noun.
We explain the last item, support verb(noun). From
the predicate or the argument to the syntactic root
along the syntactic tree, the first verb(noun) that is
met is called as the low support verb(noun), and the
nearest one to the root is called as the high support
verb(noun).
Semantic Connection. This includes semantic
1All lemmas, PoS, and FEAT for either training or test are
from automatically pre-analyzed columns of every input files.
56
FEATn 1 2 3 4 5 6 7 8 9 10 11
Catalan/Spanish postype gen num person mood tense punct
Czech SubPOS Gen Num Cas Neg Gra Voi Var Sem Per Ten
Table 1: Notations of FEATs
head (semhead), left(right) farthest(nearest) seman-
tic child (semlm, semln, semrm, semrn). We say
a predicate is its argument?s semantic head, and the
latter is the former?s child. Features related to this
type may track the current semantic parsing status.
Path. There are two basic types of path between
the predicate and the argument candidates. One is
the linear path (linePath) in the sequence, the other
is the path in the syntactic parsing tree (dpPath). For
the latter, we further divide it into four sub-types
by considering the syntactic root, dpPath is the full
path in the syntactic tree. Leading two paths to the
root from the predicate and the argument, respec-
tively, the common part of these two paths will be
dpPathShare. Assume that dpPathShare starts from
a node r?, then dpPathPred is from the predicate to
r?, and dpPathArgu is from the argument to r?.
Family. Two types of children sets for the predi-
cate or argument candidate are considered, the first
includes all syntactic children (children), the second
also includes all but excludes the left most and the
right most children (noFarChildren).
Concatenation of Elements. For all collected el-
ements according to linePath, children and so on, we
use three strategies to concatenate all those strings
to produce the feature value. The first is seq, which
concatenates all collected strings without doing any-
thing. The second is bag, which removes all dupli-
cated strings and sort the rest. The third is noDup,
which removes all duplicated neighbored strings.
In the following, we show some feature template
examples derived from the above mentioned items.
a.lm.lemma The lemma of the left most child of
the argument candidate.
p.h.dprel The dependant label of the syntactic
head of the predicate candidate.
a.pos+p.pos The concatenation of PoS of the ar-
gument and the predicate candidates.
p?1.pos+p.pos PoS of the previous word of the
predicate and PoS of the predicate itself.
a:p|dpPath.lemma.bag Collect all lemmas along
the syntactic tree path from the argument to the pred-
icate, then removed all duplicated ones and sort the
rest, finally concatenate all as a feature string.
a:p.highSupportNoun|linePath.dprel.seq Collect
all dependant labels along the line path from the ar-
gument to the high support noun of the predicate,
then concatenate all as a feature string.
(a:p|dpPath.dprel.seq)+p.FEAT1 Collect all de-
pendant labels along the line path from the argument
to the predicate and concatenate them plus the first
FEAT of the predicate.
An important feature for the task is dpTreeRela-
tion, which returns the relationship of a and p in a
syntactic parse tree and cannot be derived from com-
bining the above basic elements. The possible values
for this feature include parent, sibling etc.
5 Automatically Discovered Feature
Template Sets
For each language, starting from a basic feature tem-
plate set (a small subset of FT ) according to our
previous result in English dependency parsing, each
feature template outside the basic set is added and
each feature template inside the basic set is removed
one by one to check the effectiveness of each fea-
ture template following the performance change in
the development set. This procedure will be contin-
uously repeated until no feature template is added or
removed or the performance is not improved.
There are some obvious heuristic rules that help
us avoid trivial feature template checking, for ex-
ample, FEAT features are only suitable for Cata-
lan, Czech and Spanish. Though FEAT features are
also available for Japanese, we don?t adopt them for
this language due to the hight training cost. To sim-
plify feature representation, we use FEAT1, FEAT2,
and so on to represent different FEAT for every lan-
guages. A lookup list can be found in Table 1. Ac-
cording to the list, FEAT4 represents person for
Catalan or Spanish, but Cas for Czech.
As we don?t manually interfere the selection pro-
cedure for feature templates, ten quite different fea-
57
Ca Ch Cz En Gr Jp Sp
Ca 53
Ch 5 75
Cz 11 10 76
En 11 11 12 73
Gr 7 7 7 14 45
Jp 6 22 13 15 10 96
Sp 22 9 18 15 9 12 66
Table 2: Feature template set: argument classifier
Ch En Gr
Ch 46
En 5 9
Gr 17 2 40
Table 3: Feature template set: sense classifier
ture template sets are obtained at last. Statistical in-
formation of seven sets for argument classifiers is in
Table 2, and those for sense classifiers are in Table 3.
Numbers in the diagonals of these two tables mean
the numbers of feature templates, and others mean
how many feature templates are identical for every
language pairs. The most matched feature template
sets are for Catalan/Spanish and Chinese/Japanese.
As for the former, it is not so surprised because these
two corpora are from the same provider.
Besides the above statistics, these seven feature
template sets actually share little in common. For
example, the intersection set from six languages, as
Chinese is excluded, only includes one feature tem-
plate, p.lemma (the lemma of the predicate candi-
date). If all seven sets are involved, then such an in-
tersection set will be empty. Does this mean human
languages share little in semantic representation? :)
It is unlikely to completely demonstrate full fea-
ture template sets for all languages in this short re-
port, we thus only demonstrate two sets, one for En-
glish sense classification in Table 4 and the other for
Catalan argument classification in Table 52.
6 Word Sense Determination
The shared task of CoNLL-2009 still asks for the
predicate sense. In our work for CoNLL-2008 (Zhao
and Kit, 2008), this was done by searching for a right
2Full feature lists and their explanation for all languages will
be available at the website, http://bcmi.sjtu.edu.cn/?zhaohai.
p.lm.pos
p.rm.pos
p.lemma
p.lemma + p.lemma1
p.lemma + p.children.dprel.noDup
p.lemma + p.currentSense
p.form
p.form?1 + p.form
p.form + p.form1
Table 4: Feature set for English sense classification
example in the given dictionary. Unfortunately, we
late found this caused a poor performance in sense
determination. This time, an individual classifier is
used to determine the sense for Chinese, English or
German, and this is done by the argument classifier
by introducing a virtual root for every predicates for
the rest four languages3. Features used for sense
determination are also selected following the same
procedure in Section 5. The difference is only pred-
icate related features are used for selection.
7 Decoding
The decoding for four languages, Catalan, Czech,
Japanese and Spanish is trivial, each word pairs will
be checked one by one. The first word of the pair
is the virtual root or the predicate, the second is the
predicate or every argument candidates. Argument
candidates are checked in the order of different syn-
tactic relations to their predicate, which are enumer-
ated by the pruning algorithms in Section 3, or from
left to right for the same syntactic relation. After
the sense of the predicate is determined, the label of
each argument candidate will be directly classified,
or, it is proved non-argument.
As for the rest languages, Chinese, English or
German, after the sense classifier outputs its result,
an optimal argument structure for each predicate is
determined by the following maximal probability.
Sp = argmax
?
i
P (ai|ai?1, ai?2, ...), (1)
where Sp is the argument structure, P (ai|ai?1...)
is the conditional probability to determine the la-
bel of the i-th argument candidate label. Note that
3For Japanese, no senses for predicates are defined. Thus it
is actually a trivial classification task in this case.
58
p.currentSense + p.lemma
p.currentSense + p.pos
p.currentSense + a.pos
p?1.FEAT1
p.FEAT2
p1.FEAT3
p.semrm.semdprel
p.lm.dprel
p.form + p.children.dprel.bag
p.lemman (n = ?1, 0)
p.lemma + p.lemma1
p.pos?1 + p.pos
p.pos1
p.pos + p.children.dprel.bag
a.FEAT1 + a.FEAT3 + a.FEAT4
+ a.FEAT5 + a.FEAT6
a?1.FEAT2 + a.FEAT2
a.FEAT3 + a1.FEAT3
a.FEAT3 + a.h.FEAT3
a.children.FEAT1.noDup
a.children.FEAT3.bag
a.h.lemma
a.lm.dprel + a.form
a.lm.form
a.lm?1.lemma
a.lmn.pos (n=0,1)
a.noFarChildren.pos.bag + a.rm.form
a.pphead.lemma
a.rm.dprel + a.form
a.rm?1.form
a.rm.lemma
a.rn.dprel + a.form
a.lowSupportVerb.lemma
a?1.form
a.form + a1.form
a.form + a.children.pos
a.lemma + a.h.form
a.lemma + a.pphead.form
a1.lemma
a1.pos + a.pos.seq
a.pos + a.children.dprel.bag
a.lemma + p.lemma
(a:p|dpPath.dprel) + p.FEAT1
a:p|linePath.distance
a:p|linePath.FEAT1.bag
a:p|linePath.form.seq
a:p|linePath.lemma.seq
a:p|linePath.dprel.seq
a:p|dpPath.lemma.seq
a:p|dpPath.lemma.bag
a:p|dpPathArgu.lemma.seq
a:p|dpPathArgu.lemma.bag
Table 5: Feature set for Catalan argument classification
P (ai|ai?1, ...) in equation (1) may be simplified as
P (ai) if the input feature template set does not con-
cerned with the previous argument label output. A
beam search algorithm is used to find the parsing de-
cision sequence.
8 Evaluation Results
Our evaluation is carried out on two computational
servers, (1) LEGA, a 64-bit ubuntu Linux installed
server with double dual-core AMD Opteron proces-
sors of 2.8GHz and 24GB memory. This server was
also used for our previous participation in CoNLL-
2008 shared task. (2) MEGA, a 64-bit ubuntu Linux
installed server with six quad-core Intel Xeon pro-
cessors of 2.33GHz and 128GB memory.
Altogether nearly 60,000 machine learning rou-
tines were run to select the best fit feature template
sets for all seven languages within two months. Both
LEGA and MEGA were used for this task. How-
ever, training and test for the final submission of
Chinese, Czech and English run in MEGA, and the
rest in LEGA. As we used multiple thread training
and multiple routines run at the same time, the exact
time cost for either training or test is hard to esti-
mate. Here we just report the actual time and mem-
ory cost in Table 7 for reference.
The official evaluation results of our system are in
Table 6. Numbers in bold in the table stand for the
best performances for the specific languages. The
results in development sets are also given. The first
row of the table reports the results using golden in-
put features.
Two facts as the following suggest that our system
does output robust and stable results. The first is that
two results for development and test sets in the same
language are quite close. The second is about out-of-
domain (OOD) task. Though for each OOD task, we
just used the same model trained from the respective
language and did nothing to strengthen it, this does
not hinder our system to obtain top results in Czech
and English OOD tasks.
In addition, the feature template sets from auto-
matical selection procedure in this task were used
for the joint task of this shared task, and also output
top results according to the average score of seman-
tic labeled F1 (Zhao et al, 2009).
59
average Catalan Chinese Czech English German Japanese Spanish
Development with Gold 81.24 81.52 78.32 86.96 84.19 77.75 78.67 81.32
Development 80.46 80.66 77.90 85.35 84.01 76.55 78.41 80.39
Test (official scores) 80.47 80.32 77.72 85.19 85.44 75.99 78.15 80.46
Out-of-domain 74.34 85.44 73.31 64.26
Table 6: Semantic labeled F1
Catalan Chinese Czech English German Japanese Spanish
Sense Training memory (MB) 418.0 136.0 63.0
Training time (Min.) 11.0 2.5 1.7
Test time (Min.) 0.7 0.2 0.03
Argument Training memory (GB) 0.4 3.7 3.2 3.8 0.2 1.4 0.4
Training time (Hours) 3.0 13.8 24.9 12.4 0.2 6.1 4.4
Test time (Min.) 3.0 144.0 27.1 88.0 1.0 4.2 7.0
Table 7: Computational cost
9 Conclusion
As presented in the above sections, we have tackled
semantic parsing for the CoNLL-2009 shared task
as a word-pair classification problem. Incorporated
with a proper argument candidate pruning strategy
and a large scale feature engineering for each lan-
guage, our system produced top results.
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado?, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, Marie
Mikulova?, and Zdene?k Z?abokrtsky?. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013, Las Palmas, Canary
Islands.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143?172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008).
Mariona Taule?, Maria Anto`nia Mart??, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating fea-
tures for semantic role labeling. In 2004 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2004), pages 88?94, Barcelona, Spain,
July 25-26.
Hai Zhao and Chunyu Kit. 2008. Parsing syntac-
tic and semantic dependencies with two single-stage
maximum entropy models. In Twelfth Conference on
Computational Natural Language Learning (CoNLL-
2008), pages 203?207, Manchester, UK, August 16-
17.
Hai Zhao, Wenliang Chen, Jun?ichi Kazama, Kiyotaka
Uchimoto, and Kentaro Torisawa. 2009. Multilin-
gual dependency learning: Exploiting rich features for
tagging syntactic and semantic dependencies. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
60
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 57?60,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Transliteration of Name Entity via Improved Statistical Translation on
Character Sequences
Yan Song Chunyu Kit Xiao Chen
Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Ave., Kowloon, Hong Kong
Email: {yansong, ctckit}@cityu.edu.hk, cxiao2@student.cityu.edu.hk
Abstract
Transliteration of given parallel name en-
tities can be formulated as a phrase-based
statistical machine translation (SMT) pro-
cess, via its routine procedure compris-
ing training, optimization and decoding.
In this paper, we present our approach to
transliterating name entities using the log-
linear phrase-based SMT on character se-
quences. Our proposed work improves the
translation by using bidirectional models,
plus some heuristic guidance integrated in
the decoding process. Our evaluated re-
sults indicate that this approach performs
well in all standard runs in the NEWS2009
Machine Transliteration Shared Task.
1 Introduction
To transliterate a foreign name into a target lan-
guage, a direct instrument is to make use of ex-
isting rules for converting text to syllabus, or
at least a phoneme base to support such trans-
formation. Following this path, the well devel-
oped noisy channel model used for transliteration
usually set an intermediate layer to represent the
source and target names by phonemes or phonetic
tags (Knight and Graehl, 1998; Virga and Khu-
danpur, 2003; Gao et al, 2004). Having been
studied extensively though, the phonemes-based
approaches cannot break its performance ceiling
for two reasons (Li et al, 2004): (1) Language-
dependent phoneme representation is not easy to
obtain; (2) The phonemic representation to source
and target names usually causes error spread.
Several approaches have been proposed for di-
rect use of parallel texts for performance enhance-
ment (Li et al, 2004; Li et al, 2007; Gold-
wasser and Roth, 2008). There is no straight-
forward mean for grouping characters or letters in
the source or target language into better transliter-
ation units for a better correspondence. There is
no consistent deterministic mapping between two
languages either, especially when they belong to
different language families, such as English and
Chinese. Usually, a single character in a source
name is not enough to form a phonetic pattern
in a target name. Thus a better way to model
transliteration is to map character sequences be-
tween source and target name entities. The map-
ping is actually an alignment process. If a cer-
tain quantity of bilingual transliterated entities are
available for training, it is a straight-forward idea
to tackle this transliteration problem with a ma-
ture framework such as phrase-based SMT. It can
be considered a general statistical translation task
if the character sequences involved are treated like
phrases.
In so doing, however, a few points need to be
highlighted. Firstly, only parallel data are required
for generating transliteration outputs via SMT, and
this SMT translation process can be easily in-
tegrated as a component into a general-purpose
SMT system. Secondly, on character sequences,
the mapping between source and target name en-
tities can be performed on even larger units. Con-
sequently, contextual information can be exploited
to facilitate the alignment, for a string can be used
as a context for every one of its own characters.
It is reasonable to expect such relevant informa-
tion to produce more precisely statistical results
for finding corresponding transliterations. Thirdly,
transliteration as a monotonic word ordering trans-
formation problem allows the alignment to be per-
formed monotonously from the beginning to the
end of a text. Thus its decoding is easy to perform
as its search space shrinks this way, for re-ordering
is considered not to be involved, in contrast to the
general SMT process.
This paper is intended to present our work
on applying phrased-based SMT technologies to
tackle transliteration. The following sections will
report how we have carried out our experiments
57
for the NEWS2009 task (Li et al, 2009) and
present the experimented results.
2 Transliteration as SMT
In order to transliterate effectively via a phrase
based SMT process for our transliteration task, we
opt for the log-linear framework (Och and Ney,
2002), a straight-forward architecture to have sev-
eral feature models integrated together as
P (t|s) = exp[
?n
i=1 ?ihi(s, t)]?
t exp[
?n
i=1 ?ihi(s, t)]
(1)
Then the transliteration task is to find the proper
source and corresponding target chunks to maxi-
mize P (t|s) as
t = argmax
t
P (t|s) (2)
In (1), hi(s, t) is a feature model formulated as a
probability functions on a pair of source and target
texts in logarithmic form, and ?i is a parameter to
optimize its contribution. The two most important
models in this framework are the translation model
(i.e., the transliteration model in our case), and the
target language model. The former is defined as
hi(s, t) = log p(s, t) (3)
where p(s, t) is p(s|t) or p(t|s) according to the
direction of training corresponding phrases. (Och
and Ney, 2002) show that p(t|s) gives a result
comparable to p(s|t), as in the source-channel
framework. (Gao et al, 2004) also confirm on
transliteration that the direct model with p(t|s)
performs well while working on the phonemic
level. For our task, we have tested these choices
for p(s, t) on all our development data, arriving
at a similar result. However, we opt to use both
p(s|t) and p(t|s) if they give similar transliter-
ation quality in some language pairs. Thus we
take p(t|s) for our primary transliteration model
for searching candidate corresponding character
sequences, and p(s|t) as a supplement.
In addition to the translation model feature, an-
other feature for the language model can be de-
scribed as
hi(s, t) = log p(t) (4)
Usually the n-gram language model is used for its
effectiveness and simplicity.
2.1 Training
For the purpose of modeling the training data, the
characters from both the source and target name
entities for training are split up for alignment, and
then phrase extraction is conducted to find the
mapping pairs of character sequence.
The alignment is performed by expectation-
maximization (EM) iterations in the IBM model-4
SMT training using the GIZA++ toolkit1. In some
runs, however, e.g., English to Chinese and En-
glish to Korean transliteration, the character num-
ber of the source text is always more than that
of the target text, the training conducted only on
characters may lead to many abnormal fertilities
and then affect the character sequence alignment
later. To alleviate this, a pre-processing step before
GIZA++ training applies unsupervised learning to
identify many frequently co-occurring characters
as fixed patterns in the source texts, including all
available training, development and testing data.
All possible tokens of the source names are con-
sidered.
Afterwards, the extraction and probability esti-
mation of corresponding sequences of characters
or pre-processed small tokens aligned in the prior
step is performed by ?diag-growth-final? (Koehn
et al, 2003), with maximum length 10, which is
tuned on development data, for both the source-
to-target and the target-to-source character align-
ment. Then two transliteration models, namely
p(t|s) and p(s|t), are generated by such extraction
for each transliteration run.
Another component involved in the training is
an n-gram language model. We set n = 3 and
have it trained with the available data of the target
language in question.
2.2 Optimization
Using the development sets for the NEWS2009
task, a minimum error rate training (MERT) (Och,
2003) is applied to tune the parameters for the cor-
responding feature models in (1). The training is
performed with regard to the mean F-score, which
is also called fuzziness in top-1, measuring on av-
erage how different the top transliteration candi-
date is from its closest reference. It is worth noting
that a high mean F-score indicates a high accuracy
of top candidates, thus a high mean reciprocal rank
(MRR), which is used to quantify the overall per-
formance of transliteration.
1http://code.google.com/p/giza-pp/
58
Table 1: Comparison: baseline v.s. optimized
performance on EnCh and EnRu development
sets.
?1a ?2 ?3 Mean F MRR
EnChb B
c 1 1 1 0.803 0.654
O 2.38 0.33 0.29 0.837 0.709
EnRu B 1 1 1 0.845 0.485O 2.52 0.27 0.21 0.927 0.687
a The subscripts 1, 2 and 3 refer to the two transliter-
ation models p(t|s) and p(s|t) and another language
model respectively, and normalized asP3i=1 ?i = 3.b EnCh stands for English to Chinese run and EnRu for
English to Russian run.
c B stands for baseline configuration and O for opti-
mized case.
As shown in Table 1, the optimization of the
three major models leads to a significant per-
formance improvement, especially when training
data is limited, such as the EnRu run, only 5977
entries of name entities are provided for train-
ing. And, it is also found that the optimized fea-
ture weights for other language pairs are similar to
these for the two runs as shown in the table above2.
Note for the optimization of the parameters, that
only the training data is used for construction of
models. For the test, both the training and the de-
velopment sets are used for training.
2.3 Decoding
The trained source-to-target and target-to-source
transliteration models are integrated with the lan-
guage model as given in (1) for our decoding.
We implement a beam-search decoder to deal
with these multiple transliteration models, which
takes both the forward- and backward-directional
aligned character sequences as factors to con-
tribute to the transliteration probability. Consid-
ering the monotonic transformation order, the de-
coding is performed sequentially from the begin-
ning to the end of a source text. No re-ordering
is needed for such transliteration. As the search
space is restricted in this way, the accuracy of
matching possible transliteration pairs is not af-
fected when the decoding is maintained at a faster
speed than that for ordinary translation. In ad-
dition, another heuristic condition is also used to
guide this monotonic decoding. For those tar-
get character sequences found in the training data,
their positions in a name entity can help the decod-
2Interestingly, the first model contributes much more than
others. It can achieve a comparable result even without model
2 and 3, according to our experiments.
Table 3: Numbers of name entities in NEWS2009
training data6.
EnCh 34857 EnHi 10990
EnJa 29811 EnTa 9031
EnKo 5838 EnKa 9040
JnJk 19891 EnRu 6920
ing to find better corresponding transliterations,
for some texts appear more frequently at the be-
ginning of a name entity and others at the end. We
use the probabilities for all aligned target charac-
ter sequences in different positions, and exploit the
data as an auxiliary feature model for the gener-
ation. Finally, all possible target candidates are
generated by (2) for source names.
3 Evaluation Results
For NEWS2009, we participated in all 8 standard
runs of transliteration task, namely, EnCh (Li et
al., 2004), EnJa, EnKo, JnJk3, EnHi, EnTa, EnKa
and EnRu (Kumaran and Kellner, 2007). Ten best
candidates generated for each source name are
submitted for each run. The transliteration per-
formance is evaluated by the official script4, using
six metrics5. The official evaluation results for our
system are presented in Table 2.
The effectiveness of our approach is revealed by
the fact that many of our Mean F-scores are above
0.8 for various tasks. These high scores suggest
that our top candidates are close to the given ref-
erences. Besides, it is also interesting to look into
how well the desired targets are generated under
a certain recall rate, by examining if the best an-
swers are among the ten candidates produced for
each source name. If the recall rate goes far be-
yond MRR, it can be a reliable indication that the
desired targets are found for most source names,
but just not put at the top of the ten-best. From the
last column in Table 2, we can see a great chance
to improve our performance, especially for EnCh,
JnJk and EnRu runs.
3http://www.cjk.org
4https://translit.i2r.a-star.edu.sg/news2009/evaluation/
5The six metrics are Word Accuracy in Top-1 (ACC),
Fuzziness in Top-1 (Mean F-score), Mean Reciprocal Rank
(MRR), Precision in the n-best candidates (Map ref), Prece-
sion in the 10-best candidates (Map 10) and Precision in the
system produced candidates (Map sys).
6Note that in some of the runs, when a source name has
multiple corresponding target names, the numbers are calcu-
lated according to the total target names in both the training
and development data.
59
Table 2: Evaluation result of NEWS2009 task.
Task Source Target ACC Mean F MRR Map ref Map 10 Map sys Recall
EnCh English Chinese 0.643 0.854 0.745 0.643 0.228 0.229 0.917
EnJa English Katakana 0.406 0.800 0.529 0.393 0.180 0.180 0.786
EnKo English Hangul 0.332 0.648 0.425 0.331 0.134 0.135 0.609
JnJk Japanese Kanji 0.555 0.708 0.653 0.538 0.261 0.261 0.852
EnHi English Hindi 0.349 0.829 0.455 0.341 0.151 0.151 0.681
EnTa English Tamil 0.316 0.848 0.451 0.307 0.154 0.154 0.724
EnKa English Kannada 0.177 0.799 0.307 0.178 0.109 0.109 0.576
EnRu English Russian 0.500 0.906 0.613 0.500 0.192 0.192 0.828
But still, since SMT is a data-driven approach,
the amount of training data could affect the
transliteration results significantly. Table 3 shows
the training data size in our task. It gives a hint
on the connections between the performance, es-
pecially Mean F-score, and the data size. In spite
of the low ACC, EnKa test has a Mean F-score
close to other two runs, namely EnHi and EnTa,
of similar data size. For EnRu test, although the
training data is limited, the highest Mean F-score
is achieved thanks to the nice correspondence be-
tween English and Russian characters.
4 Conclusion
In this paper we have presented our recent work to
apply the phrase-based SMT technology to name
entity transliteration on character sequences. For
training, the alignment is carried out on characters
and on those frequently co-occurring character se-
quences identified by unsupervised learning. The
extraction of bi-directional corresponding source
and target sequence pairs is then performed for
the construction of our transliteration models. In
decoding, a beam search decoder is applied to
generate transliteration candidates using both the
source-to-target and target-to-source translitera-
tion models, the target language model and some
heuristic guidance integrated. The MERT is ap-
plied to tune the optimum feature weights for these
models. Finally, ten best candidates are submitted
for each source name. The experimental results
confirm that our approach is effective and robust
in the eight runs of the NEWS2009 transliteration
task.
Acknowledgments
The research described in this paper was sup-
ported by City University of Hong Kong through
the Strategic Research Grants (SRG) 7002267 and
7002388.
References
W. Gao, K. F. Wong, and W. Lam. 2004. Improving
transliteration with precise alignment of phoneme
chunks and using context features. In Proceedings
of AIRS-2004.
Dan Goldwasser and Dan Roth. 2008. Translitera-
tion as constrained optimization. In Proceedings of
EMNLP-2008, pages 353?362, Honolulu, USA, Oc-
tober.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Pharaoh: A beam search decoder for phrase-
base statistical machine translation models. In Pro-
ceedings of the 6th AMTA, Edomonton, Canada.
A Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proceed-
ings of the 30th SIGIR.
Haizhou Li, Min Zhang, and Jian Su. 2004. A
joint source-channel model for machine transliter-
ation. In Proceedings of ACL-04, pages 159?166,
Barcelona, Spain, July.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of personal
names. In Proceedings of ACL-07, pages 120?127,
Prague, Czech Republic, June.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report on news 2009 machine
transliteration shared task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop, Singapore.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL-
02, pages 295?302, Philadelphia, USA, July.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL-03, pages 160?167, Sapporo, Japan, July.
Paola Virga and Sanjeev Khudanpur. 2003. Translit-
eration of proper names in cross-lingual information
retrieval. In Proceedings of the ACL 2003 Workshop
on Multilingual and Mixed-language Named Entity
Recognition, pages 57?64, Sapporo, Japan, July.
60
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 448?451,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
HITSZ_CITYU: Combine Collocation, Context Words and Neighbor-
ing Sentence Sentiment in Sentiment Adjectives Disambiguation 
 
 
Ruifeng Xu1,2, Jun Xu1 
1Harbin Institute of Technology, 
Shenzhen Campus, China 
xuruifeng@hitsz.edu.cn 
hit.xujun@gmail.com 
Chunyu Kit2 
2City University of Hong Kong, 
Hong Kong 
ctckit@cityu.edu.hk  
  
 
Abstract 
This paper presents the HIT_CITYU systems 
in Semeval-2 Task 18, namely, disambiguat-
ing sentiment ambiguous adjectives. The base-
line system (HITSZ_CITYU_3) incorporates 
bi-gram and n-gram collocations of sentiment 
adjectives, and other context words as features 
in a one-class Support Vector Machine (SVM) 
classifier. To enhance the baseline system, col-
location set expansion and characteristics 
learning based on word similarity and semi-
supervised learning are investigated, respec-
tively. The final system (HITSZ_CITYU_1/2) 
combines collocations, context words and 
neighboring sentence sentiment in a two-class 
SVM classifier to determine the polarity of 
sentiment adjectives. The final systems 
achieved 0.957 and 0.953 (ranked 1st and 2nd) 
macro accuracy, and 0.936 and 0.933 (ranked 
2nd and 3rd) micro accuracy, respectively.  
 
1 Introduction 
Sentiment analysis is always puzzled by the con-
text-dependent sentiment words that one word 
brings positive, neutral or negative meanings in 
different contexts. Hatzivassiloglou and 
Mckeown (1997) predicated the polarity of ad-
jectives by using the pairs of adjectives linked by 
consecutive or negation conjunctions. Turney 
and Littman (2003) determined the polarity of 
sentiment words by estimating the point-wise 
mutual information between sentiment words 
and a set of seed words with strong polarity. An-
dreevskaia and Bergler (2006) used a Sentiment 
Tag Extraction Program to extract sentiment-
bearing adjectives from WordNet. Esuli and Se-
basian (2006) studied the context-dependent sen-
timent words in WordNet but ignored the in-
stances in real context. Wu et al (2008) applied 
collocation plus a SVM classifier in Chinese sen-
timent adjectives disambiguation. Xu et al (2008) 
proposed a semi-supervised learning algorithm to 
learn new sentiment word and their context-
dependent characteristics.  
Semeval-2 Task 18 is designed to provide a 
common framework and dataset for evaluating 
the disambiguation techniques for Chinese sen-
timent adjectives. The HITSZ_CITYU group 
submitted three runs corresponding to one base-
line system and one improved systems (two runs). 
The baseline system (HITSZ_CITYU_3) is 
based on collocations between sentiment words 
and their targets as well as their context words. 
For the ambiguous adjectives, 412 positive and 
191 negative collocations are built from a 100-
million-word corpus as the seed collocation set. 
Using the context words of seed collocations as 
features, a one-class SVM classifier is trained in 
the baseline system. Using HowNet-based word 
similarity as clue, the seed collocations are ex-
panded to improve the coverage of collocation-
based technique. Furthermore, a semi-supervised 
learning algorithm is developed to learn new col-
locations between sentiment words and their tar-
gets from raw corpus. Finally, the inner sentence 
features, such as collocations and context words, 
and the inter sentence features, i.e. neighboring 
sentence sentiments, are incorporated to deter-
mine the polarity of ambiguous adjectives. The 
improved systems (HITSZ_CITYU_1/2) 
achieved 0.957 and 0.953 macro accuracy 
(ranked 1st and 2nd) and 0.936 and 0.933 micro 
accuracy (ranked 2nd and 3rd), respectively. This 
result shows that collocation, context-words and 
neighboring sentence sentiment are effective in 
sentiment adjectives disambiguation. 
The rest of this paper is organized as follows. 
Section 2 presents the collocation extraction sub-
system based on lexical statistics. Section 3 
448
presents the baseline system and Section 4 
presents the improved systems. The experiment 
results are given in Section 5 and finally, Section 
6 concludes. 
2 Collocation Extraction 
A lexical statistics-based collocation extraction 
subsystem is developed to identify both the bi-
gram and n-gram collocations of sentiment ad-
jectives. This subsystem is based on our previous 
research on Chinese collocation extraction. It 
recognizes the co-occurring words of a headword 
as collocations which have co-occurrence fre-
quency significance among all co-occurring 
words and co-occurrence position significance 
among all co-occurring positions.  
For a sentiment adjective, noted as whead, any 
word within the [-5,+5] context window is a co-
word, denoted as wco-i for 1? i ? k, where k is the 
total number of different co-words of whead.  
BI-Strength(whead,wco-i) between a head word 
whead and a co-word w co-i (i=1, to k) is designed 
to measure the co-occurrence frequency signifi-
cance as follows:  
)()(
)()(
5.0
)()(
)()(
5.0
),(
minmaxminmax icoico
icoicohead
headhead
headicohead
icohead
wfwf
wfwwf
wfwf
wfwwf
wwStrengthBI
??
???
?
?
??+?
??
=?
(1) 
where, fmax(whead) , fmin(whead) and )( headwf are the 
highest, lowest and average co-occurrence fre-
quencies among all the co-words of whead,, re-
spectively; fmax(wco-i), fmin(wcoi) and )( icowf ?  are 
respectively the highest, lowest and average co-
occurrence frequencies of the co-words for wco-i. 
The value of BI-Strength(whead wco-j) ranges from 
-1 to 1, and a larger value means a stronger asso-
ciation. Suppose f(whead,wco-i, m) is the frequency 
that wco-i co-occurs with whead at position m(?
5<=m<=5). The BI-Spread(whead,wco-i) is de-
signed to characterizes the significance that wco-i 
around whead at neighbouring places as follows: 
 
),,(
|),(),,(|
),(
5
5
5
5
?
?
?=
?
?=
??
?
?
=?
m
icohead
m
icoheadicohead
icohead
mwwf
wwfmwwf
wwSpreadBI
(2) 
where, ),( icohead wwf ? , fmax(whead,,wco-i), and fmin 
(whea,,dwco-i) are the average, highest, and lowest 
co-occurrence frequencies among all 10 posi-
tions, respectively. The value of BI-Spread(whead, 
wco-i) ranges from 0 to 1. A larger value means 
that whead and wco-i tend to co-occur in one or two 
positions.  
The word pairs satisfying, (1) BI-
Strength(whead wco-j)>K0 and (2) BI-Spread(whead, 
wco-i)>U0, are extracted as bi-gram collocations, 
where K0 and U0 are empirical threshold.  
Based on the extracted bi-gram collocations, 
the appearance of each co-word in each position 
around whead is analyzed. For each of the possible 
relative distances from whead, only words occupy-
ing the position with a probability greater than a 
given threshold T are kept. Finally, the adjacent 
words satisfying the threshold requirement are 
combined as n-gram collocations. 
3 The Baseline System 
The baseline system incorporates collocation and 
context words as features in a one-class SVM 
classifier. It consists of two steps: 
 STEP 1: To match a test instance containing 
seed collocation set. If the instance cannot be 
matched by any collocations, go to STEP 2. 
STEP 2: Use a trained classifier to indentify 
the sentiment of the word.  
The collocations of 14 testing sentiment adjec-
tives are extracted from a 100-million-word cor-
pus. Collocations with obvious and consistent 
sentiment are manually identified. 412 positive 
and 191 negative collocations are established as 
the seed collocation set.  
We think that the polarity of a word can be de-
termined by exploiting the association of its co-
occurring words in sentence. We assume that, the 
two instances of an ambiguous sentiment adjec-
tives that have similar neighboring nouns may 
have the same polarity. Gamon and Aue (2005) 
made an assumption to label sentiment terms. 
We extract 13,859 sentences containing collo-
cations between negative adjective and targets 
in seed collocation set or collocations between 
ambiguous adjective and negative modifier 
(such as ?? too) as the training data. These 
sentences are assume negative. A single-class 
classifier is then trained to recognize negative 
sentences. Three types of features are used:  
(1) Context features include bag of words 
within context in window of [-5, +5] 
(2) Collocation features contain bi-grams in 
window [-5,+5] 
(3) Collocation features contain n-grams in 
window [-5,+5] 
In our research, SVM with linear kernel is 
employed and the open source SVM package ? 
LIBSVM is selected for the implementation.  
4 The Improved System 
The preliminary experiment shows that the base-
line system is not satisfactory, especially the 
449
coverage is low. It is observed that the seed col-
location set covers 17.54% of sentences contain-
ing the ambiguous adjectives while the colloca-
tions between adjective and negative modifier 
covers only 11.28%. Therefore, we expand the 
sentiment adjective-target collocation set based 
on word similarity and a semi-supervised learn-
ing algorithm orderly. We then incorporate both 
inner-sentence features (collocations, context 
words, etc.) and inter-sentence features in the 
improved systems for sentiment adjectives dis-
ambiguation.  
4.1 Collocation Set Expansion based on 
Word Similarity 
First, we expand the seed collocation set on the 
target side. The words strongly similar to known 
targets are identified by using a word similarity 
calculation package, provided by HowNet (a 
Chinese thesaurus). Once these words co-occur 
with adjective within a context window more 
often than a threshold, they are appended to seed 
collocation set. For example, ??-??(low ca-
pacity)?is expanded from a seed collocation ??
-?? (low capacity)?. 
Second, we manually identify the words hav-
ing the same ?trend? as the testing adjectives. 
For example, ??? increase? is selected as a 
same-trend word of ?? high?. The collocations 
of ???? are extracted from corpus. Its collo-
cated targets with confident and consistent sen-
timent are appended to the sentiment collocation 
set of ??? if they co-occurred with ??? more 
than a threshold. In this way, some low-
frequency sentiment collocation can be obtained. 
4.2 Semi-supervised Learning of Sentiment 
Collocations 
A semi-supervised learning algorithm is devel-
oped to further expand the collocation seed set, 
which is described as follows. (It is revised based 
on our previous research (Xu et al 2008). The 
basic assumption here is that, the sentiment of a 
sentence having ambiguous adjectives can be 
estimated based on the sentiment of its neighbor-
ing sentences.  
 
Input: Raw training corpus, labeled as Su,  
Step 1. The sentences holding strong polarities 
are recognized from Su which satisfies any two of 
following requirements, (1) contains known con-
text-free sentiment word (CFSW); (2) contains 
more than three known context-dependent senti-
ment words (CDSW); (3) contains collocations 
between degree adverbs and known CDSWs; (4) 
contains collocations between degree adverbs 
and opinion operators (the verbs indicate a opi-
nion operation, such as?? praise); (5) contains 
known opinion indicator and known CDSWs. 
Step 2. Identify the strong non-opinionated sen-
tences in Su. The sentences satisfying all of fol-
lowing four conditions are recognized as non-
opinionated ones, (1) have no known sentiment 
words; (2) have no known opinion operators; (3) 
have no known degree adverbs and (4) have no 
known opinion indicators.  
Step 3. Identify the opinion indicators in the rest 
sentences. Determine their polarities if possible 
and mark the conjunction (e.g.? and) or nega-
tion relationship (e.g.? but) in the sentences. 
Step 4. Match the CFSWs and known CDSWs in 
Su. The polarities of CFSWs are assigned based 
on sentiment lexicon.  
Step 5. If a CDSW occurs in a sentence with cer-
tain orientations which is determined by the opi-
nion indicators, its polarity is assigned as the 
value suggested. If a CDSW co-occur with a 
seed collocated target, it polarity is assigned ac-
cording to the seed sentiment collocation set. 
Otherwise, if a CDSW co-occur with a CFSW in 
the same sentence, or the neighboring continual 
or compound sentence, the polarity of CDSW is 
assigned as the same as CFSW, or the reversed 
polarity if a negation indicator is detected. 
Step 6. Update the polarity scores of CDSWs in 
the target set by using the cases where the polari-
ty is determined in Step 5. 
Step 7. Determine the polarities of CDSWs in 
the undetermined sentences. Suppose Si is a sen-
tence and the polarity scores of all its CFSWs 
and CDSWs are known, its polarity, labeled as 
Plo(Si), is estimated by using the polarity scores 
of all of the opinion words in this sentence, viz.: 
  
? ?+ ?= )(_)(_ )(_)(pos_)( CDSWnegPCDSWposP CFSWnegPCFSWPSiPlo (3) 
A large value (>0) of Plo(si) implies that si tends 
to be positive, and vice versa.  
Step 8. If the sentence polarity cannot be deter-
mined by its components, we use the polarity of 
its neighboring sentences sj-1 and sj+1, labeled as 
Plo(sj-1) and Plo(sj+1), respectively, to help de-
termine Plo(sj), viz.:  
)(5.0)(*)(5.0)( 11 +? ?++?= jjjj sPlosPlosPlosPlo (4) 
where, Plo*(sj) is the polarity score of Sj (Fol-
lowing Equation 3) but ignore the contribution of 
testing adjectives while 0.5 are empirical weights.  
450
Step 9. After all of the polarities of known 
CDSWs in the training data are determined, up-
date the collocation set by identifying co-
occurred pairs with consistent sentiment. 
Step 10. Repeat Step 5 to Step 9 to re-estimate 
the sentiment of CDSWs and expand the colloca-
tion set, until the collocation set converge. 
 
In this way, the seed collocation set is further 
expanded and their sentiment characteristics are 
obtained.  
4.3 Sentiment Adjectives Classifier 
We incorporate the following 8 groups of fea-
tures in a linear-kernel two-class SVM classifier 
to classify the sentences with sentiment adjec-
tives into positive or negative: 
(1) The presence of known positive/negative 
opinion indicator and opinion operator 
(2) The presence of known positive/negative 
CFSW 
(3) The presence of known positive/negative 
CDSW(exclude the testing adjectives) 
(4) The presence of known positive/negative 
adjective-target bi-gram collocations 
(5) The presence of known positive/negative 
adjective-target n-gram collocations 
(6) The coverage of context words surround-
ing the adjectives in the context words in 
training positive/negative sentences 
(7) The sentiment of -1 sentence 
(8) The sentiment of +1 sentence 
The classifier is trained by using the sentences 
with determined sentiment which is obtained in 
the semi-supervised learning stage. 
5 Evaluations and Conclusion 
The ACL-SEMEVAL task 18 testing dataset 
contains 14 ambiguous adjectives and 2,917 in-
stances. HITSZ_CITYU group submitted three 
runs. Run-1 and Run-2 are two runs correspond-
ing to the improved system and Run-3 is the 
baseline system. The achieved performances are 
listed in Table 1.  
 
Run ID Marco Accuracy Micro Accuracy
1 0.953 0.936 
2 0.957 0.933 
3(baseline) 0.629 0.665 
Table 1: Performance of HITSZ_CITYU Runs 
 
It is observed that the improved systems 
achieve promising results which is obviously 
higher than the baseline. They are ranked 1st and 
2nd in Macro Accuracy evaluation and 2nd and 3rd 
in Micro Accuracy evaluation among 16 submit-
ted runs, respectively. 
6 Conclusion 
In this paper, we proposed similarity-based and 
semi-supervised based methods to expand the 
adjective-target seed collocation set. Meanwhile, 
we incorporate both inner-sentence (collocations 
and context words) and inter-sentence features in 
a two-class SVM classifier for the disambigua-
tion of sentiment adjectives. The achieved prom-
ising results show the effectiveness of colloca-
tion features, context words features and senti-
ment of neighboring sentences. Furthermore, we 
found that the neighboring sentence sentiments 
are important features for the disambiguation of 
sentiment ambiguous adjectives, which is ob-
viously different from the traditional word sense 
disambiguation that emphasize the inner-
sentence features. 
References  
Andreevskaia, A. and Bergler, S. 2006. Mining 
WordNet for fuzzy sentiment: Sentiment tag ex-
traction from WordNet glosses. In Proceedings of 
EACL 2006, pp. 209-216 
Esuli, A. and Sebastian, F. 2006. SENTIWORDNET: 
A publicly available lexical resource for opinion 
mining. In Proceeding of LREC 2006, pp. 417-422. 
Hatzivassiloglou, V. and McKeown, K. R. 1997. Pre-
dicting the semantic orientation of adjectives. In 
Proceeding of ACL 1997, pp.174-181 
Michael Gamon and Anthony Aue. 2005. Automatic 
identification of sentiment vocabulary: Exploiting 
low association with known sentiment terms. In 
Proceedings of the ACL05 Workshop on Feature 
Engineering for Machine Learning in Natural 
Language Processing, pp.57-64 
Ruifeng Xu, Kam-Fai Wong et al 2008. Learning 
Knowledge from Relevant Webpage for Opinion 
Analysis, in Proceedings of 2008 IEEE / WIC / 
ACM Int. Conf. Web Intelligence, pp. 307-313  
Turney, P. D. and Littman, M. L. 2003. Measuring 
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on In-
formation Systems, vol. 21, no. 4, pp.315-346 
Yunfang Wu, Miao Wang and Peng Jin. 2008. Dis-
ambiguating sentiment ambiguous adjectives, In 
Proceedings of Int. Conf. on Natural Language 
Processing and Knowledge Engineering 2008, pp. 
1-8 
451
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 203?207
Manchester, August 2008
Parsing Syntactic and Semantic Dependencies with
Two Single-Stage Maximum Entropy Models
?
Hai Zhao and Chunyu Kit
Department of Chinese, Translation and Linguistics
City University of Hong Kong
83 Tat Chee Avenue, Kowloon, Hong Kong, China
haizhao@cityu.edu.hk, ctckit@cityu.edu.hk
Abstract
This paper describes our system to carry
out the joint parsing of syntactic and se-
mantic dependencies for our participation
in the shared task of CoNLL-2008. We il-
lustrate that both syntactic parsing and se-
mantic parsing can be transformed into a
word-pair classification problem and im-
plemented as a single-stage system with
the aid of maximum entropy modeling.
Our system ranks the fourth in the closed
track for the task with the following per-
formance on the WSJ+Brown test set:
81.44% labeled macro F1 for the overall
task, 86.66% labeled attachment for syn-
tactic dependencies, and 76.16% labeled
F1 for semantic dependencies.
1 Introduction
The joint parsing of syntactic and semantic depen-
dencies introduced by the shared task of CoNLL-
08 is more complicated than syntactic dependency
parsing or semantic role labeling alone (Surdeanu
et al, 2008). For semantic parsing, in particu-
lar, a dependency-based representation is given but
the predicates involved are unknown, and we also
have nominal predicates besides the verbal ones.
All these bring about more difficulties for learning.
This paper presents our research for participation
in the CoNLL-2008 shared task, with a highlight
on our strategy to select learning framework and
features for maximum entropy learning.
?
This study is supported by CERG grant 9040861 (CityU
1318/03H) and CityU Strategic Research Grant 7002037.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The rest of the paper is organized as follows.
The next section presents the technical details of
our system and Section 3 its evaluation results.
Section 4 looks into a few issues concerning our
forthcoming work for this shared task, and Section
5 concludes the paper.
2 System Description
For the sake of efficiency, we opt for the maximum
entropy model with Gaussian prior as our learning
model for both the syntactic and semantic depen-
dency parsing. Our implementation of the model
adopts L-BFGS algorithm for parameter optimiza-
tion as usual (Liu and Nocedal, 1989). No addi-
tional feature selection techniques are applied.
Our system consists of three components to deal
with syntactic and semantic dependency parsing
and word sense determination, respectively. Both
parsing is formulated as a single-stage word-pair
classification problem, and the latter is carried out
by a search through the NomBank (Meyers et al,
2004) or the PropBank (Palmer et al, 2005)
1
.
2.1 Syntactic Dependency Parsing
We use a shift-reduce scheme to implement syn-
tactic dependency parsing as in (Nivre, 2003). It
takes a step-wised, history- or transition-based ap-
proach. It is basically a word-by-word method
with a projective constraint. In each step, the clas-
sifier checks a word pair, e.g., TOP, the top of a
stack for processed words, and, NEXT, the first
word in the unprocessed word sequence, in order
to determine if a dependent label should be as-
signed to them. Besides two arc-building actions,
a shift action and a reduce action are also defined
to meet the projective constraint, as follows.
1
These two dictionaries that we used are downloaded from
CoNLL-2008 official website.
203
Notation Meaning
s Clique in the top of stack
s
?1
,... The first clique below the top of stack, etc.
i, i
+1
,... The first (second) clique in the unprocessed
sequence, etc.
dprel Dependent label
h Head
lm Leftmost child
rm Rightmost child
rn Right nearest child
form Word form
lemma Word lemma
pos Predicted PoS tag
sp Y Split Y , which may be form, lemma or pos.
. ?s, e.g., ?s.dprel? means dependent label
of the clique in the top of stack
/ Feature combination, i.e., ?s.pos/i.pos?
means s.pos and i.pos together as a
feature function.
p The current predicate candidate
a The current argument candidate
Table 1: Feature Notations
1. Left-arc: Add an arc from NEXT to TOP and
pop the stack.
2. Right-arc: Add an arc from TOP to NEXT
and push NEXT onto the stack.
3. Reduce: Pop TOP from the stack.
4. Shift: Push NEXT onto the stack.
We implement a left-to-right arc-eager parsing
model in a way that the parser scan through an in-
put sequence from left to right and the right depen-
dents are attached to their heads as soon as possible
(Hall et al, 2007). To construct a single-stage sys-
tem, we extend the left-/right-arc actions to their
correspondent multi-label actions as necessary. In-
cluding 32 left-arc and 66 right-arc actions, alto-
gether a 100-class problem is yielded for the pars-
ing action classification for this shared task.
Since only projective sequences can be handled
by the shift-reduce scheme, we apply the pseudo-
projective transformation introduced by (Nivre and
Nilsson, 2005) to projectivize those non-projective
sequences. Our statistics show that only 7.6% se-
quences and less than 1% dependencies in the cor-
pus provided for training are non-projective. Thus,
we use a simplified strategy to projectivize an input
sequence. Firstly, we simply replace the head of a
non-projective dependency by its original head?s
head but without any additional dependent label
encoding for the purpose of deprojectivizing the
output during decoding. Secondly, if the above
standard projectivization step cannot eliminate all
Basic Extension
x.sp Y itself, its previous two and next two Y s, and
all bigrams within the five-clique window,
(x is s or i, and Y is form, lemma or pos.)
x.Y (x is s or i, and Y is form, lemma or pos.)
x.Y /i.Y (x is s or s
?1
and Y is pos, sp lemma
or sp pos)
s.h.sp form
s.dprel
s.lm.dprel
s.rn.dprel
i.lm.sp pos
s.lm.dprel/s.dprel
s.lm.sp pos/s.sp pos
s.h.sp pos/s.sp pos
x.sp pos|rootscore (x is s or i.)
s.sp pos/i.sp pos|pairscore
s.curroot.sp pos/i.sp pos
Table 2: Features for Syntactic Parsing
non-projective dependencies in a sequence, then
the word with the shortest sequence (rather than
dependent tree) distance to the original head will
be chosen as the head of a non-projective depen-
dency. In practice, the above two-step projectiviza-
tion procedure can eliminate all non-projective de-
pendencies in all sequences. Our purpose here is to
provide as much data as possible for training, and
only projective sequences are input for training and
output for decoding.
While memory-based and margin-based learn-
ing approaches such as support vector machines
are popularly applied to shift-reduce parsing, our
work provides evidence that the maximum en-
tropy model can achieve a comparative perfor-
mance with the aid of a suitable feature set. With
feature notations in Table 1, we use a feature set as
shown in Table 2 for syntactic parsing.
Here, we explain ?rootscore?, ?pairscore? and
curroot in Table 2. Both rootscore and pairscore
return the log frequency for an event in the training
corpus. The former counts a given split PoS occur-
ring as ROOT, and the latter two split PoS?s com-
bination associated with a dependency label. The
feature curroot returns the root of a partial parsing
tree that includes a specified node.
2.2 Semantic Dependency Parsing
Assuming no predicates overtly known, we keep
using a word-pair classifier to perform semantic
parsing through a single-stage processing. Specif-
ically, we specify the first word in a word pair as
a predicate candidate (i.e., a semantic head, and
noted as p in our feature representation) and the
next as an argument candidate (i.e., a semantic de-
204
Basic Extension
x.sp Y itself, its previous and next cliques, and
all bigrams within the three-clique window.
(Y is form or lemma.)
a
x.sp pos itself, its previous and next two cliques, and
all bigrams within the five-clique window.
x.Y (Y is form, lemma or pos.)
p.Y /i.Y (Y is sp lemma or sp pos.)
a is the same as p
x.is Verb or Noun
bankAdvice
b
a.h.sp form
x.dprel
x.lm.dprel
p.rm.dprel
p.lm.sp pos
a.lm.dprel/a.dprel
a.lm.sp pos/a.sp pos
a.sp Y/a.dprel (Y is lemma or pos.)
x.sp lemma/x.h.sp form
p.sp lemma/p.h.sp pos
p.sp pos/p.dprel
a.preddir
c
p.voice/a.preddir
d
x.posSeq
e
x.dprelSeq
f
a.dpTreeLevel
g
a/p|dpRelation
a/p|SharedPosPath
a/p|SharedDprelPath
a/p|x.posPath
a/p|x.dprelPath
a/p|dprelPath
a
x is p or a throughout the whole table.
b
This and the following features are all concerned with a
known syntactic dependency tree.
c
preddir: the direction to the current predicate candidate.
d
voice: if the syntactic head of p is be and p is not ended
with -ing, then p is passive.
e
posSeq: PoS tag sequence of all syntactic children
f
dprelSeq: syntactic dependent label sequence of all syn-
tactic children
g
dpTreeLevel: the level in the syntactic parse tree, counted
from the leaf node.
Table 3: Features for Semantic Parsing
pendent, and noted as a). We do not differenti-
ate between nominal and verbal predicates and our
system handles them in in exactly the same way.
If decoding outputs show that no arguments can
be found for a predicate candidate in the decoding
output, then this candidate will be naturally dis-
carded from the output predicate list.
When no constraint available, however, all word
pairs in the an input sequence must be considered,
leading to very poor efficiency in computation for
no gain in effectiveness. Thus, the training sample
needs to be pruned properly.
For predicate, only nouns and verbs are consid-
ered possible candidates. That is, all words with-
out a split PoS in these two categories are filtered
out. Many prepositions are also marked as pred-
icate in the training corpus, but their arguments?
roles are ?SU?, which are not counted the official
evaluation.
For argument, a dependency version of the prun-
ing algorithm in (Xue and Palmer, 2004) is used to
find, in an iterative way, the current syntactic head
and its siblings in a parse tree in a constituent-
based representation. In this representation, the
head of a phrase governs all its sisters in the tree,
as illustrated in the conversion of constituents to
dependencies in (Lin, 1995). In our implementa-
tion, the following equivalent algorithm is applied
to select argument candidates from a syntactic de-
pendency parse tree.
Initialization: Set the given predicate candi-
date as the current node;
(1) The current node and all of its syntactic chil-
dren are selected as argument candidates.
(2) Reset the current node to its syntactic head
and repeat step (1) until the root is reached.
This algorithm can cover 98.5% arguments while
reducing about 60% of the training samples, ac-
cording to our statistics. However, this is achieved
at the price of including a syntactic parse tree as
part of the input for semantic parsing.
The feature set listed in Table 3 is adopted for
our semantic parsing, some of which are borrowed
from (Hacioglu, 2004). Among them, dpTreeRela-
tion returns the relationship of a and p in a syntac-
tic parse tree. Its possible values include parent,
sibling, child, uncle, grand parent
etc. Note that there is always a path to the ROOT in
the syntactic parse tree for either a or p. Along the
common part of these two paths, SharedDprelPath
returns the sequence of dependent labels collected
from each node, and SharedPosPath returns the
corresponding sequence of PoS tags. x.dprelPath
and x.posPath return the PoS tag sequence from x
to the beginnings of SharedDprelPath and Shared-
PosPath, respectively. a/p|dprelPath returns the
concatenation of a.dprelPath and p.dprelPath.
We may have an example to show how the fea-
ture bankAdvice works. Firstly, the current pro-
cessed semantic role labels and argument candi-
date direction are checked. Specifically, they are
the arguments A0 and A1 that have been marked
before the predicate candidate p and the current ar-
gument identification direction after p. Secondly,
205
UAS LAS Label-Acc.
Development 88.78 85.85 91.14
WSJ 89.86 87.52 92.47
Brown 85.03 79.83 86.71
WSJ+Brown 89.32 86.66 91.83
Table 4: The Results of Syntactic Parsing (%)
Data Precision Recall F-score
Development 79.76 72.25 75.82
Label. WSJ 80.57 74.97 77.67
Brown 66.28 61.29 63.69
WSJ+Brown 79.03 73.49 76.16
Development 89.58 81.15 85.16
Unlab. WSJ 89.48 83.26 86.26
Brown 83.14 76.88 79.89
WSJ+Brown 88.79 82.57 85.57
Table 5: The Results of Semantic Parsing (%)
each example
2
of p in NomBank or PropBank that
depends on the split PoS tag of p is checked if
it partially matches the current processed role la-
bels. If a unique example exists in this form, e.g.,
Before:A0-A1; After:A3, then this feature
returns A3 as feature value. If no matched or mul-
tiple matched examples exist, then this feature re-
turns a default value.
2.3 Word Sense Determination
The shared task of CoNLL-2008 for word sense
disambiguation task is to determine the sense of an
output predicate. Our system carries out this task
by searching for a right example in the given Nom-
Bank or PropBank. The semantic role set scheme
of each example for an output predicate is checked.
If a scheme is found to match the output seman-
tic role set of a predicate, then the corresponding
sense for the first match is chosen; otherwise the
system outputs ?01? as the default sense.
3 Evaluation Results
Our evaluation is carried out on a 64-bit ubuntu
Linux installed server with double dual-core AMD
Opteron processors of 2.8GHz and 8GB memory.
The full training set for CoNLL-2008 is used to
train the maximum entropy model. The training
for the syntactic parser costs about 200 hours and
2
The term ?example? means a chunk in NomBank
or PropBank, which demonstrates how semantic roles
occur around a specified predicate. For example, for
a sense item of the predicate access in PropBank,
we first have <arg n="0">a computer</arg>
<rel>access</rel> <arg n="1">its
memory</arg>, and then a role set scheme for this
sense as Before:A0;After:A1.
Data Precision Recall F-score
Development 82.80 79.05 80.88
Label. WSJ 84.05 81.25 82.62
Macro Brown 73.05 70.56 71.78
WSJ+Brown 82.85 80.08 81.44
Development 89.18 84.97 87.02
Unlab. WSJ 89.67 86.56 88.09
Macro Brown 84.08 80.96 82.49
WSJ+Brown 89.06 85.94 87.47
Development 83.69 80.71 82.17
Label. WSJ 85.07 82.88 83.96
Micro Brown 75.14 73.09 74.10
WSJ+Brown 83.98 81.80 82.88
Development 89.06 85.90 87.45
Unlab. WSJ 89.72 87.42 88.56
Micro Brown 84.38 82.07 83.21
WSJ+Brown 89.14 86.83 87.97
Table 6: Overall Scores (%)
4.1GB memory and that for the semantic parser
costs about 170 hours and 4.9GB memory. The
running time in each case is the sum of all running
time for all threads involved. When a parallel opti-
mization technique is applied to speedup the train-
ing, the time can be reduced to about 1/3.5 of the
above.
The official evaluation results for our system are
presented in Tables 4, 5 and 6. Following the
official guideline of CoNLL-2008, we use unla-
beled attachment score (UAS), labeled attachment
score (LAS) and label accuracy to assess the per-
formance of syntactic dependency parsing. For
semantic parsing, the unlabeled scores metric the
identification performance and the labeled scores
the overall performance of semantic labeling.
4 To Do
Although we are unable to follow our plan to do
more than what we have done for this shared task,
because of the inadequate computational resource
and limited time, we have a number of techniques
in our anticipation to bring in further performance
improvement.
While expecting to accomplish the joint infer-
ence of syntactic and semantic parsing, we only
have time to complete a system with the former to
enhance the latter. But we did have experiments in
the early stage of our work to show that a syntactic
dependency parser can make use of available se-
mantic dependency information to enhance its per-
formance by 0.5-1%
3
.
Most errors in our syntactic parsing are related
3
We used the outputs of a semantic parser, either predicted
or gold-standard, as features for syntactic parsing.
206
to the dependencies of comma and prepositions.
We need to take care of them, for PP attachment
is also crucial to the success of semantic parsing.
Extra effort is paid, as illustrated in previous work
such as (Xue and Palmer, 2004), to handle such
cases, especially when a PP is involved. We find in
our data that about 1% arguments occur as a grand-
child of a predicate through PP attachment.
Syntactic parsing contributes crucially to the
overall performance of the joint parsing by pro-
viding a solid basis for further semantic parsing.
Thus there is reason to believe that improvement
of syntactic dependency parsing can be more in-
fluential than that of semantic parsing to the overall
improvement. Only one model was used for syn-
tactic parsing in our system, in contrast to the exist-
ing work using an ensemble technique for further
performance enhancement, e.g., (Hall et al, 2007).
Again, the latter means much more computational
cost should be taken.
Though it was not done before submission dead-
line, we also tried to enhance the semantic parsing
with some more sophisticated inputs from the syn-
tactic parsing. One is predicted syntactic parsed
tree input that may be created by cross-validation
rather than the gold-standard syntactic input that
our submitted semantic parser was actually trained
on. Another is the n-best outputs of the syntactic
parser. However, only the single-best output of the
syntactic parser was actually used.
5 Conclusion
As presented in the above sections, our system to
participate in the CoNLL-2008 shared task is im-
plemented as two single-stage maximum entropy
learning. We have tackled both syntactic and se-
mantic parsing as a word-pair classification prob-
lem. Despite the simplicity of this approach, our
system has produced promising results.
Acknowledgements
We wish to thank Dr. Wenliang Chen of NICT,
Japan for helpful discussions on dependency pars-
ing, and two anonymous reviewers for their valu-
able comments.
References
Hacioglu, Kadri. 2004. Semantic role labeling us-
ing dependency trees. In Proceedings of the 20th
international conference on Computational Linguis-
tics (COLING-2004), pages 1273?1276, Geneva,
Switzerland, August 23rd-27th.
Hall, Johan, Jens Nilsson, Joakim Nivre,
G?ulsen Eryi?git, Be?ata Megyesi, Mattias Nils-
son, and Markus Saers. 2007. Single malt or
blended? a study in multilingual parser optimiza-
tion. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 933?939,
Prague, Czech, June.
Lin, Dekang. 1995. A dependency-based method for
evaluating broad-coverage parser. In Proceedings
of the Fourteenth International Joint Conference on
Artificial Intelligence (IJCAI-95), pages 1420?1425,
Montr?eal, Qu?ebec, Canada, August 20-25.
Liu, Dong C. and Jorge Nocedal. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Meyers, Adam, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The nombank project:
An interim report. In Proceedings of HLT/NAACL
Workshop on Frontiers in Corpus Annotation, pages
24?31, Boston, Massachusetts, USA, May 6.
Nivre, Joakim and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics (ACL-2005), pages 99?106, Ann
Arbor, Michigan, USA, June 25-30.
Nivre, Joakim. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT 03), pages 149?160, Nancy, France, April 23-
25.
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
Xue, Nianwen and Martha Palmer. 2004. Cal-
ibrating features for semantic role labeling. In
2004 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2004), pages 88?94,
Barcelona, Spain, July 25-26.
207
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 360?364,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Parameter-optimized ATEC Metric for MT Evaluation 
 
 
Billy T-M Wong               Chunyu Kit 
Department of Chinese, Translation and Linguistics 
City University of Hong Kong 
Tat Chee Avenue, Kowloon, Hong Kong 
{ctbwong, ctckit}@cityu.edu.hk 
 
  
 
Abstract 
This paper describes the latest version of the 
ATEC metric for automatic MT evaluation, 
with parameters optimized for word choice 
and word order, the two fundamental features 
of language that the metric relies on. The 
former is assessed by matching at various 
linguistic levels and weighting the informa-
tiveness of both matched and unmatched 
words. The latter is quantified in term of 
word position and information flow. We also 
discuss those aspects of language not yet 
covered by other existing evaluation metrics 
but carefully considered in the formulation of 
our metric.  
1 Introduction 
It is recognized that the proposal of the BLEU 
metric (Papineni et al, 2002) has piloted a para-
digm evolution to MT evaluation. It provides a 
computable solution to the task and turns it into 
an engineering problem of measuring text simi-
larity and simulating human judgments of trans-
lation quality. Related studies in recent years 
have extensively revealed more essential charac-
teristics of BLEU, including its strengths and 
weaknesses. This has aroused the proposal of 
different new evaluation metrics aimed at ad-
dressing such weaknesses so as to find some oth-
er hopefully better alternatives for the task. Ef-
fort in this direction brings up some advanced 
metrics such as METEOR (Banerjee and Lavie, 
2005) and TERp (Snover et al, 2009) that seem 
to have already achieved considerably strong 
correlations with human judgments. Nevertheless, 
few metrics have really nurtured our understand-
ing of possible parameters involved in our lan-
guage comprehension and text quality judgment. 
This inadequacy limits, inevitably, the applica-
tion of the existing metrics. 
The ATEC metric (Wong and Kit, 2008) was 
developed as a response to this inadequacy, with 
a focus to account for the process of human 
comprehension of sentences via two fundamental 
features of text, namely word choice and word 
order. It integrates various explicit measures for 
these two features in order to provide an intuitive 
and informative evaluation result. Its previous 
version (Wong and Kit, 2009b) has already illu-
strated a highly comparable performance to the 
few state-of-the-art evaluation metrics, showing 
a great improvement over its initial version for 
participation in MetricsMATR081. It is also ap-
plied to evaluate online MT systems for legal 
translation, to examine its applicability for lay 
users? use to select appropriate MT systems 
(Wong and Kit, 2009a). 
In this paper we describe the formulation of 
ATEC, including its new features and optimiza-
tion of parameters. In particular we will discuss 
how the design of this metric can complement 
the inadequacies of other metrics in terms of its 
treatment of word choice and word order and its 
utilization of multiple references in the evalua-
tion process. 
2 The ATEC Metric 
2.1 Word Choice 
In general, word is the basic meaning bearing 
unit of language. In a semantic theory such as 
Latent Semantic Analysis (LSA) (Landauer et al, 
1998), lexical selection is even the sole consider-
ation of the meaning of a text. A recent study of 
the major errors in MT outputs by Vilar et al 
(2006) also reveals that different kinds of error 
related to word choices constitute a majority of 
error types. It is therefore of prime importance 
                                                 
1 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/2008/ 
360
for MT evaluation metrics to diagnose the ade-
quacy of word selection by an MT system. 
It is a general consensus that the performance 
of an evaluation metric can be improved by 
matching more words between MT outputs and 
human references. Linguistic resources like 
stemmer and WordNet are widely applied by 
many metrics for matching word stems and syn-
onyms. ATEC is equipped with these two mod-
ules as well, and furthermore, with two measures 
for word similarity, including a WordNet-based 
(Wu and Palmer, 1994) and a corpus-based 
measure (Landauer et al, 1998) for matching 
word pairs of similar meanings. Our previous 
work (Wong, 2010) shows that the inclusion of 
semantically similar words results in a positive 
correlation gain comparable to the use of Word-
Net for synonym identification. 
In addition to increasing the number of legiti-
mate matches, we also consider the importance 
of each match. Although most metrics score 
every matched word with equal weight, different 
words indeed contribute different amount of in-
formation to the meaning of a sentence. In Ex-
ample 1 below, both C1 and C2 contain the same 
number of words matched with Ref, but the 
matches in C1 are more informative and there-
fore should be assigned higher weights. 
 
Example 1 
C1: it was not first time that prime minister con-
fronts northern league ? 
C2: this is not the prime the operation with the 
north ? 
Ref: this is not the first time the prime minister 
has faced the northern league ? 
 
The informativeness of a match is weighted by 
the tf-idf measure, which has been widely used in 
information retrieval to assess the relative impor-
tance of a word as an indexing term for a docu-
ment. A word is more important to a document 
when it occurs more frequently in this document 
and less in others. In ATEC, we have ?document? 
to refer to ?sentence?, the basic text unit in MT 
evaluation. This allows a more sensitive measure 
for words in different sentences, and gets around 
the problem of an evaluation dataset containing 
only one or a few long documents. Accordingly, 
the tf-idf measure is formulated as: 
)log(),( ,
i
ji sf
N
tfjitfidf ?=  
where tfi,j is the occurrences of word wi in sen-
tence sj, sfi the number of sentences containing 
word wi, and N the total number of sentences in 
the evaluation set. In case of a high-frequency 
word whose tf-idf weight is less than 1, it is then 
rounded up to 1. 
In addition to matched words, unmatched 
words are also considered to have a role to play 
in determining the quality of word choices of an 
MT output. As illustrated in Example 1, the un-
matched words in Ref for C1 and C2 are [this | is 
| the | the | has | faced | the] and [first | time | mi-
nister | has | faced | northern | league] respective-
ly. One can see that the words missing in C2 are 
more significant. It is therefore necessary to ap-
ply the tf-idf weighting to unmatched reference 
words so as to quantify the information missed in 
the MT outputs in question.  
2.2 Word Order 
In MT evaluation, word order refers to the extent 
to which an MT output is interpretable following 
the information flow of its reference translation. 
It is not rare that an MT output has many 
matched words but does not make sense because 
of a problematic word order. Currently it is ob-
served that consecutive matches represent a legi-
timate local ordering, causing some metrics to 
extend the unit of matching from word to phrase. 
Birch et al (2010) show, however, that the cur-
rent metrics including BLEU, METEOR and 
TER are highly lexical oriented and still cannot 
distinguish between sentences of different word 
orders. This is a serious problem in MT evalua-
tion, for many MT systems have become capable 
of generating more and more suitable words in 
translations, resulting in that the quality differ-
ence of their outputs lies more and more crucial-
ly in the variances of word order.  
ATEC uses three explicit features for word or-
der, namely position distance, order distance and 
phrase size. Position distance refers to the diver-
gence of the locations of matches in an MT out-
put and its reference. Example 2 illustrates two 
candidates with the same match, whose position 
in C1 is closer to its corresponding position in 
Ref than that in C2. We conceive this as a signif-
icant indicator of the accuracy of word order: the 
closer the positions of a matched word in the 
candidate and reference, the better match it is. 
 
Example 2 
C1: non-signatories these acts victims but it 
caused to incursion transcendant 
C2: non-signatories but it caused to incursion 
transcendant these acts victims 
Ref: there were no victims in this incident but 
they did cause massive damage 
361
The calculation of position distance is based 
on the position indices of words in a sentence. In 
particular, we align every word in a candidate to 
its closest counterpart in a reference. In Example 
3, all the candidate words have a match in the 
reference. As illustrated by the two ?a? in the 
candidate, the shortest alignments (strict lines) 
are preferred over any farther alternatives (dash 
lines). In a case like this, only two matches, i.e., 
thief and police, vary in position by a distance of 
3. 
 
Example 3 
Candidate: a thief chases a police 
Pos distance:        0     3         0      0      3 
Pos index: 1    2         3      4      5  
 
Reference: a police chases a thief 
Pos index: 1     2         3        4       5  
 
This position distance is sensitive to sentence 
length as it simply makes use of word position 
indices without any normalization. Example 4 
illustrates two cases of different lengths. The po-
sition distance of the bold matched words is 3 in 
C1 but 14 in C2. Indeed, the divergence of word 
order in C1 does not hinder our understanding, 
but in C2 it poses a serious problem. This exces-
sive length inevitably magnifies the interference 
effect of word order divergence. 
 
Example 4 
C1: Short1 and2 various3 international4 news5 
R1: International1 news2 brief3 
C2: Is1 on2 a3 popular4 the5 very6 in7 Iraq8 to9 
those10 just11 like12 other13 world14 in15 
which16 young17 people18 with19 the20 and21 
flowers22 while23 awareness24 by25 other26 
times27 of28 the29 countries30 of31 the32 
R2: Valentine?s1 day2 is3 a4 very5 popular6 day7 
in8 Iraq9 as10 it11 is12 in13 the14 other15 coun-
tries16 of17 the18 world19. Young20 men21 ex-
change22 with23 their24 girlfriends25 sweets26, 
flowers27, perfumes28 and29 other30 gifts31. 
 
Another feature, the order distance, concerns 
the information flow of a sentence in the form of 
the sequence of matches. Each match in a candi-
date and a reference is first assigned an order 
index in a sequential manner. Then, the differ-
ence of two counterpart indices is measured, so 
as to see if a variance exists. Examples 5a and 5b 
exemplify two kinds of order distance and their 
corresponding position distance. Both cases have 
two matches with the same sum of position dis-
tance. However, the matches are in an identical 
sequence in 5a but cause a cross in 5b, resulting 
in a larger order distance for the latter.  
 
Example 5a 
Position index 
Order index 
Candidate: 
 
Reference: 
Order index 
Position index 
 
Position distance 
Order distance 
1      2     3     4 
1            2 
A    B    C    D 
 
B    E    D    F 
1           2 
1     2     3     4 
 
(2-1) + (4-3) = 2 
(1-1) + (2-2) = 0 
 
Example 5b 
Position index 
Order index 
Candidate: 
 
Reference: 
Order index 
Position index 
 
Position distance 
Order distance 
1      2     3     4 
        1     2 
A    B    C    D 
 
C    B    E    F 
1      2 
1      2     3     4 
 
(2-2) + (3-1) = 2 
(2-1) + (2-1) = 2 
 
In practice, ATEC operates on phrases like 
many other metrics. But unlike these metrics that 
count only the number of matched phrases, 
ATEC gives extra credit to a longer phrase to 
reward its valid word sequence. In Example 6, 
C1 and C2 represent two MT outputs of the same 
length, with matched words underlined. Both 
have 10 matches in 3 phrases, and will receive 
the same evaluation score from a metric like 
METEOR or TERp, ignoring the subtle differ-
ence in the sizes of the matched phrases, which 
are [8,1,1] and [4,3,3] words for C1 and C2 re-
spectively. In contrast, ATEC uses the size of a 
phrase as a reduction factor to its position dis-
tance, so as to raise the contribution of a larger 
phrase to the metric score.  
 
Example 6 
C1: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 
C2: w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 
2.3 Multiple References 
The availability of multiple references allows 
more legitimate word choices and word order of 
an MT output to be accounted. Some existing 
metrics only compute the scores of a candidate 
against each reference and select the highest one. 
362
This deficit can be illustrated by a well-known 
example from Papineni et al (2002), as repli-
cated in Example 7 with slight modification. It 
shows that nearly all candidate words can find 
their matches in either reference. However, if we 
resort to single reference, only around half of 
them can have a match, which would seriously 
underrate the quality of the candidate.  
 
Example 7 
C:   It is a guide to action which ensures that the 
military always obeys the commands of the 
party. 
 
R1: It is a guide to action that ensures that the 
military will forever heed Party commands. 
 
R2: It is the guiding principle which guarantees 
the military forces always being under the 
commands of the party. 
 
ATEC exploits multiple references in this fa-
shion to maximize the number of matches in a 
candidate. It begins with aligning the longest 
matches with either reference. The one with the 
shortest position distance is preferred if more 
than one alternative available in the same phrase 
size. This process repeats until no more candi-
date word can find a match. 
2.4 Formulation of ATEC 
The computation of an ATEC score begins with 
alignment of phrases, as described above. For 
each matched phase, we first sum up the score of 
each word i in the phrase as 
?
?
?=
}{
)(
phrasei i
match
typematch tfidf
Info
wW  
where wtype refers to a basic score of a matched 
word depending on its match type. It is then 
minus its information load, i.e., the tf-idf score of 
the matched word with a weight factor, Infomatch. 
There is also a distance penalty for a phrase, 
orderorder
e
pospos diswc
p
diswDis +?= )
||
||
1(
 ,
 
where dispos and disorder refer to the position 
distance and order distance, and wpos and worder 
are their corresponding weight factors, 
respectively. The position distance is further 
weighted according to the size of phrase |p| with 
an exponential factor e, in proportion to the 
length of candidate |c|.  
The score of a matched phrase is then 
computed by 
??
?
?
?=
,
,
DisW
LimitW
Phrase
match
dismatch  if  Dis > Wmatch?Limitdis; 
otherwise, 
Limitdis is an upper limit for the distance penalty. 
Accordingly, the score C of all phrases in a can-
didate is  
?
?
=
}{candidatej
jPhraseC
. 
Then, we move on to calculating the informa-
tion load of unmatched reference words Wunmatch, 
approximated as 
)(
}{
?
?
?=
unmatchk k
unmatch
typeunmatch tfidf
Info
wW
.
 
The overall score M accounting for both the 
matched and unmatched is defined as 
??
?
?
?=
,
,
unmatch
Info
WC
LimitC
M
if  Wunmatch > C?LimitInfo; 
otherwise, 
LimitInfo is an upper limit for the information 
penalty of the unmatched words. 
Finally, the ATEC score is computed using the 
conventional F-measure in terms of precision P 
and recall R as  
RP
PR
ATEC
)1( ?? ?+=  
where             || c
M
P =
,
 
|| r
M
R =
.
 
The parameter ? adjusts the weights of P and R, 
and |c| and |r| refer to the length of candidate and 
reference, respectively. In the case of multiple 
references, |r| refers to the average length of ref-
erences. 
We have derived the optimized values for the 
parameters involved in ATEC calculation using 
the development data of NIST MetricsMATR10 
with adequacy assessments by a simple hill 
climbing approach. The optimal parameter set-
ting is presented in Table 1 below. 
3 Conclusion 
In the above sections we have presented the lat-
est version of our ATEC metric with particular 
emphasis on word choice and word order as two 
fundamental features of language. Each of these 
features contains multiple parameters intended to 
363
have a comprehensive coverage of different tex-
tual factors involved in our interpretation of a 
sentence. The optimal offsetting for the parame-
ters is expected to report an empirical observa-
tion of the relative merits of each factor in ade-
quacy assessment. We are currently exploring 
their relation with the errors of MT outputs, to 
examine the potential of automatic error analysis. 
The ATEC package is obtainable at:  
http://mega.ctl.cityu.edu.hk/ctbwong/ATEC/ 
 
Acknowledgments 
The research work described in this paper was 
supported by City University of Hong Kong 
through the Strategic Research Grant (SRG) 
7002267. 
References 
Satanjeev Banerjee and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Pro-
ceedings of Workshop on Intrinsic and Extrinsic 
Evaluation Measures for MT and/or Summariza-
tion at the 43th Annual Meeting of the Association 
of Computational Linguistics (ACL), pages 65-72, 
Ann Arbor, Michigan, June 2005. 
Alexandra Birch, Miles Osborne and Phil Blunsom. 
2010. Metrics for MT Evaluation: Evaluating 
Reordering. Machine Translation (forthcoming). 
Thomas Landauer, Peter W. Foltz and Darrell Laham. 
1998. Introduction to Latent Semantic Analysis. 
Discourse Processes 25: 259?284. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A Method for Auto-
matic Evaluation of Machine Translation. Proceed-
ings of 40th Annual Meeting of the Association for 
Computational Linguistics (ACL), pages 311?318, 
Philadelphia, PA, July 2002. 
Matthew Snover, Nitin Madnani, Bonnie Dorr, and 
Richard Schwartz. 2009. Fluency, Adequacy, or 
HTER? Exploring Different Human Judgments 
with a Tunable MT Metric. Proceedings of the 
Fourth Workshop on Statistical Machine Transla-
tion at the 12th Meeting of the European Chapter 
of the Association for Computational Linguistics 
(EACL), pages 259-268, Athens, Greece, March, 
2009. 
David Vilar, Jia Xu, Luis Fernando D'Haro and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation (LREC), pages 697-702, Genova, 
Italy, May 2006. 
Billy T-M Wong. 2010. Semantic Evaluation of Ma-
chine Translation. Proceedings of the 7th Interna-
tional Conference on Language Resources and 
Evaluation (LREC), Valletta, Malta, May, 2010. 
Billy T-M Wong and Chunyu Kit. 2008. Word choice 
and Word Position for Automatic MT Evaluation.  
AMTA 2008 Workshop: MetricsMATR, 3 pages, 
Waikiki, Hawai'i, October, 2008. 
Billy T-M Wong and Chunyu Kit. 2009a. Meta-
Evaluation of Machine Translation on Legal Texts. 
Proceedings of the 22nd International Conference 
on the Computer Processing of Oriental Languag-
es (ICCPOL), pages 343-350, Hong Kong, March, 
2009. 
Billy Wong and Chunyu Kit. 2009b. ATEC: Automat-
ic Evaluation of Machine Translation via Word 
Choice and Word Order. Machine Translation, 
23(2):141-155. 
Zhibiao Wu and Martha Palmer. 1994. Verb Seman-
tics and Lexical Selection. Proceedings of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics, pages 133-138, Las Cruces, 
New Mexico. 
Parameters Values 
wtype 1        (exact match), 
 0.95  (stem / synonym / 
semantically close),
 0.15  (unmatch) 
Infomatch 0.34 
Infounmatch 0.26 
wpos 0.02 
worder 0.15 
e 1.1 
Limitdis 0.95 
LimitInfo 0.5 
? 0.5 
Table 1  Optimal parameter values for ATEC
 
364
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 62?65,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Reranking with Multiple Features for Better Transliteration
Yan Song? Chunyu Kit? Hai Zhao??
?Department of Chinese, Translation and Linguistics
City University of Hong Kong, 83 Tat Chee Ave., Kowloon, Hong Kong
?Department of Computer Science and Engineering
Shanghai Jiao Tong University, #800, Dongchuan Rd, Shanghai, China
{yansong,ctckit}@cityu.edu.hk, zhaohai@cs.sjtu.edu.cn
Abstract
Effective transliteration of proper names
via grapheme conversion needs to find
transliteration patterns in training data,
and then generate optimized candidates
for testing samples accordingly. However,
the top-1 accuracy for the generated candi-
dates cannot be good if the right one is not
ranked at the top. To tackle this issue, we
propose to rerank the output candidates for
a better order using the averaged percep-
tron with multiple features. This paper de-
scribes our recent work in this direction for
our participation in NEWS2010 transliter-
ation evaluation. The official results con-
firm its effectiveness in English-Chinese
bidirectional transliteration.
1 Introduction
Since transliteration can be considered a direct or-
thographic mapping process, one may adopt gen-
eral statistical machine translation (SMT) proce-
dures for its implementation. Aimed at finding
phonetic equivalence in another language for a
given named entity, however, different translitera-
tion options with different syllabification may gen-
erate multiple choices with the symphonic form
for the same source text. Consequently, even the
overall results by SMT output are acceptable, it
is still unreliable to rank the candidates simply by
their statistical translation scores for the purpose
of selecting the best one. In order to make a proper
choice, the direct orthographic mapping requires a
precise alignment and a better transliteration op-
tion selection. Thus, powerful algorithms for ef-
fective use of the parallel data is indispensable, es-
pecially when the available data is limited in vol-
ume.
Interestingly, although an SMT based approach
could not achieve a precise top-1 transliteration re-
sult, it is found in (Song et al, 2009) that, in con-
trast to the ordinary top-1 accuracy (ACC) score,
its recall rate, which is defined in terms of whether
the correct answer is generated in the n-best output
list, is rather high. This observation suggests that
if we could rearrange those outputs into a better
order, especially, push the correct one to the top,
the overall performance could be enhanced signif-
icantly, without any further refinement of the orig-
inal generation process. This reranking strategy is
proved to be efficient in transliteration generation
with a multi-engine approach (Oh et al, 2009).
In this paper, we present our recent work on
reranking the transliteration candidates via an on-
line discriminative learning framework, namely,
the averaged perceptron. Multiple features are in-
corporated into it for performance enhancement.
The following sections will give the technical de-
tails of our method and present its results for
NEWS2010 shared task for named entity translit-
eration.
2 Generation
For the generation of transliteration candidates,
we follow the work (Song et al, 2009), using a
phrase-based SMT procedure with the log-linear
model
P (t|s) = exp[
?n
i=1 ?ihi(s, t)]?
t exp[
?n
i=1 ?ihi(s, t)]
(1)
for decoding. Originally we use two directional
phrase1 tables, which are learned for both direc-
tions of source-to-target and target-to-source, con-
taining different entries of transliteration options.
In order to facilitate the decoding by exploiting all
possible choices in a better way, we combine the
forward and backward directed phrase tables to-
gether, and recalculate the probability for each en-
1It herein refers to a character sequence as described in
(Song et al, 2009).
62
try in it. After that, we use a phoneme resource2 to
refine the phrase table by filtering out the wrongly
extracted phrases and cleaning up the noise in it.
In the decoding process, a dynamic pruning is per-
formed when generating the hypothesis in each
step, in which the threshold is variable according
to the current searching space, for we need to ob-
tain a good candidate list as precise as possible
for the next stage. The parameter for each fea-
ture function in log-linear model is optimized by
MERT training (Och, 2003). Finally, a maximum
number of 50 candidates are generated for each
source name.
3 Reranking
3.1 Learning Framework
For reranking training and prediction, we adopt
the averaged perceptron (Collins, 2002) as our
learning framework, which has a more stable per-
formance than the non-averaged version. It is pre-
sented in Algorithm 1. Where ~? is the vector of
parameters we want to optimize, x, y are the cor-
responding source (with different syllabification)
and target graphemes in the candidate list, and ?
represents the feature vector in the pair of x and
y. In this algorithm, reference y?i is the most ap-
propriate output in the candidate list according to
the true target named entity in the training data.
We use the Mean-F score to identify which candi-
date can be the reference, by locating the one with
the maximum Mean-F score value. This process
updates the parameters of the feature vector and
also relocate all of the candidates according to the
ranking scores, which are calculated in terms of
the resulted parameters in each round of training
as well as in the testing process. The number of
iteration for the final model is determined by the
development data.
3.2 Multiple Features
The following features are used in our reranking
process:
Transliteration correspondence feature, f(si, ti);
This feature describes the mapping between
source and target graphemes, similar to the
transliteration options in the phrase table in
our previous generation process, where s and
2In this work, we use Pinyin as the phonetic representa-
tion for Chinese.
Algorithm 1 Averaged perceptron training
Input: Candidate list with reference
{LIST (xj , yj)nj=1, y?i }Ni=1
Output: Averaged parameters
1: ~? ? 0, ~?a ? 0, c? 1
2: for t = 1 to T do
3: for i = 1 to N do
4: y?i ? argmaxy?LIST (xj ,yj)~? ? ?(xi, yi)
5: if y?i 6= y?i then
6: ~? ? ~? +?(x?i , y?i )? ?(x?i, y?i)
7: ~?a ? ~?a+ c ? {?(x?i , y?i )??(x?i, y?i)}
8: end if
9: c? c+ 1
10: end for
11: end for
12: return ~? ? ~?a/c
t refer to the source and target language re-
spectively, and i to the current position.
Source grapheme chain feature, f(sii?1);
It measures the syllabification for a given
source text. There are two types of units
in different levels. One is on syllable level,
e.g., ?aa/bye?, ?aa/gaar/d?, reflecting the
segmentation of the source text, and the other
on character level, such as ?a/b?, ?a/g?,
?r/d?, showing the combination power of
several characters. These features on differ-
ent source grapheme levels can help the sys-
tem to achieve a more reliable syllabification
result from the candidates. We only consider
bi-grams when using this feature.
Target grapheme chain feature, f(tii?2);
This feature measures the appropriateness of
the generated target graphemes on both char-
acter and syllables level. It performs in a
similar way as the language model for SMT
decoding. We use tri-gram syllables in this
learning framework.
Paired source-to-target transition feature, f(<
s, t >ii?1);
This type of feature is firstly proposed in
(Li et al, 2004), aiming at generating source
and target graphemes simultaneously under
a suitable constraint. We use this feature
to restrict the synchronous transition of both
source and target graphemes, measuring how
well are those transitions, such as for ?st?,
63
whether ?s? transliterated by ??? is followed
by ?t? transliterated by ???. In order to deal
with the data sparseness, only bi-gram transi-
tion relations are considered in this feature.
Hidden Markov model (HMM) style features;
There are a group of features with HMM
style constraint for evaluating the candi-
dates generated in previous SMT process,
including, previous syllable HMM features,
f(sii?n+1, ti), posterior syllable HMM fea-
tures, f(si+n?1i , ti), and posterior character
HMM features, f(si, l, ti), where l denotes
the character following the previous syllable
in the source language. For the last feature,
it is effective to use both the current sylla-
ble and the first letter of the next syllable
to bound the current target grapheme. The
reason for applying this feature in our learn-
ing framework is that, empirically, the letters
following many syllables strongly affect the
transliteration for them, e.g., Aves ? ??
?, ?a? followed by ?v? is always translated
into ??? rather than ???.
Target grapheme position feature, f(ti, p);
This feature is an improved version of that
proposed in (Song et al, 2009), where p
refers to the position of ti. We have a mea-
sure for the target graphemes according to
their source graphemes and the current posi-
tion of their correspondent target characters.
There are three categories of such position,
namely, start (S), mediate (M) and end (E). S
refers to the first character in a target name, E
to the final, and the others belong to M. This
feature is used to exploit the observation that
some characters are more likely to appear at
certain positions in the target name. Some are
always found at the beginning of a named en-
tity while others only at the middle or the end.
For example, ?re? associated to first charac-
ter in a target name is always transliterated as
???, such as Redd ???. When ?re? ap-
pears at the end of a source name, however,
its transliteration will be ??? in most cases,
just like Gore ???.
Target tone feature;
This feature is only applied to the translit-
eration task with Chinese as the target lan-
guage. It can be seen as a combination
of a target grapheme chain with some posi-
tion features, using tone instead of the target
grapheme itself for evaluation. There are 5
tones (0,1,2,3,4) for Chinese characters. It is
easy to conduct a comprehensive analysis for
the use of a higher ordered transition chain as
a better constraint. Many fixed tone patterns
can be identified in the Chinese translitera-
tion training data. The tone information can
also be extracted from the Pinyin resource we
used in the previous stage.
Besides the above string features, we also have
some numeric features, as listed below.
Transliteration score;
This score is the joint probabilities of all
transliteration options, included in the output
candidates generated by our decoder.
Target language model score;
This score is calculated from the probabilistic
tri-gram language model.
Source/target Pinyin feature;
This feature uses Pinyin representation for a
source or target name, depending on what
side the Chinese language is used. It mea-
sures how good the output candidates can be
in terms of the comparison between English
text and Pinyin representation. The resulted
score is updated according to the Levenshtein
distance for the two input letter strings of En-
glish and Pinyin.
For a task with English as the target language,
we add the following two additional features into
the learning framework.
Vowel feature;
It is noticed that when English is the target
language, vowels can sometimes be missing
in the generated candidates. This feature is
thus used to punish those outputs unqualified
to be a valid English word for carrying no
vowel.
Syllable consistent feature;
This feature measures whether an English tar-
get name generated in the previous step has
the same number of syllables as the source
name. In Chinese-to-English transliteration,
Chinese characters are single-syllabled, thus
64
Table 1: Evaluation results for our NEWS2010 task.
Task Source Target ACC Mean F MRR Map ref Recall ACCSMT
EnCh English Chinese 0.477 0.740 0.506 0.455 0.561 0.381
ChEn Chinese English 0.227 0.749 0.269 0.226 0.371 0.152
we can easily identify their number. For syl-
labification, we have an independent segmen-
tation process for calculating the syllables.
4 Results
For NEWS2010, we participated in all two
Chinese related transliteration tasks, namely,
EnCh (English-to-Chinese) and ChEn (Chinese-
to-English back transliteration). The official eval-
uation scores for our submissions are presented
in Table 1 with recall rate, and the ACC score
(ACCSMT ) for original SMT outputs. It is easy
to see the performance gain for the reranking, and
also from the recall rate that there is still some
room for improvement, in spite of the high ratio of
ACC/Recall3 calculated from Table 1. However, it
is also worth noting that, some of the source texts
cannot be correctly transliterated, due to many
multiple-word name entities with semantic com-
ponents in the test data, e.g., ?MANCHESTER
BRIDGE?, ?BRIGHAM CITY? etc. These seman-
tic parts are beyond our transliteration system?s ca-
pability to tackle, especially when the training data
is limited and the only focus of the system is on the
phonetic equivalent correspondence.
Compared to the EnCh transliteration, we get a
rather low ACC score for the ChEn back translit-
eration, suggesting that ChEn task is somewhat
harder than the EnCh (in which Chinese char-
acters are always limited). The ChEn task is a
one-to-many translation, involving a lot of pos-
sible choices and combinations of English sylla-
bles. This certainly makes it a more challenge-
able task than EnCh. However, looking into the
details of the outputs, we find that, in the ChEn
back transliteration, some characters in the test
corpus are unseen in the training and the devel-
opment data, resulting in incorrect transliterations
for many graphemes. This is another factor affect-
ing our final results for the ChEn task.
5 Conclusion
In this paper, we have presented our work on
multiple feature based reranking for transliteration
3Compared to the results from (Song et al, 2009)
generation. It NEWS2010 results show that this
approach is effective and promising, in the sense
that it ranks the best in EnCh and ChEn tasks. The
reranking used in this work can also be consid-
ered a regeneration process based on an existing
set, as part of our features are always used directly
to generate the initial transliteration output in other
researches. Though, those features are strongly
dependent on the nature of English and Chinese
languages, it is thus not an easy task to transplant
this model for other language pairs. It is an inter-
esting job to turn it into a language independent
model that can be applied to other languages.
Acknowledgments
The research described in this paper was par-
tially supported by City University of Hong Kong
through the Strategic Research Grants (SRG)
7002267 and 7008003. Dr. Hai Zhao was sup-
ported by the Natural Science Foundation of China
(NSFC) through the grant 60903119. We also
thank Mr. Wenbin Jiang for his helpful sugges-
tions on averaged perceptron learning.
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP-2002, pages 1?8, July.
Haizhou Li, Min Zhang, and Jian Su. 2004. A
joint source-channel model for machine transliter-
ation. In Proceedings of ACL-04, pages 159?166,
Barcelona, Spain, July.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL-03, pages 160?167, Sapporo, Japan, July.
Jong-Hoon Oh, Kiyotaka Uchimoto, and Kentaro Tori-
sawa. 2009. Machine transliteration using target-
language grapheme and phoneme: Multi-engine
transliteration approach. In Proceedings of NEWS
2009, pages 36?39, Suntec, Singapore, August.
Yan Song, Chunyu Kit, and Xiao Chen. 2009. Translit-
eration of name entity via improved statistical trans-
lation on character sequences. In Proceedings of
NEWS 2009, pages 57?60, Suntec, Singapore, Au-
gust.
65
Bigram HMM with Context Distribution Clustering for Unsupervised
Chinese Part-of-Speech tagging
Lidan Zhang
Department of Computer Science
the University of Hong Kong
Hong Kong
lzhang@cs.hku.hk
Kwok-Ping Chan
Department of Computer Science
the University of Hong Kong
Hong Kong
kpchan@cs.hku.hk
Abstract
This paper presents an unsupervised
Chinese Part-of-Speech (POS) tagging
model based on the first-order HMM.
Unlike the conventional HMM, the num-
ber of hidden states is not fixed and will
be increased to fit the training data. In
favor of sparse distribution, the Dirich-
let priors are introduced with variational
inference method. To reduce the emis-
sion variables, words are represented by
their contexts and clustered based on the
distributional similarities between con-
texts. Experiment results show the out-
put state sequence of HMM are highly
correlated to the latent annotations of
gold POS tags, in context of clustering
similarity measures. The other exper-
iments on a real application, unsuper-
vised dependency parsing, reveal that the
output sequence can replace the manu-
ally annotated tags without loss of accu-
racies.
1 Introduction
Recently latent variable model has shown great
potential in recovering the underlying structures.
For example, the task of POS tagging is to re-
cover the appropriate sequence structure given
the input word sequence (Goldwater and Grif-
fiths, 2007). One of the most popular exam-
ple of latent models is Hidden Markov Model
(HMM), which has been extensively studied for
many years (Rabiner, 1989). The key problem
of HMM is how to find an optimal hidden state
number and the topology appropriately.
In most cases, the topology of HMM is pre-
defined by exploiting the domain or empirical
knowledge. This topology will be fixed during
the whole process. Therefore how to select the
optimal topology for a certain application or a set
of training data is still a problem, because many
researches show that varying the size of the state
space greatly affects the performance of HMM.
Generally there are two ways to adjust the state
number: top-down and bottom-up methods. In
the bottom-up methods (Brand, 1999), the state
number is initialized with a relatively large num-
ber. During the training, the states are merged or
trimmed and ended with a small set of states. On
the other hand, the top-down methods (Siddiqi et
al., 2007) start from a small state set and split one
or some states until no further improvement can
be obtained. The bottom-up approaches require
huge computational cost in deciding the states to
be merged, which makes it impractical for appli-
cations with large state space. In this paper, we
focus on the latter approaches.
Another problem in HMM is that EM algo-
rithm might yield local maximum value. John-
son (2007) points out that training HMM with
EM gives poor results because it leads to a fairly
flat distribution of hidden states when the empiri-
cal distribution is highly skewed. A multinomial
prior, which favors sparse distribution, is a good
choice for natural language tasks. In this paper,
we proposed a new procedure for inferring the
HMM topology and estimating its parameters si-
multaneously. Gibbs sampling has been used in
infinite HMM (iHMM) (Beal et al, 2001; Fox et
al., 2008; Van Gael et al, 2008) for inference.
Unfortunately Gibbs sampling is slow and diffi-
cult to be converged. In this paper, we proposed
the variational Bayesian inference for the adap-
tive HMMmodel with Dirichlet prior. It involves
a modification to the Baum-Welch algorithm. In
each iteration, we replaced only one hidden state
with two new states until convergence.
To reduce the number of observation vari-
ables, the words are pre-clustered and repre-
sented by the exemplar within the same clus-
ter. It is a one-to-many clustering, because the
same word play different roles under different
contexts. We evaluate the similarity between the
distribution of contexts, with the assumption that
the context distribution implies syntactic pattern
of the given word (Zelling, 1968; Weeds and
Weir, 2003). With this clustering, more contex-
tual information can be considered without in-
creasing the model complexity. A relatively sim-
ple model is important for unsupervised task in
terms of computational burden and data sparse-
ness. This is the reason why we do not increase
the order of HMM(Kaji and Kitsuregawa, 2008;
Headden et al, 2008).
With unsupervised algorithms, there are two
aspects to be evaluated (Van Gael et al, 2009).
Fist one is how good the outcome clusters are.
We compare the HMM results with the manu-
ally POS tags and report the similarity measures
based on information theory. On the other hand,
we test how good the outputs act as an interme-
diate results. In many natural language tasks, the
inputs are word class, not the actual lexical item,
for reason of sparsity. In this paper, we choose
the unsupervised dependency parsing as the ap-
plication to investigate whether our clusters can
replace the manual labeled tags or not.
The paper is organized as below: in section 2,
we describe the definition of HMM and its vari-
ance inference. We present our dynamic HMM
in section 3. To overcome the context limitation
in the first-order HMM, we present our distribu-
tional similarity clustering in section 4. In sec-
tion 5, we reported the results of the mentioned
experiments while section 6 concludes the paper.
2 Terminology
The task of POS tagging is to assign a syntac-
tic category sequence to the input words. Let
S be defined as the set of all possible hidden
states, which are expected to be highly correlated
to POS tags. ? represents the set of all words.
Therefore the task is to find a sequence of tag
sequence S = s1...sn ? S given a sequence of
words (i.e. a sentence, W = w1...wn ? ?). The
optimal tags is to maximize the conditional prob-
ability p(S |W), which is equal to:
max
S
p(S |W) = max
S
p(S )p(W |S )
= max
S
p(W, S )
(1)
In this paper,we consider the first-order HMM,
where the POS tags are regarded as hidden states
and words as observed variables. According to
the Markov assumption, the best sequence of
tags S for a given sequence of words W is done
by maximizing (with s0 = 0) the joint probabil-
ity:
p(W, S ) =
n
?
i=1
p(si|si?1)p(wi|si) (2)
where w0 is the special boundary marker of sen-
tences.
2.1 Variational Inference for HMM
Let the HMM be modeled with parameter ? =
(A, B, pi), where A = {ai j} = {P(st = j|st?1 = i)}
is the transition matrix governing the dynamic of
the HMM. B = {bt(i)} = {P(wt = i|st}) is the state
emission matrix and pi = {pii} = {P(s1 = i)} as-
signs the initial probabilities to all hidden states.
In favor of sparse distributions, a natural choice
is to encode Dirichlet prior into parameters p(?).
In particular, we have:
p(A) =
N
?
i=1
Dir({ai1, ..., aiN} |u(A))
p(B) =
N
?
i=1
Dir({bi1, ..., biN} |u(B))
p(pi) = Dir({pi1, ..., piN} |u(pi))
(3)
where the Dirichlet distribution of order N with
hyperparameter vector u is defined as:
Dir(x|u) =
?(
?N
i=1 ui)
?N
i=1 ?(ui)
N
?
i=1
xui?1i . (4)
In this paper, we consider the symmetric
Dirichlet distribution with a fixed length, i.e.
u = [
?N
i=1 ui/N, ...,
?N
i=1 ui/N].
In the Bayesian framework, the model param-
eters are also regarded as hidden variables. The
marginal likelihood can be calculated by sum-
ming up all hidden variables. According to the
Jensen?s inequality, the lower bound of marginal
likelihood is defined as:
ln p(W) = ln
?
?
S
p(?)p(W, S |?)d?
?
?
?
S
q(?, S ) ln
p(W, S , ?)
q(?, S )
d?
= F
(5)
Generally, Variational Bayesian Inference
aims to find a tractable distribution q(?, s) that
maximizes the lower bound F . To make infer-
ence flexible, the posterior distribution can be
assumed to be factorized according to the mean-
field assumption. We have:
p(W, S , ?) ? q(S , ?) = q
?
(?)qS (S ) (6)
Then an extension of EM algorithm (called
Baum-Welch algorithm) can be used to alter-
nately optimize the qS and q?. The EM process
is described as follows:
? E Step: Forward-Backward algorithm to
find the optimal state sequence S (t+1) =
argmax p(S (t)|W, ?(t))
? M Step: The parameters ?(t+1) are re-
estimated given the optimal state S (t+1)
The E and M steps are repeated until a conver-
gence criteria is satisfied. Beal (2003) proved
that only need to do minor modifications in M
step (in 1) is needed, when Dirichlet prior is in-
troduced.
3 Adaptive Hidden Markov Model
As aforementioned, the key problem of HMM is
how to initialize the number of hidden states and
select the topology of HMM. In this paper, we
use the top-down scheme: starting from a small
number of states, only one state is chosen in each
step and splitted into two new states. This binary
split scheme is described in Figure 1.
Algorithm 1 Outline of our adpative HMM
Initialization: Initialize: t = 0, N(t)
repeat
Optimization: Find the optimal parameters
for current Nt
Candidate Generation: Split states and
generate candidate HMMs
Candidate Selection: Select the optimal
HMM from the candidates, whose hidden
state number is Nt+1
untilNo further improvement can be achieved
after splitting
In the following, we will discuss the details of
each step one by one.
3.1 Candidate Generation
Let N(t) represent the number of hidden states at
timestep t. The problem is how to choose the
states for splitting. A straightforward way is to
select all states and generate N(t) + 1 candidate
HMMs, including the original un-splitted one.
Obviously the exhaustive search is inefficient es-
pecially for large state space. To make the algo-
rithm more efficient, some constraints must be
set to narrow the search space.
Intuitively entropy implies uncertainty. So
hidden states with large conditional entropies are
desirable to be splitted. We can define the con-
ditional entropy of the state sequences given ob-
servation W as:
H(S |W) = ?
?
S
[P(S |W) log P(S |W)] (8)
Our assumption is the state to be splitted must
be the states sequence with the highest condi-
tional entropy value. This entropy can be recur-
sively calculated with complexity O(N2T ) (Her-
nando et al, 2005). Here N is the number of
A(t+1) = {a(t+1)i j } = exp[?(?
(A)
i j ) ? ?(
N
?
j=1
?
(A)
i j )] ; ?
(A)
i j = u
(A)
j + Eq(s)[ni j]
B(t+1) = {b(t+1)ik } = exp[?(?
(B)
ik ) ? ?(
T
?
k=1
?
(B)
ik )] ; ?
(B)
ik = u
(B)
k + Eq(s)[n
?
ik]
pi
(t+1)
= {pi(t+1)i } = exp[?(?
(pi)
i ) ? ?(
N
?
i=1
?
(pi)
j )]; ?
(pi)
i = u
(pi)
i + Eq(s)[n
??
i ]
(7)
Figure 1: Parameters update equations in M-step. Here E is the expectation with respect to the
model parameters. And ni j is the expected number of transition from state si to state s j; n?ik is the
expected number of times word wk occurs with state si; n??i is the occurrence of s0 = i
states and T is the length of sequence. Using
this entropy constraint, the size of candidate state
set is always smaller than the minimal value be-
tween N and T .
3.2 Candidate Selection
Given the above candidate set, the parameters of
each HMM are to be updated. Note that we just
update the parameters related to the split state,
whilst keep the others fixed. Suppose the i-th
hidden state is replaced by two new states. First
the transition matrix is enlarged from N(t) ? N(t)
dimension to (N(t) + 1) ? (N(t) + 1) dimension,
by inserting one column and row after the i-th
column and row. In the process of update, we
only change the items in the two (i and i + 1)
rows and columns. The other elements irrelevant
to the split state are not involved in the update
procedure. Similarly EM algorithm is used to
find the optimal parameters. Note that most of
the calculations can be skipped by making use
of the forward and backward probability matrix
achieved in the previous step. Therefore the con-
vergence is fast.
Given the candidate selection, we can use a
modified Baum-Welch algorithm to find optimal
states and parameters. Here we use the algorithm
in (Siddiqi et al, 2007) with some modifications
for the Dirichlet prior. In particular, in E step,
we follow their partial Forward-Background al-
gorithm to calculate E[ni j] and E[n?ik], if si or s j
is candidate state to be splitted. Then in M-step,
only rows and columns related to the candidate
state are updated according to equation (7). The
detailed description is given as appendix.
Finally it is natural to use variational bound
of marginal likelihood in equation (5) for model
scoring and convergence criterion.
4 Distributional Clustering
To reduce the number of observation variables,
the words are clustered before HMM training.
Intuitively, the words share the similar contexts
have similar syntactic property. The categories
of many words are varied in different contexts.
In other words, the cluster of a given word is
heavily dependent on the context it appears. For
example,?? can be a noun (meaning: discov-
ery) if it acts as an object, or a verb (meaning: to
discover) if it is followed with a noun. Further-
more the introduction of context can overcome
the limited context in the first-order HMM.
The underlying hypothesis of clustering based
on distributional similarity is that the words oc-
curring in similar contexts behave as similar syn-
tactic roles. In this work, the context of a word
is a trigram consist of the word immediately pre-
ceding the target and the word immediately fol-
lowing it. The similarity between two words
is measured by Pointwise Mutual Information
(PMI) between the context pair in which they ap-
pear:
PMI(wi,w j) = log
P(ci, c j)
P(ci)P(c j)
(9)
where ci denotes the context of wi. P(ci, c j) is
the co-occurrence probability of ci and c j, and
P(ci) =
?
j P(ci, c j) is the occurrence probabil-
ity of ci. In our experiments, the cutoff context
count is set to 10, which means the frequency
less than the threshold is labeled as the unknown
context.
The above distributional similarity can be
used as a distance measure. Hence any cluster-
ing algorithm can be adopted. In this paper, we
use the affinity propagation algorithm (Frey and
Dueck, 2007). Its parameter ?dampfact? is set
to 0.9, and the other parameters are set as de-
fault. After running the clustering algorithm, the
contexts are clustered into 1869 clusterings. It
is noted that one word might be classified into
several clusters , if its contexts are clustered into
several clusters.
5 Experiments
As aforementioned, the outputs of our HMM
model are evaluated in two ways, clustering met-
ric and parsing performance. The data used in all
experiments are the Chinese data set in CoNLL-
2007 shared task. The number of tokens in
training, development and test sets are 609,060,
49,620 and 73,153 respectively. We use all train-
ing data set for training the model, whose maxi-
mum length is 242.
The hyper parameters of Dirichlet priors are
initialized in a homogeneous way. The initial
hidden state is set to 40 in all experiments. After
several iterations, the hidden states number con-
verged to 247, which is much larger than the size
of the manually defined POS tags. Our expec-
tation is the refinement variables can reveal the
deep granularity of the POS tags.
5.1 Clustering Evaluation
In this paper, we use information theoretic based
metrics to quantify the information shared by
two clusters. The most common information-
based clustering metric is the variational of In-
formation (VI)(Meila?, 2007). Given the cluster-
ing resultCr and the gold clusteringCg, VI sums
up the conditional entropy of one cluster distri-
bution given the other one:
VI(Cr,Cg) = H(Cr) + H(Cg) ? 2I(Cr,Cg)
= H(Cr |Cg) + H(Cg|Cr)
(10)
where H(Cr) is the entropy associated with the
clustering Cr, and mutual information I(Cr,Cg)
quantifies the mutual dependence between two
clusterings, or say the shared information be-
tween two variables. It is easy to see that
VI? [0, log(N)], where N is the number of data
points. However, the standard VI is not normal-
ized, which favors clusterings with a small num-
ber of clusters. It can be normalized by divid-
ing by log(N), because the number of training
instances are fixed. However the normalized VI
score is misleadingly large, if the N is very large
which is the case in our task. In this paper only
un-normalized VI scores are reported to show the
score ranking.
To standardize the measures to have fixed
bounds, (Strehl and Ghosh, 2003) defined the
normalized Mutual Information (NMI) as:
NMI(Cr,Cg) =
I(Cr,Cg)
?
H(Cr)H(Cg)
(11)
NMI takes its lower bound of 0 if no information
is shared by two clusters and the upper bound
of 1 if two clusterings are identical. The NMI
however, still has problems, whose variation is
sensitive to the choice of the number of clusters.
Rosenberg and Hirschberg (2007) proposed
V-measure to combine two desirable properties
of clustering: homogeneity (h) and completeness
(c) as follows:
h = 1 ? H(Cg|Cr)/H(Cg)
c = 1 ? H(Cr |Cg)/H(Cr)
V = 2hc/(h + c)
(12)
Generally homogeneity and completeness
runs in opposite way, whose harmonic mean (i.e.
V-measure) is a comprise score, just like F-score
for the precision and recall.
Let us first examine the contextual word clus-
tering performance. The VI score between dis-
tributional word categories and gold standard is
2.39. The NMI and V-measure score are 0.53
and 0.48, respectively.
The clustering performance of the HMM out-
puts are reported in Figure 2. The best VI
score achieved was 3.9524, while V-measure
was 62.09% and NMI reached 0.8051. Previous
40 60 80 100 120 140 160 180 200 220 2403.8
4
4.2
4.4
4.6
4.8
5
(a) VI score
40 60 80 100 120 140 160 180 200 220 2400
0.1
0.2
0.3
0.4
0.5
0.6
0.7
 
 
NMIhomogeneitycompletenessV?measure
(b) normalized scores
Figure 2: Clustering evaluation metrics against number of hidden states
work of Chinese tagging focuses on the tagging
accuracies, e.g. Wang (Wang and Schuurmans,
) and Huang et al (Huang et al, 2007). To
our knowledge, this is the first work to report
the distributional clustering similarity measures
based on informatics view for Chinese . Simi-
lar works can be found on English of WSJ cor-
pus (Van Gael et al, 2009). Their best results of
VI, V-measure, achieved with Pitman-Yor prior,
were 3.73 and 59%. We believe the Chinese re-
sults are not good as English correspondences
because of the rich unknown words in Chinese
(Tseng et al, 2005).
5.2 Dependency Parsing Evaluation
The next experiment is to test the goodness of the
outcome states of our model in the context of real
tasks. In this work, we consider unsupervised
dependency parsing for a fully unsupervised sys-
tem. The dependency parsing is to extract the
dependency graph whose nodes are the words of
the given sentence. The dependency graph is a
directed acyclic graph in which every edge links
from a head word to its dependent. Because we
work on unsupervised methods in this paper, we
choose a simple generative head-outward model
(DependencyModel with Valence, DMV) (Klein
and Manning, 2004; Headden III et al, 2009) for
parsing. The data through the experiment is re-
stricted to the sentences up to length 10 (exclud-
ing punctuation).
Because the main purpose is to test the HMM
output rather than to improve the parsing perfor-
mance, we select the original DMV model with-
out extensions or modifications. Starting from
the root, DMV generates the head, and then each
head recursively generates its left and right de-
pendents. In each direction, the possible depen-
dents are repeatedly chosen until a STOP marker
is seen. DMV use inside-outside algorithm for
re-estimation. We choose the ?harmonic? ini-
tializer proposed in (Klein and Manning, 2004)
for initialization. The valence information is the
simplest binary value indicating the adjacency.
For different HMM candidates with varied hid-
den state number, we directly use the outputs as
the input of the DMV and trained a set of models.
Performing test on these individual models, we
report the directed dependency accuracies (the
fraction of words assigned the correct parent) in
Figure 3.
40 60 80 100 120 140 160 180 200 220 24035
40
45
50
55
Figure 3: Directed accuracies for different hid-
den states
It is noted that the accuracy monotonically
increases when the number of states increases.
The most drastic increase happened when state
changes from 40 to 120. The accuracy increased
from 38.56% to 50.60%. If the state number is
larger than 180, the increase is not obvious. The
final best accuracy is 54.20%, which improve the
standard DMV model by 5.6%. Therefore we
can see that the introduction of more annotations
can help the parsing results. However, the im-
provement is limited and stable when the num-
ber of state number is large. To further improve
the parsing performance, one might turn to the
extension of DMV model, e.g. introducing more
knowledge (prior or lexical information) or more
sophistical smoothing techniques. However, the
development of parser is not the focus of this pa-
per.
6 Conclusion and Future Work
This paper works on the unsupervised Chinese
POS tagging based on the first-order HMM. Our
contributions are: 1). The number of hidden
states can be adjusted to fit the data. 2). For in-
ference, we use the variational inference, which
is faster and is guaranteed theoretically to con-
vergence. 3). To overcome the context limitation
in HMM, the words are clustered based on dis-
tributional similarities. It is a 1-to-many cluster-
ing, which means one word might be classified
into different clusters under different contexts.
Finally, experiments show the hidden states are
correlated to the latent annotations of the stan-
dard POS tags.
The future work includes to improve the per-
formance by incorporating a small amount of su-
pervision. The typical supervision used before
is dictionary extracted from a large corpus like
Chinese Gigaword. Another interesting idea is
to select some exemplars (Haghighi and Klein,
2006).
References
Beal, Matthew J., Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The infinite hidden
markov model. In NIPS, pages 577?584.
Beal, M. J. 2003. Variational algorithms for
approximate bayesian inference. Phd Thesis.
Gatsby Computational Neuroscience Unit, Uni-
versity College London.
Brand, Matthew. 1999. An entropic estimator for
structure discovery. In Proceedings of the 1998
conference on Advances in neural information pro-
cessing systems II, pages 723?729, Cambridge,
MA, USA. MIT Press.
Fox, Emily B., Erik B. Sudderth, Michael I. Jordan,
and Alan S. Willsky. 2008. An hdp-hmm for sys-
tems with state persistence. In ICML ?08: Pro-
ceedings of the 25th international conference on
Machine learning.
Frey, Brendan J. and Delbert Dueck. 2007. Clus-
tering by passing messages between data points.
Science, 315:972?976.
Goldwater, Sharon and Tom Griffiths. 2007. A
fully bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 744?751, Prague, Czech Republic,
June. Association for Computational Linguistics.
Haghighi, Aria and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 320?327.
Headden, III, William P., David McClosky, and Eu-
gene Charniak. 2008. Evaluating unsupervised
part-of-speech tagging for grammar induction. In
COLING ?08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 329?336, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Headden III, William P., Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 101?109, Boulder, Colorado,
June. Association for Computational Linguistics.
Hernando, D., V. Crespi, and G. Cybenko. 2005. Ef-
ficient computation of the hidden markov model
entropy for a given observation sequence. vol-
ume 51, pages 2681?2685.
Huang, Zhongqiang, Mary Harper, and Wen Wang.
2007. Mandarin part-of-speech tagging and dis-
criminative reranking. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
1093?1102, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Johnson, Mark. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the 2007
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Nat-
ural Language Learning (EMNLP-CoNLL), pages
296?305, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Kaji, Nobuhiro and Masaru Kitsuregawa. 2008. Us-
ing hidden markov random fields to combine dis-
tributional and pattern-based word clustering. In
COLING ?08: Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics,
pages 401?408, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
478?485, Barcelona, Spain, July.
Meila?, Marina. 2007. Comparing clusterings?an in-
formation based distance. volume 98, pages 873?
895.
Rabiner, Lawrence R. 1989. A tutorial on hidden
markov models and selected applications in speech
recognition. In Proceedings of the IEEE, pages
257?286.
Rosenberg, Andrew and Julia Hirschberg. 2007.
V-measure: A conditional entropy-based exter-
nal cluster evaluation measure. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 410?420.
Siddiqi, Sajid, Geoffrey Gordon, and Andrew Moore.
2007. Fast state discovery for hmm model selec-
tion and learning. In Proceedings of the Eleventh
International Conference on Artificial Intelligence
and Statistics (AI-STATS).
Strehl, Alexander and Joydeep Ghosh. 2003. Clus-
ter ensembles ? a knowledge reuse framework
for combining multiple partitions. Journal of Ma-
chine Learning Research, 3:583?617.
Tseng, Huihsin, Daniel Jurafsky, and Christopher
Manning. 2005. Morphological features help pos
tagging of unknown words across language vari-
eties. pages 32?39.
Van Gael, Jurgen, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for
the infinite hidden markov model. In ICML ?08:
Proceedings of the 25th international conference
on Machine learning.
Van Gael, Jurgen, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 678?687, Singapore, Au-
gust. Association for Computational Linguistics.
Wang, Qin Iris and Dale Schuurmans. Improved es-
timation for unsupervised part-of-speech tagging.
page 2005, Wuhan, China.
Weeds, Julie and David Weir. 2003. A general
framework for distributional similarity. In Pro-
ceedings of the 2003 conference on Empirical
methods in natural language processing, pages
81?88, Morristown, NJ, USA. Association for
Computational Linguistics.
Zelling, Harris. 1968. Mathematical sturcture of lan-
guage. NewYork:Wiley.
APPENDIX
Pseudo-code of the extended Baum-Welch Al-
gorithm in our dynamic HMM
Input: Time step t:
State Candidate: k ? (k(1), k(2)) ;
Sate Number: Nt;
Model Parameter: ?(t) = (A(t), B(t), pi(t));
Initialize
u(l)[k(1), k(2)]? [ u
(l)[k]
2 ,
u(l)[k]
2 ], l ? {A, B, pi}
pik(1) ? 12pik; pik(2) ?
1
2pik
ak?k(i) ? 12ak?k(i) ; ak(i)k? ? ak(i)k?;
ak(i)k( j) ? 12ak(i)k( j) , here i, j ? 1, 2, k? , k
repeat
E step:
update forward: ?t(k(1)) and ?t(k(2))
backward: ?t(k(1)) and ?t(k(2))
update ?t(i, j) and ?t(i); if i, j ? {k(1), k(2)}
update E[ni j] =
?
t ?t(i, j)/
?
t ?t(i)
E[nik] =
?
t,wt=k ?t( j)/
?
t ?t( j)
M step:
update ?(t+1) using equation (7)
until (4F < ?)
Output: ?(t+1), F
Automatic Identification of Predicate Heads in Chinese Sentences 
 
Xiaona Rena   Qiaoli Zhoua   Chunyu Kitb   Dongfeng Caia 
Knowledge Engineering Research Centera 
Shenyang Aerospace University 
Department of Chinese, Translation and Linguisticsb 
City University of Hong Kong 
rxn_nlp@163.com    ctckit@cityu.edu.hk 
 
  
Abstract 
We propose an effective approach to auto-
matically identify predicate heads in Chinese 
sentences based on statistical pre-processing 
and rule-based post-processing. In the pre-
processing stage, the maximal noun phrases in 
a sentence are recognized and replaced by 
?NP? labels to simplify the sentence structure. 
Then a CRF model is trained to recognize the 
predicate heads of this simplified sentence. In 
the post-processing stage, a rule base is built 
according to the grammatical features of 
predicate heads. It is then utilized to correct 
the preliminary recognition results. Experi-
mental results show that our approach is feasi-
ble and effective, and its accuracy achieves 
89.14% on Tsinghua Chinese Treebank. 
1 Introduction 
It is an important issue to identify predicates in 
syntactic analysis. In general, a predicate is con-
sidered the head of a sentence. In Chinese, it 
usually organizes two parts into a well-formed 
sentence, one with a subject and its adjunct, and 
the other with an object and/or complement (Luo 
et al, 1994). Accurate identification of predicate 
head is thus critical in determining the syntactic 
structure of a sentence. Moreover, a predicate 
head splitting a long sentence into two shorter 
parts can alleviate the complexity of syntactic 
analysis to a certain degree. This is particularly 
useful when long dependency relations are in-
volved. Without doubt, this is also a difficult task 
in Chinese dependency parsing (Cheng et al, 
2005). 
Predicate head identification also plays an im-
portant role in facilitating various tasks of natural 
language processing. For example, it enhances 
shallow parsing (Sun et al, 2000) and head-
driven parsing (Collins, 1999), and also improves 
the precision of sentence similarity computation 
(Sui et al, 1998a). There is reason to expect it to 
be more widely applicable to other tasks, e.g. 
machine translation, information extraction, and 
question answering. 
In this paper, we propose an effective ap-
proach to automatically recognize predicate 
heads of Chinese sentences based on a preproc-
essing step for maximal noun phrases 1(MNPs). 
MNPs usually appear in the location of subject 
and object in a sentence. The proper identifica-
tion of them is thus expected to assist the analy-
sis of sentence structure and/or improve the ac-
curacy of predicate head recognition. 
In the next section, we will first review some 
related works and discuss their limitations, fol-
lowed by a detailed description of the task of 
recognizing predicate heads in Section 3. Section 
4 illustrates our proposed approach and Section 5 
presents experiments and results. Finally we 
conclude the paper in Section 6. 
2 Related Works 
There exist various approaches to identify predi-
cate heads in Chinese sentences. Luo and Zheng 
(1994) and Tan (2000) presented two rule-based 
methods based on contextual features and part of 
speeches. A statistical approach was presented in 
Sui and Yu (1998b), which utilizes a decision 
tree model. Gong et al (2003) presented their 
hybrid method combining both rules and statis-
tics. These traditional approaches only make use 
of the static and dynamic grammatical features of 
the quasi-predicates to identify the predicate 
heads. On this basis, Li and Meng (2005) pro-
posed a method to further utilize syntactic rela-
tions between the subject and the predicate in a 
sentence. Besides the above monolingual pro-
posals, Sui and Yu (1998a) discussed a bilingual 
strategy to recognize predicate heads in Chinese 
                                                 
1 Maximal noun phrase is the noun phrase which is not con-
tained by any other noun phrases.  
sentences with reference to those in their coun-
terpart English sentences. 
Nevertheless, these methods have their own 
limitations. The rule-based methods require ef-
fective linguistic rules to be formulated by lin-
guists according to their own experience. Cer-
tainly, this is impossible to cover all linguistic 
situations concerned, due to the complexity of 
language and the limitations of human observa-
tion. In practice, we also should not underesti-
mate the complexity of feature application, the 
computing power demanded and the difficulties 
in handing irregular sentence patterns. For in-
stance, a sentence without subject may lead to an 
incorrect recognition of predicate head. For cor-
pus-based approaches, they rely on language data 
in huge size but the available data may not be 
adequate. Those bilingual methods may first en-
counter the difficulty of determining correct sen-
tence alignment in the case that the parallel data 
consist of much free translation. 
Our method proposed here focuses on a simple 
but effective means to help identify predicate 
heads, i.e., MNP pre-processing. At present, 
there has some substantial progress in automatic 
recognition of MNP. Zhou et al (2000) proposed 
an efficient algorithm for identifying Chinese 
MNPs by using their structure combination, 
achieving an 85% precision and an 82% recall. 
Dai et al (2008) presented another method based 
on statistics and rules, reaching a 90% F-score on 
HIT Chinese Treebank. Jian et al (2009) em-
ployed both left-right and right-left sequential 
labeling and developed a novel ?fork position? 
based probabilistic algorithm to fuse bidirec-
tional results, obtaining an 86% F-score on the 
Penn Chinese Treebank. Based on these previous 
works, we have developed an approach that first 
identifies the MNPs in a sentence, which are then 
used in determining the predicate heads in the 
next stage. 
3 Task  Description 
The challenge of accurate identification of predi-
cate heads is to resolve the problem of quasi-
predicate heads in a sentence. On the one hand, 
the typical POSs of predicate heads in Chinese 
sentences are verbs, adjectives and descriptive 
words 2 . Each of them may have multiple in-
stances in a sentence. On the other hand, while a 
simple sentence has only one predicate head, a 
complex sentence may have multiple ones. The 
                                                 
2 We only focus on Verbs and adjectives in this work. 
latter constitutes 8.25% in our corpus. Thus, the 
real difficulty lies in how to recognize the true 
predicate head of a sentence among so many 
possibilities. 
Take a simple sentence as example: 
?/rN ?/qN ?/v ??/a ??/n ?
/uJDE ?/a ?/n ??/v ??/aD ?
/uJDE ??/v ?/n ?/cC ??/v ?
/n ?/wE 
The quasi-predicate heads (verbs and adjectives) 
include ?/v, ??/a, ?/a, ??/v, ??/v, 
and ??/v. However, there are two MNPs in 
this sentence, namely, ??/rN ?/qN ?/v ??
/a ??/n ?/uJDE ?/a ?/n? and ???/aD 
?/uJDE ??/v ?/n ?/cC ??/v ?/n?. 
These two MNPs cover most quasi-predicate 
heads in the sentence, except ??/v, the true 
predicate head that we want. 
An MNP is a complete semantic unit, and its 
internal structure may include different kinds of 
constituents (Jian et al, 2009). Therefore, the 
fundamental structure of a sentence can be made 
clear after recognizing its MNPs. This can help 
filter out those wrong quasi-predicates for a bet-
ter shortlist of good candidates for the true predi-
cate head in a sentence. 
In practice, the identification of predicate head 
begins with recognizing MNPs in the same sen-
tence. It turns the above example sentence into: 
[ ?/rN ?/qN ?/v ??/a ??/n ?
/uJDE ?/a ?/n ] ??/v [ ??/aD 
?/uJDE ??/v ?/n ?/cC ??/v ?
/n ] ?/wE 
These MNPs are then replaced with the conven-
tional label ?NP? for noun phrase, resulting in a 
simplified sentence structure as follows. 
NP/NP  ??/v  NP/NP ?/wE 
This basic sentence structure can largely allevi-
ates the complexity of the original sentence and 
narrows down the selection scope of quasi-
predicates for the true head. In this particular 
example, the only verb left in the sentence after 
MNP recognition is the true predicate head. 
4 Predicate Head Identification  
This section describes the process of identifying 
predicate heads in sentences. As illustrated in 
Figure 1 below, it can be divided into three steps: 
Step 1: recognize the MNPs in a sentence and 
replace the MNPs with ?NP? label to simplify 
the sentence structure. 
Step 2: recognize the predicate heads in the 
resulted simplified structure. 
Step 3: post-process the preliminary results to 
correct the wrong predicate heads according to 
heuristics in a rule base. 
4.1 MNP Recognition 
The MNP recognition is performed via a trained 
CRF model on unlabeled data. We adopt the 
method in Dai et al (2008), with modified tem-
plates for the different corpus. Each feature is 
composed of the words and POS tags surround-
ing the current word i, as well as different com-
bination of them. The context window of tem-
plate is set to size 3. Table 1 shows the feature 
template we use.  
Type Features 
Unigram Wordi Posi 
Bigram Wordi/Posi  
Surrounding Wordi-1/Wordi Posi-1/Posi 
 Wordi/Wordi+1 Posi/Posi+1 
 Wordi-2/Posi-2 Posi-2/Posi-1 
 Posi-2/Posi-1/Posi Posi-3/Posi-2 
 Posi-1/Posi/Posi+1 Wordi+3/Posi+3 
 Posi+1/Posi+2/Posi+3 Wordi+2/Wordi+3
 
Table 1: Feature Template 
 
Test data Final results 
Preliminar
 
 
Figure 1: Flow Chart of Predicate Head Identification 
 
The main effective factors for MNPs recogni-
tion are the lengths of MNPs and the complexity 
of sentence in question. We analyze the length 
distribution of MNPs in TCT 3  corpus, finding 
that their average length is 6.24 words and the 
longest length is 119 words. Table 2 presents this 
distribution in detail. 
 
Length of MNP Occurrences Percentage (%)
len?5 3260 48.82 
5?len?10 2348 35.17 
len?10 1069 16.01 
 
Table 2: Length Distribution of MNPs in TCT Corpus 
 
The MNPs longer than 5 words cover 50% of 
total occurrences, indicating the relatively high 
complexity of sentences. We trained a CRF 
model using this data set, which achieves an F-
score of 83.7% on MNP recognition. 
4.2 Predicate Head Identification 
After the MNPs in a sentence are recognized, 
they are replaced by ?NP? label to rebuild a sim-
plified sentence structure. It largely reduces the 
difficulty in identifying predicate heads from this 
simplified structure.  
We evaluate our models by their precision in 
the test set, which is formulated as 
                                                 
3 Tsinghua Chinese Treebank ver1.0. 
_
100%
_
right sentences
Precision
Sum sentences
= ?      (1) 
The right_sentences refer to the number of sen-
tences whose predicate heads are successfully 
identified, and the sum_sentences to the total 
number of sentences in the test set. We count a 
sentence as right_sentence if and only if all its 
predicate heads are successfully identified, in-
cluding those with multiple predicate heads. 
For each predicate head, we need an appropri-
ate feature representation f (i, j). We test the 
model performance with different context win-
dow sizes of template. The results are shown in 
Table 3 as follows. 
 
Template Context window size Precision (%) 
Temp1 2 79.27 
Temp2 3 82.59 
Temp3 4 81.37 
 
Table 3: Precisions of Predicate Heads Recognition under 
Different Context Window Sizes 
 
It shows that the window size of 3 words gives 
the highest precision (82.59%). Therefore we 
apply this window size, together with other fea-
tures in our CRF model, including words, POSs, 
phrase tags and their combinations. There are 24 
template types in total. 
4.3 Post-processing 
The post-processing stage is intended to correct 
errors in the preliminary identification results of 
MNP recognition MNP replacement Predicate head recognition y results 
Predicate head recognition model Rule base MNP recognition model 
predicate heads, by applying linguistic rules for-
mulated heuristically. We test each rule to see if 
it improves the recognition accuracy, so as to 
retrieve a validated rule base. The labeling of 
predicate heads follows the standard of TCT and 
a wrong labeling is treated as an error. 
There are three main types of error, according 
to our observation. The first is that no predicate 
head is identified. The second is that the whole 
sentence is recognized as an MNP, such that no 
predicate head is recognized. The third is that the 
predicate head is incorrectly identified, such as  
??? in the expression ???????, where the 
correct answer is ???? according to the TCT 
standard.  
 
Error types Percentage Improved  percentage 
No predicate head 17.50% 2.44% 
a sentence as an MNP 10.63% 1.11% 
??????? 8.75% 0.56% 
Others 63.12% 2.77% 
 
Table 4: Types of Error  
 
Table 4 lists different types of error, together 
with their percentage in all sentences whose 
predicate heads have been mistakenly identified, 
and the improvement in percentage after the 
post-processing. To correct these errors, a num-
ber of rules for post-processing are formulated. 
The main rules are the followings: 
? If no predicate head is recognized in a sen-
tence, we label the first verb as the predi-
cate head. 
Error sample??/p [ ????/m ?/qT ?
???/nR ] ?/f ?/wP [ ??/nS ??/d 
??/v ????/b ???/b ??/n ] ?
/wE 
Corrected??/p [ ????/m ?/qT ??
??/nR ] ?/f ?/wP [ ??/nS ??/d 
??/v ????/b ???/b ??/n ] ?
/wE 
? If the whole sentence is recognized as an 
MNP, such that no predicate head is identi-
fied, we label the first verb as the predicate 
head. 
 Error sample?[ ??/n ??/v ?/n ?
/cC ?/n ?/m ??/n ] ?/wE 
Corrected?[ ??/n ??/v ?/n ?/cC ?
/n ?/m ??/n ] ?/wE 
? For expression ???????, we label ??
?? as the predicate head. 
Error sample?[ ?/rB ?/m ?/qN ??/n ] 
??/v ???/n ?/vC [ ?/d ?/v ??
??/n ?/cC ????/n ??/n ?/uJDE 
???/b ??/n ] ?/wE 
Corrected?[ ?/rB ?/m ?/qN ??/n ] 
??/v ???/n ?/vC [ ?/d ?/v ??
??/n ?/cC ????/n ??/n ?/uJDE 
???/b ??/n ] ?/wE 
There are also other rules in the rule base be-
sides the above ones. For example, if the first 
word of a sentences is ??? or ????, it is la-
beled as the predicate head. 
5 Experiments 
5.1 Data Sets 
Our experiments are carried out on the Tsinghua 
Chinese Treebank (TCT). Every constituent of a 
sentence in TCT is labeled by human expert. We 
randomly extract 5000 sentences from TCT and 
remove those sentences that do not have predi-
cate head. Finally, our data set contains 4613 
sentences, in which 3711 sentences are randomly 
chosen as training data and 902 sentences as test-
ing data. The average length of these sentences 
in training set is 20 words. 
The number of quasi-predicate heads in a sen-
tence is a critical factor to determine the per-
formance of predicate head recognition. Reduc-
ing the number of quasi-predicate heads can im-
prove the recognition precision. Table 5 shows 
the percentage of quasi-predicate heads in train-
ing data before and after MNP replacement. 
 
Number of 
quasi-
predicates 
Percentage before 
MNP replace-
ment(%) 
Percentage after 
MNP replace-
ment(%) 
1 12.50 49.69 
2 19.62 27.22 
3 20.37 12.37 
>3 47.51 10.72 
 
Table 5: The Percentage of Quasi-predicate Heads Before 
and After MNP Replacement 
 
From Table 5, we can see that almost half sen-
tences contain more than three quasi-predicate 
heads. Only 12.5% of sentences have only one 
quasi-predicate head before MNP replacement. 
However, after MNPs are replaced with the ?NP? 
label, only 10.72% contain more than three 
quasi-predicate heads and nearly 50% contain 
only one quasi-predicate head. We have evidence 
that MNP pre-processing can reduce the number 
of quasi-predicate heads and lower the complex-
ity of sentence structures. 
5.2 Results and Discussion 
For comparison purpose, we developed four dif-
ferent models for predicate head recognition. 
Models 1 and 2 are CRF models, the former rec-
ognizing predicate heads directly and the later 
recognizing MNPs at the same time. Model 3 
recognizes predicate heads based on MNP pre-
processing. Model 4 is based on model 3, includ-
ing the post-processing stage. Table 6 shows the 
recognition performance of each model using the 
best context window size. 
 
Model Context window size 
Number of cor-
rect sentences 
Preci-
sion(%) 
model 1 4 680 75.39 
model 2 4 687 76.16 
model 3 3 745 82.59 
model 4 3 804 89.14 
 
Table 6: Performance of Different Models 
 
Comparing these models, we can see that the 
additional feature in model 2 leads to 1% im-
provement in precision over model 1. Moreover, 
the MNP pre-processing in model 3 results in a 
large increase in accuracy, compared to model 1. 
It indicates that the MNP pre-processing does 
improve the precision of recognition. Compared 
with model 3, model 4 achieves a precision even 
6.55% higher, indicating that the post-processing 
is also an effective step for recognition. 
As shown, the performance is affected by the 
effect of MNP recognition. There are three kinds 
of relation between the predicate heads and the 
types of MNP recognition error: 
Relation 1: The whole sentence is recognized 
as an MNP. 
Relation 2: The boundaries of an MNP are in-
correctly recognized and the MNP does not con-
tain the predicate head. 
Relation 3: The boundaries of an MNP are in-
correctly recognized and the MNP contains the 
predicate head. Table 7 shows the distribution of 
these three relations in the recognition errors. 
 
Relation Number of sentences Percentage(%)
Relation 1 17 5.47 
Relation 2  281 90.35 
Relation 3 13 4.18 
 
Table 7: Distribution of the Three Relations in 
Recognition Errors 
In our approach, the errors of relation 1 and 
relation 3 can be solved by the post-processing, 
as presented in Section 4.3. Relation 2 holds the 
largest proportion among the three. But the error 
rate of predicate head recognition only reaches 
31.67% in this case. That is to say, although the 
MNP boundaries are incorrectly recognized, the 
accuracy of predicate head recognition can still 
reach 68.33%. 
Chen (2007) proposed a probabilistic model 
(model 5) for recognizing predicate heads in Chi-
nese sentences. The probabilities of quasi-
predicates are estimated by maximum likelihood 
estimation. A discounted model is used to 
smooth parameters. We compare his model with 
our model 3 using different contextual features 
on TCT corpus. Table 8 shows the comparison 
results.  
The highest precision of model 3 is 82.59% 
when the context window size is set to 3. For 
model 5, it is 70.62% at a context window size of 
4. Experimental results show that the precision of 
our method is about 12% higher than Chen?s. 
 
Context window size Model Precision (%) 
model 5 69.18 2 
model 3 79.27 
model 5 70.18 
3 
model 3 82.59 
model 5 70.62 
4 
model 3 81.37 
 
Table 8: Comparison between model 3 and Chen?s model 
 
Beside Chen?s method, the Stanford Parser 
can also recognize the predicate heads in simple 
Chinese sentences. The root node of dependency 
tree is the predicate head. For a comparison, we 
randomly extract two hundred simple sentences 
in our test data to compare it with the outputs of 
our model 3. We also train a model of predicate 
head recognition (model 6), which assumes that 
all MNPs are successfully identified. The com-
parison is shown in Table 9. We can see that the 
precision of model 6 is 8.35% higher than model 
3. This means that our method still has a certain 
room for further improvement. 
 
Stanford Parser model 3 model6 
78.17% 83.15% 91.5% 
 
Table 9: Comparison between model 3 and Stanford 
Parser 
5.3 Error Analysis 
As shown above, the post-processing can correct 
most errors in the recognition of predicate heads. 
But we also observe some errors that cannot be 
corrected this way. For example, 
???/n?/p ???/n ??/v [ ??/n 
??/n ] ??/v ?/wE 
The predicate head here is ????, but usually 
???? is recognized as the predicate head. This 
is because ???? can be used either as a verb or 
a noun. There are many verbs of this kind in Chi-
nese, such as ??? ? and ??? ?. Mistakes 
caused by the flexibility of Chinese verb and the 
ambiguity of sentence structure appear to deserve 
more of our effort. Meanwhile, there are also 
some other unusual cases that cannot be properly 
solved with statistical methods. 
6 Conclusion 
Identification of predicate heads is important to 
syntactic parsing. In this paper, we have pre-
sented a novel method that combines both statis-
tical and rule-based approaches to identify predi-
cate heads based on MNP pre-processing and 
rule-based post-processing. We have had a series 
of experiments to show that this method achieves 
a significant improvement over some state-of-
the-art approaches. Furthermore, it also provides 
a simple structure of sentence that can be utilized 
for parsing. 
In the future, we will study how semantic in-
formation can be applied to further improve the 
precision of MNP recognition and predicate head 
identification. It is also very interesting to ex-
plore how this approach can facilitate parsing, 
including shallow parsing. 
Acknowledgments 
We would like to thank the anonymous review-
ers for their helpful comments and suggestions. 
We also thank Billy Wong of City University of 
Hong Kong for his much-appreciated input dur-
ing the writing process. 
References  
Zhiqun Chen. 2007. Study on recognizing predicate of 
Chinese sentences. Computer Engineering and 
Applications, 43(17): 176-178. 
Yuchang Cheng, Asahara Masayuki, and Matsumoto 
Yuji. 2005. Chinese deterministic dependency ana-
lyzer: examining effects of global features and root 
node finder. In Proceedings of the Fourth 
SIGHAN Wordshop on Chinese Language 
Processing, pp. 17-24. 
Cui Dai, Qiaoli Zhou, and Dongfeng Cai. 2008. 
Automatic recognition of Chinese maximal-length 
noun phrase based on statistics and rules. Journal 
of Chinese Information Processing, 22(6): 110-
115. 
Xiaojin Gong, Zhensheng Luo, and Weihua Luo. 
2003. Recognizing the predicate head of Chinese 
sentences. Journal of Chinese Information 
Processing, 17(2): 7-13. 
Ping Jian, and Chengqing Zong. 2009. A new ap-
proach to identifying Chinese maximal-length 
phrase using bidirectional labeling. CAAI Trans-
actions on Intelligent Systems, 4(5): 406-413. 
Guochen Li, and Jing Meng. 2005. A method of iden-
tifying the predicate head based on the correpon-
dence between the subject and the predicate. Jour-
nal of Chinese Information Processing, 19(1): 
1-7. 
Zhensheng Luo, and Bixia Zheng. 1994. An approach 
to the automatic analysis and frequency statistics of 
Chinese sentence patterns. Journal of Chinese 
Information Processing, 8(2): 1-9. 
Zhifang Sui, and Shiwen Yu. 1998a. The research on 
recognizing the predicate head of a Chinese simple 
sentence in EBMT. Journal of Chinese Informa-
tion Processing, 12(4): 39-46. 
Zhifang Sui, and Shiwen Yu. 1998b. The acquisition 
and application of the knowledge for recognizing 
the predicate head of a Chinese simple sentence. 
Journal of Peking University (Science Edition), 
34(2-3): 221-229. 
Honglin Sun, and Shiwen Yu. 2000. Shallow parsing: 
an overview. Contemporary Linguistics, 2(2): 
74-83. 
Hui Tan. 2000. Center predicate recognization for 
scientific article. Journal of WuHan University 
(Natural Science Edition), 46(3): 1-3. 
Qiang Zhou, Maosong Sun, and Changning Huang. 
2000. Automatically identify Chinese maximal 
noun phrase. Journal of Software, 11(2): 195-201. 
Michael Collins. 1999. Head-driven statistical 
models for natural language parsing. Ph. D. 
Thesis, University of Pennsylvania. 
Active Learning Based Corpus Annotation 
Hongyan Song1 and Tianfang Yao2
Shanghai Jiao Tong University 
Department of Computer Science and Engineering 
Shanghai, China 200240 
1songhongyan@sjtu.org 
2yao-tf@cs.sjtu.edu.cn 
Abstract
Opinion Mining aims to automatically acquire 
useful opinioned information and knowledge 
in subjective texts. Research of Chinese Opin-
ioned Mining requires the support of annotated 
corpus for Chinese opinioned-subjective texts. 
To facilitate the work of corpus annotators, 
this paper implements an active learning based 
annotation tool for Chinese opinioned ele-
ments which can identify topic, sentiment, and 
opinion holder in a sentence automatically. 
1 Introduction 
Opinion Mining is a novel and important re-
search topic, aiming to automatically acquire 
useful opinioned information and knowledge in 
subjective texts (Liu et al 2008). This technique 
has wide and many real world applications, such 
as e-commerce, business intelligence, informa-
tion monitoring, public opinion poll, e-learning, 
newspaper and publication compilation, and 
business management. For instance, a typical 
opinion mining system produces statistical re-
sults from online product reviews, which can be 
used by potential customers when deciding 
which model to choose, by manufacturers to find 
out the possible areas of improvement, and by 
dealers for sales plan evaluation (Yao et al 
2008).
   According to Kim and Hovy (2004), an opin-
ion is composed of four parts, namely, topic, 
holder, sentiment, and claim, in which the holder 
expresses the claim including positive or nega-
tive sentiment towards the topic. For example, in 
the sentence I like this car, I is the holder, like is 
the positive sentiment, car is the topic, and the 
whole sentence is the claim. 
   Research on Chinese opinion mining technol-
ogy requires the support of annotated corpus for 
Chinese opinioned-subjective text. Since the cor-
pus includes deep level information related to 
word segmentation, part-of-speech, syntax, se-
mantics, opinioned elements, and some other 
information, the finished annotation is very com-
plicated. Hence, it is necessary to develop an 
automatic tool to facilitate the work of annotators 
so that the efficiency and accuracy of annotation 
can be improved. 
   When developing the automatic annotation tool, 
we find it is most difficult for the tool to annotate 
opinioned elements automatically. Because 
unlike other elements such as part-of-speech, and 
dependency relationship that needed to be anno-
tated in the corpus, there is no available tool that 
can identify opinioned elements automatically. 
Special classifiers should be constructed to solve 
this problem. 
   In traditional supervised learning tasks, train-
ing process consumes all the available annotated 
training instances, so a classifier with high classi-
fication accuracy might be constructed. When 
training a classifier for opinioned elements, it is 
very expensive and time-consuming to get anno-
tated instances. On the other hand, unannotated 
instances are abundant in this case, because all 
the texts in the corpus can be regarded as unan-
notated instances before being annotated. This 
scenario is very appropriate for active learning 
application. An active learning algorithm picks 
up the instances which will improve the per-
formance of the classifier to the largest extent 
into the training set, and often produce classifier 
with higher accuracy using less training instances. 
   Active learning algorithm is featured with 
smaller training set size, less influence from un-
balanced training data and better classification 
performance comparing to classical learning al-
gorithm. This paper experimentally demonstrates 
the validity of active learning algorithm when 
used for opinioned elements identification and 
proposes a computational method for overall sys-
tem performance evaluation which consists of F-
measure, training time, and number of training 
instances.
2 Related Work 
Common active learning algorithms can be di-
vided into two classes, membership query and 
selective sampling (Dagan and Engelson, 1995).
For membership query, algorithm constructs 
learning instances by itself according to the 
knowledge learnt, and submits the instances for 
human processing (Angluin, 1988) (Sammut and 
Banerji, 1986) (Shapiro, 1982). Although this 
method has proved high learning efficiency (Da-
gan and Engelson, 1995), it can be applied in 
fewer scenarios. Since constructing meaningful 
training instance without the knowledge of target 
concept is rather difficult. As to selective sam-
pling, algorithm picks up training instances 
which can improve the performance of the classi-
fier to the largest extent from a large variety of 
available instances. Algorithm in this class can 
be further divided into stream-based algorithm 
and pool-based algorithm according to how in-
stances are saved (Long et al 2008). For stream-
based algorithm (Engelson and Dagon, 1999) 
(Freund et al 1997), unannotated instances are 
submitted to the system successively. All the 
instances not selected by the algorithms will be 
discarded. As to pool-based algorithm (Muslea et 
al, 2006) (McCallum and Nigam, 1998) (Lewis
and Gail, 1994), the algorithm choose the most 
appropriate training instances from all the avail-
able instances. Instance not selected might have 
chance to be picked up in the next round. Though 
its computational complexity is higher, selective 
sampling is widely used as an active learning 
method for no prior knowledge of the target con-
cept is required. 
Although much research has been made in 
the field, we found no case which deals with 
multi-classification problem in active learning. 
Besides, there is no available method to evaluate 
the performance of active learning in information 
extraction.
3 Active Learning Based Corpus Anno-
tation
3.1 System Structure 
The pool-based active learning algorithm is 
composed of two main parts: a learning engine 
and a selecting engine (Figure 1). The learning 
engine uses instances in the training set to im-
prove the performance of the classifier. The se-
lecting engine picks up unannotated instances 
according to preset rules, submits these instances 
for human annotation, and incorporates these 
instances into the training set after the annotation 
is completed. The learning engine and the select-
ing engine work in turns. The performance of the 
classifier tends to improve with the increasing of 
the training set size. When the preset condition is 
met, the training process will finish. 
Figure 1 System Workflow 
For our active learning based annotation tool, 
the workflow is as follows. 
1. Convert raw texts into the format which 
the algorithm can deal with. 
2. Selecting engine picks up instances which 
are expected to improve the performance of the 
classifier to the largest extent. 
3. Annotate these instances manually. 
4. Learning engine incorporate these anno-
tated instances into the training set, and use the 
new training set to train the classifier. 
5. Find out whether the performance of the 
classifier satisfies the preset standard. If not, go 
to step 2. 
6. Use the classifier to identify the opinioned 
element in the unannotated dataset. 
7. Convert the result into the required format. 
3.2 Learning Engine 
The learning engine maintains the classifier by 
iteratively training classifiers with new training 
sets. The classifier adopted determines the up 
limit of the system performance. We use Support 
Vector Machine (SVM) (Vapnik, 1995) (Boser et
al, 1992) (Chang and Lin, 1992) as the classifier 
for our system for its high generalization per-
formance even with feature vectors of high di-
mension and its ability to manage kernel func-
tions that map input data to higher dimensional 
space without increasing computational com-
plexity. 
3.3 Selecting Engine 
In our system, selecting engine picks up in-
stances for human annotation, and puts the anno-
tated instance into the training set. The strategy 
adopted when selecting training instance is criti-
cal to the overall performance of the active learn-
ing algorithm. A good strategy will more likely 
to produce a classifier with high accuracy from 
less training instances. 
The strategy we adopted here is to choose the 
instances which the classifier is most unsure 
about which class they belong to. For a linear bi-
classification SVM, these instances are the ones 
closest to the separating hyper plane. That means, 
the selecting engine will choose training in-
stances according to their geometric distances to 
the hyper plane. The instance with least distance 
will be selected as the next instance to be added 
into the training set while the other instances will 
be saved for future reference. 
The computational complexity of getting the 
distance between an instance and the hyper plane 
is low. However, this method can not be applied 
to SVM with non-linear kernel for geometric 
distances are meaningless in these cases. We use 
radial basis function, which is non-linear, as the 
kernel function in our system for it outperforms 
linear kernel in the experiment. Hence, we must 
find another method to pick up training instances. 
Non-linear SVM decides the class an in-
stance belongs to according to its decision func-
tion value.
S
( ) ( )
s
s s s
x
y x y K x x bD
?
 ??&
& & &               (1)       
The instance will be classified into one cer-
tain class if , or the other class 
if . However, it will be difficult to clas-
sify the instance according to SVM theory 
if
( ) 0y x !&
( ) 0y x &
( ) 0y x  & . Hence, we may deduce that SVM is 
most unsure when classifying an instance with 
least absolute decision function value. 
We define the Predict Value (PV) as the 
value based on which selecting engine picks up 
training instances. 
For bi-classification SVM, we have PV 
equals to the absolute decision function value, 
namely, 
PV( ) ( )x
&
y x
&
                                       (2) 
Instances with the minimum PV will be selected 
into the training set before other instances. 
For example, if we want to identify all the 
topics in the sentence,  
I like this car very much, but the price is a little 
bit too high. 
????????????????
The PV of each instance in the sentences are 
listed in Table 1. They are calculated from the 
decision function of the SVM gained from the 
last round of iteration.  
Instances PV
?   I 0.260306643320642 
?   very 0.553855024703612 
?? like 0.427269428974918 
?   this 0.031682276068012 
?   type 0.366598504697780 
?   car 0.095961213527654 
? 0.178633448748979 
?? but 0.092571306234562 
?? price 0.052164989563922 
?   high 0.539913276317129 
?   (auxiliary word) 0.458036102580422 
?   a little bit 0.439936293288062 
? 0.375263535139242 
Table 1 Example of 2-Classification SVM 
Predict Value 
Suppose all the instances in this sentence 
have not been added into the training set. This
(0.0316), price (0.0521), and but (0.0925) will be 
selected into the training set successively for 
they have the minimal PVs. 
For multi-classification SVM, it will be more 
complicated to find the training instances. Be-
cause common multi-classification SVM is im-
plemented by voting process (Hsu and Lin, 2002),
there are
1
( 1)
2
t t?  decision function values in t-
classification SVM. 
In our system, we need to classify instances 
into 4 classes, namely, topic, holder, sentiment
and other. So a 4-classification SVM is adopted. 
Suppose for an instance, we get 6 Decision Func-
tion Values from 6 bi-classification SVMs as in 
Table 2. 
No. Classification Decision Function Value Result
1 Class 0 Vs Class 1 1.00032792289507 0
2 Class 0 Vs Class 2 0.999999993721249 0
3 Class 0 Vs Class 3 1.00032792289507 0
4 Class 1 Vs Class 2 0.106393804825973 1
5 Class 1 Vs Class 3 -5.20417042793042E-18 3
6 Class 2 Vs Class 3 -0.106393804825973 3
Table 2 Example of 4-Classification SVM Decision 
Process
For each bi-classification SVM, the class in-
stance belongs to is determined by whether the 
decision function value is greater than or less 
than zero. The instance in Table 2 belongs to 
Class 0 since there 3 votes out of 6 votes for 
Class 0. When deciding which class an instance 
belongs to, only the decision function values 
from bi-classification SVMs with correct votes 
will work on the certainty of the final result. 
Hence, we define Predict Value for multi-
classification SVMs as the arithmetic mean value 
of the absolute decision function value of every 
bi-classification SVM with correct vote, 
          
^
t
1, bi classification SVMs with correct votes
1
( ) y ( )
k
t t `
x x
k  ?
?
&

39  
&
ir
      (3) 
For the instance in Table2, the value is calculated 
from the decision function values from bi-
classification SVMs numbered 1, 2, and 3. 
3.4 Experiments 
To prove the validity of active learning algorithm 
and find out the relations between the perform-
ance of the classifiers and the way the classifiers 
are trained, we carried out batches of experi-
ments.
In most information extraction tasks, a word 
and its context are considered a learning sample, 
and encoded as feature vectors. In our experi-
ments, context data includes the part-of-speech 
tag, dependency relation, word semantic mean-
ing, and word disambiguation information of the 
word being classified, its neighboring words and 
its parent word in dependency grammar. Part-of-
speech tag and dependency relation are common 
features for Chinese Natural Language Process-
ing (NLP) tasks1. We get word semantic mean-
ing from HowNet, which is an online common-
sense knowledge base unveiling inter-conceptual
relations and inter-attribute relations of concepts
as connoting in lexicons of the Chinese and the
English equivalents (Zhendong Dong and Qiang 
Dong, 1999). Given an occurrence of a word in 
natural language text, word sense disambiguation 
is the process of identifying which sense of the 
word is intended if the word has a number of dis-
tinct senses. According to Song and Yao (2009), 
this information may help in Chinese NLP tasks 
such as topic identification. 
Lack of explicit boundary between training 
instances and testing instances is a great differ-
ence between common machine learning algo-
rithm and learning algorithm designed for corpus 
annotation. For common machine learning algo-
rithm such as human face recognition, the quan-
tity of training instances is limited while the test-
ing instances could be infinite. It is unnecessary 
and impossible to annotate all the testing in-
stances. However, when annotating a corpus, all 
the texts need to be annotated are decided be-
forehand. Although tools automated part of the 
annotation process, the results still need to be 
reviewed for several times to ensure the quality 
of annotation. That means in an annotation sce-
nario, all the data to be processed are available 
during the training stage. 
The raw texts used in our experiments are 
taken from forums of chinacars.com. These texts 
include explicit subjective opinion and informal 
network language, which are necessary for opin-
ion mining research. Most of them are comments 
composed of one or more sentences on certain 
type of vehicle. The detailed opinion elements 
distributions are showed in table 3. 
We use all the texts as testing data set and a 
subset of it as a training data set. First of all, we 
pick up 10 instances for each class, and train a 
simple classification model with them. Then, the 
baseline system picks up k instances in sequence 
and adds them into the training data set to train a 
new classification model iteratively until the 
training data set is as large as the testing data set, 
1 We use Language Technology Platform (LTP), developed 
by Center for Information Retrieval, Harbin Institute of 
Technology, for part-of-speech tagging, dependency rela-
tionship analysis and word sense disambiguation in our 
experiment.
while the active learning system picks up in-
stances according to the strategy in Chapter 3.3.  
Type No. of Instances
Topic 638
Sentiment 769
Holder 46
Other 1500
Total 2953
Table 3 Detailed Information of the Data Set 
We use three bi-classification model to test 
the performance of the active learning system on 
topic, sentiment, and holder identification sepa-
rately and a four-classification model to identify 
the three opinion elements simultaneously. The 
results of the experiments are illustrated in Fig-
ure 2, 3, 4, and 5 respectively. Table 4, 5, and 6 
provide the detailed F-measure trends while dif-
ferent numbers of instances are added into the 
training data set in each rounds. For each ex-
periment, we try to compare the performances 
when we add different number of instances into 
the training data set in each round of iteration. 
Figure 2 Topic Identification 
Figure 3 Sentiment Identification 
Figure 4 Holder Identification 
Figure 5 All Opinion Elements Identification 
As are illustrated in the figures, the active 
learning system can always achieve better or at 
least no worse performance than baseline system. 
For example, when adding 200 instances in each 
round for topic identification task (Figure2 and 
Table 4), the active learning system reaches its 
peak value in F-measure (0.8644) with only 600 
training instances. This F-measure value is even 
higher than the value the baseline system get 
(0.8604) after taking all the 2953 training in-
stances.
The active learning system outperforms the 
baseline system greatly especially when dealing 
with unbalanced data set (Figure 4 and Table 4). 
In opinion holder identification task, the baseline 
system can not find any holder until 1600 train-
ing instances are taken while the active learning 
system reaches its peak F-measure value (0.8810) 
with only 600 training instances. That means 
when using active learning algorithm, it is possi-
ble for us to save some time for optimizing the 
parameters when dealing with unbalanced data. 
The number of instances added to the training 
data set in each round (k) influences the perform-
ance of the active learning algorithm in a large 
extent. When a smaller value is assigned to k, the 
active learning system will tend to achieve better 
F-measure (Table 4) with less training instances 
comparing to the baseline system. Advantages of 
the active learning system will be diminished by 
the increase in k (Table 6). 
4 Evaluation of Active Learning Algo-
rithm
For active learning algorithm based on member-
ship query, its training process will probably take 
longer time by the time the optimum classier is 
found, since the training process consists of sev-
eral rounds of iteration. At the beginning of the 
iteration, the classification speed of the model is 
much faster due to less training instances are 
used and the model is simple. With more and 
more training instances are added into the train-
ing data set, the model will become more com-
plex and more time will be needed for classifica-
? 
Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
200 0.7118 0.6221  0.6481 0.0103 0.0000 0.0000 0.6968  0.3874 
400 0.8072 0.8287  0.7344 0.6239 0.0000 0.0000 0.7691  0.7336 
600 0.8237 0.8644  0.7845 0.7860 0.0000 0.8810 0.7907  0.7979 
800 0.8250 0.8625  0.7876 0.8133 0.0000 0.8810 0.8020  0.8240 
1000 0.8386 0.8613  0.7878 0.8189 0.0000 0.8810 0.8101  0.8378 
1200 0.8389 0.8588  0.7992 0.8153 0.0000 0.8810 0.8128  0.8377 
1400 0.8489 0.8588  0.8011 0.8141 0.0000 0.8810 0.8178  0.8471 
1600 0.8450 0.8581  0.8033 0.8150 0.0426 0.8810 0.8211  0.8468 
1800 0.8521 0.8581  0.8059 0.8183 0.1224 0.8810 0.8271  0.8479 
2000 0.8528 0.8585  0.8169 0.8197 0.6857 0.8810 0.8348  0.8481 
2200 0.8560 0.8583  0.8109 0.8200 0.8101 0.8810 0.8372  0.8468 
2400 0.8592 0.8592  0.8186 0.8195 0.8395 0.8810 0.8404  0.8474 
2600 0.8620 0.8610  0.8165 0.8205 0.8675 0.8810 0.8440  0.8463 
2800 0.8578 0.8610  0.8138 0.8177 0.8810 0.8810 0.8464  0.8443 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 
Table 4 F-measure Trends when k=200 

Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
500 0.8198 0.7730  0.7616 0.1369 0.0000 0.0000 0.7831  0.5173 
1000 0.8386 0.8508  0.7878 0.7566 0.0000 0.8837 0.8101  0.7776 
1500 0.8468 0.8592  0.8039 0.8175 0.0833 0.8810 0.8194  0.8398 
2000 0.8528 0.8610  0.8169 0.8183 0.6857 0.8810 0.8348  0.8484 
2500 0.8626 0.8583  0.8168 0.8205 0.8395 0.8810 0.8427  0.8463 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 

Table 5  F-measure Trends when k=500

Topic Sentiment Holder All Three Elements No. of    
Instances Baseline
Active 
Learning 
Baseline
Active 
Learning
Baseline
Active 
Learning
Baseline 
Active 
Learning
1000 0.8386 0.8335  0.7878 0.3514 0.0000 0.0000 0.8101  0.7534 
2000 0.8528 0.8581  0.8169 0.8170 0.6857 0.8810 0.8348  0.8376 
2953 0.8604 0.8604  0.8183 0.8183 0.8810 0.8810 0.8446  0.8446 

Table 6  F-measure Trends when k=1000
tion. On account of the features of active learn-
ing algorithm, we believe it is necessary to find a 
way to balance the performance of the classifier 
and the time it take in training process for a thor-
ough evaluation of the algorithm. 
We define the measurement for time as: 
k
T
C
                                                 (4) 
where C is the number of all the possible training 
instances available, k is the number of training 
instances added into the training data set in each 
round of iteration. T is the approximate value of 
the inverse ratio of the time it takes for training 
process. T will have a greater value if the training 
process takes less time. Its range is (0, 1] just 
similar to F-measure. 
We define the measurement for the training 
instances used as: 
(1 )
n
K
C
  (5) 
where n is the number of the training instances 
actually used. K will have a greater value if less 
training instances are used in the training process. 
The range of K is [0, 1). 
To judge the overall performance of an active 
learning algorithm, we consider the F-measure 
(F) of the classifier, the time it takes during the 
training process, and the training instances used. 
We define the Active Learning Performance 
(ALP) as the harmonic mean of the three aspects: 
1
( )
(6)
( ) ( )
ALP
K F T
F k C n
F C k k C n F C C n
D E J
D E J
 
 
? ?  
? ? ?  ?   ? ? 
where + + =1D E J , and > @, , 0,1D E J ? . They 
are the weights for the three measurements. The 
greater the value of a certain weight is, the more 
important the measurement is in the overall per-
formance. The greater the value of the ALP is, 
the better the performance of the active learning 
algorithm. For instance, when training a classi-
fier for sentiment identification using active 
learning algorithm, we get a classifier with F-
measure of 0.8189 using 1000 training instances 
and a classifier with F-measure of 0.8200 using 
2200 training instances (Table 4). Sup-
pose
1
= = =
3
D E J , we calculate the value of ALP
for the two cases according to equation (6) and 
get 0.1714 and 0.1507 as results respectively. 
That means a people with no preference among 
F-measure, the number of training instances 
adopted and the time used during training proc-
ess will choose to get a classifier with less train-
ing instances, less training time and less F-
measure value. 
5 Conclusion
This paper experimentally demonstrates the va-
lidity of active learning algorithm when used for 
opinioned elements identification and proposed a 
computational method for overall system per-
formance evaluation which consists of F-
measure, training time, and number of training 
instances. According to our tests, active learning 
algorithm outperforms the base line system in 
most of the cases especially when fewer in-
stances are added into the training data set in 
each round of iteration. However, the method 
could extent the training time in a large scale. To 
balance the pros and cons of active learning algo-
rithm, it might be helpful to adjust the number of 
training instances added in each round dynami-
cally in the training process. For instance, add 
less training instances at the beginning of the 
training process to ensure a high peak value of F-
measure could be achieved and add more train-
ing instances later so that time spent on training 
process could be reduced. 
Acknowledgments  
The author of this paper would like to thank In-
formation Retrieval Lab, Harbin Institute of 
Technology for providing the tool (LTP) used in 
experiments. This research was supported by 
National Natural Science Foundation of China 
Grant No.60773087.  
References
Andrew K. McCallum, Kamal Nigam. 1998. Employ-
ing EM in Pool-based Active Learning for Text 
Classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning.
Bernhard E. Boser, Isabelle M. Guyon, and Vladimir 
N. Vapnik. 1992. A Training Algorithm for Opti-
mal Margin Classifiers. In Proceedings of the Fifth 
Annual Workshop on Computational Learning 
Theory.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: 
a library for support vector machines. Software 
available at http://www.csie.ntu.edu.tw/~cjlin/ 
libsvm 
Chih-Wei Hsu and Chih-Jen Lin. 2002. A Compari-
son of Methods for Multi-class Support Vector 
Machines. IEEE Transactions on Neural Networks.
Claude Sammut and Ranan B. Banerji. 1986. Learn-
ing Concepts by Asking Questions. Machine 
Learning: An Artificial Intelligence Approach,
1986, 2: 167-191 
Dana Angluin. 1988. Queries and Concept Learning. 
Machine Learning, 1988, 2(4): 319-342 
David D. Lewis, William A. Gail. 1994. A Sequential 
Algorithm for Training Text Classifiers. In Pro-
ceedings of the 17th Annual International ACM 
SIGIR Conference on Research and Development 
in Information Retrieval.
Ehud Y. Shapiro. 1982. Algorithmic Program Debug-
ging. M.I.T. Press. 
Ido Dagan, Sean P. Engelson. 1995. Committee-
Based Sampling for Training Probabilistic Classi-
fiers. In Proceedings of the International Confer-
ence on Machine Learning.
Ion Muslea, Steven Minton, Craig A. Knoblock. 2006. 
Active Learning with Multiple Views. Journal of 
Artificial Intelligence Research, 2006, 27(1): 203-
233. 
Quansheng Liu, Tianfang Yao, Gaohui Huang, Jun 
Liu, Hongyan Song. 2008. A Survey of Opinion 
Mining for Texts. Journal of Chinese Information 
Processing. 2008, 22(6):63-68. 
Jun Long, Jianping Yin, En Zhu, and Wentao Zhao. A 
Survey of Active Learning. 2008. Journal of Com-
puter Research and Development, 2008, 45(z1): 
300-304. 
Shlomo A. Engelson, Ido Dagon. 1999. Committee-
based Sample Selection for Probabilistic Classifi-
ers. Journal of Artificial Intelligence Research,
1999, 11: 335-360. 
Hongyan Song, Jun Liu, Tianfang Yao, Quansheng 
Liu, Gaohui Huang. 2009. Construction of an An-
notated Corpus for Chinese Opinioned-Subjective 
Texts. Journal of Chinese Information Processing,
2009, 23(2): 123-128. 
Hongyan Song and Tianfang Yao. 2009. Improving 
Chinese Topic Extraction Using Word Sense Dis-
ambiguation Information. In Proceedings of the 4th 
International Conference on Innovative Computing, 
Information and Control.
Soo-Min Kim and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. In Proceedings of the 
Conference on Computational Linguistics: 1367-
1373. 
Tianfang Yao, Xiwen Cheng, Feiyu Xu, Hans 
Uszkoreit, and Rui Wang. 2008. A Survey of Opin-
ion Mining for Texts. Journal of Chinese Informa-
tion Processing, 2008, 22(3): 71-80. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer.  
Yoav Freund, H.Sebastian Seung, Eli Shamir, Naftali 
Tishby. 1997. Selective Sampling Using the Query 
by Committee Algorithm. Machine Learning,
28(2-3): 133-168 
Zhendong Dong and Qiang Dong. 1999. HowNet. 
http://www.keenage.com 
 Combine Person Name and Person Identity Recognition and Docu-
ment Clustering for Chinese Person Name Disambiguation 
Ruifeng Xu1,2,Jun Xu1,Xiangying Dai1
Harbin Institute of Technology,  
Shenzhen Postgraduate School, China 
{xuruifeng.hitsz;hit.xujun; 
mi-chealdai}@gmail.com 
     Chunyu Kit2 
         2City University of Hong Kong,    
        Hong Kong, China 
    ctckit@cityu.edu.hk 
 
Abstract 
This paper presents the HITSZ_CITYU 
system in the CIPS-SIGHAN bakeoff 
2010 Task 3, Chinese person name dis-
ambiguation. This system incorporates 
person name string recognition, person 
identity string recognition and an agglo-
merative hierarchical clustering for 
grouping the documents to each identical 
person. Firstly, for the given name index 
string, three segmentors are applied to 
segment the sentences having the index 
string into Chinese words, respectively. 
Their outputs are compared and analyzed. 
An unsupervised clustering is applied 
here to help the personal name recogni-
tion. The document set is then divided 
into subsets according to each recog-
nized person name string. Next, the sys-
tem identifies/extracts the person identity 
string from the sentences based on lex-
icon and heuristic rules. By incorporat-
ing the recognized person identity string, 
person name, organization name and 
contextual content words as features, an 
agglomerative hierarchical clustering is 
applied to group the similar documents 
in the document subsets to obtain the fi-
nal person name disambiguation results. 
Evaluations show that the proposed sys-
tem, which incorporates extraction and 
clustering technique, achieves encourag-
ing recall and good overall performance. 
1 Introduction 
Many people may have the same name which 
leads to lots of ambiguities in text, especially for 
some common person names. This problem puz-
zles many information retrieval and natural lan-
guage processing tasks. The person name ambi-
guity problem becomes more serious in Chinese 
text. Firstly, Chinese names normally consist of 
two to four characters. It means that for a two-
character person name, it has only one character 
as surname to distinguish from other person 
names with the same family name. It leads to 
thousands of people have the same common 
name, such ??  and ?? . Secondly, some 
three-character or four-character person name 
may have one two-character person name as its 
substring such as ?? and ???, which leads 
to more ambiguities. Thirdly, some Chinese per-
son name string has the sense beyond the person 
name. For example, a common Chinese name, 
?? has a sense of ?Peak?. Thus, the role of a 
string as person name or normal word must be 
determined. Finally, Chinese text is written in 
continuous character strings without word gap. It 
leads to the problem that some person names 
may be segmented into wrong forms.  
In the recent years, there have been many re-
searches on person name disambiguation 
(Fleischman and Hovy 2004; Li et al 2004; Niu 
et al 2004; Bekkerman and McCallum 2005; 
Chen and Martin 2007; Song et al 2009). To 
promote the research in this area, Web People 
Search (WePS and WePS2) provides a standard 
evaluation, which focuses on information extrac-
tion of personal named-entities in Web data (Ar-
tiles et al, 2007; Artiles et al, 2009; Sekine and 
Artiles, 2009). Generally speaking, both cluster-
based techniques which cluster documents cor-
responding to one person with similar contexts, 
global features and document features (Han et al 
2004; Pedersen et al 2005; Elmacioglu et al 
2007; Pedersen and Anagha 2007; Rao et al 
2007) and information extraction based tech-
niques which recognizes/extracts the description 
features of one person name (Heyl and Neumann 
2007; Chen et al 2009) are adopted. Consider-
ing that these evaluations are only applied to 
English text, CIPS-SIGHAN 2010 bakeoff pro-
posed the first evaluation campaign on Chinese 
person name disambiguation. In this evaluation, 
corresponding to given index person name string, 
the systems are required to recognize each iden-
tical person having the index string as substring 
and classify the document corresponding to each 
identical person into a group. 
This paper presents the design and implemen-
tation of HITSZ_CITYU system in this bakeoff. 
This system incorporates both recogni-
tion/extract technique and clustering technique 
for person name disambiguation. It consists of 
two major components. Firstly, by incorporating 
word segmentation, named entity recognition, 
and unsupervised clustering, the system recog-
nize the person name string in the document and 
then classify the documents into subsets corres-
ponding to the person name. Secondly, for the 
documents having the same person name string, 
the system identifies the person identify string, 
other person name, organization name and con-
textual context words as features. An agglomera-
tive hierarchical clustering algorithm is applied 
to cluster the documents to each identical person. 
In this way, the documents corresponding to 
each identical person are grouped, i.e. the person 
name ambiguities are removed. The evaluation 
results show that the HITSZ_CITYU system 
achieved 0.8399(B-Cubed)/0.8853(P-IP) preci-
sions and 0.9329(B-Cubed)/0.9578(P-IP) recall, 
respectively. The overall F1 performance 
0.8742(B-Cubed)/0.915(P-IP) is ranked 2nd in 
ten participate teams. These results indicate that 
the proposed system incorporating both extrac-
tion and clustering techniques achieves satisfac-
tory recall and overall performance. 
The rest of this report is organized as follows. 
Section 2 describes and analyzes the task. Sec-
tion 3 presents the word segmentation and per-
son name recognition and Section 4 presents the 
person description extraction and document 
clustering. Section 5 gives and discusses the 
evaluation results. Finally, Section 6 concludes.  
2 Task Description 
CIPS-SIGHAN bakeoff on person name disam-
biguation is a clustering task. Corresponding to 
26 person name query string, the systems are 
required to cluster the documents having the in-
dex string into multiple groups, which each 
group representing a separate entity.   
HITSZ_CITYU system divided the whole 
task into two subtasks: 
1. Person name recognition. It includes:  
1.1  Distinguish person name/ non person 
name in the document. For a given index 
string ??, in Example 1, ?? is a person 
name while in Example 2, ?? is a noun 
meaning ?peak? rather than a person name. 
Example 1. ????????????
???????(Gaofeng, the Negotiator 
and professor of Beijing People's Police 
College, said). 
Example 2. ??????? 11.83%??
?? (This value raise to the peak value of 
11.83%). 
1.2  Recognize the exact person name, espe-
cially for three-character to four-character 
names. For a given index string, ??, a 
person name ?? should be identified in 
Example 3 while??? should be identi-
fied from Example 4. 
Example 3. ????????????
??????? (Li Yan from Chinese 
team one is the highest one in the female 
athletes participating this game). 
Example 4. ?????????  (The 
soldier Li YanQing is an orphan) 
2. Cluster the documents for each identical 
person. That is for each person recognized 
person name, cluster documents into groups 
while each group representing an individual 
person.  For the non person names instances 
(such as Example 2), they are clustered into 
a discarded group. Meanwhile, the different 
person with the same name should be sepa-
rated. For example, ?? in the Example 3 
and Example 5 is a athlete and a painter, re-
spectively. These two sentences should be 
cluster into different groups. 
Example 5. ????????????
????(The famous painter Li Yan , who 
involved in hosting this exhibition, said that) 
3 Person Name Recognition 
As discussed in Section 2, HITSZ_CITYU sys-
tem firstly recognizes the person names from the 
text including distinguish the person name/ non-
person name word and recognize the different 
person name having the name index string. In 
our study, we adopted three developed word 
segmentation and named entity recognition tools 
to generate the person name candidates. The 
three tools are: 
1. Language Processing Toolkit from Intel-
ligent Technology & Natural Language 
Processing Lab (ITNLP) of Harbin Insti-
tute of Technology (HIT).  
http://www.insun.hit.edu.cn/ 
2. ICTCLAS from Chinese Academy of 
Sciences. http://ictclas.org/ 
3. The Language Technology Platform from 
Information Retrieval Lab of Harbin Insti-
tute of Technology. http://ir.hit.edu.cn 
We apply the three tools to segment and tag 
the documents into Chinese words. The recog-
nized person name having the name index string 
will be labeled as /nr while the index string is 
labeled as discard if it is no recognized as a per-
son name even not a word.  For the sentences 
having no name index string, we simply vote the 
word segmentation results by as the output. As 
for the sentences having name index string, we 
conduct further analysis on the word segmenta-
tion results.  
1. For the cases that the matched string is 
recognized as person name and non-
person name by different systems, respec-
tively, we selected the recognized person 
name as the output. For example, in  
Example 6. ???????????
????????????? (Secre-
tary for Health, Welfare and Food, Yang 
Yongqiang commended the excellent work 
of Tse Wanwen). 
the segmentation results by three segmen-
tors are ???/nr |discarded|???/nr, 
respectively. We select ???/nr as the 
output. 
2. For the cases that three systems generate 
different person names, we further incor-
porating unsupervised clustering results 
for determination. Here, an agglomerative 
hierarchical clustering with high threshold 
is applied (the details of clustering will be 
presented in Section 4).  
Example 7. ?????? (Zhufang 
overcome three barriers) 
In this example, the word segmentation 
results are ??/nr, ???/nr, ???
/nr, respectively. It is shown that there is 
a segmentation ambiguity here because 
both ?? and ??? are legal Chinese 
person names. Such kinds of ambiguity 
cannot be solved by segmentors indivi-
dually. We further consider the clustering 
results. Since the Example 7 is clustered 
with the documents having the segmenta-
tion results of ??, two votes (emphasize 
the clustering confidence) for ?? are as-
signed. Thus, ?? and ??? obtained 3 
votes and 2 votes in this case, respectively, 
and thus ?? is selected as the output. 
3. For cases that the different person name 
forms having the same votes, the longer 
person name is selected. In the following 
example, 
Example 8. ???????????
??????????? (Prof. Zhang 
Mingxuan, the deputy director of Shang-
hai Municipal Education Commission, 
said at the forum) 
The segmentation form of ?? and ??
?  received the same votes, thus, the 
longer one??? is selected as the out-
put. 
In this component, we applied three segmen-
tors (normally using the local features only) with 
the help of clustering to (using both the local and 
global features) recognize person name in the 
text with high accuracy. It is important to ensure 
the recall performance of the final output. Noted, 
in order to ensure the high precision of cluster-
ing, we set a high similarity threshold here.  
4 Person Name Disambiguation 
4.1 Person Identity Recognition/Extraction 
A person is distinguished by its associated 
attributes in which its identity description is es-
sential. For example, a person name has the 
identity of ?? president and ?? farmer, re-
spectively, tends to be two different persons. 
Therefore, in HITSZ_CITYU system, the person 
identity is extracted based on lexicon and heuris-
tic rules before person name disambiguation. 
We have an entity lexicon consisting of 85 
suffixes and 248 prefix descriptor for persons as 
the initial lexicon. We further expand this lex-
icon through extracting frequently used entity 
words from Gigaword. Here, we segmented 
documents in Gigaword into word sequences. 
For each identified person name, we collect its 
neighboring nouns. The associations between the 
nouns and person name can be estimated by their 
?2 test value. For a candidate entity wa and per-
son name wb, (here, wb is corresponding to per-
son name class with the label /nr), the following 
2-by-2 table shown the dependence of their oc-
currence.  
Table 1 The co-occurrence of two words 
 
awx = awx ?  
bwy =  C11 C12 
bwy ?  C21 C22 
For wa and wb, ?2 test (chi-square test) esti-
mates the differences between observed and ex-
pected values as follows: 
)()()()(
)(
2221221221112211
2
211222112
CCCCCCCC
CCCCN
+++++++
??=?       (1) 
where, N is the total number of words in the 
corpus. The nouns having the ?2 value greater 
than a threshold are extracted as entity descrip-
tors. 
In person entity extraction subtask, for each 
sentence has the recognized person name, the 
system matches its neighboring nouns (-2 to +2 
words surrounding the person name) with the 
entries in entity descriptor lexicon. The matched 
entity descriptors are extracted.  
In this part, several heuristic rules are applied 
to handle some non-neighboring cases. Two ex-
ample rules with cases are given below. 
Example Rule 1. The prefix entity descriptor 
will be assigned to parallel person names with 
the split mark of ?/? , ???and ???,???(and). 
??????? /??  (Chinese players 
Gong Yuechun/Wang Hui)?>  
?? player-??? Gong Yuechun 
?? player-?? Wang Hui 
Example Rule 2. The entity descriptor will be 
assigned to each person in the structure of paral-
lel person name following ??(etc.)? and then a 
entity word. 
???????????????????
??? (The painter, Liu Bingsen, Chen Daz-
hang, Li Yan, Jin Hongjun, etc., paint a.. ) -> 
??? Liu Bingsen - ??? painter 
??? Chen Dazhang - ??? painter 
?? Li Yan - ??? painter 
??? Jin Hongjun - ??? painter 
Furthermore, the HITSZ_CITYU system ap-
plies several rules to identify a special kind of 
person entity, i.e. the reporter or author using 
structure information. For example, in the be-
ginning or the end of a document, there is a per-
son name in a bracket means this person and this 
name appear in the document for only once; 
such person name is regarded as the reporter or 
author. (????????) ?>??? Jin Lin-
peng - ?? reporter 
(??? ??) ?>??? Jin Linpeng - ?? 
reporter 
4.2 Clustering-based Person Name Disam-
biguation 
For the document set corresponding to each giv-
en index person name, we firstly split the docu-
ment set into: (1) Discarded subset, (2) Subset 
with different recognized person name. The sub-
sets are further split into (2-1) the person is the 
author/reporter and (2-2) the person is not the 
author/reporter. The clustering techniques are 
then applied to group documents in each (2-2) 
subset into several clusters which each cluster is 
corresponding to each identical person.  
In the Chinese Person Name Disambiguation 
task, the number of clusters contained in a subset 
is not pre-available. Thus, the clustering method 
which fixes the number of clusters, such as k-
nearest neighbor (k-NN) is not applicable. Con-
sidering that Agglomerative Hierarchical Clus-
tering (AHC) algorithm doesn?t require the fixed 
number of cluster and it performs well in docu-
ment categorization (Jain and Dubes 1988), it is 
adopted in HITSZ_CITYU system. 
Preprocessing and Document Representation 
Before representing documents, a series of pro-
cedures are adopted to preprocess these docu-
ments including stop word removal. Next, we 
select feature words for document clustering. 
Generally, paragraphs containing the target per-
son name usually contain more person-related 
information, such as descriptor, occupation, af-
filiation, and partners. Therefore, larger weights 
should be assigned to these words. Furthermore, 
we further consider the appearance position of 
the features. Intuitively, local feature words with 
small distance are more important than the glob-
al features words with longer distance. 
We implemented some experiments on the 
training data to verify our point. Table 2 and Ta-
ble 3 show the clustering performance achieved 
using different combination of global features 
and local features as well as different similarity 
thresholds.  
Table 2. Performance achieved on training set 
with different weights (similarity threshold 0.1) 
Feature words Precision Recall F-1 
Paragraph 0.820 0.889 0.849 
All 0.791 0.880 0.826 
All+ Paragraph?1 0.791 0.904 0.839 
All+ Paragraph?2 0.802 0.908 0.848 
All+ Paragraph?3 0.824 0.909 0.860 
All+ Paragraph?4 0.831 0.911 0.865 
All+ Paragraph?5 0.839 0.910 0.869 
All+ Paragraph?6 0.833 0.905 0.864 
All+ Paragraph?7 0.838 0.904 0.867 
 
Table 3. Performance achieved on training set 
with different weights (similarity threshold 0.15) 
Feature words Precision Recall F-1 
Paragraph 0. 901       0.873        0.883 
All 0.859        0.867 0.859 
All+ Paragraph?1 0.875 0.887 0.877 
All+ Paragraph?2 0.885 0.890 0.884 
All+ Paragraph?3 0.889 0.887 0.885 
All+ Paragraph?4 0.896 0.887 0.880 
All+ Paragraph?5 0.906 0.882 0.891 
All+ Paragraph?6 0.905 0.884 0.891 
All+ Paragraph?7 0.910 0.882 0.893 
In this two tables, ?Paragraph? means that we 
only select words containing in paragraph which 
contains the person index name as feature words 
(which are the local features), and ?All? means 
that we select all words but stop words in a doc-
ument as feature words. ?All+ Paragraph?k? 
means feature words consist of two parts, one 
part is obtained from ?All?, the other is gained 
from ?Paragraph?, at the same time, we assign 
the feature weights to the two parts, respectively. 
The feature weight coefficient of ?All? is 
)1(1 +k , while the feature weight coefficient of 
?All+ Paragraph?k? is )1( +kk . 
It is shown that, the system perform best using 
appropriate feature weight coefficient distribu-
tion. Therefore, we select all words in the docu-
ment (besides stop words) as global feature 
words and the words in paragraph having the 
index person name as local feature words. We 
then assign the corresponding empirical feature 
weight coefficient to the global/local features, 
respectively. A document is now represented as 
a vector of feature words as follows: 
)))(,());(,());(,(()( 2211 dwtdwtdwtdV nnL?   (2) 
where, d is a document, it  is a feature word, 
)(dwi  is the feature weight of it  in the document 
d . In this paper, we adopt a widely used weight-
ing scheme, named Term Frequency with In-
verse Document Frequency (TF-IDF). In addi-
tion, for each document, we need to normalize 
weights of features because documents have dif-
ferent lengths. The weight of word it in docu-
ment d  is shown as: 
 
? +
+?
=
=
n
i i
i
i
i
i
df
N
dtf
df
N
dtf
dw
1
2))05.0log(*)((
)05.0log()(
)(
        (3)
 
where )(dtf i means how many times word it oc-
curs in the document d , idf  means how many 
documents contains word it , and N  is the num-
ber of documents in the corpus. 
Similarity Estimation 
We use the cosine distance as similarity calcula-
tion function. After the normalization of weights 
of each document, the similarity between docu-
ment 1d  and document 2d  is computed as: 
? ?=
?? 21
2121 )()(),(
ddit
ii dwdwddsim   (4) 
where it  is the term which appears in document 
1d  and document 2d  simultaneously, )( 1dwi  and 
)( 1dwi  are the weights of it  in document 1d  and  
document 2d  respectively. If it  does not appear 
in a document, the corresponding weight in the 
document is zero. 
Agglomerative Hierarchical Clustering (AHC) 
AHC is a bottom-up hierarchical clustering 
method. The framework of AHC is described as 
follows: 
Assign each document to a single cluster. 
Calculate all pair-wise similarities between 
clusters. 
Construct a distance matrix using the similari-
ty values.  
Look for the pair of clusters with the largest 
similarity.  
Remove the pair from the matrix and merge 
them. 
Evaluate all similarities from this new cluster 
to all other clusters, and update the matrix. 
Repeat until the largest similarity in the matrix 
is smaller than some similarity criteria. 
There are three methods to estimate the simi-
larity between two different clusters during the 
cluster mergence: single link method, average 
link method and complete link method (Nallapati 
et al 2004). The three methods define the similar-
ity between two clusters 1c  and 2c  as follows: 
Single link method: The similarity is the 
largest of all similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
),(max),(
2,1
21 ji
cjdcid
ddsimccsim
??
=      (5) 
Average link method: The similarity is the 
average of the similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
21
1 2
21
),(
),(
cc
ddsim
ccsim
cid cjd
ji
?
? ?
= ? ?         (6) 
Complete link method: The similarity is the 
smallest of all similarities of all pairs of docu-
ments between clusters 1c  and 2c  and defined as: 
),(min),(
2,1
21 ji
cjdcid
ddsimccsim
??
=       (7) 
where, id   and jd   are the documents belongs 
to clusters 1c  and 2c , respectively.  
We evaluated the AHC algorithm with the 
above three link methods. The achieved perfor-
mance are given in Table 4. It is shown that the 
system performs best with the complete link me-
thod. Therefore, the complete link method is 
selected for the bakeoff testing. 
Table 4. Performance achieved on training set 
with different link method 
Similarity 
threshold 
Link method Precision Recall F1 
0.1 Single link 0.048 1.000 0.089 
0.1 Average link 0.839 0.910 0.869 
0.1 Complete link 0.867 0.888 0.874 
0.15 Single link 0.048 1.000 0.089 
0.15 Average link 0.906 0.882 0.891 
0.15 Complete link 0.923 0.868 0.891 
5 Evaluations 
The task organizer provides two set of evalua-
tion criteria. They are purity-based score (usual-
ly used in IR), B-cubed score (used in WePS-2), 
respectively. The details of the evaluation crite-
ria are given in the task overview.  
The performance achieved by the top-3 sys-
tems are shown in Table 5. 
Table 5. Performance of Top-3 Systems 
 B-Cubed P-IP 
System Precision Recall F1 Precision Recall F1 
NEU 0.957 0.883 0.914 0.969 0.925 0.945
HITSZ 0.839 0.932 0.874 0.885 0.958 0.915
DLUT 0.826 0.913 0.863 0.879 0.942 0.907
 
The evaluation results show that the 
HITSZ_CITYU system achieved overall F1 per-
formance of 0.8742(B-Cubed)/ 0.915(P-IP), re-
spectively.   
It is also shown that HITSZ_CITYU achieves 
the highest the recall performance. It shows that 
the proposed system is good at split the docu-
ment to different identical persons. Meanwhile, 
this system should improve the capacity on 
merge small clusters to enhance the precision 
and overall performance. 
6 Conclusions 
The presented HITSZ_CITYU system applies 
multi-segmentor and unsupervised clustering to 
achieve good accuracy on person name string 
recognition. The system then incorporates entity 
descriptor extraction, feature word extraction 
and agglomerative hierarchical clustering me-
thod for person name disambiguation. The 
achieved encouraging performance shown the 
high performance word segmentation/name rec-
ognition and extraction-based technique are 
helpful to improve the cluster-based person 
name disambiguation. 
References 
Andrea Heyl and G?nter Neumann. DFKI2: An In-
formation Extraction based Approach to People 
Disambiguation. Proceedings of ACL SEMEVAL 
2007, 137-140, 2007. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine, The 
SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task, Pro-
ceedings of Semeval 2007, Association for Com-
putational Linguistics, 2007. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 
?WePS 2 Evaluation Campaign: Overview of the 
Web People Search Clustering Task, In 2nd Web 
People Search Evaluation Workshop (WePS 2009), 
18th WWW Conference, 2009 
Bekkerman, Ron and McCallum, Andrew, Disambi-
guating Web Appearances of People in a Social 
Network, Proceedings of WWW2005, pp.463-470, 
2005 
Ergin Elmacioglu, Yee Fan Tan, Su Yan, Min-Yen 
Kan, and Dongwon Lee. PSNUS: Web People 
Name Disambiguation by Simple Clustering with 
Rich Features. Proceedings of ACL SEMEVAL 
2007, 268-271, 2007. 
Fei Song, Robin Cohen, Song Lin, Web People 
Search Based on Locality and Relative Similarity 
Measures, Proceedings of WWW 2009 
Fleischman M. B. and Hovy E., Multi-document Per-
son Name Resolution, Proceedings of ACL-42, 
Reference Resolution Workshop, 2004 
Hui Han , Lee Giles , Hongyuan Zha , Cheng Li , 
Kostas Tsioutsiouliklis, Two Supervised Learning 
Approaches for Name Disambiguation in Author 
Citations, Proceedings of the 4th ACM/IEEE-CS 
joint conference on Digital libraries, 2004 
Jain, A. K. and Dubes, R.C. Algorithms for Cluster-
ing Data, Prentice Hall, Upper Saddle River, N.J., 
1988  
Nallapati, R., Feng, A., Peng, F., Allan, J., Event 
Threading within News Topics, Proceedings of-
CIKM 2004, pp. 446?453, 2004 
Niu, Cheng, Wei Li, and Rohini K. Srihari,Weakly 
Supervised Learning for Cross-document Person 
Name Disambiguation Supported by Information 
Extraction, Proceedings of ACL 2004 
Pedersen, Ted, Amruta Purandare, and Anagha Kul-
karni, Name Discrimination by Clustering Similar 
Contexts, Proceedings of the Sixth International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico, 
2005 
Pedersen, Ted and Anagha Kulkarni, Unsupervised 
Discrimination of Person Names in Web Contexts, 
Proceedings of the Eighth International Confe-
rence on Intelligent Text Processing and Computa-
tional Linguistics, Mexico City, 2007. 
Rao, Delip, Nikesh Garera and David Yarowsky, 
JHU1: An Unsupervised Approach to Person 
Name Disambiguation using Web Snippets, In 
Proceedings of ACL Semeval 2007 
Sekine, Satoshi and Javier Artiles. WePS 2 Evalua-
tion Campaign: overview of the Web People 
Search Attribute Extraction Task, Proceedings of 
2nd Web People Search Evaluation Workshop 
(WePS 2009), 18th WWW Conference, 2009  
Xin Li, Paul Morie, and Dan Roth, Robust Reading: 
Identification and Tracing of Ambiguous Names, 
Proceedings of NAACL,pp. 17-24, 2004. 
Ying Chen, Sophia Yat Mei Lee, Chu-Ren Huang, 
PolyUHK: A Robust Information Extraction Sys-
tem for Web Personal Names, Proceedings of 
WWW 2009 
Ying Chen and Martin J.H. CU-COMSEM: Explor-
ing Rich Features for Unsupervised Web Personal 
Name Disambiguation, Proceedings of ACL Se-
meval 2007 
 
