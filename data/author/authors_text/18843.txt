Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2?13,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Modeling Interestingness with Deep Neural Networks 
Jianfeng Gao, Patrick Pantel, Michael Gamon, Xiaodong He, Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,ppantel,mgamon,xiaohe,deng}@microsoft.com 
 
 
Abstract 
This paper presents a deep semantic simi-
larity model (DSSM), a special type of 
deep neural networks designed for text 
analysis, for recommending target docu-
ments to be of interest to a user based on a 
source document that she is reading. We 
observe, identify, and detect naturally oc-
curring signals of interestingness in click 
transitions on the Web between source and 
target documents, which we collect from 
commercial Web browser logs. The DSSM 
is trained on millions of Web transitions, 
and maps source-target document pairs to 
feature vectors in a latent space in such a 
way that the distance between source doc-
uments and their corresponding interesting 
targets in that space is minimized. The ef-
fectiveness of the DSSM is demonstrated 
using two interestingness tasks: automatic 
highlighting and contextual entity search. 
The results on large-scale, real-world da-
tasets show that the semantics of docu-
ments are important for modeling interest-
ingness and that the DSSM leads to signif-
icant quality improvement on both tasks, 
outperforming not only the classic docu-
ment models that do not use semantics but 
also state-of-the-art topic models. 
1 Introduction 
Tasks of predicting what interests a user based on 
the document she is reading are fundamental to 
many online recommendation systems. A recent 
survey is due to Ricci et al. (2011). In this paper, 
we exploit the use of a deep semantic model for 
two such interestingness tasks in which document 
semantics play a crucial role: automatic highlight-
ing and contextual entity search. 
Automatic Highlighting. In this task we want 
a recommendation system to automatically dis-
cover the entities (e.g., a person, location, organi-
zation etc.) that interest a user when reading a doc-
ument and to highlight the corresponding text 
spans, referred to as keywords afterwards. We 
show in this study that document semantics are 
among the most important factors that influence 
what is perceived as interesting to the user. For 
example, we observe in Web browsing logs that 
when a user reads an article about a movie, she is 
more likely to browse to an article about an actor 
or character than to another movie or the director. 
Contextual entity search. After identifying 
the keywords that represent the entities of interest 
to the user, we also want the system to recommend 
new, interesting documents by searching the Web 
for supplementary information about these enti-
ties. The task is challenging because the same key-
words often refer to different entities, and interest-
ing supplementary information to the highlighted 
entity is highly sensitive to the semantic context. 
For example, ?Paul Simon? can refer to many peo-
ple, such as the singer and the senator. Consider 
an article about the music of Paul Simon and an-
other about his life. Related content about his up-
coming concert tour is much more interesting in 
the first context, while an article about his family 
is more interesting in the second. 
At the heart of these two tasks is the notion of 
interestingness. In this paper, we model and make 
use of this notion of interestingness with a deep 
semantic similarity model (DSSM). The model, 
extending from the deep neural networks shown 
recently to be highly effective for speech recogni-
tion (Hinton et al., 2012; Deng et al., 2013) and 
computer vision (Krizhevsky et al., 2012; Mar-
koff, 2014), is semantic because it maps docu-
ments to feature vectors in a latent semantic space, 
also known as semantic representations. The 
model is deep because it employs a neural net-
work with several hidden layers including a spe-
cial convolutional-pooling structure to identify 
keywords and extract hidden semantic features at 
different levels of abstractions, layer by layer. The 
semantic representation is computed through a 
deep neural network after its training by back-
propagation with respect to an objective tailored 
2
to the respective interestingness tasks. We obtain 
naturally occurring ?interest? signals by observ-
ing Web browser transitions, from a source docu-
ment to a target document, in Web usage logs of a 
commercial browser. Our training data is sampled 
from these transitions. 
The use of the DSSM to model interestingness 
is motivated by the recent success of applying re-
lated deep neural networks to computer vision 
(Krizhevshy et al. 2012; Markoff, 2014), speech 
recognition (Hinton et al. 2012), text processing 
(Collobert et al. 2011),  and Web search (Huang 
et al. 2013). Among them, (Huang et al. 2013) is 
most relevant to our work. They also use a deep 
neural network to map documents to feature vec-
tors in a latent semantic space. However, their 
model is designed to represent the relevance be-
tween queries and documents, which differs from 
the notion of interestingness between documents 
studied in this paper. It is often the case that a user 
is interested in a document because it provides 
supplementary information about the entities or 
concepts she encounters when reading another 
document although the overall contents of the sec-
ond documents is not highly relevant. For exam-
ple, a user may be interested in knowing more 
about the history of University of Washington af-
ter reading the news about President Obama?s 
visit to Seattle. To better model interestingness, 
we extend the model of Huang et al. (2013) in two 
significant aspects. First, while Huang et al. treat 
a document as a bag of words for semantic map-
ping, the DSSM treats a document as a sequence 
of words and tries to discover prominent key-
words. These keywords represent the entities or 
concepts that might interest users, via the convo-
lutional and max-pooling layers which are related 
to the deep models used for computer vision 
(Krizhevsky et al., 2013) and speech recognition 
(Deng  et al., 2013a) but are not used in Huang et 
al.?s model. The DSSM then forms the high-level 
semantic representation of the whole document 
based on these keywords. Second, instead of di-
rectly computing the document relevance score 
using cosine similarity in the learned semantic 
space, as in Huang et al. (2013), we feed the fea-
tures derived from the semantic representations of 
documents to a ranker which is trained in a super-
vised manner. As a result, a document that is not 
highly relevant to another document a user is read-
ing (i.e., the distance between their derived feature 
                                                          
1 We stress here that, although the click signal is available to 
form a dataset and a gold standard ranker (to be described in 
vectors is big) may still have a high score of inter-
estingness because the former provides useful in-
formation about an entity mentioned in the latter. 
Such information and entity are encoded, respec-
tively, by (some subsets of) the semantic features 
in their corresponding documents. In Sections 4 
and 5, we empirically demonstrate that the afore-
mentioned two extensions lead to significant qual-
ity improvements for the two interestingness tasks 
presented in this paper.  
Before giving a formal description of the 
DSSM in Section 3, we formally define the inter-
estingness function, and then introduce our data 
set of naturally occurring interest signals. 
2 The Notion of Interestingness 
Let ?  be the set of all documents. Following 
Gamon et al. (2013), we formally define the inter-
estingness modeling task as learning the mapping 
function: 
	 ?: ? ? ? ? ??	 		
where the function ???, ?? is the quantified degree 
of interest that the user has  in the target document 
? ? ? after or while reading the source document 
? ? ?. 
Our notion of a document is meant in its most 
general form as a string of raw unstructured text. 
That is, the interestingness function should not 
rely on any document structure such as title tags, 
hyperlinks, etc., or Web interaction data. In our 
tasks, documents can be formed either from the 
plain text of a webpage or as a text span in that 
plain text, as will be discussed in Sections 4 and 5. 
2.1 Data 
We can observe many naturally occurring mani-
festations of interestingness on the Web. For ex-
ample, on Twitter, users follow shared links em-
bedded in tweets. Arguably the most frequent sig-
nal, however, occurs in Web browsing events 
where users click from one webpage to another 
via hyperlinks. When a user clicks on a hyperlink, 
it is reasonable to assume that she is interested in 
learning more about the anchor, modulo cases of 
erroneous clicks. Aggregate clicks can therefore 
serve as a proxy for interestingness. That is, for a 
given source document, target documents that at-
tract the most clicks are more interesting than doc-
uments that attract fewer clicks1.  
Section 4), our task is to model interestingness between un-
structured documents, i.e., without access to any document 
structure or Web interaction data. Thus, in our experiments, 
3
We collect a large dataset of user browsing 
events from a commercial Web browser. Specifi-
cally, we sample 18 million occurrences of a user 
click from one Wikipedia page to another during 
a one year period. We restrict our browsing events 
to Wikipedia since its pages tend to contain many 
anchors (79 on average, where on average 42 have 
a unique target URL). Thus, they attract enough 
traffic for us to obtain robust browsing transition 
data2. We group together all transitions originat-
ing from the same page and randomly hold out 
20% of the transitions for our evaluation data 
(EVAL), 20% for training the DSSM described in 
Section 3.2 (TRAIN_1), and the remaining 60% 
for training our task specific rankers described in 
Section 3.3 (TRAIN_2). In our experiments, we 
used different settings for the two interestingness 
tasks. Thus, we postpone the detailed description 
of these datasets and other task-specific datasets 
to Sections 4 and 5. 
3 A Deep Semantic Similarity Model 
(DSSM) 
This section presents the architecture of the 
DSSM, describes the parameter estimation, and 
the way the DSSM is used in our tasks. 
                                                          
we remove all structural information (e.g., hyperlinks and 
XML tags) in our documents, except that in the highlighting 
experiments (Section 4) we use anchor texts to simulate the 
candidate keywords to be highlighted. We then convert each 
3.1 Network Architecture 
The heart of the DSSM is a deep neural network 
with convolutional structure, as shown in Figure 
1. In what follows, we use lower-case bold letters, 
such as ?, to denote column vectors, ???? to de-
note the ??? element of ?, and upper-case letters, 
such as ?, to denote matrices. 
Input Layer ?. It takes two steps to convert a doc-
ument ?, which is a sequence of words, into a vec-
tor representation ? for the input layer of the net-
work: (1) convert each word in ? to a word vector, 
and (2) build ? by concatenating these word vec-
tors. To convert a word ? into a word vector, we 
first represent ? by a one-hot vector using a vo-
cabulary that contains ?  high frequent words 
(? ? 150K in this study). Then, following Huang 
et al. (2013), we map ? to a separate tri-letter vec-
tor. Consider the word ?#dog#?, where # is a word 
boundary symbol. The nonzero elements in its tri-
letter vector are ?#do?, ?dog?, and ?og#?. We then 
form the word vector of ? by concatenating its 
one-hot vector and its tri-letter vector. It is worth 
noting that the tri-letter vector complements the 
one-hot vector representation in two aspects. First, 
different OOV (out of vocabulary) words can be 
represented by tri-letter vectors with few colli-
sions. Second, spelling variations of the same 
word can be mapped to the points that are close to 
each other in the tri-letter space. Although the 
number of unique English words on the Web is 
extremely large, the total number of distinct tri-
letters in English is limited (restricted to the most 
frequent 30K in this study). As a result, incorpo-
rating tri-letter vectors substantially improves the 
representation power of word vectors while keep-
ing their size small.  
To form our input layer ? using word vectors, 
we first identify a text span with a high degree of 
relevance, called focus, in ?  using task-specific 
heuristics (see Sections 4 and 5 respectively). Sec-
ond, we form ? by concatenating each word vec-
tor in the focus and a vector that is the summation 
of all other word vectors, as shown in Figure 1. 
Since the length of the focus is much smaller than 
that of its document, ? is able to capture the con-
textual information (for the words in the focus) 
Web document into plain text, which is white-space to-
kenized and lowercased. Numbers are retained and no stem-
ming is performed. 
2 We utilize the May 3, 2013 English Wikipedia dump con-
sisting of roughly 4.1 million articles from http://dumps.wiki-
media.org. 
Figure 1: Illustration of the network architec-
ture and information flow of the DSSM 
 
4
useful to the corresponding tasks, with a manage-
able vector size. 
Convolutional Layer ? . A convolutional layer 
extracts local features around each word ??	in a 
word sequence of length ?  as follows. We first 
generate a contextual vector ??  by concatenating 
the word vectors of ?? and its surrounding words defined by a window (the window size is set to 3 
in this paper). Then, we generate for each word a 
local feature vector ??  using a tanh  activation 
function and a linear projection matrix ??, which 
is the same across all windows ? in the word se-
quence, as: 
?? ? tanh??????? , where	? ? 1? 1) ?) 
Max-pooling Layer ?. The size of the output ? 
depends on the number of words in the word se-
quence. Local feature vectors have to be com-
bined to obtain a global feature vector, with a 
fixed size independent of the document length, in 
order to apply subsequent standard affine layers. 
We design ? by adopting the max operation over 
each ?time? ? of the sequence of vectors computed 
by (1), which forces the network to retain only the 
most useful, partially invariant local features pro-
duced by the convolutional layer: 
???? ? max???,?,??u????? (2) 
where the max operation is performed for each di-
mension of ? across ? ? 1,? , ? respectively.  
That convolutional and max-pooling layers are 
able to discover prominent keywords of a docu-
ment can be demonstrated using the procedure in 
Figure 2 using a toy example. First, the convolu-
tional layer of (1) generates for each word in a 5-
word document a 4-dimensional local feature vec-
tor, which represents a distribution of four topics. 
For example, the most prominent topic of ?? within its three word context window is the first 
topic, denoted by ???1?, and the most prominent 
topic of ?? is ???3?. Second, we use max-pooling of (2) to form a global feature vector, which rep-
resents the topic distribution of the whole docu-
ment. We see that ??1? and ??3? are two promi-
nent topics. Then, for each prominent topic, we 
trace back to the local feature vector that survives 
max-pooling: 
??1? ? max???,?,?????1?? ? ???1?  
??3? ? max???,?,?????3?? ? ???3?.  
Finally, we label the corresponding words of these 
local feature vectors, ?? and ??, as keywords of the document.  
Figure 3 presents a sample of document snip-
pets and their keywords detected by the DSSM ac-
cording to the procedure elaborated in Figure 2. It 
is interesting to see that many names are identified 
as keywords although the DSSM is not designed 
explicitly for named entity recognition. 
Fully-Connected Layers ?  and ? . The fixed 
sized global feature vector ? of (2) is then fed to 
several standard affine network layers, which are 
stacked and interleaved with nonlinear activation 
functions, to extract highly non-linear features ? 
at the output layer. In our model, shown in Figure 
1, we have: 
? ? tanh?????? (3) 
? ? tanh?????? (4) 
where ?? and ?? are learned linear projection matri-ces. 
3.2 Training the DSSM 
To optimize the parameters of the DSSM of Fig-
ure 1, i.e., ? ? ???,??,???, we use a pair-wise rank loss as objective (Yih et al. 2011). Consider 
a source document ?  and two candidate target 
documents ??	and ??, where ?? is more interesting 
than ??  to a user when reading ?. We construct 
two pairs of documents ??, ??? and ??, ???, where the former is preferred and should have a higher 
u1 u2 u3 u4 u5
w1 w2 w3 w4 w5
2
3
4
1
 
w1 w2 w3 w4 w5
v
2
3
4
1
Figure 2: Toy example of (upper) a 5-word 
document and its local feature vectors ex-
tracted using a convolutional layer, and (bot-
tom) the global feature vector of the document 
generated after max-pooling. 
 
 
5
interestingness score. Let ? be the difference of 
their interestingness scores: ?	? ???, ??? ?
???, ??? , where ?  is the interestingness score, computed as the cosine similarity: 
???, ?? ? sim???, ?? ?
?????
???????? 
(5) 
where ?? and ?? are the feature vectors of ? and ?, respectively, which are generated using the 
DSSM, parameterized by ?. Intuitively, we want 
to learn ? to maximize ?. That is, the DSSM is 
learned to represent documents as points in a hid-
den interestingness space, where the similarity be-
tween a document and its interesting documents is 
maximized.  
We use the following logistic loss over ? , 
which can be shown to upper bound the pairwise 
accuracy: 
???; ?? ? log?1 ? exp?????? (6) 
                                                          
3 In our experiments, we observed better results by sampling 
more negative training examples (e.g., up to 100) although 
this makes the training much slower. An alternative approach 
The loss function in (6) has a shape similar to the 
hinge loss used in SVMs. Because of the use of 
the cosine similarity function, we add a scaling 
factor ? that magnifies ? from [-2, 2] to a larger 
range. Empirically, the value of ? makes no dif-
ference as long as it is large enough. In the exper-
iments, we set ? ? 10. Because the loss function 
is differentiable, optimizing the model parameters 
can be done using gradient-based methods. Due to 
space limitations, we omit the derivation of the 
gradient of the loss function, for which readers are 
referred to related derivations (e.g., Collobert et 
al. 2011; Huang et al. 2013; Shen et al. 2014). 
In our experiments we trained DSSMs using 
mini-batch Stochastic Gradient Descent. Each 
mini-batch consists of 256 source-target docu-
ment pairs. For each source document ?, we ran-
domly select from that batch four target docu-
ments which are not paired with ?  as negative 
training samples3. The DSSM trainer is imple-
mented using a GPU-accelerated linear algebra li-
brary, which is developed on CUDA 5.5. Given 
the training set (TRAIN_1 in Section 2), it takes 
approximately 30 hours to train a DSSM as shown 
in Figure 1, on a Xeon E5-2670 2.60GHz machine 
with one Tesla K20 GPU card. 
In principle, the loss function of (6) can be fur-
ther regularized (e.g. by adding a term of 2? norm) 
to deal with overfitting. However, we did not find 
a clear empirical advantage over the simpler early 
stop approach in a pilot study, hence we adopted 
the latter in the experiments in this paper. Our ap-
proach adjusts the learning rate ?  during the 
course of model training. Starting with ? ? 1.0, 
after each epoch (a pass over the entire training 
data), the learning rate is adjusted as ? ? 0.5 ? ? 
if the loss on validation data (held-out from 
TRAIN_1) is not reduced. The training stops if 
either ?  is smaller than a preset threshold 
(0.0001) or the loss on training data can no longer 
be reduced significantly. In our experiments, the 
DSSM training typically converges within 20 
epochs. 
3.3 Using the DSSM 
We experiment with two ways of using the DSSM 
for the two interestingness tasks. First, we use the 
DSSM as a feature generator. The output layer of 
the DSSM can be seen as a set of semantic fea-
tures, which can be incorporated in a boosted tree 
is to approximate the partition function using Noise Contras-
tive Estimation (Gutmann and Hyvarinen 2010). We leave it 
to future work.  
? the comedy festival formerly known as 
the us comedy arts festival is a comedy 
festival held each year in las vegas 
nevada from its 1985 inception to 2008 
. it was held annually at the wheeler 
opera house and other venues in aspen 
colorado . the primary sponsor of the 
festival was hbo with co-sponsorship by 
caesars palace . the primary venue tbs 
geico insurance twix candy bars and 
smirnoff vodka hbo exited the festival 
business in 2007 and tbs became the pri-
mary sponsor the festival includes 
standup comedy performances appearances 
by the casts of television shows? 
 
? bad samaritans is an american comedy
series produced by walt becker kelly
hayes and ross putman . it premiered on 
netflix on march 31 2013 cast and char-
acters . the show focuses on a community 
service parole group and their parole 
officer brian kubach as jake gibson an 
aspiring professional starcraft player 
who gets sentenced to 2000 hours of com-
munity service for starting a forest 
fire during his breakup with drew prior 
to community service he had no real am-
bition in life other than to be a pro-
fessional gamer and become wealthy 
overnight like mark zuckerberg as in 
life his goal during ? 
Figure 3: A sample of document snippets and 
the keywords (in bold) detected by the DSSM. 
 
 
6
based ranker (Friedman 1999) trained discrimina-
tively on the task-specific data. Given a source-
target document pair ??, ??, the DSSM generates 
600 features (300 from the output layers ?? and ?? 
for each ? and ?, respectively). 
Second, we use the DSSM as a direct imple-
mentation of the interestingness function ?. Re-
call from Section 3.2 that in model training, we 
measure the interestingness score for a document 
pair using the cosine similarity between their cor-
responding feature vectors (?? and ??). Similarly 
at runtime, we define	? ? 	sim???, ?? as (5). 
4 Experiments on Highlighting 
Recall from Section 1 that in this task, a system 
must select ? most interesting keywords in a doc-
ument that a user is reading. To evaluate our mod-
els using the click transition data described in Sec-
tion 2, we simulate the task as follows. We use the 
set of anchors in a source document ? to simulate 
the set of candidate keywords that may be of in-
terest to the user while reading ?, and treat the text 
of a document that is linked by an anchor in ? as a 
target document ?. As shown in Figure 1, to apply 
DSSM to a specific task, we need to define the fo-
cus in source and target documents. In this task, 
the focus in s is defined as the anchor text, and the 
focus in t is defined as the first 10 tokens in t. 
We evaluate the performance of a highlighting 
system against a gold standard interestingness 
function ?? which scores the interestingness of an 
anchor as the number of user clicks on ? from the 
anchor in ? in our data. We consider the ideal se-
lection to then consist of the ?  most interesting 
anchors according to ??. A natural metric for this 
task is Normalized Discounted Cumulative Gain 
(NDCG) (Jarvelin and Kekalainen 2000). 
We evaluate our models on the EVAL dataset 
described in Section 2. We utilize the transition 
distributions in EVAL to create three other test 
sets, following the stratified sampling methodol-
ogy commonly employed in the IR community, 
for the frequently, less frequently, and rarely 
viewed source pages, referred to as HEAD, 
TORSO, and TAIL, respectively. We obtain 
these sets by first sorting the unique source docu-
ments according to their frequency of occurrence 
in EVAL. We then partition the set so that HEAD 
corresponds to all transitions from the source 
pages at the top of the list that account for 20% of 
the transitions in EVAL; TAIL corresponds to the 
transitions at the bottom also accounting for 20% 
of the transitions in EVAL; and TORSO corre-
sponds to the remaining transitions. 
4.1 Main Results 
Table 1 summarizes the results of various models 
over the three test sets using NDCG at truncation 
levels 1, 5, and 10. 
Rows 1 to 3 are simple heuristic baselines. 
RAND selects ?  random anchors, 1stK selects 
the first ? anchors and LastK the last ? anchors.  
The other models in Table 1 are boosted tree 
based rankers trained on TRAIN_2 described in 
Section 2. They vary only in their features. The 
ranker in Row 4 uses Non-Semantic Features 
(NSF) only. These features are derived from the 
 # Models HEAD TORSO TAIL 
   @1 @5 @10 @1 @5 @10 @1 @5 @10 
src
  o
nly
 
1 RAND 0.041 0.062 0.081 0.036 0.076 0.109 0.062 0.195 0.258 
2 1stK 0.010 0.177 0.243 0.072 0.171 0.240 0.091 0.274 0.348 
3 LastK 0.170 0.022 0.027 0.022 0.044 0.062 0.058 0.166 0.219
4 NSF 0.215 0.253 0.295 0.139 0.229 0.282 0.109 0.293 0.365 
5 NSF+WCAT 0.438 0.424 0.463 0.194 0.290 0.346 0.118 0.317 0.386 
6 NSF+JTT 0.220 0.302 0.343 0.141 0.241 0.295 0.111 0.300 0.369 
7 NSF+DSSM_BOW 0.312 0.351 0.391 0.162 0.258 0.313 0.110 0.299 0.372 
8 NSF+DSSM 0.362 0.386 0.421 0.178 0.275 0.330 0.116 0.312 0.382 
src
+ta
r 9 NSF+WCAT 0.505 0.475 0.501 0.224 0.304 0.356 0.129 0.324 0.391 10 NSF+JTT 0.345 0.380 0.418 0.183 0.280 0.332 0.131 0.321 0.390 
11 NSF+DSSM_BOW 0.416 0.393 0.428 0.197 0.274 0.325 0.123 0.311 0.380 
12 NSF+DSSM 0.554 0.524 0.547 0.241 0.317 0.367 0.135 0.329 0.398 
Table 1: Highlighting task performance (NDCG @ K) of interest models over HEAD, TORSO and 
TAIL test sets. Bold indicates statistical significance over all non-shaded results using t-test (? ?
0.05). 
 
 
7
source document s and from user session infor-
mation in the browser log. The document features 
include: position of the anchor in the document, 
frequency of the anchor, and anchor density in the 
paragraph.  
The rankers in Rows 5 to 12 use the NSF and 
the semantic features computed from source and 
target documents of a browsing transition. We 
compare semantic features derived from three dif-
ferent sources. The first feature source comes 
from our DSSMs (DSSM and DSSM_BOW) us-
ing the output layers as feature generators as de-
scribed in Section 3.3. DSSM is the model de-
scribed in Section 3 and DSSM_BOW is the 
model proposed by Huang et al. (2013) where 
documents are view as bag of words (BOW) and 
the convolutional and max-pooling layers are not 
used. The two other sources of semantic features 
are used as a point of comparison to the DSSM. 
One is a generative semantic model (Joint Transi-
tion Topic model, or JTT) (Gamon et al. 2013). 
JTT is an LDA-style model (Blei et al. 2003) that 
is trained jointly on source and target documents 
linked by browsing transitions. JTT generates a 
total of 150 features from its latent variables, 50 
each for the source topic model, the target topic 
model and the transition model. The other seman-
tic model of contrast is a manually defined one, 
which we use to assess the effectiveness of auto-
matically learned models against human model-
ers. To this effect, we use the page categories that 
editors assign in Wikipedia as semantic features 
(WCAT). These features number in the multiple 
thousands. Using features such as WCAT is not a 
viable solution in general since Wikipedia catego-
ries are not available for all documents. As such, 
we use it solely as a point of comparison against 
DSSM and JTT. 
We also distinguish between two types of 
learned rankers: those which draw their features 
only from the source (src only) document and 
those that draw their features from both the source 
and target (src+tar) documents. Although our 
task setting allows access to the content of both 
source and target documents, there are practical 
scenarios where a system should predict what in-
terests the user without looking at the target doc-
ument because the extra step of identifying a suit-
able target document for each candidate concept 
or entity of interest is computationally expensive.  
4.2 Analysis of Results 
As shown in Table 1, NSF+DSSM, which incor-
porates our DSSM, is the overall best performing 
system across test sets. The task is hard as evi-
denced by the weak baseline scores. One reason is 
the large average number of candidates per page. 
On HEAD, we found an average of 170 anchors 
(of which 95 point to a unique target URL). For 
TORSO and TAIL, we found the average number 
of anchors to be 94 (52 unique targets) and 41 (19 
unique targets), respectively. 
Clearly, the semantics of the documents form 
important signals for this task: WCAT, JTT, 
DSSM_BOW, and DSSM all significantly boost 
the performance over NSF alone. There are two 
interesting comparisons to consider: (a) manual 
semantics vs. learned semantics; and (b) deep se-
mantic models vs. generative topic models. On 
(a), we observe somewhat surprisingly that the 
learned DSSM produces features that outperform 
the thousands of features coming from manually 
(editor) assigned Wikipedia category features 
(WCAT), in all but the TAIL where the two per-
form statistically the same. In contrast, features 
from the generative model (JTT) perform worse 
than WCAT across the board except on TAIL 
where JTT and WCAT are statistically tied. On 
(b), we observe that DSSM outperforms a state-
of-the-art generative model (JTT) on HEAD and 
TORSO. On TAIL, they are statistically indistin-
guishable. 
We turn now to inspecting the scenario where 
features are only drawn from the source document 
(Rows 1-8 in Table 1). Again we observe that se-
mantic features significantly boost the perfor-
mance against NSF alone, however they signifi-
cantly deteriorate when compared to using fea-
tures from both source and target documents. In 
this scenario, the manual semantics from WCAT 
outperform all other models, but with a diminish-
ing effect as we move from HEAD through 
TORSO to TAIL. DSSM is the best performing 
learned semantic model. 
Finally, we present the results to justify the two 
modifications we made to extend the model of 
Huang et al. (2013) to the DSSM, as described in 
Section 1. First, we see in Table 1 that 
DSSM_BOW, which has the same network struc-
ture of Huang et al.?s model, is much weaker than 
DSSM, demonstrating the benefits of using con-
volutional and max-pooling layers to extract se-
mantic features for the highlighting task. Second, 
we conduct several experiments by using the co-
sine scores between the output layers of DSSM 
for ? and ? as features (following the procedure in 
Section 3.3 for using the DSSM as a direct imple-
mentation of ?). We found that adding the cosine 
8
features to NSF+DSSM does not lead to any im-
provement. We also combined NSF with solely 
the cosine features from DSSM (i.e., without the 
other semantic features drawn from its output lay-
ers). But we still found no improvement over us-
ing NSF alone. Thus, we conclude that for this 
task it is much more effective to feed the features 
derived from DSSM to a supervised ranker than 
directly computing the interestingness score using 
cosine similarity in the learned semantic space, as 
in Huang et al. (2013). 
5 Experiments on Entity Search 
We construct the evaluation data set for this sec-
ond task by randomly sampling a set of documents 
from a traffic-weighted set of Web documents. In 
a second step, we identify the entity names in each 
document using an in-house named entity recog-
nizer. We issue each entity name as a query to a 
commercial search engine, and retain up to the 
top-100 retrieved documents as candidate target 
documents. We form for each entity a source doc-
ument which consists of the entity text and its sur-
rounding text defined by a 200-word window. We 
define the focus (as in Figure 1) in ? as the entity 
text, and the focus in ? as the first 10 tokens in ?. 
The final evaluation data set contains 10,000 
source documents. On average, each source docu-
ment is associated with 87 target documents. Fi-
nally, the source-target document pairs are labeled 
in terms of interestingness by paid annotators. The 
label is on a 5-level scale, 0 to 4, with 4 meaning 
the target document is the most interesting to the 
source document and 0 meaning the target is of no 
interest. 
We test our models on two scenarios. The first 
is a ranking scenario where ?  interesting docu-
ments are displayed to the user. Here, we select 
the top-? ranked documents according to their in-
terestingness scores. We measure the performance 
via NDCG at truncation levels 1 and 3. The sec-
ond scenario is to display to the user all interesting 
results. In this scenario, we select all target docu-
ments with an interestingness score exceeding a 
predefined threshold. We evaluate this scenario 
using ROC analysis and, specifically, the area un-
der the curve (AUC). 
5.1 Main Results 
The main results are summarized in Table 2. Rows 
1 to 6 are single model results, where each model 
is used as a direct implementation of the interest-
ingness function ?. Rows 7 to 9 are ranker results, 
where ? is defined as a boosted tree based ranker 
that incorporates different sets of features ex-
tracted from source and target documents, includ-
ing the features derived from single models. As in 
the highlighting experiments, all the machine-
learned single models, including the DSSM, are 
trained on TRAIN_1, and all the rankers are 
trained on TRAIN_2. 
5.2 Analysis of Results 
BM25 (Rows 1 and 2 in Table 2) is the classic 
document model (Robertson and Zaragoza 2009). 
It uses the bag-of-words document representation 
and the BM25 term weighting function. In our set-
ting, we define the interestingness score of a doc-
ument pair as the dot product of their BM25-
weighted term vectors. To verify the importance 
of using contextual information, we compare two 
different ways of forming the term vector of a 
source document. The first only uses the entity 
text (Row 1). The second (Row 2) uses both the 
entity text and and its surrounding text in a 200-
word window (i.e., the entire source document). 
Results show that the model using contextual in-
formation is significantly better. Therefore, all the 
other models in this section use both the entity 
texts and their surrounding text. 
WTM (Row 3) is our implementation of the 
word translation model for IR (Berger and Laf-
ferty 1999; Gao et al. 2010). WTM defines the in-
terestingness score as: 
???, ?? ? ? ? ????|???????|?????????? ,  
# Models @1 @3 AUC 
1 BM25 (entity)  0.133 0.195 0.583 
2 BM25 0.142 0.227 0.675 
3 WTM 0.191 0.287 0.678 
4 BLTM 0.214 0.306 0.704 
5 DSSM 0.259* 0.356* 0.711* 
6 DSSM_BOW 0.223 0.322 0.699 
7 Baseline ranker 0.283 0.360 0.723 
8 7 + DSSM(1) 0.301# 0.385# 0.758# 
9 7 + DSSM(600) 0.327## 0.402## 0.782##
Table 2: Contextual entity search task perfor-
mance (NDCG @ K and AUC). * indicates sta-
tistical significance over all non-shaded single 
model results (Rows 1 to 6) using t-test (? ?
0.05). # indicates statistical significance over re-
sults in Row 7. ## indicates statistical signifi-
cance over results in Rows 7 and 8. 
 
 
 
9
where ????|?? is the unigram probability of word 
?? in ?, and ????|??? is the probability of trans-
lating ?? into ??, trained on source-target docu-ment pairs using EM (Brown et al. 1993). The 
translation-based approach allows any pair of 
non-identical but semantically related words to 
have a nonzero matching score. As a result, it sig-
nificantly outperforms BM25. 
BTLM (Row 4) follows the best performing 
bilingual topic model described in Gao et al. 
(2011), which is an extension of PLSA (Hofmann 
1999). The model is trained on source-target doc-
ument pairs using the EM algorithm with a con-
straint enforcing a source document ? and its tar-
get document ? to not only share the same prior 
topic distribution, but to also have similar frac-
tions of words assigned to each topic. BLTM de-
fines the interestingness score between s and t as: 
???, ?? ? ? ? ????|??????|???????? .  
The model assumes the following story of gener-
ating ? from ?. First, for each topic ? a word dis-
tribution ?? is selected from a Dirichlet prior with 
concentration parameter ? . Second, given ? , a 
topic distribution ??  is drawn from a Dirichlet 
prior with parameter ? . Finally, ?  is generated 
word by word. Each word ?? is generated by first 
selecting a topic ?  according to ?? , and then 
drawing a word from ?? . We see that BLTM models interestingness by taking into account the 
semantic topic distribution of the entire docu-
ments. Our results in Table 2 show that BLTM 
outperforms WTM by a significant margin in 
both NDCG and AUC. 
DSSM (Row 5) outperforms all the competing 
single models, including the state-of-the-art topic 
model BLTM. Now, we inspect the difference be-
tween DSSM and BLTM in detail. Although both 
models strive to generate the semantic representa-
tion of a document, they use different modeling 
approaches. BLTM by nature is a generative 
model. The semantic representation in BLTM is a 
distribution of hidden semantic topics. Such a dis-
tribution is learned using Maximum Likelihood 
Estimation in an unsupervised manner, i.e., max-
imizing the log-likelihood of the source-target 
document pairs in the training data. On the other 
hand, DSSM represents documents as points in a 
hidden semantic space using a supervised learning 
method, i.e., paired documents are closer in that 
latent space than unpaired ones. We believe that 
the superior performance of DSSM is largely due 
to the fact that the model parameters are discrimi-
natively trained using an objective that is tailored 
to the interestingness task.  
In addition to the difference in training meth-
ods, DSSM and BLTM also use different model 
structures. BLTM treats a document as a bag of 
words (thus losing some important contextual in-
formation such as word order and inter-word de-
pendencies), and generates semantic representa-
tions of documents using linear projection. 
DSSM, on the other hand, treats text as a sequence 
of words and better captures local and global con-
text, and generates highly non-linear semantic 
features via a deep neural network. To further ver-
ify our analysis, we inspect the results of a variant 
of DSSM, denoted as DSSM_BOW (Row 6), 
where the convolution and max-pooling layers are 
removed. This model treats a document as a bag 
of words, just like BLTM. These results demon-
strate that the effectiveness of DSSM can also be 
attributed to the convolutional architecture in the 
neural network, in addition to being deep and be-
ing discriminative. 
We turn now to discussing the ranker results in 
Rows 7 to 9. The baseline ranker (Row 7) uses 158 
features, including many counts and single model 
scores, such as BM25 and WMT. DSSM (Row 5) 
alone is quite effective, being close in perfor-
mance to the baseline ranker with non-DSSM fea-
tures. Integrating the DSSM score computed in (5) 
as one single feature into the ranker (Row 8) leads 
to a significant improvement over the baseline. 
The best performing combination (Row 9) is ob-
tained by incorporating the DSSM feature vectors 
of source and target documents (i.e., 600 features 
in total) in the ranker. 
We thus conclude that on both tasks, automatic 
highlighting and contextual entity search, features 
drawn from the output layers of our deep semantic 
model result in significant gains after being added 
to a set of non-semantic features, and in compari-
son to other types of semantic models used in the 
past. 
6 Related Work 
In addition to the notion of relevance as described 
in Section 1, related to interestingness is also the 
notion of salience (also called aboutness) (Gamon 
et al. 2013; 2014; Parajpe 2009; Yih et al. 2006). 
Salience is the centrality of a term to the content 
of a document. Although salience and interesting-
ness interact, the two are not the same. For exam-
ple, in a news article about President Obama?s 
visit to Seattle, Obama is salient, yet the average 
user would probably not be interested in learning 
more about Obama while reading that article.  
10
There are many systems that identify popular 
content in the Web or recommend content (e.g., 
Bandari et al. 2012; Lerman and Hogg 2010; 
Szabo and Huberman 2010), which is closely re-
lated to the highlighting task. In contrast to these 
approaches, we strive to predict what term a user 
is likely to be interested in when reading content, 
which may or may not be the same as the most 
popular content that is related to the current docu-
ment. It has empirically been demonstrated in 
Gamon et al. (2013) that popularity is in fact a ra-
ther poor predictor for interestingness. The task of 
contextual entity search, which is formulated as an 
information retrieval problem in this paper, is also 
related to research on entity resolution (Stefanidis 
et al. 2013).  
Latent Semantic Analysis (Deerwester et al. 
1990) is arguably the earliest semantic model de-
signed for IR. Generative topic models widely 
used for IR include PLSA (Hofmann 1990) and 
LDA (Blei et al. 2003). Recently, these models 
have been extended to handle cross-lingual cases, 
where there are pairs of corresponding documents 
in different languages (e.g., Dumais et al. 1997; 
Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). 
By exploiting deep architectures, deep learning 
techniques are able to automatically discover from 
training data the hidden structures and the associ-
ated features at different levels of abstraction use-
ful for a variety of tasks (e.g., Collobert et al. 
2011; Hinton et al. 2012; Socher et al. 2012; 
Krizhevsky et al., 2012; Gao et al. 2014). Hinton 
and Salakhutdinov (2010) propose the most origi-
nal approach based on an unsupervised version of 
the deep neural network to discover the hierar-
chical semantic structure embedded in queries and 
documents. Huang et al. (2013) significantly ex-
tends the approach so that the deep neural network 
can be trained on large-scale query-document 
pairs giving much better performance. The use of 
the convolutional neural network for text pro-
cessing, central to our DSSM, was also described 
in Collobert et al. (2011) and Shen et al. (2014) 
but with very different applications. The DSSM 
described in Section 3 can be viewed as a variant 
of the deep neural network models used in these 
previous studies. 
7 Conclusions 
Modeling interestingness is fundamental to many 
online recommendation systems. We obtain natu-
rally occurring interest signals by observing Web 
browsing transitions where users click from one 
webpage to another. We propose to model this 
?interestingness? with a deep semantic similarity 
model (DSSM), based on deep neural networks 
with special convolutional-pooling structure, 
mapping source-target document pairs to feature 
vectors in a latent semantic space. We train the 
DSSM using browsing transitions between docu-
ments. Finally, we demonstrate the effectiveness 
of our model on two interestingness tasks: auto-
matic highlighting and contextual entity search. 
Our results on large-scale, real-world datasets 
show that the semantics of documents computed 
by the DSSM are important for modeling interest-
ingness and that the new model leads to signifi-
cant improvements on both tasks. DSSM is shown 
to outperform not only the classic document mod-
els that do not use (latent) semantics but also state-
of-the-art topic models that do not have the deep 
and convolutional architecture characterizing the 
DSSM. 
One area of future work is to extend our 
method to model interestingness given an entire 
user session, which consists of a sequence of 
browsing events. We believe that the prior brows-
ing and interaction history recorded in the session 
provides additional signals for predicting interest-
ingness. To capture such signals, our model needs 
to be extended to adequately represent time series 
(e.g., causal relations and consequences of ac-
tions). One potentially effective model for such a 
purpose is based on the architecture of recurrent 
neural networks (e.g., Mikolov et al. 2010; Chen 
and Deng, 2014), which can be incorporated into 
the deep semantic model proposed in this paper. 
Additional Authors 
Yelong Shen (Microsoft Research, One Microsoft 
Way, Redmond, WA 98052, USA, email: 
yeshen@microsoft.com). 
Acknowledgments 
The authors thank Johnson Apacible, Pradeep 
Chilakamarri, Edward Guo, Bernhard Kohlmeier, 
Xiaolong Li, Kevin Powell, Xinying Song and 
Ye-Yi Wang for their guidance and valuable dis-
cussions. We also thank the three anonymous re-
viewers for their comments. 
References 
Bandari, R., Asur, S., and Huberman, B. A. 2012. 
The pulse of news in social media: forecasting 
popularity. In ICWSM. 
11
Bengio, Y., 2009. Learning deep architectures for 
AI. Fundamental Trends in Machine Learning, 
2(1):1?127. 
Berger, A., and Lafferty, J. 1999. Information re-
trieval as statistical translation. In SIGIR, pp. 
222-229.  
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet allocation. Journal of Machine 
Learning Research, 3. 
Broder, A., Fontoura, M., Josifovski, V., and 
Riedel, L. 2007. A semantic approach to contex-
tual advertising. In SIGIR. 
Brown, P. F., Della Pietra, S. A., Della Pietra, V. 
J., and Mercer, R. L. 1993. The mathematics of 
statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263-
311. 
Burges, C., Shaked, T., Renshaw, E., Lazier, A., 
Deeds, M., Hamilton, and Hullender, G. 2005. 
Learning to rank using gradient descent. In 
ICML, pp. 89-96.  
Chen, J. and Deng, L. 2014. A primal-dual method 
for training recurrent neural networks con-
strained by the echo-state property. In ICLR. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P., 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 12. 
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Indexing 
by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
Deng, L., Hinton, G., and Kingsbury, B. 2013. 
New types of deep neural network learning for 
speech recognition and related applications: An 
overview. In ICASSP. 
Deng, L., Abdel-Hamid, O., and Yu, D., 2013a. A 
deep convolutional neural network using heter-
ogeneous pooling for trading acoustic invari-
ance with phonetic confusion. In ICASSP. 
Dumais, S. T., Letsche, T. A., Littman, M. L., and 
Landauer, T. K. 1997. Automatic cross-linguis-
tic information retrieval using latent semantic 
indexing. In AAAI-97 Spring Symposium Series: 
Cross-Language Text and Speech Retrieval. 
Friedman, J. H. 1999. Greedy function approxi-
mation: a gradient boosting machine. Annals of 
Statistics, 29:1189-1232. 
Gamon, M., Mukherjee, A., Pantel, P. 2014. Pre-
dicting interesting things in text. In COLING. 
Gamon, M., Yano, T., Song, X., Apacible, J. and 
Pantel, P. 2013. Identifying salient entities in 
web pages. In CIKM. 
Gao, J., He, X., and Nie, J-Y. 2010. Clickthrough-
based translation models for web search: from 
word models to phrase models. In CIKM. pp. 
1139-1148. 
Gao, J., He, X., Yih, W-t., and Deng, L. 2014. 
Learning continuous phrase representations for 
translation modeling. In ACL. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR. pp. 675-684.  
Graves, A., Mohamed, A., and Hinton, G. 2013. 
Speech recognition with deep recurrent neural 
networks. In ICASSP. 
Gutmann, M. and Hyvarinen, A. 2010. Noise-con-
trastive estimation: a new estimation principle 
for unnormalized statistical models. In Proc. 
Int. Conf. on Artificial Intelligence and Statis-
tics (AISTATS2010). 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, 29:82-97. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering binary codes for documents by learning 
deep generative models. Topics in Cognitive 
Science, pp. 1-18. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR. pp. 50-57. 
Huang, P., He, X., Gao, J., Deng, L., Acero, A., 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM. 
Jarvelin, K. and Kekalainen, J. 2000. IR evalua-
tion methods for retrieving highly relevant doc-
uments. In SIGIR. pp. 41-48. 
Krizhevsky, A., Sutskever, I. and Hinton, G. 
2012. ImageNet classification with deep convo-
lutional neural networks. In NIPS. 
Lerman, K., and Hogg, T. 2010. Using a model of 
social dynamics to predict popularity of news. 
In WWW. pp. 621-630. 
Markoff, J. 2014. Computer eyesight gets a lot 
more accurate. In New York Times. 
Mikolov, T.. Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In 
INTERSPEECH. pp. 1045-1048. 
Paranjpe, D. 2009. Learning document aboutness 
from implicit user feedback and document 
structure. In CIKM. 
12
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual document representations from 
discriminative projections. In EMNLP. pp. 251-
261. 
Ricci, F., Rokach, L., Shapira, B., and Kantor, P. 
B. (eds) 2011. Recommender System Handbook, 
Springer. 
Robertson, S., and Zaragoza, H. 2009. The proba-
bilistic relevance framework: BM25 and be-
yond. Foundations and Trends in Information 
Retrieval, 3(4):333-389. 
Shen, Y., He, X., Gao. J., Deng, L., and Mesnil, G. 
2014. A latent semantic model with convolu-
tional-pooling structure for information re-
trieval. In CIKM. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic compositionality through recursive 
matrix-vector spaces. In EMNLP. 
Stefanidis, K., Efthymiou, V., Herschel, M., and 
Christophides, V. 2013. Entity resolution in the 
web of data.  CIKM?13 Tutorial. 
Szabo, G., and Huberman, B. A. 2010. Predicting 
the popularity of online content. Communica-
tions of the ACM, 53(8). 
Wu, Q., Burges, C.J.C., Svore, K., and Gao, J. 
2009. Adapting boosting for information re-
trieval measures. Journal of Information Re-
trieval, 13(3):254-270. 
Yih, W., Goodman, J., and Carvalho, V. R. 2006. 
Finding advertising keywords on web pages. In 
WWW. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
 
13
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 292?301,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Maximum Expected BLEU Training of Phrase and Lexicon  Translation Models    Xiaodong He Li Deng Microsoft Research Microsoft Research One Microsoft Way, Redmond, WA, USA One Microsoft Way, Redmond, WA, USA xiaohe@microsoft.com  deng@microsoft.com   Abstract This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.  1. Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g., Och et al, 2002, 2003, Liang et al, 2006, Blunsom et al, 2008, Chiang et al, 2009, Foster et al 2010, Xiao et al 2011). Och (2003) proposed using a log-linear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric.  While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics (Koehn et al, 2003, Brown et al, 1993). Moreover, the 
parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU) (Papineni et al, 2002). Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g., Liang et al, 2006, Chiang et al, 2009). Liang et al (2006) proposed a large set of lexical and Part-of-Speech  features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by picking a pseudo reference. Many such non-desirable heuristics led to moderate gains reported in that work. Chiang et al (2009) improved a syntactic SMT system by adding as many as ten thousand syntactic features, and used Margin Infused Relaxed Algorithm (MIRA) to train the feature weights. However, the number of parameters in common phrase and lexicon translation models is much larger.  In this work, we present a new, highly effective discriminative learning method for phrase and lexicon translation models. The training objective is an expected BLEU score, which is closely linked to translation quality. Further, we apply a Kullback?Leibler (KL) divergence regularization to prevent over-fitting. For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A 
292
similar GT technique has been successfully used in speech recognition (Gopalakrishnan et al, 1991, Povey, 2004, He et al, 2008). Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-to-English dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvement over a state-of-the-art baseline, and the system using the proposed method achieved the best single system translation result in the Chinese-to-English MT track.  2. Related Work One best known approach in discriminative training for SMT is proposed by Och (2003). In that work, multiple features, most of them are derived from generative models, are incorporated into a log-linear model, and the relative weights of them are tuned discriminatively on a small tuning set. However, in practice, this approach only works with a handful of parameters.  More closely related to our work, Liang et al (2006) proposed a large set of lexical and Part-of-Speech features in addition to the phrase translation model. Weights of these features are trained using perceptron on a training set of 67K sentences. In that paper, the authors pointed out that forcing the model to update towards the reference translation could be problematic. This is because the hidden structure such as phrase segmentation and alignment could be abused if the system is forced to produce a reference translation. Therefore, instead of pushing the parameter update towards the reference translation (a.k.a. bold updating), the author proposed a local updating strategy where the model parameters are updated towards a pseudo-reference (i.e., the hypothesis in the n-best list that gives the best BLEU score). Experimental results showed that their approach outperformed a baseline by 0.8 BLEU point when using monotonic decoding, but there was no 
significant gain over a stronger baseline with a full-distortion model. In our work, we use the expectation of BLEU scores as the objective. This avoids the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective.  As another closely related study, Chiang et al (2009) incorporated about ten thousand syntactic features in addition to the baseline features. The feature weights are trained on a tuning set with 2010 sentences using MIRA. In our work, we have many more parameters to train, and the training is conducted on the entire training corpora. Our GT based optimization algorithm is highly parallelizable and efficient, which is the key for large scale discriminative training. As a further related work, Rosti et al (2011) have proposed using differentiable expected BLEU score as the objective to train system combination parameters. Other work related to the computation of expected BLEU in common with ours includes minimum Bayes risk approaches (Smith and Eisner 2006, Tromble et al, 2008) and lattice-based MERT (Macherey et al, 2008). In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning. Wuebker et al (2010) proposed a method to train the phrase translation model using Expectation-Maximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence. However, as pointed out by Liang et al(2006), the same problem as in the bold updating existed, i.e., forced alignment between a source sentence and its reference translation was tricky, and the proposed alignment was likely to be unreliable. The method presented in this paper is free from this problem. 3. Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each 
293
source phrase to a target phrase, re-order target phrases into target sentence (Koehn et al, 2003).  In decoding, the optimal translation ? given the source sentence F is obtained according to   ? = argmax? ? ? ?  (1) where  ? ? ? = 1? ??? ??log ? ??(?, ?)?  (2)  and ? = ??? ??log ? ??(?, ?)??  is the normalization denominator to ensure that the probabilities sum to one. Note that we define the feature functions {??(?, ?)}  in log domain to simplify the notation in later sections. Feature weights ? =  ? {??} are usually tuned by MERT. Features used in a phrase-based system usually include LM, reordering model, word and phrase counts, and phrase and lexicon translation models. Given the focus of this paper, we review only the phrase and lexicon translation models below.   3.1. Phrase translation model A set of phrase pairs are extracted from word-aligned parallel corpus according to phrase extraction rules (Koehn et al, 2003). Phrase translation probabilities are then computed as relative frequencies of phrases over the training dataset. i.e., the probability of translating a source phrase ? to a target phrase ? is computed by   ? ? ? =  ??(?, ?)?(?)  (3)  where ?(?, ?) is the joint counts of ? and ?, and ?(?) is the marginal counts of ?. In translation, the input sentence is segmented into K phrases, and the source-to-target forward phrase (FP) translation feature is scored as:  ? ? ?, ? = ? ?? ???  (4)  where ?? and ?? are the k-th phrase in E and F, respectively.  The target-to-source (backward) phrase translation model is defined similarly.   
3.2. Lexicon translation model There are several variations in lexicon translation features (Ayan and Dorr 2006, Koehn et al, 2003, Quirk et al, 2005). We use the word translation table from IBM Model 1 (Brown et al, 1993) and compute the sum over all possible word alignments within a phrase pair without normalizing for length (Quirk et al, 2005). The source-to-target forward lexicon (FL) translation feature is:  ? ? ?, ? = ? ??,? ??,????  (5)  where ??,?  is the m-th word of the k-th target phrase ??,  ??,? is the r-th word in the k-th source phrase ?? , and ?(??,?|??,?)  is the probability of translating word ??,? to word ??,?. In IBM model 1, these probabilities are learned via maximizing a joint likelihood between the source and target sentences. The target-to-source (backward) lexicon translation model is defined similarly. 4. Maximum Expected-BLEU Training  4.1. Objective function We denote by ? the set of all the parameters to be optimized, including forward phrase and lexicon translation probabilities and their backward counterparts. For simplification of notation,  ??  is formed as a matrix, where its elements {?? } are probabilities subject to ??? = 1. E.g., each row is a probability distribution.  The utility function over the entire training set is defined as:  ?  ?(?)  ?=  ? ??(??,? , ??|??,? , ??) ????(??, ???)????  ???,?,??  ?(6)  where N is the number of sentences in the training set, ???  is the reference translation of the n-th source sentence ??, and ?? ? ???(??) that denotes the list of translation hypotheses of ??. Since the sentences are independent with each other, the joint posterior can be decomposed: 
?? ??,? , ?? ??,? , ?? =  ? ?? ?? ??????    (7) 
294
and ?? ?? ??  is the posterior defined in (2), the subscript ? indicates that it is computed based on the parameter set ?. ? ?  is proportional (with a factor of N) to the expected sentence BLEU score over the entire training set, i.e., after some algebra,  
?(?)  ?=  ? ??(??|??)????(??, ???)??
?
???  In a phrase-based SMT system, the total number of parameters of phrase and lexicon translation models, which we aim to learn discriminatively, is very large (see Table 1). Therefore, regularization is critical to prevent over-fitting. In this work, we regularize the parameters with KL regularization. KL divergence is commonly used to measure the distance between two probability distributions. For the whole parameter set ? , the KL regularization is defined in this work as the sum of KL divergence over the entire parameter space:  ??(??||?) = ??? log ???????  (8)  where ??  is a constant prior parameter set. In training, we want to improve the utility function while keeping the changes of the parameters from ?? at minimum. Therefore, we design the objective function to be maximized as:  ? ? = log? ? ? ? ? ??(??||?) ? (9)  where the prior model ?? in our approach is the relative-frequency-based phrase translation model and the maximum-likelihood-estimated IBM model 1 (word translation model). ? is a hyper-parameter controlling the degree of regularization.   4.2. Optimization In this section, we derived GT formulas for iteratively updating the parameters so as to optimize objective (9). GT is based on extended Baum-Welch (EBW) algorithm first proposed by Gopalakrishnan et al (1991) and commonly used in speech recognition (e.g., He et al 2008).  4.2.1. Extended Baum-Welch Algorithm Baum-Eagon inequality (Baum and Eagon, 1967) gives the GT formula to iteratively maximize positive-coefficient polynomials of random 
variables that are subject to sum-to-one constants. Baum-Welch algorithm is a model update algorithm for hidden Markov model which uses this GT. Gopalakrishnan et al (1991) extended the algorithm to handle rational function, i.e., a ratio of two polynomials, which is more commonly encountered in discriminative training.  Here we briefly review EBW. Assuming a set of random variables ? = {?? }  that subject to the constraint that ??? = 1 , and assume ?(?)and ?(?) are two positive polynomial functions of ? , a GT of ? for the rational function ? ? = ?(?)?(?)  can be obtained through the following two steps:  i) Construct the auxiliary function:  ? ? = ? ? ? ? ?? ? ?  (10)  where ?? are the values from the previous iteration. Increasing f guarantees an increase of r, i.e., ?? ?  > 0 and ? ? ? ? ?? =  ? ?? ? ? ? ? ? ?? .  ii) Derive GT formula for ? ?    
?? = ???
??(?)??? ???? + ? ? ?????? ??(?)??? ????? + ?  
    (11) 
 where D is a smoothing factor.   4.2.2. GT of Translation Models Now we derive the GTs of translation models for our objective.  Since maximizing ? ?  is equivalent to maximizing ?? ? , we have the following auxiliary function:  ? ? = ?(?)???? ? (??||?)    (12)  After substituting (2) and (7) into (6), and drop optimization irrelevant terms in KL regularization, we have ? ?  in a rational function form:  ? ? = ? ? ? ? ??(?)     (13) where     ? ? = ???? ??, ???????  ???,?,?? , ? ? = ?? ??????  , and ? ? =  
295
???? ??, ??????? ???? ??, ???????  ???,?,??  are all positive polynomials of ?. Therefore, we can follow the two steps of EBW to derive the GT formulas for ?. If we denote by  ??  the probability of translating the source phrase i to the target phrase j. Then, the updating formula is (derivation omitted):  ?? = ?? (??, ?, ?, ?)??? + ? ?? ? ? ??? + ??????? (??, ?, ?, ?)???? + ? ?? ? ? + ??  (14) where ? ? = ?/? ?  and  ?? ??, ?, ?, ? =  ??? ?? ??  ?  ? ???? ??, ??? ??? ??  ? ?(??,? = ?, ??,? = ?)? . In which ?? ??  takes a form similar to (6), but is the expected BLEU score for sentence n using models from the previous iteration. ??,? and ??,? are the k-th phrases of ?? and ??, respectively. The smoothing factor set of  ?? according to the Baum-Eagon inequality is usually far too large for practical use. In practice, one general guide of setting ??  is to make all updated value positive. Similar to (Povey 2004), we set ?? by  ?? = max ?(0,??? (??, ?, ?, ?)???? )   (15)  to ensure the denominator of (15) is positive. Further, we set a low-bound of ??  as max?{? ? ? ??,?,?,???? ??? }  to guarantee the numerator to be positive. We denote by ? ?  the probability of translating the source word i to the target word j. Then following the same derivation, we get the updating formula for forward lexicon translation model:  ? ? = ?? (??, ?, ?, ?)??? + ? ?? ? ? ? ?? + ??? ???? (??, ??, ?, ?)???? + ? ?? ? ? + ??  (16) where ? ? = ?/? ?  and ?? ??, ?, ?, ?  ?=  ??? ?? ??  ?  ? ???? ??, ??? ??? ?? ? ?(??,?,? = ?)?? ? ?, ?,?, ? , and  ?? ?, ?,?, ? = ?(??,?,???)??(??,?,?|??,?,?)? ??(??,?,?|??,?,?)? , in which ??,?,? and ??,?,? are the r-th and m-th word in the k-th phrase of the source sentence ?? and the target hypothesis ??, respectively. Value of ??  is set in a 
way similar to (15). GTs for updating backward phrase and lexicon translation models can be derived in a similar way, and is omitted here.  4.3. Implementation issues   4.3.1. Normalizing ? The posterior ??? ?? ??  in the model updating formula is computed according to (2). In decoding, only the relative values of ? matters. However, the absolute value will affect the posterior distribution, e.g., an overly large absolute value of ? would lead to a very sharp posterior distribution. In order to control the sharpness of the posterior distribution, we normalize ? by its L1 norm:  ?? = ??|??|?   (17)  4.3.2. Computing the sentence BLEU sore The commonly used BLEU-4 score is computed by  ????-?? 4 = BP ? exp 14 log??????    (18)  In the updating formula, we need to compute the sentence-level ???? ??, ??? . Since the matching count may be sparse at the sentence level, we smooth raw precisions of high-order n-grams by:  ?? = #(?-?????? ????????) + ? ? ???#(?-??????) + ?    (19)  where ??? is the prior value of ??, ? is a smoothing factor usually takes a value of 5 and  ??? can be set by ??? = ???? ? ???? ????, for n = 3, 4. ?? and ?? are estimated empirically. Brevity penalty (BP) also plays a key role. Instead of clip it at 1, we use a non-clipped BP, ?? = ?(????), for sentence-level BLEU1. We further scale the reference length, r, by a factor such that the total length of references on the training set equals that of the baseline output2.                                                              1 This is to better approximate corpus-level BLEU, i.e., as discussed in (Chiang, et al, 2008), the per-sentence BP might effectively exceed unity in corpus-level BLEU computation. 2  This is to focus the training on improving BLEU by improving n-gram match instead of by improving BP, e.g., this makes the BP of the baseline output already being perfect. 
296
4.3.3. Training procedure The parameter set ? is optimized on the training set while the feature weights ? are tuned on a small tuning set3. Since ? and ? affect the training of each other, we train them in alternation. I.e., at each iteration, we first fix ? and update ?, then we re-tune ? given the new ?. Due to mismatch between training and tuning data, the training process might not always converge. Therefore, we need a validation set to determine the stop point of training. At the end, ? and ? that give the best score on the validation set are selected and applied to the test set. Fig. 1 gives a summary of the training procedure. Note that step 2 and 4 are parallelize-able across multiple processors.   
 Figure 1. The max expected-BLEU training algorithm. 5. Evaluation In evaluating the proposed method, we use two separate datasets. We first describe the experiments with the Europarl dataset (Koehn 2002), followed by the experiments with the more recent IWSLT-2011 task (Federico et al, 2011).  5.1 Experimental setup in the Europarl task In evaluating the proposed method, we use two separate datasets. First, we conduct experiments on the Europarl German-to-English dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ?  tuning, and the rest for validation. The test set consists of 2000 sentences.                                                              3 Usually, the tuning set matches the test condition better, and therefore is preferable for ? tuning. 
To build the baseline phrase-based SMT system, we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extract the phrase table from the word aligned bilingual texts (Koehn et al, 2003). The maximum phrase length is set to four. Other models used in the baseline system include lexicalized ordering model, word count and phrase count, and a 3-gram LM trained on the English side of the parallel training corpus. Feature weights are tuned by MERT. A fast beam-search phrase-based decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. Details of the phrase and lexicon translation models are given in Table 1. This baseline achieves a BLEU score of 26.22% on the test set. This baseline system is also used to generate a 100-best list of the training corpus during maximum expected BLEU training.       Translation model  # parameters Phrase models (fore. & back.)   9.2 M Lexicon model (IBM-1 src-to-tgt) 12.9 M Lexicon model (IBM-1 tgt-to-src) 11.9 M Table 1. Summary of phrase and lexicon translation models  5.2 Experimental results on the Europarl task During training, we first tune the regularization factor ? based on the performance on the validation set. For simplicity reasons, the tuning of ? makes use of only the phrase translation models.  Table 2 reports the BLEU scores and gains over the baseline given different values of ?. The results highlight the importance of regularization. While ? =  ? ? ?5?10?? gives the best score on the validation set, the gain is shown to be substantially reduced to merely 0.2 BLEU point when ? = 0, i.e., no regularization.  We set the optimal value of ? = 5?10?? in all remaining experiments.   Test on Validation Set ????% ?????% Baseline 26.70 -- ? = 0 (no regularization) 26.91 +0.21 ? =  ? ? ?1?10?? 27.31 +0.61 ? =  ? ? ?5?10?? 27.44 +0.74 ? = 10?10?? 27.27 +0.57 Table 2. Results on degrees of regularizations. BLEU scores are reported on the validation set. ????? denotes the gain over the baseline.  Fixing the optimal regularization factor ?, we then study the relationship between the expected 
1. Build the baseline system, estimate { ?, ? }. 2. Decode N-best list for training corpus using the baseline system, compute ????(??,???). 3. set ?? = ?, ?? = ?. 4. Max expected BLEU training  a. Go through the training set. i. Compute ????(??|??) and ??(??) . ii. Accumulate statistics {?}. b. Update: ?? ?  ?? by one iteration of GT. 5. MERT on the tuning set: ??? ? ?. 6. Test on the validation set using { ?, ? }. 7. Go to step 3 unless training converges or reaches a certain number of iterations. 8. Pick the best { ?, ? } on the validation set.  
297
sentence-level BLEU (Exp. BLEU) score of N-best lists and the corpus-level BLEU score of 1-best translations. The conjectured close relationship between the two is important in justifying our use of the former as the training objective. Fig. 2 shows these two scores on the training set over training iterations. Since the expected BLEU is affected by ? strongly, we fix the value of ? in order to make the expected BLEU comparable across different iterations. From Fig. 2 it is clear that the expected BLEU score correlates strongly with the real BLEU score, justifying its use as our training objective.  
        Figure 2. Expected sentence BLEU and 1-best corpus BLEU on the 751K sentence of training data.  Next, we study the effects of training the phrase translation probabilities and the lexicon translation probabilities according to the GT formulas presented in the preceding section. The break-down results are shown in Table 3. Compared with the baseline, training phrase or lexicon models alone gives a gain of 0.7 and 0.5 BLEU points, respectively, on the test set. For a full training of both phrase and lexicon models, we adopt two learning schedules: update both models together at each iteration (simultaneously), or update them in two stages (two-stage), where the phrase models are trained first until reaching the best score on the validation set and then the lexicon models are trained. Both learning schedules give significant improvements over the baseline and also over training phrase or lexicon models alone. The two-stage training of both models gives the best result of 27.33%, outperforming the baseline by 1.1 BLEU points.  
More detail of the two-stage training is provided in Fig. 3, where BLEU scores in each stage are shown as a function of the GT training iteration. The phrase translation probabilities (PT) are trained alone in the first stage, shown in blue color. After five iterations, the BLEU score on the validation set reaches the peak value, with further iteration giving BLEU score fluctuation. Hence, we perform lexicon model (LEX) training starting from the sixth iteration with the corresponding BLEU scores shown in red color in Fig. 3. The BLEU score is further improved by 0.4 points after additional three iterations of training the lexicon models. In total, nine iterations are performed to complete the two-stage GT training of all phrase and lexicon models.   BLEU (%) validation test Baseline 26.70 26.22 Train phrase models alone 27.44 26.94* Train lexicon models alone 27.36 26.71 Both models: simultaneously  27.65 27.13* Both models: two-stage 27.82 27.33* Table 3. Results on the Europarl German-to-English dataset. The BLEU measures from various settings of maximum expected BLEU training are compared with the baseline, where * denotes that the gain over the baseline is statistically significant with a significance level > 99%, measured by paired bootstrap resampling method proposed by Koehn (2004).              
  Figure 3. BLEU scores on the validation set as a function of the GT training iteration in two-stage training of both the phrase translation models (PT) and the lexicon models (LEX). The BLEU scores on training phrase models are shown in blue, and on training lexicon models in red.  
32%	 ?
33%	 ?
34%	 ?
35%	 ?
36%	 ?
37%	 ?
25%	 ?
26%	 ?
27%	 ?
28%	 ?
29%	 ?
30%	 ?
0	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ?
Exp.	 ?BLEU	 ?
	 ?BLEU	 ?
26.0%	 ?
26.5%	 ?
27.0%	 ?
27.5%	 ?
28.0%	 ?
0	 ? 1	 ? 2	 ? 3	 ? 4	 ? 5	 ? 6	 ? 7	 ? 8	 ? 9	 ? 10	 ?
PT	 ?
LEX	 ?
#iteration 
Expec
ted BL
EU 
BLEU
 
BLEU
 
#iteration 
298
5.3 Experiments on the IWSLT2011 benchmark As the second evaluation task, we apply our new method described in this paper to the 2011 IWSLT Chinese-to-English machine translation benchmark (Federico et al, 2011). The main focus of the IWSLT2011 Evaluation is the translation of TED talks (www.ted.com). These talks are originally given in English. In the Chinese-to-English translation task, we are provided with human translated Chinese text with punctuations inserted. The goal is to match the human transcribed English speech with punctuations.  This is an open-domain spoken language translation task. The training data consist of 110K sentences in the transcripts of the TED talks and their translations, in English and Chinese, respectively. Each sentence consists of 20 words on average. Two development sets are provided, namely, dev2010 and tst2010. They consist of 934 sentences and 1664 sentences, respectively. We use dev2010 for ? tuning and tst2010 for validation. The test set tst2011 consists of 1450 sentences. In our system, a primary phrase table is trained from the 110K TED parallel training data, and a 3-gram LM is trained on the English side of the parallel data. We are also provided additional out-of-domain data for potential usage. From them, we train a secondary 5-gram LM on 115M sentences of supplementary English data, and a secondary phrase table from 500K sentences selected from the supplementary UN corpus by the method proposed by Axelrod et al (2011).  In carrying out the maximum expected BLEU training, we use 100-best list and tune the regularization factor to the optimal value of ? = 1?10?? . We only train the parameters of the primary phrase table. The secondary phrase table and LM are excluded from the training process since the out-of-domain phrase table is less relevant to the TED translation task, and the large LM slows down the N-best generation process significantly.  At the end, we perform one final MERT to tune the relative weights with all features including the secondary phrase table and LM.  The translation results are presented in Table 4. The baseline is a phrase-based system with all features including the secondary phrase table and LM. The new system uses the same features except that the primary phrase table is discriminatively 
trained using maximum expected-BLEU and GT optimization as described earlier in this paper.   The results are obtained using the two-stage training schedule, including six iterations for training phrase translation models and two iterations for training lexicon translation models. The results in Table 4 show that the proposed method leads to an improvement of 1.2 BLEU point over the baseline. This gives the best single system result on this task.  BLEU (%) Validation Test Baseline 11.48 14.68 Max expected  BLEU training 12.39 15.92 Table 4. The translation results on IWSLT 2011 MT_CE task.  6. Summary  The contributions of this work can be summarized as follows. First, we propose a new objective function (Eq. 9) for training of large-scale translation models, including phrase and lexicon models, with more parameters than all previous methods have attempted. The objective function consists of 1) the utility function of expected BLEU score, and 2) the regularization term taking the form of KL divergence in the parameter space. The expected BLEU score is closely linked to translation quality and the regularization is essential when many parameters are trained at scale. The importance of both is verified experimentally with the results presented in this paper.  Second, through non-trivial derivation, we show that the novel objective function of Eq. (9) is amenable to iterative GT updates, where each update is equipped with a closed-form formula.  Third, the new objective function and new optimization technique are successfully applied to two important machine translation tasks, with implementation issues resolved (e.g., training schedule and hyper-parameter tuning, etc.).  The superior results clearly demonstrate the effectiveness of the proposed algorithm. Acknowledgments  The authors are grateful to Chris Quirk, Mei-Yuh Hwang, and Bowen Zhou for the assistance with the MT system and/or for the valuable discussions.  
299
References  Amittai Axelrod, Xiaodong He, Jianfeng Gao. 2011, Domain adaptation via pseudo in-domain data selection. In Proc. of EMNLP, 2011. Necip Fazil Ayan, and Bonnie J. Dorr. Going. 2006. Beyond AER: an extensive analysis of word alignments and their impact on MT. In Proc. of COLING-ACL, 2006. Leonard Baum and J. A. Eagon. 1967. An inequality with applications to statistical prediction for functions of Markov processes and to a model of ecology, Bulletin of the American Mathematical Society, Jan. 1967. Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A discriminative latent variable model for statistical machine translation. In Proc. of ACL 2008. Peter F. Brown, Stephen A. Della Pietra, Vincent Della J.Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 1993. David Chiang, Steve DeNeefe, Yee Seng Chan, and Hwee Tou Ng, 2008. Decomposability of translation metrics for improved evaluation and efficient algorithms. In Proc. of EMNLP, 2008. David Chiang, Kevin Knight and Weri Wang, 2009. 11,001 new features for statistical machine translation. In Proc. of NAACL-HLT, 2009. Marcello Federico, L. Bentivogli, M. Paul, and S. Stueker. 2011. Overview of the IWSLT 2011 Evaluation Campaign. In Proc. of IWSLT, 2011. George Foster, Cyril Goutte and Roland Kuhn. 2010. Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation. In Proc. of EMNLP, 2010. P. S. Gopalakrishnan, Dimitri Kanevsky, Arthur Nadas, and David Nahamoo. 1991. An inequality for rational functions with applications to some statistical estimation problems. IEEE Trans. Inform. Theory, 1991. Xiaodong He. 2007. Using Word-Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation. 
In Proc. of the Second ACL Workshop on Statistical Machine Translation. Xiaodong He, Li Deng, Wu Chou, 2008.  Discriminative learning in sequential pattern recognition. IEEE Signal Processing Magazine, Sept. 2008. Philipp Koehn. 2002. Europarl: A Multilingual Corpus for Evaluation of Machine Translation. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase based translation. In Proc. of NAACL. 2003. Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP 2004. Percy Liang, Alexandre Bouchard-Cote, Dan Klein and Ben. Taskar. 2006. An end-to-end discriminative approach to machine translation, In Proc. of COLING-ACL, 2006. Wolfgang Macherey, Franz Josef Och, gnacio Thayer, and Jakob Uskoreit. 2008. Lattice-based minimum error rate training for statistical machine translation. In Proc. of EMNLP 2008. Robert Moore and Chris Quirk. 2007. Faster Beam-Search Decoding for Phrasal Statistical Machine Translation. In Proc. of MT Summit XI. Franz Josef Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation, In Proc. of ACL 2002. Franz Josef Och, 2003, Minimum error rate training in statistical machine translation. In Proc. of ACL 2003. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL 2002. Daniel Povey. 2004. Discriminative Training for large Vocabulary Speech Recognition. Ph.D. dissertation, Cambridge University, Cambridge, UK, 2004. Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proc. of ACL 2005. 
300
Antti-Veikko Rosti, Bing hang, Spyros Matsoukas, and Richard Schard Schwartz. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Proc. of workshop on statistical machine translation 2011.  David A Smith, Jason Eisner. 2006. Minimum risk annealing for training log-linear models, In Proc. of COLING-ACL 2006. Joern Wuebker, Arne Mauser and Hermann Ney. 2010. Training phrase translation models with leaving-one-out, In Proc. of ACL 2010. Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang Macherey. 2008. Lattice Minimum Bayes-Risk decoding for statistical machine translation. In Proc. of EMNLP 2008. Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin. 2011. Fast Generation of Translation Forest for Large-Scale SMT Discriminative Training. In Proc. Of  EMNLP 2011.   
301


References
 Amittai Axelrod, Xiaodong He, Jianfeng Gao.
2011, Domain adaptation via pseudo in-domain 
data selection. In Proc. of EMNLP, 2011.
Necip Fazil Ayan, and Bonnie J. Dorr. Going. 
2006. Beyond AER:  an extensive analysis of 
word alignments and their impact on MT. In 
Proc. of COLING-ACL, 2006.
Leonard Baum and J.  A.  Eagon. 1967.  An 
inequality with applications to statistical 
prediction for functions of Markov processes 
and to a model of ecology, Bulletin of the
American Mathematical Society, Jan. 1967.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 
2008. A discriminative latent variable model for 
statistical machine translation. In Proc. of ACL
2008.
Peter F. Brown, Stephen A. Della Pietra, Vincent 
Della J.Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
Parameter estimation. Computational 
Linguistics, 1993.
David Chiang, Steve DeNeefe, Yee Seng Chan, 
and Hwee Tou Ng, 2008. Decomposability of 
translation metrics for improved evaluation and 
efficient algorithms. In Proc. of EMNLP, 2008.
David Chiang, Kevin Knight and Weri Wang, 
2009.  11,001 new features for  statistical 
machine translation. In  Proc. of NAACL-HLT, 
2009.
Marcello Federico, L. Bentivogli, M. Paul, and S. 
Stueker. 2011. Overview of the IWSLT 2011 
Evaluation Campaign. In Proc. of IWSLT, 2011.
George Foster,  Cyril Goutte and Roland Kuhn. 
2010. Discriminative Instance Weighting for 
Domain Adaptation in  Statistical Machine 
Translation. In Proc. of EMNLP, 2010.
P.  S.  Gopalakrishnan, Dimitri Kanevsky, Arthur 
Nadas, and David Nahamoo. 1991. An 
inequality for rational functions with 
applications to some statistical estimation 
problems. IEEE Trans. Inform. Theory, 1991.
Xiaodong He. 2007. Using Word-Dependent 
Transition Models in HMM based Word 
Alignment for Statistical Machine Translation. 
In  Proc. of the Second ACL Workshop on 
Statistical Machine Translation.
Xiaodong He, Li Deng, Wu Chou,  2008.  
Discriminative learning in sequential pattern 
recognition. IEEE Signal Processing Magazine, 
Sept. 2008.
Philipp Koehn. 2002. Europarl: A Multilingual 
Corpus for Evaluation of Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase based 
translation. In Proc. of NAACL. 2003.
Philipp Koehn. 2004. Statistical significance tests 
for machine translation evaluation. In  Proc. of 
EMNLP 2004.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein 
and Ben. Taskar. 2006. An end-to-end 
discriminative approach to machine translation,
In Proc. of COLING-ACL, 2006.
Wolfgang Macherey, Franz Josef Och, gnacio 
Thayer, and Jakob Uskoreit. 2008. Lattice-based 
minimum error rate training for statistical 
machine translation. In Proc. of EMNLP 2008.
Robert Moore and Chris Quirk. 2007. Faster 
Beam-Search Decoding for Phrasal Statistical 
Machine Translation. In Proc. of MT Summit 
XI.
Franz Josef Och and H. Ney. 2002. Discriminative 
training and maximum entropy models for 
statistical machine translation, In Proc. of ACL 
2002.
Franz Josef Och, 2003, Minimum  error rate 
training in statistical machine translation. In 
Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proc. of ACL 2002.
Daniel Povey. 2004. Discriminative Training for 
large Vocabulary Speech Recognition. Ph.D. 
dissertation, Cambridge University, Cambridge, 
UK, 2004.
Chris Quirk, Arul Menezes, and Colin Cherry. 
2005. Dependency treelet translation: 
Syntactically informed phrasal SMT. In Proc. of 
ACL 2005.
Antti-Veikko Rosti, Bing hang, Spyros Matsoukas, 
and Richard Schard Schwartz. 2011. Expected 
BLEU  training for graphs: bbn system 
description for WMT system combination task. 
In  Proc. of  workshop on statistical machine 
translation 2011. 
David A Smith, Jason Eisner. 2006. Minimum risk 
annealing for training log-linear models,  In 
Proc. of COLING-ACL 2006.
Joern Wuebker, Arne Mauser and Hermann Ney.
2010. Training phrase translation models with 
leaving-one-out, In Proc. of ACL 2010.
Roy Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice Minimum 
Bayes-Risk decoding for statistical machine 
translation. In Proc. of EMNLP 2008.
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun 
Lin. 2011. Fast Generation of Translation Forest 
for Large-Scale SMT Discriminative
Training. In Proc. Of EMNLP 2011.Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699?709,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Learning Continuous Phrase Representations for                                     
Translation Modeling 
 
Jianfeng Gao    Xiaodong He    Wen-tau Yih    Li Deng 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{jfgao,xiaohe,scottyih,deng}@microsoft.com 
 
 
 
Abstract 
This paper tackles the sparsity problem in 
estimating phrase translation probabilities 
by learning continuous phrase representa-
tions, whose distributed nature enables the 
sharing of related phrases in their represen-
tations. A pair of source and target phrases 
are projected into continuous-valued vec-
tor representations in a low-dimensional 
latent space, where their translation score 
is computed by the distance between the 
pair in this new space. The projection is 
performed by a neural network whose 
weights are learned on parallel training 
data. Experimental evaluation has been 
performed on two WMT translation tasks. 
Our best result improves the performance 
of a state-of-the-art phrase-based statistical 
machine translation system trained on 
WMT 2012 French-English data by up to 
1.3 BLEU points. 
1 Introduction 
The phrase translation model, also known as the 
phrase table, is one of the core components of 
phrase-based statistical machine translation (SMT) 
systems. The most common method of construct-
ing the phrase table takes a two-phase approach 
(Koehn et al 2003). First, the bilingual phrase 
pairs are extracted heuristically from an automat-
ically word-aligned training data. The second 
phase, which is the focus of this paper, is parame-
ter estimation where each phrase pair is assigned 
with some scores that are estimated based on 
counting these phrases or their words using the 
same word-aligned training data. 
Phrase-based SMT systems have achieved 
state-of-the-art performance largely due to the fact 
that long phrases, rather than single words, are 
used as translation units so that useful context in-
formation can be captured in selecting translations. 
However, longer phrases occur less often in train-
ing data, leading to a severe data sparseness prob-
lem in parameter estimation. There has been a 
plethora of research reported in the literature on 
improving parameter estimation for the phrase 
translation model (e.g., DeNero et al 2006; 
Wuebker et al 2010; He and Deng 2012; Gao and 
He 2013).  
This paper revisits the problem of scoring a 
phrase translation pair by developing a Continu-
ous-space Phrase Translation Model (CPTM). 
The translation score of a phrase pair in this model 
is computed as follows. First, we represent each 
phrase as a bag-of-words vector, called word vec-
tor henceforth. We then project the word vector, 
in either the source language or the target lan-
guage, into a respective continuous feature vector 
in a common low-dimensional space that is lan-
guage independent. The projection is performed 
by a multi-layer neural network. The projected 
feature vector forms the continuous representa-
tion of a phrase. Finally, the translation score of a 
source-target phrase pair is computed by the dis-
tance between their feature vectors.  
The main motivation behind the CPTM is to 
alleviate the data sparseness problem associated 
with the traditional counting-based methods by 
grouping phrases with a similar meaning across 
different languages. This style of grouping is 
made possible because of the distributed nature of 
the continuous-space representations for phrases. 
No such sharing was possible in the original sym-
bolic space for representing words or phrases.  In 
this model, semantically or grammatically related 
phrases, in both the source and the target lan-
guages, would tend to have similar (close) feature 
vectors in the continuous space, guided by the 
training objective. Since the translation score is a 
smooth function of these feature vectors, a small 
699
change in the features should only lead to a small 
change in the translation score. 
The primary research task in developing the 
CPTM is learning the continuous representation 
of a phrase that is effective for SMT. Motivated 
by recent studies on continuous-space language 
models (e.g., Bengio et al 2003; Mikolov et al 
2011; Schwenk et al, 2012), we use a neural net-
work to project a word vector to a feature vector. 
Ideally, the projection would discover those latent 
features that are useful to differentiate good trans-
lations from bad ones, for a given source phrase. 
However, there is no training data with explicit 
annotation on the quality of phrase translations. 
The phrase translation pairs are hidden in the par-
allel source-target sentence pairs, which are used 
to train the traditional translation models. The 
quality of a phrase translation can only be judged 
implicitly through the translation quality of the 
sentences, as measured by BLEU, which contain 
the phrase pair. In order to overcome this chal-
lenge and let the BLEU metric guide the projec-
tion learning, we propose a new method to learn 
the parameters of a neural network. This new 
method, via the choice of an appropriate objective 
function in training, automatically forces the fea-
ture vector of a source phrase to be closer to the 
feature vectors of its candidate translations. As a 
result, the BLEU score is improved when these 
translations are selected by an SMT decoder to 
produce final, sentence-level translations. The 
new learning method makes use of the L-BFGS 
algorithm and the expected BLEU as the objective 
function defined on N-best lists. 
To the best of our knowledge, the CPTM pro-
posed in this paper is the first continuous-space 
phrase translation model that makes use of joint 
representations of a phrase in the source language 
and its translation in the target language (to be de-
tailed in Section 4) and that is shown to lead to 
significant improvement over a standard phrase-
based SMT system (to be detailed in Section 6).  
Like the traditional phrase translation model, 
the translation score of each bilingual phrase pair 
is modeled explicitly in our model. However, in-
stead of estimating the phrase translation score on 
aligned parallel data, our model intends to capture 
the grammatical and semantic similarity between 
a source phrase and its paired target phrase by pro-
jecting them into a common, continuous space 
that is language independent. 
                                                          
1 Niehues et al (2011) use different translation units in order 
to integrate the n-gram translation model into the phrase-
based approach. However, it is not clear how a continuous 
The rest of the paper is organized as follows. 
Section 2 reviews previous work. Section 3 re-
views the log-linear model for phrase-based SMT 
and Sections 4 presents the CPTM. Section 5 de-
scribes the way the model parameters are esti-
mated, followed by the experimental results in 
Section 6. Finally, Section 7 concludes the paper. 
2 Related Work 
Representations of words or documents as contin-
uous vectors have a long history. Most of the ear-
lier latent semantic models for learning such vec-
tors are designed for information retrieval 
(Deerwester et al 1990; Hofmann 1999; Blei et al 
2003). In contrast, recent work on continuous 
space language models, which estimate the prob-
ability of a word sequence in a continuous space 
(Bengio et al 2003; Mikolov et al 2010), have ad-
vanced the state of the art in language modeling, 
outperforming the traditional n-gram model on 
speech recognition (Mikolov et al 2012; Sunder-
meyer et al 2013) and machine translation 
(Mikolov 2012; Auli et al 2013). 
Because these models are developed for mono-
lingual settings, word embedding from these mod-
els is not directly applicable to translation. As a 
result, variants of such models for cross-lingual 
scenarios have been proposed so that words in dif-
ferent languages are projected into the shared la-
tent vector space (Dumais et al 1997; Platt et al 
2010; Vinokourov et al 2002; Yih et al 2011; 
Gao et al 2011; Huang et al 2013; Zou et al 
2013). In principle, a phrase table can be derived 
using any of these cross-lingual models, although 
decoupling the derivation from the SMT training 
often results in suboptimal performance (e.g., 
measured in BLEU), as we will show in Section 6. 
Recently, there is growing interest in applying 
continuous-space models for translation. The 
most related to this study is the work of continu-
ous space n-gram translation models (Schwenk et 
al. 2007; Schwenk 2012; Son et al 2012), where 
the feed-forward neural network language model 
is extended to represent translation probabilities. 
However, these earlier studies focused on the n-
gram translation models, where the translation 
probability of a phrase or a sentence is decom-
posed as a product of n-gram probabilities as in a 
standard n-gram language model. Therefore, it is 
not clear how their approaches can be applied to 
the phrase translation model1, which is much more 
version of such a model can be trained efficiently because the 
factor models used by Son et al cannot be applied directly. 
700
widely used in modern SMT systems. In contrast, 
our model learns jointly the representations of a 
phrase in the source language as well as its trans-
lation in the target language. The recurrent contin-
uous translation models proposed by Kalchbren-
ner and Blunsom (2013) also adopt the recurrent 
language model (Mikolov et al 2010). But unlike 
the n-gram translation models above, they make 
no Markov assumptions about the dependency of 
the words in the target sentence. Continuous space 
models have also been used for generating trans-
lations for new words (Mikolov et al 2013a) and 
ITG reordering (Li et al 2013). 
There has been a lot of research on improving 
the phrase table in phrase-based SMT (Marcu and 
Wong 2002; Lamber and Banchs 2005; Denero et 
al. 2006; Wuebker et al 2010; Zhang et al, 2011; 
He and Deng 2012; Gao and He 2013). Among 
them, (Gao and He 2013) is most relevant to the 
work described in this paper. They estimate 
phrase translation probabilities using a discrimi-
native training method under the N-best reranking 
framework of SMT. In this study we use the same 
objective function to learn the continuous repre-
sentations of phrases, integrating the strengths as-
sociated with these earlier studies. 
3 The Log-Linear Model for SMT 
Phrase-based SMT is based on a log-linear model 
which requires learning a mapping between input 
? ? ? to output ? ? ?. We are given 
? Training samples (?? , ??)  for ? = 1??,  
where each source sentence ?? is paired with 
a reference translation in target language ??; 
? A procedure GEN to generate a list of N-best 
candidates GEN(??) for an input ?? , where 
GEN  in this study is the baseline phrase-
based SMT system, i.e., an in-house 
implementation of the Moses system (Koehn 
et al 2007) that does not use the CPTM, and 
each ? ? GEN(??)  is labeled by the 
sentence-level BLEU score (He and Deng 
2012), denoted by sBleu(??, ?) , which 
measures the quality of ? with respect to its 
reference translation ??; 
? A vector of features ? ? ?? that maps each 
(??, ?) to a vector of feature values
2; and 
? A parameter vector ? ? ??, which assigns a 
real-valued weight to each feature. 
                                                          
2 Our baseline system uses a set of standard features sug-
gested in Koehn et al (2007), which is also detailed in Sec-
tion 6. 
The components GEN(. ), ? and ?  define a log-
linear model that maps ?? to an output sentence as 
follows: 
?? = argmax
(?,?)?GEN(??)
?T?(??, ?, ?) (1) 
which states that given ? and ?, argmax returns 
the highest scoring translation ??,  maximizing 
over  correspondences ?. In phrase-based SMT, ? 
consists of a segmentation of the source and target 
sentences into phrases and an alignment between 
source and target phrases. Since computing the 
argmax  exactly is intractable, it is commonly 
performed approximatedly by beam search (Och 
and Ney 2004). Following Liang et al (2006), we 
assume that every translation candidate is always 
coupled with a corresponding ?, called the Viterbi 
derivation, generated by (1). 
4 A Continuous-Space Phrase Transla-
tion Model (CPTM) 
The architecture of the CPTM is shown in Figures 
1 and 2, where for each pair of source and target 
phrases (??, ??)  in a source-target sentence pair, 
we first project them into feature vectors ??? and 
??? in a latent, continuous space via a neural net-
work with one hidden layer (as shown in Figure 
2), and then compute the translation score, 
score(??, ??), by the distance of their feature vec-
tors in that space. 
We start with a bag-of-words representation of 
a phrase ? ? ??, where ? is a word vector and ? 
is the size of the vocabulary consisting of words 
in both source and target languages, which is set 
to 200K in our experiments. We then learn to pro-
ject ? to a low-dimensional continuous space ??: 
?(?): ?? ? ??  
The projection is performed using a fully con-
nected neural network with one hidden layer and 
tanh activation functions. Let ?1 be the projec-
tion matrix from the input layer to the hidden layer 
and ?2  the projection matrix from the hidden 
layer to the output layer, we have 
? ? ?(?) = tanh (?2
T(tanh(?1
T?))) (2) 
 
 
701
 
Figure 2. A neural network model for phrases 
giving rise to their continuous representations. 
The model with the same form is used for both 
source and target languages. 
 
  
The translation score of a source phrase f and a 
target phrase e can be measured as the similarity   
(or distance) between their feature vectors. We 
choose the dot product as the similarity function3: 
score(?, ?) ? sim?(?? , ??) = ??
T?? (3) 
According to (2), we see that the value of the scor-
ing function is determined by the projection ma-
trices ? = {?1,?2}. 
The CPTM of (2) and (3) can be incorporated 
into the log-linear model for SMT (1) by 
                                                          
3 In our experiments, we compare dot product and the cosine 
similarity functions and find that the former works better for 
nonlinear multi-layer neural networks, and the latter works 
better for linear neural networks. For the sake of clarity, we 
choose dot product when we describe the CPTM and its train-
ing in Sections 4 and 5, respectively. 
4 The baseline SMT needs to be reasonably good in the 
sense that the oracle BLEU score on the generated n-best 
introducing a new feature ??+1  and a new feature 
weight ??+1. The new feature is defined as 
??+1(??, ?, ?) = ? sim?(?? , ??)(?,? )??   (4) 
Thus, the phrase-based SMT system, into which 
the CPTM is incorporated, is parameterized by 
(?, ?), where ? is a vector of a handful of param-
eters used in the log-linear model of (1), with one 
weight for each feature; and ? is the projection 
matrices used in the CPTM defined by (2) and (3). 
In our experiments we take three steps to learn 
(?, ?): 
1. We use a baseline phrase-based SMT sys-
tem to generate for each source sentence in 
training data an N-best list of translation hy-
potheses4. 
2. We set ? to that of the baseline system and 
let ??+1 = 1, and optimize ? w.r.t. a loss 
function on training data5. 
3. We fix ? , and optimize ?  using MERT 
(Och 2003) to maximize BLEU on dev data. 
In the next section, we will describe Step 2 in de-
tail as it is directly related to the CPTM training. 
 
lists needs to be significantly higher than that of the top-1 
translations so that the CPTM can be effectively trained. 
5 The initial value of ??+1 can also be tuned using the dev 
set. However, we find in a pilot study that it is good enough 
to set it to 1 when the values of all the baseline feature 
weights, used in the log-linear model of (1), are properly nor-
malized, such as by setting ?? = ??/?  for ? = 1?? , 
where ? is the unnormalized weight value of the target lan-
guage model. 
 
Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous repre-
sentation is shown in Figure 2. 
 
 
200K (d) 
100 
100 (?) 
(?1???) 
Word vector 
Neural network 
Feature vector 
?1 
?2 
? 
? 
Raw phrase  ? or ? 
  ?(the process of)         (machine translation)     
(consists of). . . 
 ?(le processus de)    
(traduction automatique)  (consiste en). . .  
???1 ?? ??+1 
???1 ?? ??+1 
??? 
 
 
??? 
 
??(?)
score(??, ??) = ???
T ???  
 
Target phrases  
Continuous representations of  
target phrases  
Source phrases  
 
Continuous representations of  
source phrases  
Translation score as dot product of 
feature vectors in the continuous space 
702
5 Training CPTM 
This section describes the loss function we em-
ploy with the CPTM and the algorithm to train the 
neural network weights. 
We define the loss function ?(?) as the nega-
tive of the N-best list based expected BLEU, de-
noted by xBleu(?). In the reranking framework of 
SMT outlined in Section 3, xBleu(?) over one 
training sample (??, ??) is defined as 
xBleu(?) = ? ?(?|??)sBleu(??, ?)??GEN(??)  (5) 
where sBleu(??, ?)  is the sentence-level BLEU 
score, and  ?(?|??) is the translation probability 
from ?? to ? computed using softmax as  
?(?|??) =
exp(??T?(??,?,?))
? exp(??T?(??,??,?))???GEN(??)
  (6) 
where ?T? is the log-linear model of (1), which 
also includes the feature derived from the CPTM 
as defined by (4), and ? is a tuned smoothing fac-
tor. 
Let ?(?) be a loss function which is differen-
tiable w.r.t. the parameters of the CPTM, ?. We 
can compute the gradient of the loss and learn ? 
using gradient-based numerical optimization al-
gorithms, such as L-BFGS or stochastic gradient 
descent (SGD).  
5.1 Computing the Gradient 
Since the loss does not explicitly depend on ?, we 
use the chain rule for differentiation: 
??(?)
??
= ?
??(?)
?sim?(?? , ??)
?sim?(?? , ??)
??
(?,? )
 
= ? ??(?,?)
?sim?(?? , ??)
??
(?,? )
 (7) 
which takes the form of summation over all phrase 
pairs occurring either in a training sample (sto-
chastic mode) or in the entire training data (batch 
mode). ?(?,?) in (7) is known as the error term of 
the phrase pair (?, ?), and is defined as   
?(?,?) = ?
??(?)
?sim?(??,??)
  (8) 
It describes how the overall loss changes with the 
translation score of the phrase pair (?, ?). We will 
leave the derivation of ?(?,?) to Section 5.1.2, and 
will first describe how the gradient of 
sim?(?? , ??) w.r.t. ? is computed. 
5.1.1 Computing ?????(??, ??)/?? 
Without loss of generality, we use the following 
notations to describe a neural network: 
? ?? is the projection matrix for the l-th layer 
of the neural network; 
? ? is the input word vector of a phrase; 
? ?? is the sum vector of the l-th layer; and  
? ?? = ?(??) is the output vector of the l-th 
layer, where ? is an activation function; 
Thus, the CPTM defined by (2) and (3) can be rep-
resented as  
?1 = ?1
T?  
?1 = ?(?1)  
?2 = ?2
T?1  
?2 = ?(?2)  
sim?(?? , ??) = (??
2)
T
??
2  
The gradient of the matrix ?2 which projects the 
hidden vector to the output vector is computed as: 
?sim?(?? , ??)
??2
=
?(??
2)
T
??2
??
2 + (??
2)
T ???
2
??2
 
= ??
1 (??
2 ? ??(??
2))
T
+ ??
1 (??
2 ? ??(??
2))
T
 (9) 
where ? is the element-wise multiplication (Hada-
mard product). Applying the back propagation 
principle, the gradient of the projection matrix 
mapping the input vector to the hidden vector ?1 
is computed as 
?sim?(?? , ??)
??1
 
= ?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  
+?? (?2 (??
2 ? ??(??
2)) ? ??(??
1))
T
  (10) 
The derivation can be easily extended to a neural 
network with multiple hidden layers.  
5.1.2 Computing ?(?,?) 
To simplify the notation, we rewrite our loss func-
tion of (5) and (6) over one training sample as  
703
?(?) = ?xBleu(?) = ?
G(?)
Z(?)
 (11) 
where 
G(?) = ? sBleu(?, ??) exp(?
T?(??, ?, ?))?   
Z(?) = ? exp(?T?(??, ?, ?))?   
Combining (8) and (11), we have 
?(?,?) =
?xBleu(?)
?sim?(?? , ??)
 (12) 
=
1
Z(?)
(
?G(?)
?sim?(?? , ??)
?
?Z(?)
?sim?(?? , ??)
xBleu(?)) 
Because ? is only relevant to ??+1 which is de-
fined in (4), we have 
??T?(??, ?, ?)
?sim?(?? , ??)
= ??+1
???+1(??, ?, ?)
?sim?(?? , ??)
  
= ??+1?(?, ?; ?) (13) 
where ?(?, ?; ?)  is the number of times the 
phrase pair (?, ?)  occurs in ? . Combining (12) 
and (13), we end up with the following equation 
?(?,?)
= ? U(?,?)?(?|??)??+1?(?, ?; ?)
(?,?)????(??)
 
where  (14) 
U(?, ?) = sBleu(??, ?) ? xBleu(?).  
5.2 The Training Algorithm 
In our experiments we train the parameters of the 
CPTM, ?, using the L-BFGS optimizer described 
in Andrew and Gao (2007), together with the loss 
function described in (5). The gradient is com-
puted as described in Sections 5.1. Although SGD 
has been advocated for neural network training 
due to its simplicity and its robustness to local 
minima (Bengio 2009), we find that in our task 
that the L-BFGS minimizes the loss in a desirable 
fashion empirically when iterating over the com-
plete training data (batch mode). For example, the 
convergence of the algorithm was found to be 
smooth, despite the non-convexity in our loss. An-
other merit of batch training is that the gradient 
over all training data can be computed efficiently. 
As shown in Section 5.1, computing 
?sim?(x? , x?)/??  requires large-scale matrix 
multiplications, and is expensive for multi-layer 
neural networks. Eq. (7) suggests that 
?sim?(x? , x?)/??  and ?(?,?)  can be computed 
separately, thus making the computation cost of 
the former term only depends on the number of 
phrase pairs in the phrase table, but not the size of 
training data. Therefore, the training method de-
scribed here can be used on larger amounts of 
training data with little difficulty.  
As described in Section 4, we take three steps 
to learn the parameters for both the log-linear 
model of SMT and the CPTM. While steps 1 and 
3 can be easily parallelized on a computer cluster, 
the CPTM training is performed on a single ma-
chine. For example, given a phrase table contain-
ing 16M pairs and a 1M-sentence training set, it 
takes a couple of hours to generate the N-best lists 
on a cluster, and about 10 hours to train the CPTM 
on a Xeon E5-2670 2.60GHz machine.   
For a non-convex problem, model initialization 
is important. In our experiments we always initial-
ize ?1 using a bilingual topic model trained on 
parallel data (see detail in Section 6.2), and ?2 as 
an identity matrix. In principle, the loss function 
of (5) can be further regularized (e.g. by adding a 
term of ?2 norm) to deal with overfitting. How-
ever, we did not find clear empirical advantage 
over the simpler early stop approach in a pilot 
study, which is adopted in the experiments in this 
paper.   
6 Experiments 
This section evaluates the CPTM presented on 
two translation tasks using WMT data sets. We 
first describe the data sets and baseline setup. 
Then we present experiments where we compare 
different versions of the CPTM and previous 
models. 
6.1 Experimental Setup 
Baseline. We experiment with an in-house 
phrase-based system similar to Moses (Koehn et 
al. 2007), where the translation candidates are 
scored by a set of common features including 
maximum likelihood estimates of source given 
target phrase mappings ????(?|?) and vice versa 
????(?|?), as well as lexical weighting estimates 
???(?|?) and ???(?|?), word and phrase penal-
ties, a linear distortion feature, and a lexicalized 
reordering feature. The baseline includes a stand-
ard 5-gram modified Kneser-Ney language model 
trained on the target side of the parallel corpora 
described below. Log-linear weights are estimated 
with the MERT algorithm (Och 2003). 
704
Evaluation. We test our models on two different 
data sets. First, we train an English to French sys-
tem based on the data of WMT 2006 shared task 
(Koehn and Monz 2006). The parallel corpus in-
cludes 688K sentence pairs of parliamentary pro-
ceedings for training. The development set con-
tains 2000 sentences, and the test set contains 
other 2000 sentences, all from the official WMT 
2006 shared task. 
Second, we experiment with a French to Eng-
lish system developed using 2.1M sentence pairs 
of training data, which amounts to 102M words, 
from the WMT 2012 campaign. The majority of 
the training data set is parliamentary proceedings 
except for 5M words which are newswire. We use 
the 2009 newswire data set, comprising 2525 sen-
tences, as the development set. We evaluate on 
four newswire domain test sets from 2008, 2010 
and 2011 as well as the 2010 system combination 
test set, containing 2034 to 3003 sentences. 
In this study we perform a detailed empirical 
comparison using the WMT 2006 data set, and 
verify our best models and results using the larger 
WMT 2012 data set. 
The metric used for evaluation is case insensi-
tive BLEU score (Papineni et al 2002). We also 
perform a significance test using the Wilcoxon 
signed rank test. Differences are considered statis-
tically significant when the p-value is less than 
0.05. 
6.2 Results of the CPTM 
Table 1 shows the results measured in BLEU eval-
uated on the WMT 2006 data set, where Row 1 is 
the baseline system. Rows 2 to 4 are the systems 
enhanced by integrating different versions of the 
CPTM. Rows 5 to 7 present the results of previous 
models. Row 8 is our best system. Table 2 shows 
the main results on the WMT 2012 data set. 
CPTM is the model described in Sections 4. 
As illustrated in Figure 2, the number of the nodes 
in the input layer is the vocabulary size ?. Both 
the hidden layer and the output layer have 100 
nodes6. That is, ?1 is a ? ? 100 matrix and ?2 
a 100 ? 100  matrix. The result shows that 
CPTM leads to a substantial improvement over 
the baseline system with a statistically significant 
margin of 1.0 BLEU points as in Table 1.  
We have developed a set of variants of CPTM 
to investigate two design choices we made in de-
veloping the CPTM: (1) whether to use a linear 
                                                          
6 We can achieve slightly better results using more nodes in 
the hidden and output layers, say 500 nodes. But the model 
projection or a multi-layer nonlinear projection; 
and (2) whether to compute the phrase similarity 
using word-word similarities as suggested by e.g., 
the lexical weighting model (Koehn et al 2003). 
We compare these variants on the WMT 2006 
data set, as shown in Table 1. 
CPTML (Row 3 in Table 1) uses a linear neural 
network to project a word vector of a phrase ? to 
a feature vector ?: ? ? ?(?) = ?T?, where ? is 
a ? ? 100  projection matrix. The translation 
score of a source phrase f and a target phrase e is 
measured as the similarity of their feature vectors. 
We choose cosine similarity because it works bet-
ter than dot product for linear projection. 
CPTMW (Row 4 in Table 1) computes the phrase 
similarity using word-word similarity scores. This 
follows the common smoothing strategy of ad-
dressing the data sparseness problem in modeling 
phrase translations, such as the lexical weighting 
model (Koehn et al 2003) and the word factored 
n-gram translation model (Son et al 2012). Let ? 
denote a word, and ? and ? the source and target 
phrases, respectively. We define 
sim(?, ?) =
1
|?|
? sim?(?, ?) +???
1
|?|
? sim?(?, ?)???   
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
sim?(?, ?)
=
? sim(?,??) exp(?sim(?,??))????
? exp(?sim(?,??))????
 
 
training is too slow to perform a detailed study within a rea-
sonable time. Therefore, all the models reported in this paper 
use 100 nodes.   
# Systems WMT test2006 
1 Baseline 33.06 
2 CPTM 34.10? 
3 CPTML 33.60
?? 
4 CPTMW 33.25
? 
5 BLTMPR 33.15
? 
6 DPM 33.29? 
7 MRFP 33.91
? 
8 Comb (2 + 7) 34.39?? 
Table 1: BLEU results for the English to French 
task using translation models and systems built 
on the WMT 2006 data set. The superscripts ? 
and ? indicate statistically significant difference 
(p < 0.05) from Baseline and CPTM, respec-
tively. 
 
705
where sim?(?, ?)  (or sim?(?, ?) ) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function  
where ? is the tuned smoothing parameter.  
Similar to CPTM, CPTMW also uses a nonlin-
ear projection to map each word (not a phrase vec-
tor as in CPTM) to a feature vector. 
Two observations can be made by comparing 
CPTM in Row 2 to its variants in Table 1. First of 
all, it is more effective to model the phrase trans-
lation directly than decomposing it into word-
word translations in the CPTMs. Second, we see 
that the nonlinear projection is able to generate 
more effective features, leading to better results 
than the linear projection. 
We  also compare the best version of the CPTM 
i.e., CPTM, with three related models proposed 
previously. We start the discussion with the re-
sults on the WMT 2006 data set in Table 1. 
Rows 5 and 6 in Table 1 are two state-of-the-
art latent semantic models that are originally 
trained on clicked query-document pairs (i.e., 
clickthrough data extracted from search logs) for 
query-document matching (Gao et al 2011). To 
adopt these models for SMT, we view source-tar-
get sentence pairs as clicked query-document 
pairs, and trained both models using the same 
methods as in Gao et al (2011) on the parallel bi-
lingual training data described earlier. Specifi-
cally, BTLMPR is an extension to PLSA, and is 
the best performer among different versions of the 
Bi-Lingual Topic Model (BLTM) described in 
Gao et al (2011). BLTM with Posterior Regular-
ization (BLTMPR) is trained on parallel training 
data using the EM algorithm with a constraint en-
forcing a source sentence and its paralleled target 
sentence to not only share the same prior topic dis-
tribution, but to also have similar fractions of 
words assigned to each topic. We incorporated the 
model into the log-linear model for SMT (1) as 
                                                          
7 Gao and He (2013) reported results of MRF models with 
different feature sets. We picked the MRF using phrase fea-
tures only (MRFP) for comparison since we are mainly inter-
ested in phrase representation. 
follows. First of all, the topic distribution of a 
source sentence ?? , denoted by ?(?|??) , is in-
duced from the learned topic-word distributions 
using EM. Then, each translation candidate ? in 
the N-best list GEN(??) is scored as 
?(?|??) = ? ? ?(?|?)?(?|??)????    
?(??|?) can be similarly computed. Finally, the 
logarithms of the two probabilities are incorpo-
rated into the log-linear model of (1) as two addi-
tional features. DPM is the Discriminative Projec-
tion Model described in Gao et al (2011), which 
is an extension of LSA. DPM uses a matrix to pro-
ject a word vector of a sentence to a feature vector. 
The projection matrix is learned on parallel train-
ing data using the S2Net alorithm (Yih et al 
2011). DPM can be incorporated into the log-lin-
ear model for SMT (1) by introducing a new fea-
ture ??+1 for each phrase pair, which is defined 
as the cosine similarity of the phrases in the pro-
ject space.  
As we see from Table 1, both latent semantic 
models, although leading to some slight improve-
ment over Baseline, are much less effective than 
CPTM. 
Finally, we compare the CPTM with the Mar-
kov Random Field model using phrase features 
(MRFP in Tables 1 and 2), proposed by Gao and 
He (2013)7, on both the WMT 2006 and WMT 
2012 datasets. MRFp is a state-of-the-art large 
scale discriminative training model that uses the 
same expected BLEU training criterion, which 
has proven to give superior performance across a 
range of MT tasks recently (He and Deng 2012, 
Setiawan and Zhou 2013, Gao and He 2013).  
Unlike CPTM, MRFp is a linear model that 
simply treats each phrase pair as a single feature. 
Therefore, although both are trained using the 
# Systems dev news2011 news2010 news2008 newssyscomb2010 
1 Baseline 23.58 25.24 24.35 20.36 24.14 
2 MRFP 24.07
? 26.00? 24.90 20.84? 25.05? 
3 CPTM 24.12? 26.25? 25.05? 21.15?? 24.91? 
4 Comb (2 + 3) 24.46?? 26.56?? 25.52?? 21.64?? 25.22? 
Table 2:   BLEU results for the French to English task using translation models and systems built on 
the WMT 2012 data set. The superscripts ? and ? indicate statistically significant difference (p < 
0.05) from Baseline and MRFp, respectively. 
 
 
706
same expected BLEU based objective function, 
CPTM and MRFp model the translation relation-
ship between two phrases from different angles. 
MRFp estimates one translation score for each 
phrase pair explicitly without parameter sharing, 
while in CPTM, all phrases share the same neural 
network that projects raw phrases to the continu-
ous space, providing a more smoothed estimation 
of the translation score for each phrase pair.  
The results in Tables 1 and 2 show that CPTM 
outperforms MRFP on most of the test sets across 
the two WMT data sets, but the difference be-
tween them is often not significant. Our interpre-
tation is that although CPTM provides a better 
smoothed estimation for low-frequent phrase 
pairs, which otherwise suffer the data sparsity is-
sue, MRFp provides a more precise estimation for 
those high-frequent phrase pairs. That is, CPTM 
and MRFp capture complementary information 
for translation. We thus combine CPTM and 
MRFP (Comb in Tables 1 and 2) by incorporating 
two features, each for one model, into the log-lin-
ear model of SMT (1). We observe that for both 
translation tasks, accuracy improves by up to 0.8 
BLEU over MRFP alone (e.g., on the news2008 
test set in Table 2). The results confirm that 
CPTM captures complementary translation infor-
mation to MRFp. Overall, we improve accuracy 
by up to 1.3 BLEU over the baseline on both 
WMT data sets. 
7 Conclusions 
The work presented in this paper makes two major 
contributions. First, we develop a novel phrase 
translation model for SMT, where joint represen-
tations are exploited of a phrase in the source lan-
guage and of its translation in the target language, 
and where the translation score of the pair of 
source-target phrases are represented as the dis-
tance between their feature vectors in a low-di-
mensional, continuous space. The space is derived 
from the representations generated using a multi-
layer neural network. Second, we present a new 
learning method to train the weights in the multi-
layer neural network for the end-to-end BLEU 
metric directly. The training method is based on 
L-BFGS. We describe in detail how the gradient 
in closed form, as required for efficient optimiza-
tion, is derived. The objective function, which 
takes the form of the expected BLEU computed 
from N-best lists, is very different from the usual 
objective functions used in most existing architec-
tures of neural networks, e.g., cross entropy (Hin-
ton et al 2012) or mean square error (Deng et al 
2012). We hence have provided details in the der-
ivation of the gradient, which can serve as an ex-
ample to guide the derivation of neural network 
learning with other non-standard objective func-
tions in the future. 
Our evaluation on two WMT data sets show 
that incorporating the continuous-space phrase 
translation model into the log-linear framework 
significantly improves the accuracy of a state-of-
the-art phrase-based SMT system, leading to a 
gain up to 1.3 BLEU. Careful implementation of 
the L-BFGS optimization based on the BLEU-
centric objective function, together with the asso-
ciated closed-form gradient, is a key to the suc-
cess.  
A natural extension of this work is to expand 
the model and learning algorithm from shallow to 
deep neural networks. The deep models are ex-
pected to produce more powerful and flexible se-
mantic representations (e.g., Tur et al, 2012), and 
thus greater performance gain than what is pre-
sented in this paper. 
8 Acknowledgements 
We thank Michael Auli for providing a dataset 
and for helpful discussions. We also thank the four 
anonymous reviewers for their comments.  
References 
Andrew, G. and Gao, J. 2007. Scalable training 
of L1-regularized log-linear models. In 
ICML.  
Auli, M., Galley, M., Quirk, C. and Zweig, G. 
2013 Joint language and translation modeling 
with recurrent neural networks. In EMNLP. 
Bengio, Y. 2009. Learning deep architectures for 
AI. Fundamental Trends Machine Learning, 
vol. 2, no. 1, pp. 1?127. 
Bengio, Y., Duharme, R., Vincent, P., and Janvin, 
C. 2003. A neural probabilistic language 
model. JMLR, 3:1137-1155. 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. 
Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3: 993-1022. 
Collobert, R., Weston, J., Bottou, L., Karlen, M., 
Kavukcuoglu, K., and Kuksa, P. 2011. Natural 
language processing (almost) from scratch. 
Journal of Machine Learning Research, vol. 
12. 
707
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T., and Harshman, R. 1990. Index-
ing by latent semantic analysis. Journal of the 
American Society for Information Science, 
41(6): 391-407 
DeNero, J., Gillick, D., Zhang, J., and Klein, D. 
2006. Why generative phrase models underper-
form surface heuristics. In Workshop on Statis-
tical Machine Translation, pp. 31-38. 
Deng, L., Yu, D., and Platt, J. 2012. Scalable 
stacking and learning for building deep archi-
tectures. In ICASSP. 
Diamantaras, K. I., and Kung, S. Y. 1996. Princi-
ple Component Neural Networks: Theory and 
Applications. Wiley-Interscience. 
Dumais S., Letsche T., Littman M. and Landauer 
T. 1997. Automatic cross-language retrieval us-
ing latent semantic indexing. In AAAI-97 
Spring Symposium Series: Cross-Language 
Text and Speech Retrieval. 
Ganchev, K., Graca, J., Gillenwater, J., and 
Taskar, B. 2010. Posterior regularization for 
structured latent variable models. Journal of 
Machine Learning Research, 11 (2010): 2001-
2049. 
Gao, J., and He, X. 2013. Training MRF-based 
translation models using gradient ascent. In 
NAACL-HLT, pp. 450-459. 
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web 
search. In SIGIR, pp. 675-684.  
He, X., and Deng, L. 2012. Maximum expected 
bleu training of phrase and lexicon translation 
models. In ACL, pp. 292-301. 
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering Binary Codes for Documents by Learn-
ing Deep Generative Models. Topics in Cogni-
tive Science, pp. 1-18. 
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, 
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012. 
Deep neural networks for acoustic modeling in 
speech recognition. IEEE Signal Processing 
Magazine, vol. 29, no. 6, pp. 82-97. 
Hofmann, T. 1999. Probabilistic latent semantic 
indexing. In SIGIR, pp. 50-57. 
Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. 
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM.  
Kalchbrenner, N. and Blunsom, P. 2013. Recur-
rent continuous translation models. In EMNLP. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, 
C., Federico, M., Bertoldi, N., Cowan, B., Shen, 
W., Moran, C., Zens, R., Dyer, C., Bojar, O., 
Constantin, A., and Herbst, E. 2007. Moses: 
open source toolkit for statistical machine trans-
lation. In ACL 2007, demonstration session. 
Koehn, P. and Monz, C. 2006. Manual and auto-
matic evaluation of machine translation be-
tween European languages. In Workshop on 
Statistical Machine Translation, pp. 102-121. 
Koehn, P., Och, F., and Marcu, D. 2003. Statisti-
cal phrase-based translation. In HLT-NAACL, 
pp. 127-133. 
Lambert, P. and Banchs, R. E. 2005. Data inferred 
multi-word expressions for statistical machine 
translation. In MT Summit X, Phuket, Thailand. 
Li, P., Liu, Y., and Sun, M. 2013. Recursive auto-
encoders for ITG-based translation. In EMNLP. 
Liang,P., Bouchard-Cote,A., Klein, D. and 
Taskar, B. 2006. An end-to-end discriminative 
approach to machine translation. In COLING-
ACL. 
Marcu, D., and Wong, W. 2002. A phrase-based, 
joint probability model for statistical machine 
translation. In EMNLP. 
Mikolov, T., Karafiat, M., Burget, L., Cernocky, 
J., and Khudanpur, S. 2010. Recurrent neural 
network based language model. In INTER-
SPEECH, pp. 1045-1048. 
Mikolov, T., Kombrink, S., Burget, L., Cernocky, 
J., and Khudanpur, S. 2011. Extensions of re-
current neural network language model. In 
ICASSP, pp. 5528-5531. 
Mikolov, T. 2012. Statistical Language Model 
based on Neural Networks. Ph.D. thesis, Brno 
University of Technology. 
Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. 
Exploiting similarities among languages for 
machine translation. CoRR. 2013; 
abs/1309.4148. 
Mikolov, T., Yih, W. and Zweig, G. 2013b. Lin-
guistic Regularities in Continuous Space Word 
Representations. In NAACL-HLT. 
Mimno, D., Wallach, H., Naradowsky, J., Smith, 
D. and McCallum, A. 2009. Polylingual topic 
models. In EMNLP. 
708
Niehues J., Herrmann, T., Vogel, S., and Waibel, 
A. 2011. Wider context by using bilingual lan-
guage models in machine translation. 
Och, F. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL, pp. 160-
167.  
Och, F., and Ney, H. 2004. The alignment tem-
plate approach to statistical machine translation. 
Computational Linguistics, 29(1): 19-51. 
Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 
2002. BLEU: a method for automatic evaluation 
of machine translation. In ACL. 
Platt, J., Toutanova, K., and Yih, W. 2010. 
Translingual Document Representations from 
Discriminative Projections. In EMNLP. 
Rosti, A-V., Hang, B., Matsoukas, S., and 
Schwartz, R. S. 2011. Expected BLEU training 
for graphs: bbn system description for WMT 
system combination task. In Workshop on Sta-
tistical Machine Translation. 
Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. 
A. R. 2007. Smooth bilingual n-gram transla-
tion. In EMNLP-CoNLL, pp. 430-438. 
Schwenk, H. 2012. Continuous space translation 
models for phrase-based statistical machine 
translation. In COLING. 
Schwenk, H., Rousseau, A., and Mohammed A. 
2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine 
translation. In NAACL-HLT Workshop on the 
future of language modeling for HLT, pp. 11-
19. 
Setiawan, H. and Zhou, B., 2013. Discriminative 
training of 150 million translation parameters 
and its application to pruning. In NAACL. 
Socher, R., Huval, B., Manning, C., Ng, A., 2012. 
Semantic Compositionality through Recursive 
Matrix-Vector Spaces. In EMNLP. 
Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 
2011. Parsing natural scenes and natural lan-
guage with recursive neural networks. In ICML. 
Son, L. H., Allauzen, A., and Yvon, F. 2012. Con-
tinuous space translation models with neural 
networks. In NAACL-HLT, pp. 29-48. 
Sundermeyer, M., Oparin, I., Gauvain, J-L. 
Freiberg, B., Schluter, R. and Ney, H. 2013. 
Comparison of feed forward and recurrent neu-
ral network language models. In ICASSP, pp. 
8430?8434. 
Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 
2012. Towards deeper understanding: deep con-
vex networks for semantic utterance classifica-
tion. In ICASSP. 
Vinokourov,A., Shawe-Taylor,J. and Cristia-
nini,N. 2002. Inferring a semantic representa-
tion of text via cross-language correlation anal-
ysis. In NIPS. 
Weston, J., Bengio, S., and Usunier, N. 2011. 
Large scale image annotation: learning to rank 
with joint word-image embeddings. In IJCAI. 
Wuebker, J., Mauser, A., and Ney, H. 2010. Train-
ing phrase translation models with leaving-one-
out. In ACL, pp. 475-484. 
Yih, W., Toutanova, K., Platt, J., and Meek, C. 
2011. Learning discriminative projections for 
text similarity measures. In CoNLL. 
Zhang, Y., Deng, L., He, X., and Acero, A. 2011. 
A novel decision function and the associated de-
cision-feedback learning for speech translation. 
In ICASSP. 
Zhila, A., Yih, W., Meek, C., Zweig, G. and 
Mikolov, T. 2013. Combining heterogeneous 
models for measuring relational similarity. In 
NAACL-HLT. 
Zou, W. Y., Socher, R., Cer, D., and Manning, C. 
D. 2013. Bilingual word embeddings for 
phrase-based machine translation. In EMNLP. 
709
