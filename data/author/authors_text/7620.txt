A Dynamic Language Model 
Based on Individual Word Domains 
E. I. Sicilia-Garcia, Ji Ming, F. J. Smith 
School Computer Science 
Queen's University of Belfast 
Belfast BT7 INN, Northern Ireland 
e.sicilia@qub.ac.uk 
Abstract 
We present a new statistical language 
model based on a Colnbination of 
individual word language models. Each 
word model is built from an individual 
corpus which is formed by extracting 
those subsets of the entire training corpus 
which contain that significant word. We 
also present a novel way of combining 
language models called the "union 
model", based on a logical union of 
intersections, and use this to combine the 
language models obtained for the 
significant words from a cache. The 
initial results with the new model provide 
a 20% reduction in language model 
perplexity over the standard 3-gram 
approach. 
Introduction 
Statistical language models are based on 
information obtained fiom the analysis of large 
samples of a target language. Such models 
estimate the conditional probability of a word 
given a sequence of preceding words. The 
conditional probability can be further used to 
determine the likelihood of a sentence through 
lhe product of the individual word probabilities. 
A popular type of statistical language model is 
lhe dynamic language model, which dynamically 
modifies conditional probabilities depending on 
the recent word history. For example the cached- 
based natural anguage models (Kuhn R. & De 
Mori R., 1990) incorporates a cache component 
into the model, which estimates the probability 
of a word depending upon its recent usage. 
Trigger based models go a step further by 
triggering associated words to each content word 
in a cache giving each associated word a higher 
probability (Lau et al, 1993). 
Our statistical language model, based 
upon individual word domains, extends these 
ideas by creating a new language model for each 
significant word in the cache. A significant word 
is hard to define; it is any word that significantly 
contributes to the content of the text. We define 
it as any word which is not a stop word, i. e. 
articles, prepositions and some of the most 
fiequcntly used words in the language such as 
"will", "now", "very", etc. Our model combines 
individual word language models with a standard 
global n-gram language model. 
A training corpus for each significant 
word is formed from the amalgamation of the 
text fiagments taken fiom the global training 
corpus in which that word appears. As such these 
corpora are smaller and closely constrained; 
hence the individual anguage models are more 
precise than the global language model and 
thereby should offer performance gains. One 
aspect of the performance of this joint model is 
how the global language model is to be 
combined with the individual word language 
models. This is explored later. 
This paper is organised as follows. 
Section 1 explains the basis for this model. The 
mathematical background and how the models 
are combined are explained in section 2. In the 
third section, a novel method of combining the 
word models, the probabilistic-union model is 
explained. Finally, results and conclusion are 
drawn. 
789 
1 Dynamic Language Model based on 
Word Language Models 
Our dynamic language model builds a 
language model for each individual word. In 
order to do this we need to select which words 
are to be classified as significant and furthermore 
create a language model for them. We excluded 
all the stop words ('is', 'to', 'all', 'some') due to 
their high frequency within tile text and their 
limited contribution to the thematic of the text. A 
list of stop words was obtained by merging 
together the lists used by various www search 
engines, for example Altavista. 
Secondly we need to create a dictionary 
that contains the frequency of each word ill the 
corpus. This is needed because we want to 
exclude those non-stop words which appear too 
often in the training corpus, for example words 
like 'dollars', 'point', etc. A hash file is 
constructed to store large amounts of information 
so that it can be retrieved quickly. 
The next step is to create tile global 
language model by obtaining the text phrases and 
their probabilities. Frequencies of words and 
phrases are derived fiom a large text corpus and 
the conditional probability of a word given a 
sequence of preceding words is estimated. These 
conditional probabilities are combined to 
produce an overall language model probability 
for any given word sequence. The probability of 
a sequence of words is: 
P(w 1 "" w,, ) = P(w\[' ) = 
- -P (w l )X  P(w2 I w l )x""  P(w" \[ w~'-l) = (1) 
i=I 
where w\[' ={wl,w2,w3,...,w,, } is a sentence or 
sequence of words. The individual conditional 
probabilities are approximated by the maximum 
likelihoods: 
PML(W~ Iw l -b  --  f req(w\ [ )  _ f , ' eq(w,  . . .  w~_,wp 
freq(wl q) freq(wl "" ~h-l) (2) 
where freq(X)is the frequency of the 
phrase X in the text. 
In equation (2), there are often unknown 
sequences of words i.e. phrases which are not in 
the dictionary. The maximum likelihood 
probability is then zero. In order to improve this 
prediction of all unseen event, and hence the 
language model, a number of techniques have 
been explored, for example, the Good-Turing 
estimate (Good I. J., 1953), the backing-off 
method (Katz S. M., 1987), deleted interpolation 
(Jelinek F. and Mercer R. L., 1984) or the 
weighted average n-gram model (O'Boyle P., 
Owens M. and Smith F. J., 1994). We use the 
weighted average n-gram technique (WA), which 
combines n-grain ~ phrase distributions of several 
orders using a series of weighting fnnctions. The 
WA n-gram model has been shown to exhibit 
similar predictive powers to other n-gram 
techniques whilst enjoying several benefits. 
Firstly an algorithm for a WA model is relatively 
straightforward to implement ill computer 
software, secondly it is a variable n-gram model 
with the length depending on the context and 
finally it facilitates easy model extension 2. The 
weighted average probability of a woM given the 
preceding words is 
,,tj,,,, (w) + ~ & l,,,,.(,, I ,',,,, ,...,,',.,) 
&,,, (,,, I ,,,, ..-w,,, ) - '-' (3) 
i 0 
where the weighted funct ions  are: 
,,t o = Ln( N),  (4) 
-- 2 
N is tile number of tokens ill tile corpus and 
freq(wm+l_i...w,,~) is the frequency of tile 
senteuce Win+l_  i " ' "  W m in the text. 
The maximum likelihood probability of a word 
is: 
J A n-gram model contains the conditional probability 
of a word dependant on the previous i1 words. (Jclinek 
F., Mercer R.L. and Bahl L. R., 1983) 
2 Tile "ease of extension" applies to the fact that 
additional training data can be incorporated into an 
existing WA model without the need to re-estimate 
smoothing parameters. 
790 
P,,,,.(,,,)--??q (')  (5) 
N 
J?eq(w) is the frequency of the word w in the 
text. This language model (defined by equation 
(3) and (5)) is what we term a standard n-gram 
language model or global language model. 
Finally the last step is the creation of a 
hmguage model for each significant word, which 
is formed in the same manner as the global 
language model. The word language-training 
corpus to be used is tlle amalgamation of the text 
fiagments taken from the global training corpus 
in which the significant word appears. A number 
of choices can be made as to how the word- 
training corpus for each significant word can be 
selected. We initially construct what we termed 
the "paragraph context model", entailing that the 
global training corpus is scanned for a particular 
word and each time the word is found the 
paragraph containing that word is extracted. The 
paragraphs of text extracted for a particular word 
are joined together to form an individual word- 
training corpus, from which an individual word 
language model is built. Alternative methods 
include storing only the sentences where the 
word appears or extracting a piece of the text M- 
words before and M + words after the search 
word .  
Additionally some restrictions on the 
number of words were imposed. This was done 
due to the high frequency of certain words. Such 
words were omitted since the additional 
information that they provide is minimal 
(conversely language models for "rare" words 
are desirable as they provide significant 
additional iuformation to that contained within 
the global language model). Once individual 
language models have been formed for each 
significaut word (trained using the standard n- 
grain approach as used for the global lnodel), 
the.m remains the problem of how the individual 
word language models will be combined together 
with the global language model. 
2 Combining the Models 
We need to combine the probabilities 
obtained from each word language model and 
fiom the global language model, in order to 
obtain a conditional probability for a word given 
a sequence of words. The first model to be tested 
is an arithmetic combination of the global 
hmguage model and the word language models. 
All the word hmguage models and the global 
language model are weighted equally. We 
believe that words, which appear far away in the 
previous word history, do not have as nmch 
importance as the ones closest to the word. 
Therefore we need to lnake a restriction in the 
number of language models. First, the 
conditional probabilities obtained from the word 
hmguage models and the global language 
model can be combined in a linear 
interpolated model as follows: 
m 
P(w Iw") = ;co e?,,o,.., (w Iw;') + Z & v, (w I w',' ) 
i-~ (6) 
I l l  
where 2 c + .y_, 2 i = 1 (7) 
i I 
and l ' (wl , , i ' ) is  the conditional probability in 
the word language model for the significant word 
w i, 2 iare the correspondent weights and m is 
the maxinmm number of word models that we 
are including. 
If the same weight is given to all the 
word language models but not to the global 
language model and if a restriction on tim 
lmmber of word language models to be included 
is enforced, the weighted model is defined as: 
and ~ is a parameter which is chosen to optimise 
the model. 
Furthermore, a method was used based 
on all exponential decay of the word model 
probabilities with distance. This stands to reason, 
as a word appearing several words previously 
will generally be less relevant han more recent 
791 
words. Given a sequence of words, for example, 
"We had happy times in America..." 
We ltad Happy Times In America 
5 4 3 2 1 
where 5, 4, 3, 2, 1 represent the distance of the 
word from the word America, Happy and Times 
are significant words for which we have an 
individual word language models. The 
exponential decay model for the word w, where 
in this case w represents the significant word 
America, is as follows: 
/ . . . . . .  / I {;iot,,,t( w\[ w I ) + P.,.,py (w I wl )' exp(-3/d) 
P(wlw, )=\[  + 1,,,,,,,,(~, \]w, ).exp(21d) ) (9) 
l + exp(-3/d) + exp(-2/d) 
where Patot,,t(wl (') is the conditional 
probability of the word w following a phrase 
wl "" w,, in the global language model. 
Pmppy(Wl w~') is the conditional probability of the 
word w following a phrase w 1. . .%word 
language model for the significant word Happy. 
The same defnition applies for the word model 
Times. d is the exponential decay distance with 
d=5, 10, 15,etc. The decaying factor exp(-I/d) 
introduces a cut off: 
if l>d ~ exp(-l/d)=O 
where l is the word modelto word distance 
d is the decay distance 
Presently the combination methods 
outlined above have been experimentally 
explored. However, they offer a reasonably 
simplistic means of combining the individual and 
global language models. More sophisticated 
models are likely to offer improved performance 
gains. 
3 The Probabilistic-Union Model 
The next method is the Probabilistic- 
Union model. This model is based on the logical 
concept of a disjunction of conjunction which is 
implemented as a sum of products. The union 
model has been previously applied in problems 
of noisy speech recognition, (Ming J. et al, 
1999). Noisy conditions during speech 
recognition can have a serious effect on the 
likelihood of some features which are normally 
combined using the geometric mean. This noise 
has a zeroing effect upon the overall likelihood 
produced for that particular speech frame. The 
use of the probabilistic-union reduces the overall 
effect that each feature has in the colnbination, 
therefore loosening any zeroing effect. 
For the word language model, some of the 
conditional probabilities are zero or very small 
due to the small size of some of the word model 
corpora. For these word models, many of the 
words in the global training corpus are not in the 
word-model training-corpus dictionary. And so, 
the conditional probability will be in many cases 
zero or near zero reducing the overall 
probability. As in noisy speech @cognition we 
wish to reduce the effect of this zeroing in the 
combined model. The probabilistic-union model 
is one of the possible solutions for the zeroing 
problem when combining language models. 
The union model is best illustrated with 
an example when the number of word models to 
be included is m=4 and if they are assumed to be 
independent probabilities. 
(') , w,(e, ( lo)  &,,i,,,, (" ) = /2" 8 
-%9;P= . @v, p2 @ Pu,,io,, (w)  P3 P4 "" )  (1 1) 
/} (3 )  z . u,,io,,tw)-- W3(I:~P2 @ P,P~ ? PiP4 @'") (12) 
v(4) , . 't'4(P, ? ? ) u,,io,, tW) = \]}2 P'~ @ P4 (13) 
where P'~,io,,(w) =P~U,,io,, (wlw;') is the nnion 
model of order k. P/ = P,.(w\[ w(') is the 
conditional probability for the significant word 
w i and ~ is a normalizing constant. The 
symbol '?' is a probabilistic sum, i.e. its 
equivalent for 1 and 2 is: 
8,,,,d2 =8 ?",2 --e, +/'2 -8P2 (14) 
Tile combination of the global language 
model with the probabilistic-union model is 
792 
defined as follows: 
p(w\[ w~') = ~J~,o~,,,~(wl ,,\[')+(l-a)/}j,,,,,,(w \[ i') (15) 
Results 
To evaluate the behaviour of one language model 
with respect to others we use perplexity. It 
measures the average branching factor (per 
word) of the sequence at every new word, with 
respect to some source model. The lower the 
branching l'actor, the lower the model errors rate. 
Therefore, the lower the branching (l~erplexity) 
the better the model. Let w i be a word in the 
language model and w\[" = {wl, w 2, w3,..-, w,,, } 
a sentence or sequence of words. The perplexity 
of this sequence of words is: 
Peq) lex i ty (  w 1 w2 ...  w,, ) = PP(w~'  ) = 
t 1 " ._ \] 
= Z ('", I ,,'i' )) 
J I t  i=1 
(J6) 
The Wall Street Journal (version 
WSJ03) contains about 38 million words, and a 
dictionary of approxilnately 65,000 words. We 
select one quarter of the articles in the global 
training corpus as our training corpus (since the 
global training corpus is large and the 
normalisation process takes time). To test the 
new language model we use a subset of the test 
file given by WSJ0, selected at random. The 
training corpus that we are using contains 
172,796 paragraphs, 376,589 sentences, 
9526,187 tokens. The test file contains 150 
paragraphs, 486 sentences, 8824 tokens and 1908 
words types. Although the size of this test file is 
small, limher experilnents with bigger training 
corpora and test files are planned. 
Although in our first experiments we use 
5--grams in the calculation of the word models, 
the size of the n-gram has been reduced to 3- 
grains because the process of norlnalisation is 
slow in these experiments. 
The model based on a simple weighted 
colnbination offers ilnproved results, up to 10% 
when o~=0.6 in Eq. (8) and a combination of a 
maxilnuln of 10 word lnodels. Better esults were 
found when the word models were weighted 
depending on their distance from the current 
word, that is, for the exponential decay model in 
Eq. (9) where d=7 and the number of word 
models is selected by the exponential cut off 
(Table 1 ). For this model ilnprovelnents of over 
17% have been found. 
F ~  Decay d 
 .,11 5 I 6 I 7 I 
4d 15.53% 16.31% 16.46% 16.44% 
5d 15.90% 16.42% 16.52% 16.43% 
6d 15.92% 16.45% 16.53% 16.41% 
7d 16.02% 16,4~% 16.51% 16.40% 
8d 16.1)2% 16.46% 16.51% 16.39% 
9(1 15.97% 16.45% 16.51% 16.39% 
Table 1. Improvement in perplexity for the 
exponetial decay models with respect o the Global 
Language Model over the basic 3-gram model. 
For tile probabilistic-union model, we 
have as many nlodels as numbers of word 
language nlodels. For example, if we wish to 
include m=4 word language Jnodels, tile four 
union models are those with orders I to 4 
(equation (13) to (15)). The results for the 
probabilistic union model when the number of 
words models is m=5 and m=6 are shown in the 
tables below. 
Union Model Order 
5 I 4 I 3 I 2 I 1 
0.3 13% 15% -2% -15% -25% 
0.4 13% 18% 6% -3% -10% 
0.5 12% 19% I 1% 4% -I% 
0.6 12% 19% 13% 9% 5% 
0.7 11% 18% 14% 11% 8% 
0.8 9% 15% 13% 11% 9% 
0.9 6% 10% 9% 8% 8% 
3 CSR-I(WSJ0) Sennheiser, published by LDC , 
ISBN: 1-58563-007-I 
793 
t t U, fiox%odel Order t t 
0.3 13% 15% -2% -13% -22% -30% 
0.4 13% 18% 6% -2% -8% -13% 
0.5 13% 20% 11% 5% 1% -3% 
0.6 12% 20% 14% 9% 6% 3% 
0.7 11% 18% 14% 11% 9% 7% 
0.8 9% 16% 13% I I% 10% 9% 
0.9 6% 11% 10% 9% 8% 7% 
Table 2. Improvement in perplexity of the 
Probabilistie-Union Model with respect to the 
Global Language Model over the basic 3-gram 
model. 
The best result obtained so far, is an 
improvement of 20% when a maximum ot' 6 word 
models and the order is 5, i.e. sums of the 
products of pairs (Table 2).The value of alpha is 
0.6. 
Conclusion 
In this paper we have introduced the 
concept of individual word language models to 
improve language model performance. 
Individual word language models permit an 
accurate capture of the domains in which 
significant words occur and hence improve the 
language model performance. We also describe a 
new method of combining models called the 
probabilistic union model, which has yet to be 
fully explored but the first results show good 
performance. Even though the results are 
preliminary, they indicate that individual word 
models combined with the union model offer a 
promising means of reducing the perplexity. 
Weighted Eq. (8) 10% 
Exponential Decay Eq. (9) 17% 
Union Model 5 words 19% 
Union Model 6 words 20% 
Union Model 7 words 19% 
Table 3. hnprovement in perplexity for different 
combinations of word models. 
Acknowledgements  
Our thanks go to Dr. Phil Halma for his 
collaboration i  this research. 
References 
Good I. J. (1953) "The Population Frequencies ot' 
Species and the Estimation of Population 
Parameters". Biometrika, Vol. 40, pp.237-254. 
Jelinek F., Mercer R. L. and Bahl L. R. (1983) "A 
Maximum Likelihood Approach to Continuous 
Speech Recognition". IEEE Transactions on 
Pattern Analysis and Machine Intelligence. Vol. 5, 
pp. 179-190. 
Jelinek F. and Mercer R. L. (1984) "Interpolated 
estimation of Markov Source Parameters from 
Sparse Data". Pattern Recognition in Practice. 
Gelsema E., Kanal L. eds. Amsterdam: Norlh- 
Holland Publishing Co. 
Katz S. M. (1987) "Estimation of Probabilities from 
Sparse Data for the Language Model Component of 
a Speech Recogniser". IEEE Transactions On 
Acoustic Speech and Signal Processing. Vol. 35(3), 
pp. 400-401. 
Kuhn R. and De Mori R. (1990) "A Cache-Based 
Natural Language Model for Speech Recognition". 
IEEE Transactions on Pattern Analysis and 
Machine Intelligence. Vol. 12 (6), pp. 570-583. 
Lau R., Rosenfeld R., Roukos S. (1993). "Trigger- 
based Language models: A Maximum entropy 
approach". IEEE ICASSP 93 Vo12, pp 45-48, 
Minneapolis, MN, U.S.A., April. 
Ming J., Stewart D., Hanna P. and Smith F. J. (1999) 
"A probabilistic Union Model Jbr Partial and 
temporal Corruption ql~ Speech ''. Automatic Speech 
Recognition and Understanding Workshop. 
Keystone, Colorado, U. S. A., December. 
O'Boyle P., Owens M. and Smith F. J. (1994) 
"Average n-gram Model of Natural Language". 
Computer Speech and Language. Vol. 8 pp 337- 
349. 
794 
Extension of Zipf?s Law to Words and Phrases 
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, F. J. Smith 
School Computer Science 
Queen?s University of Belfast 
Belfast BT7 1NN, Northern Ireland 
q.le@qub.ac.uk 
 
Abstract 
Zipf?s law states that the frequency of 
word tokens in a large corpus of natural 
language is inversely proportional to the 
rank. The law is investigated for two 
languages English and Mandarin and for n-
gram word phrases as well as for single 
words. The law for single words is shown 
to be valid only for high frequency words. 
However, when single word and n-gram 
phrases are combined together in one list 
and put in order of frequency the combined 
list follows Zipf?s law accurately for all 
words and phrases, down to the lowest 
frequencies in both languages. The Zipf 
curves for the two languages are then 
almost identical.  
 
1. Introduction 
 
The law discovered empirically by Zipf 
(1949) for word tokens in a corpus states that if 
f is the frequency of a word in the corpus and r 
is the rank, then: 
r
kf =  (1) 
where k is a constant for the corpus. When 
log(f) is drawn against log(r) in a graph (which 
is often called a Zipf curve), a straight line is 
obtained with a slope of ?1. An example with a 
small corpus of 250,000 tokens is given in 
Figure 1. Zipf?s discovery was followed by a 
large body of literature reviewed in a series of 
papers edited by Guiter and Arapov (1982). It 
continues to stimulate interest today 
(Samuelson, 1996; Montermurro, 2002; Ferrer 
and Sol?, 2002) and, for example, it has been 
applied to citations Silagadze (1997) and to 
DNA sequences (Yonezawa & Motohasi, 1999; 
Li, 2001). 
 
1
10
100
1000
10000
100000
1 10 100 1000 10000 100000
log rank
 Figure 1 Zipf curve for the unigrams extracted from a 
250,000 word tokens corpus 
 
Zipf discovered the law by analysing 
manually the frequencies of words in the novel 
?Ulysses? by James Joyce. It contains a 
vocabulary of 29,899 different word types 
associated with 260,430 word tokens. 
Following its discovery in 1949, 
several experiments aided by the appearance of 
the computer in the 1960?s, confirmed that the 
law was correct for the small corpora which 
could be processed at that time. The slope of 
the curve was found to vary slightly from ?1 
for some corpora; also the frequencies for the 
highest ranked words sometimes deviated 
slightly from the straight line, which suggested 
several modifications of the law, and in 
particular one due to Mandelbrot (1953): 
?? )( += r
kf  (2) 
where ? and ? are constants for the corpus 
being analysed. However, generally the 
constants ? and ? were found to be only small 
statistical deviations from the original law by 
Zipf (exceptions are legal texts which have 
smaller ? values (?0.9) showing that lawyers 
use more words than other people!)(Smith & 
Devine, 1985). 
A number of theoretical developments 
of Zipf?s law had been derived in the 50?s and 
60?s and have been reviewed by Fedorowicz 
(1982), notably those due to Mandelbrot (1954, 
1957) and Booth (1967). A well-known 
derivation, due to Simon (1955), is based on 
empirically derived distribution functions. 
However, Simon?s derivation was controversial 
and a correspondence in the scientific press 
developed between Mandelbrot and Simon on 
the validity of this derivation (1959-1961); the 
dispute was not resolved by the time Zipf 
curves for larger corpora were beginning to be 
computed. 
The processing of larger corpora with 1 
million words or more was facilitated by the 
development of PC?s in the 1980?s. When Zipf 
curves for these corpora were drawn they were 
found to drop below the Zipf straight line with 
slope of ?1 at the bottom of the curve, starting 
for rank greater than about 5000. This is 
illustrated in Figure 2 which shows the Zipf 
curve for the Brown corpus of 1 million words 
of American English (Francis & Kucera, 1964). 
  
1
10
100
1000
10000
100000
1 10 100 1000 10000 100000
log rank
 Figure 2 Zipf curve for the unigrams extracted from 
the 1 million words of the Brown corpus 
 
This appeared to confirm the opinion of 
the opponents of Simon?s derivation: the law 
clearly did not hold for r>5000; so it appeared 
that the derivation must be invalid. 
 
2. Zipf Curves for Large Corpora 
 
This paper is principally concerned 
with exploring the above invalidity of Zipf?s 
law for large corpora in two languages, English 
and Mandarin. We begin with English. 
 
English corpora 
 
The English corpora used in our 
experiments are taken from the Wall Street 
journal (Paul & Baker, 1992) for 1987, 1988, 
1989, with sizes approximately 19 million, 16 
million and 6 million tokens respectively. The 
Zipf curves for the 3 corpora are shown in 
Figure 3.  
1
10
100
1000
10000
100000
1000000
1 10 100 1000 10000 100000 1000000
log rank
WSJ87 1-gram
WSJ88 1-gram
WSJ89 1-gram
 Figure 3 Zipf curves for the unigrams extracted from 
the 3 training corpora of the WSJ 
The curves are parallel, showing 
similar structures and all 3 deviating from 
Zipf?s law for larger r. Their separation is due 
to their different sizes. 
Language is not made of individual 
words but also consists of phrases of 2, 3 and 
more words, usually called n-grams for n=2, 3, 
etc. For each value of n between 2 and 5, we 
computed the frequencies of all n-gram in each 
corpus and put them in rank order as we had 
done for the words. This enabled us to draw the 
Zipf curves for 2-grams to 5-grams which are 
shown along with the single word curves in 
Figures 4, 5 and 6 for the three corpora. These 
curves are similar to the first Zipf curves drawn 
for n-grams by Smith and Devine (1985); but 
these earlier curves were for a much smaller 
corpus. 
110
100
1000
10000
100000
1000000
log rank
1-gram
2-gram
3-gram
4-gram
5-gram
Figure 4 Zipf curves for the WSJ87 corpus 
1
10
100
1000
10000
100000
1000000
log rank
1-gram
2-gram
3-gram
4-gram
5-gram
 Figure 5 Zipf curves for the WSJ88 corpus 
1
10
100
1000
10000
100000
1000000
log rank
1-gram
2-gram
3-gram
4-gram
5-gram
 Figure 6 Zipf curves for the WSJ89 corpus 
The n-gram Zipf curves approximately 
follow straight lines and can be represented by 
a single Mandelbrot form: 
?
r
kf =  (3) 
where ? is the magnitude of the negative slope 
of each line. We found the values of ? for the 
WSJ in Table 1. 
 
Table 1 Slopes for best-fit straight line 
approximations to the Zipf curves 
 WSJ87 WSJ88 WSJ89 Mandarin 
2-gram 0.67 0.66 0.65 0.75 
3-gram 0.51 0.50 0.46 0.59 
4-gram 0.42 0.42 0.39 0.53 
5-gram 0.42 0.41 0.34 0.48 
Note that the unigram curves crosses the 
bigram curves when the rank ? 3000 in all three 
cases. 
The ten most common words, bigrams 
and trigrams in the combined WSJ corpus of 40 
million words are listed in Table 2. 
 
Mandarin corpora 
 
The Mandarin corpus used in our experiments 
is the TREC Corpus. It was obtained from the 
People?s Daily Newspaper from 01/1991 to 
12/1993 and from the Xinhua News Agency for 
04/1994 to 09/1995 from the Linguistic Data 
Consortium (http://www.ldc.upenn.edu). TREC 
has  19,546,872  tokens  similar in size to the 
largest of the English corpora. The Mandarin 
language is a syllable-class language, in which 
each syllable is at the same time a word and a 
Chinese character. Other words, compound 
words, are built up by combining syllables 
together, similar to word n-grams in English. 
The most common unigrams, bigrams and 
trigrams are listed in Table 3.  
The number of syllable-types (i.e. 
unigrams) in the TREC corpus is  only  6,300, 
very different from English (the WSJ87 corpus 
has 114,718 word types); so it is not surprising 
that the Zipf curve for unigrams in Mandarin in 
Figure 7 is very different from the Zipf curve 
for unigrams in English. It is similar to a 
previous curve for a smaller Mandarin corpus 
of 2,022,604 tokens by Clark, Lua and 
McCallum (1986). The Zipf curves for n-grams  
Table 2 The 10-highest frequency unigrams, bigrams and trigrams in the WSJ corpus 
Unigrams Bigrams Trigrams 
Frequency Token Frequency Token Frequency Token 
2057968  
973650 
940525 
853342 
825489 
711462 
368012 
362771 
298646 
281190 
THE 
OF 
TO 
A 
AND 
IN 
THAT 
FOR 
ONE 
IS 
217427  
173797  
110291  
89184  
83799  
76187  
72312  
65565  
63838 
55014  
OF THE 
IN THE 
MILLION DOLLARS 
U. S. 
NINETEEN EIGHTY 
FOR THE 
TO THE 
ON THE 
ONE HUNDRED 
THAT THE 
42030  
27260  
24165  
18233  
16786  
15316  
14943  
14517  
12327  
11981  
THE U. S. 
IN NINETEEN EIGHTY 
CENTS A SHARE 
NINETEEN EIGHTY SIX 
NINETEEN EIGHTY SEVEN 
FIVE MILLION DOLLARS 
MILLION DOLLARS OR 
MILLION DOLLARS IN 
IN NEW YORK 
A YEAR EARLIER 
 
Table 3 The 10-highest frequency unigrams, bigrams and trigrams in the Mandarin TREC corpus.
for the Mandarin corpus are also shown in 
Figure 7. 
1
10
100
1000
10000
100000
1000000
log rank
1-gram
2-gram
3-gram
4-gram
5-gram
Figure 7 Zipf curves for the TREC Mandarin corpus 
Except for the unigrams, the shapes of 
the other TREC n-gram Zipfian curves are 
similar to but not quite the same as those for 
the  English corpora.  In particular  the   bigram 
curve  for  Mandarin  is  more  curved  than  the 
English curve because there are more 
compound words in Mandarin than English.  
The Mandarin ?-values in Table 1 are 
also higher than for English, on average by 
about 0.1, which is due to the different 
distribution of unigrams. For TREC, the 
crossing point between the unigram curve and 
the bigram curve is at rank: 1224, frequency: 
1750, unigram: " ", bigram: " ". The 
unigram curve and the trigram curve cross each 
other at rank: 1920, frequency: 491, unigram: 
" ", trigram: " ". This is very 
different from English. 
Comparisons between the n-grams 
curves (n = 1 to 4) for English and Mandarin 
are made in Figure 8, 9, 10 and 11. The English 
curves are for the 3 WSJ corpora joined 
together making a 40 million word corpus. 
 1
10
100
1000
10000
100000
1000000
10000000
log rank
WSJ-Unigrams
TREC-Unigrams
Figure 8 Zipf curve for the unigrams for the WSJ 
English and TREC Mandarin corpora 
1
10
100
1000
10000
100000
1000000
log rank
WSJ-2-gram
TREC-2-gram
Figure 9 Zipf curve for the bigrams for the WSJ 
English and TREC Mandarin corpora 
1
10
100
1000
10000
100000
log rank
WSJ-3-gram
TREC-3-gram
 Figure 10 Zipf curve for the trigrams for WSJ English 
and TREC Mandarin corpora 
1
10
100
1000
10000
100000
log rank
WSJ-4grams
TREC-4grams
Figure 11 Zipf curve for the 4-grams for the WSJ 
English and TREC Mandarin corpora 
 
3. Combined n-grams 
 
 The derivation of Zipf?s law by Simon 
was based solely on single words and it failed 
for English when the number of word types 
was greater than about 5000 words. In 
Mandarin it failed almost immediately for 
unigrams because of the limited number of 
characters. However it might not have failed if 
the Mandarin compound words in the bigram, 
trigram and higher n-gram statistics had been 
included; this suggested that the n-gram and 
unigram curves should be combined. Perhaps 
the same may be true for English. So we should 
combine the English curves also. 
 This can be justified in another way. In 
a critical part of his derivation Simon gives an 
initial probability to a new word found in the 
corpus as it introduces some new meaning not 
expressed by previous words. However, as the 
number of words increases new ideas are 
frequently expressed not in single words, but in 
multi-word phrases or compound words. This 
was left out of Simon?s derivation. If he had 
included it, the law he derived would have 
included phrases as well as words. So perhaps 
Zipf?s law should include words and phrases. 
We therefore put all unigram and n-
gram together with their frequencies into one 
large file, sorted on frequency and put in rank 
order as previously. The resulting Zipf curve 
for the combined curves for both English and 
Mandarin are shown in Figure 12. 
110
100
1000
10000
100000
1000000
10000000
log rank
WSJ
TREC
 Figure 12 Combined Zipf curves for the English WSJ 
and the TREC Mandarin corpora 
 
This shows that the n-grams (n > 2) 
exactly make up for the deviation of the two 
very different unigram curves from Zipf?s law 
and the combined curves for both languages are 
straight lines with slopes close to ?1 for all 
ranks>100. This result appears to vindicate 
Simon?s derivation. However, whether Simon?s 
derivation is entirely valid or not, the results in 
Figure 12 are a new confirmation of Zipf?s 
original law in an extended form. This 
remarkable result has been found to be valid for 
3 other natural languages: Irish, Latin and 
Vietnamese, in preliminary experiments. 
References  
Booth, A. D. (1967) ?A Law of Occurrences for Words 
of Low Frequency?. Inform. & Control Vol. 10, No. 
4, pp 386-393. April. 
Clark, J. L., Lua, K. T. & McCallum, J. (1986). ?Using 
Zipf's Law to Analyse the Rank Frequency 
Distribution of Elements in Chinese Text?. In Proc. 
Int. Conf. on Chinese Computing, pp. 321-324. 
August, Singapore. 
Fedorowicz, J. (1982) ?A Zipfian Model of an 
Automatic Bibliographic System: an Application to 
MEDLINE", Journal of American Society of 
Information Science, Vol. 33, pp 223-232. 
Ferrer Cancho, R. & Sol?, R. V. (2002)  ?Two Regimes 
in the Frequency of Words and the Origin of Complex 
Lexicons? To appear in Journal of Quantitative 
Linguistics. 
Francis, W. N. & Kucera, H. (1964). ?Manual of 
Information to Accompany A Standard Corpus of 
Present-Day Edited American English, for use with 
Digital Computers? Department of Linguistics, 
Brown University, Providence, Rhode Island 
Guiter H. & Arapov M., editors. (1982) "Studies on 
Zipf 's Law". Brochmeyer, Bochum.  
Li, W. (2001) ?Zipf's Law in Importance of Genes for 
Cancer Classification Using Microarray Data? Lab 
of Statistical Genetics, Rockefeller University, NY. 
Mandelbrot, B. (1953). ?An Information Theory of the 
Statistical Structure of Language?. Communication 
Theory, ed. By Willis Jackson, pp 486-502. New 
York: Academic Press. 
Mandelbrot, B. (1954) ?Simple Games of Strategy 
Occurring in Communication through Natural 
Languages?. Transactions of the IRE Professional 
Group on Information Theory, 3, 124-137 
Mandelbrot, B. (1957) ?A probabilistic Union Model 
for Partial and temporal Corruption of Speech?. 
Automatic Speech Recognition and Understanding 
Workshop. Keystone, Colorado, December. 
Mandelbrot, B. (1959) "A note on a class of skew 
distribution function analysis and critique of a paper 
by H.A. Simon", Inform. & Control, Vol. 2, pp 90-99. 
Mandelbrot, B. (1961) "Final note on a class of skew 
distribution functions: analysis and critique of a 
model due to H.A. Simon", Inform. & Control, Vol. 4, 
pp 198-216. 
Mandelbrot, B. B. (1961) "Post Scriptum to 'final 
note'", Inform. & Control, Vol. 4, pp 300-304. 
Montemurro, M. (2002) ?Beyond the Zipf-Mandelbrot 
Law in Quantitative Linguistics?.  To appear in 
Physica A. 
Paul, D. B. & Baker, J.M. (1992) ?The Design for the 
Wall Street Journal-based CSR Corpus?, Proc. 
ICSLP 92, pp 899-902, November. 
Samuelson, C. (1996). ?Relating Turing's Formula and 
Zipf's Law?.  Proceedings of the 4th Workshop on 
Very Large Corpora, Copenhagen, Denmark. 
Silagadze, Z. K. (1997) ?Citations and the Zipf-
Mandelbrot Law?. Complex Systems, Vol. 11, No.  
6, pp 487-499. 
Simon, H. A. (1955) "On a Class of Skew Distribution 
Functions", Biometrika, Vol. 42, pp 425-440. 
Simon, H. A. (1960) "Some Further Notes on a Class 
of Skew Distribution Functions", Inform. & Control, 
Vol. 3, pp 80-88. 
Simon, H. A. (1961) "Reply to Dr. Mandelbrot's post 
Scriptum" Inform. & Control, Vol. 4, pp 305-308. 
Simon, H. A. (1961) "Reply to 'final note' by Benoit 
Mandelbrot", Inform. & Control, Vol. 4, pp 217-223. 
Smith, F. J. & Devine, K. (1985) ?Storing and 
Retrieving Word Phrases? Information Processing & 
Management, Vol. 21, No. 3, pp 215-224. 
Yonezawa, Y. & Motohasi, H. (1999) ?Zipf-Scaling 
Description in the DNA Sequence? 10th Workshop on 
Genome Informatics. Japan. 
Zipf, G. K. (1949) ?Human Behaviour and the 
Principle of Least Effort? Reading, MA: Addison- 
Wesley Publishing co. 
 
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 309?315,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Reduced n-gram models for English and Chinese corpora 
 
 
Le Q Ha, P Hanna, D W Stewart and F J Smith 
School of Electronics, Electrical Engineering and Computer Science, 
Queen?s University Belfast 
Belfast BT7 1NN, Northern Ireland, United Kingdom 
lequanha@lequanha.com 
 
  
 
Abstract 
Statistical language models should       
improve as the size of the n-grams         
increases from 3 to 5 or higher. However, 
the number of parameters and                 
calculations, and the storage requirement 
increase very rapidly if we attempt to 
store all possible combinations of           
n-grams. To avoid these problems, the 
reduced n-grams? approach previously 
developed by O?Boyle (1993) can be     
applied. A reduced n-gram language 
model can store an entire corpus?s 
phrase-history length within feasible 
storage limits. Another theoretical         
advantage of reduced n-grams is that they 
are closer to being semantically complete 
than traditional models, which include all 
n-grams. In our experiments, the reduced 
n-gram Zipf curves are first presented, 
and compared with previously obtained 
conventional n-grams for both English 
and Chinese. The reduced n-gram model 
is then applied to large English and       
Chinese corpora. For English, we can       
reduce the model sizes, compared to       
7-gram traditional model sizes, with       
factors of 14.6 for a 40-million-word 
corpus and 11.0 for a 500-million-word 
corpus while obtaining 5.8% and 4.2% 
improvements in perplexities. For              
Chinese, we gain a 16.9% perplexity       
reductions and we reduce the model size 
by a factor larger than 11.2. This paper is 
a step towards the modeling of English 
and Chinese using semantically complete 
phrases in an n-gram model. 
1 Introduction to the Reduced N-Gram 
Approach 
Shortly after this laboratory first published a 
variable n-gram algorithm (Smith and O?Boyle, 
1992), O?Boyle (1993) proposed a statistical 
method to improve language models based on the 
removal of overlapping phrases. 
A distortion in the use of phrase frequencies 
had been observed in the small railway timetable 
Vodis Corpus when the bigram ?RAIL            
ENQUIRIES? and its super-phrase ?BRITISH 
RAIL ENQUIRIES? were examined. Both occur 
73 times, which is a large number for such a 
small corpus. ?ENQUIRIES? follows ?RAIL? 
with a very high probability when it is preceded 
by ?BRITISH.? However, when ?RAIL? is      
preceded by words other than ?BRITISH,?        
?ENQUIRIES? does not occur, but words like 
?TICKET? or ?JOURNEY? may. Thus, the     
bigram ?RAIL ENQUIRIES? gives a misleading 
probability that ?RAIL? is followed by          
?ENQUIRIES? irrespective of what precedes it. 
At the time of their research, O?Boyle reduced 
the frequencies of ?RAIL ENQUIRIES? by        
subtracting the frequency of the larger trigram, 
which gave a probability of zero for                
?ENQUIRIES? following ?RAIL? if it was not 
preceded by ?BRITISH.? The phrase with a new 
reduced frequency is called a reduced phrase. 
Therefore, a phrase can occur in a corpus as a 
reduced n-gram in some places and as part of a 
larger reduced n-gram in other places. In a         
reduced model, the occurrence of an n-gram is 
not counted when it is a part of a larger reduced 
n-gram. One algorithm to detect/identify/extract 
reduced n-grams from a corpus is the so-called 
reduced n-gram algorithm. In 1992, O?Boyle was 
able to use it to analyse the Brown corpus of 
American English (Francis and Kucera, 1964) (of 
one million word tokens, whose longest phrase-
309
length is 30), which was a considerable               
improvement at the time. The results were used 
in an n-gram language model by O?Boyle, but 
with poor results, due to lack of statistics from 
such a small corpus. We have developed and       
present here a modification of his method, and 
we discuss its usefulness for reducing n-gram 
perplexity. 
2 Similar Approaches and Capability 
Recent progress in variable n-gram language 
modeling has provided an efficient                   
representation of n-gram models and made the 
training of higher order n-grams possible.    
Compared to variable n-grams, class-based     
language models are more often used to reduce 
the size of a language model, but this typically 
leads to recognition performance degradation. 
Classes can alternatively be used to smooth a 
language model or provide back-off estimates, 
which have led to small performance gains. For 
the LOB corpus, the varigram model obtained 
11.3% higher perplexity in comparison with the 
word-trigram model (Niesler and Woodland, 
1996.) 
Kneser (1996) built up variable-context length 
language models based on the North American 
Business News (NAB-240 million words) and 
the German Verbmobil (300,000 words with a 
vocabulary of 5,000 types.) His results show that 
the variable-length model outperforms            
conventional models of the same size, and if a 
moderate loss in performance is acceptable, that 
the size of a language model can be reduced 
drastically by using his pruning algorithm.      
Kneser?s results improve with longer contexts 
and a same number of parameters. For example, 
reducing the size of the standard NAB trigram 
model by a factor of 3 results in a loss of only 
7% in perplexity and 3% in the word error rate. 
The improvement obtained by Kneser?s method 
depended on the length of the fixed context and 
on the amount of available training data. In the 
case of the NAB corpus, the improvement was 
10% in perplexity. 
Siu and Ostendorf (2000) developed Kneser?s 
basic ideas further and applied the variable        
4-gram, thus improving the perplexity and word 
error rate results compared to a fixed trigram 
model. They obtained word error reductions of 
0.1 and 0.5% (absolute) in development and    
evaluation test sets, respectively. However, the 
number of parameters was reduced by 60%. By 
using the variable 4-gram, they were able to 
model a longer history while reducing the size of 
the model by more than 50%, compared to a 
regular trigram model, and at the same time      
improve both the test-set perplexity and           
recognition performance. They also reduced the 
size of the model by an additional 8%. 
Other related work are those of Seymore and 
Rosenfeld (1996); Hu, Turin and Brown (1997); 
Blasig (1999); and Goodman and Gao (2000.) 
In order to obtain an overview of variable        
n-grams, Table 1 combines all of their results. 
COMBINATION OF LANGUAGE MODEL TYPES 
Basic 
n-gram 
Variable 
n-grams 
Category Skipping 
distance 
Classes #params Perplexity Size Source 
Trigram?     987k 474 
  Bigram?   - 603.2 
  Trigram?   - 544.1 
 ? ?   - 534.1 
1M LOB 
Trigram?     743k 81.5 
 Trigram?    379k 78.1 
 Trigram?  ?  363k 78.0 
 Trigram?  ? ? 338k 77.7 
 4-gram?    580k 108 
 4-gram?  ?  577k 108 
 4-gram?  ? ? 536k 107 
 5-gram?    383k 77.5 
 5-gram?  ?  381k 77.4 
 5-gram?  ? ? 359k 77.2 
2M Switch
board 
Corpus 
Table 1. Comparison of combinations of variable n-grams and other Language Models 
310
3 Reduced N-Gram Algorithm 
The main goal of this algorithm (Ha, 2005) is to 
produce three main files from the training text: 
? The file that contains all the complete   
n-grams appearing at least m times is 
called the PHR file (m ? 2.) 
? The file that contains all the n-grams   
appearing as sub-phrases, following the 
removal of the first word from any other 
complete n-gram in the PHR file, is called 
the SUB file. 
? The file that contains any overlapping   
n-grams that occur at least m times in the 
SUB file is called the LOS file. 
The final list of reduced phrases is called the FIN 
file, where 
SUBLOSPHRFIN ?+=:
 
(1) 
Before O?Boyle?s work, a student (Craig) in an 
unpublished project used a loop algorithm that 
was equivalent to FIN:=PHR?SUB. This yields 
negative frequencies for some resulting n-grams 
with overlapping, hence the need for the LOS 
file. 
There are 2 additional files 
? To create the PHR file, a SOR file is 
needed that contains all the complete       
n-grams regardless of m (the SOR file is 
the PHR file in the special case where      
m = 1.) To create the PHR file, words are 
removed from the right-hand side of each 
SOR phrase in the SOR file until the      
resultant phrase appears at least m times (if 
the phrase already occurs more than m 
times, no words will be removed.) 
? To create the LOS file, O?Boyle applied 
a POS file: for any SUB phrase, if one 
word can be added back on the right-hand 
side (previously removed when the PHR 
file was created from the SOR file), then 
one POS phrase will exist as the added 
phrase. Thus, if any POS phrase appears at 
least m times, its original SUB phrase will 
be an overlapping n-gram in the LOS file. 
The application scope of O?Boyle?s reduced       
n-gram algorithm is limited to small corpora, 
such as the Brown corpus (American English) of 
1 million words (1992), in which the longest 
phrase has 30 words. Now their algorithm,        
re-checked by us, still works for medium size 
and large corpora. In order to work well for very 
large corpora, it has been implemented by file 
distribution and sort processes. 
Ha, Seymour, Hanna and Smith (2005) have 
investigated a reduced n-gram model for the 
Chinese TREC corpus of the Linguistic Data 
Consortium (LDC) (http://www.ldc.upenn.edu/), 
catalog no. LDC2000T52.  
4 Reduced N-Grams and Zipf?s Law 
By re-applying O?Boyle and Smith?s algorithm, 
we obtained reduced n-grams from two English  
large corpora and a Chinese large corpus. 
The two English corpora used in our            
experiments are the full text of articles appearing 
in the Wall Street Journal (WSJ) (Paul and 
Baker, 1992) for 1987, 1988, 1989, with sizes 
approximately 19 million, 16 million and 6    
million tokens respectively; and the North 
American News Text (NANT) corpus from the 
LDC, sizing 500 million tokens, including Los 
Angeles Times & Washington Post for May 
1994-August 1997, New York Times News    
Syndicate for July 1994-December 1996, Reuters 
News Service (General & Financial) for April 
1994-December 1996 and Wall Street Journal for 
July 1994-December 1996. Therefore, the WSJ 
parts from the two English corpora are not    
overlapping together. 
The Mandarin News corpus from the LDC, 
catalog no. LDC95T13 was obtained from the 
People?s Daily Newspaper from 1991 to 1996 
(125 million syllables); from the Xinhua News 
Agency from 1994 to 1996 (25 million             
syllables); and from transcripts of China Radio 
International broadcast from 1994 to 1996 (100 
million syllables), altogether over 250 million 
syllables. The number of syllable types (i.e.    
unigrams) in the Mandarin News corpus is 6,800. 
Ha, Sicilia-Garcia, Ming and Smith (2003)      
produced a compound word version of the     
Mandarin News corpus with 50,000 types; this 
version was employed in our study for reduced 
n-grams. 
We next present the Zipf curves (Zipf, 1949) 
for the English and Chinese reduced n-grams. 
4.1 Wall Street Journal 
The WSJ reduced n-grams can be created by the 
original O?Boyle-Smith algorithm implemented 
on a Pentium II 586 of 512MByte RAM for over 
40 hours, the disk storage requirement being  
only 5GBytes. 
311
The conventional 10-highest frequency WSJ 
words have been published by Ha et al (2002) 
and the most common WSJ reduced unigrams, 
bigrams and trigrams are shown in Table 2. It 
illustrates that the most common reduced word is 
not THE; even OF is not in the top ten. These 
words are now mainly part of longer n-grams 
with large n. 
The Zipf curves are plotted for reduced      
unigrams and n-grams in Figure 1 showing all 
the curves have slopes within [-0.6, -0.5]. The 
WSJ reduced bigram, trigram, 4-gram and          
5-gram curves become almost parallel and 
straight, with a small observed noise between the 
reduced 4-gram and 5-gram curves when they cut 
each other at the beginning. Note that               
information theory tells us that an ideal              
information channel would be made of symbols 
with the same probability. So having a slope of        
?0.5 is closer than ?1 to this ideal. 
Unigrams Bigrams Trigrams Rank 
Freq Token Freq Token Freq Token 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
4,273 
2,469 
2,422 
2,144 
1,918 
1,660 
1,249 
1,101 
1,007 
 997 
Mr. 
but 
and 
the 
says 
or 
said 
however 
while 
meanwhile 
2,268 
2,052 
1,945 
1,503 
1,332 
 950 
 856 
 855 
 832 
 754 
he said 
he says 
but the 
but Mr. 
and the 
says Mr. 
in addition 
and Mr. 
last year 
for example 
1,231 
709 
664 
538 
524 
523 
488 
484 
469 
466 
terms weren?t disclosed 
the company said 
as previously reported 
he said the 
a spokesman for 
the spokesman said 
as a result 
earlier this year 
in addition to 
according to Mr. 
Table 2. Most common WSJ reduced n-grams 
log rank
lo
g 
fr
eq
ue
nc
y
1-gram
2-gram
3-gram
4-gram
5-gram
0
1
2
3
4
 2  510  4 3 6  7
slope -1
 
Figure 1. The WSJ reduced n-gram Zipf curves 
4.2 North American News Text corpus 
The NANT reduced n-grams are created by the 
improved algorithm after over 300 hours           
processing, needing a storage requirement of 
100GBytes on a Pentium II 586 of 512MByte 
RAM. 
Their Zipf curves are plotted for reduced       
unigrams and n-grams in Figure 2 showing all 
the curves are just sloped around [-0.54, -0.5]. 
The reduced unigrams of NANT still show the    
2-slope behavior when it starts with slope ?0.54 
and then drop with slope nearly ?2 at the end of 
the curve. We have found that the traditional       
n-grams also show this behaviour, with an initial 
slope of ?1 changing to ?2 for large ranks (Ha 
and Smith, 2004.) 
log rank
lo
g 
fr
eq
ue
nc
y
1-gram
2-gram
3-gram
4-gram
5-gram
slope -1
 2 510 4 3 6 7 8
0
1
2
3
4
5
6
 
Figure 2. The NANT reduced n-gram Zipf curves 
4.3 Mandarin News compound words 
The Zipf curves are plotted for the smaller       
Chinese TREC reduced unigrams and n-grams 
were shown by Ha et al (2005.) 
312
log rank
lo
g 
fr
eq
ue
nc
y
1-gram
2-gram
3-gram
4-gram
5-gram
slope -1
 2  510  43 6 7
0
1
2
3
4
5
6
 
Figure 3. Mandarin reduced n-gram Zipf curves 
The Mandarin News reduced word n-grams were 
created in 120 hours, using 20GB of disk space.  
The Zipf curves are plotted in Figure 3 showing 
that the unigram curve now has a larger slope 
than ?1, it is around ?1.2. All the n-gram curves 
are now straighter and more parallel than the    
traditional n-gram curves, have slopes within      
[-0.67, -0.5]. 
Usually, Zipf?s rank-frequency law with a 
slope ?1 is confirmed by empirical data, but the 
reduced n-grams for English and Chinese shown 
in Figure 1, Figure 2 and Figure 3 do not confirm 
it. In fact, various more sophisticated models for 
frequency distributions have been proposed by 
Baayen (2001) and Evert (2004.) 
5 Perplexity for Reduced N-Grams 
The reduced n-gram approach was used to build 
a statistical language model based on the 
weighted average model of O?Boyle, Owens and 
Smith (1994.) We rewrite this model in formulae 
(2) and (3) 
( ) ( )( ) 11 2log +?? ?= jiijij wfwwgt
 
(2) 
( ) ( ) ( ) ( ) ( )( )?
?
?
=
?
?
=
?
??
?
+?
?+?
= 1
0
1
1
1
1
1 N
l
i
li
N
l
i
lii
i
liii
i
NiiWA
wwgt
wwPwwgtwPwwgt
wwP  
(3) 
This averages the probabilities of a word wi      
following the previous one word, two words, 
three words, etc. (i.e. making the last word of an 
n-gram.) The averaging uses weights that          
increase slowly with their frequency and rapidly 
with the length of n-gram. This weighted average 
model is a variable length model that gives        
results comparable to the Katz back-off method 
(1987), but is quicker to use. 
The probabilities of all of the sentences mw1 in 
a test text are then calculated by the weighted 
average model 
( ) ( ) ( ) ( )111211 ... ?= mmWAWAWAm wwPwwPwPwP
 
(4) 
and an average perplexity of each sentence is 
evaluated using Equation (5) 
( ) ( )( )?
?
??
?
??= ?
=
?
L
i
iiWA
m wwwwPLn
L
wPP
1
1211 ...
1exp
 
(5) 
Ha et al (2005) already investigated and         
analysed the main difficulties arising from       
perplexity calculations for our reduced model: a 
statistical model problem, an unseen word    
problem and an unknown word problem. Their 
solutions are applied in this paper also. Similar 
problems have been found by other authors, e.g. 
Martin, Liermann and Ney (1997); Kneser and 
Ney (1995.) 
The perplexity calculations for both the            
English and Chinese reduced n-grams includes 
statistics on phrase lengths starting with          
unigrams, bigrams, trigrams?and on up to the 
longest phrase which occur in the reduced model. 
The perplexities of the WSJ reduced model are 
shown in Table 3, North American News Text 
corpus in Table 4 and Mandarin News words in 
Table 5. 
The nature of the reduced model makes the       
reporting of results for limited sizes of n-grams 
to be inappropriate, although these are valid for a 
traditional n-gram model. Therefore we show 
results for several n-gram sizes for the traditional 
model, but only one perplexity for the reduced 
model. 
313
Tokens 0 Unknowns 
Types 0 
 Unigrams 762.69 
 Bigrams 144.33 
 Trigrams 75.36 
Traditional Model 4-grams 60.73 
 5-grams 56.85 
 6-grams 55.66 
 7-grams 55.29 
Reduced Model 70.98 
%Improvement of Reduced 
Model on baseline Trigrams 
5.81% 
Model size reduction 14.56 
Table 3. Reduced perplexities for English WSJ 
Tokens 24 Unknowns 
Types 23 
 Unigrams 1,442.99 
 Bigrams 399.61 
 Trigrams 240.52 
Traditional Model 4-grams 202.59 
 5-grams 194.06 
 6-grams 191.91 
 7-grams 191.23 
Reduced Model 230.46 
%Improvement of Reduced 
Model on baseline Trigrams 
4.18% 
Model size reduction 11.01 
Table 4. Reduced perplexities for English NANT 
Tokens 84 Unknowns 
Types 26 
 Unigrams 1,620.56 
 Bigrams 377.43 
 Trigrams 179.07 
Traditional Model 4-grams 135.69 
 5-grams 121.53 
 6-grams 114.96 
 7-grams 111.69 
Reduced Model 148.71 
%Improvement of Reduced 
Model on baseline Trigrams 
16.95% 
Model size reduction 11.28 
Table 5. Reduced perplexities for Mandarin 
News words 
In all three cases the reduced model produces a 
modest improvement over the traditional                
3-gram model, but is not as good as the                 
traditional 4-gram or higher models. However in 
all three cases the result is obtained with a      
significant reduction in model size, from a factor 
of 11 to almost 15 compared to the traditional        
7-gram model size.  
We did expect a greater improvement in          
perplexity than we obtained and we believe that a 
further look at the methods used to solve the          
difficult problems listed by Ha et al (2005) 
(mentioned above) and others mentioned by Ha 
(2005) might lead to an improvement. Missing 
word tests are also needed. 
6 Conclusions 
The conventional n-gram language model is   
limited in terms of its ability to represent        
extended phrase histories because of the            
exponential growth in the number of parameters. 
To overcome this limitation, we have                 
re-investigated the approach of O?Boyle (1993) 
and created reduced n-gram models. Our aim 
was to try to create an n-gram model that used 
semantically more complete n-grams than      
traditional n-grams in the expectation that this 
might lead to an improvement in language      
modeling. The improvement in perplexity is 
modest, but there is a large decrease in model 
size. So this represents an encouraging step     
forward, although still very far from the final 
step in language modelling. 
Acknowledgements 
The authors would like to thank Dr Ji Ming for 
his support and the reviewers for their valuable 
comments. 
References 
Douglas B. Paul and Janet B. Baker. 1992. The      
Design for the Wall Street Journal based CSR   
Corpus. In Proc. of the DARPA SLS Workshop, 
pages 357-361. 
Francis J. Smith and Peter O?Boyle. 1992. The         
N-Gram Language Model. The Cognitive Science 
of Natural Language Processing Workshop, pages 
51-58. Dublin City University.  
George K. Zipf. 1949. Human Behaviour and the 
Principle of Least Effort. Reading, MA: Addison- 
Wesley Publishing Co. 
Harald R. Baayen. 2001. Word Frequency                
Distributions. Kluwer Academic Publishers. 
Jianying Hu, William Turin and Michael K. Brown. 
1997. Language Modeling using Stochastic        
Automata with Variable Length Contexts.           
Computer Speech and Language, volume 11, pages 
1-16. 
314
Joshua Goodman and Jianfeng Gao. 2000. Language 
Model Size Reduction By Pruning And Clustering. 
ICSLP?00. Beijing, China. 
Kristie Seymore and Ronald Rosenfeld. 1996.        
Scalable Backoff Language Models. ICSLP?96, 
pages 232-235. 
Le Q. Ha and Francis. J. Smith. 2004. Zipf and Type-
Token rules for the English and Irish languages. 
MIDL workshop. Paris. 
Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and Francis 
J. Smith. 2002. Extension of Zipf?s Law to Words 
and Phrases. COLING?02, volume 1, pages 315-
320. 
Le Q. Ha, Elvira I. Sicilia-Garcia, Ji Ming and Francis 
J. Smith. 2003. Extension of Zipf?s Law to Word 
and Character N-Grams for English and Chinese. 
CLCLP, 8(1):77-102. 
Le Q. Ha, Rowan Seymour, Philip Hanna and Francis 
J. Smith. 2005. Reduced N-Grams for Chinese 
Evaluation. CLCLP, 10(1):19-34. 
Manhung Siu and Mari Ostendorf. 2000. Integrating a 
Context-Dependent Phrase Grammar in the         
Variable N-Gram framework. ICASSP?00, volume 
3, pages 1643-1646. 
Manhung Siu and Mari Ostendorf. 2000. Variable     
N-Grams and Extensions for Conversational 
Speech Language Modelling. IEEE Transactions 
on Speech and Audio Processing, 8(1):63-75. 
Nelson Francis and Henry Kucera. 1964. Manual of 
Information to Accompany A Standard Corpus of 
Present-Day Edited American English, for use with 
Digital Computers. Department of Linguistics, 
Brown University, Providence, Rhode Island. 
Peter  L.  O?Boyle.    1993.    A  study  of  an  N-Gram 
Language Model for Speech Recognition. PhD     
thesis. Queen?s University Belfast. 
Peter O?Boyle, John McMahon and Francis J. Smith. 
1995. Combining a Multi-Level Class Hierarchy 
with Weighted-Average Function-Based      
Smoothing. IEEE Automatic Speech Recognition 
Workshop. Snowbird, Utah. 
Peter O?Boyle, Marie Owens and Francis J. Smith. 
1994. A weighted average N-Gram model of     
natural language. Computer Speech and Language, 
volume 8, pages 337-349. 
Ramon Ferrer I. Cancho and Ricard V. Sol?. 2002. 
Two Regimes in the Frequency of Words and the 
Origin of Complex Lexicons. Journal of             
Quantitative Linguistics, 8(3):165-173. 
Reinhard Blasig. 1999. Combination of Words and 
Word Categories in Varigram Histories. 
ICASSP?99, volume 1, pages 529-532. 
Reinhard Kneser and Hermann Ney. 1995. Improved 
Backing-off for M-Gram Language Modeling. 
ICASSP?95, volume 1, pages 181-184. Detroit.  
Reinhard Kneser. 1996. Statistical Language            
Modeling Using a Variable Context Length. 
ICSLP?96, volume 1, pages 494-497. 
Slava M. Katz. 1987. Estimation of Probabilities from 
Sparse Data for the Language Model Component 
of a Speech Recognizer. In IEEE Transactions on 
Acoustics, Speech and Signal Processing, volume 
ASSP-35, pages 400-401. 
Stefan Evert. 2004. A Simple LNRE Model for      
Random Character Sequences. In Proc. of the 
7?mes Journ?es Internationales d'Analyse           
Statistique des Donn?es Textuelles, pages 411-422. 
Sven C. Martin, J?rg Liermann and Hermann Ney. 
1997. Adaptive Topic-Dependent Language      
Modelling Using Word-Based Varigrams.           
EuroSpeech?97, volume 3, pages 1447-1450.      
Rhodes. 
Thomas R. Niesler and Phil C. Woodland. 1996. A 
Variable-Length Category-Based N-Gram        
Language Model. ICASSP?96, volume 1, pages 
164-167. 
Thomas R. Niesler. 1997. Category-based statistical 
language models. St. John?s College, University of 
Cambridge. 
315
