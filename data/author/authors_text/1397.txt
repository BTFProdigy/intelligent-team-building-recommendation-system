In: Proceedings of CoNLL-2000 and LLL-2000, pages 91-94, Lisbon, Portugal, 2000. 
Inducing Syntactic Categories by Context Distribution Clustering 
Alexander  Clark  
School of Cognitive and Computing Sciences 
University of Sussex 
a lexc?cogs ,  susx. ac. uk 
Abst rac t  
This paper addresses the issue of the automatic 
induction of syntactic ategories from unanno- 
tared corpora. Previous techniques give good 
results, but fail to cope well with ambiguity or 
rare words. An algorithm, context distribution 
clustering (CDC), is presented which can be 
naturally extended to handle these problems. 
1 In t roduct ion  
In this paper I present a novel program that in- 
duces syntactic ategories from comparatively 
small corpora of unlabelled text, using only dis- 
tributional information. There are various mo- 
tivations for this task, which affect the algo- 
rithms employed. Many NLP systems use a 
set of tags, largely syntactic in motivation, that 
have been selected according to various criteria. 
In many circumstances it would be desirable for 
engineering reasons to generate a larger set of 
tags, or a set of domain-specific tags for a par- 
ticular corpus. Furthermore, the construction 
of cognitive models of language acquisit ion- 
that will almost certainly involve some notion 
of syntactic ategory - requires an explanation 
of the acquisition of that set of syntactic ate- 
gories. The amount of data used in this study 
is 12 million words, which is consistent with a 
pessimistic lower bound on the linguistic experi- 
ence of the infant language learner in the period 
from 2 to 5 years of age, and has had capitalisa- 
tion removed as being information ot available 
in that circumstance. 
2 Prev ious  Work  
Previous work falls into two categories. A num- 
ber of researchers have obtained good results 
using pattern recognition techniques. Finch 
and Chater (1992), (1995) and Schfitze (1993), 
(1997) use a set of features derived from the 
co-occurrence statistics of common words to- 
gether with standard clustering and information 
extraction techniques. For sufficiently frequent 
words this method produces satisfactory esults. 
Brown et al (1992) use a very large amount 
of data, and a well-founded information theo- 
retic model to induce large numbers of plausi- 
ble semantic and syntactic lusters. Both ap- 
proaches have two flaws: they cannot deal well 
with ambiguity, though Schfitze addresses this 
issue partially, and they do not cope well with 
rare words. Since rare and ambiguous words are 
very common in natural language, these limita- 
tions are serious. 
3 Context  D is t r ibut ions  
Whereas earlier methods all share the same ba- 
sic intuition, i.e. that similar words occur in 
similar contexts, I formalise this in a slightly 
different way: each word defines a probability 
distribution over all contexts, namely the prob- 
ability of the context given the word. If the 
context is restricted to the word on either side, 
I can define the context distribution to be a dis- 
tribution over all ordered pairs of words: the 
word before and the word after. The context 
distribution of a word can be estimated from 
the observed contexts in a corpus. We can then 
measure the similarity of words by the simi- 
larity of their context distributions, using the 
Kullback-Leibler (KL) divergence as a distance 
function. 
Unfortunately it is not possible to cluster 
based directly on the context distributions for 
two reasons: first the data is too sparse to es- 
timate the context distributions adequately for 
any but the most frequent words, and secondly 
some words which intuitively are very similar 
91 
(Schfitze's example is 'a' and 'an') have rad- 
ically different context distributions. Both of 
these problems can be overcome in the normal 
way by using clusters: approximate the context 
distribution as being a probability distribution 
over ordered pairs of clusters multiplied by the 
conditional distributions of the words given the 
clusters :
p(< Wl, W2 >) -= p(< Cl, C2 >)p(wlICl)p(w2\[c2) 
I use an iterative algorithm, starting with a 
trivial clustering, with each of the K clusters 
filled with the kth most frequent word in the 
corpus. At each iteration, I calculate the con- 
text distribution of each cluster, which is the 
weighted average of the context distributions of 
each word in the cluster. The distribution is cal- 
culated with respect o the K current clusters 
and a further ground cluster of all unclassified 
words: each distribution therefore has (K + 1) 2 
parameters. For every word that occurs more 
than 50 times in the corpus, I calculate the con- 
text distribution, and then find the cluster with 
the lowest KL divergence from that distribution. 
I then sort the words by the divergence from 
the cluster that is closest to them, and select 
the best as being the members of the cluster 
for the next iteration. This is repeated, grad- 
ually increasing the number of words included 
at each iteration, until a high enough propor- 
tion has been clustered, for example 80%. Af- 
ter each iteration, if the distance between two 
clusters falls below a threshhold value, the clus- 
ters are merged, and a new cluster is formed 
from the most frequent unclustered word. Since 
there will be zeroes in the context distributions, 
they are smoothed using Good-Turing smooth- 
ing(Good, 1953) to avoid singularities in the KL 
divergence. At this point we have a preliminary 
clustering - no very rare words will be included, 
and some common words will also not be as- 
signed, because they are ambiguous or have id- 
iosyncratic distributional properties. 
4 Ambiguity and Sparseness 
Ambiguity can be handled naturally within 
this framework. The context distribution p(W) 
of a particular ambiguous word w can be 
modelled as a linear combination of the con- 
text distributions of the various clusters. We 
can find the mixing coefficients by minimising 
D(p(W)ll (w) a~w) oLi qi) where the are some co- 
efficients that sum to unity and the qi are the 
context distributions of the clusters. A mini- 
mum of this function can be found using the 
EM algorithm(Dempster et al, 1977). There 
are often several ocal minima - in practice this 
does not seem to be a major problem. 
Note that with rare words, the KL divergence 
reduces to the log likelihood of the word's con- 
text distribution plus a constant factor. How- 
ever, the observed context distributions of rare 
words may be insufficient to make a definite de- 
termination of its cluster membership. In this 
case, under the assumption that the word is 
unambiguous, which is only valid for compar- 
atively rare words, we can use Bayes's rule to 
calculate the posterior probability that it is in 
each class, using as a prior probability the dis- 
tribution of rare words in each class. This in- 
corporates the fact that rare words are much 
more likely to be adjectives or nouns than, for 
example, pronouns. 
5 Results 
I used 12 million words of the British Na- 
tional Corpus as training data, and ran this al- 
gorithm with various numbers of clusters (77, 
100 and 150). All of the results in this paper 
are produced with 77 clusters corresponding to
the number of tags in the CLAWS tagset used 
to tag the BNC, plus a distinguished sentence 
boundary token. In each case, the clusters in- 
duced contained accurate classes corresponding 
to the major syntactic categories, and various 
subgroups of them such as prepositional verbs, 
first names, last names and so on. Appendix A 
shows the five most frequent words in a cluster- 
ing with 77 clusters. In general, as can be seen, 
the clusters correspond to traditional syntactic 
classes. There are a few errors - notably, the 
right bracket is classified with adverbial parti- 
cles like "UP". 
For each word w, I then calculated the opti- 
mal coefficents c~ w). Table 1 shows some sam- 
ple ambiguous words, together with the clusters 
with largest values of c~ i. Each cluster is repre- 
sented by the most frequent member of the clus- 
ter. Note that "US" is a proper noun cluster. 
As there is more than one common noun clus- 
ter, for many unambiguous nouns the optimum 
is a mixture of the various classes. 
92 
Word Clusters 
ROSE 
VAN 
MAY 
US 
HER 
THIS 
CAME CHARLES GROUP 
JOHN TIME GROUP 
WILL US JOHN 
YOU US NEW 
THE YOU 
THE IT LAST 
Table 1: Ambiguous words. For each word, the 
clusters that have the highest a are shown, if 
a > 0.01. 
Model 
Freq 
1 0.66 0.21 
2 0.64 0.27 
3 0.68 0.36 
5 0.69 0.40 
10 0.72 0.50 
20 0.73 0.61 
CDC Brown CDC Brown 
NN1 NN1 A J0 A J0 
0.77 0.41 
0.77 0.58 
0.82 0.73 
0.83 0.81 
0.92 0.94 
0.91 0.94 
Table 2: Accuracy of classification ofrare words 
with tags NN1 (common oun) and A J0 (adjec- 
tive). 
Table 2 shows the accuracy of cluster assign- 
ment for rare words. For two CLAWS tags, A J0 
(adjective) and NNl(singular common noun) 
that occur frequently among rare words in the 
corpus, I selected all of the words that oc- 
curred n times in the corpus, and at least half 
the time had that CLAWS tag. I then tested 
the accuracy of my assignment algorithm by 
marking it as correct if it assigned the word 
to a 'plausible' cluster - for A J0, either of the 
clusters "NEW" or "IMPORTANT", and for 
NN1, one of the clusters "TIME", "PEOPLE", 
"WORLD", "GROUP" or "FACT". I did this 
for n in {1, 2, 3, 5, 10, 20}. I proceeded similarly 
for the Brown clustering algorithm, selecting 
two clusters for NN1 and four for A J0. This can 
only be approximate, since the choice of accept- 
able clusters is rather arbitrary, and the BNC 
tags are not perfectly accurate, but the results 
are quite clear; for words that occur 5 times or 
less the CDC algorithm is clearly more accurate. 
Evaluation is in general difficult with unsu- 
pervised learning algorithms. Previous authors 
have relied on both informal evaluations of the 
plausibility of the classes produced, and more 
formal statistical methods. Comparison against 
existing tag-sets is not meaningful - one set of 
Test set 1 2 3 4 
CLAWS 411 301 478 413 
Brown et al 380 252 444 369 
CDC 372 255 427 354 
Mean 
395 
354 
346 
Table 3: Perplexities of class tri-gram models 
on 4 test sets of 100,000 words, together with 
geometric mean. 
tags chosen by linguists would score very badly 
against another without his implying any fault 
as there is no 'gold standard'. I therefore chose 
to use an objective statistical measure, the per- 
plexity of a very simple finite state model, to 
compare the tags generated with this cluster- 
ing technique against he BNC tags, which uses 
the CLAWS-4 tag set (Leech et al, 1994) which 
had 76 tags. I tagged 12 million words of BNC 
text with the 77 tags, assigning each word to 
the cluster with the highest a posteriori proba- 
bility given its prior cluster distribution and its 
context. 
I then trained 2nd-order Markov models 
(equivalently class trigram models) on the orig- 
inal BNC tags, on the outputs from my algo- 
rithm (CDC), and for comparision on the out- 
put from the Brown algorithm. The perplexities 
on held-out data are shown in table 3. As can 
be seen, the perplexity is lower with the model 
trained on data tagged with the new algorithm. 
This does not imply that the new tagset is bet- 
ter; it merely shows that it is capturing statisti- 
cal significant generalisations. In absolute terms 
the perplexities are rather high; I deliberately 
chose a rather crude model without backing off 
and only the minimum amount of smoothing, 
which I felt might sharpen the contrast. 
6 Conc lus ion  
The work of Chater and Finch can be seen as 
similar to the work presented here given an in- 
dependence assumption. We can model the con- 
text distribution as being the product of inde- 
pendent distributions for each relative position; 
in this case the KL divergence is the sum of 
the divergences for each independent distribu- 
tion. This independence assumption is most 
clearly false when the word is ambiguous; this 
perhaps explains the poor performance of these 
algorithms with ambiguous words. The new 
algorithm currently does not use information 
93 
about the orthography of the word, an impor- 
tant source of information. In future work, I will 
integrate this with a morphology-learning pro- 
gram. I am currently applying this approach 
to the induction of phrase structure rules, and 
preliminary experiments have shown encourag- 
ing results. 
In summary, the new method avoids the limi- 
tations of other approaches, and is better suited 
to integration into a complete unsupervised lan- 
guage acquisition system. 
References  
Peter F. Brown, Vincent J. Della Pietra, Peter V. 
de Souza, Jenifer C. Lai, and Robert Mercer. 
1992. Class-based n-gram models of natural an- 
guage. Computational Linguistics, 18:467-479. 
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm. Journal o/ the Royal Statistical 
Society Series B, 39:1-38. 
S. Finch and N. Chater. 1992. Bootstrapping syn- 
tactic categories. In Proceedings o/ the l~th An- 
nual Meeting of the Cognitive Science Society, 
pages 820-825. 
S. Finch, N. Chater, and Redington M. 1995. Ac- 
quiring syntactic information from distributional 
statistics. In Joseph P. Levy, Dimitrios Bairak- 
taris, John A. Bullinaria, and Paul Cairns, edi- 
tors, Connectionist Models o/Memory and Lan- 
guage. UCL Press. 
I. J. Good. 1953. The population frequencies of 
species and the estimation of population parame- 
ters. Biometrika, 40:237-264. 
G. Leech, R. Garside, and M Bryant. 1994. 
CLAWS4: the tagging of the British National 
Corpus. In Proceedings o/the 15th International 
Con/erence on Computational Linguistics, pages 
622-628. 
Hinrich Schfitze. 1993. Part of speech induction 
from scratch. In Proceedings o/ the 31st an- 
nual meeting o/ the Association /or Computa- 
tional Linguistics, pages 251-258. 
Hinrich Schfitze. 1997. Ambiguity Resolution in 
Language Learning. CSLI Publications. 
A C lus ters  
Here are the five most ~equent words in each of the 
77 clusters, one cluster per line except where indi- 
cated with a double slash \ \  
THE A HIS THIS AN 
PEOPLE WORK LIFE RIGHT END 
OF IN FOR 0N WITH \\ , ~MDASH ( : ; 
NEW OTHER FIRST OWN G00D 
~SENTENCE \\ . ? ! 
AND AS 0R UNTIL SUCHuAS 
NOT BEEN N'T $0 0NLY 
IS WAS HAD HAS DID 
MADE USED FOUND LEFT PUT 
0NE ALL MORE S0ME TWO 
TIME WAY YEAR DAY MAN \\ T0 
WORLD GOVERNMENT PARTY FAMILY WEST 
BE HAVE D0 MAKE GET 
HE I THEY SHE WE 
US BRITAIN LONDON GOD LABOUR 
BUT WHEN IF WHERE BECAUSE 
) UP 0UTBACK DOWN 
WILL WOULD CAN C0ULD MAY 
USE HELP FORM CHANGE SUPPORT 
THAT BEFOREABOVE 0UTSIDE BEL0W 
IT EVERYBODY GINA 
GROUP NUMBER SYSTEM 0FFICE CENTRE 
Y0U THEM HIM ME THEMSELVES 
~BQU0 \\ ~EQU0 \\ ARE WERE \\ 'S ' 
CHARLES MARK PHILIP HENRY MARY 
WHAT HOW WHY HAVING MAKING 
IMPORTANT POSSIBLE CLEAR HARD CLOSE 
WHICH WH0 
CAME WENT LOOKED SEEMED BEGAN 
JOHN SIR DAVID ST DE 
YEARS PERuCENT DAYS TIMES MONTHS 
GOING ABLE LOOKING TRYING COMING 
THOUGHT FELT KNEW DECIDED HOPE 
SEE SAY FEEL MEAN REMEMBER 
SAID SAYS WROTE EXPLAINED REPLIED 
GO COME TRY CONTINUE APPEAR \\ THERE 
L00K RUN LIVE MOVE TALK 
SUCH USING PROVIDING DEVELOPING WINNING 
T00K TOLD SAW GAVE MAKES 
HOWEVER 0FuCOURSE FORuEXAMPLE INDEED 
PART S0RT THINKING LACK NONE 
SOMETHING ANYTHING SOMEONE EVERYTHING 
MR MRS DR HONG MR. 
NEED NEEDS SEEM ATTEMPT OPPORTUNITY 
WANT WANTED TRIED WISH WANTS 
BASED RESPONSIBLE COMPARED INTERESTED 
THAN \\ LAST NEXT GOLDEN FT-SE \\ THOSE 
THINK BELIEVE SUPPOSE INSIST RECKON 
KNOWUNDERSTAND REALISE 
LATER AG0 EARLIER THEREAFTER 
BETTER WORSE LONGER BIGGER STRONGER 
aHELLIP .. 
ASKED LIKED WATCHED SMILED INVITED 
'M AM \\ 'D 
FACT IMPRESSION ASSUMPTION IMPLICATION 
NOTHING NOWHERE RISEN 
BEC0ME \\ ENOUGH \\ FAR INFINITELY 
'LL \\ 'RE \\ 'VE \\ CA W0 AI 
COPE DEPEND C0NCENTRATE SUCCEED C0MPETE 
RO HVK AMEN 
KLERK CLOWES HOWE C0LI GAULLE 
NEZ KHMER 
94 
Partially Distribution-Free Learning of Regular Languages from
Positive Samples
Alexander Clark
ISSCO / TIM, University of Geneva
UNI-MAIL, Boulevard du Pont-d?Arve,
CH-1211 Gene`ve 4, Switzerland
asc@aclark.demon.co.uk
Franck Thollard
EURISE, Universite? Jean Monnet,23,
Rue du Docteur Paul Michelon,
42023 Saint-Etienne Ce?dex 2, France
thollard@univ-st-etienne.fr
Abstract
Regular languages are widely used in NLP to-
day in spite of their shortcomings. Efficient
algorithms that can reliably learn these lan-
guages, and which must in realistic applications
only use positive samples, are necessary. These
languages are not learnable under traditional
distribution free criteria. We claim that an ap-
propriate learning framework is PAC learning
where the distributions are constrained to be
generated by a class of stochastic automata with
support equal to the target concept. We discuss
how this is related to other learning paradigms.
We then present a simple learning algorithm
for regular languages, and a self-contained proof
that it learns according to this partially distri-
bution free criterion.
1 Introduction
Regular languages, especially generated by de-
terministic finite state automata are widely used
in Natural Language processing, for various dif-
ferent tasks (Mohri, 1997). Efficient learning
algorithms, that have some guarantees of cor-
rectness, would clearly be useful. Existing al-
gorithms for learning deterministic automata,
such as (Carrasco and Oncina, 1994) have only
guarantees of identification in the limit (Gold,
1967), generally considered not to be a good
guide to practical utility. Unforunately the
prospects for learning according to the more
useful PAC-learning criterion are poor after the
well known result of (Kearns and Valiant, 1989).
Distribution-free learning criteria require algo-
rithms to learn for every possible combination
of concept and distribution. Under this worst-
case analysis many simple concept classes are
unlearnable. However in many situations it is
more realistic to assume that there is some re-
lationship between the concept and the distri-
bution, and furthermore in general only positive
examples will be available.
There are two ways of modelling this. The
simplest is to study the learnability of distribu-
tions (Kearns et al, 1994; Ron et al, 1995). In
this case the samples are drawn from the distri-
bution that is being learned. The choice of error
function then becomes critical ? the most nat-
ural (and difficult) being the Kullback-Leibler
divergence. This means that any successful al-
gorithm must produce hypotheses that assign a
non-zero probability to every string. If what we
are interested in is learning the underlying non-
probabilistic concept then these hypotheses will
be useless. We have elsewhere proved (Clark
and Thollard, 2004) a suitable result similar to
that of (Ron et al, 1995), bounding the diver-
gence, but that proof involves some more elab-
orate technical machinery.
The second way is to consider a traditional
concept-learning problem, but to restrict the
class of distributions to some set that only gen-
erates positive examples, and has some relation
to the target concept. It is this latter possibility
that we explore here.
In the particular case of learning languages
we will have an instance space of ?? for some
finite alphabet ?, and we shall have a concept
class, in this paper, corresponding to the class
of all regular languages. In a distribution-free
setting this is not learnable from positive and
negative samples, nor a fortiori from positive
samples alone. In our partially distribution-free
framework however, we are able to prove learn-
ability with an additional parameter in the sam-
ple complexity polynomial, that bounds a sim-
ple property of the distribution. We are able to
present a simple stand alone proof for this well
studied class of languages.
The rest of the paper is structured as follows.
Section 2 argues for a modified version of PAC
learning as being an appropriate learning frame-
work for a range of NLP problems. After defin-
ing some notation in Section 3 we then define
an algorithm that learns regular languages (Sec-
tion 4) and then in Section 5 prove that it does
so according to this modified PAC-learnability
criterion. We conclude with a critical analysis
of our results.
2 Appropriateness
Regular languages are widely used in a num-
ber of different applications drawn from nu-
merous domains such as computational biology,
robotics etc. In many of these areas, efficient
learning algorithms are desirable but in each the
exact requirements will be different since the
sources of information, and the desired proper-
ties of the algorithms vary widely. We argue
here that learning algorithms in NLP have cer-
tain special properties that make the particu-
lar learnability result we study here useful. The
most important feature in our opinion is the ne-
cessity for learning from positive examples only.
Negative examples in NLP are rarely available.
Even in a binary classification problem, there
will often be some overlap between the classes
so that examples labelled with ? are not nec-
essarily negative examples of the class labelled
with +. For this reason alone we consider a tra-
ditional distribution-free PAC-learning frame-
work to be wholly inappropriate. An essential
part of the PAC-learning framework is a sort
of symmetry between the positive and negative
examples. Furthermore, there are a number
of negative results which rule out distribution
free learning of regular languages (Kearns et al,
1994).
A related problem is that in the sorts of learn-
ing situations that occur in practice in NLP
problems, and also those such as first language
acquisition that one wishes to model formally,
the distribution of examples is dependent on the
concept being learned. Thus if we are modelling
the acquisition of the grammar of a language,
the positive examples are the grammatical, or
perhaps acceptable, sentences of the target lan-
guage. The distribution of examples is clearly
highly dependent on the particular language,
simply as a matter of fact, in that the sentences
in the sample are generated by people who have
acquired the language.
It thus seems reasonable to require the dis-
tribution to be drawn from some limited class
that depends on the target concept and gen-
erates only positive examples ? i.e. where the
support of the distribution is identical to the
positive part of the target concept.
Our proposal is that when the class of lan-
guages is defined by some simple class of au-
tomata, we can consider only those distribu-
tions generated by the corresponding stochas-
tic automata. The set of distributions is re-
stricted and thus we call this partially distribu-
tion free. Thus when learning the class of regu-
lar languages, which are generated by determin-
istic finite-state automata, we select the class
of distributions which are generated by PDFAs.
Similarly, context free languages are normally
defined by context-free grammmars which can
be extended again to probabilistic or stochastic
context free grammars.
Formally, for every class of languages, L, de-
fined by some formal device define a class of
distributions, D, defined by a stochastic variant
of that device. Then for each language L, we
select the set of distributions whose support is
equal to the language:
D+L = {D ? D : ?s ? ?? s ? L ? PD(s) > 0}
Samples are drawn from one of these distri-
butions. There are two technical problems here:
first, this doesn?t penalise over-generalisation.
Since the distribution is over positive examples,
negative examples have zero weight ? which
would give a hypothesis of all strings zero er-
ror. We therefore need some penalty function
over negative examples or alternatively require
the hypothesis to be a subset of the target, and
use a one-sided loss function as in Valiant?s orig-
inal paper (Valiant, 1984), which is what we
do here. Secondly, this definition is too vague.
The exact way in which you extend the ?crisp?
language to a stochastic one can have serious
consequences. When dealing with regular lan-
guages, for example, though the class of lan-
guages defined by deterministic automata is the
same as that defined by non-deterministic lan-
guages, the same is not true for their stochas-
tic variants. Additionally, one can have expo-
nential blow-ups in the number of states when
determinizing automata. Similarly, with con-
text free languages, (Abney et al, 1999) showed
that converting between two parametrisations
of models for stochastic context free languages
are equivalent but that there are blow-ups in
both directions.
It is interesting to compare this to the PAC-
learning with simple distributions model (De-
nis, 2001). There, the class of distributions
is limited to a single distribution derived from
algorithmic complexity theory. There are a
number of reasons why this is not appropriate.
First there is a computational issue: since Kol-
mogorov complexity is not computable, sam-
pling from the distribution is not possible,
though a lower bound on the probabilities can
be defined. Secondly, there are very large con-
stants in the sample complexity polynomial. Fi-
nally and most importantly, there is no reason
to think that in the real world, samples will be
drawn from this distribution; in some sense it
is the easiest distribution to learn from since it
dominates every other distribution up to a mul-
tiplicative factor.
We reject the identification in the limit
paradigm introduced by (Gold, 1967) as un-
suitable for three reasons. First it is only an
asymptotic bound that says nothing about the
performance of the algorithms on finite amounts
of data; secondly because it must learn under
all presentations of the data even when these
are chosen by an adversary to make it hard to
learn, and thirdly because it has no bounds on
the amount of computation allowed.
An alternative way to conceive of this prob-
lem is to consider the task of learning distri-
butions directly (Kearns et al, 1994), a task
related to probability density estimation and
language modelling, where the algorithm is
given examples drawn from a distribution and
must approximate the distribution closely ac-
cording to some distance metric: usually the
Kullback-Leibler divergence or the variational
distance. We consider the choice between the
distribution-learning analysis, and the analysis
we present here to depend on what the under-
lying task or phenomena to be modelled is. If
it is the probability of the event occurring, then
the distribution modelling analysis is better. If
on the other hand it concerns binary judgments
about the membership of strings in some set
then the analysis we present here is preferable.
The result of (Kearns et al, 1994) shows up
a further problem. Under a standard crypto-
graphic assumption the class of acyclic PDFAs
over a two-letter alphabet are not learnable
since the class of noisy parity functions can be
embedded in this simple subclass of PDFAs.
(Ron et al, 1995) show that this can be cir-
cumvented by adding an additional parameter
to the sample complexity polynomial, the dis-
tinguishability, which we define below.
3 Preliminaries
We will write ? for letters and s for strings.
We have a finite alphabet ?, and ?? is the
free monoid generated by ?, i.e. the set of all
strings with letters from ?, with ? the empty
string (identity). For s ? ?? we define |s| to
be the length of s. The subset of ?? of strings
of length d is denoted by ?d. A distribution
or stochastic language D over ?? is a function
D : ?? ? [0, 1] such that ?s??? D(s) = 1. The
L? norm between two distributions is defined as
maxs |D1(s) ? D2(s)|. For a multiset of strings
S we write S? for the empirical distribution de-
fined by that multiset ? the maximum likelihood
estimate of the probability of the string.
A probabilistic deterministic finite state
automaton is a mathematical object that
stochastically generates strings of symbols.
It has a finite number of states one of which
is a distinguished start state. Parsing or
generating starts in the start state, and at
any given moment makes a transition with a
certain probability to another state and emits
a symbol. We have a particular symbol and
state which correspond to finishing.
A PDFA A is a tuple (Q, ?, q0, qf , ?, ?, ?) ,
where
? Q is a finite set of states,
? ? is the alphabet, a finite set of symbols,
? q0 ? Q is the single initial state,
? qf 6? Q is the final state,
? ? 6? ? is the final symbol,
? ? : Q???{?} ? Q?{qf} is the transition
function and
? ? : Q ? ? ? {?} ? [0, 1] is the next sym-
bol probability function. ?(q, ?) = 0 when
?(q, ?) is not defined.
We will sometimes refer to automata by the
set of states. All transitions that emit ? go to
the final state. In the following ? and ? will
be extended to strings recursively in the normal
way.
The sum of the output transition from each
states must be one: so for all q ? Q
?
????{?}
?(q, ?) = 1 (1)
Assuming further that there is a non zero proba-
bility of reaching the final state from each state:
i.e.
?q ? Q?s ? ?? : ?(q, s?) = qf ? ?(q, s?) > 0
(2)
the PDFA then defines a probability distribu-
tion over ??, where the probability of generat-
ing a string s ? ?? is PA(s) = ?(q0, s?). We will
write L(A) for the support of this distribution,
L(A) = {s ? ?? : PA(s) > 0}. We will also
define Pq(s) = ?(q, s?) which we call the suffix
distribution of the state q.
We say that two states q, q? are ?-
distinguishable if L?(Pq, Pq?) > ? for some ? >
0. An automaton is ?-distinguishable iff every
pair of states is ?-distinguishable. Since we can
merge states q, q? which have L?(Pq, Pq?) = 0,
we can assume without loss of generality that
every PDFA has a non-zero distinguishability.
Note that ?(q0, s) where s ? ?? is the prefix
probability of the string s, i.e. the probability
that the automaton will generate a string that
starts with s.
We will use a similar notation, neglecting the
probability function for (non-probabilistic) de-
terministic finite-state automata (DFAs).
4 Algorithm
We shall first state our main result.
Theorem 1 For any regular language L, when
samples are generated by a PDFA A where
L(A) = L, with distinguishability ? and num-
ber of states n, for any , ? > 0, the algorithm
LearnDFA will with probability at least 1? ? re-
turn a DFA H which defines a language L(H)
that is a subset of L with PA(L(A)?L(H)) < .
The algorithm will draw a number of samples
bounded by a polynomial in |?|, n, 1/?, 1/, 1/?,
and the computation is bounded by a polynomial
in the number of samples and the total length of
the strings in the sample.
We now define the algorithm LearnDFA. We
incrementally construct a sequence of DFAs
that will generate subsets of the target lan-
guage. Each state of the hypothesis automata
will represent a state of the target and will have
attached a multiset of strings that approximates
the distribution of strings generated by that
state. We calculate the following quantities m0
and N from the input parameters.
m0 =
8
?2 log
48n|?|(n|?| + 2)
?? (3)
N = 2n|?|m0 (4)
We start with an automaton that consists of a
single state and no transitions, and the attached
multiset is a sample of strings from the target.
At each step we sample N strings from the tar-
get distribution. This re-sampling ensures the
independence of all of the samples, and allows
us to apply bounds in a straightforward way.
For each state u in the hypothesis automaton
and letter ? in the alphabet, such that there is
no arc labelled with ? out of u we construct a
candidate node (u, ?) which represents the state
reached from u by the transition labelled with ?.
For each string in the sample, we trace the cor-
responding path through the hypothesis. When
we reach a candidate node, we remove the pre-
ceding part of the string, and add the rest to
the multiset of the candidate node. Otherwise,
in the case when the string terminates in the
hypothesis automaton we discard the string.
After we have done this for every string in
the sample, we select a candidate node (u, ?)
that has a multiset of size at least m0. If there
is no such candidate node, the algorithm ter-
minates, Otherwise we compare this candidate
node with each of the nodes already in the hy-
pothesis. The comparison we use calculates the
L?-norm between the empirical distributions of
the two multisets and says they are similar if
this distance is less than ?/4. We will make
sure that with high probability these empirical
distributions are close in the L?-norm to the
suffix distributions of the states they represent.
Since we know that the suffix distributions of
different states will be at least ? apart, we can
be confident that we will only rarely make mis-
takes. If there is a node, v, which is similar then
we conclude that v and (u, ?) represent the same
state. We therefore add an arc labelled with ?
leading from u to v. If it is not similar to any
node in the hypothesis, then we conclude that
it represents a new node, and we create a new
node u? and add an arc labelled with ? leading
from u to u?. In this case we attach the mul-
tiset of the candidate node to the new node in
the hypothesis. Intuitively this multiset will be
a sample from the suffix distribution of the state
of the target that it represents. We then discard
all of the candidate nodes and their associated
multisets, but keep the multisets attached to the
states of the hypothesis, and repeat.
5 Proof
We can now prove that this algorithm has the
properties we claim. We use one technical
lemma that we prove in the appendix.
Lemma 1 Given a distribution D over ??, for
any ?? < 1/2, when we independently draw
a number of samples m more than m0 =
1
2??2 log
12
???? , into a multiset S then L?(S?, D) <
?? with probability greater than 1 ? ??.
Let H0, H1, . . . , Hk be the sequence of finite
automata, the states labelled with multisets,
generated by the algorithm when samples are
generated by a target PDFA A.
We will say that a hypothesis automaton Hi
is ?-good if there is a bijective function ? from
a subset of states of A including q0, to all the
states of Hi such that ?(q0) is the root node
of Hi, and if there is an edge in Hi such that
?(u, ?) = v then ?(??1(u), ?) = ??1(v) i.e. if
Hi is isomorphic to a subgraph of the target that
includes the root. If ?(q) = u then we say that
u represents q. In this case the language gen-
erated by Hi is a subset of the target language.
Additionally we require that for every state v in
the hypothesis, the corresponding multiset sat-
isfies L?(S?v, P??1(v)) < ?/4. When a multiset
satisfies this we will say it is ?-good.
We will extend the function ? to candidate
nodes in the obvious way, and also the definition
of ?-good.
Definition 1 (Good sample) We say that a
sample of size N is ?--good given a good hy-
pothesis DFA H and a target A if all the candi-
date nodes with multisets larger than the thresh-
old m0 are ?-good, and that if PA(L(A) ?
L(H)) >  then the number of strings that
exit the hypothesis automaton is more than
1
2NPA(L(A) ? L(H)).
5.1 Approximately Correct
We will now show if all the samples are good,
that for all i?0, 1, . . . , k, the hypothesis Hi will
be good, and that when the algorithm termi-
nates the final hypothesis will have low error.
We will do this by induction on the index i of
the hypothesis Hi. Clearly H0 is good. Sup-
pose Hi?1 is good, and we draw a good sample.
Consider a candidate node (u, ?) with multiset
greater than m0.
Since the previous hypothesis was good, this
will be a representative of a state q and thus
the multiset will be a sequence of independent
draws from the suffix distribution of this state
Pq. Thus L?( ?Su,?, Pq) < ?/4 by the good-
ness of the sample. We compare it to a state
in the hypothesis v. If this state is a rep-
resentative of the same state in the target v,
then L?(S?v, Pq) < ?/4 (by the goodness of the
multisets), the triangle inequality shows that
L?( ?Su,?, S?v) < ?/2, and therefore the compar-
ison will return true. On the other hand, let us
suppose that v is a representative of a different
state qv. We know that L?( ?Su,?, Pq) < ?/4
and L?(S?v, Pqv) < ?/4 (by the goodness of
the multisets), and L?(Pq, Pqv) ? ? (by the
?-distinguishability of the target). By the tri-
angle inequality L?(Pq, Pqv) ? L?( ?Su,?, Pq) +
L?( ?Su,?, S?v) + L?(S?v, Pqv), which implies that
L?( ?Su,?, S?v) > ?/2 and the comparison will
return false. In these cases Hi will be good.
Alternatively there is no candidate node above
threshold in which case the algorithm termi-
nates, and i = k. The total number of strings
that exit the hypotheis must then be less than
n|?|m0 since there are at most n|?| candidate
nodes each of which has multiset of size less than
m0. By the definition of N and the goodness of
the sample PA(L(A) ? L(H)) < . Since it is
good and thus defines a subset of the target lan-
guage, this is a suitably close hypothesis.
5.2 Probably Correct
We must now show that by setting m0 suffi-
ciently large we can be sure that with probabil-
ity greater than 1? ? all of the samples will be
good. We need to show that with high prob-
ability a sample of size N will be good for a
given hypotheis G. We can assume that the
hypothesis is good at each step. Each step of
the algorithm will increase the number of tran-
sitions in the active set by at least 1. There are
at most n|?| transitions in the target; so there
are at most n|?|+2 steps in the algorithm since
we need an initial step to get the multiset for
the root node and another at the end when we
terminate. So we want to show that a particu-
lar sample will be good with probability at least
1 ? ?n|?|+2 .
There are two sorts of errors that can make
the sample bad. First, one of the multisets could
be bad, and secondly too few strings might exit
the graph. There are at most n|?| candidate
nodes, so we will make the probability of getting
a bad multiset less than ?/2n|?|(n|?|+ 2), and
we will make the probability of the second sort
of error less than ?/2(n|?| + 2).
First we bound the probability of getting a
bad multiset of size m0. This will be satisfied
if we set ?? = ?/4 and ?? = ?/2n|?|(n|?| + 2),
and use Lemma 1.
We next need to show that at each step the
number of strings that exit the graph will be
not too far from its expectation, if PA(L(A) ?
L(H)) > . We can use Chernoff bounds to
show that the probability too few strings exit
the graph will be less than ?/2(n|?| + 2)
e?N(G)PA(L(A)?L(H))/4 < e?N/4
< ?/2(n|?| + 2)
which will be satisfied by the value of N de-
fined earlier, as can be easily verified.
5.3 Polynomial complexity
Since we need to draw at most n|?|+2 samples
of size N the overall sample complexity will be
(n|?| + 2)N , which ignoring log factors gives a
sample complexity of O(n2|?|2??2?1), which
is quite benign. It is easy to see that the com-
putational complexity is polynomial. Produc-
ing an exact bound is difficult since it depends
on the length of the strings. The precise com-
plexity also depends on the relative magnitudes
of ?, |?| and so on. The complexity is domi-
nated by the cost of the comparisons. We can
limit each multiset comparison to at most m0
strings, which can be compared naively with m20
string comparisons or much more efficiently us-
ing hashing or sorting. The number of nodes
in the hypothesis is at most n, and the num-
ber of candidate nodes is at most n|?|, so the
number of comparisons at each step is bounded
by n2|?| and thus the total number of multiset
comparisons by n2|?|(n|?|+2). Construction of
multisets can be performed in time linear in the
sample size. These observations suffice to show
that the computation is polynomially bounded.
6 Discussion
The convergence of these sorts of algorithms
has been studied before in the identification in
the limit framework, but previous proofs have
not been completely convincing (Carrasco and
Oncina, 1999), and this criterion gives no guide
to the practical utility of the algorithms since it
applies only asymptotically. The partially dis-
tribution free learning problem we study here
is novel. as is the extension of the results of
(Ron et al, 1995) to cyclic automata and thus
to infinite languages.
Before we examine our results critically, we
would like to point out some positive aspects of
the algorithm. First, this class of algorithms is
in practice efficient and reliable. This particular
algorithm is designed to have a provably good
worst-case performance, and thus we anticipate
its average performance on naturally occurring
data to be marginally worse than comparable
algorithms. We have established that we can
learn an exponentially large family of infinite
languages using polynomial amounts of data
and computation. Mild properties of the in-
put distributions suffice to guarantee learnabil-
ity. The algorithm we present here is however
not intended to be efficient or cognitively plau-
sible: our intention was to find one that allowed
a simple proof.
The major weakness of this approach in our
opinion is that the parameter n in the sample
complexity polynomial is the number of states
in the PDFA generating the distribution, and
not the number of states in the minimal FA gen-
erating the language. Since determinisation of
finite automata can cause exponential blow ups
this is potentially a serious problem, depending
on the application domain. A second problem
is the need for a distinguishability parameter,
which again in specific cases could be exponen-
tially small. An alternative to this is to define
a class of ?-distinguishable automata where the
distinguishability is bounded by an inverse poly-
nomial in the number of states. Formally this is
equivalent, but it has the effect of removing the
parameter from the sample complexity polyno-
mial at the cost of having a further restriction
on the class of distributions. Indeed we can deal
with the previous objection in the same way if
necessary by requiring the number of states in
the generating PDFA to be bounded by a poly-
nomial in the minimal number of states needed
to generate the target language. However both
of these limitations are unavoidable given the
negative results previously discussed.
References
S. Abney, D. McAllester, and F. Pereira.
1999. Relating probabilistic grammars and
automata. In Proceedings of ACL ?99.
R. C. Carrasco and J. Oncina. 1994. Learn-
ing stochastic regular grammars by means of
a state merging method. In R. C. Carrasco
and J. Oncina, editors, Grammatical Infer-
ence and Applications, ICGI-94, number 862
in LNAI, pages 139?152, Berlin, Heidelberg.
Springer Verlag.
R. C. Carrasco and J. Oncina. 1999. Learning
deterministic regular grammars from stochas-
tic samples in polynomial time. Theoretical
Informatics and Applications, 33(1):1?20.
Alexander Clark and Franck Thollard. 2004.
Pac-learnability of probabilistic determinis-
tic finite state automata. Journal of Machine
Learning Research, 5:473?497, May.
F. Denis. 2001. Learning regular languages
from simple positive examples. Machine
Learning, 44(1/2):37?66.
E. M. Gold. 1967. Language indentification in
the limit. Information and control, 10(5):447
? 474.
M. Kearns and G. Valiant. 1989. Crypto-
graphic limitations on learning boolean for-
mulae and finite automata. In 21st annual
ACM symposium on Theory of computation,
pages 433?444, New York. ACM, ACM.
M.J. Kearns, Y. Mansour, D. Ron, R. Rubin-
feld, R.E. Schapire, and L. Sellie. 1994. On
the learnability of discrete distributions. In
Proc. of the 25th Annual ACM Symposium
on Theory of Computing, pages 273?282.
Mehryar Mohri. 1997. Finite-state transducers
in language and speech processing. Compu-
tational Linguistics, 23(2):269?311.
D. Ron, Y. Singer, and N. Tishby. 1995. On the
learnability and usage of acyclic probabilistic
finite automata. In COLT 1995, pages 31?40,
Santa Cruz CA USA. ACM.
L. Valiant. 1984. A theory of the learnable.
Communications of the ACM, 27(11):1134 ?
1142.
Appendix
Proof of Lemma 1.
We write p(s) for the true probability and
p?(s) = c(s)/m for the empirical probability of
the string in the sample ? i.e. the maximum
likelihood estimate. We want to bound the
probability over an infinite number of strings,
which rules out a naive application of Hoeffding
bounds. It will suffice to show that every string
with probability less than ??/2 will have empir-
ical probability less than ??, and that all other
strings will have probability within ?? of their
true values. The latter is straightforward: since
there are at most 2/?? of these frequent strings.
For any given frequent string s, by Hoeffding
bounds:
Pr[|p?(s) ? p(s)| > ??] < 2e?2m??2 < 2e?2m0??2
(5)
So the probability of making an error on a
frequent string is less than 4/??e?2m0??2 .
Consider all of the strings whose probability
is in [??2?(k+1), ??2?k).
Sk = {s ? ?? : ?(q, s?) ? [??2?(k+1), ??2?k)}
(6)
We define Srare =
??
k=1 Sk. The Chernoff
bound says that for any ? > 0, for the sum of n
bernouilli variables with prob p and
Pr(X > (1 + ?)np) <
( e?
(1 + ?)(1+?)
)np
(7)
Now we bound each group separately, using
the binomial Chernoff bound where n = m?? >
mp (which is true since p < ??)
Pr
[
p?(s) ? ??
]
?
(mp
n
)n
en?mp (8)
This bound decreases with p, so we can re-
place this for all strings in Sk with the upper
bound for the probability, and we can replace
m with m0.
Pr
[
p?(s) ? ??
]
?
(m0??2?k
m0??
)m0??
em0???m0??2?k
?
(
2?ke1?2?k
)m0??
< 2?km0??
Assuming that m0?? > 3
Pr
[
p?(s) ? ??
]
< 2?2k22?m0??
Pr
[
?s ? Sk : p?(s) ? ??
]
? |Sk|2?2k22?m0?
?
? 8?2
?k2?m0??
Using the factor of the form 2?k, we can sum
over all of the k.
Pr[?s ? Srare : p?(s) ? ??] <
8
?? 2
?m0??
?
?
k=1
2?k
< 8?2
?m0??
Putting these together we can show that the
probability of the bound being exceeded will be
4
?? e
?2m0??2 + 8?2
?m0?? < 12?? e
?2m0??2 (9)
This will be less than ?? if
m0 =
1
2??2 log
12
???? (10)
which establishes the result.
59
60
61
62
63
64
65
66
Memory-Based Learning of Morphology with Stochastic Transducers
Alexander Clark
ISSCO / TIM
University of Geneva
UNI-MAIL, Boulevard du Pont-d?Arve,
CH-1211 Gene`ve 4,
Switzerland
Alex.Clark@issco.unige.ch
Abstract
This paper discusses the supervised learn-
ing of morphology using stochastic trans-
ducers, trained using the Expectation-
Maximization (EM) algorithm. Two ap-
proaches are presented: first, using the
transducers directly to model the process,
and secondly using them to define a sim-
ilarity measure, related to the Fisher ker-
nel method (Jaakkola and Haussler, 1998),
and then using a Memory-Based Learn-
ing (MBL) technique. These are evaluated
and compared on data sets from English,
German, Slovene and Arabic.
1 Introduction
Finite-state methods are in large part adequate to
model morphological processes in many languages.
A standard methodology is that of two-level mor-
phology (Koskenniemi, 1983) which is capable of
handling the complexity of Finnish, though it needs
substantial extensions to handle non-concatenative
languages such as Arabic (Kiraz, 1994). These mod-
els are primarily concerned with the mapping from
deep lexical strings to surface strings, and within
this framework learning is in general difficult (Itai,
1994). In this paper I present algorithms for learn-
ing the finite-state transduction between pairs of un-
inflected and inflected words. ? supervised learning
of morphology. The techniques presented here are,
however, applicable to learning other types of string
transductions.
Memory-based techniques, based on principles of
non-parametric density estimation, are a powerful
form of machine learning well-suited to natural lan-
guage tasks. A particular strength is their ability to
model both general rules and specific exceptions in
a single framework (van den Bosch and Daelemans,
1999).
However they have generally only been used in
supervised learning techniques where a class label or
tag has been associated to each feature vector. Given
these manual or semi-automatic class labels, a set of
features and a pre-defined distance function new in-
stances are classified according to the class label of
the closest instance. However these approaches are
not a complete solution to the problem of learning
morphology, since they do not directly produce the
transduction. The problem must first be converted
into an appropriate feature-based representation and
classified in some way. The techniques presented
here operate directly on sequences of atomic sym-
bols, using a much less articulated representation,
and much less input information.
2 Stochastic Transducers
It is possible to apply the EM algorithm to learn the
parameters of stochastic transducers, (Ristad, 1997;
Casacuberta, 1995; Clark, 2001a). (Clark, 2001a)
showed how this approach could be used to learn
morphology by starting with a randomly initialized
model and using the EM algorithm to find a local
maximum of the joint probabilities over the pairs of
inflected and uninflected words. In addition rather
than using the EM algorithm to optimize the joint
probability it would be possible to use a gradient de-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 513-520.
                         Proceedings of the 40th Annual Meeting of the Association for
scent algorithm to maximize the conditional proba-
bility.
The models used here are Stochastic Non-
Deterministic Finite-State Transducers (FST), or
Pair Hidden Markov Models (Durbin et al, 1998),
a name that emphasizes the similarity of the train-
ing algorithm to the well-known Forward-Backward
training algorithm for Hidden Markov Models.
Instead of outputting symbols in a single stream,
however, as in normal Hidden Markov Models they
output them on two separate streams, the left and
right streams. In general we could have different
left and right alphabets; here we assume they are the
same. At each transition the FST may output the
same symbol on both streams, a symbol on the left
stream only, or a symbol on the right stream only. I
call these   ,   and   outputs respectively. For
each state  the sum of all these output parameters
over the alphabet 	 must be one.



 

 

 
ff25
Grammatical Inference and First Language Acquisition
Alexander Clark (asc@aclark.demon.co.uk)
ISSCO / TIM, University of Geneva
UNI-MAIL, Boulevard du Pont-d?Arve,
CH-1211 Gene`ve 4, Switzerland
Abstract
One argument for parametric models of language
has been learnability in the context of first language
acquisition. The claim is made that ?logical? ar-
guments from learnability theory require non-trivial
constraints on the class of languages. Initial formal-
isations of the problem (Gold, 1967) are however
inapplicable to this particular situation. In this pa-
per we construct an appropriate formalisation of the
problem using a modern vocabulary drawn from sta-
tistical learning theory and grammatical inference
and looking in detail at the relevant empirical facts.
We claim that a variant of the Probably Approxi-
mately Correct (PAC) learning framework (Valiant,
1984) with positive samples only, modified so it is
not completely distribution free is the appropriate
choice. Some negative results derived from crypto-
graphic problems (Kearns et al, 1994) appear to ap-
ply in this situation but the existence of algorithms
with provably good performance (Ron et al, 1995)
and subsequent work, shows how these negative re-
sults are not as strong as they initially appear, and
that recent algorithms for learning regular languages
partially satisfy our criteria. We then discuss the
applicability of these results to parametric and non-
parametric models.
1 Introduction
For some years, the relevance of formal results
in grammatical inference to the empirical question
of first language acquisition by infant children has
been recognised (Wexler and Culicover, 1980). Un-
fortunately, for many researchers, with a few no-
table exceptions (Abe, 1988), this begins and ends
with Gold?s famous negative results in the identifi-
cation in the limit paradigm. This paradigm, though
still widely used in the grammatical inference com-
munity, is clearly of limited relevance to the issue
at hand, since it requires the model to be able to
exactly identify the target language even when an
adversary can pick arbitrarily misleading sequences
of examples to provide. Moreover, the paradigm as
stated has no bounds on the amount of data or com-
putation required for the learner. In spite of the inap-
plicability of this particular paradigm, in a suitable
analysis there are quite strong arguments that bear
directly on this problem.
Grammatical inference is the study of machine
learning of formal languages. It has a vast formal
vocabulary and has been applied to a wide selec-
tion of different problems, where the ?languages?
under study can be (representations of) parts of nat-
ural languages, sequences of nucleotides, moves of
a robot, or some other sequence data. For any con-
clusions that we draw from formal discussions to
have any applicability to the real world, we must
be sure to select, or construct, from the rich set of
formal devices available an appropriate formalisa-
tion. Even then, we should be very cautious about
making inferences about how the infant child must
or cannot learn language: subsequent developments
in GI might allow a more nuanced description in
which these conclusions are not valid. The situation
is complicated by the fact that the field of grammti-
cal inference, much like the wider field of machine
learning in general, is in a state of rapid change.
In this paper we hope to address this problem by
justifying the selection of the appropriate learning
framework starting by looking at the actual situa-
tion the child is in, rather than from an a priori deci-
sion about the right framework. We will not attempt
a survey of grammatical inference techniques; nor
shall we provide proofs of the theorems we use here.
Arguments based on formal learnability have been
used to support the idea of parameter based theo-
ries of language (Chomsky, 1986). As we shall see
below, under our analysis of the problem these ar-
guments are weak. Indeed, they are more pertinent
to questions about the autonomy and modularity of
language learning: the question whether learning of
some level of linguistic knowledge ? morphology
or syntax, for example ? can take place in isolation
from other forms of learning, such as the acquisition
of word meaning, and without interaction, ground-
ing and so on.
26
Positive results can help us to understand how hu-
mans might learn languages by outlining the class of
algorithms that might be used by humans, consid-
ered as computational systems at a suitable abstract
level. Conversely, negative results might be help-
ful if they could demonstrate that no algorithms of a
certain class could perform the task ? in this case we
could know that the human child learns his language
in some other way.
We shall proceed as follows: after briefly de-
scribing FLA, we describe the various elements of
a model of learning, or framework. We then make
a series of decisions based on the empirical facts
about FLA, to construct an appropriate model or
models, avoiding unnecessary idealisation wherever
possible. We proceed to some strong negative re-
sults, well-known in the GI community that bear on
the questions at hand. The most powerful of these
(Kearns et al, 1994) appears to apply quite directly
to our chosen model. We then discuss an interest-
ing algorithm (Ron et al, 1995) which shows that
this can be circumvented, at least for a subclass of
regular languages. Finally, after discussing the pos-
sibilities for extending this result to all regular lan-
guages, and beyond, we conclude with a discussion
of the implications of the results presented for the
distinction between parametric and non-parametric
models.
2 First Language Acquisition
Let us first examine the phenomenon we are con-
cerned with: first language acquisition. In the space
of a few years, children almost invariably acquire,
in the absence of explicit instruction, one or more of
the languages that they are exposed to. A multitude
of subsidiary debates have sprung up around this
central issue covering questions about critical peri-
ods ? the ages at which this can take place, the ex-
act nature of the evidence available to the child, and
the various phases of linguistic use through which
the infant child passes. In the opinion of many re-
searchers, explaining this ability is one of the most
important challenges facing linguists and cognitive
scientists today.
A difficulty for us in this paper is that many of
the idealisations made in the study of this field are
in fact demonstrably false. Classical assumptions,
such as the existence of uniform communities of
language users, are well-motivated in the study of
the ?steady state? of a system, but less so when
studying acquisition and change. There is a regret-
table tendency to slip from viewing these idealisa-
tions correctly ? as counter-factual idealizations ? to
viewing them as empirical facts that need to be ex-
plained. Thus, when looking for an appropriate for-
mulation of the problem, we should recall for exam-
ple the fact that different children do not converge to
exactly the same knowledge of language as is some-
times claimed, nor do all of them acquire a language
competently at all, since there is a small proportion
of children who though apparently neurologically
normal fail to acquire language. In the context of
our discussion later on, these observations lead us
to accept slightly less stringent criteria where we al-
low a small probability of failure and do not demand
perfect equality of hypothesis and target.
3 Grammatical Inference
The general field of machine learning has a spe-
cialised subfield that deals with the learning of for-
mal languages. This field, Grammatical Inference
(GI), is characterised above all by an interest in for-
mal results, both in terms of formal characterisa-
tions of the target languages, and in terms of formal
proofs either that particular algorithms can learn ac-
cording to particular definitions, or that sets of lan-
guage cannot be learnt. In spite of its theoretical
bent, GI algorithms have also been applied with
some success. Natural language, however is not the
only source of real-world applications for GI. Other
domains include biological sequence data, artificial
languages, such as discovering XML schemas, or
sequences of moves of a robot. The field is also
driven by technical motives and the intrinsic ele-
gance and interest of the mathematical ideas em-
ployed. In summary it is not just about language,
and accordingly it has developed a rich vocabulary
to deal with the wide range of its subject matter.
In particular, researchers are often concerned
with formal results ? that is we want algorithms
where we can prove that they will perform in a cer-
tain way. Often, we may be able to empirically es-
tablish that a particular algorithm performs well, in
the sense of reliably producing an accurate model,
while we may be unable to prove formally that the
algorithm will always perform in this way. This
can be for a number of reasons: the mathematics
required in the derivation of the bounds on the er-
rors may be difficult or obscure, or the algorithm
may behave strangely when dealing with sets of data
which are ill-behaved in some way.
The basic framework can be considered as a
game played between two players. One player, the
teacher, provides information to another, the learner,
and from that information the learner must identify
the underlying language. We can break down this
situation further into a number of elements. We as-
sume that the languages to be learned are drawn
27
in some way from a possibly infinite class of lan-
guages, L, which is a set of formal mathematical
objects. The teacher selects one of these languages,
which we call the target, and then gives the learner
a certain amount of information of various types
about the target. After a while, the learner then re-
turns its guess, the hypothesis, which in general will
be a language drawn from the same class L. Ide-
ally the learner has been able to deduce or induce
or abduce something about the target from the in-
formation we have given it, and in this case the hy-
pothesis it returns will be identical to, or close in
some technical sense, to the target. If the learner
can conistently do this, under whatever constraints
we choose, then we say it can learn that class of lan-
guages. To turn this vague description into some-
thing more concrete requires us to specify a number
of things.
? What sort of mathematical object should we
use to represent a language?
? What is the target class of languages?
? What information is the learner given?
? What computational constraints does the
learner operate under?
? How close must the target be to the hypothesis,
and how do we measure it?
This paper addresses the extent to which negative
results in GI could be relevant to this real world sit-
uation. As always, when negative results from the-
ory are being applied, a certain amount of caution
is appropriate in examining the underlying assump-
tions of the theory and the extent to which these are
applicable. As we shall see, in our opinion, none
of the current negative results, though powerful, are
applicable to the empirical situation. We shall ac-
cordingly, at various points, make strong pessimistic
assumptions about the learning environment of the
child, and show that even under these unrealistically
stringent stipulations, the negative results are still
inapplicable. This will make the conclusions we
come to a little sharper. Conversely, if we wanted
to show that the negative results did apply, to be
convincing we would have to make rather optimistic
assumptions about the learning environment.
4 Applying GI to FLA
We now have the delicate task of selecting, or rather
constructing, a formal model by identifying the vari-
ous components we have identified above. We want
to choose the model that is the best representation
of the learning task or tasks that the infant child
must perform. We consider that some of the em-
pirical questions do not yet have clear answers. In
those cases, we shall make the choice that makes the
learning task more difficult. In other cases, we may
not have a clear idea of how to formalise some in-
formation source. We shall start by making a signif-
icant idealisation: we consider language acquisition
as being a single task. Natural languages as tradi-
tionally describe have different levels. At the very
least we have morphology and syntax; one might
also consider inter-sentential or discourse as an ad-
ditional level. We conflate all of these into a single
task: learning a formal language; in the discussion
below, for the sake of concreteness and clarity, we
shall talk in terms of learning syntax.
4.1 The Language
The first question we must answer concerns the lan-
guage itself. A formal language is normally defined
as follows. Given a finite alphabet ?, we define the
set of all strings (the free monoid) over ? as ??.
We want to learn a language L ? ??. The alpha-
bet ? could be a set of phonemes, or characters, or
a set of words, or a set of lexical categories (part
of speech tags). The language could be the set of
well-formed sentences, or the set of words that obey
the phonotactics of the language, and so on. We re-
duce all of the different learning tasks in language
to a single abstract task ? identifying a possibly in-
finite set of strings. This is overly simplistic since
transductions, i.e. mappings from one string to an-
other, are probably also necessary. We are using
here a standard definition of a language where every
string is unambiguously either in or not in the lan-
guage.. This may appear unrealistic ? if the formal
language is meant to represent the set of grammati-
cal sentences, there are well-known methodological
problems with deciding where exactly to draw the
line between grammatical and ungrammatical sen-
tences. An alternative might be to consider accept-
ability rather than grammaticality as the defining
criterion for inclusion in the set. Moreover, there
is a certain amount of noise in the input ? There
are other possibilities. We could for example use a
fuzzy set ? i.e. a function from ?? ? [0, 1] where
each string has a degree of membership between 0
and 1. This would seem to create more problems
than it solves. A more appealing option is to learn
distributions, again functions f from ?? ? [0, 1]
but where
?
s?L f(s) = 1. This is of course the
classic problem of language modelling, and is com-
pelling for two reasons. First, it is empirically well
grounded ? the probability of a string is related to its
frequency of occurrence, and secondly, we can de-
28
duce from the speech recognition capability of hu-
mans that they must have some similar capability.
Both possibilities ? crisp languages, and distri-
butions ? are reasonable. The choice depends on
what one considers the key phenomena to be ex-
plained are ? grammaticality judgments by native
speakers, or natural use and comprehension of the
language. We favour the latter, and accordingly
think that learning distributions is a more accurate
and more difficult choice.
4.2 The class of languages
A common confusion in some discussions of this
topic is between languages and classes of lan-
guages. Learnability is a property of classes of
languages. If there is only one language in the
class of languages to be learned then the learner
can just guess that language and succeed. A class
with two languages is again trivially learnable if
you have an efficient algorithm for testing member-
ship. It is only when the set of languages is expo-
nentially large or infinite, that the problem becomes
non-trivial, from a theoretical point of view. The
class of languages we need is a class of languages
that includes all attested human languages and ad-
ditionally all ?possible? human languages. Natu-
ral languages are thought to fall into the class of
mildly context-sensitive languages, (Vijay-Shanker
and Weir, 1994), so clearly this class is large
enough. It is, however, not necessary that our class
be this large. Indeed it is essential for learnability
that it is not. As we shall see below, even the class
of regular languages contains some subclasses that
are computationally hard to learn. Indeed, we claim
it is reasonable to define our class so it does not con-
tain languages that are clearly not possible human
languages.
4.3 Information sources
Next we must specify the information that our learn-
ing algorithm has access to. Clearly the primary
source of data is the primary linguistic data (PLD),
namely the utterances that occur in the child?s envi-
ronment. These will consist of both child-directed
speech and adult-to-adult speech. These are gen-
erally acceptable sentences that is to say sentences
that are in the language to be learned. These are
called positive samples. One of the most long-
running debates in this field is over whether the
child has access to negative data ? unacceptable sen-
tences that are marked in some way as such. The
consensus (Marcus, 1993) appears to be that they do
not. In middle-class Western families, children are
provided with some sort of feedback about the well-
formedness of their utterances, but this is unreliable
and erratic, not a universal of global child-raising.
Furthermore this appears to have no effect on the
child. Children do also get indirect pragmatic feed-
back if their utterances are incomprehensible. In our
opinion, both of these would be better modelled by
what is called a membership query: the algorithm
may generate a string and be informed whether that
string is in the language or not. However, we feel
that this is too erratic to be considered an essential
part of the process. Another question is whether the
input data is presented as a flat string or annotated
with some sort of structural evidence, which might
be derived from prosodic or semantic information.
Unfortunately there is little agreement on what the
constituent structure should be ? indeed many lin-
guistic theories do not have a level of constituent
structure at all, but just dependency structure.
Semantic information is also claimed as an im-
portant source. The hypothesis is that children can
use lexical semantics, coupled with rich sources of
real-world knowlege to infer the meaning of utter-
ances from the situational context. That would be
an extremely powerful piece of information, but it is
clearly absurd to claim that the meaning of an utter-
ance is uniquely specified by the situational context.
If true, there would be no need for communication
or information transfer at all. Of course the context
puts some constraints on the sentences that will be
uttered, but it is not clear how to incorporate this
fact without being far too generous. In summary it
appears that only positive evidence can be unequiv-
ocally relied upon though this may seem a harsh and
unrealistic environment.
4.4 Presentation
We have now decided that the only evidence avail-
able to the learner will be unadorned positive sam-
ples drawn from the target language. There are var-
ious possibilities for how the samples are selected.
The choice that is most favourable for the learner is
where they are slected by a helpful teacher to make
the learning process as easy as possible (Goldman
and Mathias, 1996). While it is certainly true that
carers speak to small children in sentences of sim-
ple structure (Motherese), this is not true for all of
the data that the child has access to, nor is it uni-
versally valid. Moreover, there are serious techni-
cal problems with formalising this, namely what is
called ?collusion? where the teacher provides exam-
ples that encode the grammar itself, thus trivialising
the learning process. Though attempts have been
made to limit this problem, they are not yet com-
pletely satisfactory. The next alternative is that the
examples are selected randomly from some fixed
29
distribution. This appears to us to be the appropri-
ate choice, subject to some limitations on the dis-
tributions that we discuss below. The final option,
the most difficult for the learner, is where the se-
quence of samples can be selected by an intelli-
gent adversary, in an attempt to make the learner
fail, subject only to the weak requirement that each
string in the language appears at least once. This is
the approach taken in the identification in the limit
paradigm (Gold, 1967), and is clearly too stringent.
The remaining question then regards the distribu-
tion from which the samples are drawn: whether the
learner has to be able to learn for every possible dis-
tribution, or only for distributions from a particular
class, or only for one particular distribution.
4.5 Resources
Beyond the requirement of computability we will
wish to place additional limitations on the computa-
tional resources that the learner can use. Since chil-
dren learn the language in a limited period of time,
which limits both the amount of data they have ac-
cess to and the amount of computation they can use,
it seems appropriate to disallow algorithms that use
unbounded or very large amounts of data or time.
As normal, we shall formalise this by putting poly-
nomial bounds on the sample complexity and com-
putational complexity. Since the individual samples
are of varying length, we need to allow the compu-
tational complexity to depend on the total length of
the sample. A key question is what the parameters
of the sample complexity polynomial should be. We
shall discuss this further below.
4.6 Convergence Criteria
Next we address the issue of reliability: the extent
to which all children acquire language. First, vari-
ability in achievement of particular linguistic mile-
stones is high. There are numerous causes including
deafness, mental retardation, cerebral palsy, specific
language impairment and autism. Generally, autis-
tic children appear neurologically and physically
normal, but about half may never speak. Autism,
on some accounts, has an incidence of about 0.2%.
Therefore we can require learning to happen with
arbitrarily high probability, but requiring it to hap-
pen with probability one is unreasonable. A related
question concerns convergence: the extent to which
children exposed to a linguistic environment end
up with the same language as others. Clearly they
are very close since otherwise communication could
not happen, but there is ample evidence from stud-
ies of variation (Labov, 1975), that there are non-
trivial differences between adults, who have grown
up with near-identical linguistic experiences, about
the interpretation and syntactic acceptability of sim-
ple sentences, quite apart from the wide purely lex-
ical variation that is easily detected. A famous ex-
ample in English is ?Each of the boys didn?t come?.
Moreover, language change requires some chil-
dren to end up with slightly different grammars
from the older generation. At the very most, we
should require that the hypothesis should be close
to the target. The function we use to measure the
?distance? between hypothesis and target depends on
whether we are learnng crisp languages or distribu-
tions. If we are learning distributions then the ob-
vious choice is the Kullback-Leibler divergence ? a
very strict measure. For crisp languages, the prob-
ability of the symmetric difference with respect to
some distribution is natural.
4.7 PAC-learning
These considerations lead us to some variant of the
Probably Approximately Correct (PAC) model of
learning (Valiant, 1984). We require the algorithm
to produce with arbitrarily high probability a good
hypothesis. We formalise this by saying that for any
? > 0 it must produce a good hypothesis with prob-
ability more than 1 ? ?. Next we require a good
hypothesis to be arbitrarily close to the target, so we
have a precision  and we say that for any  > 0, the
hypothesis must be less than  away from the target.
We allow the amount of data it can use to increase as
the confidence and precision get smaller. We define
PAC-learning in the following way: given a finite
alphabet ?, and a class of languages L over ?, an
algorithm PAC-learns the class L, if there is a poly-
nomial q, such that for every confidence ? > 0 and
precision  > 0, for every distribution D over ??,
for every language L in L, whenever the number of
samples exceeds q(1/, 1/?, |?|, |L|), the algorithm
must produce a hypothesis H such that with prob-
ability greater than 1 ? ?, PrD(H?L > ). Here
we use A?B to mean the symmetric difference be-
tween two sets. The polynomial q is called the
sample complexity polynomial. We also limit the
amount of computation to some polynomial in the
total length of the data it has seen. Note first of all
that this is a worst case bound ? we are not requiring
merely that on average it comes close. Additionally
this model is what is called ?distribution-free?. This
means that the algorithm must work for every com-
bination of distribution and language. This is a very
stringent requirement, only mitigated by the fact
that the error is calculated with respect to the same
distribution that the samples are drawn from. Thus,
if there is a subset of ?? with low aggregate proba-
bility under D, the algorithm will not get many sam-
30
ples from this region but will not be penalised very
much for errors in that region. From our point of
view, there are two problems with this framework:
first, we only want to draw positive samples, but the
distributions are over all strings in ??, and include
some that give a zero probability to all strings in
the language concerned. Secondly, this is too pes-
simistic because the distribution has no relation to
the language: intuitively it?s reasonable to expect
the distribution to be derived in some way from the
language, or the structure of a grammar generating
the language. Indeed there is a causal connection
in reality since the sample of the language the child
is exposed to is generated by people who do in fact
know the language.
One alternative that has been suggested is the
PAC learning with simple distributions model intro-
duced by (Denis, 2001). This is based on ideas from
complexity theory where the samples are drawn ac-
cording to a universal distribution defined by the
conditional Kolmogorov complexity. While math-
ematically correct this is inappropriate as a model
of FLA for a number of reasons. First, learnability
is proven only on a single very unusual distribution,
and relies on particular properties of this distribu-
tion, and secondly there are some very large con-
stants in the sample complexity polynomial.
The solution we favour is to define some natu-
ral class of distributions based on a grammar or au-
tomaton generating the language. Given a class of
languages defined by some generative device, there
is normally a natural stochastic variant of the de-
vice which defines a distribution over that language.
Thus regular languages can be defined by a finite-
state automaton, and these can be naturally ex-
tended to Probabilistic finite state automaton. Sim-
ilarly context free languages are normally defined
by context-free grammmars which can be extended
again to to Probabilistic or stochastic CFG. We
therefore propose a slight modification of the PAC-
framework. For every class of languages L, defined
by some formal device define a class of distribu-
tions defined by a stochastic variant of that device.
D. Then for each language L, we select the set of
distributions whose support is equal to the language
and subject to a polynomial bound (q)on the com-
plexity of the distribution in terms of the complex-
ity of the target language: D+L = {D ? D : L =
supp(D)?|D| < q(|L|)}. Samples are drawn from
one of these distributions.
There are two technical problems here: first, this
doesn?t penalise over-generalisation. Since the dis-
tribution is over positive examples, negative exam-
ples have zero weight, so we need some penalty
function over negative examples or alternatively
require the hypothesis to be a subset of the tar-
get. Secondly, this definition is too vague. The
exact way in which you extend the ?crisp? lan-
guage to a stochastic one can have serious con-
sequences. When dealing with regular languages,
for example, though the class of languages defined
by deterministic automata is the same as that de-
fined by non-deterministic languages, the same is
not true for their stochastic variants. Additionally,
one can have exponential blow-ups in the number
of states when determinising automata. Similarly,
with CFGs, (Abney et al, 1999) showed that con-
verting between two parametrisations of stochastic
Context Free languages are equivalent but that there
are blow-ups in both directions. We do not have a
completely satisfactory solution to this problem at
the moment; an alternative is to consider learning
the distributions rather than the languages.
In the case of learning distributions, we have the
same framework, but the samples are drawn accord-
ing to the distribution being learned T , and we re-
quire that the hypothesis H has small divergence
from the target: D(T ||H) < . Since the divergence
is infinite if the hypothesis gives probability zero to
a string in the target, this will have the consequence
that the target must assign a non-zero probability to
every string.
5 Negative Results
Now that we have a fairly clear idea of various ways
of formalising the situation we can consider the ex-
tent to which formal results apply. We start by con-
sidering negative results, which in Machine Learn-
ing come in two types. First, there are information-
theoretic bounds on sample complexity, derived
from the Vapnik-Chervonenkis (VC) dimension of
the space of languages, a measure of the complex-
ity of the set of hypotheses. If we add a parameter
to the sample complexity polynomial that represents
the complexity of the concept to be learned then this
will remove these problems. This can be the size of
a representation of the target which will be a poly-
nomial in the number of states, or simply the num-
ber of non-terminals or states. This is very standard
in most fields of machine learning.
The second problem relates not to the amount
of information but to the computation involved.
Results derived from cryptographic limitations on
computational complexity, can be proved based on
widely held and well supported assumptions that
certain hard cryptographic problems are insoluble.
In what follows we assume that there are no effi-
cient algorithms for common cryptographic prob-
31
lems such as factoring Blum integers, inverting RSA
function, recognizing quadratic residues or learning
noisy parity functions.
There may be algorithms that will learn with rea-
sonable amounts of data but that require unfeasibly
large amounts of computation to find. There are
a number of powerful negative results on learning
in the purely distribution-free situation we consid-
ered and rejected above. (Kearns and Valiant, 1989)
showed that acyclic deterministic automata are not
learnable even with positive and negative exam-
ples. Similarly, (Abe and Warmuth, 1992) showed
a slightly weaker representation dependent result on
learning with a large alphabet for non-deterministic
automata, by showing that there are strings such that
maximising the likelihood of the string is NP-hard.
Again this does not strictly apply to the partially dis-
tribution free situation we have chosen.
However there is one very strong result that ap-
pears to apply. A straightforward consequence of
(Kearns et al, 1994) shows that Acyclic Determinis-
tic Probabilistic FSA over a two letter alphabet can-
not be learned under another cryptographic assump-
tion (the noisy parity assumption). Therefore any
class of languages that includes this comparatively
weak family will not be learnable in out framework.
But this rests upon the assumption that the class
of possible human languages must include some
cryptographically hard functions. It appears that
our formal apparatus does not distinguish between
these cryptographic functions which hav been con-
sciously designed to be hard to learn, and natu-
ral languages which presumably have evolved to be
easy to learn since there is no evolutionary pressure
to make them hard to decrypt ? no intelligent preda-
tors eavesdropping for example. Clearly this is a
flaw in our analysis: we need to find some more
nuanced description for the class of possible human
languages that excludes these hard languages or dis-
tributions.
6 Positive results
There is a positive result that shows a way forward.
A PDFA is ?-distinguishable the distributions gen-
erated from any two states differ by at least ? in
the L?-norm, i.e. there is a string with a differ-
ence in probability of at least ?. (Ron et al, 1995)
showed that ?-distinguishable acyclic PDFAs can
be PAC-learned using the KLD as error function
in time polynomial in n, 1/, 1/?, 1/?, |?|. They
use a variant of a standard state-merging algorithm.
Since these are acyclic the languages they define
are always finite. This additional criterion of distin-
guishability suffices to guarantee learnability. This
work can be extended to cyclic automata (Clark and
Thollard, 2004a; Clark and Thollard, 2004b), and
thus the class of all regular languages, with the ad-
dition of a further parameter which bounds the ex-
pected length of a string generated from any state.
The use of distinguishability seems innocuous; in
syntactic terms it is a consequence of the plausible
condition that for any pair of distinct non-terminals
there is some fairly likely string generated by one
and not the other. Similarly strings of symbols in
natural language tend to have limited length. An
alternate way of formalising this is to define a class
of distinguishable automata, where the distinguisha-
bility of the automata is lower bounded by an in-
verse polynomial in the number of states. This is
formally equivalent, but avoids adding terms to the
sample complexity polynomial. In summary this
would be a valid solution if all human languages
actually lay within the class of regular languages.
Note also the general properties of this kind of al-
gorithm: provably learning an infinite class of lan-
guages with infinite support using only polynomial
amounts of data and computation.
It is worth pointing out that the algorithm does
not need to ?know? the values of the parameters.
Define a new parameter t, and set, for example n =
t, L = t, ? = e?t,  = t?1 and ? = t?1. This gives
a sample complexity polynomial in one parameter
q(t). Given a certain amount of data N we can just
choose the largest value of t such that q(t) < N ,
and set the parameters accordingly.
7 Parametric models
We can now examine the relevance of these re-
sults to the distinction between parametric and non-
parametric languages. Parametric models are those
where the class of languages is parametrised by a
small set of finite-valued (binary) parameters, where
the number of paameters is small compared to the
log2 of the complexity of the languages. Without
this latter constraint the notion is mathematically
vacuous, since, for example, any context free gram-
mar in Chomsky normal form can be parametrised
with N3 + NM + 1 binary parameters where N
is the number of non-terminals and M the num-
ber of terminals. This constraint is also necessary
for parametric models to make testable empirical
predictions both about language universals, devel-
opmental evidence and relationships between the
two (Hyams, 1986). We neglect here the important
issue of lexical learning: we assume, implausibly,
that lexical learning can take place completely be-
fore syntax learning commences. It has in the past
been stated that the finiteness of a language class
32
suffices to guarantee learnability even under a PAC-
learning criterion (Bertolo, 2001). This is, in gen-
eral, false, and arises from neglecting constraints on
the sample complexity and the computational com-
plexities both of learning and of parsing. The neg-
ative result of (Kearns et al, 1994) discussed above
applies also to parametric models. The specific class
of noisy parity functions that they prove are unlearn-
able, are parametrised by a number of binary pa-
rameters in a way very reminiscent of a parametric
model of language. The mere fact that there are a
finite number of parameters does not suffice to guar-
antee learnability, if the resulting class of languages
is exponentially large, or if there is no polynomial
algorithm for parsing. This does not imply that all
parametrised classes of languages will be unlearn-
able, only that having a small number of parame-
ters is neither necessary nor sufficient to guarantee
efficient learnability. If the parameters are shallow
and relate to easily detectable properties of the lan-
guages and are independent then learning can oc-
cur efficiently (Yang, 2002). If they are ?deep? and
inter-related, learning may be impossible. Learn-
ability depends more on simple statistical properties
of the distributions of the samples than on the struc-
ture of the class of languages.
Our conclusion then is ultimately that the theory
of learnability will not be able to resolve disputes
about the nature of first language acquisition: these
problems will have to be answered by empirical re-
search, rather than by mathematical analysis.
Acknowledgements
This work was supported in part by the IST
Programme of the European Community, under
the PASCAL Network of Excellence, IST-2002-
506778, funded in part by the Swiss Federal Office
for Education and Science (OFES). This publication
only reflects the authors? views.
References
N. Abe and M. K. Warmuth. 1992. On the com-
putational complexity of approximating distribu-
tions by probabilistic automata. Machine Learn-
ing, 9:205?260.
N. Abe. 1988. Feasible learnability of formal gram-
mars and the theory of natural language acquisi-
tion. In Proceedings of COLING 1988, pages 1?
6.
S. Abney, D. McAllester, and F. Pereira. 1999. Re-
lating probabilistic grammars and automata. In
Proceedings of ACL ?99.
Stefano Bertolo. 2001. A brief overview of learn-
ability. In Stefano Bertolo, editor, Language Ac-
quisition and Learnability. Cambridge University
Press.
Noam Chomsky. 1986. Knowledge of Language :
Its Nature, Origin, and Use. Praeger.
Alexander Clark and Franck Thollard. 2004a.
PAC-learnability of probabilistic deterministic fi-
nite state automata. Journal of Machine Learning
Research, 5:473?497, May.
Alexander Clark and Franck Thollard. 2004b. Par-
tially distribution-free learning of regular lan-
guages from positive samples. In Proceedings of
COLING, Geneva, Switzerland.
F. Denis. 2001. Learning regular languages from
simple positive examples. Machine Learning,
44(1/2):37?66.
E. M. Gold. 1967. Language indentification in the
limit. Information and control, 10(5):447 ? 474.
S. A. Goldman and H. D. Mathias. 1996. Teach-
ing a smarter learner. Journal of Computer and
System Sciences, 52(2):255?267.
N. Hyams. 1986. Language Acquisition and the
Theory of Parameters. D. Reidel.
M. Kearns and G. Valiant. 1989. Cryptographic
limitations on learning boolean formulae and fi-
nite automata. In 21st annual ACM symposium
on Theory of computation, pages 433?444, New
York. ACM, ACM.
M.J. Kearns, Y. Mansour, D. Ron, R. Rubinfeld,
R.E. Schapire, and L. Sellie. 1994. On the learn-
ability of discrete distributions. In Proc. of the
25th Annual ACM Symposium on Theory of Com-
puting, pages 273?282.
W. Labov. 1975. Empirical foundations of linguis-
tic theory. In R. Austerlitz, editor, The Scope of
American Linguistics. Peter de Ridder Press.
G. F. Marcus. 1993. Negative evidence in language
acquisition. Cognition, 46:53?85.
D. Ron, Y. Singer, and N. Tishby. 1995. On the
learnability and usage of acyclic probabilistic fi-
nite automata. In COLT 1995, pages 31?40,
Santa Cruz CA USA. ACM.
L. Valiant. 1984. A theory of the learnable. Com-
munications of the ACM, 27(11):1134 ? 1142.
K. Vijay-Shanker and David J. Weir. 1994.
The equivalence of four extensions of context-
free grammars. Mathematical Systems Theory,
27(6):511?546.
Kenneth Wexler and Peter W. Culicover. 1980. For-
mal Principles of Language Acquisition. MIT
Press.
C. Yang. 2002. Knowledge and Learning in Natu-
ral Language. Oxford.
Multi-level Dialogue Act Tags
Alexander Clark and Andrei Popescu-Belis
ISSCO / TIM / ETI
University of Geneva
UNI-MAIL, Boulevard du Pont-d?Arve 40
CH-1211 Geneva 4
Switzerland
asc@aclark.demon.co.uk andrei.popescu-belis@issco.unige.ch
Abstract
In this paper we discuss the use of multi-
layered tagsets for dialogue acts, in the con-
text of dialogue understanding for multi-
party meeting recording and retrieval ap-
plications. We discuss some desiderata for
such tagsets and critically examine some
previous proposals. We then define MAL-
TUS, a new tagset based on the ICSI-MR
and Switchboard tagsets, which satisfies
these requirements. We present some ex-
periments using MALTUS which attempt
to compare the merits of integrated versus
multi-level classifiers for the detection of di-
alogue acts.
1 Introduction
The processing of dialogues by computers serves
two main applicative goals: understanding of hu-
man dialogues, for information extraction or sum-
marization, and human-computer dialogue manage-
ment, for language-based or multimodal interfaces.
Whether the computer takes part in a dialogue or
only attempts to monitor a recorded one, it is im-
portant to detect the functions of each of the human
utterances that constitute the dialogue. In addition,
when the computer must generate an utterance as
a reply, this must also bear some of the functions
expected by the hearer in return.
In this article, we focus on dialogue understand-
ing for a dialogue storage and retrieval application,
developed in the (IM)2 project1. The goal of the
application is the multimodal recording of meetings
(such as staff or business meetings), the processing
and storage of the recordings into a database, and the
1(IM)2 stands for Interactive Multimodal Information
Management, a project sponsored by the Swiss Govern-
ment (see http://www.im2.ch).
possibility of querying the dialogue database (Arm-
strong et al, 2003). The query interface and the
processing of the dialogue must therefore meet the
needs of the potential users of the system, who will
attempt to retrieve various types of information from
the meeting recordings. While the result of the query
is in general a chunk of recorded dialogue (prefer-
ably with multimedia rendering), the criteria used to
query the database can vary from trivial (?who at-
tended the meeting??) to very abstract (?what were
the main decisions??). Some form of understanding
of the dialogue structure is thus required for a sig-
nificant proportion of potential queries (more about
requirements in subsection 2.3).
The utterance functions with which we deal in this
paper are dialogue acts. Although dialogue acts (DA)
tags are commonly used as a simple representation
of the function of an utterance in dialogue, there is
little consensus amongst researchers about what set
of DA tags is appropriate in a particular situation.
Our own application domain, meeting recording, is
comparatively open-ended and we do not yet have
a clear understanding of precisely what features will
be most useful. In section 2, we will try to under-
stand the multiplicity of DA tagsets, then we will an-
alyze (section 3) the dialogue data and annotations
on which we work. These considerations prompted
us to abstract a new DA tagset, of which we explain
the merits in section 4. Experiments on the auto-
matic annotation of DAs using the MALTUS tagset
are described in section 5; the results (subsection 5.2)
are followed by a brief discussion.
2 Understanding Dialogue Structure:
Dialogue Acts
2.1 The Concepts behind Dialogue Acts
Dialogues are series of speaker turns. Utterances can
be defined as the atomic subparts of a turn that ac-
complish one or more ?functions? with respect to
speaker interaction. Utterances are in general sig-
nalled by syntactic and/or prosodic means, but the
specificity of their ?function? belongs to pragmat-
ics (Levinson, 1983, ch. 4). Linguists have identified
several dimensions for the role of sentences uttered
in a dialogue. These dimensions are not mutually ex-
clusive, and there are certainly correlations between
some of them (e.g. ?question? as a speech act and
as a member of an adjacency pair).
? Speech acts (Searle, 1969; Vanderveken, 1990):
(1) representatives, such as assertions or con-
clusions; (2) directives, such as requests, ques-
tions, suggestions; (3) commissives, such as
promises, threatenings, offers; (4) expressives
such as thanks, apologies, congratulations; (5)
declarations, such as excommunications, decla-
rations of war, christening, firing from employ-
ment, etc.
? Turn management: backchannel, floor holder,
floor grabber, hold;
? Adjacency pairs: utterances can be the first part
or the second part of exchange pairs such as re-
quest / accept (or refuse); offer / accept; assess
/ (dis)agree; question / answer; etc.
? Overall organization and topics: openings, clos-
ings, topic-changers, topic-continuers, etc.
? Politeness management: face-threatening, face-
saving, neutral;
? Rhetorical role: elaboration, purpose, restate-
ment, etc.
2.2 Dialogue Acts in Computational
Linguistics
There is not much agreement, within the CL/NLP
community, on the definition of a dialogue act. The
term denotes some function of an utterance in a dia-
logue, not reducible to its syntactic or semantic con-
tent. The function is selected, in general, among
a set of possible dialogue acts (a DA tagset) that
depends on the goals of its creator (Traum, 2000).
One of the main inspiration sources for DA tagsets
are speech acts, but the original repertoire (Searle,
1969; Vanderveken, 1990) has been gradually en-
riched with other possible functions. From the nu-
merous DA tagsets (Klein and Soria, 1998), the fol-
lowing are particularly relevant to a general-domain
meeting recording application.
The DA tags in DAMSL (Allen and Core, 1997)
are nearly all independent: the DAMSL guidelines
state that all tags (i.e. all ?functions?) that charac-
terize an utterance should be associated with it. The
DAMSL tags are grouped in four dimensions: com-
municative status, information level, forward-looking
function and backward-looking function. In fact,
several theories are conflated in DAMSL, which was
initially designed as a shared resource with a focus
primarily on task-oriented dialogs (Core and Allen,
1997). There are about 4 million possible combi-
nations of DAMSL tags, which make a huge search
space for automatic annotation.
The application of DAMSL to the Switchboard
data (two-party telephone conversations) lead to
SWBD-DAMSL (Jurafsky et al, 1997), a smaller
tagset than DAMSL. About 200,000 SWBD utter-
ances were first annotated with DAMSL tags: it was
observed that only 220 combinations of tags occurred
(Jurafsky et al, 1998). These 220 labels were then
clustered into 42 tags, such as: statement (36%),
opinion (13%), agree/accept (5%), yes-no-question
(2%). The resulting search space (42 mutually ex-
clusive tags) was well adapted to the initial goals,
viz., the automatic annotation of dialogue acts and
the use of dialogue act specific language models in
speech recognition (Stolcke et al, 2000).
2.3 Requirements for the Definition of a
DA Tagset
In this paper, our goal is to design a new DA tagset
for our application, with the following constraints in
mind (see also the analysis by D. Traum (2000)):
? Relation to one or more existing theories (de-
scriptive, explanatory, etc.).
? Compatibility with the observed functions of ac-
tual utterances in context, in a given domain.
? Empirical validation: reliability of human appli-
cation of the tagset to typical data (high inter-
annotator agreement, at least potentially).
? Possibility of automatic annotation (this re-
quirement is specific to NLP).
? Relevance to the targeted NLP application:
there are numerous possible functions of utter-
ances, but only some of them are really use-
ful to the application. Within our IM2.MDM
project, a study has been conducted on the rel-
evance of dialogue acts (in particular) to typical
user queries on meeting recordings (Lisowska,
2003)2.
? Mapping (at least partially) to existing tagsets,
so that useful insights are preserved, and data
can be reused.
2Many other potential uses of dialogue act informa-
tion have been hypothesized, such as their use to increase
ASR accuracy (Stolcke et al, 2000), or to locate ?hot
spots? in meetings (Wrede and Shriberg, 2003).
3 Available Data and Annotations:
ICSI Meeting Recorder
The volume of available annotated data suffers from
the diversity of DA tagsets (Klein and Soria, 1998).
One of the most significant resources is the Switch-
board corpus mentioned above, but telephone con-
versations have many differences with multi-party
meetings. Apart from the data recently available in
the IM2 project, results reported in this paper make
use of the ICSI Meeting Recording (MR) corpus of
transcribed and annotated dialogues (Morgan et al,
2003; Shriberg et al, 2004)3.
3.1 Overview of ICSI MR Corpus
The ICSI-MR corpus consists of 75 one-hour record-
ings of staff meetings, each involving up to eight
speakers on separate mike channels. Each channel
was manually transcribed and timed, then annotated
with dialogue act and adjacency pair information
(Shriberg et al, 2004). Following a preliminary re-
lease in November 2003 (sound files, transcriptions,
and annotations), the full corpus was released in
February 2004 to IM2 partners.
The dialogue act annotation makes use of the pre-
existing segmentation of each channel into (prosodic)
utterances, sometimes segmented further into func-
tional utterances, each of them bearing a separate di-
alogue act. There are about 112,000 prosodic utter-
ances, and about 7,200 are segmented into two func-
tional utterances (only one is segmented in three).
3.2 Discussion of the ICSI-MR DA Tagset
Each functional utterance from the ICSI-MR corpus
is marked with a dialogue label, composed of one
or more tags from the ICSI-MR tagset (Dhillon et
al., 2004). The tagset, which is well documented,
is based on SWBD-DAMSL, but unlike SWBD-
DAMSL, it allows one utterance to be marked with
multiple tags. Also, the SWBD-DAMSL tagset was
extended, for instance with disruption tags such as
?interrupted?, ?abandoned?, etc. Utterances can also
be marked as ?unintelligible? or ?non-speech?. An
ICSI-MR label is made of a general tag, followed
by zero or more specific tags, followed or not by a
disruption tag:
gen_tag [^spec_tag_1 ... ^spec_tag_n] [.d]
Our formalization of the guidelines using rewriting
rules (Popescu-Belis, 2003) shows that few tags are
mutually exclusive. The number of possible combi-
nations (DA labels) reaches several millions. For in-
stance, even when not considering disruption marks,
3See http://www.icsi.berkeley.edu/Speech/mr/
the labels are a combination of one general tag out
of 11, and one or more specific tags out of 39. If up
to five specific tags are allowed (as observed empir-
ically in the annotated data), there are more than
7,000,000 possible labels; if specific tags are limited
to four, there are about 1,000,000 possible labels.
Some studies acknowledge the difficulties of an-
notating precisely with ICSI-MR, but also the
fine-grained distinctions it allows for, e.g. be-
tween the possible functions of four related dis-
course particles (?yeah?, ?right?, ?okay?, and ?uhhuh?):
agreement/acceptance, acknowledgment, backchan-
nel, floor grabber (Bhagat et al, 2003). Conversely,
inter-annotator agreement on such fine-grained dis-
tinctions (specific tags) is lower than agreement on
major classes, though the kappa-statistic normally
used to measure agreement adjusts to a certain ex-
tent for this. In fact, ICSI-MR also provided a set
of five ?classmaps? that indicate how to group tags
into categories which reduce the number of possible
labels. For instance, the simplest one reduces all
DA labels to only five classes: statement, question,
backchannel, floor holder/grabber, disruption. Our
MALTUS proposal (see 4.1 below) could be viewed
as a classmap too: it preserves however more ICSI-
MR tags than the existing classmaps, and assigns in
addition conditions of mutual exclusiveness.
We also note that, while SWBD-DAMSL was an
attempt to reduce the dimensionality of the DAMSL
tagset (which had a clear theoretical base), the ICSI-
MR tagset alows SWBD tags to be combined again
instead of going back to DAMSL tags. Although
our proposal that we proceed to describe (MALTUS)
remains close to ICSI-MR for reusability reasons, we
are also working on a more principled DA tagset that
departs from ICSI-MR (Popescu-Belis, 2003).
3.3 Some Figures for the ICSI-MR Data
In the process of conversion to MALTUS (see 4.2
below), we validated the ICSI-MR data and made
several observations. Detected incoherent combina-
tions of tags (e.g., two general tags in a label) and
other remarks have also been sent back to ICSI.
We first separate prosodic utterances into func-
tional utterances, so that each utterance has one DA
label (and not two, separated by ?|?), thus obtaining
120,205 utterances. Also at this stage, we split ut-
terances that correspond to reported speech (marked
with ?:?). We then discard the disruption marks to fo-
cus on the DA labels only ? about 12,000 labels out of
ca. 120,000 are disruption marks, or contain one. We
are left with 113,560 utterances with DA labels, with
776 observed types of labels. An important param-
eter is the number of occurring vs. possible labels,
Nb. of Nb. of Nb. of Nb. of
tags in theoretical occurring tokens
label comb. comb.
1 11 11 68,213
2 429 129 37,889
3 8,151 402 5,054
4 100,529 176 2,064
5 904,761 49 326
6 6,333,327 9 14
7 . . . 0 0
Total: 7,347,208 776 113,560
Table 1: Number of possible labels (combinations of
tags): theoretical vs. actual.
Maximal nb. Maximal theoretical
of tags accuracy on ICSI-MR
1 0.601
2 0.934
3 0.979
4 0.997
5 0.999
6 1
Table 2: Maximal accuracy of DA tagging of the
ICSI-MR data that could be reached using a limited
number of tags per label.
which depends a lot on the number of specific tags
in a label, as summarized in table 1. The maximum
observed in the available data is five specific tags in
a label (hence six tags in all).
There is no guarantee that meaningful labels can-
not have more than six tags. However, such labels
are probably very infrequent, and a reasonable op-
tion for automatic tagging is to limit the number
of tag combinations, which is the main goal of the
MALTUS tagset. The maximal accuracies that could
be obtained on the available ICSI-MR data if the
number of tags in a label was limited to 1, 2, etc.
are shown in Table 2. In computing the accuracy we
consider here only perfect matches, but scores could
be higher if partial matches count too. Two or three
tags per label already allow very high accuracy, while
considerable reducing the search space.
4 The MALTUS DA Tagset
4.1 Definition
We defined MALTUS (Multidimensional Abstract
Layered Tagset for Utterances) in order to reduce
the number of possible combinations by assigning
exclusiveness constraints among tags, while remain-
ing compatible with ICSI-MR (Popescu-Belis, 2003).
MALTUS is more abstract than ICSI-MR, but can
be refined if needed. An utterance is either marked
U (undecipherable) or it has a general tag and zero
or more specific tags. It can also bear a disruption
mark. More formally (? means optional):
DA -> (U | (gen_tag (spec_tags)?)) (.D)?
gen_tag -> S | Q | B | H
spec_tags -> (RP | RN | RU)? AT? DO? PO?
The glosses of the tags, generally inspired from
ICSI-MR, are:
? U = undecipherable (unclear, noisy)
? S = statement
? Q = question
? B = backchannel
? H = hold (floor holder, floor grabber, hold)
? RP = positive answer (or positive response)
? RN = negative answer (or negative response)
? RU = other answer (or undecided answer or re-
sponse)
? RI = restated information
? DO = command or other performative (can be
refined into: command, commitment, sugges-
tion, open-option, explicit performative)
? AT = the utterance is related to attention man-
agement (can be refined into: acknowledgement,
rhetorical question backchannel, understanding
check, follow me, tag question)
? PO = the utterance is related to politeness (can
be refined into sympathy, apology, downplayer,
?thanks?, ?you?re welcome?)
? D = the utterance has been interrupted or aban-
doned
4.2 Conversion of ICSI-MR to MALTUS
There are only about 500 possible MALTUS labels,
but observations of the converted ICSI-MR data
show again that the probability distribution is very
skewed. An explicit correspondence table and con-
version procedure were designed to convert ICSI-MR
to MALTUS, so that the considerable ICSI-MR re-
source can be reused.
Correspondences between MALTUS and other
tagsets (Klein and Soria, 1998) were also provided
(Popescu-Belis, 2003). Such ?mappings? are imper-
fect for two reasons: first, they work only in one
direction, from the more specific tagset (ICSI-MR /
SWBD / DAMSL) to the more abstract one (MAL-
TUS). Second, a mapping is incomplete if one does
not state which tags must be mutually exclusive.
For MALTUS too, the idea to use at most three
tags per label in an automatic annotation program
might reduce the search space without decreasing the
accuracy too much. Another idea is to use only the
labels that appear in the data that is, only 50 labels.
An even smaller search space is provided by the 26
MALTUS labels that occur more than 10 times each.
If only these are used for tagging, then only 70 occur-
rences (only 0.061% of the total) would be incorrectly
tagged, on the ICSI-MR reference data. Occurring
labels ordered alphabetically and their frequencies
(when greater than 10) are listed below.
B (15180)
H (12288)
Q (5320)
Q^AT (3137)
Q^AT^RI (69)
Q^DO (239)
Q^RI (60)
Q^RN (19)
S (51304)
S^AT (8280)
S^AT^RI (273)
S^DO (3935)
S^DO^RI (32)
S^DO^RN (38)
S^DO^RP (41)
S^DO^RU (16)
S^PO (791)
S^PO^RI (13)
S^PO^RU (61)
S^RI (765)
S^RI^RN (46)
S^RI^RP (436)
S^RI^RU (18)
S^RN (2219)
S^RP (7612)
S^RU (1298)
Further analysis will tell whether this list should
be enriched with useful labels that are absent from
it. Also, a comparison of MALTUS to the SWBD
set (26 labels vs. 42) should determine whether the
loss in informativeness in MALTUS is compensated
by the gain in search space size and in theoretical
grounding.
5 Automatic Classification
As discussed above, one of the desiderata for a tagset
in this application domain is that the tags can be ap-
plied automatically. A requirement for annotations
that can only be applied manually is clearly unre-
alistic except for meetings of very high importance.
The ICSI-MR corpus on the other hand is concerned
with producing a body of annotated data that can
be used by researchers for a wide range of different
purposes: linguists who are interested in particular
forms of interaction, researchers in acoustics and so
on. It is by no means a criticism of their work that
some of the distinctions that they annotate or at-
tempt to annotate cannot be reliably automated.
Here we report some preliminary experiments on
the automatic annotation of meeting transcripts with
these tagsets. Our focus here is not so much on eval-
uating a classifier for this task but rather evaluating
the tagsets: we are interest in the extent to which
they can be predicted reliably from easily extracted
features of the utterance and its context. Addition-
ally we are interested in the multi-level nature of the
tagsets and exploring the extent to which the internal
structure of the tags allows other options for classi-
fiers. Therefore, our goal in these experiments is not
to build a high performance classifier; rather, it is to
explore the extent to which multi level tagsets can
be predicted by classifying each level separately ?
i.e. by having a set of ?orthogonal? classifiers ? as
opposed to classifying the entire structured object
in a single step using a single multi-class classifier
on a flattened representation. Accordingly there are
a number of areas in which our experimental setup
differs from that which would be appropriate when
performing experiments to evaluate a classifier.
Since in this paper we are not using prosodic or
acoustic information, but just the manual transcrip-
tions, there are two sources of information that can
be used to classify utterances. First, the sequence of
words that constitutes the utterance, and secondly
the surrounding utterances and their classification.
generally in prior research in this field, some form
of sequential inference algorithm has been used to
combine the local decisions about the DA of each ut-
terance into a classification of the whole utterance.
The common way of doing this has been to use a
hidden Markov model to model the sequence and to
use a standard decoding algorithm to find either the
sequence with maximum a posteriori (MAP) likeli-
hood or to select for each utterance the DA with
MAP likelihood. In the work here, we will ignore
this complexity and allow our classifier access to the
gold standard classification of the surrounding utter-
ances. This will make the task substantially easier,
since in a real application, there will be some noise
in the labels.
5.1 Feature selection
There are two sorts of features that we shall use here
? internal lexical features derived from the words in
the utterance, and contextual features derived from
the surrounding utterances. At our current state of
knowledge we have a very good idea about what the
lexically derived features should be, and how they
should be computed ? namely n-grams or gappy n-
grams including positional information. Addition-
ally, there are ways of computing these efficiently.
However, with regard to the contextually derived fea-
tures, our knowledge is much less complete. (Stolcke
et al, 2000) showed that in the Switchboard corpus
there was little dependence beyond the immediately
adjacent utterance, but whether this also applies in
this multi-party domain is unknown. Thus we find
ourselves in a rather asymmetric position with re-
gard to these two information sources. As we are
not here primarily interested in constructing a high
performance classifier, but rather identifying the pre-
dictable elements of the tag, we have resolved this
problem by deliberately selecting a rather limited set
of lexical features, together with a limited set of con-
textual features. Otherwise, we feel that our experi-
ments would be overly biased towards those elements
of the tag that are predictable from the internal lex-
ical evidence.
We used as lexical features the 1000 most frequent
words, together with additional features for these
words occurring at the beggining or end of the ut-
terance. This gives an upper bound of 3000 lexical
features. We experimented with a variety of simple
contextual features.
Preceding same label (SL) the immediately pre-
ceding utterance on the same channel has a par-
ticular DA tag.
Preceding label (PL) a preceding utterance on a
different channel has a particular DA tag. We
consider an utterance to be preceding if it starts
before the start of the current utterance.
Overlapping label (OL) an utterance on another
channel with a particular DA tag overlaps the
current utterance. We anticipate this being use-
ful for identifying backchannels.
Containing label (CL) an utterance on another
channel with a particular DA tag contains the
current channel ? i.e. the start is before the start
of the current utterance and the end is after the
end of the current utterance.
Figure 1 shows an artificial example in a multi-
party dialog with four channels. This illustrates the
features that will be defined for the classification of
the utterance that is shaded. In this example we
will have the following features SL:C1, PL:B1, PL:D1,
CL:D1, OL:A1, OL:B1, OL:B2, OL:D1. We have found
A
B
C
D
A1
B1 B2
C1
D1
Figure 1: Artificial example illustrating contextual
features defined for a particular utterance (shaded).
There are four channels labelled A to D; each box
represents an utterance, and the DA tag is repre-
sented by the characters inside each box.
that the overlapping label feature set does not help
the classifiers here, so we have used the remaining
three contextual feature sets. Note the absence of
contextual features corresponding to labels of utter-
ances that strictly follow the target utterance. We
felt that given the fact that we use the gold standard
tags this would be too powerful.
The data made available to us was preprocessed
in a number of ways. The most significant change
was to split utterances that had been labelled with a
sequence of DA labels (joined with pipes). We sep-
arated the utterances and the labels at the appro-
priate points and realigned. The data was provided
with individual time stamps for each word using a
speech recognizer in forced recognition mode: where
there were errors or mismatches we discarded the
words.
5.2 Results
We use a Maximum Entropy (ME) classifier (Man-
ning and Klein, 2003) which allows an efficient com-
bination of many overlapping features. We selected
5 meetings (6771 utterances after splitting) to use as
our test set and 40 as our training set leaving a fur-
ther five for possible later experiments. As a simple
baseline we use the classifier which just guesses the
most likely class. We first performed some experi-
ments on the original tag sets to see how predictable
they are.
We started by defining a simple six-way classifica-
tion task which classifies disruption forms, and unde-
cipherable forms as well as the four general tags de-
fined above. This is an empirically very well-founded
distinction: the ICSI-MR group have provided some
inter-annotator agreement figures(Carletta et al,
1997) for a very similar task and report a kappa
of 0.79. Our ME classifier scored 77.9% (baseline
54.0%).
We also tested a few simple binary classifications
to see how predictable they are. Utterances are anno-
tated for example with a tag J if they are a joke. As
would be expected, the Joke/Non-Joke classification
produced results not distinguishable from chance.
The performance of the classifiers on separating dis-
rupted utterances from non disrupted forms scored
slightly above chance at 89.9% (against baseline of
87.0%). We suspect that more sophisticated contex-
tual features could allow better performance here.
A more relevant performance criterion for our appli-
cation is the accuracy of classification into the four
general tags. In this case we removed disrupted and
undecipherable utterances, slightly reducing the size
of the test set, and achieved a score of 84.9% (base-
line 64.1%).
With regard to the larger sets of tags, since they
have some internal structure it should accordingly
be possible to identify the different parts separately,
and then combine the results. We have therefore per-
formed some preliminary experiments with classifiers
that classify each level separately. We again removed
the disruption tags since with out current framework
we are unable to predict them accurately. The base-
line for this task is again a classifier that chooses the
most likely tag (S) which gives 41.9% accuracy. Us-
ing a single classifier on this complex task gave an
accuracy of 73.2%.
We then constructed six classifiers as follows
Primary classifier S, H, Q or B
Politeness classifier PO or not PO
Attention classifier AT or not AT
Order classifier DO or not DO
Restatement classifier RI or not RI
Response classifier RP, RN, RU or no response
These were trained separately in the obvious way and
the results combined. This complex classifier gave an
accuracy 70.5%. This mild decrease in performance
is rather surprising ? one would expect the perfor-
mance to increase as the data sets for each distinction
get larger. This can be explained by dependences be-
tween the classifications. There are a number of ways
this could be treated ? for example, one could use a
sequence of classifiers, where each classifier can use
the output of the previous classifier as a feature in
the next. It is also possible that these dependencies
reflect idiosyncracies of the tagging process: tenden-
cies of the annotators for whatever reasons to favour
or avoid certain combinations of tags.
6 Conclusion
We have discussed some issues concerning the design
and use of dialogue act tagsets. It is too early to
draw firm conclusions from this preliminary study.
We can note the obvious point that simplified smaller
tagsets are easier to predict accurately than larger
ones. There appear to be non-trivial dependencies
between the tags for reasons that are not yet clear.
We expect the performance of a final, fully automatic
classifier to be substantially higher than the results
presented here, owing to the use of more powerful
classifiers and, more importantly, larger and richer
feature sets. Finally we note that an important point
of tagset design has not been addressed empirically
here: the question of whether particular distinctions
in the tagset are actually useful in our application.
Future studies will address this point by studying
the queries formulated by potential users of meeting
processing and retrieval systems.
Acknowledgments
We are grateful to the ICSI MR group for shar-
ing with us the data as part of the IM2/ICSI
agreement ? in particular to Barbara Peskin
and Liz Shriberg. This research is part of the
Multimodal Dialogue Management module (see
http://www.issco.unige.ch/projects/im2/mdm)
of the IM2 project.
References
James F. Allen and Mark G. Core. 1997. DAMSL:
Dialog act markup in several layers (draft 2.1).
Technical report, Multiparty Discourse Group,
Discourse Research Initiative, September/October
1997.
Susan Armstrong, Alexander Clark, Giovanni Coray,
Maria Georgescul, Vincenzo Pallotta, Andrei
Popescu-Belis, David Portabella, Martin Rajman,
and Marianne Starlander. 2003. Natural language
queries on natural language data: a database of
meeting dialogues. In NLDB?2003 (8th Inter-
national Conference on Applications of Natural
Language to Information Systems), Burg/Cottbus,
Germany.
Sonali Bhagat, Hannah Carvey, and Elizabeth
Shriberg. 2003. Automatically generated prosodic
cues to lexically ambiguous dialog acts in multi-
party meetings. In ICPhS 2003, Barcelona.
Jean Carletta, Amy Isard, Stephen Isard, Jacque-
line C. Kowtko, Gwyneth Doherty-Sneddon, and
Anne H. Anderson. 1997. The reliability of a di-
alogue structure coding scheme. Computational
Linguistics, 23:13?31.
Mark G. Core and James F. Allen. 1997. Coding
dialogues with the DAMSL annotation scheme. In
David Traum, editor, Working Notes: AAAI Fall
Symposium on Communicative Action in Humans
and Machines, pages 28?35, Menlo Park, CA.
American Association for Artificial Intelligence.
Rajdip Dhillon, Sonali Bhagat, Hannah Carvey,
and Elizabeth Shriberg. 2004. Meeting recorder
project: Dialog act labeling guide. Technical
Report TR-04-002, ICSI (International Computer
Science Institute), Berkeley, CA.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Bi-
asca. 1997. Switchboard SWBD-DAMSL shallow-
discourse-function annotation (coders manual,
draft 13). Technical Report 97-02, University of
Colorado, Institute of Cognitive Science.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox,
and Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In ACL/COLING-98
Workshop on Discourse Relations and Discourse
Markers, pages 114?120.
Marion Klein and Claudia Soria. 1998. Dialogue
acts. In Marion Klein, Niels Ole Bernsen, Sarah
Davies, Laila Dybkjaer, Juanma Garrido, Hen-
rik Kasch, Andreas Mengel, Vito Pirrelli, Mas-
simo Poesio, Silvia Quazza, and Claudia Soria,
editors, MATE Deliverable 1.1: Supported Coding
Schemes, MATE (Multilevel Annotation, Tools
Engineering) European Project LE4-8370.
Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press, Cambridge, UK.
Agnes Lisowska. 2003. Multimodal interface design
for the multimodal meeting domain: Preliminary
indications from a query analysis study. Technical
report, IM2.MDM, 11/2003.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In Tutorial at HLT-NAACL
2003 and ACL 2003. ACL, Edmonton, Canada.
Nelson Morgan, Don Baron, Sonali Bhagat, Hannah
Carvey, Rajdip Dhillon, Jane A. Edwards, David
Gelbart, Adam Janin, Ashley Krupski, Barbara
Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas
Stolcke, and Chuck Wooters. 2003. Meetings
about meetings: research at ICSI on speech in
multiparty conversations. In ICASSP 2003 (In-
ternational Conference on Acoustics, Speech, and
Signal Processing), Hong Kong, China.
Andrei Popescu-Belis. 2003. Dialogue act tagsets for
meeting understanding: an abstraction based on
the DAMSL, Switchboard and ICSI-MR tagsets.
Technical report, IM2.MDM, v1.1, 09/2003.
John R. Searle. 1969. Speech Acts. Cambridge Uni-
versity Press, Cambridge, UK.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat,
Jeremy Ang, and Hannah Carvey. 2004. The ICSI
meeting recorder dialog act (MRDA) corpus. In
Proceedings of SIGDIAL ?04 (5th SIGdial Work-
shop on Discourse and Dialog), Cambridge, MA.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-
abeth Shriberg, Rebecca Bates, Daniel Jurafsky,
Paul Taylor, Rachel Martin, Marie Meteer, and
Carol Van Ess-Dykema. 2000. Dialogue act mod-
eling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3):339?371.
David R. Traum. 2000. 20 questions for dialogue act
taxonomies. Journal of Semantics, 17(1):7?30.
Daniel Vanderveken. 1990. Meaning and speech acts.
Cambridge University Press, Cambridge, UK.
Britta Wrede and Elizabeth Shriberg. 2003. The
relationship between dialogue acts and hot spots
in meetings. In IEEE Speech Recognition and Un-
derstanding Workshop, St. Thomas, U.S. Virgin
Islands.
A Comparative Study of Mixture Models for Automatic Topic Segmentation
of Multiparty Dialogues
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
susan.armstrong@issco.unige.ch
Abstract
In this article we address the task of auto-
matic text structuring into linear and non-
overlapping thematic episodes at a coarse
level of granularity. In particular, we
deal with topic segmentation on multi-party
meeting recording transcripts, which pose
specific challenges for topic segmentation
models. We present a comparative study
of two probabilistic mixture models. Based
on lexical features, we use these models in
parallel in order to generate a low dimen-
sional input representation for topic segmen-
tation. Our experiments demonstrate that in
this manner important information is cap-
tured from the data through less features.
1 Introduction
Some of the earliest research related to the prob-
lem of text segmentation into thematic episodes used
the word distribution as an intrinsic feature of texts
(Morris and Hirst, 1991). The studies of (Reynar,
1994; Hearst, 1997; Choi, 2000) continued in this
vein. While having quite different emphasis at dif-
ferent levels of detail (basically from the point of
view of the employed term weighting and/or the
adopted inter-block similarity measure), these stud-
ies analyzed the word distribution inside the texts
through the instrumentality of merely one feature,
i.e. the one-dimensional inter-block similarity.
More recent work use techniques from graph the-
ory (Malioutov and Barzilay, 2006) and machine
learning (Galley et al, 2003; Georgescul et al,
2006; Purver et al, 2006) in order to find patterns
in vocabulary use.
We investigate new approaches for topic segmen-
tation on corpora containing multi-party dialogues,
which currently represents a relatively less explored
domain. Compared to other types of audio content
(e.g. broadcast news recordings), meeting record-
ings are less structured, often exhibiting a high de-
gree of participants spontaneity and there may be
overlap in finishing one topic while introducing an-
other. Moreover while ending the discussion on a
certain topic, there can be numerous new attempts
to introduce a new topic before it becomes the fo-
cus of the dialogue. Therefore, the task of automatic
topic segmentation of meeting recordings is more
difficult and requires a more refined analysis. (Gal-
ley et al, 2003; Georgescul et al, 2007) dealt with
the problem of topic segmentation of multiparty di-
alogues by combining various features based on cue
phrases, syntactic and prosodic information. In this
article, our investigation is based on using merely
lexical features.
We study mixture models in order to group the
words co-occurring in texts into a small number
of semantic concepts in an automatic unsupervised
way. The intuition behind these models is that a
text document has an underlying structure of ?la-
tent? topics, which is hidden. In order to reveal
these latent topics, the basic assumption made is that
words related to a semantic concept tend to occur in
the proximity of each other. The notion of proxim-
ity between semantically related words can vary for
various tasks. For instance, bigrams can be consid-
ered to capture correlation between words at a very
925
short distance. At the other extreme, in the domain
of document classification, it is often assumed that
the whole document is concerned with one specific
topic and in this sense all words in a document are
considered to be semantically related. We consider
for our application that words occurring in the same
thematic episode are semantically related.
In the following, the major issues we will discuss
include the formulations of two probabilistic mix-
ture approaches, their methodology, aspects of their
implementation and the results obtained when ap-
plied in the topic segmentation context. Section 2
presents our approach on using probabilistic mix-
ture models for topic segmentation and shows com-
parisons between these techniques. In Section 3 we
discuss our empirical evaluation of these models for
topic segmentation. Finally, some conclusions are
drawn in Section 4.
2 Probabilistic Mixture Models
The probabilistic latent models described in the fol-
lowing exploit hierarchical Bayesian frameworks.
Based on prior distributions of word rate variability
acquired from a training corpus, we will compute a
density function to further analyze the text content in
order to perform topic segmentation at a coarse level
of granularity. In this model, we will be working
with ?blocks? of text which consist of a fixed num-
ber of consecutive utterances.
In the following two subsections, we use the fol-
lowing notation:
? We consider a text corpus B = {b1, b2, ..., bM}
containing M blocks of text with words from
a vocabulary W = {w1, w2, ..., wN}. M is
a constant scalar representing the number of
blocks of text. N is a constant scalar represent-
ing the number of terms in vocabulary W .
? We pre-process the data by eliminating con-
tent free words such as articles, prepositions
and auxiliary verbs. Then, we proceed by lem-
matizing the remaining words and by adopt-
ing a bag-of-words representation. Next,
we summarize the data in a matrix F =
(f(bi, wi,j))(i,j)?M?N , where f(bi, wi,j) de-
notes the log.entropy weighted frequency of
word wi,j in block bi.
? Each occurrence of a word in a block of
text is considered as representing an ob-
servation (wm,n, bm), i.e. a realization from
an underlying sequence of random variables
(Wm,n, Bm)
1?m?M
1?n?N . wm,n denotes the term
indicator for the n-th word in the m-th block
of text.
? Each pair (wm,n, bm) is associated with a dis-
crete hidden random variable Zm,n over some
finite set Z ={z1, z2, ..., zK}. K is a constant
scalar representing the number of mixture com-
ponents to generate.
? We denote by P (zm,n = zk) or simply by
P (zk) the probability that the k-th topic has
been sampled for the n-th word in the m-th
block of text.
2.1 Aspect Model for Dyadic Data (AMDD)
In this section we describe how we apply latent mod-
eling for dyadic data (Hofmann, 2001) to text repre-
sentation for topic segmentation.
2.1.1 Model Setting
 
 
 
 
 
n,mw  
n,mz  
mb  
M 
block  plate 
n,mw  
n,mz  
mb  
block  plate 
M 
word  plate 
N 
word  plate 
N 
1) Asymmetric PLSA parameterization 2) Symmetric PLSA parameterization 
Figure 1: Graphical model representation of the as-
pect model.
We express the joint or conditional probability
of words and blocks of text, by assuming that the
choice of a word during the generation of a block
of text is independent of the block itself, given some
(unobserved) hidden variable, also called latent vari-
able or aspect.
The graphical representation of the AMDD data
generation process is illustrated in Figure 1 by using
926
the plate notation. That is, the ovals (i.e. the nodes
of the graph) represent probabilistic variables. The
double ovals around the variables wm,n and bm de-
note observed variables. zm,n is the mixture indi-
cator, the hidden variable, that chooses the topic for
the n-th word in the m-th block of text. Arrows in-
dicate conditional dependencies between variables.
For instance, the wm,n variable in the word space
and the bm variable in the block space have no di-
rect dependencies, i.e. it is assumed that the choice
of words in the generation of a block of text is in-
dependent of the block given a hidden variable. The
boxes represent ?plates?, i.e. replicates of sampling
steps with the variable in the lower left corner re-
ferring to the number of samples. For instance, the
?word plate? in Figure 1 illustrates N independently
and identically distributed repeated trials of the ran-
dom variable wm,n.
According to the topology of the asymmetric
AMDD Bayesian network from Figure 1, we can
specify the joint distribution of a word wm,n, a latent
topic zk and a block of text bm: P (wm,n, zk, bm) =
P (bm) ? P (zk|bm) ? P (wm,n|zk). The joint distribu-
tion of a block of text bm and a word wm,n is thus:
P (bm, wm,n) =
K?
k=1
P (wm,n, zk, bm) = P (bm)
?
?K
k=1 P (zk|bm)? ?? ?
mixing proportions
? P (wm,n|zk)
? ?? ?
mixture components
(1)
Equation 1 describes a special case of a finite mix-
ture model, i.e. it uses a convex combination of a set
of component distributions to model the observed
data. That is, each word in a block of text is seen
as a sample from a mixture model, where mixture
components are multinomials P (wm,n|zk) and the
mixing proportions are P (zk|bm).
2.1.2 Inferring and Employing the AMDD
Model
The Expectation-Maximization (EM) algorithm is
the most popular method to estimate the parameters
for mixture models to fit a training corpus. The
EM algorithm for AMDD is based on iteratively
maximizing the log-likelihood function: LPLSA =?M
m=1
?N
n=1f(bm, wm,n) ? logP (wm,n, bm). How-
ever, the EM algorithm for AMDD is prone to over-
fitting since the number of parameters to be esti-
mated grows linearly with the number of blocks of
text. In order to avoid this problem, we employed
the tempered version of the EM algorithm that has
been proposed by Hofmann (2001).
We use the density estimation method in AMDD
to reduce the dimension of the blocks-by-words
space. Thus, instead of using the words as ba-
sic units for each block of text representation, we
employ a ?topic? basis, assuming that a few top-
ics will capture more information than the entire
huge amount of words in the vocabulary. Thus,
the m-th block of text is represented by the vector
(P (z1|bm), P (z2|bm), ..., P (zk|bm)). Then, we use
these posterior probabilities as a threshold to iden-
tify the boundaries of thematic episodes via sup-
port vector classification (Georgescul et al, 2006).
That is, we consider the topic segmentation task as a
binary-classification problem, where each utterance
should be classified as marking the presence or the
absence of a topic shift in the dialogue.
2.2 Latent Dirichlet Allocation (LDA)
Latent Dirichlet Allocation (Blei et al, 2003) can
be seen as an extension of AMDD by defining a
probabilistic mixture model that includes Dirichlet-
distributed priors over the masses of the multinomi-
als P (w|z) and P (z|b).
2.2.1 Model Setting
In order to describe the formal setting of LDA in
our context, we use the following notation in addi-
tion to those given at the beginning of Section 2:
? ~?m is a parameter notation for P (z|b = bm),
the topic mixture proportion for the m-th block
of text;
? ~? is a hyperparameter (a vector of dimension
K) on the mixing proportions ~?m;
? ? =
{
~?m
}M
m=1
is a matrix (of dimension
M ? K), composed by placing the vectors
~?1, ~?2, ..., ~?M as column components;
? ~?k is a parameter notation for P (w|zk), the
mixture component for topic k;
? ~? is a hyperparameter (a vector of dimension
N ) on the mixture components ~?k ;
927
? ? = {~?k}
K
k=1 is a matrix of dimension
K ? N composed by placing the vectors
~?1, ~?2, ..., ~?K as column components;
? Nm denotes the length of the m-th block of text
and is modeled with a Poisson distribution with
constant parameter ?;
 
 
 
 
 
word plate 
?
?  
??  
topic plate 
K 
n,mw  ?k?  
n,mz  
Nm 
m
??  
M 
block  plate 
Figure 2: Graphical model representation of latent
Dirichlet alocation.
LDA generates a stream of observable words
wm,n partitioned into blocks of text ~bm as shown
by the graphical model in Figure 2. The Bayesian
network can be interpreted as follows: the variables
?, ? and z are the three sets of latent variables that
we would like to infer. The plate surrounding ~?k il-
lustrates the repeated sampling of word distributions
for each topic zk until K topics have been generated.
The plate surrounding ~?m illustrates the sampling of
a distribution over topics for each block b for a to-
tal of M blocks of text. The inner plate over zm,n
and wm,n illustrates the repeated sampling of topics
and words until Nm words have been generated for
a block~bm.
Each block of text is first generated by drawing
a topic proportion ~?m, i.e. by picking a distribution
over topics from a Dirichlet distribution. For each
word wm,n from a block of text~bm, a topic indicator
k is sampled for zm,n according to the block-specific
mixture proportion ~?m. That is, ~?m determines
P (zm,n). The topic probabilities ~?k are also sam-
pled from a Dirichlet distribution. The words in each
block of text are then generated by using the corre-
sponding topic-specific term distribution ~?zm,n .
Given the graphical representation of LDA illus-
trated in Figure 2, we can write the joint distribution
of a word wm,n and a topic zk as:
P (wm,n, zk|~?m,?) = P (zk|~?m) ? P (wm,n|~?k).
Summing over k, we obtain the marginal distribu-
tion:
P (wm,n|~?m,?) =
?K
k=1
?
?
? P (zk|~?m)
? ?? ?
mixture proportion
? P (wm,n|~?k)
? ?? ?
mixture component
?
?
?.
Hence, similarly to AMDD (see Equation 1), the
LDA model assumes that a word wm,n is generated
from a random mixture over topics. Topic proba-
bilities are conditioned on the block of text a word
belongs to. Moreover LDA leaves flexibility to
assign a different topic to every observed word and
a different proportion of topics for every block of
text.
The joint distribution of a block of text ~bm
and the latent variables of the model ~zm, ~?m,
?, given the hyperparameters ~?, ~? is further
specified by: P (~bm, ~zm, ~?m,?|~?, ~?) =
topic plate
? ?? ?
P (?|~?) ?
P (~?m|~?) ?
Nm?
n=1
word plate
? ?? ?
P (zm,n|~?m) ? P (wm,n|~?zm,n)
? ?? ?
block plate
.
Therefore, the likelihood of a block~bm is derived
as the marginal distribution obtained by summing
over the zm,n and integrating out the distributions
~?m and ?.
2.2.2 Inferring and Employing the LDA Model
Since the integral involved in computing the like-
lihood of a block ~bm is computationally intractable,
several methods for approximating this posterior
have been proposed, including variational expecta-
tion maximization (Blei et al, 2003) and Markov
chain Monte Carlo methods (Griffiths and Steyvers,
2004).
We follow an approach based on Gibbs sampling
as proposed in (Griffiths and Steyvers, 2004). As
the convergence criteria for the Markov chain, we
928
check how well the parameters cluster semantically
related blocks of text in a training corpus and then
we use these values as estimates for comparable set-
tings.
The LDA model provides a soft clustering of the
blocks of text, by associating them to topics. We
exploit this clustering information, by using the dis-
tribution of topics over blocks of text to further
measure the inter-blocks similarity. As in Section
2.1.2, the last step of our system consists in em-
ploying binary support vector classification to iden-
tify the boundaries of thematic episodes in the text.
That is, we consider as input features for support
vector learning the component values of the vector
(?m,z1 , ?m,z2 , ..., ?m,zk).
3 Experiments
In order to evaluate the performance of AMDD and
LDA for our task of topic segmentation, in our ex-
periments we used the transcripts of ICSI-MR cor-
pus (Janin et al, 2004), which consists of 75 meet-
ing recordings. A subset of 25 meetings, which are
transcribed by humans and annotated with thematic
boundaries (Galley et al, 2003), has been kept for
testing purposes and support vector machine train-
ing. The transcripts of the remaining 50 meetings
have been used for the unsupervised inference of
our latent models. The fitting phase of the mix-
ture models rely on the same data set that have been
pre-processed by tokenization, elimination of stop-
words and lemmatization.
Once the models? parameters are learned, the in-
put data representation is projected into the lower
dimension latent semantic space. The evaluation
phase consists in checking the performance of each
model for predicting thematic boundaries. That is,
we check the performance of the models for predict-
ing thematic boundaries on the same test set. The
size of a block of text during the testing phase has
been set to one, i.e. each utterance has been consid-
ered as a block of text.
Figure 3 compares the performance obtained for
various k values, i.e. various dimensions of the latent
semantic space, or equivalently different numbers of
latent topics. We have chosen k={50, ...400} using
incremental steps of 50.
The performance of each latent model is mea-
0.000
0
0.100
0
0.200
0
0.300
0
0.400
0
0.500
0
0.600
0
0.700
0
0.800
0
0.900
0
50
100
150
200
250
300
350
400
Laten
t spa
ce di
men
sion
Accuracy
PLSA LDA
Figure 3: Results of applying the mixture models for
topic segmentation.
sured by the accuracy Acc = 1 ? Pk, where Pk
denotes the error measure proposed by (Beeferman
et al, 1999). Note that the Pk error allows for a
slight variation in where the hypothesized thematic
boundaries are placed. That is, wrong hypothesized
thematic boundaries occurring in the proximity of
a reference boundary (i.e. in a fixed-size interval of
text) are tolerated. As proposed by (Beeferman et
al., 1999), we set up the size of this interval to half
of the average number of words per segment in the
gold standard segmentation.
As we observe from Figure 3, LDA and AMDD
achieved rather comparable thematic segmenta-
tion accuracy. While LDA steadily outperformed
AMDD, the results do not show a notable advan-
tage of LDA over AMDD. In contrast, AMDD has
better performances for less dimensionality reduc-
tion. That is, the LDA performance curve goes down
when the number of latent topics exceeds over 300.
LDA LCSeg SVMs
Pk error rate 21% 32 % 22%
Table 1: Comparative performance results.
In Table 1, we provide the best results obtained
on ICSI data via LDA modeling. We also reproduce
the results reported on in the literature by (Galley
et al, 2003) and (Georgescul et al, 2006), when
the evaluation of their systems was also done on
ICSI data. The LCSeg system proposed by (Gal-
ley et al, 2003) is based on exploiting merely lex-
ical features. Improved performance results have
929
been obtained by (Galley et al, 2003) when extra
non-lexical features have been adopted in a decision
tree classifier. The system proposed by (Georges-
cul et al, 2006) is based on support vector machines
(SVMs) and is labeled in the table as SVMs. We
observe from the table that our approach based on
combining LDA modeling with SVM classification
outperforms LCSeg and performs comparably to the
system of Georgescul et al (2006). Thus, our exper-
iments show that the LDA word density estimation
approach does capture important information from
the data through 90% less features than a bag-of-
words representation.
4 Conclusions
With the goal of performing linear topic segmen-
tation by exploiting word distributions in the input
text, the focus of this article was on both comparing
theoretical aspects and experimental results of two
probabilistic mixture models. The algorithms are
applied to a meeting transcription data set and are
found to provide an appropriate method for reduc-
ing the size of the data representation, by perform-
ing comparably to previous state-of-the-art methods
for topic segmentation.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34:177?210. Special Issue on Natural
Language Learning.
David M. Blei, Andrew Y. Ng, and Michael Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, pages 993?1022.
Freddy Choi. 2000. Advances in Domain Indepen-
dent Linear Text Segmentation. In Proceedings of the
1st Conference of the North American Chapter of the
Association for Computational Linguistics (NAACL),
Seattle, USA.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multi-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pages 562?569,
Sapporo, Japan.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. Word Distributions for Thematic Seg-
mentation in a Support Vector Machine Approach. In
Proceedings of the 10th Conference on Computational
Natural Language Learning (CoNLL), pages 101?108,
New York City, USA.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2007. Exploiting Structural Meeting-Specific
Features for Topic Segmentation. In Actes de la
14e`me Confe?rence sur le Traitement Automatique des
Langues Naturelles (TALN), pages 15?24, Toulouse,
France.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing Scientific Topics. In Proceedings of the National
Academy of Sciences, volume 101, pages 5228?5235.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Thomas Hofmann. 2001. Unsupervised Learning by
Probabilistic Latent Semantic Analysis. Machine
Learning, 42:177?196.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004.
The ICSI Meeting Project: Resources and Research.
In Proceedings of the International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
Meeting Recognition Workshop, Montreal, Quebec,
Canada.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics (COL-
ING/ACL), pages 25?32, Sydney, Australia.
Jane Morris and Graeme Hirst. 1991. Lexical Cohe-
sion Computed by Thesaural Relations as an Indicator
of the Structure of Text. Computational Linguistics,
17(1):21?48.
Matthew Purver, Konrad P. Ko?rding, Thomas L. Grif-
fiths, and Joshua B. Tenenbaum. 2006. Unsupervised
Topic Modelling for Multi-Party Spoken Discourse.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING/ACL), pages 17?24, Sydney, Australia.
Jeffrey Reynar. 1994. An Automatic Method of Finding
Topic Boundaries. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 331?333, Las Cruces, New Mexico,
USA.
930
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 144?151,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Analysis of Quantitative Aspects in the Evaluation of Thematic
Segmentation Algorithms
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX, UK
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
susan.armstrong@issco.unige.ch
Abstract
We consider here the task of linear the-
matic segmentation of text documents, by
using features based on word distributions
in the text. For this task, a typical and of-
ten implicit assumption in previous stud-
ies is that a document has just one topic
and therefore many algorithms have been
tested and have shown encouraging results
on artificial data sets, generated by putting
together parts of different documents. We
show that evaluation on synthetic data is
potentially misleading and fails to give an
accurate evaluation of the performance on
real data. Moreover, we provide a criti-
cal review of existing evaluation metrics in
the literature and we propose an improved
evaluation metric.
1 Introduction
The goal of thematic segmentation is to iden-
tify boundaries of topically coherent segments
in text documents. Giving a rigorous definition
of the notion of topic is difficult, but the task
of discourse/dialogue segmentation into thematic
episodes is usually described by invoking an ?in-
tuitive notion of topic? (Brown and Yule, 1998).
Thematic segmentation also relates to several no-
tions such as speaker?s intention, topic flow and
cohesion.
Since it is elusive what mental representations
humans use in order to distinguish a coherent
text, different surface markers (Hirschberg and
Nakatani, 1996; Passonneau and Litman, 1997)
and external knowledge sources (Kozima and Fu-
rugori, 1994) have been exploited for the purpose
of automatic thematic segmentation. Halliday and
Hasan (1976) claim that the text meaning is re-
alised through certain language resources and they
refer to these resources by the term of cohesion.
The major classes of such text-forming resources
identified in (Halliday and Hasan, 1976) are: sub-
stitution, ellipsis, conjunction, reiteration and col-
location. In this paper, we examine one form of
lexical cohesion, namely lexical reiteration.
Following some of the most prominent dis-
course theories in literature (Grosz and Sidner,
1986; Marcu, 2000), a hierarchical representation
of the thematic episodes can be proposed. The
basis for this is the idea that topics can be re-
cursively divided into subtopics. Real texts ex-
hibit a more intricate structure, including ?seman-
tic returns? by which a topic is suspended at one
point and resumed later in the discourse. However,
we focus here on a reduced segmentation prob-
lem, which involves identifying non-overlapping
and non-hierarchical segments at a coarse level of
granularity.
Thematic segmentation is a valuable initial
tool in information retrieval and natural language
processing. For instance, in information ac-
cess systems, smaller and coherent passage re-
trieval is more convenient to the user than whole-
document retrieval and thematic segmentation has
been shown to improve the passage-retrieval per-
formance (Hearst and Plaunt, 1993). In cases such
as collections of transcripts there are no headers
or paragraph markers. Therefore a clear separa-
tion of the text into thematic episodes can be used
together with highlighted keywords as a kind of
?quick read guide? to help users to quickly navi-
gate through and understand the text. Moreover
automatic thematic segmentation has been shown
to play an important role in automatic summariza-
tion (Mani, 2001), anaphora resolution and dis-
144
course/dialogue understanding.
In this paper, we concern ourselves with the task
of linear thematic segmentation and are interested
in finding out whether different segmentation sys-
tems can perform well on artificial and real data
sets without specific parameter tuning. In addi-
tion, we will refer to the implications of the choice
of a particular error metric for evaluation results.
This paper is organized as follows. Section 2
and Section 3 describe various systems and, re-
spectively, different input data selected for our
evaluation. Section 4 presents several existing
evaluation metrics and their weaknesses, as well
as a new evaluation metric that we propose. Sec-
tion 5 presents our experimental set-up and shows
comparisons between the performance of different
systems. Finally, some conclusions are drawn in
Section 6.
2 Comparison of Systems
Combinations of different features (derived for ex-
ample from linguistic, prosodic information) have
been explored in previous studies like (Galley et
al., 2003) and (Kauchak and Chen, 2005). In
this paper, we selected for comparison three sys-
tems based merely on the lexical reiteration fea-
ture: TextTiling (Hearst, 1997), C99 (Choi, 2000)
and TextSeg (Utiyama and Isahara, 2001). In the
following, we briefly review these approaches.
2.1 TextTiling Algorithm
The TextTiling algorithm was initially developed
by Hearst (1997) for segmentation of exposi-
tory texts into multi-paragraph thematic episodes
having a linear, non-overlapping structure (as re-
flected by the name of the algorithm). TextTiling
is widely used as a de-facto standard in the eval-
uation of alternative segmentation systems, e.g.
(Reynar, 1998; Ferret, 2002; Galley et al, 2003).
The algorithm can briefly be described by the fol-
lowing steps.
Step 1 includes stop-word removal, lemmatiza-
tion and division of the text into ?token-sequences?
(i.e. text blocks having a fixed number of words).
Step 2 determines a score for each gap between
two consecutive token-sequences, by computing
the cosine similarity (Manning and Schu?tze, 1999)
between the two vectors representing the frequen-
cies of the words in the two blocks.
Step 3 computes a ?depth score? for each token-
sequence gap, based on the local minima of the
score computed in step 2.
Step 4 consists in smoothing the scores.
Step 5 chooses from any potential boundaries
those that have the scores smaller than a certain
?cutoff function?, based on the average and stan-
dard deviation of score distribution.
2.2 C99 Algorithm
The C99 algorithm (Choi, 2000) makes a linear
segmentation based on a divisive clustering strat-
egy and the cosine similarity measure between any
two minimal units. More exactly, the algorithm
consists of the following steps.
Step 1: after the division of the text into min-
imal units (in our experiments, the minimal unit
is an utterance1), stop words are removed and a
stemmer is applied.
The second step consists of constructing a sim-
ilarity matrix Sm?m, where m is the number of
utterances and an element sij of the matrix corre-
sponds to the cosine similarity between the vectors
representing the frequencies of the words in the i-
th utterance and the j-th utterance.
Step 3: a ?rank matrix? Rm?m is computed, by
determining for each pair of utterances, the num-
ber of neighbors in Sm?m with a lower similarity
value.
In the final step, the location of thematic bound-
aries is determined by a divisive top-down cluster-
ing procedure. The criterion for division of the
current segment B into b1, ...bm subsegments is
based on the maximisation of a ?density? D, com-
puted for each potential repartition of boundaries
as
D =
?m
k=1 sumk?m
k=1 areak
,
where sumk and areak refers to the sum of rank
and area of the k-th segment in B, respectively.
2.3 TextSeg Algorithm
The TextSeg algorithm (Utiyama and Isahara,
2001) implements a probabilistic approach to de-
termine the most likely segmentation, as briefly
described below.
The segmentation task is modeled as a problem
of finding the minimum cost C(S) of a segmenta-
tion S. The segmentation cost is defined as:
C(S) ? ?logPr(W|S)Pr(S),
1Occasionally within this document we employ the term
utterance to denote either a sentence or an utterance in its
proper sense.
145
where W = w1w2...wn represents the text con-
sisting of n words (after applying stop-words re-
moval and stemming) and S = S1S2...Sm is a po-
tential segmentation of W in m segments. The
probability Pr(W|S) is defined using Laplace
law, while the definition of the probability Pr(S)
is chosen in a manner inspired by information the-
ory.
A directed graph G is defined such that a path
in G corresponds to a possible segmentation of
W . Therefore, the thematic segmentation pro-
posed by the system is obtained by applying a dy-
namic programming algorithm for determining the
minimum cost path in G.
3 Input Data
When evaluating a thematic segmentation system
for an application, human annotators should pro-
vide the gold standard. The problem is that the
procedure of building such a reference corpus is
expensive. That is, the typical setting involves an
experiment with several human subjects, who are
asked to mark thematic segment boundaries based
on specific guidelines and their intuition. The
inter-annotator agreement provides the reference
segmentation. This expense can be avoided by
constructing a synthetic reference corpus by con-
catenation of segments from different documents.
Therefore, the use of artificial data for evaluation
is a general trend in many studies, e.g. (Ferret,
2002; Choi, 2000; Utiyama and Isahara, 2001).
In our experiment, we used artificial and real
data, i.e. the algorithms have been tested on the
following data sets containing English texts.
3.1 Artificially Generated Data
Choi (2000) designed an artificial dataset, built by
concatenating short pieces of texts that have been
extracted from the Brown corpus. Any test sample
from this dataset consists of ten segments. Each
segment contains the first n sentences (where 3 ?
n ? 11) of a randomly selected document from
the Brown corpus. From this dataset, we randomly
chose for our evaluation 100 test samples, where
the length of a segment varied between 3 and 11
sentences.
3.2 TDT Data
One of the commonly used data sets for topic seg-
mentation emerged from the Topic Detection and
Tracking (TDT) project, which includes the task
of story segmentation, i.e. the task of segmenting
a stream of news data into topically cohesive sto-
ries. As part of the TDT initiative several datasets
of news stories have been created. In our evalua-
tion, we used a subset of 28 documents randomly
selected from the TDT Phase 2 (TDT2) collection,
where a document contains an average of 24.67
segments.
3.3 Meeting Transcripts
The third dataset used in our evaluation contains
25 meeting transcripts from the ICSI-MR corpus
(Janin et al, 2004). The entire corpus contains
high-quality close talking microphone recordings
of multi-party dialogues. Transcriptions at word
level with utterance-level segmentations are also
available. The gold standard for thematic segmen-
tations has been kindly provided by (Galley et
al., 2003) and has been chosen by considering the
agreement between at least three human annota-
tions. Each meeting is thus divided into contigu-
ous major topic segments and contains an average
of 7.32 segments.
Note that thematic segmentation of meeting
data is a more challenging task as the thematic
transitions are subtler than those in TDT data.
4 Evaluation Metrics
In this section, we will look in detail at the error
metrics that have been proposed in previous stud-
ies and examine their inadequacies. In addition,
we propose a new evaluation metric that we con-
sider more appropriate.
4.1 Pk Metric
(Passonneau and Litman, 1996; Beeferman et al,
1999) underlined that the standard evaluation met-
rics of precision and recall are inadequate for the-
matic segmentation, namely by the fact that these
metrics did not account for how far away is a hy-
pothesized boundary (i.e. a boundary found by
the automatic procedure) from a reference bound-
ary (i.e. a boundary found in the reference data).
On the other hand, it is desirable that an algorithm
that places for instance a boundary just one utter-
ance away from the reference boundary to be pe-
nalized less than an algorithm that places a bound-
ary two (or more) utterances away from the ref-
erence boundary. Hence (Beeferman et al, 1999)
proposed a new metric, called PD, that allows for
a slight vagueness in where boundaries lie. More
146
specifically, (Beeferman et al, 1999) define PD
as follows2:
PD(ref, hyp) =
?
1?i?j?N D(i, j)[?ref (i, j) ?
?hyp(i, j)].
N is the number of words in the reference data.
The function ?ref (i, j) is evaluated to one if the
two reference corpus indices specified by its pa-
rameters i and j belong in the same segment, and
zero otherwise. Similarly, the function ?hyp(i, j)
is evaluated to one, if the two indices are hypothe-
sized by the automatic procedure to belong in the
same segment, and zero otherwise. The ? opera-
tor is the XNOR function ?both or neither?. D(i, j)
is a ?distance probability distribution over the set
of possible distances between sentences chosen
randomly from the corpus?. In practice, a distri-
bution D having ?all its probability mass at a fixed
distance k? (Beeferman et al, 1999) was adopted
and the metric PD was thus renamed Pk.
In the framework of the TDT initiative, (Allan
et al, 1998) give the following formal definition
of Pk and its components:
Pk = PMiss ? Pseg + PFalseAlarm ? (1? Pseg),
where:
PMiss =
PN?k
i=1 [?hyp(i,i+k)]?[1??ref (i,i+k)]
PN?k
i=1 [1??ref (i,i+k)]
,
PFalseAlarm =
PN?k
i=1 [1??hyp(i,i+k)]?[?ref (i,i+k)]
PN?k
i=1 ?ref (i,i+k)
,
and Pseg is the a priori probability that in
the reference data a boundary occurs within an
interval of k words. Therefore Pk is calculated by
moving a window of a certain width k, where k is
usually set to half of the average number of words
per segment in the gold standard.
Pevzner and Hearst (2002) highlighted several
problems of the Pk metric. We illustrate below
what we consider the main problems of the Pk
metric, based on two examples.
Let r(i, k) be the number of boundaries be-
tween positions i and i + k in the gold standard
segmentation and h(i, k) be the number of bound-
aries between positions i and i+k in the automatic
hypothesized segmentation.
? Example 1: If r(i, k) = 2 and h(i, k) = 1
then obviously a missing boundary should
2Let ref be a correct segmentation and hyp be a segmen-
tation proposed by a text segmentation system. We will keep
this notations in equations introduced below.
be counted in Pk, i.e. PMiss should be in-
creased.
? Example 2: If r(i, k) = 1 and h(i, k) =
2 then obviously PFalseAlarm should be in-
creased.
However, considering the first example, we will
obtain ?ref (i, i + k) = 0, ?hyp(i, i + k) = 0
and consequently PMiss is not increased. By tak-
ing the case from the second example we obtain
?ref (i, i + k) = 0 and ?hyp(i, i + k) = 0, involv-
ing no increase of PFalseAlarm.
In (TDT, 1998), a slightly different defini-
tion is given for the Pk metric: the definition of
miss and false alarm probabilities is replaced with:
P ?Miss =
PN?k
i=1 [1??hyp(i,i+k)]?[1??ref (i,i+k)]
PN?k
i=1 [1??ref (i,i+k)]
,
P ?FalseAlarm =
PN?k
i=1 [1??hyp(i,i+k)]?[?ref (i,i+k)]
PN?k
i=1 ?ref (i,i+k)
,
where:
?hyp(i, i+ k) =
{
1, if r(i, k) = h(i, k),
0, otherwise.
We will refer to this new definition of Pk by
P ?k. Therefore, by taking the definition of
P ?k and the first example above, we obtain
?ref (i, i+ k) = 0 and ?hyp(i, i+ k) = 0 and thus
P ?Miss is correctly increased. However for the case
of example 2 we will obtain ?ref (i, i + k) = 0
and ?hyp(i, i + k) = 0, involving no increase of
P ?FalseAlarm and erroneous increase of P ?Miss.
4.2 WindowDiff metric
Pevzner and Hearst (2002) propose the alternative
metric called WindowDiff. By keeping our nota-
tions concerning r(i, k) and h(i, k) introduced in
the subsection 4.1, WindowDiff is defined as:
WindowDiff =
PN?k
i=1 [|r(i,k)? h(i,k)|>0]
N?k .
Similar to both Pk and P ?k, WindowDiff is
also computed by moving a window of fixed size
across the test set and penalizing the algorithm
misses or erroneous algorithm boundary detec-
tions. However, unlike Pk and P ?k, WindowDiff
takes into account how many boundaries fall
within the window and is penalizing in ?how
many discrepancies occur between the reference
and the system results? rather than ?determining
how often two units of text are incorrectly labeled
147
as being in different segments? (Pevzner and
Hearst, 2002).
Our critique concerning WindowDiff is that
misses are less penalised than false alarms and
we argue this as follows. WindowDiff can be
rewritten as:
WindowDiff = WDMiss +WDFalseAlarm,
where:
WDMiss =
PN?k
i=1 [r(i,k)>h(i,k)]
N?k ,
WDFalseAlarm =
PN?k
i=1 [r(i,k)<h(i,k)]
N?k .
Hence both misses and false alarms are weighted
by 1N?k .
Note that, on the one hand, there are indeed (N-
k) equiprobable possibilities to have a false alarm
in an interval of k units. On the other hand, how-
ever, the total number of equiprobable possibil-
ities to have a miss in an interval of k units is
smaller than (N-k) since it depends on the num-
ber of reference boundaries (i.e. we can have a
miss in the interval of k units only if in that interval
the reference corpus contains at least one bound-
ary). Therefore misses, being weighted by 1N?k ,
are less penalised than false alarms.
Let Bref be the number of thematic boundaries
in the reference data. Let?s say that the refer-
ence data contains about 20% boundaries and 80%
non-boundaries from the total number of potential
boundaries. Therefore, since there are relatively
few boundaries compared with non-boundaries, a
strategy introducing no false alarms, but introduc-
ing a maximum number of misses (i.e. k ? Bref
misses) can be judged as being around 80% cor-
rect by the WindowDiff measure. On the other
hand, a segmentation with no misses, but with a
maximum number of false alarms (i.e. (N ? k)
false alarms) is judged as being 100% erroneous
by the WindowDiff measure. That is, misses and
false alarms are not equally penalised.
Another issue regarding WindowDiff is that it is
not clear ?how does one interpret the values pro-
duced by the metric? (Pevzner and Hearst, 2002).
4.3 Proposal for a New Metric
In order to address the inadequacies of Pk and
WindowDiff, we propose a new evaluation metric,
defined as follows:
Prerror = Cmiss ? Prmiss + Cfa ? Prfa,
where:
Cmiss (0 ? Cmiss ? 1) is the cost of a miss, Cfa
(0 ? Cfa ? 1) is the cost of a false alarm,
Prmiss =
PN?k
i=1 [?ref hyp(i,k)]
PN?k
i=1 [?ref (i,k)]
,
P rfa =
PN?k
i=1 [?ref hyp(i,k)]
N?k ,
?ref hyp(i, k) =
{
1, if r(i, k) > h(i, k)
0, otherwise
?ref hyp(i, k) =
{
1, if r(i, k) < h(i, k)
0, otherwise.
?ref (i, k) =
{
1, if r(i, k) > 0
0, otherwise.
Prmiss could be interpreted as the probability
that the hypothesized segmentation contains less
boundaries than the reference segmentation in an
interval of k units3, conditioned by the fact that
the reference segmentation contains at least one
boundary in that interval. Analogously Prfa is
the probability that the hypothesized segmentation
contains more boundaries than the reference seg-
mentation in an interval of k units.
For certain applications where misses are more
important than false alarms or vice versa, the
Prerror can be adjusted to tackle this trade-off via
the Cfa and Cmiss parameters. In order to have
Prerror ? [0, 1], we suggest that Cfa and Cmiss
be chosen such that Cfa + Cmiss = 1. By choos-
ing Cfa=Cmiss=12 , the penalization of misses and
false alarms is thus balanced. In consequence, a
strategy that places no boundaries at all is penal-
ized as much as a strategy proposing boundaries
everywhere (i.e. after every unit). In other words,
both such degenerate algorithms will have an error
rate Prerror of about 50%. The worst algorithm,
penalised as having an error rate Prerror of 100%
when k = 2, is the algorithm that places bound-
aries everywhere except the places where refer-
ence boundaries exist.
5 Results
5.1 Test Procedure
For the three datasets we first performed two
common preprocessing steps: common words are
eliminated using the same stop-list and remaining
words are stemmed by using Porter?s algorithm
(1980). Next, we ran the three segmenters de-
scribed in Section 2, by employing the default val-
ues for any system parameters and by letting the
3A unit can be either a word or a sentence / an utterance.
148
systems estimate the number of thematic bound-
aries.
We also considered the fact that C99 and
TextSeg algorithms can take into account a fixed
number of thematic boundaries. Even if the num-
ber of segments per document can vary in TDT
and meeting reference data, we consider that in a
real application it is impossible to provide to the
systems the exact number of boundaries for each
document to be segmented. Therefore, we ran C99
and TextSeg algorithms (for a second time), by
providing them only the average number of seg-
ments per document in the reference data, which
gives an estimation of the expected level of seg-
mentation granularity.
Four additional naive segmentations were also
used for evaluation, namely: no boundaries,
where the whole text is a single segment; all
boundaries, i.e. a thematic boundary is placed af-
ter each utterance; random known, i.e. the same
number of boundaries as in gold standard, distrib-
uted randomly throughout text; and random un-
known: the number of boundaries is randomly
selected and boundaries are randomly distributed
throughout text. Each of the segmentations was
evaluated with Pk, P ?k and WindowDiff, as de-
scribed in Section 4.
5.2 Comparative Performance of
Segmentation Systems
The results of applying each segmentation algo-
rithm to the three distinct datasets are summa-
rized in Figures 1, 2 and 3. Percent error values
are given in the figures and we used the follow-
ing abbreviations: WD to denote WindowDiff er-
ror metric; TextSeg KA to denote the TextSeg algo-
rithm (Utiyama and Isahara, 2001) when the av-
erage number of boundaries in the reference data
was provided to the algorithm; C99 KA to denote
the C99 algorithm (Choi, 2000) when the aver-
age number of boundaries in the reference data
was provided to the algorithm; N0 to denote the al-
gorithm proposing a segmentation with no bound-
aries; All to denote the algorithm proposing the de-
generate segmentation all boundaries; RK to de-
note the algorithm that generates a random known
segmentation; and RU to denote the algorithm that
generates a random unknown segmentation.
5.2.1 Comparison of System Performance
from Artificial to Realistic Data
From the artificial data to the more realistic
data, we expect to have more noise and thus the
algorithms to constantly degrade, but as our ex-
periments show a reversal of the assessment can
appear. More exactly: as can be seen from Figure
1, both C99 and TextSeg algorithms significantly
outperformed TextTiling algorithm on the artifi-
cially created dataset, when the number of seg-
ments was determined by the systems. A com-
parison between the error rates given in Figure
1 and Figure 2 show that C99 and TextSeg have
a similar trend, by significantly decreasing their
performance on TDT data, but still giving bet-
ter results than TextTiling on TDT data. When
comparing the systems by Prerror, C99 has simi-
lar performance with TextTiling on meeting data
(see Figure 3). Moreover, when assessment is
done by using WindowDiff, Pk or P ?k, both C99
and TextSeg came out worse than TextTiling on
meeting data. This demonstrates that rankings ob-
tained when evaluating on artificial data are dif-
ferent from those obtained when evaluating on re-
alistic data. An alternative interpretation can be
given by taking into account that the degenerative
no boundaries segmentation has an error rate of
only 30% by the WindowDiff, Pk and P ?k metrics
on meeting data. That is, we could interpret that
all three systems give completely wrong segmen-
tations on meeting data (due to the fact that topic
shifts are subtler and not as abrupt as in TDT and
artificial data). Nevertheless, we tend to adopt the
first interpretation, given the weaknesses of Pk, P ?k
and WindowDiff (where misses are less penalised
than false alarms), as discussed in Section 4.
5.2.2 The Influence of the Error Metric on
Assessment
By following the quantitative assessment given
by the WindowDiff metric, we observe that the
algorithm labeled N0 is three times better than
the algorithm All on meeting data (see Figure 3),
while the same algorithm N0 is considered only
two times better than All on the artificial data (see
Figure 1). This verifies the limitation of the Win-
dowDiff metric discussed in Section 4.
The four error metrics described in detail in
Section 4 have shown that the effect of knowing
the average number of boundaries on C99 is posi-
tive when testing on meeting data. However if we
want to take into account all the four error met-
149
0
20
40
60
80
100
120
Er
ro
r r
ate
Pk 34.75 11.01 7.89 10 7.15 44.12 55.5 47.71 52.51
P'k 35.1 13.21 8.55 10.94 7.87 44.13 99.58 48.85 80.84
WD 35.73 13.58 9.21 11.34 8.59 43.1 99.59 48.89 80.63
Pr_error 33.33 9.1 7.71 9.34 6.87 49.87 49.79 41.61 45.01
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 1: Error rates of the segmentation systems on artificial data, where k = 42 and Pseg = 0.44.
0
20
40
60
80
100
120
Err
or
 
ra
te
Pk 40.7 21.36 13.97 18.83 11.33 36.02 63.93 37.03 60.04
P'k 44.92 29.5 20.37 27.69 21.4 36.04 100 45.28 89.93
WD 44.76 36.28 30.3 40.26 31.46 46.69 100 53.75 91.92
Pr_error 34.09 25.69 25.62 27.17 21.05 49.96 50 44.89 48.31
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 2: Error rates of the segmentation systems on TDT data, where k = 55 and Pseg = 0.3606.
rics, it is difficult to draw definite conclusions re-
garding the influence of knowing the average num-
ber of boundaries on TextSeg and C99 algorithms.
For example, when tested on TDT data, C99 KA
seems to work better than C99 by Pk and P ?k met-
rics, while the WindowDiff metric gives a contra-
dictory assessment.
6 Conclusions
By comparing the performance of three systems
for thematic segmentation on different kinds of
data, we address two important issues in a quan-
titative evaluation. Strong emphasis was put on
the kind of data used for evaluation and we have
demonstrated experimentally that evaluation on
synthetic data is potentially misleading. The sec-
ond major issue addressed in this paper concerns
the choice of a valuable error metric and its side
effects on the evaluation assessment.
Acknowledgments
This work is supported by the Interactive
Multimodal Information Management project
(http://www.im2.ch/). Many thanks to Andrei
Popescu-Belis and the anonymous reviewers for
their valuable comments. We are grateful to the
International Computer Science Institute (ICSI),
University of California for sharing the data with
us. We also wish to thank Michael Galley who
kindly provided us the thematic annotations of
ICSI data.
References
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
Detection and Tracking Pilot Study: Final Re-
port. In DARPA Broadcast News Transcription and
Understanding Workshop, pages 194?218, Lands-
downe, VA. Morgan Kaufmann.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation.
Machine Learning, 34(Special Issue on Natural Lan-
guage Learning):177?210.
Gillian Brown and George Yule. 1998. Discourse
Analysis. (Cambridge Textbooks in Linguistics),
Cambridge.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
150
0
20
40
60
80
100
120
Err
or
 
ra
te
P_k 38.22 54.62 40.82 35.65 35.94 30.82 69.09 45.42 68.48
P'_k 39.12 66.78 45.66 39.04 39.6 30.89 100 47.97 95.99
WD 40.82 69.41 49.27 41.98 42.48 29.31 100 49.64 95.48
Pr_error 40.17 40.27 35.45 35.83 36.61 49.8 50 50.7 53.38
TextTiling C99 TextSeg C99_KA TextSeg_KA N0 All RK RU
Figure 3: Error rates of the segmentation systems on meeting data, where k = 85 and Pseg = 0.3090.
Conference of the North American Chapter of the
Association for Computational Linguistics, Seattle,
USA.
Olivier Ferret. 2002. Using Collocations for Topic
Segmentation and Link Detection. In The 19th In-
ternational Conference on Computational Linguis-
tics, Taipei, Taiwan.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Annual
Meeting of the Association for Computational Lin-
guistics, pages 562?569.
Barbara J. Grosz and Candace L. Sidner. 1986. At-
tention, Intentions and the Structure of Discourse.
Computational Linguistics, 12:175?204.
Michael A. K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Marti Hearst and Christian Plaunt. 1993. Subtopic
Structuring for Full-Length Document Access.
In Proceedings of the 16th Annual International
ACM/SIGIR Conference, pages 59?68, Pittsburgh,
Pennsylvania, United States.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Julia Hirschberg and Christine Nakatani. 1996.
A Prosodic Analysis of Discourse Segments in
Direction-Giving Monologues. In Proceedings of
the 34th Annual Meeting on Association for Com-
putational Linguistics, pages 286 ? 293, Santa Cruz,
California.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip
Dhillon, Jane Edwards, Javier Macias-Guarasa, Nel-
son Morgan, Barbara Peskin, Elizabeth Shriberg,
Andreas Stolcke, Chuck Wooters, and Britta Wrede.
2004. The ICSI Meeting Project: Resources and Re-
search. In ICASSP 2004 Meeting Recognition Work-
shop (NIST RT-04 Spring Recognition Evaluation),
Montreal.
David Kauchak and Francine Chen. 2005. Feature-
based segmentation of narrative documents. In Pro-
ceedings of the ACL Workshop on Feature Engi-
neering for Machine Learning in Natural Language
Processing, pages 32?39, Ann Arbor; MI; USA.
Hideki Kozima and Teiji Furugori. 1994. Segmenting
Narrative Text into Coherent Scenes. Literary and
Linguistic Computing, 9:13?19.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Pub Co.
Chris Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing.
MIT Press Cambridge, MA, USA.
Daniel Marcu. 2000. The Theory and Practice of
Discourse Parsing and Summarization. MIT Press
Cambridge, MA, USA.
Rebecca J. Passonneau and Diane J. Litman. 1996.
Empirical Analysis of Three Dimensions of Spoken
Discourse: Segmentation, Coherence and Linguistic
Devices.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse Segmentation by Human and Automated
Means. Computational Linguistics, 23(1).
Lev Pevzner and Marti Hearst. 2002. A Critique and
Improvement of an Evaluation Metric for Text Seg-
mentation. Computational Linguistics, 16(1):19?
36.
Martin Porter. 1980. An Algorithm for Suffix Strip-
ping. Program, 14:130 ? 137.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Penn-
sylvania.
TDT. 1998. The Topic Detection and Tracking - Phase
2 Evaluation Plan. Available from World Wide Web:
http://www.nist.gov/speech/tests/tdt/tdt98/index.htm.
Masao Utiyama and Hitoshi Isahara. 2001. A Statisti-
cal Model for Domain-Independent Text Segmenta-
tion. In ACL/EACL, pages 491?498.
151
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 101?108, New York City, June 2006. c?2006 Association for Computational Linguistics
Word Distributions for Thematic Segmentation in a Support Vector
Machine Approach
Maria Georgescul
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
maria.georgescul@eti.unige.ch
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX, UK
alexc@cs.rhul.ac.uk
Susan Armstrong
ISSCO/TIM, ETI
University of Geneva
1211 Geneva, Switzerland
susan.armstrong@issco.unige.ch
Abstract
We investigate the appropriateness of us-
ing a technique based on support vector
machines for identifying thematic struc-
ture of text streams. The thematic seg-
mentation task is modeled as a binary-
classification problem, where the different
classes correspond to the presence or the
absence of a thematic boundary. Exper-
iments are conducted with this approach
by using features based on word distri-
butions through text. We provide em-
pirical evidence that our approach is ro-
bust, by showing good performance on
three different data sets. In particu-
lar, substantial improvement is obtained
over previously published results of word-
distribution based systems when evalua-
tion is done on a corpus of recorded and
transcribed multi-party dialogs.
1 Introduction
(Todd, 2005) distinguishes between ?local-level top-
ics (of sentences, utterances and short discourse seg-
ments)? and ?discourse topics (of more extended
stretches of discourse)?.1 (Todd, 2005) points out
that ?discourse-level topics are one of the most elu-
sive and intractable notions in semantics?. Despite
this difficulty in giving a rigorous definition of dis-
course topic, the task of discourse/dialogue segmen-
tation into thematic episodes can be described by
1In this paper, we make use of the term topic or theme as
referring to the discourse/dialogue topic.
invoking an ?intuitive notion of topic? (Brown and
Yule, 1998). Thematic segmentation also relates
to several notions such as speaker?s intention, topic
flow and cohesion.
In order to find out if thematic segment identi-
fication is a feasible task, previous state-of-the-art
works appeal to experiments, in which several hu-
man subjects are asked to mark thematic segment
boundaries based on their intuition and a minimal
set of instructions. In this manner, previous studies,
e.g. (Passonneau and Litman, 1993; Galley et al,
2003), obtained a level of inter-annotator agreement
that is statistically significant.
Automatic thematic segmentation (TS), i.e. the
segmentation of a text stream into topically coher-
ent segments, is an important component in ap-
plications dealing with large document collections
such as information retrieval and document brows-
ing. Other tasks that could benefit from the thematic
textual structure include anaphora resolution, auto-
matic summarisation and discourse understanding.
The work presented here tackles the problem
of TS by adopting a supervised learning approach
for capturing linear document structure of non-
overlapping thematic episodes. A prerequisite for
the input data to our system is that texts are divided
into sentences or utterances.2 Each boundary be-
tween two consecutive utterances is a potential the-
matic segmentation point and therefore, we model
the TS task as a binary-classification problem, where
each utterance should be classified as marking the
2Occasionally within this document we employ the term ut-
terance to denote either a sentence or an utterance in its proper
sense.
101
presence or the absence of a topic shift in the dis-
course/dialogue based only on observations of pat-
terns in vocabulary use.
The remainder of the paper is organised as fol-
lows. The next section summarizes previous tech-
niques, describes how our method relates to them
and presents the motivations for a support vector ap-
proach. Sections 3 and 4 present our approach in
adopting support vector learning for thematic seg-
mentation. Section 5 outlines the empirical method-
ology and describes the data used in this study. Sec-
tion 6 presents and discusses the evaluation results.
The paper closes with Section 7, which briefly sum-
marizes this work and offers some conclusions and
future directions.
2 Related Work
As in many existing approaches to the thematic seg-
mentation task, we make the assumption that the
thematic coherence of a text segment is reflected at
lexical level and therefore we attempt to detect the
correlation between word distribution and thematic
changes throughout the text. In this manner, (Hearst,
1997; Reynar, 1998; Choi, 2000) start by using a
similarity measure between sentences or fixed-size
blocks of text, based on their word frequencies in
order to find changes in vocabulary use and there-
fore the points at which the topic changes. Sen-
tences are then grouped together by using a cluster-
ing algorithm. (Utiyama and Isahara, 2001) models
the problem of TS as a problem of finding the mini-
mum cost path in a graph and therefore adopts a dy-
namic programming algorithm. The main advantage
of such methods is that no training time and corpora
are required.
By modeling TS as binary-classification problem,
we introduce a new technique based on support vec-
tor machines (SVMs). The main advantage offered
by SVMs with respect to methods such as those de-
scribed above is related to the distance (or similarity)
function used. Thus, although (Choi, 2000; Hearst,
1997) employ a distance function (i.e. cosine dis-
tance) to detect thematic shifts, SVMs are capable
of using a larger variety of similarity functions.
Moreover, SVMs can employ distance functions
that operate in extremely high dimensional feature
spaces. This is an important property for our task,
where handling high dimensionality data represen-
tation is necessary (see section 4).
An alternative to dealing with high dimension
data may be to reduce the dimensionality of the
data representation. Therefore, linear algebra di-
mensionality reduction methods like singular value
decomposition have been adopted by (Choi et al,
2001; Popescu-Belis et al, 2004) in Latent Seman-
tic Analysis (LSA) for the task of thematic segmen-
tation. A Probabilistic Latent Semantic Analysis
(PLSA) approach has been adopted by (Brants et
al., 2002; Farahat and Chen, 2006) for the TS task.
(Blei and Moreno, 2001) proposed a TS approach,
by embedding a PLSA model in an extended Hid-
den Markov Model (HMM) approach, while (Yam-
ron et al, 1998) have previously proposed a HMM
approach for TS.
A shortcoming of the methods described above
is due to their typically generative manner of train-
ing, i.e. using the maximum likelihood estimation
for a joint sampling model of observation and la-
bel sequences. This poses the challenge of finding
more appropriate objective functions, i.e. alterna-
tives to the log-likelihood that are more closely re-
lated to application-relevant performance measures.
Secondly, efficient inference and learning for the TS
task often requires making questionable conditional
independence assumptions. In such cases, improved
performance may be obtained by using methods
with a more discriminative character, by allowing
direct dependencies between a label and past/future
observations and by efficient handling higher-order
combinations of input features. Given the discrim-
inative character of SVMs, we expect our model to
attain similar benefits.
3 Support Vector Learning Task and
Thematic Segmentation
The theory of Vapnik and Chervonenkis (Vapnik,
1995) motivated the introduction of support vector
learning. SVMs have originally been used for clas-
sification purposes and their principles have been ex-
tended to the task of regression, clustering and fea-
ture selection. (Kauchak and Chen, 2005) employed
SVMs using features (derived for instance from in-
formation given by the presence of paragraphs, pro-
nouns, numbers) that can be reliably used for topic
102
segmentation of narrative documents. Aside from
the fact that we consider the TS task on different
datasets (not only on narrative documents), our ap-
proach is different from the approach proposed by
(Kauchak and Chen, 2005) mainly by the data repre-
sentation we propose and by the fact that we put the
emphasis on deriving the thematic structure merely
from word distribution, while (Kauchak and Chen,
2005) observed that the ?block similarities provide
little information about the actual segment bound-
aries? on their data and therefore they concentrated
on exploiting other features.
An excellent general introduction to SVMs and
other kernel methods is given for instance in (Cris-
tianini and Shawe-Taylor, 2000). In the section be-
low, we give some highlights representing the main
elements in using SVMs for thematic segmentation.
The support vector learner L is given a training
set of n examples, usually denoted by Strain= ((~u1,
y1),...,(~un, yn))? (U ? Y )n drawn independently
and identically distributed according to a fixed dis-
tribution Pr(u, y) = Pr(y|u)Pr(u). Each train-
ing example consists of a high-dimensional vector ~u
describing an utterance and the class label y. The
utterance representations we chose are further de-
scribed in Section 4. The class label y has only
two possible values: ?thematic boundary? or ?non-
thematic boundary?. For notational convenience, we
replace these values by +1 and -1 respectively, and
thus we have y ? {-1, 1}. Given a hypothesis space
H, of functions h : U ? {?1,+1} having the form
h(~u) = sign(< ~w, ~u > +b), the inductive sup-
port vector learner Lind seeks a decision function
hind from H, using Strain so that the expected num-
ber of erroneous predictions is minimized. Using
the structural risk minimization principle (Vapnik,
1995), the support vector learner gets the optimal de-
cision function h by minimizing the following cost
function:
W ind(~w, b, ?1, ?2, ..., ?n) = 12 < ~w, ~w > +
+ C+
n?
i=0,yi=1
?i + C?
n?
i=0,yi=?1
?i,
subject to:
yi[< ~w ? ~ui > +b] ? 1? ?i for i = 1, 2, ..., n;
?i ? 0 for i = 1, 2, ..., n.
The parameters ~w and b follow from the optimi-
sation problem, which is solved by applying La-
grangian theory. The so-called slack variables ?i,
are introduced in order to be able to handle non-
separable data. The positive parameters C+ and C?
are called regularization parameters and determine
the amount up to which errors are tolerated. More
exactly, training data may contain noisy or outlier
data that are not representative of the underlying dis-
tribution. On the one hand, fitting exactly to the
training data may lead to overfitting. On the other
hand, dismissing true properties of the data as sam-
pling bias in the training data will result in low accu-
racy. Therefore, the regularization parameter is used
to balance the trade-off between these two compet-
ing considerations. Setting the regularization para-
meter too low can result in poor accuracy, while set-
ting it too high can lead to overfitting. In the TS task,
we used an automated procedure to select the regu-
larization parameters, as further described in section
5.3.
In cases where non-linear hypothesis functions
should be optimised, each ~ui can be mapped into
?(~ui) ? F , where F is a higher dimensional space
usually called feature space, in order to make linear
the relation between ~ui and yi. Thus the original lin-
ear learning machine can be adopted in finding the
classification solution in the feature space.
When using a mapping function ? : U ? F ,
if we have a way of computing the inner product
??(~ui), ?(~uj)? directly as a function of the origi-
nal input point, then the so-called kernel function
K(~ui, ~uj) = ??(~ui), ?(~uj)? is proved to simplify
the computational complexity implied by the direct
use of the mapping function ?. The choice of appro-
priate kernels and its specific parameters is an empir-
ical issue. In our experiments, we used the Gaussian
radial basis function (RBF) kernel:
KRBF (~ui, ~uj) = exp(??
2||~ui ? ~uj ||
2).
For the SVM calculations, we used the LIBSVM li-
brary (Chang and Lin, 2001).
4 Representation of the information used
to determine thematic boundaries
As presented in section 3, in the thematic segmen-
tation task, an input ~ui to the support vector classi-
fier is a vectorial representation of the utterance to
103
be classified and its context. Each dimension of the
input vector indicates the value of a certain feature
characterizing the utterance. All input features here
are indicator functions for a word occurring within
a fixed-size window centered on the utterance being
labeled. More exactly, the input features are com-
puted in the following steps:
1. The text has been pre-processed by tokeniza-
tion, elimination of stop-words and lemmatiza-
tion, using TreeTagger (Schmid, 1996).
2. We make use of the so-called bag of words ap-
proach, by mapping each utterance to a bag, i.e.
a set that contains word frequencies. Therefore,
word frequencies have been computed to count
the number of times that each term (i.e. word
lemma) is used in each utterance. Then a trans-
formation of the raw word frequency counts
is applied in order to take into account both
the local (i.e. for each utterance) word fre-
quencies as well as the overall frequencies of
their occurrences in the entire text collection.
More exactly, we made experiments in paral-
lel with three such transformations, which are
very commonly used in information retrieval
domain (Dumais, 1991): tf.idf, tf.normal and
log.entropy.
3. Each i-th utterance is represented by a vector
~ui, where a j-th element of ~ui is computed as:
ui,j =
?
?
i?
t=i?winSize
ft,j
?
?
?
?
i+winSize?
k=i+1
fk,j
?
? ,
where winSize ? 1 and fi,j is the weighted
frequency (determined in the previous step) of
the j-th word from the vocabulary in the i-th ut-
terance. In this manner, we will have ui,j > 0 if
and only if at least two occurrences of the j-th
term occur within (2 ? winSize) utterances on
opposite sides of a boundary candidate. That
is, each ui,j is capturing how many word co-
occurrences appear across the candidate utter-
ance in an interval (of (2?winSize) utterances)
centered in the boundary candidate utterance.
4. Each attribute value from the input data is
scaled to the interval [0, 1].
Note that the vector space representation adopted in
the previous steps will result in a sparse high dimen-
sional input data for our system. More exactly, table
1 shows the average number of non-zero features per
example corresponding to each data set (further de-
scribed in section 5.1).
Data set Non zero features
ICSI 3.67%
TDT 0.40%
Brown 0.12%
Table 1: The percentage of non-zero features per ex-
ample.
5 Experimental Setup
5.1 Data sets used
In order to evaluate how robust our SVM approach
is, we performed experiments on three English data
sets of approximately the same dimension (i.e. con-
taining about 260,000 words).
The first dataset is a subset of the ICSI-MR cor-
pus (Janin et al, 2004), where the gold standard for
thematic segmentations has been provided by tak-
ing into account the agreement of at least three hu-
man annotators (Galley et al, 2003). The corpus
consists of high-quality close talking microphone
recordings of multi-party dialogues. Transcriptions
at word level with utterance-level segmentations are
also available. A test sample from this dataset con-
sists of the transcription of an approximately one-
hour long meeting and contains an average of about
seven thematic episodes.
The second data set contains documents randomly
selected from the Topic Detection and Tracking
(TDT) 2 collection, made available by (LDC, 2006).
The TDT collection includes broadcast news and
newswire text, which are segmented into topically
cohesive stories. We use the story segmentation pro-
vided with the corpus as our gold standard labeling.
A test sample from our subset contains an average
of about 24 segments.
The third dataset we use in this study was origi-
nally proposed in (Choi, 2000) and contains artifi-
cial thematic episodes. More precisely, the dataset
is built by concatenating short pieces of texts that
104
Data set Weighting schema winSize ? C
ICSI log.entropy 57 0.0625 0.01
TDT tf.idf 17 0.0625 0.1
Brown tf.idf 5 0.0625 0.001
Table 2: The optimal settings found for the SVM model, using the RBF kernel.
have been randomly extracted from the Brown cor-
pus. Any test sample from this dataset consists of
ten segments. Each segment contains at least three
sentences and no more than eleven sentences.
While the focus of our paper is not on the method
of evaluation, it is worth pointing out that the per-
formance on the synthetic data set is a very poor
guide to the performance on naturally occurring data
(Georgescul et al, 2006). We include the synthetic
data for comparison purposes.
5.2 Handling unbalanced data
We have a small percentage of positive examples
relative to the total number of training examples.
Therefore, in order to ensure that positive points are
not considered as being noisy labels, we change the
penalty of the minority (positive) class by setting the
parameter C+ of this class to:
C+ = ? ?
(
n
n+ ? 1
? 1
)
? C?,
where n+ is the number of positive training exam-
ples, n is the total number of training examples and
? is the scaling factor. In the experiments reported
here, we set the value for the scale factor ? to ? = 1
and we have: C+ = 7 ? C? for the synthetic data
derived from Brown corpus; C+ = 18 ? C?for the
TDT data and C+ = 62 ? C? for the ICSI meeting
data.
5.3 Model selection
We used 80% of each dataset to determine the best
model settings, while the remaining 20% is used
for testing purposes. Each training set (for each
dataset employed) was divided into disjoint subsets
and five-fold cross-validation was applied for model
selection.
In order to avoid too many combinations of pa-
rameter settings, model selection is done in two
phases, by distinguishing two kinds of parameters.
First, the parameters involved in data representation
(see section 4) are addressed. We start with choosing
an appropriate term weighting scheme and a good
value for the winSize parameter. This choice is
based on a systematic grid search over 20 differ-
ent values for winSize and the three variants tf.idf,
tf.normal and log.entropy for term weighting. We
ran five-fold cross validation, by using the RBF ker-
nel with its parameter ? fixed to ? = 1. We also set
the regularization parameter C equal to C = 1.
In the second phase of model selection, we
take the optimal parameter values selected in the
previous phase as a constant factor and search
the most appropriate values for C and ? para-
meters. The range of values we select from is:
C ?
{
10?3, 10?2, 10?1, 1, 10, 102, 103
}
and ? ?
{
2?6, 2?5, 2?4, ..., 24, 26
}
and for each possible
value we perform five-fold cross validation. There-
fore, we ran the algorithm five times for the 91 =
7 ? 13 parameter settings. The most suitable model
settings found are shown in Table 2. For these set-
tings, we show the algorithm?s results in section 6.
6 Evaluation
6.1 Evaluation Measures
Beeferman et al (1999) underlined that the stan-
dard evaluation metrics of precision and recall are
inadequate for thematic segmentation, namely by
the fact that these metrics did not account for how
far away a hypothesized boundary (i.e. a boundary
found by the automatic procedure) is from the ref-
erence boundary. On the other hand, for instance,
an algorithm that places a boundary just one utter-
ance away from the reference boundary should be
penalized less than an algorithm that places a bound-
ary ten (or more) utterances away from the reference
boundary.
Hence the use of two other evaluation metrics
is favored in thematic segmentation: the Pk met-
ric (Beeferman et al, 1999) and the WindowDiff
error metric (Pevzner and Hearst, 2002). In con-
105
020406080100120 Algorith
ms
Error rates
P_k18.
5411.0
152.51
20.492
1.3660
.04
21.683
1.912
354.6
268.48
WD19.
4713.5
880.63
23.993
6.2891
.92
25.535
.8825.4
769.41
95.48
SVMC
99   Ran
d      
SVMC
99Ran
d  
SVMG
03 G03
* C99
Rand
Brown d
ata
TDT da
ta
ICSI da
ta
Figure 1: Error rates of the segmentation systems.
trast to precision and recall, these metrics allow for a
slight vagueness in where the hypothesized thematic
boundaries are placed and capture ?the notion of
nearness in a principled way, gently penalizing algo-
rithms that hypothesize boundaries that aren?t quite
right, and scaling down with the algorithm?s degra-
dation? (Beeferman et al, 1999). That is, comput-
ing both Pk and WindowDiff metrics involves the
use of a fixed-size (i.e. having a fixed number of
either words or utterances) window that is moved
step by step over the data. At each step, Pk and
WindowDiff are basically increased (each metric in
a slightly different way) if the hypothesized bound-
aries and the reference boundaries are not within the
same window.
During the model selection phase, we used pre-
cision and recall in order to measure the system?s
error rate. This was motivated by the fact that pos-
ing the TS task as a classification problem leads to a
loss of the sequential nature of the data, which is an
inconvenient in computing the Pk and WindowDiff
measures. However, during the final testing phase
of our system, as well as for the evaluation of the
previous systems, we use both the Pk and the Win-
dowDiff error metric.
The relatively small size of our datasets does not
allow for dividing our test set into multiple sub-test
sets for applying statistical significance tests. This
would be desirable in order to indicate whether the
differences in system error rates are statistically sig-
nificant over different data sets. Nevertheless, we
believe that measuring differences in error rates ob-
tained on the test set is indicative of the relative per-
formance. Thus, the experimental results shown in
this paper should be considered as illustrative rather
than exhaustive.
6.2 Results
In order to determine the adequacy of our SVM ap-
proach over different genres, we ran our system over
three datasets, namely the ICSI meeting data, the
TDT broadcast data and the Brown written genre
data.
By measuring the system error rates using the
Pk and the WindowDiff metrics, Figure 1 summa-
rizes the quantitative results obtained in our empir-
ical evaluation. In Figure 1, our SVM approach is
labeled as SVM and we abbreviate WindowDiff as
WD. The results of our SVM system correspond to
the parameter values detected during model selec-
tion (see Table 2). We compare our system against
an existing thematic segmenter in the literature: C99
(Choi, 2000). We also give for comparison the
error rates of a naive algorithm, labeled as Rand
algorithm, which randomly distributes boundaries
throughout the text.
The LCseg system (Galley et al, 2003), labeled
here as G03, is to our knowledge the only word dis-
tribution based system evaluated on ICSI meeting
data. Therefore, we replicate the results reported by
(Galley et al, 2003) when evaluation of LCseg was
done on ICSI data. The so-labeled G03* algorithm
106
indicates the error rates obtained by (Galley et al,
2003) when extra (meeting specific) features have
been adopted in a decision tree classifier. However,
note that the results reported by (Galley et al) are
not directly comparable with our results because of
a slight difference in the evaluation procedure: (Gal-
ley et al) performed 25-fold cross validation and the
average Pk and WD error rates have been computed
on the held-out sets.
Figure 1 illustrates the following interesting re-
sults. For the ICSI meeting data, our SVM approach
provides the best performance relative to the com-
peting word distribution based state-of-the-art meth-
ods. This proves that our SVM-based system is able
to build a parametric model that leads to a segmenta-
tion that highly correlates to a human thematic seg-
mentation. Furthermore, by taking into account the
relatively small size of the data set we used for train-
ing, it can be concluded that the SVM can build
qualitatively good models even with a small train-
ing data. The work of (Galley et al, 2003) shows
that the G03* algorithm is better than G03 by ap-
proximately 10%, which indicates that on meeting
data the performance of our word-distribution based
approach could possibly be increased by using other
meeting-specific features.
By examining the error rates given by Pk metric
for the three systems on the TDT data set, we ob-
serve that our system and C99 performed more or
less equally. With respect to the WindowDiff met-
ric, our system has an error rate approximately 10%
smaller than C99.
On the synthetic data set, the SVM approach
performed slightly worse than C99, avoiding how-
ever catastrophic failure, as observed with the C99
method on ICSI data.
7 Conclusions
We have introduced a new approach based on word
distributions for performing thematic segmentation.
The thematic segmentation task is modeled here as
a binary classification problem and support vector
machine learning is adopted. In our experiments, we
make a comparison of our approach versus existing
linear thematic segmentation systems reported in the
literature, by running them over three different data
sets. When evaluating on real data, our approach ei-
ther outperformed the other existing methods or per-
forms comparably to the best. We view this as a
strong evidence that our approach provides a unified
and robust framework for the thematic segmentation
task. The results also suggest that word distributions
themselves might be a good candidate for capturing
the thematic shifts of text and that SVM learning can
play an important role in building an adaptable cor-
relation.
Our experiments also show the sensitivity of a
segmentation method to the type of a corpus on
which it is tested. For instance, the C99 algorithm
which achieves superior performance on a synthetic
collection performs quite poorly on the real-life data
sets.
While we have shown empirically that our tech-
nique can provide considerable gains by using sin-
gle word distribution features, future work will in-
vestigate whether the system can be improved by ex-
ploiting other features derived for instance from syn-
tactic, lexical and, when available, prosodic infor-
mation. If further annotated meeting data becomes
available, it would be also interesting to replicate our
experiments on a bigger data set in order to verify
whether our system performance improves.
Acknowledgments This work is partially sup-
ported by the Interactive Multimodal Information
Management project (http://www.im2.ch/). Many
thanks to the reviewers for their insightful sugges-
tions. We are grateful to the International Computer
Science Institute (ICSI), University of California for
sharing the data with us. The authors also thank
Michael Galley who kindly provided us the thematic
annotations of the ICSI data.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning, 34(1-3):177?210.
David M. Blei and Pedro J. Moreno. 2001. Topic Seg-
mentation with an Aspect Hidden Markov Model. In
Proceedings of the 24th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 343?348. ACM Press.
Thorsten Brants, Francine Chen, and Ioannis Tsochan-
taridis. 2002. Topic-Based Document Segmentation
with Probabilistic Latent Semantic Analysis. In Pro-
ceedings of the Eleventh International Conference on
107
Information and Knowledge Management, pages 211?
218, McLean, Virginia, USA. ACM Press.
Gillian Brown and George Yule. 1998. Discourse Analy-
sis. Cambridge Textbooks in Linguistics, Cambridge.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM:
a library for support vector machines. Software avail-
able at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Freddy Choi, Peter Wiemer-Hastings, and Johanna
Moore. 2001. Latent Semantic Analysis for Text Seg-
mentation. In Proceedings of the 6th Conference on
Empirical Methods in Natural Language Processing,
Seattle, WA.
Freddy Choi. 2000. Advances in Domain Independent
Linear Text Segmentation. In Proceedings of the 1st
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33,
Seattle, USA.
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and other
kernel-based learning methods. Cambridge Univer-
sity Press, Cambridge, UK.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229?236.
Ayman Farahat and Francine Chen. 2006. Improving
Probabilistic Latent Semantic Analysis with Principal
Component Analysis. In Proceedings of the 11th Con-
ference of the European Chapter of the Asociation for
Computational Linguistics, Trento, Italy.
Michael Galley, Kathleen McKeown, Eric Fosler-
Luissier, and Hongyan Jing. 2003. Discourse Seg-
mentation of Multy-Party Conversation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 562?569.
Maria Georgescul, Alexander Clark, and Susan Arm-
strong. 2006. An Analysis of Quantitative Aspects in
the Evaluation of Thematic Segmentation Algorithms.
To appear.
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Adam Janin, Jeremy Ang, Sonali Bhagat, Rajdip Dhillon,
Jane Edwards, Javier Macias-Guarasa, Nelson Mor-
gan, Barbara Peskin, Elizabeth Shriberg, Andreas
Stolcke, Chuck Wooters, and Britta Wrede. 2004. The
ICSI Meeting Project: Resources and Research. In
ICASSP 2004 Meeting Recognition Workshop (NIST
RT-04 Spring Recognition Evaluation), Montreal.
David Kauchak and Francine Chen. 2005. Feature-
Based Segmentation of Narrative Documents. In Pro-
ceedings of the ACL Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, pages 32?39, Ann Arbor; MI; USA.
LDC. 2006. The Linguistic Data Consortium. Available
from World Wide Web: http://www.ldc.upenn.edu.
Rebecca J. Passonneau and Diane J. Litman. 1993.
Intention-based Segmentation: Human Reliability and
Correlation with Linguistic Cues. In Proceedings of
the 31st conference on Association for Computational
Linguistics, pages 148 ? 155, Columbus, Ohio.
Lev Pevzner and Marti Hearst. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics, 16(1):19?36.
Andrei Popescu-Belis, Alexander Clark, Maria Georges-
cul, Sandrine Zufferey, and Denis Lalanne. 2004.
Shallow Dialogue Processing Using Machine Learn-
ing Algorithms (or Not). In Bourlard H. and Ben-
gio S., editors, Multimodal Interaction and Related
Machine Learning Algorithms, pages 277?290. LNCS
3361, Springer-Verlag, Berlin.
Jeffrey Reynar. 1998. Topic Segmentation: Algorithms
and Applications. Ph.D. thesis, University of Pennsyl-
vania.
Helmut Schmid. 1996. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Technical report, Insti-
tute for Computational Linguistics of the University
of Stuttgart.
Richard Watson Todd. 2005. A fuzzy approach to dis-
course topics. Journal of the International Association
for Semiotic Studies, 155:93?123.
Masao Utiyama and Hitoshi Isahara. 2001. A Statis-
tical Model for Domain-Independent Text Segmenta-
tion. In Proceedings of the 39th Annual Meeting of
the ACL joint with the 10th Meeting of the European
Chapter of the ACL, pages 491?498, Toulouse, France.
Vladimir Naumovich Vapnik. 1995. The Nature of Sta-
tistical Learning Theory. Springer-Verlag, New York.
Jonathan P. Yamron, Ira Carp, Lawrence Gillick, Stewe
Lowe, and Paul van Mulbregt. 1998. A Hidden
Markov Model Approach to Text Segmentation and
Event Tracking. In Proceedings of the IEEE Confer-
ence on Acoustics, Speech, and Signal Processing, vol-
ume 17, pages 333?336, Seattle, WA.
108
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 125?132, New York City, June 2006. c?2006 Association for Computational Linguistics
Learning Auxiliary Fronting with Grammatical Inference
Alexander Clark
Department of Computer Science
Royal Holloway University of London
Egham, Surrey TW20 0EX
alexc@cs.rhul.ac.uk
Re?mi Eyraud
EURISE
23, rue du Docteur Paul Michelon
42023 Saint- ?Etienne Cedex 2
France
remi.eyraud@univ-st-etienne.fr
Abstract
We present a simple context-free gram-
matical inference algorithm, and prove
that it is capable of learning an inter-
esting subclass of context-free languages.
We also demonstrate that an implementa-
tion of this algorithm is capable of learn-
ing auxiliary fronting in polar interroga-
tives (AFIPI) in English. This has been
one of the most important test cases in
language acquisition over the last few
decades. We demonstrate that learning
can proceed even in the complete absence
of examples of particular constructions,
and thus that debates about the frequency
of occurrence of such constructions are ir-
relevant. We discuss the implications of
this on the type of innate learning biases
that must be hypothesized to explain first
language acquisition.
1 Introduction
For some years, a particular set of examples has
been used to provide support for nativist theories
of first language acquisition (FLA). These exam-
ples, which hinge around auxiliary inversion in the
formation of questions in English, have been con-
sidered to provide a strong argument in favour of
the nativist claim: that FLA proceeds primarily
through innately specified domain specific mecha-
nisms or knowledge, rather than through the oper-
ation of general-purpose cognitive mechanisms. A
key point of empirical debate is the frequency of oc-
currence of the forms in question. If these are van-
ishingly rare, or non-existent in the primary linguis-
tic data, and yet children acquire the construction in
question, then the hypothesis that they have innate
knowledge would be supported. But this rests on the
assumption that examples of that specific construc-
tion are necessary for learning to proceed. In this
paper we show that this assumption is false: that this
particular construction can be learned without the
learner being exposed to any examples of that par-
ticular type. Our demonstration is primarily mathe-
matical/computational: we present a simple experi-
ment that demonstrates the applicability of this ap-
proach to this particular problem neatly, but the data
we use is not intended to be a realistic representation
of the primary linguistic data, nor is the particular
algorithm we use suitable for large scale grammar
induction.
We present a general purpose context-free gram-
matical algorithm that is provably correct under a
certain learning criterion. This algorithm incorpo-
rates no domain specific knowledge: it has no spe-
cific information about language; no knowledge of
X-bar schemas, no hidden sources of information to
reveal the structure. It operates purely on unanno-
tated strings of raw text. Obviously, as all learn-
ing algorithms do, it has an implicit learning bias.
This very simple algorithm has a particularly clear
bias, with a simple mathematical description, that al-
lows a remarkably simple characterisation of the set
of languages that it can learn. This algorithm does
not use a statistical learning paradigm that has to be
tested on large quantities of data. Rather it uses a
125
symbolic learning paradigm, that works efficiently
with very small quantities of data, while being very
sensitive to noise. We discuss this choice in some
depth below.
For reasons that were first pointed out by Chom-
sky (Chomsky, 1975, pages 129?137), algorithms
of this type are not capable of learning all of nat-
ural language. It turns out, however, that algorithms
based on this approach are sufficiently strong to
learn some key properties of language, such as the
correct rule for forming polar questions.
In the next section we shall describe the dispute
briefly; in the subsequent sections we will describe
the algorithm we use, and the experiments we have
performed.
2 The Dispute
We will present the dispute in traditional terms,
though later we shall analyse some of the assump-
tions implicit in this description. In English, po-
lar interrogatives (yes/no questions) are formed by
fronting an auxiliary, and adding a dummy auxiliary
?do? if the main verb is not an auxiliary. For exam-
ple,
Example 1a The man is hungry.
Example 1b Is the man hungry?
When the subject NP has a relative clause that also
contains an auxiliary, the auxiliary that is moved is
not the auxiliary in the relative clause, but the one in
the main (matrix) clause.
Example 2a The man who is eating is hungry.
Example 2b Is the man who is eating hungry?
An alternative rule would be to move the first oc-
curring auxiliary, i.e. the one in the relative clause,
which would produce the form
Example 2c Is the man who eating is hungry?
In some sense, there is no reason that children
should favour the correct rule, rather than the in-
correct one, since they are both of similar com-
plexity and so on. Yet children do in fact, when
provided with the appropriate context, produce sen-
tences of the form of Example 2b, and rarely if ever
produce errors of the form Example 2c (Crain and
Nakayama, 1987). The problem is how to account
for this phenomenon.
Chomsky claimed first, that sentences of the type
in Example 2b are vanishingly rare in the linguis-
tic environment that children are exposed to, yet
when tested they unfailingly produce the correct
form rather than the incorrect Example 2c. This is
put forward as strong evidence in favour of innately
specified language specific knowledge: we shall re-
fer to this view as linguistic nativism.
In a special volume of the Linguistic Review, Pul-
lum and Scholz (Pullum and Scholz, 2002), showed
that in fact sentences of this type are not rare at all.
Much discussion ensued on this empirical question
and the consequences of this in the context of ar-
guments for linguistic nativism. These debates re-
volved around both the methodology employed in
the study, and also the consequences of such claims
for nativist theories. It is fair to say that in spite
of the strength of Pullum and Scholz?s arguments,
nativists remained completely unconvinced by the
overall argument.
(Reali and Christiansen, 2004) present a possible
solution to this problem. They claim that local statis-
tics, effectively n-grams, can be sufficient to indi-
cate to the learner which alternative should be pre-
ferred. However this argument has been carefully re-
butted by (Kam et al, 2005), who show that this ar-
gument relies purely on a phonological coincidence
in English. This is unsurprising since it is implausi-
ble that a flat, finite-state model should be powerful
enough to model a phenomenon that is clearly struc-
ture dependent in this way.
In this paper we argue that the discussion about
the rarity of sentences that exhibit this particular
structure is irrelevant: we show that simple gram-
matical inference algorithms can learn this property
even in the complete absence of sentences of this
particular type. Thus the issue as to how frequently
an infant child will see them is a moot point.
3 Algorithm
Context-free grammatical inference algorithms are
explored in two different communities: in gram-
matical inference and in NLP. The task in NLP is
normally taken to be one of recovering appropri-
ate annotations (Smith and Eisner, 2005) that nor-
mally represent constituent structure (strong learn-
ing), while in grammatical inference, researchers
126
are more interested in merely identifying the lan-
guage (weak learning). In both communities, the
best performing algorithms that learn from raw posi-
tive data only 1, generally rely on some combination
of three heuristics: frequency, information theoretic
measures of constituency, and finally substitutabil-
ity. 2 The first rests on the observation that strings
of words generated by constituents are likely to oc-
cur more frequently than by chance. The second
heuristic looks for information theoretic measures
that may predict boundaries, such as drops in condi-
tional entropy. The third method which is the foun-
dation of the algorithm we use, is based on the distri-
butional analysis of Harris (Harris, 1954). This prin-
ciple has been appealed to by many researchers in
the field of grammatical inference, but these appeals
have normally been informal and heuristic (van Za-
anen, 2000).
In its crudest form we can define it as follows:
given two sentences ?I saw a cat over there?, and ?I
saw a dog over there? the learner will hypothesize
that ?cat? and ?dog? are similar, since they appear
in the same context ?I saw a __ there?. Pairs of
sentences of this form can be taken as evidence that
two words, or strings of words are substitutable.
3.1 Preliminaries
We briefly define some notation.
An alphabet ? is a finite nonempty set of sym-
bols called letters. A string w over ? is a finite se-
quence w = a1a2 . . . an of letters. Let |w| denote
the length of w. In the following, letters will be in-
dicated by a, b, c, . . ., strings by u, v, . . . , z, and the
empty string by ?. Let ?? be the set of all strings,
the free monoid generated by ?. By a language we
mean any subset L ? ??. The set of all substrings
of a language L is denoted Sub(L) = {u ? ?+ :
?l, r, lur ? L} (notice that the empty word does not
belong to Sub(L)). We shall assume an order ? or
 on ? which we shall extend to ?? in the normal
way by saying that u ? v if |u| < |v| or |u| = |v|
and u is lexicographically before v.
A grammar is a quadruple G = ?V, ?, P, S?
where ? is a finite alphabet of terminal symbols, V
1We do not consider in this paper the complex and con-
tentious issues around negative data.
2For completeness we should include lexical dependencies
or attraction.
is a finite alphabet of variables or non-terminals, P
is a finite set of production rules, and S ? V is a
start symbol.
If P ? V ? (??V )+ then the grammar is said to
be context-free (CF), and we will write the produc-
tions as T ? w.
We will write uTv ? uwv when T ? w ? P .
?? is the reflexive and transitive closure of ?.
In general, the definition of a class L relies on
a class R of abstract machines, here called rep-
resentations, together with a function L from rep-
resentations to languages, that characterize all and
only the languages of L: (1) ?R ? R,L(R) ? L
and (2) ?L ? L, ?R ? R such that L(R) = L.
Two representations R1 and R2 are equivalent iff
L(R1) = L(R2).
3.2 Learning
We now define our learning criterion. This is identi-
fication in the limit from positive text (Gold, 1967),
with polynomial bounds on data and computation,
but not on errors of prediction (de la Higuera, 1997).
A learning algorithm A for a class of represen-
tations R, is an algorithm that computes a function
from a finite sequence of strings s1, . . . , sn to R. We
define a presentation of a language L to be an infinite
sequence of elements of L such that every element
of L occurs at least once. Given a presentation, we
can consider the sequence of hypotheses that the al-
gorithm produces, writing Rn = A(s1, . . . sn) for
the nth such hypothesis.
The algorithm A is said to identify the class R in
the limit if for every R ? R, for every presentation
of L(R), there is an N such that for all n > N ,
Rn = RN and L(R) = L(RN ).
We further require that the algorithm needs only
polynomially bounded amounts of data and compu-
tation. We use the slightly weaker notion defined by
de la Higuera (de la Higuera, 1997).
Definition A representation class R is identifiable
in the limit from positive data with polynomial time
and data iff there exist two polynomials p(), q() and
an algorithm A such that S ? L(R)
1. Given a positive sample S of size m A returns
a representation R ? R in time p(m), such that
2. For each representation R of size n there exists
127
a characteristic set CS of size less than q(n)
such that if CS ? S, A returns a representation
R? such that L(R) = L(R?).
3.3 Distributional learning
The key to the Harris approach for learning a lan-
guage L, is to look at pairs of strings u and v and to
see whether they occur in the same contexts; that is
to say, to look for pairs of strings of the form lur and
lvr that are both in L. This can be taken as evidence
that there is a non-terminal symbol that generates
both strings. In the informal descriptions of this that
appear in Harris?s work, there is an ambiguity be-
tween two ideas. The first is that they should appear
in all the same contexts; and the second is that they
should appear in some of the same contexts. We can
write the first criterion as follows:
?l, r lur ? L if and only if lvr ? L (1)
This has also been known in language theory by the
name syntactic congruence, and can be written u ?L
v.
The second, weaker, criterion is
?l, r lur ? L and lvr ? L (2)
We call this weak substitutability and write it as
u .=L v. Clearly u ?L v implies u .=L v when u is
a substring of the language. Any two strings that do
not occur as substrings of the language are obviously
syntactically congruent but not weakly substitutable.
First of all, observe that syntactic congruence is a
purely language theoretic notion that makes no ref-
erence to the grammatical representation of the lan-
guage, but only to the set of strings that occur in
it. However there is an obvious problem: syntac-
tic congruence tells us something very useful about
the language, but all we can observe is weak substi-
tutability.
When working within a Gold-style identification
in the limit (IIL) paradigm, we cannot rely on statis-
tical properties of the input sample, since they will
in general not be generated by random draws from a
fixed distribution. This, as is well known, severely
limits the class of languages that can be learned un-
der this paradigm. However, the comparative sim-
plicity of the IIL paradigm in the form when there
are polynomial constraints on size of characteristic
sets and computation(de la Higuera, 1997) makes it
a suitable starting point for analysis.
Given these restrictions, one solution to this prob-
lem is simply to define a class of languages where
substitutability implies congruence. We call these
the substitutable languages: A language L is substi-
tutable if and only if for every pair of strings u, v,
u .=L v implies u ?L v. This rather radical so-
lution clearly rules out the syntax of natural lan-
guages, at least if we consider them as strings of
raw words, rather than as strings of lexical or syn-
tactic categories. Lexical ambiguity alone violates
this requirement: consider the sentences ?The rose
died?, ?The cat died? and ?The cat rose from its bas-
ket?. A more serious problem is pairs of sentences
like ?John is hungry? and ?John is running?, where
it is not ambiguity in the syntactic category of the
word that causes the problem, but rather ambigu-
ity in the context. Using this assumption, whether
it is true or false, we can then construct a simple
algorithm for grammatical inference, based purely
on the idea that whenever we find a pair of strings
that are weakly substitutable, we can generalise the
hypothesized language so that they are syntactically
congruent.
The algorithm proceeds by constructing a graph
where every substring in the sample defines a node.
An arc is drawn between two nodes if and only if
the two nodes are weakly substitutable with respect
to the sample, i.e. there is an arc between u and v if
and only if we have observed in the sample strings
of the form lur and lvr. Clearly all of the strings in
the sample will form a clique in this graph (consider
when l and r are both empty strings). The connected
components of this graph can be computed in time
polynomial in the total size of the sample. If the
language is substitutable then each of these compo-
nents will correspond to a congruence class of the
language.
There are two ways of doing this: one way, which
is perhaps the purest involves defining a reduction
system or semi-Thue system which directly captures
this generalisation process. The second way, which
we present here, will be more familiar to computa-
tional linguists, and involves constructing a gram-
mar.
128
3.4 Grammar construction
Simply knowing the syntactic congruence might not
appear to be enough to learn a context-free gram-
mar, but in fact it is. In fact given the syntactic con-
gruence, and a sample of the language, we can sim-
ply write down a grammar in Chomsky normal form,
and under quite weak assumptions this grammar will
converge to a correct grammar for the language.
This construction relies on a simple property of
the syntactic congruence, namely that is in fact a
congruence: i.e.,
u ?L v implies ?l, r lur ?L lvr
We define the syntactic monoid to be the quo-
tient of the monoid ??/ ?L. The monoid operation
[u][v] = [uv] is well defined since if u ?L u? and
v ?L v? then uv ?L u?v?.
We can construct a grammar in the following triv-
ial way, from a sample of strings where we are given
the syntactic congruence.
? The non-terminals of the grammar are iden-
tified with the congruence classes of the lan-
guage.
? For any string w = uv , we add a production
[w] ? [u][v].
? For all strings a of length one (i.e. letters of ?),
we add productions of the form [a] ? a.
? The start symbol is the congruence class which
contains all the strings of the language.
This defines a grammar in CNF. At first sight, this
construction might appear to be completely vacu-
ous, and not to define any strings beyond those in
the sample. The situation where it generalises is
when two different strings are congruent: if uv =
w ? w? = u?v? then we will have two different rules
[w] ? [u][v] and [w] ? [u?][v?], since [w] is the
same non-terminal as [w?].
A striking feature of this algorithm is that it makes
no attempt to identify which of these congruence
classes correspond to non-terminals in the target
grammar. Indeed that is to some extent an ill-posed
question. There are many different ways of assign-
ing constituent structure to sentences, and indeed
some reputable theories of syntax, such as depen-
dency grammars, dispense with the notion of con-
stituent structure all together. De facto standards,
such as the Penn treebank annotations are a some-
what arbitrary compromise among many different
possible analyses. This algorithm instead relies on
the syntactic monoid, which expresses the combina-
torial structure of the language in its purest form.
3.5 Proof
We will now present our main result, with an outline
proof. For a full proof the reader is referred to (Clark
and Eyraud, 2005).
Theorem 1 This algorithm polynomially identi-
fies in the limit the class of substitutable context-free
languages.
Proof (Sketch) We can assume without loss of
generality that the target grammar is in Chomsky
normal form. We first define a characteristic set, that
is to say a set of strings such that whenever the sam-
ple includes the characteristic set, the algorithm will
output a correct grammar.
We define w(?) ? ?? to be the smallest word,
according to ?, generated by ? ? (? ? V )+. For
each non-terminal N ? V define c(N) to be the
smallest pair of terminal strings (l, r) (extending ?
from ?? to ?? ? ??, in some way), such that S ??
lNr.
We can now define the characteristic set CS =
{lwr|(N ? ?) ? P, (l, r) = c(N), w = w(?)}.
The cardinality of this set is at most |P | which
is clearly polynomially bounded. We observe that
the computations involved can all be polynomially
bounded in the total size of the sample.
We next show that whenever the algorithm en-
counters a sample that includes this characteristic
set, it outputs the right grammar. We write G? for
the learned grammar. Suppose [u] ??G? v. Then
we can see that u ?L v by induction on the max-
imum length of the derivation of v. At each step
we must use some rule [u?] ? [v?][w?]. It is easy
to see that every rule of this type preserves the syn-
tactic congruence of the left and right sides of the
rules. Intuitively, the algorithm will never generate
too large a language, since the languages are sub-
stitutable. Conversely, if we have a derivation of a
string u with respect to the target grammar G, by
129
construction of the characteristic set, we will have,
for every production L ? MN in the target gram-
mar, a production in the hypothesized grammar of
the form [w(L)] ? [w(M)][w(N)], and for every
production of the form L ? a we have a produc-
tion [w(L)] ? a. A simple recursive argument
shows that the hypothesized grammar will generate
all the strings in the target language. Thus the gram-
mar will generate all and only the strings required
(QED).
3.6 Related work
This is the first provably correct and efficient gram-
matical inference algorithm for a linguistically in-
teresting class of context-free grammars (but see for
example (Yokomori, 2003) on the class of very sim-
ple grammars). It can also be compared to An-
gluin?s famous work on reversible grammars (An-
gluin, 1982) which inspired a similar paper(Pilato
and Berwick, 1985).
4 Experiments
We decided to see whether this algorithm without
modification could shed some light on the debate
discussed above. The experiments we present here
are not intended to be an exhaustive test of the learn-
ability of natural language. The focus is on deter-
mining whether learning can proceed in the absence
of positive samples, and given only a very weak gen-
eral purpose bias.
4.1 Implementation
We have implemented the algorithm described
above. There are a number of algorithmic issues
that were addressed. First, in order to find which
pairs of strings are substitutable, the naive approach
would be to compare strings pairwise which would
be quadratic in the number of sentences. A more
efficient approach maintains a hashtable mapping
from contexts to congruence classes. Caching hash-
codes, and using a union-find algorithm for merging
classes allows an algorithm that is effectively linear
in the number of sentences.
In order to handle large data sets with thousands
of sentences, it was necessary to modify the al-
gorithm in various ways which slightly altered its
formal properties. However for the experiments
reported here we used a version which performs
the man who is hungry died .
the man ordered dinner .
the man died .
the man is hungry .
is the man hungry ?
the man is ordering dinner .
is the man who is hungry ordering dinner ?
?is the man who hungry is ordering dinner ?
Table 1: Auxiliary fronting data set. Examples
above the line were presented to the algorithm dur-
ing the training phase, and it was tested on examples
below the line.
exactly in line with the mathematical description
above.
4.2 Data
For clarity of exposition, we have used extremely
small artificial data-sets, consisting only of sen-
tences of types that would indubitably occur in the
linguistic experience of a child.
Our first experiments were intended to determine
whether the algorithm could determine the correct
form of a polar question when the noun phrase had a
relative clause, even when the algorithm was not ex-
posed to any examples of that sort of sentence. We
accordingly prepared a small data set shown in Ta-
ble 1. Above the line is the training data that the
algorithm was trained on. It was then tested on all of
the sentences, including the ones below the line. By
construction the algorithm would generate all sen-
tences it has already seen, so it scores correctly on
those. The learned grammar also correctly generated
the correct form and did not generate the final form.
We can see how this happens quite easily since the
simple nature of the algorithm allows a straightfor-
ward analysis. We can see that in the learned gram-
mar ?the man? will be congruent to ?the man who
is hungry?, since there is a pair of sentences which
differ only by this. Similarly, ?hungry? will be con-
gruent to ?ordering dinner?. Thus the sentence ?is
the man hungry ?? which is in the language, will be
congruent to the correct sentence.
One of the derivations for this sentence would be:
[is the man hungry ?] ? [is the man hungry] [?] ?
[is the man] [hungry] [?] ? [is] [the man] [hungry]
[?] ? [is] [the man][who is hungry] [hungry] [?] ?
130
it rains
it may rain
it may have rained
it may be raining
it has rained
it has been raining
it is raining
it may have been raining
?it may have been rained
?it may been have rain
?it may have been rain
Table 2: English auxiliary data. Training data above
the line, and testing data below.
[is] [the man][who is hungry] [ordering dinner] [?].
Our second data set is shown in Table 2, and is a
fragment of the English auxiliary system. This has
also been claimed to be evidence in favour of na-
tivism. This was discussed in some detail by (Pilato
and Berwick, 1985). Again the algorithm correctly
learns.
5 Discussion
Chomsky was among the first to point out the limi-
tations of Harris?s approach, and it is certainly true
that the grammars produced from these toy exam-
ples overgenerate radically. On more realistic lan-
guage samples this algorithm would eventually start
to generate even the incorrect forms of polar ques-
tions.
Given the solution we propose it is worth look-
ing again and examining why nativists have felt that
AFIPI was such an important issue. It appears that
there are several different areas. First, the debate
has always focussed on how to construct the inter-
rogative from the declarative form. The problem
has been cast as finding which auxilary should be
?moved?. Implicit in this is the assumption that the
interrogative structure must be defined with refer-
ence to the declarative, one of the central assump-
tions of traditional transformational grammar. Now,
of course, given our knowledge of many differ-
ent formalisms which can correctly generate these
forms without movement we can see that this as-
sumption is false. There is of course a relation be-
tween these two sentences, a semantic one, but this
does not imply that there need be any particular syn-
tactic relation, and certainly not a ?generative? rela-
tion.
Secondly, the view of learning algorithms is very
narrow. It is considered that only sentences of that
exact type could be relevant. We have demonstrated,
if nothing else, that that view is false. The distinction
can be learnt from a set of data that does not include
any example of the exact piece of data required: as
long as the various parts can be learned separately,
the combination will function in the natural way.
A more interesting question is the extent to which
the biases implicit in the learning algorithm are do-
main specific. Clearly the algorithm has a strong
bias. It overgeneralises massively. One of the advan-
tages of the algorithm for the purposes of this paper
is that its triviality allows a remarkably clear and ex-
plicit statement of its bias. But is this bias specific to
the domain of language? It in no way refers to any-
thing specific to the field of language, still less spe-
cific to human language ? no references to parts of
speech, or phrases, or even hierarchical phrase struc-
ture. It is now widely recognised that this sort of re-
cursive structure is domain-general (Jackendoff and
Pinker, 2005).
We have selected for this demonstration an algo-
rithm from grammatical inference. A number of sta-
tistical models have been proposed over the last few
years by researchers such as (Klein and Manning,
2002; Klein and Manning, 2004) and (Solan et al,
2005). These models impressively manage to ex-
tract significant structure from raw data. However,
for our purposes, neither of these models is suitable.
Klein and Manning?s model uses a variety of differ-
ent cues, which combine with some specific initial-
isation and smoothing, and an explicit constraint to
produce binary branching trees. Though very im-
pressive, the model is replete with domain-specific
biases and assumptions. Moreover, it does not learn
a language in the strict sense (a subset of the set of
all strings), though it would be a simple modification
to make it perform such a task. The model by Solan
et al would be more suitable for this task, but again
the complexity of the algorithm, which has numer-
ous components and heuristics, and the lack of a the-
oretical justification for these heuristics again makes
the task of identifying exactly what these biases are,
and more importantly how domain specific they are,
131
a very significant problem.
In this model, the bias of the algorithm is com-
pletely encapsulated in the assumption u .= v im-
plies u ? v. It is worth pointing out that this does
not even need hierarchical structure ? the model
could be implemented purely as a reduction system
or semi-Thue system. The disadvantage of using
that approach is that it is possible to construct some
bizarre examples where the number of reductions
can be exponential.
Using statistical properties of the set of strings,
it is possible to extend these learnability results to
a more substantial class of context free languages,
though it is unlikely that these methods could be ex-
tended to a class that properly contains all natural
languages.
6 Conclusion
We have presented an analysis of the argument that
the acquisition of auxiliary fronting in polar inter-
rogatives supports linguistic nativism. Using a very
simple algorithm based on the ideas of Zellig Har-
ris, with a simple domain-general heuristic, we show
that the empirical question as to the frequency of oc-
currence of polar questions of a certain type in child-
directed speech is a moot point, since the distinction
in question can be learned even when no such sen-
tences occur.
Acknowledgements This work has been partially
supported by the EU funded PASCAL Network of
Excellence on Pattern Analysis, Statistical Mod-
elling and Computational Learning.
References
D. Angluin. 1982. Inference of reversible languages.
Communications of the ACM, 29:741?765.
Noam Chomsky. 1975. The Logical Structure of Lin-
guistic Theory. University of Chicago Press.
Alexander Clark and Remi Eyraud. 2005. Identification
in the limit of substitutable context free languages. In
Sanjay Jain, Hans Ulrich Simon, and Etsuji Tomita,
editors, Proceedings of The 16th International Confer-
ence on Algorithmic Learning Theory, pages 283?296.
Springer-Verlag.
S. Crain and M. Nakayama. 1987. Structure dependence
in grammar formation. Language, 63(522-543).
C. de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
(27):125?138. Kluwer Academic Publishers. Manu-
factured in Netherland.
E. M. Gold. 1967. Language indentification in the limit.
Information and control, 10(5):447 ? 474.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146?62.
Ray Jackendoff and Steven Pinker. 2005. The nature of
the language faculty and its implications for the evolu-
tion of language. Cognition, 97:211?225.
X. N. C. Kam, I. Stoyneshka, L. Tornyova, J. D. Fodor,
and W. G. Sakas. 2005. Non-robustness of syntax
acquisition from n-grams: A cross-linguistic perspec-
tive. In The 18th Annual CUNY Sentence Processing
Conference, April.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
of the ACL.
Dan Klein and Chris Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Proceedings of the 42nd Annual
Meeting of the ACL.
Samuel F. Pilato and Robert C. Berwick. 1985. Re-
versible automata and induction of the english auxil-
iary system. In Proceedings of the ACL, pages 70?75.
Geoffrey K. Pullum and Barbara C. Scholz. 2002. Em-
pirical assessment of stimulus poverty arguments. The
Linguistic Review, 19(1-2):9?50.
Florencia Reali and Morten H. Christiansen. 2004.
Structure dependence in language acquisition: Uncov-
ering the statistical richness of the stimulus. In Pro-
ceedings of the 26th Annual Conference of the Cogni-
tive Science Society, Mahwah, NJ. Lawrence Erlbaum.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 354?
362, Ann Arbor, Michigan, June.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proc. Natl. Acad. Sci., 102:11629?11634.
Menno van Zaanen. 2000. ABL: Alignment-based learn-
ing. In COLING 2000 - Proceedings of the 18th Inter-
national Conference on Computational Linguistics.
Takashi Yokomori. 2003. Polynomial-time identification
of very simple grammars from positive data. Theoret-
ical Computer Science, 298(1):179?206.
132
Proceedings of the EACL 2009 Workshop on Cognitive Aspects of Computational Language Acquisition, pages 26?33,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Another look at indirect negative evidence
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Shalom Lappin
Department of Philosophy
King?s College, London
shalom.lappin@kcl.ac.uk
Abstract
Indirect negative evidence is clearly an im-
portant way for learners to constrain over-
generalisation, and yet a good learning
theoretic analysis has yet to be provided
for this, whether in a PAC or a proba-
bilistic identification in the limit frame-
work. In this paper we suggest a theoreti-
cal analysis of indirect negative evidence
that allows the presence of ungrammati-
cal strings in the input and also accounts
for the relationship between grammatical-
ity/acceptability and probability. Given
independently justified assumptions about
lower bounds on the probabilities of gram-
matical strings, we establish that a limited
number of membership queries of some
strings can be probabilistically simulated.
1 Introduction
First language acquisition has been studied for a
long time from a theoretical point of view, (Gold,
1967; Niyogi and Berwick, 2000), but a consen-
sus has not emerged as to the most appropriate
model for learnability. The two main competing
candidates, Gold-style identification in the limit
and PAC-learning both have significant flaws.
For most NLP researchers, these issues are sim-
ply not problems: for all empirical purposes, one
is interested in modelling the distribution of exam-
ples or the conditional distribution of labels given
examples and the obvious solution ? an  ? ?
bound on some suitable loss function such as the
Kullback-Leibler Divergence ? is sufficient (Horn-
ing, 1969; Angluin, 1988a). There may be some
complexity issues involved with computing these
approximations, but there is no debate about the
appropriateness of the learning paradigm.
However, such an approach is unappealing to
linguists for a number of reasons: it fails to draw
a distinction between grammatical and ungram-
matical sentences, and for many linguists the key
data are not the ?performance? data but rather the
?voice of competence? as expressed in grammat-
icality and acceptability judgments. Many of the
most interesting sentences for syntacticians are
comparatively rare and unusual and may occur
with negligible frequency in the data.
We do not want to get into this debate here: in
this paper, we will assume that there is a categori-
cal distinction between grammatical and ungram-
matical sentences. See (Schu?tze, 1996) for exten-
sive discussion.
Within this view learnability is technically quite
difficult to formalise in a realistic way. Children
clearly are provided with examples of the lan-
guage ? so-called positive data ? but the status
of examples not in the language ? negative data
? is one of the endless and rather circular de-
bates in the language acquisition literature (Mar-
cus, 1993). Here we do not look at the role of
corrections and other forms of negative data but
we focus on what has been called indirect nega-
tive evidence (INE). INE is the non-occurrence of
data in the primary linguistic data; informally, if
the child does not hear certain ungrammatical sen-
tences, then by their absence the child can infer
that those strings are ungrammatical.
Indirect negative evidence has long been recog-
nised as an important source of information
(Pinker, 1979). However it has been surpris-
ingly difficult to find an explicit learning theo-
retic account of INE. Indeed, in both the PAC
and IIL paradigms it can be shown, that under
the standard assumptions, INE cannot help the
learner. Thus in many of these models, there
is a sharp and implausible distinction between
learning paradigms where the learner is provided
systematically with every negative example, and
those where the learner is denied any negative ev-
idence at all. Neither of these is very realistic.
In this paper, we suggest a resolution for this
conflict, by re-examining the standard learnability
assumptions. We make three uncontroversial ob-
26
servations: first that the examples the child is pro-
vided with are unlabelled, secondly that there are
a small proportion of ungrammatical sentences in
the input to the child, and thirdly that in spite of
this, the child does in fact learn.
We then draw a careful distinction between
probability and grammaticality and propose a re-
striction on the class of distributions allowed to
take account of the fact that children are exposed
to some ungrammatical utterances. We call this
the Disjoint Distribution Assumption: the assump-
tion that the classes of distributions for different
languages must be disjoint. Based on this assump-
tion, we argue that the learner can infer lower
bounds on the probabilities of grammatical strings,
and that using these lower bounds allow a prob-
abilistic approximation to membership queries of
some strings.
On this basis we conclude that the learner does
have some limited access to indirect negative evi-
dence, and we discuss some of the limitations on
this data and the implications for learnability.
2 Background
The most linguistically influential learnability
paradigm is undoubtedly that of Gold (Gold,
1967). In this paradigm the learner is required to
converge to exactly the right answer after a finite
time. In one variant of the paradigm the learner
is provided with only positive examples, and must
learn on every presentation of the language. Un-
der this paradigm no suprafinite class of languages
is learnable. If alternatively the learner is pro-
vided with a presentation of labelled examples,
then pretty much anything is learnable, but clearly
this paradigm has little relevance to the course of
language acquisition.
The major problem with the Gold positive data
paradigm is that the learner is required to learn
under every presentation; given the minimal con-
straints on what counts as a presentation, this re-
sults in a model which is unrealistically hard. In
particular, it is difficult for the learner to recover
from an overly general hypothesis; since it is has
only positive examples, such a hypothesis will
never be directly contradicted.
Indirect negative evidence is the claim that the
absence of sentences in the PLD can allow a
learner to infer that those sentences are ungram-
matical. As (Chomsky, 1981, p. 9) says:
A not unreasonable acquisition sys-
tem can be devised with the opera-
tive principle that if certain structures
or rules fail to be exemplified in rel-
atively simple expressions, where they
would expect to be found, then a (pos-
sibly marked) option is selected exclud-
ing them in the grammar, so that a kind
of ?negative evidence? can be available
even without corrections, adverse reac-
tions etc.
While this informal argument has been widely
accepted, and is often appealed to, it has so far not
been incorporated explicitly into a formal model
of learnability. Thus there are no learning mod-
els that we are aware of where positive learning
results have been achieved using indirect negative
evidence. Instead positive learnability results have
typically used general probabilistic models of con-
vergence without explicitly modelling grammati-
cality.
In what follows we will use the following no-
tation. ? is a finite alphabet, and ?? is the
set of all finite strings over ?. A (formal) lan-
guage L is a subset of ??. A distribution D over
?? is a function pD from ?? to [0, 1] such that?
w??? pD(w) = 1. We will write D(?
?) for the
set of all distributions over ??. The support of a
distribution D is the set of strings with positive
probability supp(D) = {w|pD(w) > 0}.
3 Probabilistic learning
The solution is to recognise the probabilistic na-
ture of how the samples are generated. We can
assume they are generated by some stochastic pro-
cess. On its own this says nothing ? anything can
be modelled by a stochastic process. To get learn-
ability we will need to add some constraints.
Suppose the child has seen thousands of times
sentences of the type ?I am AP?, and ?He is
AP? where AP is an adjective phrase, but he has
never heard anybody say ?He am AP?. Intuitively
it seems reasonable in this case to assume that
the child can infer from this that sentences of the
form?He am AP? are ungrammatical. Now, in the
case of the Gold paradigm, the child can make
no such inference. No matter how many millions
or trillions of times he has heard other examples,
the Gold paradigm does not allow any inference
to be made from frequency. The teacher, or en-
vironment, is an adversary who might be deliber-
ately withholding this data in order to confuse the
27
learner. The learner has to ignore this information.
However, in a more plausible learning environ-
ment, the learner can reason as follows. First, the
number of times that the learner has observed sen-
tences of the form ?He am AP? is zero. From this,
the learner can infer that sentences of this type are
rare: i.e. that they are not very probable. Similarly
from the high frequency of examples of the type ?I
am AP? and so on in the observed data, the learner
can infer that the probability of these sentences is
high.
The second step is that the learner can con-
clude from the difference in probability of these
two similar sets of sentences, that there must be a
difference in grammaticality between ?He am AP?
and ?He is AP?, and thus that sentences of the type
?He am AP? are ungrammatical.
It is important to recognise that the inference
proceeds in two steps:
1. the first is the inference from low frequency
in the observed data to low probability and
2. the second is the inference from compara-
tively low probability to ungrammaticality.
Both of these steps need justification, but if they
are valid, then the learner can extract evidence
about what is not in the language from stochastic
evidence about what is in the language. The first
step will be justified by some obvious and reason-
able probabilistic assumptions about the presenta-
tion of the data; the second step is more subtle and
requires some assumptions about the way the dis-
tribution of examples relates to the language being
learned.
3.1 Stochastic assumptions
The basic assumption we make is that the sam-
ples are being generated randomly in some way;
here we will make the standard assumption that
each sentence is generated independently from
the same fixed distribution, the Independently and
Identically Distributed (IID) assumption. While
this is a very standard assumption in statistics and
probability, it has been criticised as a modelling
assumption for language acquisition (Chater and
Vita?nyi, 2007).
Here we are interested in the acquisition of syn-
tax. We are therefore modelling the dependencies
between words and phrases in sentences, but as-
suming that there are no dependencies between
different sentences in discourse. That is to say, we
assume that the probability that a child hears a par-
ticular sentence does not depend on the previously
occurring sentence. Clearly, there are dependen-
cies between sentences. After questions, come an-
swers; a polar interrogative is likely to be followed
by a ?yes? or a ?no?; topics relate consecutive
sentences semantically, and numerous other fac-
tors cause inter-sentential relationships and regu-
larities of various types. Moreover, acceptability
does depend a great deal on the immediate context.
?Where did who go?? is marginal in most con-
texts; following ?Where did he go?? it is perfectly
acceptable. Additionally, since there are multiple
people generating Child Directed Speech (CDS),
this also introduces dependencies: each person
speaks in a slightly different way; while a rela-
tive is visiting, there will be a higher probability
of certain utterances, and so on. These correspond
to a violation of the ?identically? part of the IID
assumption: the distribution will change in time.
The question is whether it is legitimate to ne-
glect these issues in order to get some mathemat-
ical insight: do these idealising assumptions criti-
cally affect learnability? All of the computational
work that we are aware of makes these assump-
tions, whether in a nativist paradigm, (Niyogi and
Berwick, 2000; Sakas and Fodor, 2001; Yang,
2002) or an empiricist one (Clark and Thollard,
2004). We do need to make some assumptions,
otherwise even learning the class of observed nat-
ural languages would be too hard. The minimal
assumptions if we wish to allow any learnability
under stochastic presentation are that the process
generating the data is stationary and mixing. All
we need is for the law of large numbers to hold,
and for there to be rapid convergence of the ob-
served frequency to the expectation. We can get
this easily with the IID assumption, or with a bit
more work using ergodic theory. Thus in what fol-
lows we will make the IID assumption; effectively
using it as a place-holder for some more realistic
assumption, based on ergodic processes. See for
example (Gamarnik, 2003) for a an extension of
PAC analysis in this direction. The inference from
low frequency to low probability follows from the
minimal assumptions, specifically the IID, which
we are making here.
4 Probability and Grammaticality
We now look at the second step in the probabilistic
inference: how can the child go from low probabil-
28
ity to ungrammaticality? More generally the ques-
tion is what is the relation between probability and
grammaticality. There are lots of factors that affect
probability other than grammaticality: length of
utterance, lexical frequency, semantic factors and
real world factors all can have an impact on prob-
ability.
Low probability on its own cannot imply un-
grammaticality: if there are infinitely many gram-
matical sentences then there cannot be a lower
bound on the probability: if all grammatical
sentences have probability at least  then there
could be at most 1/ grammatical sentences which
would make the language finite. A very long
grammatical sentence can have very low probabil-
ity, lower than a short ungrammatical sentence, so
a less naive approach is necessary: the key point is
that the probability must be comparatively low.
Since we are learning from unlabelled data, the
only information that the child has comes from
from the distribution of examples, and so the dis-
tribution must pick out the language precisely. To
see this more clearly, suppose that the learner had
access to an ?Oracle? that would tell it the true
probability of any string, and has no limit on how
many strings it sees. A learner in this unrealistic
model is clearly more powerful than any learner
that just looks at a finite sample of the data. If this
learner could not learn, then no real learner could
learn on the basis of finite data.
More precisely for any language L we will have
a corresponding set of distributions D(L), and we
require the learner to learn under any of these dis-
tributions. What we require is that if we have two
distinct languages L and L? then the two sets of
distributionsD(L) andD(L?) must be disjoint, i.e.
have no elements in common. If they did have a
distribution D in common, then no learner could
tell the two languages apart as the information be-
ing provided would be identical. Of course, given
two distinct languages L and L?, it is possible that
they intersect, that is to say that there are strings
w in L?L?; a natural language example would be
two related dialects of the same language such as
some dialect of British English and some dialect of
American; though the languages are distinct in for-
mal terms, they are not disjoint, as there are sen-
tences that are grammatical in both. When we con-
sider the sets of distributions that are allowed for
each language D(L) and D(L?), we may find that
there are elements D ? D(L) and D? ? D(L?),
whose supports overlap, or even whose supports
are identical, supp(D) = supp(D?), and we may
well find that there are even some strings whose
probabilities are identical; i.e. there may be a
string w such that pD(w) = pD?(w) > 0. But
what we do not allow is that we have a distribution
D that is an element of both D(L) and D(L?). If
there were such an element, then when the learner
was provided with samples drawn from this dis-
tribution, since the samples are unlabelled, there
is absolutely no way that the learner could work
out whether the target was L or L?; the distribu-
tion would not determine the language. Therefore
there must be a function from distributions to lan-
guages. We cannot have a single distribution that
could be from two different languages. Let?s call
this the disjoint distribution assumption (DDA):
the assumption that the sets of distributions for dis-
tinct languages are disjoint.
Definition 1 The Disjoint Distribution Assump-
tion: If L 6= L? then D(L) ? D(L?) = ?.
This assumption seems uncontroversial; indeed
every proposal for a formal probabilistic model of
language acquisition that we are aware of makes
this assumption implicitly.
Now consider the convergence criterion: we
wish to measure the error with respect to the distri-
bution. There are two error terms, corresponding
to false positives and false negatives. Suppose our
target language is T and our hypothesis is H . De-
fine PD(S) for some set S to be
?
w?S pD(s).
e+ = PD(H \ T ) (1)
e? = PD(T \H) (2)
We will require both of these error terms to con-
verge to zero rapidly, and uniformly, as the amount
of data the learner has increases.
5 Modelling the DDA
If we accept this assumption, then we will require
some constraints on the sets of distributions. There
are a number of ways to model this: the most ba-
sic way is to assume that strings have probability
greater than zero if and only if the string is in the
language. Formally, for all D in D(L)
pD(w) > 0 ? w ? L (3)
Here we clearly have a function from distribu-
tions to languages: we just take the support of the
29
distribution to be the language: for all D in D(L),
supp(D) = L. Under this assumption alone how-
ever, indirect negative evidence will not be avail-
able.
That is because, in this situation, low probabil-
ity does not imply ungrammaticality: only zero
probability implies ungrammaticality. The fact
that we have never seen a sentence in a finite sam-
ple of size n means that we can say that it is likely
to have probability less than about 1/n, but we
cannot say that its probability is likely to be zero.
Thus we can never conclude that a sentence is un-
grammatical, if we make the assumption in Equa-
tion 3, and assume that there are no other limita-
tions on the set of distributions. Since we have
to learn for any distribution, we must learn even
when the distribution is being picked adversari-
ally. Suppose we have never seen an occurrence
of a string; this could be because the probability
has been artificially lowered to some infinitesimal
quantity by the adversary to mislead us. Thus we
gain nothing. Since there is no non-trivial lower
bound on the probability of grammatical strings,
effectively there is no difference between the re-
quirement pD(w) > 0 ? w ? L and the weaker
condition pD(w) > 0 ? w ? L.
But this is not the only possibility: indeed, it is
not a very good model at all. First, the assump-
tion that ungrammatical strings have zero proba-
bility is false. Ungrammatical sentences, that is
strings w, such that w 6? L, do occur in the en-
vironment, albeit with low probability. There are
performance errors, poetry and songs, other chil-
dren with less than adult competence, foreigners
and many other potential sources of ungrammat-
ical sentences. The orthodox view is that CDS
is ?unswervingly well-formed? (Newport et al,
1977): this is a slight exaggeration as a quick look
at CHILDES (MacWhinney, 2000) will confirm.
However, if we allow probabilities to be non-zero
for ungrammatical sentences, and put no other re-
strictions on the distributions then the learner will
fail on everything, since any distribution could be
for any language.
Secondly, the convergence criterion becomes
vacuous. As the probability of ungrammatical sen-
tences is now zero, this means that PD(H \ T ) =
e+ = 0, and thus the vacuous learner that always
returns the hypothesis ?? will have zero error. The
normal way of dealing with this (Shvaytser, 1990)
is to require the learner to hypothesize a subset of
the target. This is extremely undesirable, as it fails
to account for the presence of over-generalisation
errors in the child ? or any form of production of
ungrammatical sentences. On the basis of these
arguments, we can see that this naive approach is
clearly inadequate.
There are a number of other arguments why dis-
tribution free approaches are inappropriate here,
even though they are desirable in standard appli-
cations of statistical estimation (Collins, 2005).
First, the distribution of examples causally de-
pends on the people who are uttering the examples
who are native speakers of the language the learner
is learning and use that knowledge to construct ut-
terances. Second, suppose that we are trying to
learn a class of languages that includes some in-
finite regular language Lr. For concreteness sup-
pose it consists of {a?b?c?}; any number of a?s fol-
lowed by any number of b?s followed by any num-
ber of c?s. The learner must learn under any dis-
tribution: in particular it will have to learn under
the distribution where every string except an in-
finitesimally small amount has the number of ?a?s
equal to the number of ?b?s, or under the distribu-
tion where the number of occurrences of all three
letters must be equal, or any other arbitrary subset
of the target language. The adversary can distort
the probabilities so that with probability close to
one, at a fixed finite time, the learner will only see
strings from this subset. In effect the learner has
to learn these arbitrary subsets, which could be of
much greater complexity than the language.
Indeed researchers doing computational or
mathematical modelling of language acquisition
often find it convenient to restrict the distribu-
tions in some way. For example (Niyogi and
Berwick, 2000), in some computational modelling
of a parameter-setting model of language acquisi-
tion say
In the earlier section we assumed
that the data was uniformly distributed.
. . . In particular we can choose a dis-
tribution which will make the conver-
gence time as large as we want. Thus
the distribution-free convergence time
for the three parameter system is infi-
nite.
However, finding an alternative is not easy.
There are no completely satisfactory ways of re-
stricting the class of distributions, while maintain-
ing the property that the support of the distribu-
30
tion is equal to the language. (Clark and Thollard,
2004) argue for limiting the class of distributions
to those defined by the probabilistic variants of the
standard Chomsky representations. While this is
sufficient to achieve some interesting learning re-
sults, the class of distributions seems too small,
and is primarily motivated by the requirements of
the learning algorithm, rather than an analysis of
the learning situation.
5.1 Other bounds
Rather than making the simplistic assumption that
the support of the distribution must equal the lan-
guage, we can instead make the more realistic as-
sumption that every sentence, grammatical or un-
grammatical, can in principle appear in the input
and have non zero probability. In this case then
we do not need to require the learner to produce a
hypothesis that is a subset of the target, because if
the learner overgeneralises, e+ will be non-zero.
However, we clearly need to add some con-
straints to enforce the DDA. We can model this as
a function from distributions to languages. It is ob-
vious that grammaticality is correlated with prob-
ability in the sense that grammatical sentences are,
broadly speaking, more likely than ungrammatical
sentences; a natural way of articulating this is to
say that that there must be a real valued threshold
function gD(w) such that if pD(w) > gD(w) then
w ? L. Using this we define the set of allowable
distributions for a language L to be:
D(L, g) = {D : pD(w) > gD(w) ? w ? L}
(4)
Clearly this will satisfy the DDA. On its own this
is vacuous ? we have just changed notation, but
this notation gives us a framework in which to
compare some alternatives.
The original assumption that the support is
equal to the languages in this framework then just
has the simple form gD(w) = 0. The naive con-
stant bound we rejected above would be to have
this threshold as a constant that depends neither on
D nor on w i.e. for all w , gD(w) =  > 0. Both
of these bounds are clearly false, in the sense that
they do not hold for natural distributions: the first
because there are ungrammatical sentences with
non-zero probability; the second because there are
grammatical sentences with arbitrarily low proba-
bility. But the bound here need not be a constant,
and indeed it can depend both on the distribution
D and the word w.
5.2 Functional bound
We now look at variants of these bounds that pro-
vide a more accurate picture of the set of distribu-
tions that the child is exposed to. Recall that what
we are trying to do is to characterise a range of dis-
tributions that is large enough to include those that
the child will be exposed to. A slightly more nu-
anced way would be to have this as a very simple
function ofw, that ignoresD, and is just a function
of length. For example, we could have a simple
uniform exponential model:
gD(w) = ?g?
|w|
g (5)
This is in some sense an application of Harris?s
idea of equiprobability (Harris, 1991):
whatever else there is to be said
about the form of language, a fun-
damental task is to state the depar-
tures from equiprobability in sound- and
word-sequences
Using this model, we do not assume that the
learner is provided with information about the
threshold g; rather the learner will have cer-
tain, presumably domain general mechanisms that
cause it to discard anomalies, and pay attention
to significant deviations from equiprobability. We
can view the threshold g as defining a bound on
equiprobability; the role of syntax is to charac-
terise these deviations from the assumption that all
sequences are in some sense equally likely.
A more realistic model would depend also on
D; for example once could define these thresholds
to depend on some simple observable properties of
the distribution that could take account of lexical
probabilities: more sophisticated versions of this
bound could be derived from a unigram model, or
a class-based model (Pereira, 2000).
Alternatively we could take account of the pre-
fix and suffix probability of a string: for example,
where for some ? < 1: 1
gD(w) = ? max
uv=w
pD(u?
?)pD(?
?v) (6)
6 Using the lower bound
Putting aside the specific proposal for the lower
bound g, and going back to the issue of indirect
1A prefix is just an initial segment of a string and has no
linguistic and similarly for a suffix as the final segment.
31
negative evidence, we can see that the bound g is
the missing piece in the inference: if we observe
that a string w has zero frequency in our data set,
then we can conclude it has low probability, say
p; if p is less than g(w), then the string will be
ungrammatical; therefore the inference from low
probability to ungrammaticality in this case will
be justified.
The bound here is justified independently:
given the indubitable fact that there is a non-zero
probability of ungrammatical strings in the child?s
input, and the DDA, which again seems unassail-
able, together with the fact that learners do learn
some languages, it is a logical necessity that there
is such a bound. This bound then justifies indirect
negative evidence.
It is important to realise how limited this neg-
ative evidence is: it does not give the learner un-
limited access to negative examples. The learner
can only find out about sentences that would be
frequent if they were grammatical; this may be
enough to constrain overgeneralisation.
The most straightforward way of formalising
this indirect negative evidence is with membership
queries (Valiant, 1984; Angluin, 1988b). Mem-
bership queries are a model of learning where the
learner, rather than merely passively receiving ex-
amples, can query an oracle about whether an ex-
ample is in the language or not. In the model we
propose, the learner can approximate a member-
ship query with high probability by seeing the fre-
quency of an example with a high g in a large sam-
ple. If the frequency is low, often zero, in this sam-
ple, then with high probability this example will be
ungrammatical.
In particular given a functional bound, and some
polynomial thresholds on the probability, and us-
ing Chernoff bounds we can simulate a polyno-
mial number of membership queries, using large
samples of data. Note that membership queries
were part of the original PAC model (Valiant,
1984). Thus we can precisely define a limited
form of indirect negative evidence.
In particular given a bound g, we can test to see
whether a polynomial number of strings are un-
grammatical by taking a large sample and examin-
ing their frequency.
The exact details here depend on the form of
gD(w); if the bound depends on D in some re-
spect the learner will need to estimate some aspect
of D to compute the bound. This corresponds to
working out how probable the sentence would be
if it were grammatical. In the cases we have con-
sidered here, given sufficient data, we can estimate
gD(w) with high probability to an accuracy of 1;
call the estimate g?D(w). We can also estimate the
actual probability of the string with high probabil-
ity again with accuracy 2: let us denote this es-
timate by p?D(w). If p?D(w) + 2 < g?D(w) ? 1,
then we can conclude that pD(w) < gD(w) and
therefore that the sentence is ungrammatical. Con-
versely, the fact that a string has been observed
once does not necessarily mean that it is grammat-
ical. It only means that the probability is non-zero.
For the learner to conclude that it is grammatical,
s/he needs to have seen it enough times to con-
clude that the probability is above threshold. This
will be if p?D(w)? 2 > g?D(w) + 1
Note that this may be slightly too weak and
we might want to have a separate lower bound
for grammaticality and upper bound for ungram-
maticality. Otherwise if the distribution is such
that many strings are very close to the boundary
it will not be possible for the learner to determine
whether they are grammatical or not.
We can thus define learnability with respect to a
bound g that defines a set of distributionsD(L,G).
Thus this model differs from the PACmodel in two
respects: first the data is unlabelled, and secondly
is is not distribution free.
Definition An algorithm A learns the class of
languagesL if there is a polynomial p such that for
every language L ? L, where n is the size of the
smallest representation of L, for all distributions
D ? D(L, g) for all , ? > 0, when the algorithm
A is provided with at least p(n, ?1, ??1,?) un-
labelled examples drawn IID from D, it produces
with probability at least 1?? a hypothesis H such
that the error PD(H \T ?T \H) <  and further-
more it runs in time polynomial in the total size of
the sample.
7 Discussion
The unrealistic assumptions of the Gold paradigm
were realised quite early on (Horning, 1969). It
is possible to modify the Gold paradigm by in-
corporating a probabilistic presentation in the data
and requiring the learner to learn with probabil-
ity one. Perhaps surprisingly this does not change
anything, if we put no constraints on the target dis-
tribution (Angluin, 1988a).
In particular given a presentation on which the
32
normal non-probabilistic learner fails, we can con-
struct a distribution on which the probabilistic
learner will fail. Thus allowing an adversary to
pick the distribution is just as bad as allowing an
adversary to pick the presentation. However, the
distribution free assumption with unlabelled data
cannot account for the real variety of distributions
of CDS. In this model we propose restrictions on
the class of distributions, motivated by the oc-
currence of ungrammatical sentences. This also
means that we do not require a separate bound for
over-generalisation. As a result, we conclude that
there are limited amounts of negative evidence,
and suggest that these can be formalised as a lim-
ited number of membership queries, of strings that
would occur infrequently if they were ungrammat-
ical.
To be clear, we are not claiming that this is a di-
rect model of how children learn languages: rather
we hope to get some insight into the fundamen-
tal limitations of learning from unlabelled data by
switching to a more nuanced model. Here we have
not presented any positive results using this model,
but we observe that distribution dependent results
for learning regular languages and some context
free languages could be naturally modified to learn
in this framework. We hope that the recognition of
the validity of indirect negative evidence will di-
rect attention away from the supposed problems of
controlling overgeneralisation and towards the real
problems: the computational complexity of infer-
ring complex models.
References
D. Angluin. 1988a. Identifying languages from
stochastic examples. Technical Report YALEU/
DCS/RR-614, Yale University, Dept. of Computer
Science, New Haven, CT.
D. Angluin. 1988b. Queries and concept learning.
Machine Learning, 2(4):319?342, April.
N. Chater and P. Vita?nyi. 2007. ?Ideal learning? of nat-
ural language: Positive results about learning from
positive evidence. Journal of Mathematical Psy-
chology, 51(3):135?163.
N. Chomsky. 1981. Lectures on Government and
Binding.
Alexander Clark and Franck Thollard. 2004. Par-
tially distribution-free learning of regular languages
from positive samples. In Proceedings of COLING,
Geneva, Switzerland.
M. Collins. 2005. Parameter estimation for statistical
parsing models: Theory and practice of distribution-
free methods. In Harry Bunt, John Carroll, and
Giorgio Satta, editors, New Developments In Pars-
ing Technology, chapter 2, pages 19?55. Springer.
D Gamarnik. 2003. Extension of the PAC framework
to finite and countable Markov chains. IEEE Trans-
actions on Information Theory, 49(1):338?345.
E. M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447 ? 474.
Z.S. Harris. 1991. A Theory of Language and Informa-
tion: A Mathematical Approach. Clarendon Press.
James Jay Horning. 1969. A study of grammatical
inference. Ph.D. thesis, Computer Science Depart-
ment, Stanford University.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum Associates
Inc, US.
G.F. Marcus. 1993. Negative evidence in language
acquisition. Cognition, 46(1):53?85.
E.L. Newport, H. Gleitman, and L.R. Gleitman. 1977.
Mother, I?d rather do it myself: Some effects and
non-effects of maternal speech style. In Talking
to children: Language input and acquisition, pages
109?149. Cambridge University Press.
Partha Niyogi and Robert C. Berwick. 2000. Formal
models for learning in the principle and parameters
framework. In Peter Broeder and Jaap Murre, ed-
itors, Models of Language Acquisition, pages 225?
243. Oxford University Press.
F. Pereira. 2000. Formal grammar and information
theory: Together again? In Philosophical Transac-
tions of the Royal Society, pages 1239-1253. Royal
Society, London.
Steven Pinker. 1979. Formal models of language
learning. Cognition, 7:217?282.
W. Sakas and J.D. Fodor. 2001. The structural triggers
learner. In Language Acquisition and Learnability,
pages 172?233. Cambridge University Press.
Carson T. Schu?tze. 1996. The Empirical Base of Lin-
guistics. University of Chicago Press.
H. Shvaytser. 1990. A necessary condition for learn-
ing from positive examples. Machine Learning,
5(1):101?113.
L. Valiant. 1984. A theory of the learnable. Communi-
cations of the ACM, 27(11):1134 ? 1142.
C.D. Yang. 2002. Knowledge and Learning in Natural
Language. Oxford University Press, USA.
33
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 33?40,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
A note on contextual binary feature grammars
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Re?mi Eyraud and Amaury Habrard
Laboratoire d?Informatique Fondamentale
de Marseille, CNRS,
Aix-Marseille Universite?, France
remi.eyraud,amaury.habrard@lif.univ-mrs.fr
Abstract
Contextual Binary Feature Grammars
were recently proposed by (Clark et al,
2008) as a learnable representation for
richly structured context-free and con-
text sensitive languages. In this pa-
per we examine the representational
power of the formalism, its relationship
to other standard formalisms and lan-
guage classes, and its appropriateness
for modelling natural language.
1 Introduction
An important issue that concerns both natu-
ral language processing and machine learning
is the ability to learn suitable structures of a
language from a finite sample. There are two
major points that have to be taken into ac-
count in order to define a learning method use-
ful for the two fields: first the method should
rely on intrinsic properties of the language it-
self, rather than syntactic properties of the
representation. Secondly, it must be possible
to associate some semantics to the structural
elements in a natural way.
Grammatical inference is clearly an impor-
tant technology for NLP as it will provide a
foundation for theoretically well-founded un-
supervised learning of syntax, and thus avoid
the annotation bottleneck and the limitations
of working with small hand-labelled treebanks.
Recent advances in context-free grammati-
cal inference have established that there are
large learnable classes of context-free lan-
guages. In this paper, we focus on the ba-
sic representation used by the recent approach
proposed in (Clark et al, 2008). The authors
consider a formalism called Contextual Binary
Feature Grammars (CBFG) which defines a
class of grammars using contexts as features
instead of classical non terminals. The use of
features is interesting from an NLP point of
view because we can associate some semantics
to them, and because we can represent com-
plex, structured syntactic categories. The no-
tion of contexts is relevant from a grammatical
inference standpoint since they are easily ob-
servable from a finite sample. In this paper
we establish some basic language theoretic re-
sults about the class of exact Contextual Bi-
nary Feature Grammars (defined in Section 3),
in particular their relationship to the Chomsky
hierarchy: exact CBFGs are those where the
contextual features are associated to all the
possible strings that can appear in the corre-
sponding contexts of the language defined by
the grammar.
The main results of this paper are proofs
that the class of exact CBFGs:
? properly includes the regular languages
(Section 5),
? does not include some context-free lan-
guages (Section 6),
? and does include some non context-free
languages (Section 7).
Thus, this class of exact CBFGs is orthog-
onal to the classic Chomsky hierarchy but
can represent a very large class of languages.
Moreover, it has been shown that this class
is efficiently learnable. This class is therefore
an interesting candidate for modeling natural
language and deserves further investigation.
2 Basic Notation
We consider a finite alphabet ?, and ?? the
free monoid generated by ?. ? is the empty
string, and a language is a subset of ??. We
will write the concatenation of u and v as uv,
and similarly for sets of strings. u ? ?? is a
substring of v ? ?? if there are strings l, r ? ??
such that v = lur.
33
A context is an element of ?? ? ??. For a
string u and a context f = (l, r) we write f 
u = lur; the insertion or wrapping operation.
We extend this to sets of strings and contexts
in the natural way. A context is also known in
structuralist linguistics as an environment.
The set of contexts, or distribution, of a
string u of a language L is, CL(u) = {(l, r) ?
?? ? ??|lur ? L}. We will often drop the
subscript where there is no ambiguity. We
define the syntactic congruence as u ?L v iff
CL(u) = CL(v). The equivalence classes un-
der this relation are the congruence classes of
the language. In general we will assume that
? is not a member of any language.
3 Contextual Binary Feature
Grammars
Most definitions and lemmas of this section
were first introduced in (Clark et al, 2008).
3.1 Definition
Before the presentation of the formalism, we
give some results about contexts to help to
give an intuition of the representation. The
basic insight behind CBFGs is that there is a
relation between the contexts of a string w and
the contexts of its substrings. This is given by
the following trivial lemma:
Lemma 1. For any language L and for any
strings u, u?, v, v? if C(u) = C(u?) and C(v) =
C(v?), then C(uv) = C(u?v?).
We can also consider a slightly stronger result:
Lemma 2. For any language L and for any
strings u, u?, v, v? if C(u) ? C(u?) and C(v) ?
C(v?), then C(uv) ? C(u?v?).
C(u) ? C(u?) means that we can replace
any occurrence of u in a sentence, with a u?,
without affecting the grammaticality, but not
necessarily vice versa. Note that none of these
strings need to correspond to non-terminals:
this is valid for any fragment of a sentence.
We will give a simplified example from En-
glish syntax: the pronoun it can occur every-
where that the pronoun him can, but not vice
versa1. Thus given a sentence ?I gave him
away?, we can substitute it for him, to get the
1This example does not account for a number of syn-
tactic and semantic phenomena, particularly the distri-
bution of reflexive anaphors.
grammatical sentence I gave it away, but we
cannot reverse the process. For example, given
the sentence it is raining, we cannot substi-
tute him for it, as we will get the ungrammat-
ical sentence him is raining. Thus we observe
C(him) ( C(it).
Looking at Lemma 2 we can also say that,
if we have some finite set of strings K, where
we know the contexts, then:
Corollary 1.
C(w) ?
?
u?,v?:
u?v?=w
?
u?K:
C(u)?C(u?)
?
v?K:
C(v)?C(v?)
C(uv)
This is the basis of the representation: a
word w is characterised by its set of contexts.
We can compute the representation of w, from
the representation of its parts u?, v?, by looking
at all of the other matching strings u and v
where we understand how they combine (with
subset inclusion). In order to illustrate this
concept, we give here a simple example.
Consider the language {anbn|n > 0} and
the set K = {aabb, ab, abb, aab, a, b}. Suppose
we want to compute the set of contexts of
aaabbb, Since C(abb) ? C(aabbb), and vacu-
ously C(a) ? C(a), we know that C(aabb) ?
C(aaabbb). More generally, the contexts of ab
can represent anbn, those of aab the strings
an+1bn and the ones of abb the strings anbn+1.
The key relationships are given by context
set inclusion. Contextual binary feature gram-
mars allow a proper definition of the combina-
tion of context inclusion:
Definition 1. A Contextual Binary Feature
Grammar (CBFG) G is a tuple ?F, P, PL,??.
F is a finite set of contexts, called features,
where we write C = 2F for the power set of F
defining the categories of the grammar, P ?
C ? C ? C is a finite set of productions that
we write x ? yz where x, y, z ? C and PL ?
C ? ? is a set of lexical rules, written x ? a.
Normally PL contains exactly one production
for each letter in the alphabet (the lexicon).
A CBFG G defines recursively a map fG
34
from ?? ? C as follows:
fG(?) = ? (1)
fG(w) =
?
(c?w)?PL
c iff |w| = 1
(2)
fG(w) =
?
u,v:uv=w
?
x?yz?P :
y?fG(u)?
z?fG(v)
x iff |w| > 1.
(3)
We give here more explanation about the
map fG. It defines in fact the analysis of a
string by a CBFG. A rule z ? xy is applied
to analyse a string w if there is a cut uv = w
s.t. x ? fG(u) and y ? fG(v), recall that x
and y are sets of contexts. Intuitively, the re-
lation given by the production rule is linked
with Lemma 2: z is included in the set of fea-
tures of w = uv. From this relationship, for
any (l, r) ? z we have lwr ? L(G).
The complete computation of fG is then jus-
tified by Corollary 1: fG(w) defines all the
possible features associated by G to w with all
the possible cuts uv = w (i.e. all the possible
derivations).
Finally, the natural way to define the mem-
bership of a string w in L(G) is to have the
context (?, ?) ? fG(w) which implies that
?u? = u ? L(G).
Definition 2. The language defined by a
CBFG G is the set of all strings that are as-
signed the empty context: L(G) = {u|(?, ?) ?
fG(u)}.
As we saw before, we are interested in cases
where there is a correspondence between the
language theoretic interpretation of a context,
and the occurrence of that context as a feature
in the grammar. From the basic definition of
a CBFG, we do not require any specific con-
dition on the features of the grammar, except
that a feature is associated to a string if the
string appears in the context defined by the
feature. However, we can also require that fG
defines exactly all the possible features that
can be associated to a given string according
to the underlying language.
Definition 3. Given a finite set of contexts
F = {(l1, r1), . . . , (ln, rn)} and a language L
we can define the context feature map FL :
?? ? 2F which is just the map u 7? {(l, r) ?
F |lur ? L} = CL(u) ? F .
Using this definition, we now need a cor-
respondence between the language theoretic
context feature map FL and the representa-
tion in the CBFG fG.
Definition 4. A CBFG G is exact if for all
u ? ??, fG(u) = FL(G)(u).
Exact CBFGs are a more limited formalism
than CBFGs themselves; without any limits
on the interpretation of the features, we can
define a class of formalisms that is equal to
the class of Conjunctive Grammars (see Sec-
tion 4). However, exactness is an important
notion because it allows to associate intrinsic
components of a language to strings. Contexts
are easily observable from a sample and more-
over it is only when the features correspond to
the contexts that distributional learning algo-
rithms can infer the structure of the language.
A basic example of such a learning algorithm
is given in (Clark et al, 2008).
3.2 A Parsing Example
To clarify the relationship with CFG
parsing, we will give a simple worked
example. Consider the CBFG G =
?{(?, ?), (aab, ?), (?, b), (?, abb), (a, ?)(aab, ?)},
P, PL, {a, b}? with PL =
{{(?, b), (?, abb)} ? a, {(a, ?), (aab, ?)} ? b}
and P =
{{(?, ?)} ? {(?, b)}{(aab, ?)},
{(?, ?)} ? {(?, abb)}{(a, ?)},
{(?, b)} ? {(?, abb)}{(?, ?)},
{(a, ?)} ? {(?, ?)}{(aab, ?)}}.
If we want to parse the string w = aabb the
usual way is to have a bottom-up approach.
This means that we recursively compute the
fG map on the substrings of w in order to
check whether (?, ?) belongs to fG(w).
The Figure 1 graphically gives the main
steps of the computation of fG(aabb). Ba-
sically there are two ways to split aabb that
allow the derivation of the empty context:
aab|b and a|abb. The first one correspond
to the top part of the figure while the sec-
ond one is drawn at the bottom. We can
see for instance that the empty context be-
longs to fG(ab) thanks to the rule {(?, ?)} ?
{(?, abb)}{(a, ?)}: {(?, abb)} ? fG(a) and
{(a, ?)} ? fG(b). But for symmetrical reasons
35
the result can also be obtained using the rule
{(?, ?)} ? {(?, b)}{(aab, ?)}.
As we trivially have fG(aa) = fG(bb) = ?,
since no right-hand side contains the concate-
nation of the same two features, an induction
proof can be written to show that (?, ?) ?
fG(w) ? w ? {anbn : n > 0}.
a         a         b         bfG
{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)} {(a,?),(aab,?)}
fG fG fG
Rule: (?,?) ? (?,b) (aab,?)
fG(ab)  ? {(?,?)}
Rule: (a,?) ? (?,?) (aab,?)
fG(abb)  ? {(a,?)}
Rule: (?,?) ? (?,abb) (a,?)
fG(aabb)  ? {(?,?)}
f G
{(?,b),(?,abb)} {(?,b),(?,abb)} {(a,?),(aab,?)} {(a,?),(aab,?)}
f G f G f G
Rule: (?,?) ? (?,abb) (a,?)
fG(ab)  ? {(?,?)}
Rule: (?,b) ? (?,abb) (?,?)
fG(aab)  ? {(?,b)}
Rule: (?,?) ? (?,b) (aab,?)
fG(aabb)  ? {(?,?)}
Figure 1: The two derivations to obtain (?, ?)
in fG(aabb) in the grammar G.
This is a simple example that illustrates
the parsing of a string given a CBFG. This
example does not characterize the power of
CBFG since no right handside part is com-
posed of more than one context. A more inter-
esting, example with a context-sensitive lan-
guage, will be presented in Section 7.
4 Non exact CBFGs
The aim here is to study the expressive power
of CBFG compare to other formalism recently
introduced. Though the inference can be done
only for exact CBFG, where features are di-
rectly linked with observable contexts, it is
still worth having a look at the more general
characteristics of CBFG. For instance, it is in-
teresting to note that several formalisms in-
troduced with the aim of representing natural
languages share strong links with CBFG.
Range Concatenation Grammars
Range Concatenation Grammars are a very
powerful formalism (Boullier, 2000), that is a
current area of research in NLP.
Lemma 3. For every CBFG G, there is
a non-erasing positive range concatenation
grammar of arity one, in 2-var form that de-
fines the same language.
Proof. Suppose G = ?F, P, PL,??. Define
a RCG with a set of predicates equal to F
and the following clauses, and the two vari-
ables U, V . For each production x ? yz in
P , for each f ? x, where y = {g1, . . . gi},
z = {h1, . . . hj} add clauses
f(UV ) ? g1(U), . . . gi(U), h1(V ), . . . hj(V ).
For each lexical production {f1 . . . fk} ? a
add clauses fi(a) ? . It is straightforward
to verify that f(w) `  iff f ? fG(w).
Conjunctive Grammar
A more exact correspondence is to the class of
Conjunctive Grammars (Okhotin, 2001), in-
vented independently of RCGs. For every ev-
ery language L generated by a conjunctive
grammar there is a CBFG representing L#
(where the special character # is not included
in the original alphabet).
Suppose we have a conjunctive grammar
G = ??, N, P, S? in binary normal form (as
defined in (Okhotin, 2003)). We construct the
equivalent CBFG G? = ?F, P ?, PL,?? as fol-
lowed:
? For every letter a we add a context (la, ra)
to F such that laara ? L;
? For every rules X ? a in P , we create a
rule {(la, ra)} ? a in PL.
? For every non terminal X ? N , for every
rule X ? P1Q1& . . .&PnQn we add dis-
tinct contexts {(lPiQi , rPiQi)} to F, such
that for all i it exists ui, lPiQiuirPiQi ? L
and PiQi
?
?G ui;
? Let FX,j = {(lPiQi , rPiQi) : ?i} the
set of contexts corresponding to the
jth rule applicable to X. For all
36
(lPiQi , rPiQi) ? FX,j , we add to P
? the
rules (lPiQi , rPiQi) ? FPi,kFQi,l (?k, l).
? We add a new context (w, ?) to F such
that S
?
?G w and (w, ?) ? # to PL;
? For all j, we add to P ? the rule (?, ?) ?
FS,j{(w, ?)}.
It can be shown that this construction gives
an equivalent CBFG.
5 Regular Languages
Any regular language can be defined by an ex-
act CBFG. In order to show this we will pro-
pose an approach defining a canonical form for
representing any regular language.
Suppose we have a regular language L, we
consider the left and right residual languages:
u?1L = {w|uw ? L} (4)
Lu?1 = {w|wu ? L} (5)
They define two congruencies: if l, l? ? u?1L
(resp. r, r? ? Lu?1) then for all w ? ??, lw ?
L iff l?w ? L (resp. wr ? L iff wr? ? L).
For any u ? ??, let lmin(u) be the lexico-
graphically shortest element such that l?1minL =
u?1L. The number of such lmin is finite by
the Myhil-Nerode theorem, we denote by Lmin
this set, i.e. {lmin(u)|u ? ??}. We de-
fine symmetrically Rmin for the right residuals
(Lr?1min = Lu
?1).
We define the set of contexts as:
F (L) = Lmin ?Rmin. (6)
F (L) is clearly finite by construction.
If we consider the regular language de-
fined by the deterministic finite automata
of Figure 2, we obtain Lmin = {?, a, b}
and Rmin = {?, b, ab} and thus F (L) =
{(?, ?), (a, ?), (b, ?), (?, b), (a, b), (b, b), (?, ab),
(a, ab), (b, ab)}.
By considering this set of features, we
can prove (using arguments about congruence
classes) that for any strings u, v such that
FL(u) ? FL(v), then CL(u) ? CL(v). This
means the set of feature F is sufficient to rep-
resent context inclusion, we call this property
the fiduciality.
Note that the number of congruence classes
of a regular language is finite. Each congru-
ence class is represented by a set of contexts
Figure 2: Example of a DFA. The left residuals
are defined by ??1L, a?1L, b?1L and the right
ones by L??1, Lb?1, Lab?1 (note here that
La?1 = L??1).
FL(u). Let KL be finite set of strings formed
by taking the lexicographically shortest string
from each congruence class. The final gram-
mar can be obtained by combining elements
of KL. For every pair of strings u, v ? KL, we
define a rule
FL(uv) ? FL(u), FL(v) (7)
and we add lexical productions of the form
FL(a) ? a, a ? ?.
Lemma 4. For all w ? ??, fG(w) = FL(w).
Proof. (Sketch) Proof in two steps: ?w ?
??, FL(w) ? fG(w) and fG(w) ? FL(w). Each
step is made by induction on the length of w
and uses the rules created to build the gram-
mar, the derivation process of a CBFG and
the fiduciality for the second step. The key
point rely on the fact that when a string w is
parsed by a CBFG G, there exists a cut of w
in uv = w (u, v ? ??) and a rule z ? xy in G
such that x ? fG(u) and y ? fG(v). The rule
z ? xy is also obtained from a substring from
the set used to build the grammar using the
FL map. By inductive hypothesis you obtain
inclusion between fG and FL on u and v.
For the language of Figure 2, the following
set is sufficient to build an exact CBGF:
{a, b, aa, ab, ba, aab, bb, bba} (this corresponds
to all the substrings of aab and bba). We have:
FL(a) = F (L)\{(?, ?), (a, ?)} ? a
FL(b) = F (L) ? b
FL(aa) = FL(a) ? FL(a), FL(a)
FL(ab) = F (L) ? FL(a), FL(b) = FL(a), F (L)
FL(ba) = F (L) ? FL(b), FL(a) = F (L), FL(a)
FL(bb) = F (L) ? FL(b), FL(b) = F (L), F (L)
37
FL(aab) = FL(bba) = FL(ab) = FL(ba)
The approach presented here gives a canon-
ical form for representing a regular language
by an exact CBFG. Moreover, this is is com-
plete in the sense that every context of every
substring will be represented by some element
of F (L): this CBFG will completely model the
relation between contexts and substrings.
6 Context-Free Languages
We now consider the relationship between
CFGs and CBFGs.
Definition 5. A context-free grammar (CFG)
is a quadruple G = (?, V, P, S). ? is a fi-
nite alphabet, V is a set of non terminals
(? ? V = ?), P ? V ? (V ? ?)+ is a finite
set of productions, S ? V is the start symbol.
In the following, we will suppose that a CFG
is represented in Chomsky Normal Form, i.e.
every production is in the form N ? UW with
N,U,W ? V or N ? a with a ? ?.
We will write uNv ?G u?v if there is a pro-
duction N ? ? ? P .
?
?G is the reflexive tran-
sitive closure of ?G. The language defined by
a CFG G is L(G) = {w ? ??|S
?
?G w}.
6.1 A Simple Characterization
A simple approach to try to represent a CFG
by a CBFG is to define a bijection between the
set of non terminals and the set of context fea-
tures. Informally we define each non terminal
by a single context and rewrite the productions
of the grammar in the CBFG form.
To build the set of contexts F , it is sufficient
to choose |V | contexts such that a bijection bC
can be defined between V and F with bC(N) =
(l, r) implies that S
?
? lNr. Note that we fix
bT (S) = (?, ?).
Then, we can define a CBFG
?F, P ?, P ?L,??, where P
? = {bT (N) ?
bT (U)bT (W )|N ? UW ? P} and
P ?L = {bT (N) ? a|N ? a ? P, a ? ?}.
A similar proof showing that this construction
produces an equivalent CBFG can be found
in (Clark et al, 2008).
If this approach allows a simple syntactical
convertion of a CFG into a CBFG, it is not
relevant from an NLP point of view. Though
we associate a non-terminal to a context, this
may not correspond to the intrinsic property
of the underlying language. A context could
be associated with many non-terminals and we
choose only one. For example, the context
(He is, ?) allows both noun phrases and ad-
jective phrases. In formal terms, the resulting
CBFG is not exact. Then, with the bijection
we introduced before, we are not able to char-
acterize the non-terminals by the contexts in
which they could appear. This is clearly what
we don?t want here and we are more interested
in the relationship with exact CBFG.
6.2 Not all CFLs have an exact CBFG
We will show here that the class of context-
free grammars is not strictly included in the
class of exact CBFGs. First, the grammar
defined in Section 3.2 is an exact CBFG for
the context-free and non regular language
{anbn|n > 0}, showing the class of exact
CBFG has some elements in the class of CFGs.
We give now a context-free language L that
can not be defined by an exact CBFG:
L = {anb|n > 0} ? {amcn|n > m > 0}.
Suppose that there exists an exact CBFG that
recognizes it and let N be the length of the
biggest feature (i.e. the longuest left part of
the feature). For any sufficiently large k >
N , the sequences ck and ck+1 share the same
features: FL(ck) = FL(ck+1). Since the CBFG
is exact we have FL(b) ? FL(ck). Thus any
derivation of ak+1b could be a derivation of
ak+1ck which does not belong to the language.
However, this restriction does not mean that
the class of exact CBFG is too restrictive for
modelling natural languages. Indeed, the ex-
ample we have given is highly unnatural and
such phenomena appear not to occur in at-
tested natural languages.
7 Context-Sensitive Languages
We now show that there are some exact
CBFGs that are not context-free. In particu-
lar, we define a language closely related to the
MIX language (consisting of strings with an
equal number of a?s, b?s and c?s in any order)
which is known to be non context-free, and
indeed is conjectured to be outside the class
of indexed grammars (Boullier, 2003).
38
Let M = {(a, b, c)?}, we consider the language
L = Labc?Lab?Lac?{a?a, b?b, c?c, dd?, ee?, ff ?}:
Lab = {wd|w ? M, |w|a = |w|b},
Lac = {we|w ? M, |w|a = |w|c},
Labc = {wf |w ? M, |w|a = |w|b = |w|c}.
In order to define a CBFG recognizing L, we
have to select features (contexts) that can rep-
resent exactly the intrinsic components of the
languages composing L. We propose to use the
following set of features for each sublanguages:
? For Lab: (?, d) and (?, ad), (?, bd).
? For Lac: (?, e) and (?, ae), (?, ce).
? For Labc: (?, f).
? For the letters a?, b?, c?, a, b, c we add:
(?, a), (?, b), (?, c), (a?, ?), (b?, ?), (c?, ?).
? For the letters d, e, f, d?, e?, f ? we add;
(?, d?), (?, e?), (?, f ?), (d, ?), (e, ?), (f, ?).
Here, Lab will be represented by (?, d), but we
will use (?, ad), (?, bd) to define the internal
derivations of elements of Lab. The same idea
holds for Lac with (?, e) and (?, ae), (?, ce).
For the lexical rules and in order to have an
exact CBFG, note the special case for a, b, c:
{(?, bd), (?, ce), (a?, ?)} ? a
{(?, ad), (b?, ?)} ? b
{(?, ad), (?, ae), (c?, ?)} ? c
For the nine other letters, each one is defined
with only one context like {(?, d?)} ? d.
For the production rules, the most impor-
tant one is: (?, ?) ? {(?, d), (?, e)}, {(?, f ?)}.
Indeed, this rule, with the presence of two
contexts in one of categories, means that an
element of the language has to be derived
so that it has a prefix u such that fG(u) ?
{(?, d), (?, e)}. This means u is both an ele-
ment of Lab and Lac. This rule represents the
language Labc since {(?, f ?)} can only repre-
sent the letter f .
The other parts of the language will be
defined by the following rules:
(?, ?) ? {(?, d)}, {(?, d?)},
(?, ?) ? {(?, e)}, {(?, e?)},
(?, ?) ? {(?, a)}, {(?, bd), (?, ce), (a?, ?)},
(?, ?) ? {(?, b)}, {(?, ad), (b?, ?)},
(?, ?) ? {(?, c)}, {(?, ad), (?, ae), (c?, ?)},
(?, ?) ? {(?, d?)}, {(d, ?)},
(?, ?) ? {(?, e?)}, {(e, ?)},
(?, ?) ? {(?, f ?)}, {(f, ?)}.
This set of rules is incomplete, since for rep-
resenting Lab, the grammar must contain the
rules ensuring to have the same number of a?s
and b?s, and similarly for Lac. To lighten the
presentation here, the complete grammar is
presented in Annex.
We claim this is an exact CBFG for a
context-sensitive language. L is not context-
free since if we intersect L with the regular
language {??d}, we get an instance of the
non context-free MIX language (with d ap-
pended). The exactness comes from the fact
that we chose the contexts in order to ensure
that strings belonging to a sublanguage can
not belong to another one and that the deriva-
tion of a substring will provide all the possible
correct features with the help of the union of
all the possible derivations.
Note that the Mix language on its own is
probably not definable by an exact CBFG: it
is only when other parts of the language can
distributionally define the appropriate partial
structures that we can get context sensitive
languages. Far from being a limitation of this
formalism (a bug), we argue this is a feature:
it is only in rather exceptional circumstances
that we will get properly context sensitive lan-
guages. This formalism thus potentially ac-
counts not just for the existence of non context
free natural language but also for their rarity.
8 Conclusion
The chart in Figure 3 summarises the different
relationship shown in this paper. The substi-
tutable languages (Clark and Eyraud, 2007)
and the very simple ones (Yokomori, 2003)
form two different learnable class of languages.
There is an interesting relationship with Mar-
cus External Contextual Grammars (Mitrana,
2005): if we defined the language of a CBFG
to be the set {fG(u)  u : u ? ??} we would
be taking some steps towards contextual gram-
mars.
In this paper we have discussed the weak
generative power of Exact Contextual Binary
Feature Grammars; we conjecture that the
class of natural language stringsets lie in this
class. ECBFGs are efficiently learnable (see
(Clark et al, 2008) for details) which is a com-
39
Context-free                                                   
Regular                        
Context sensitive
very simple
substi-tutable
Range Concatenation                         
                          Conjunctive = CBFG                                Exact CBFG
                   
Figure 3: The relationship between CBFG and
other classes of languages.
pelling technical advantage of this formalism
over other more traditional formalisms such as
CFGs or TAGs.
References
Pierre Boullier. 2000. A Cubic Time Extension
of Context-Free Grammars. Grammars, 3:111?
131.
Pierre Boullier. 2003. Counting with range con-
catenation grammars. Theoretical Computer
Science, 293(2):391?416.
Alexander Clark and Re?mi Eyraud. 2007. Polyno-
mial identification in the limit of substitutable
context-free languages. Journal of Machine
Learning Research, 8:1725?1745, Aug.
Alexander Clark, Re?mi Eyraud, and Amaury
Habrard. 2008. A polynomial algorithm for the
inference of context free languages. In Proceed-
ings of International Colloquium on Grammati-
cal Inference, pages 29?42. Springer, September.
V. Mitrana. 2005. Marcus external contextual
grammars: From one to many dimensions. Fun-
damenta Informaticae, 54:307?316.
Alexander Okhotin. 2001. Conjunctive grammars.
J. Autom. Lang. Comb., 6(4):519?535.
Alexander Okhotin. 2003. An overview of con-
junctive grammars. Formal Language Theory
Column, bulletin of the EATCS, 79:145?163.
Takashi Yokomori. 2003. Polynomial-time iden-
tification of very simple grammars from pos-
itive data. Theoretical Computer Science,
298(1):179?206.
Annex
(?, ?) ? {(?, d), (?, e)}, {(?, f ?)}
(?, ?) ? {(?, d)}, {(?, d?)}
(?, ?) ? {(?, e)}, {(?, e?)}
(?, ?) ? {(?, a)}, {(?, bd), (?, ce), (a?, ?)}
(?, ?) ? {(?, b)}, {(?, ad), (b?, ?)}
(?, ?) ? {(?, c)}, {(?, ad), (?, ae), (c?, ?)}
(?, ?) ? {(?, d?)}, {(d, ?)}
(?, ?) ? {(?, e?)}, {(e, ?)}
(?, ?) ? {(?, f ?)}, {(f, ?)}
(?, d) ? {(?, d)}, {(?, d)}
(?, d) ? {(?, ad)}, {(?, bd)}
(?, d) ? {(?, bd)}, {(?, ad)}
(?, d) ? {(?, d)}, {(?, ad), (?, ae), (c?, ?)}
(?, d) ? {(?, ad), (?, ae), (c?, ?)}, {(?, d)}
(?, ad) ? {(?, ad), (?, ae), (c?, ?)}, {(?, ad)}
(?, ad) ? {(?, ad)}, {(?, ad), (?, ae), (c?, ?)}
(?, ad) ? {(?, ad), (b?, ?)}, {(?, d)}
(?, ad) ? {(?, d)}, {(?, ad), (b?, ?)}
(?, bd) ? {(?, ad), (?, ae), (c?, ?)}, {(?, bd)}
(?, bd) ? {(?, bd)}, {(?, ad), (?, ae), (c?, ?)}
(?, bd) ? {(?, bd), (?, ce), (a?, ?)}, {(?, d)}
(?, bd) ? {(?, d)}, {(?, bd), (?, ce), (a?, ?)}
(?, e) ? {(?, e)}, {(?, e)}
(?, e) ? {(?, ae)}, {(?, ce)}
(?, e) ? {(?, ce)}, {(?, ae)}
(?, e) ? {(?, e)}, {(?, ad), (b?, ?)}
(?, e) ? {(?, ad), (b?, ?)}, {(?, e)}
(?, ae) ? {(?, ad), (b?, ?)}, {(?, ae)}
(?, ae) ? {(?, ae)}, {(?, ad), (b?, ?)}
(?, ae) ? {(?, ad), (?, ae), (c?, ?)}, {(?, e)}
(?, ae) ? {(?, e)}, {(?, ad), (?, ae), (c?, ?)}
(?, ce) ? {(?, ad), (b?, ?)}, {(?, ce)}
(?, ce) ? {(?, ce)}, {(?, ad), (b?, ?)}
(?, ce) ? {(?, bd), (?, ce), (a?, ?)}, {(?, e)}
(?, ce) ? {(?, e)}, {(?, bd), (?, ce), (a?, ?)}
{(?, bd), (?, ce), (a?, ?)} ? a
{(?, ad), (b?, ?)} ? b
{(?, ad), (?, ae), (c?, ?)} ? c
{(?, d?)} ? d
{(?, e?)} ? e
{(?, f ?)} ? f
{(?, a)} ? a?
{(?, b)} ? b?
{(?, c)} ? c?
{(d, ?)} ? d?
{(e, ?)} ? e?
{(f, ?)} ? f ?
40
Unsupervised Induction of Stochastic Context-Free Grammars using
Distributional Clustering
Alexander Clark
Cognitive and Computing Sciences,
University of Sussex,
Brighton BN1 9QH,
United Kingdom
alexc@cogs.susx.ac.uk
ISSCO / ETI,
University of Geneva,
UNI-MAIL, Boulevard du Pont-d?Arve, 40
CH-1211 Gene`ve 4,
Switzerland
Abstract
An algorithm is presented for learning a
phrase-structure grammar from tagged
text. It clusters sequences of tags to-
gether based on local distributional in-
formation, and selects clusters that sat-
isfy a novel mutual information crite-
rion. This criterion is shown to be re-
lated to the entropy of a random vari-
able associated with the tree structures,
and it is demonstrated that it selects lin-
guistically plausible constituents. This
is incorporated in a Minimum Descrip-
tion Length algorithm. The evaluation
of unsupervised models is discussed,
and results are presented when the al-
gorithm has been trained on 12 million
words of the British National Corpus.
1 Introduction
In this paper I present an algorithm using con-
text distribution clustering (CDC) for the un-
supervised induction of stochastic context-free
grammars (SCFGs) from tagged text. Previ-
ous research on completely unsupervised learn-
ing has produced poor results, and as a result re-
searchers have resorted to mild forms of super-
vision. Magerman and Marcus(1990) use a dis-
tituent grammar to eliminate undesirable rules.
Pereira and Schabes(1992) use partially bracketed
corpora and Carroll and Charniak(1992) restrict
the set of non-terminals that may appear on the
right hand side of rules with a given left hand
side. The work of van Zaanen (2000) does not
have this problem, and appears to perform well on
small data sets, but it is not clear whether it will
scale up to large data sets. Adriaans et al (2000)
presents another algorithm but its performance on
authentic natural language data appears to be very
limited.
The work presented here can be seen as one
more attempt to implement Zellig Harris?s distri-
butional analysis (Harris, 1954), the first such at-
tempt being (Lamb, 1961).
The rest of the paper is arranged as follows:
Section 2 introduces the technique of distribu-
tional clustering and presents the results of a pre-
liminary experiment. Section 3 discusses the
use of a novel mutual information (MI) criterion
for filtering out spurious candidate non-terminals.
Section 4 shows how this criterion is related to
the entropy of a certain random variable, and Sec-
tion 5 establishes that it does in fact have the de-
sired effect. This is then incorporated in a Min-
imum Description Length (MDL) algorithm out-
lined in Section 6. I discuss the difficulty of eval-
uating this sort of unsupervised algorithm in Sec-
tion 7, and present the results of the algorithm
on the British National Corpus (BNC). The pa-
per then concludes after a discussion of avenues
for future research in Section 8.
2 Distributional clustering
Distributional clustering has been used in many
applications at the word level, but as has been
noticed before (Finch et al, 1995), it can also
be applied to the induction of grammars. Sets
of tag sequences can be clustered together based
on the contexts they appear in. In the work here
I consider the context to consist of the part of
speech tag immediately preceding the sequence
and the tag immediately following it. The depen-
dency between these is critical, as we shall see, so
the context distribution therefore has
 
parame-
ters, where
 
is the number of tags, rather than
AT0 AJ0 NN0 AJ0 AJ0
AT0 AJ0 NN1 AJ0 CJC AJ0
AT0 AJ0 NN2 AV0 AJ0
AT0 AV0 AJ0 NN1 AV0 AV0 AJ0
AT0 NN0 ORD
AT0 NN1 PRP AT0 NN1
AT0 NN1
Table 1: Some of the more frequent sequences in
two good clusters
the 
 
parameters it would have under an inde-
pendence assumption. The context distribution
can be thought of as a distribution over a two-
dimensional matrix.
The data set for all the results in this paper con-
sisted of 12 million words of the British National
Corpus, tagged according to the CLAWS-5 tag
set, with punctuation removed.
There are 76 tags; I introduced an additional
tag to mark sentence boundaries. I operate ex-
clusively with tags, ignoring the actual words.
My initial experiment clustered all of the tag se-
quences in the corpus that occurred more than
5000 times, of which there were 753, using the   -
means algorithm with the  -norm or city-block
metric applied to the context distributions. Thus
sequences of tags will end up in the same cluster
if their context distributions are similar; that is to
say if they appear predominantly in similar con-
texts. I chose the cutoff of 5000 counts to be of
the same order as the number of parameters of the
distribution, and chose the number of clusters to
be 100.
To identify the frequent sequences, and to cal-
culate their distributions I used the standard tech-
nique of suffix arrays (Gusfield, 1997), which al-
lows rapid location of all occurrences of a desired
substring.
As expected, the results of the clustering
showed clear clusters corresponding to syntac-
tic constituents, two of which are shown in Ta-
ble 1. Of course, since we are clustering all
of the frequent sequences in the corpus we will
also have clusters corresponding to parts of con-
stituents, as can be seen in Table 2. We obvi-
ously would not want to hypothesise these as con-
stituents: we therefore need some criterion for fil-
tering out these spurious candidates.
AJ0 NN1 AT0 CJC AT0 AJ0
AJ0 NN1 PRF AT0 CJC AT0
AJ0 NN1 PRP AT0 CJC CRD
NN1 AT0 AJ0 CJC DPS
NN1 AT0 CJC PRP AT0
NN1 CJC AJ0 PRF AJ0
Table 2: Some of the sequencess in two bad clus-
ters
3 Mutual Information
The criterion I propose is that with real con-
stituents, there is high mutual information be-
tween the symbol occurring before the putative
constituent and the symbol after ? i.e. they are not
independent. Note that this is unrelated to Mager-
man and Marcus?s MI criterion which is the (gen-
eralised) mutual information of the sequence of
symbols itself. I will justify this in three ways ?
intuitively, mathematically and empirically.
Intuitively, a true constituent like a noun phrase
can appear in a number of different contexts. This
is one of the traditional constituent tests. A noun
phrase, for example, appears frequently either as
the subject or the object of a sentence. If it ap-
pears at the beginning of a sentence it is accord-
ingly quite likely to be followed by a finite verb.
If on the other hand it appears after the finite verb,
it is more likely to be followed by the end of the
sentence or a preposition. A spurious constituent
like PRP AT0 will be followed by an N-bar re-
gardless of where it occurs. There is therefore no
relation between what happens immediatly before
it, and what happens immediately after it. Thus
there will be a higher dependence or correlation
with the true constituent than with the erroneous
one.
4 Mathematical Justification
We can gain some insight into the significance of
the MI criterion by analysing it within the frame-
work of SCFGs. We are interested in looking
at the properties of the two-dimensional distribu-
tions of each non-terminal. The terminals are the
part of speech tags of which there are  . For each
terminal or non-terminal symbol  we define
four distributions, 
				 ,
over  or equivalently  -dimensional vectors.
Two of these, 	 and 	 are just the pre-
fix and suffix probability distributions for the
symbol(Stolcke, 1995): the probabilities that the
string derived from  begins (or ends) with a par-
ticular tag. The other two 
		 for left
distribution and right distribution, are the distri-
butions of the symbols before and after the non-
terminal. Clearly if  is a terminal symbol, the
strings derived from it are all of length 1, and thus
begin and end with  , giving 	 and 	 a
very simple form.
If we consider each non-terminal  in a SCFG,
we can associate with it two random variables
which we can call the internal and external vari-
ables. The internal random variable is the more
familiar and ranges over the set of rules expand-
ing that non-terminal. The external random vari-
able, Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 28?37,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Efficient, correct, unsupervised learning of context-sensitive languages
Alexander Clark
Department of Computer Science
Royal Holloway, University of London
alexc@cs.rhul.ac.uk
Abstract
A central problem for NLP is grammar in-
duction: the development of unsupervised
learning algorithms for syntax. In this pa-
per we present a lattice-theoretic represen-
tation for natural language syntax, called
Distributional Lattice Grammars. These
representations are objective or empiri-
cist, based on a generalisation of distribu-
tional learning, and are capable of repre-
senting all regular languages, some but not
all context-free languages and some non-
context-free languages. We present a sim-
ple algorithm for learning these grammars
together with a complete self-contained
proof of the correctness and efficiency of
the algorithm.
1 Introduction
Grammar induction, or unsupervised learning of
syntax, no longer requires extensive justification
and motivation. Both from engineering and cog-
nitive/linguistic angles, it is a central challenge
for computational linguistics. However good al-
gorithms for this task are thin on the ground.
There are numerous heuristic algorithms, some of
which have had significant success in inducing
constituent structure (Klein and Manning, 2004).
There are algorithms with theoretical guarantees
as to their correctness ? such as for example
Bayesian algorithms for inducing PCFGs (John-
son, 2008), but such algorithms are inefficient: an
exponential search algorithm is hidden in the con-
vergence of the MCMC samplers. The efficient
algorithms that are actually used are heuristic ap-
proximations to the true posteriors. There are al-
gorithms like the Inside-Outside algorithm (Lari
and Young, 1990) which are guaranteed to con-
verge efficiently, but not necessarily to the right
answer: they converge to a local optimum that
may be, and in practice nearly always is very far
from the optimum. There are naive enumerative
algorithms that are correct, but involve exhaus-
tively enumerating all representations below a cer-
tain size (Horning, 1969). There are no correct
and efficient algorithms, as there are for parsing,
for example.
There is a reason for this: from a formal point
of view, the problem is intractably hard for the
standard representations in the Chomsky hierar-
chy. Abe and Warmuth (1992) showed that train-
ing stochastic regular grammars is hard; Angluin
and Kharitonov (1995) showed that regular gram-
mars cannot be learned even using queries; these
results obviously apply also to PCFGs and CFGs
as well as to the more complex representations
built by extending CFGs, such as TAGs and so
on. However, these results do not necessarily ap-
ply to other representations. Regular grammars
are not learnable, but deterministic finite automata
are learnable under various paradigms (Angluin,
1987). Thus it is possible to learn by changing to
representations that have better properties: in par-
ticular DFAs are learnable because they are ?ob-
jective?; there is a correspondence between the
structure of the language, (the residual languages)
and the representational primitives of the formal-
ism (the states) which is expressed by the Myhill-
Nerode theorem.
In this paper we study the learnability of a class
of representations that we call distributional lat-
tice grammars (DLGs). Lattice-based formalisms
were introduced by Clark et al (2008) and Clark
(2009) as context sensitive formalisms that are po-
tentially learnable. Clark et al (2008) established
a similar learnability result for a limited class of
context free languages. In Clark (2009), the ap-
proach was extended to a significantly larger class
but without an explicit learning algorithm. Most of
the building blocks are however in place, though
we need to make several modifications and ex-
28
tensions to get a clean result. Most importantly,
we need to replace the representation used there,
which naively could be exponential, with a lazy,
exemplar based model.
In this paper we present a simple algorithm
for the inference of these representations and
prove its correctness under the following learning
paradigm: we assume that as normal there is a sup-
ply of positive examples, and additionally that the
learner can query whether a string is in the lan-
guage or not (an oracle for membership queries).
We also prove that the algorithm is efficient in
the sense that it will use a polynomial amount of
computation and makes a polynomial number of
queries at each step.
The contributions of this paper are as follows:
after some basic discussion of distributional learn-
ing in Section 2, we define in Section 3 an
exemplar-based grammatical formalism which we
call Distributional Lattice Grammars. We then
give a learning algorithm under a reasonable learn-
ing paradigm, together with a self contained proof
in elementary terms (not presupposing any exten-
sive knowledge of lattice theory), of the correct-
ness of this algorithm.
2 Basic definitions
We now define our notation; we have a finite al-
phabet ?; let ?? be the set of all strings (the free
monoid) over ?, with ? the empty string. A (for-
mal) language is a subset of ??. We can concate-
nate two languagesA andB to getAB = {uv|u ?
A, b ? B}.
A context or environment, as it is called in struc-
turalist linguistics, is just an ordered pair of strings
that we write (l, r) where l and r refer to left and
right; l and r can be of any length. We can com-
bine a context (l, r) with a string u with a wrap-
ping operation that we write : so (l, r)  u is
defined to be lur. We will sometimes write f for
a context (l, r). There is a special context (?, ?):
(?, ?)  w = w. We will extend this to sets of
contexts and sets of strings in the natural way. We
will write Sub(w) = {u|?(l, r) : lur = w} for
the set of substrings of a string, and Con(w) =
{(l, r)|?u ? ?? : lur = w}.
For a given string w we can define the distribu-
tion of that string to be the set of all contexts that it
can appear in: CL(w) = {(l, r)|lwr ? L}, equiv-
alently {f |f  w ? L}. Clearly (?, ?) ? CL(w)
iff w ? L.
Distributional learning (Harris, 1954) as a tech-
nical term refers to learning techniques which
model directly or indirectly properties of the dis-
tribution of strings or words in a corpus or a lan-
guage. There are a number of reasons to take
distributional learning seriously: first, historically,
CFGs and other PSG formalisms were intended
to be learnable by distributional means. Chomsky
(2006) says (p. 172, footnote 15):
The concept of ?phrase structure
grammar? was explicitly designed to ex-
press the richest system that could rea-
sonably be expected to result from the
application of Harris-type procedures to
a corpus.
Second, empirically we know they work well at
least for lexical induction, (Schu?tze, 1993; Cur-
ran, 2003) and are a component of some imple-
mented unsupervised learning systems (Klein and
Manning, 2001). Linguists use them as one of the
key tests for constituent structure (Carnie, 2008),
and finally there is some psycholinguistic evidence
that children are sensitive to distributional struc-
ture, at least in artificial grammar learning tasks
(Saffran et al, 1996). These arguments together
suggest that distributional learning has a some-
what privileged status.
3 Lattice grammars
Clark (2009) presents the theory of lattice based
formalisms starting algebraically from the theory
of residuated lattices. Here we will largely ig-
nore this, and start from a straightforward com-
putational treatment. We start by defining the rep-
resentation.
Definition 1. Given a non-empty finite alphabet,
?, a distributional lattice grammar (DLG) is a 3-
tuple consisting of ?K,D,F ?, where F is a finite
subset of ?? ? ??, such that (?, ?) ? F , K is a
finite subset of ?? which contains ? and ?, and D
is a subset of (F KK).
K here can be thought of as a finite set of exem-
plars, which correspond to substrings or fragments
of the language. F is a set of contexts or fea-
tures, that we will use to define the distributional
properties of these exemplars; finally D is a set
of grammatical strings, the data; a finite subset of
the language. F KK using the notation above is
{luvr|u, v ? K, (l, r) ? F}. This is the finite part
of the language that we examine. If the language
29
we are modeling is L, then D = L ? (F KK).
Since ? ? K,K ? KK.
We define a concept to be an ordered pair ?S,C?
where S ? K and C ? F , which satisfies the fol-
lowing two conditions: first C  S ? D; that is
to say every string in S can be combined with any
context in C to give a grammatical string, and sec-
ondly they are maximal in that neither K nor F
can be increased without violating the first condi-
tion.
We define B(K,D,F ) to be the set of all such
concepts. We use theB symbol (Begriff ) to bring
out the links to Formal Concept Analysis (Ganter
and Wille, 1997; Davey and Priestley, 2002). This
lattice may contain exponentially many concepts,
but it is clearly finite, as the number of concepts is
less than min(2|F |, 2|K|).
There is an obvious partial order defined by
?S1, C1? ? ?S2, C2? iff S1 ? S2, Note that
S1 ? S2 iff C2 ? C1.
Given a set of strings S we can define a set of
contexts S? to be the set of contexts that appear
with every element of S.
S? = {(l, r) ? F : ?w ? S, lwr ? D}
Dually we can define for a set of contexts C the
set of strings C ? that occur with all of the elements
of C:
C ? = {w ? K : ?(l, r) ? C, lwr ? D}
The concepts ?S,C? are just the pairs that sat-
isfy S? = C and C ? = S; the two maps denoted
by ? are called the polar maps. For any S ? K,
S??? = S? and for any C ? F , C ??? = C ?. Thus we
can form a concept from any set of strings S ? K
by taking ?S??, S??; this is a concept as S??? = S?.
We will write this as C(S), and for any C ? F ,
we will write C(C) = ?C ?, C ???.
If S ? T then T ? ? S?, and S?? ? T ??. For any
set of strings S ? K, S ? S??.
One crucial concept here is the concept de-
fined by (?, ?) or equivalently by the set K ? D
which corresponds to all of the elements in the
language. We will denote this concept by L =
C({(?, ?)}) = C(K ?D).
We also define a meet operation by
?S1, C1? ? ?S2, C2? = ?S1 ? S2, (S1 ? S2)
??
This is the greatest lower bound of the two con-
cepts; this is a concept since if S??1 = S1 and
S??2 = S2 then (S1 ? S2)
?? = (S1 ? S2). Note that
this operation is associative and commutative. We
can also define a join operation dually; with these
operationsB(K,D,D) is a complete lattice.
So far we have only used strings in F K; we
now define a concatenation operation as follows.
?S1, C1? ? ?S2, C2? = ?(S1S2)
??, (S1S2)
????
Since S1 and S2 are subsets of K, S1S2 is a sub-
set of KK, but not necessarily of K. (S1S2)? is
the set of contexts shared by all elements of S1S2
and (S1S2)?? is the subset of K, not KK, that
has all of the contexts of (S1S2)?. (S1S2)??? might
be larger than (S1S2)?. We can also write this as
C((S1S2)?).
Both ? and ? are monotonic in the sense that if
X ? Y then X ? Z ? Y ? Z, Z ? X ? Z ? Y
and X ? Z ? Y ? Z. Note that all of these oper-
ations can be computed efficiently; using a perfect
hash, and a naive algorithm, we can do the polar
maps and ? operations in timeO(|K||F |), and the
concatenation in time O(|K|2|F |).
We now define the notion of derivation in this
representation. Given a string w we recursively
compute a concept for every substring of w; this
concept will approximate the distribution of the
string. We define ?G as a function from ?? to
B(K,D,F ); we define it recursively:
? If |w| ? 1, then ?G(w) = ?{w}??, {w}??
? If |w| > 1 then
?G(w) =
?
u,v??+:uv=w ?G(u) ? ?G(v)
The first step is well defined because all of the
strings of length at most 1 are already in K so
we can look them up directly. To clarify the sec-
ond step, if w = abc then ?G(abc) = ?G(a) ?
?G(bc) ? ?G(ab) ? ?G(c); we compute the string
from all possible non-trivial splits of the string
into a prefix and a suffix. By using a dynamic
programming table that stores the values of ?(u)
for all u ? Sub(w) we can compute this in time
O(|K|2|F ||w|3); this is just an elementary variant
of the CKY algorithm. We define the language de-
fined by the DLG G to be
L(G) = {w|?G(w) ? C({(?, ?)})}
That is to say, a string is in the language if we
predict that a string has the context (?, ?). We now
consider a trivial example: the Dyck language.
30
Example 1. Let L be the Dyck language (matched
parenthesis language) over ? = {a, b}, where
a corresponds to open bracket, and b to close
bracket. Define
? K = {?, a, b, ab}
? F = {(?, ?), (?, b), (a, ?)}.
? D = {?, ab, abab, aabb}
G = ?K,D,F ? is a DLG. We will now write down
the 5 elements of the lattice:
? > = ?K, ??
? ? = ??, F ?
? L = ?{?, ab}, {(?, ?)}?
? A = ?{a}, {(?, b)}?
? B = ?{b}, {(a, ?)}?
To compute the concatenation A ? B we first
compute {a}{b} = {ab}; we then compute {ab}?
which is {(?, ?)}, and {(?, ?)}? = {?, ab}, so
A ? B = L. Similarly to compute L ? L, we first
take {?, ab}{?, ab} = {?, ab, abab}. These all
have the context (?, ?), so the result is the con-
cept L. If we compute A ?A we get {a}{a} which
is {aa} which has no contexts so the result is >.
We have ?G(?) = L, ?G(a) = A,?G(b) = B.
Applying the recursive computation we can verify
that ?G(w) = L iff w ? L and so L(G) = L. We
can also see that D = L ? (F KK).
4 Search
In order to learn these grammars we need to find a
suitable set of contexts F , a suitable set of strings
K, and then work out which elements of F KK
are grammatical. So given a choice for K and F
it is easy to learn these models under a suitable
regime: the details of how we collect information
about D depend on the learning model.
The question is therefore whether it is easy
to find suitable sets, K and F . Because of the
way the formalism is designed, it transpires that
the search problem is entirely tractable. In or-
der to analyse the search space, we define two
maps between the lattices as K and F are in-
creased. We are going to augment our notation
slightly; we will write B(K,L, F ) for B(K,L ?
(FKK), F ) and similarly ?K,L, F ? for ?K,L?
(F KK), F ?. When we use the two polar maps
(such as C ?, S?), though we are dealing with more
than one lattice, there is no ambiguity as the maps
agree; we will when necessary explicitly restrict
the output (e.g. C ? ? J) to avoid confusion.
Definition 2. For any language L and any set of
contexts F ? G, and any sets of strings J ?
K ? ??. We define a map g from B(J, L, F ) to
B(K,L, F ) (from the smaller lattice to the larger
lattice) as g(?S,C?) = ?C ?, C?.
We also define a map f from B(K,L,G)
to B(K,L, F ), (from larger to smaller) as
f(?S,C?) = ?(C ? F )?, C ? F ?.
These two maps are defined in opposite direc-
tions: this is because of the duality of the lattice.
By defining them in this way, as we will see, we
can prove that these two maps have very similar
properties. We can verify that the outputs of these
maps are in fact concepts.
We now need to define two monotonicity lem-
mas: these lemmas are crucial to the success of
the formalism. We show that as we increase K
the language defined by the formalism decreases
monotonically, and that as we increase F the lan-
guage increases monotonically. There is some du-
plication in the proofs of the two lemmas; we
could prove them both from more abstract prop-
erties of the maps f, g which are what are called
residual maps, but we will do it directly.
Lemma 1. Given two lattices B(K,L, F ) and
B(K,L,G) where F ? G; For all X,Y ?
B(K,L,G) we have that
1. f(X) ? f(Y ) ? f(X ? Y )
2. f(X) ? f(Y ) ? f(X ? Y )
Proof. The proof is elementary but difficult to
read. We write X = ?SX , CX? and similarly for
Y . For part 1 of the lemma: Clearly (S?X ? F ) ?
S?X , so (S
?
X ? F )
? ? S??X = SX and the same for
SY . So (S?X ?F )
?(S?Y ?F )
? ? SXSY (as subsets
ofKK). So ((S?X?F )
?(S?Y ?F )
?)? ? (SXSY )? ?
(SXSY )???. Now by definition, f(X) ? f(Y ) is
C(Z) where Z = ((S?X ?F )
?(S?Y ?F )
?)? ?F and
f(X ?Y ) has the set of contexts ((SXSY )??? ?F ).
Therefore f(X ? Y ) has a bigger set of contexts
than f(X) ? f(Y ) and is thus a smaller concept.
For part 2: by definition f(X ? Y ) = ?((SX ?
Sy)? ? F )?, (SX ? Sy)? ? F ? and f(X) ? f(Y ) =
?(S?X?F )
??(S?y?F )
?, ((S?X?F )
??(S?y?F )
?)??F ?
Now S?X ? F ? S
?
X , so (since S
??
X = SX ) SX ?
(S?X?F )
?, and so SX?Sy ? (S?X?F )
??(S?y?F )
?.
31
So (SX ? Sy)? ? ((S?X ? F )
? ? (S?y ? F )
?) which
gives the result by comparing the context sets of
the two sides of the inequality.
Lemma 2. For any language L, and two sets of
contexts F ? G, and any K, if we have two DLGs
?K,L, F ? with map ?F : ?? ? B(K,L, F ) and
?K,L,G? with map ?G : ?? ? B(K,L,G) then
for all w, f(?G(w)) ? ?F (w).
Proof. By induction on the length of w; clearly
if |w| ? 1, f(?G(w)) = ?F (w). We now take
the inductive step; by definition, (suppressing the
definition of u, v in the meet)
f(?G(w)) = f(
?
u,v
?G(u) ? ?G(v))
By Lemma 1, part 2:
f(?G(w)) ?
?
u,v
f(?G(u) ? ?G(v))
By Lemma 1, part 1:
f(?G(w)) ?
?
u,v
f(?G(u)) ? f(?G(v))
By the inductive hypothesis we have f(?G(u)) ?
?F (u) and similarly for v and so by the mono-
tonicity of ? and ?:
f(?G(w)) ?
?
u,v
?F (u) ? ?F (v)
Since the right hand side is equal to ?F (w), the
proof is done.
It is then immediate that
Lemma 3. If F ? G then L(?K,L, F ?) ?
L(?K,L,G?),
Proof. If w ? L(?K,L, F ?), then ?F (w) ? L,
and so f(?G(w)) ? L and so ?G(w) has the con-
text (?, ?) and is thus in L(?K,L,G?).
We now prove the corresponding facts about g.
Lemma 4. For any J ? K and any conceptsX,Y
inB(J, L, F ), we have that
1. g(X) ? g(Y ) ? g(X ? Y )
2. g(X) ? g(Y ) ? g(X ? Y )
Proof. For the first part: Write X = ?SX , CX? as
before. Note that SX = C ?X ? J . SX ? C
?
X , so
SXSY ? C ?XC
?
Y , and so (SXSY )
?? ? (C ?XC
?
Y )
??,
and ((SXSY )?? ? J)? ? (C ?XC
?
Y )
???. By calcu-
lation g(X) ? g(Y ) = ?(C ?XC
?
Y )
??, (C ?XC
?
Y )
????
On the other hand, g(X ? Y ) = ?((SXSY )?? ?
J)??, ((SXSY )?? ? J)?? and so g(X ? Y ) is smaller
as it has a larger set of contexts.
For the second part: g(X) ? g(Y ) = ?C ?X ?
C ?Y , (C
?
X ? C
?
Y )
?? and g(X ? Y ) = ?(SX ?
SY )??, (SX ? SY )??. Since SX = C ?X ? J , SX ?
C ?X , so (SX ? SY ) ? C
?
X ? C
?
Y , and therefore
(SX ? SY )?? ? (C ?X ? C
?
Y )
?? = C ?X ? C
?
Y .
We now state and prove the monotonicity
lemma for g.
Lemma 5. For all J ? K ? ?????, and for all
strings w; we have that g(?J(w)) ? ?K(w).
Proof. By induction on length of w. Both J and
K include the basic elements of ? and ?. First
suppose |w| ? 1, then ?J(w) = ?(CL(w) ?
F )??J,CL(w)?F ?, and g(?J(w)) = ?(CL(w)?
F )?, CL(w) ? F ? which is equal to ?K(w).
Now suppose true for all w of length at most k,
and take some w of length k + 1. By definition of
?J :
g(?J(w)) = g
(
?
u,v
?J(u) ? ?J(v)
)
Next by Lemma 4, Part 2
g(?J(w)) ?
?
u,v
g(?J(u) ? ?J(v))
By Lemma 4, Part 1
g(?J(w)) ?
?
u,v
g(?J(u)) ? g(?J(v))
By the inductive hypothesis and monotonicity of ?
and ?:
g(?J(w)) ?
?
u,v
?K(u) ? ?K(v) = ?K(w)
Lemma 6. If J ? K then L(?J, L, F ?) ?
L(?K,L, F ?)
Proof. Suppose w ? L(?K,L, F ?). this means
that ?K(w) ? LK . therefore g(?J(w)) ?
Lk; which means that (?, ?) is in the concept
g(?J(w)), which means it is in the concept ?J(w),
and therefore w ? L(?J, L, F ?).
32
Given these two lemmas we can make the fol-
lowing observations. First, if we have a fixed L
and F , then as we increase K, the language will
decrease until it reaches a limit, which it will at-
tain after a finite limit.
Lemma 7. For all L, and finite context sets F ,
there is a finite K such that for all K2, K ? K2,
L(?K,L, F ?) = L(?K2, L, F ?).
Proof. We can define the latticeB(??, L, F ). De-
fine the following equivalence relation between
pairs of strings, where (u1, v1) ? (u2, v2) iff
C(u1) = C(u2) and C(v1) = C(v2) and C(u1v1) =
C(u2v2). The number of equivalence classes is
clearly finite. If K is sufficiently large that there is
a pair of strings (u, v) in K for each equivalence
class, then clearly the lattice defined by thisK will
be isomorphic toB(??, L, F ). Any superset ofK
will not change this lattice.
Moreover this language is unique for each L,F .
We will call this the limit language ofL,F , and we
will write it as L(???, L, F ?).
If F ? G, then L(???, L, F ?) ?
L(???, L,G?). Finally, we will show that
the limit languages never overgeneralise.
Lemma 8. For any L, and for any F ,
L(???, L, F ?) ? L.
Proof. Recall that C(w) = ?{w}??, {w}?? is the
real concept. If G is a limit grammar, we can
show that we always have ?G(w) > C(w), which
will give us the result immediately. First note
that C(u) ? C(v) ? C(uv), which is immedi-
ate by the definition of ?. We proceed, again,
by induction on the length of w. For |w| ? 1,
?G(w) = C(w). For the inductive step we have
?G(w) =
?
u,v ?G(u) ? ?G(v); by inductive hy-
pothesis we have that this must be more than
?
u,v C(u) ? C(v) >
?
u,v C(uv) = C(w)
5 Weak generative power
First we make the following observation: if we
consider an infinite variant of this, where we set
K = ?? and F = ?? ? ?? and D = L, we
can prove easily that, allowing infinite ?represen-
tations?, for any L, L(?K,D,F ?) = L. In this
infinite data limit, ? becomes associative, and the
structure ofB(K,D,F ) becomes a residuated lat-
tice, called the syntactic concept lattice of the lan-
guage L, B(L). This lattice is finite iff the lan-
guage is regular. The fact that this lattice now has
residuation operations suggest interesting links to
the theory of categorial grammar. It is the finite
case that interests us.
We will use LDLG to refer to the class of lan-
guages that are limit languages in the sense de-
fined above.
LDLG = {L|?F,L(??
?, L, F ?) = L}
Our focus in this paper is not on the language
theory: we present the following propositions.
First LDLG properly contains the class of regular
languages. Secondly LDLG contains some non-
context-free languages (Clark, 2009). Thirdly it
does not contain all context-free languages.
A natural question to ask is how to convert a
CFG into a DLG. This is in our view the wrong
question, as we are not interested in modeling
CFGs but modeling natural languages, but given
the status of CFGs as a default model for syn-
tactic structure, it will help to give a few exam-
ples, and a general mechanism. Consider a non-
terminal N in a CFG with start symbol S. We
can define C(N) = {(l, r)|S
?
? lNr} and the
yield Y (N) = {w|N
?
? w}. Clearly C(N) 
Y (N) ? L, but these are not necessarily maxi-
mal, and thus ?C(N), Y (N)? is not necessarily a
concept. Nonetheless in most cases, we can con-
struct a grammar where the non-terminals will cor-
respond to concepts, in this way.
The basic approach is this: for each non-
terminal, we identify a finite set of contexts that
will pick out only the set of strings generated
from that non-terminal: we find some set of con-
texts FN typically a subset of C(N) such that
Y (N) = {w|?(l, r) ? FN , lwr ? L}. We say
that we can contextually define this non-terminal
if there is such a finite set of contexts FN . If a
CFG in Chomsky normal form is such that every
non-terminal can be contextually defined then the
language defined by that grammar is in LDLG. If
we can do that, then the rest is trivial. We take
any set of features F that includes all of these FN ;
probably just F =
?
N FN ; we then pick a set of
strings K that is sufficiently large to rule out all
incorrect generalisations, and then define D to be
L ? (F KK).
Consider the language L = {anbncm|n,m ?
0} ? {ambncn|n,m ? 0}. L is a classic ex-
ample of an inherently ambiguous and thus non-
deterministic language.
The natural CFG in CNF for L has
non-terminals that generate the following
33
sets: {anbn|n ? 0}, {an+1bn|n ? 0},
{bncn|n ? 0}, {bncn+1|n ? 0}, {a?}
and {c?}. We note that the six contexts
(aa, bbc), (aa, bbbc), (abb, cc)(abbb, cc), (?, a)
and (c, ?) will define exactly these sets, in
the sense that the set of strings that oc-
cur in each context will be exactly the
corresponding set. We can also pick out
?, a, b, c with individual contexts. Let F =
{(?, ?), (aaabb, bccc), (aaabbc, ?), (?, abbccc),
(aaab, bccc), (aa, bbc), (aa, bbbc), (abb, cc),
(abbb, cc), (?, a), (c, ?)}. If we take a sufficiently
large set K, say ?, a, b, c, ab, aab, bc, bcc, abc, and
set D = L ? F KK, then we will have a DLG
for the language L. In this example, it is sufficient
to have one context per non-terminal. This is not
in general the case.
Consider L = {anbn|n ? 0} ? {anb2n|n ?
0}. Here we clearly need to identify sets of strings
corresponding to the two parts of this language,
but it is easy to see that no one context will suffice.
However, note that the first part is defined by the
two contexts (?, ?), (a, b) and the second by the
two contexts (?, ?), (a, bb). Thus it is sufficient to
have a set F that includes these four contexts, as
well as similar pairs for the other non-terminals in
the grammar, and some contexts to define a and b.
We can see that we will not always be able to do
this for every CFG. One fixable problem is if the
CFG has two separate non-terminals, M,N such
that C(M) ? C(N). If this is the case, then we
must have that Y (N) ? Y (M), If we pick a set
of contexts to define Y (N), then clearly any string
in Y (M) will also be picked out by the same con-
texts. If this is not the case, then we can clearly try
to rectify it by adding a rule N ? M which will
not change the language defined.
However, we cannot always pick out the non-
terminals with a finite set of contexts. Consider
the language L = {anb|n > 0} ? {ancm|m >
n > 0} defined in Clark et al (2008). Sup-
pose wlog that F contains no context (l, r) such
that |l| + |r| ? k. Then it is clear that we will
not be able to pick out b without also picking out
ck+1, since CL(ck+1) ? F ? CL(b) ? F . Thus
L, which is clearly context-free, is not in LDLG.
Luckily, this example is highly artificial and does
not correspond to any phenomena we are aware of
in linguistics.
In terms of representing natural languages, we
clearly will in many cases need more than one
context to pick out syntactically relevant groups
of strings. Using a very simplified example from
English, if we want to identify say singular noun
phrases, a context like (that is, ?) will not be suf-
ficient since as well as noun phrases we will also
have some adjective phrases. However if we in-
clude multiple contexts such as (?, is over there)
and so on, eventually we will be able to pick out
exactly the relevant set of strings. One of the
reasons we need to use a context sensitive repre-
sentation, is so that we can consider every possi-
ble combination of contexts simultaneously: this
would require an exponentially large context free
grammar.
6 Learning Model
In order to prove correctness of the learning algo-
rithm we will use a variant of Gold-style inductive
inference (Gold, 1967). Our choice of this rather
old-fashioned model requires justification. There
are two problems with learning ? the information
theoretic problems studied under VC-dimension
etc., and the computational complexity issues of
constructing a hypothesis from the data. In our
view, the latter problems are the key ones. Ac-
cordingly, we focus entirely on the efficiency is-
sue, and allow ourself a slightly unrealistic model;
see (Clark and Lappin, 2009) for arguments that
this is a plausible model.
We assume that we have a sequence of posi-
tive examples, and that we can query examples for
membership. Given a language L a presentation
for L is an infinite sequence of strings w1, w2, . . .
such that {wi|i ? N} = L. An algorithm receives
a sequence T and an oracle, and must produce a
hypothesis H at every step, using only a polyno-
mial number of queries to the membership oracle
? polynomial in the total size of the presentation.
It identifies in the limit the language L iff for ev-
ery presentation T of L there is a N such that for
all n > N Hn = HN , and L(HN ) = L. We say
it identifies in the limit a class of languages L iff
it identifies in the limit all L in L. We say that it
identifies the class in polynomial update time iff
there is a polynomial p, such that at each step the
model uses an amount of computation (and thus
also a number of queries) that is less than p(n, l),
where n is the number of strings and l is the max-
imum length of a string in the observed data. We
note that this is slightly too weak. It is possible
to produce vacuous enumerative algorithms that
34
can learn anything by only processing a logarith-
mically small prefix of the string (Pitt, 1989).
7 Learning Algorithm
We now define a simple learning algorithm, that
establishes learnability under this paradigm.
There is one minor technical detail we need to
deal with. We need to be able to tell when adding
a string to a lazy DLG will leave the grammar un-
changed. We use a slightly weaker test. Given
G1 = ?K,D,F ? we define as before the equiva-
lence relation between pairs of strings ofK, where
(u1, v1) ?G1 (u2, v2) iff CD(u1) = CD(u2) and
CD(v1) = CD(v2) and CD(u1v1) = CD(u2v2).
Note that CD(u) = {(l, r)|lur ? D}.
Given two grammars G1 = ?K,D,F ? and
G2 = ?K2, D2, F ? where K ? K2 and D ? D2
but F is unchanged, we say that these two are
indistinguishable iff the number of equivalence
classes of K ?K under ?G1 is equal to the num-
ber of equivalence classes of K2?K2 under?G2 .
This can clearly be computed efficiently using a
union-find algorithm, in time polynomial in |K|
and |F |. If they are indistinguishable then they de-
fine the same language.
7.1 Algorithm
Algorithm 1 presents the basic algorithm. At var-
ious points we compute sets of strings like (F 
KK)?L; these can be computed using the mem-
bership oracle.
First we prove that the program is efficient in
the sense that it runs in polynomial update time.
Lemma 9. There is a polynomial p, such that Al-
gorithm 1, for each wn, runs in time bounded by
p(n, l) where l is the maximum length of a string
in w1, . . . wn.
Proof. First we note that K,K2 and F are always
subsets of Sub(E)?? andCon(E), and thus both
|K| and |F | are bounded by nl(l+1)/2+ |?|+1.
Computing D is efficient as |F KK| is bounded
by |K|2|F |. We can compute ?G as mentioned
above in time |K|2|F |l3; distinguishability is as
observed earlier also polynomial.
Before we prove the correctness of the algo-
rithm we make some informal points. First, we
are learning under a rather pessimistic model ? the
positive examples may be chosen to confuse us,
so we cannot make any assumptions. Accordingly
we have to very crudely add all substrings and all
Algorithm 1: DLG learning algorithm
Data: Input strings S = {w1, w2 . . . , },
membership oracle O
Result: A sequence of DLGs G1, G2, . . .
K ? ? ? {?}, K2 = K ;
F ? {(?, ?)}, E = {} ;
D = (F KK) ? L ;
G = ?K,D,F ? ;
for wi do
E ? E ? {wi} ;
K2 ? K2 ? Sub(wi) ;
if there is some w ? E that is not in
L(G) then
F ? Con(E) ;
K ? K2 ;
D = (F KK) ? L ;
G = ?K,D,F ? ;
end
else
D2 ? (F K2K2) ? L ;
if ?K2, D2, F ? not indistinguishable
from ?K,D,F ? then
K ? K2 ;
D = (F KK) ? L ;
G = ?K,D,F ? ;
end
end
Output G;
end
contexts, rather than using sensible heuristics to
select frequent or likely ones.
Intuitively the algorithm works as follows: if we
observe a string not in our current hypothesis, then
we increase the set of contexts which will increase
the language defined. Since we only see positive
examples, we will never explicitly find out that our
hypothesis overgenerates, accordingly we always
add strings to a tester set K2 and see if this gives
us a more refined model. If this seems like it might
give a tighter hypothesis, then we increase K.
In what follows we will say that the hypothesis
at step n, Gn = ?Kn, Dn, Fn?, and the language
defined is Ln. We will assume that the target lan-
guage is some L ? LDLG and w1, . . . is a presen-
tation of L.
Lemma 10. Then there is a point n, and a finite set
of contexts F such that for all N > n, FN = F .,
and L(???, L, F ?) = L.
Proof. Since L ? LDLG there is some set of con-
35
texts G ? Con(L), such that L = L(???, L,G?).
Any superset ofG will define the correct limit lan-
guage. Let n be the smallest n such that G is a
subset of Con({w1, . . . , wn}). Consider Fn. If
Fn defines the correct limit language, then we will
never change F as the hypothesis will be a super-
set of the target. Otherwise it must define a subset
of the correct language. Then either there is some
N > n at which it has converged to the limit lan-
guage which will cause the first condition in the
loop to be satisfied and F will be increased to a
superset ofG, or F will be increased before it con-
verges, and thus the result holds.
Lemma 11. After F converges according to the
previous lemma, there is some n, such that for all
N > n, KN = Kn and L(?Kn, L, Fn?) = L.
Proof. let n0 be the convergence point of F ; for
all n > n0 the hypothesis will be a superset of
the target language; therefore the only change that
can happen is that K will increase. By definition
of the limit language, it must converge after a finite
number of examples.
Theorem 1. For every language L ? LDLG, and
every presentation of L, Algorithm 1 will converge
to a grammar G such that L(G) = L.
This result is immediate by the two preceding
lemmas.
8 Conclusion
We have presented an efficient, correct learning al-
gorithm for an interesting class of languages; this
is the first such learning result for a class of lan-
guages that is potentially large enough to describe
natural language.
The results presented here lack a couple of tech-
nical details to be completely convincing. In par-
ticular we would like to show that given a repre-
sentation of size n, we can learn once we have seen
a set of examples that is polynomially bounded by
n. This will be challenging, as the size of the K
we need to converge can be exponentially large
in F . We can construct DFAs where the num-
ber of congruence classes of the language is an
exponential function of the number of states. In
order to learn languages like this, we will need
to use a more efficient algorithm that can learn
even with ?insufficient? K: that is to say when
the lattice B(K,L, F ) has fewer elements that
B(KK,L, F ).
This algorithm can be implemented directly and
functions as expected on synthetic examples, but
would need modification to run efficiently on nat-
ural languages. In particular rather than consider-
ing whole contexts of the form (l, r) it would be
natural to restrict them just to a narrow window
of one or two words or tags on each side. Rather
than using a membership oracle, we could prob-
abilistically cluster the data in the table of counts
of strings in F  K. In practice we will have a
limited amount of data to work with and we can
control over-fitting in a principled way by control-
ling the relative size of K and F .
This formalism represents a process of anal-
ogy from stored examples, based on distributional
learning ? this is very plausible in terms of what
we know about cognitive processes, and is com-
patible with much non-Chomskyan theorizing in
linguistics (Blevins and Blevins, 2009). The class
of languages is a good fit to the class of natural
languages; it contains, as far as we can tell, all
standard examples of context free grammars, and
includes non-deterministic and inherently ambigu-
ous grammars. It is hard to say whether the class
is in fact large enough to represent natural lan-
guages; but then we don?t know that about any for-
malism, context-free or context-sensitive. All we
can say is that there are no phenomena that we are
aware of that don?t fit. Only large scale empirical
work can answer this question.
Ideologically these models are empiricist ? the
structure of the representation is based on the
structure of the data: this has to be a good thing
for computational modeling. By minimizing the
amount of hidden, unobservable structure, we can
improve learnability. Languages are enormously
complex, and it would be simplistic to try to re-
duce their acquisition to a few pages of mathe-
matics; nonetheless, we feel that the representa-
tions and grammar induction algorithms presented
in this paper could be a significant piece of the
puzzle.
36
References
N. Abe and M. K. Warmuth. 1992. On the computa-
tional complexity of approximating distributions by
probabilistic automata. Machine Learning, 9:205?
260.
D. Angluin and M. Kharitonov. 1995. When won?t
membership queries help? J. Comput. Syst. Sci.,
50:336?355.
D. Angluin. 1987. Learning regular sets from queries
and counterexamples. Information and Computa-
tion, 75(2):87?106.
James P. Blevins and Juliette Blevins. 2009. Analogy
in grammar: Form and acquisition. Oxford Univer-
sity Press.
A. Carnie. 2008. Constituent structure. Oxford Uni-
versity Press, USA.
Noam Chomsky. 2006. Language and mind. Cam-
bridge University Press, 3rd edition.
Alexander Clark and Shalom Lappin. 2009. Another
look at indirect negative evidence. In Proceedings of
the EACL Workshop on Cognitive Aspects of Com-
putational Language Acquisition, Athens, March.
Alexander Clark, Re?mi Eyraud, and Amaury Habrard.
2008. A polynomial algorithm for the inference of
context free languages. In Proceedings of Interna-
tional Colloquium on Grammatical Inference, pages
29?42. Springer, September.
Alexander Clark. 2009. A learnable representation
for syntax using residuated lattices. In Proceedings
of the 14th Conference on Formal Grammar, Bor-
deaux, France.
J.R. Curran. 2003. From distributional to semantic
similarity. Ph.D. thesis, University of Edinburgh.
B. A. Davey and H. A. Priestley. 2002. Introduction to
Lattices and Order. Cambridge University Press.
B. Ganter and R. Wille. 1997. Formal Concept Analy-
sis: Mathematical Foundations. Springer-Verlag.
E. M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447 ? 474.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146?62.
J. J. Horning. 1969. A Study of Grammatical Infer-
ence. Ph.D. thesis, Stanford University, Computer
Science Department, California.
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In 46th Annual Meeting of the ACL,
pages 398?406.
Dan Klein and Chris Manning. 2001. Distribu-
tional phrase structure induction. In Proceedings of
CoNLL 2001, pages 113?121.
Dan Klein and Chris Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of the 42nd
Annual Meeting of the ACL.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35?56.
L. Pitt. 1989. Inductive inference, dfa?s, and computa-
tional complexity. In K. P. Jantke, editor, Analogical
and Inductive Inference, number 397 in LNAI, pages
18?44. Springer-Verglag.
J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996.
Statistical learning by eight month old infants. Sci-
ence, 274:1926?1928.
Hinrich Schu?tze. 1993. Part of speech induction from
scratch. In Proceedings of the 31st annual meet-
ing of the Association for Computational Linguis-
tics, pages 251?258.
37
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 28?36,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Statistical Representation of Grammaticality Judgements: the Limits of
N-Gram Models
Alexander Clark, Gianluca Giorgolo, and Shalom Lappin
Department of Philosophy, King?s College London
firstname.lastname@kcl.ac.uk
Abstract
We use a set of enriched n-gram models to track
grammaticality judgements for different sorts of
passive sentences in English. We construct these
models by specifying scoring functions to map the
log probabilities (logprobs) of an n-gram model for
a test set of sentences onto scores which depend
on properties of the string related to the parame-
ters of the model. We test our models on classifica-
tion tasks for different kinds of passive sentences.
Our experiments indicate that our n-gram models
achieve high accuracy in identifying ill-formed pas-
sives in which ill-formedness depends on local rela-
tions within the n-gram frame, but they are far less
successful in detecting non-local relations that pro-
duce unacceptability in other types of passive con-
struction. We take these results to indicate some of
the strengths and the limitations of word and lexical
class n-gram models as candidate representations of
speakers? grammatical knowledge.
1 Introduction
Most advocates (Pereira, 2000; Bod et al, 2003)
and critics (Chomsky, 1957; Fong et al, 2013) of a
probabilistic view of grammatical knowledge have
assumed that this view identifies the grammatical
status of a sentence directly with the probability of
its occurrence. By contrast, we seek to character-
ize grammatical knowledge statistically, but with-
out reducing grammaticality directly to probabil-
ity. Instead we specify a set of scoring procedures
for mapping the logprob value of a sentence into
a relative grammaticality score, on the basis of the
properties of the sentence and of the logprobs that
an n-gram word model generates for the corpus
containing the sentence. A scoring procedure in
this set generates scores in terms of which we con-
struct a grammaticality classifier, using a param-
eterized standard deviation from the mean value.
The classifier provides a procedure for testing the
accuracy of different scoring criteria in separat-
ing grammatical from ungrammatical passive sen-
tences.
We evaluate this approach by applying it to
the task of distinguishing well and ill-formed sen-
tences with passive constructions headed by four
different sorts of verbs: intransitives (appear,
last), pseudo-transitives, which take a restricted
set of notional objects (laugh a hearty laugh,
weigh 10 kg), ambiguous transitives, which allow
both agentive and thematic subjects (the jeans /
the tailor fitted John), and robust transitives that
passivize freely (write, move). Intransitives and
pseudo-transitives generally yield ill-formed pas-
sives. Passives formed from ambiguous transitives
tend to be well-formed only on the agentive read-
ing. Robust transitives, for the most part, yield
acceptable passives, even if they are semantically
(or pragmatically) odd.
Experimenting with several scoring procedures
and alternative values for our standard deviation
parameter, we found that our classifier can distin-
guish pairwise between elements of the first two
classes of passives and those of the latter two with
a high degree of accuracy. However, its perfor-
mance is far less reliable in identifying the differ-
ence between ambiguous and robust transitive pas-
sives. The first classification task relies on local
lexical patterns that can be picked up by n-gram
models, while the second requires identification of
anomalous relations between passivized verbs and
by-phrases, which are not generally accessible to
measurement within the range of an n-gram.
We also observed that as we increased the size
of the training corpus, the performance of our en-
riched models on the classification task also in-
creased. This result suggests that better n-gram
language models are more sensitive to the sorts of
patterns that our scoring procedures rely on to gen-
erate accurate grammaticality classifications.
We note the important difference between
28
grammaticality and acceptability. Following stan-
dard assumptions, we take grammaticality to be
a theoretical notion, and acceptability to be an
empirically testable property. Acceptability is, in
part, determined by grammaticality, but also by
factors such as sentence length, processing limi-
tations, semantic acceptability and many other el-
ements. Teasing apart these two concepts, and ex-
plicating their precise relationship raises a host of
subtle methodological issues that we will not ad-
dress here. Oversimplifying somewhat, we are try-
ing to reconstruct a gradient notion of grammati-
cality which is derived from probabilistic models,
that can serve as a core component of a full model
of acceptability.
We distinguish our task from the standard task
of error detection in NLP (e.g. Post (2011)),
that can be used in various language processing
systems, such as machine translation (Pauls and
Klein, 2012), language modeling and so on. In
error detection, the problem is a supervised learn-
ing task. Given a corpus of examples labeled as
grammatical or ungrammatical, the problem is to
learn a classifier to distinguish them. We use su-
pervised learning as well, but only to measure the
upper bound of an unsupervised learning method.
We assume that native speakers do not, in general,
have access to systematic sets of ungrammatical
sentences that they can use to calibrate their judge-
ment of acceptability. Rather ungrammatical sen-
tences are unusual or unlikely. However, we use
some ungrammatical sentences to set an optimal
threshold for our scoring procedures.
2 Enriched N-Gram Language Models
We assume that we have some high quality lan-
guage model which defines a probability distri-
bution over whole sentences. As has often been
noted, it is not possible to reduce grammatical-
ity directly to a probability of this type, for sev-
eral reasons. First, if one merely specifies a fixed
probability value as a threshold for grammatical-
ity, where strings are deemed to be grammatical
if and only if their probability is higher than the
threshold, then one is committed to the existence
of only a finite number of grammatical sentences.
The probabilities of the possible strings of words
in a language sum to 1, and so at most 1/ sen-
tences can have a probability of at least . Second,
probability can be affected by factors that do not
influence grammaticality. For example, the word
?yak? is rarer (and therefore less probable) than the
word ?horse?, but this does not affect the relative
grammaticality of ?I saw a horse? versus ?I saw a
yak?. Third, a short ungrammatical sentence may
have a higher probability than a long grammatical
sentence with many rare words.
In spite of these arguments against a naive re-
duction of grammaticality, probabilistic inference
does play a role in linguistic judgements, as in-
dicated by the fact that they are often gradient.
Probabilistic inference is pervasive throughout all
domains of cognition (Chater et al, 2006), and
therefore it is plausible to assume that knowledge
of language is also probabilistic in nature. More-
over language models do seem to play a crucial
role in speech recognition and sentence process-
ing. Without them we would not be able to under-
stand speech in a noisy environment.
We propose to accommodate these different
considerations by using a scoring function to map
probabilities to grammaticality rankings. This
function does not apply directly to probabilities,
but rather to the parameters of the language model.
The probability of a particular sentence with re-
spect to a log-linear language model will be the
product of certain parameters: in log space, the
sum. We define scores that operate on this collec-
tion of parameters.
2.1 Scores
We have experimented with scores of two differ-
ent types that correlate with the grammaticality
of a sentence. Those of the first type are dif-
ferent implementations of the idea of normaliz-
ing the logprob assigned by an n-gram model to
a string by eliminating the significance of factors
that do not influence the grammatical status of a
sentence, such as sentence length and word fre-
quency. Scores of the second type are based on the
intuition that the (un)grammaticality of a sentence
is largely determined by its problematic compo-
nents. These scores are functions of the lowest
scoring n-grams in the sentence.
Mean logprob (ML) This score is the logprob
of the entire sentence divided by the length of the
sentence, or equivalently the mean of the logprobs
for the single trigrams:
ML = 1n logPTRIGRAM(?w1, . . . , wn?)
By normalizing the logprob for the entire sentence
by its length we eliminate the effect of sentence
length on the acceptability score.
29
Weighted mean logprob (WML) This score is
calculated by dividing the logprob of the entire
sentence by the sum of the unigram probabilities
of the lexical items that compose the sentence:
WML = logPTRIGRAM(?w1,...,wn?)logPUNIGRAM(?w1,...,wn?)
This score eliminates at the same time the effect of
the length of the sentence and the lower probabil-
ity assigned to sentences with rare lexical items.
Synctactic log odds ratio (SLOR) This score
was first used by Pauls and Klein (2012) and
performs a normalization very similar to WML
(we will see below that in fact the two scores are
basically equivalent):
SLOR =
logPTRIGRAM(?w1,...,wn?)?logPUNIGRAM(?w1,...,wn?)
n
Minimum (Min) This score is equal to the low-
est logprob assigned by the model to the n-grams
of the sentence divided by the unigram logprob of
the lexical item heading the n-gram:
Min = mini
[
logP (wi|wi?2wi?1)
logP (wi)
]
In this way, if a single n-gram is assigned a low
probability (normalized for the frequency of its
head lexical item), then this low score is in some
sense propagated to the whole sentence.
Mean of the first quartile (MFQ) This score
is a generalization of the Min score. We order
the single n-gram logprobs from the lowest to the
highest, and we consider the first (lowest) quar-
tile. We then normalize the logprobs for these n-
grams by the unigram probability of the head lex-
ical item, and we take the mean of these scores.
In this way we obtain a score that is more robust
than the simple Min, as, in general, a grammatical
anomaly influences the logprob of more than one
n-gram.
2.2 N-Gram Models
We are using n-gram models on the understand-
ing that they are fundamentally inadequate for de-
scribing natural languages in their full syntactic
complexity. In spite of their limitations, they are a
good starting point, as they perform well as lan-
guage models across a wide range of language
modeling tasks. They are easy to train, as they
do not require annotated training data.
We do not expect that our n-gram based gram-
maticality scores will be able to idenitfy all of the
cases of ungrammaticality that we encounter. Our
working hyposthesis is that they can capture cases
of ill-formedness that depend on local factors, that
can be identified within n-gram frames, as op-
posed to those which involve non-local relations.
If these models can detect local grammaticality vi-
olations, then we will have a basis for thinking
that richer, more structured language models can
recognize non-local as well as local sources of un-
grammaticality.
3 Experiments with Passives
Rather than trying to test the performance of these
models over all types of ungrammaticality, we
limit ourselves to a case study of the passive. By
tightly controlling the verb types and grammat-
ical construction to which we apply our models
we are better able to study the power and the lim-
its of these models as candidate representations of
grammatical knowledge.
3.1 Types of Passives
Our controlled experiments on passives are, in
part, inspired by speakers? judgments discussed in
Ambridge et al (2008). Their experimental work
measures the acceptability of various passive sen-
tences.
The active-passive alternation in English is ex-
emplified by the pair of sentences
? John broke the window.
? The window was broken by John.
The acceptability of the passive sentence de-
pends largely on lexical properties of the verb.
Some verbs do not allow the formation of the pas-
sive, as in the case of pure intransitive verbs like
appear, discussed below, which permit neither the
active transitive, nor the passive.
We conducted some prelimiary experiments,
not reported here, on modelling the data on pas-
sives from recent work in progress that Ben Am-
bridge and his colleagues are doing, and which
he was kind enough to make available to us. We
observed that the scores we obtained for our lan-
guage models did not fully track these judgements,
but we did notice that we obtained much better
correlation at the low end of the judgment distri-
bution. In Ambridge?s current data this judgement
range corresponds to passives constructed with in-
transitive verbs.
The Ambridge data indicates that the capacity
of verbs to yield well-formed passive verb phrases
30
forms a continuum. Studying the judgement pat-
terns in this data we identified four reasonably
salient points along this hierarchial continuum.
First, at the low end, we have intransitives
like appear: (*John appeared the book. *The
book was appeared). Next we have what may be
described as pseudo-transitives verbs like laugh,
which permit only notional NP objects and do not
easily passivize (Mary laughed a hearty laugh/*a
joke. ?A hearty laugh/*A joke was laughed by
Mary) above them. These are followed by cases
of ambiguous transitives like fit, which, in active
form, carry two distinct readings that correspond
to an agentive and a thematic subject, respectively.
? The tailor fitted John for a new suit.
? The jeans fitted John
Only the agentive reading can be passivized.
? John was fitted by the tailor.
? *John was fitted by the jeans.
Finally, the most easily passivized verbs are ro-
bust transitives, which take the widest selection of
NP subjects in passive form (John wrote the book.
The book was written by John).
This continuum causes well-formedness in pas-
sivization to be a gradient property, as the Am-
bridge data illustrates. Passives tend to be more
or less acceptable along this spectrum. The gradi-
ence of acceptability for passives implies the par-
tial overlap of the score distributions for the differ-
ent types of passives that our experiments show.
The experiments were designed to test our hy-
pothesis that n-gram based language models are
capable of detecting ungrammatical patterns only
in cases where they do not depend on relations
between words that cross the n-word boundary
applied in training. Therefore we expect such a
model to be capable of detecting the ungrammati-
cality of a sentence like A horrible death was died
by John, because the trigrams death was died, was
died by and died by John are unlikely to appear
in any corpus of English. On the other hand, we
do not expect a trigram model to store the infor-
mation necessary to identify the relative anomaly
of a sentence like Two hundred people were held
by the theater, because all the trigrams (as well as
the bigrams and the unigrams) that constitute the
sentence are likely to appear with reasonable fre-
quency in a large corpus of English.
The experiments generalize this observation
and test the performance of n-gram models on a
wider range of verb types. To quantify the per-
formance of the different models we derive simple
classifiers using the scores we have defined and
testing them in a binary classification task. This
task measures the ability of the classifier to dis-
tinguish between grammatical sentences, and sen-
tences containing different types of grammatical
errors.
The models are trained in an unsupervised man-
ner using only corpus data, which we assume to be
uniformly grammatical. In order to evaluate the
scoring methods, we use some supervised data to
set the optimal value of a simple threshold. This is
not however a supervised classification task: we
want to see how well the scores could be used
to separate grammatical and ungrammatical data,
and though unorthodox, this seems a more direct
way of measuring this conditional property than
stipulating some fixed threshold.
3.2 Training data
We used the British National Corpus (BNC) (BNC
Consortium, 2007) to obtain our training data. We
trained six different language models, using six
different subcorpora of the BNC. The first model
used the entire collection of written texts anno-
tated in the BNC, for a total of approximately 100
million words. The other models were trained on
increasingly smaller portions of the written texts
collection: 40 million words, 30 million words, 15
million words, 7.6 million words, and 3.8 million
words. We constructed these corpora by randomly
sampling an appropriate number of complete sen-
tences.
All models were trained on word sequences.
For smoothing the n-gram probability distribu-
tions we used Kneser-Ney interpolation, as de-
scribed in Goodman (2001).
3.3 Test data
We constructed the test data for our hypothesis in
a controlled fashion. We first compiled a list of
verbs for each of the four verb types that we con-
sider (intransitives, pseudo-transitives, ambiguous
transitives, and robust transitives). We selected
verbs from the BNC that appeared at least 100
times in their past participle form in the entire cor-
pus in order to ensure a sufficient number of pas-
31
sive uses in the training data.1 We selected 40 in-
transitive verbs, 13 pseudo transitives, 23 ambigu-
ous transitives and 40 transitive verbs. To clas-
sify the verbs we relied on our intuitions as native
speakers of English.
Using these lists we automatically generated
four corpora by selecting an agent and a patient
from a predefined pool of NPs, randomly select-
ing a determiner (if necessary) and a number (if
the NP allows plurals). The resulting corpora are
of the following sizes:
? intransitive verbs ? 24480 words, 3240 sen-
tences,
? pseudo transitive verbs ? 7956 words, 1053
sentences,
? ambiguous transitive verbs ? 14076 words,
1863 sentences,
? robust transitive verbs ? 24480 words, 3240
sentences.
Each corpus was evaluated by the six models.
We computed our derived scores for each sentence
on the basis of the logprobs that the language mod-
els assigns.
3.4 Binary classifiers
For each model and for each score we constructed
a set of simple binary classifiers on the basis of
the results obtained for the transitive verb corpus.
We took the mean of each score assigned by the
model to the transitive sentences, and we set dif-
ferent thresholds by subtracting from this value
a number of standard deviations ranging from 0
to 2.75. The rationale behind these classifiers is
that, assuming the passives of the robust transi-
tives to be grammatical, the scores for the other
cases should be comparatively lower. Therefore
by setting a threshold ?to the left? of the mean we
should be able to distinguish between grammati-
cal sentences, whose score is to the right of the
threshold, and ungrammatical ones, expected to a
have a score lower than the threshold. Formally
the classifier is defined as follows:
cs(w) =
{
+ if s(w) ? m? S ? ?
? otherwise
(1)
1Notice that in most cases the past participle form is the
same as the simple past form, and for this reason we set the
threshold to such a high value.
where s is one of our scores, w is the sentence to
be classified, s(w) represents the value assigned
by the score to sentence w, m is the mean for
the score in the transitive condition, ? is the stan-
dard deviation for the score again in the transitive
condition, and S is a factor by which we move
the threshold away from the mean. The classi-
fier assigns the grammatical (+) tag only to those
sentences that are assigned values higher than the
threshold m? S ? ?.
Alternatively in terms of the widely used z-
score, defined as zs(w) = (s(w) ?m)/? we can
say that w is classified as grammatical iff zs(w) ?
?S.
4 Results
For reasons of space we will limit the presenta-
tion of our detailed results to the 100 million word
model, as it offers the sharpest effects. We will,
however, also report comparisons on the most im-
portant metrics for the complete set of models.
In Figure 1 we show the distribution of the five
scores for the four different corpora (transitive,
ambiguous, pseudo, and intransitive) obtained us-
ing the 100 million word model. In all cases we
observe the same general pattern: the sentences in
the corpus generated with robust transitives are as-
signed comparatively high scores, and these grad-
ually decrease when we consider the ambiguous,
the pseudo and the intransitive conditions. Inter-
estingly, this order reflects the degree of ?transi-
tivity? that these verb types exhibit. Notice, how-
ever, that the four conditions seem to group into
two different macro-distributions. On the right
we have the transitive-ambiguous sentences and
on the left the pseudo-intransitive cases. This par-
tially confirms our hypothesis that n-gram mod-
els have problems recognizing lexical dependen-
cies that determine the felicitousness of passives
constructed using ambiguous transitive verbs, as
these are, for the most part, non-local. Neverthe-
less, it is important to note that the overlap of the
distributions for these two cases is also due to the
fact that many cases in the ambiguous transitive
corpus are indeed grammatical.
Figure 2 summarizes the (balanced) accuracies
obtained by our classifiers for each comparison,
by each model. These results confirm our hy-
pothesis that the classifiers tend to perform better
when distinguishing passive sentences constructed
with a robust transitive verbs from those headed by
32
Logprob ML WML
SLOR Min MFQ
0.00
0.05
0.10
0.15
0.0
0.5
1.0
1.5
2.0
0
2
4
6
0.0
0.5
1.0
1.5
2.0
0
2
4
6
0
1
2
3
4
?30 ?25 ?20 ?15 ?2.5 ?2.0 ?1.5 ?1.0 ?0.9 ?0.8 ?0.7 ?0.6 ?0.5
0.0 0.5 1.0 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75
den
sity
condition
transitive
ambiguous
pseudo
intransitive
Figure 1: Distributions of the six scores Logprob, ML, WML, SLOR, Min and MFQ for the four differ-
ent conditions (robust transitive passives, ambiguous transitive passives, pseudo transitive passives and
intransitive passives) for the 100 million words language model.
pseudo-transitives and intransitives.
In the comparison between transitive and am-
biguous transitive sentences, the classifiers are
?stuck? at around 60% accuracy. Using larger
training corpora produces only a marginal im-
provement. This contrasts with what we observe
for the transitive/pseudo and transitive/intransitive
classification tasks. In the transitive/pseudo task,
we already obtain reasonable accuracy with the
model trained with the smallest BNC subset.
Oddly, the overall best result is achieved with 30
million words, although the result obtained with
the model trained on the full BNC corpus is not
much lower. For the transitive/intransitive classifi-
cation task we observe a much steadier and larger
growth in accuracy, reaching the overall best result
of 85.1%. Table 1 reports the best results for each
comparison by each language model. For each
condition we report the best accuracy obtained, the
corresponding F1 score, the score that achieves the
best result, and the best accuracy obtained by just
using the logprobs. These results are obtained us-
ing different values for the S parameter. However,
in general the best results are obtained when the S
parameter is set to a value in the interval [0.5, 1.5].
In comparing the performance of the individ-
ual scores, we first notice that, while for the tran-
sitive/ambiguous comparison all scores perform
pretty much at the same level, there is a clear hier-
archy between scores for the other comparisons.
We observe that the baseline raw logprob as-
signed by the n-grams models performs much
worse than the scores, resulting in roughly 10%
less accuracy than the best performing score in ev-
ery condition. ML performs slightly better, obtain-
ing around 5% greater accuracy than logprob as a
predictor. This shows that even though the length
of the sentences in our test data is relatively con-
stant (between 9 and 11 words), there is still an
improvement if we take this structural factor into
account. The two scores WML and SLOR display
the same pattern, showing that they are effectively
equivalent. This is not surprising given that they
are designed to modify the raw logprob by tak-
33
transitive
ambiguous transitivepseudo transitiveintransitive
l l l l l l l l l l l l
l l l l l l
l l l l l l l l ll l l l
l l l
l l l
l l
l l l
l l l
l l
l l l l l l l l l l l l
l l l l l l l l l
l l l l l l l
l l l
l
l l l l l l l l
l
l
l l l
l l l l l
l
l
l l l l l l l l l l l l
l l l l l l l l
l l l l l l l
l l
l l
l l l l l l
l l l l
l l l
l l l
l l l l l l l l l l l l
l l l l l l l l l
l l
l l l l l l l l l
l l
l l
l l l l
l l l l
l l l
l l l l l
l
l l l
l l l l l l l l l l l l
l l l l l l l l l l
l l l
l l l
l l l l l l
l l l
l l l
l l l l l
l l l l
l
l
l l l
l l
l
l
l l l l l l l l l l l l
l l l l l l l l l l
l l
l l l
l l l l l l l
l l l l l l l l l l
l
l l
l l l
l l l l l
l l l
l l l l
l l l l l
l
l
l
l l
l
l l l l l
l
l
l
l l
l
l
l
l l l
l l l l l
l l l l
l
l
l
l
l
l
l
l l l
l l
l l l l l
l l
l
l
l l l
l l
l l l
l
l
l
l
l
l l l l l
l
l
l
l
l
l
l l
l l l l l l
l l
l
l
l
l
l
l l
l
l
l
l
l l
l l l l l l
l l l
l l l
l l l l l l
l
l
l
l
l l l l l
l
l
l
l
l
l
l
l
l l l l l l
l l l
l
l
l l
l l l
l
l
l
l
l l
l l
l l l
l l
l
l l
l l
l
l l l l l
l
l
l
l
l l
l
l
l l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l
l l
l l
l
l
l
l
l
l
l
l l l l
l
l
l l l l l l
l
l l
l l l
l l
l l l l
l
l
l
l
l
l
l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l l
l l l
l
l
l
l
l
l l
l
l
l
l l
l l l
l
l
l
l l
l l
l
l l
l l l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l l
l l
l l l
l
l
l
l l
l
l l l
l
l
l
l l
l l l l
l
l
l
l l l l l l
l l
l l l l
l l l l l
l
l
l
l l l
l
l l l l
l
l
l
l
l l
l
l
l
l
l l
l l l l l
l
l
l
l
l
l
l l l l
l
l
l l l
l l l l l l l
l l l l l
l l
l l l
l
l
l
l
l
l
l l l l l
l
l
l
l l
l
l l
l l l l l l
l l
l
l
l
l
l
l l l
l
l
l l l
l l l l l l
l
l
l
l l l
l l
l l l
l
l
l
l
l
l
l
l l
l l l
l
l
l
l
l
l
l l
l
l
l
l l l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l l
l l
l l l l l
l
l
l
l l
l
l l l l l
l
l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l
l
l
l
l
l l l
l
l l
l
l
l
l
l
l
l l l l
l
l
l
l l
l l
l l l l
l
l
l
l l l
l
l l
l l l
l
l
l
l
l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l l l l
l
l
l
l
l
l
l
l
l l
l
l
l
l
l
l l
l l l
l
l
l
l
l
l l
l
l
l l l l
l
l
l
l
l
l
l
l l
l l
l
l
l
l
l
l
l
l
l l l l l
l
l
l
l
l
l
l
l
l
l
l l l
l
l
l
l
l
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
0.5
0.6
0.7
0.8
3.8M
7.6M
15M
30M
40M
100M
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75S
Bala
nced
 acc
urac
y Scorel
l
l
l
l
l
LogprobMLWMLSLORMinMFQ
Figure 2: Accuracies for the classifiers for each model. S represents the number of standard deviations
?to the left? of the mean of the transitive condition score, used to set the threshold.
ing into account exactly the same factors (length
of the sentence and frequency of the unigrams that
compose the sentence). These two scores perform
generally better in the transitive/ambiguous com-
parison, and they achieve good performance when
the size of the training model is small. However,
for the most part, the two scores derived from the
logprobs of the least probable n-grams in the sen-
tence, Min and MFQ, get the best results. Min
exhibits erratic behavior (mainly due to its non-
normal distribution for each condition, as shown
in figure 1), and it seems to be more stable only
in the presence of a large training set. MFQ has
a much more robust contour, as it is significantly
less dependent on the choice of S.
5 Conclusions and Future Work
In Clark and Lappin (2011) we propose a model
of negative evidence that uses probability of oc-
currence in primary linguistic data as the basis for
estimating non-grammaticality through relatively
34
Model Comparison Best accuracy F1 Best performing score Logprob accuracy
transitive/ambiguous 60.9% 0.7 SLOR 57.3%
3.8M transitive/pseudo 77% 0.81 MFQ 67.6%
transitive/intransitive 73.8% 0.72 SLOR 65.6%
transitive/ambiguous 62.9% 0.68 MFQ 57.8%
7.6M transitive/pseudo 78.5% 0.76 MFQ 69.1%
transitive/intransitive 75.8% 0.72 MFQ 67.3%
transitive/ambiguous 62.3% 0.66 WML 57.8%
15M transitive/pseudo 72.6% 0.78 SLOR 66.5%
transitive/intransitive 79.5% 78.3 MFQ 69.5%
transitive/ambiguous 63.3% 0.75 WML 58.9%
30M transitive/pseudo 83.1% 0.88 Min 71.2%
transitive/intransitive 81.8% 0.82 MFQ 72.2%
transitive/ambiguous 63.8% 0.75 SLOR 59.5%
40M transitive/pseudo 80.1% 0.86 Min 69.7%
transitive/intransitive 83.5% 0.83 SLOR 72.6%
transitive/ambiguous 63.3% 0.75 SLOR 58.4%
100M transitive/pseudo 80.3% 0.9 MFQ 71.3%
transitive/intransitive 85.1% 0.85 SLOR 73.8%
Table 1: Best accuracies
low frequency in a sample of this data. Here we
follow Clark et al (2013) in effectively inverting
this strategy.
We identify a set of scoring functions based on
parameters of probabilistic models that we use to
define a grammaticality threshold, which we use
to classify strings as grammatical or ill-formed.
This model offers a stochastic characterisation of
grammaticality without reducing grammaticality
to probability.
We expect enriched lexical n-gram models of
the kind that we use here to be capable of rec-
ognizing the distinction between grammatical and
ungrammatical sentences when it depends on local
factors within the frame of the n-grams on which
they are trained. We further expect them not to be
able to identify this distinction when it depends on
non-local relations that fall outside of the n-gram
frame.
It might be thought that this hypothesis con-
cerning the capacities and limitations of n-gram
models is too obvious to require experimental sup-
port. In fact, this is not the case. Reali and Chris-
tiansen (2005) show that n-gram models can be
used to distinguish grammatical from ungrammat-
ical auxiliary fronted polar questions with a high
degree of success. More recently Frank et al
(2012) argue for the view that a purely sequen-
tial, non-hierarchical view of linguistic structure is
adequate to account for most aspects of linguistic
knowledge and processing.
We have constructed an experiment with differ-
ent (pre-identified) passive structures that provides
significant support for our hypothesis that lexical
n-gram models are very good at capturing local
syntactic relations, but cannot handle more distant
dependencies.
In future work we will be experimenting with
more expressive language models that can repre-
sent non-local syntactic relations. We will pro-
ceed conservatively by first extending our enriched
lexical n-gram models to chunking models, and
then to dependency grammar models, using only
as much syntactic structure as is required to iden-
tify the judgement patterns that we are studying.
To the extent that this research is successful it
will provide motivation for the view that syntactic
knowledge is inherently probabilistic in nature.
Acknowledgments
The research described in this paper was done in the
framework of the Statistical Models of Grammaticality
(SMOG) project at King?s College London, funded by grant
ES/J022969/1 from the Economic and Social Research Coun-
cil of the UK. We are grateful to Ben Ambridge for providing
us with the data from his experiments and for helpful dis-
cussion of the issues that we address in this paper. We also
thank the three anonymous CMCL 2013 reviewers for useful
comments and suggestions, that we have taken account of in
preparing the final version of the paper.
35
References
Ben Ambridge, Julian M Pine, Caroline F Rowland, and
Chris R Young. 2008. The effect of verb semantic
class and verb frequency (entrenchment) on childrens and
adults graded judgements of argument-structure overgen-
eralization errors. Cognition, 106(1):87?129.
BNC Consortium. 2007. The British National Corpus, ver-
sion 3 (BNC XML Edition). Distributed by Oxford Uni-
versity Computing Services on behalf of the BNC Consor-
tium.
R. Bod, J. Hay, and S. Jannedy. 2003. Probabilistic linguis-
tics. MIT Press.
N. Chater, J.B. Tenenbaum, and A. Yuille. 2006. Probabilis-
tic models of cognition: Conceptual foundations. Trends
in Cognitive Sciences, 10(7):287?291.
N. Chomsky. 1957. Syntactic Structures. Mouton, The
Hague.
A. Clark and S. Lappin. 2011. Linguistic Nativism and the
Poverty of the Stimulus. Wiley-Blackwell, Malden, MA.
A. Clark, G. Giorgolo, and S. Lappin. 2013. Towards a sta-
tistical model of grammaticality. In Proceedings of the
35th Annual Conference of the Cognitive Science Society.
Sandiway Fong, Igor Malioutov, Beracah Yankama, and
Robert C. Berwick. 2013. Treebank parsing and
knowledge of language. In Aline Villavicencio, Thierry
Poibeau, Anna Korhonen, and Afra Alishahi, editors, Cog-
nitive Aspects of Computational Language Acquisition,
Theory and Applications of Natural Language Processing,
pages 133?172. Springer Berlin Heidelberg.
Stefan Frank, Rens Bod, and Morten Christiansen. 2012.
How hierarchical is language use? In Proceedings of the
Royal Society B, number doi: 10.1098/rspb.2012.1741.
J.T. Goodman. 2001. A bit of progress in language model-
ing. Computer Speech & Language, 15(4):403?434.
A. Pauls and D. Klein. 2012. Large-scale syntactic language
modeling with treelets. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguistics,
pages 959?968. Jeju, Korea.
F. Pereira. 2000. Formal grammar and information theory:
together again? Philosophical Transactions of the Royal
Society of London. Series A: Mathematical, Physical and
Engineering Sciences, 358(1769):1239?1253.
M. Post. 2011. Judging grammaticality with tree substitution
grammar derivations. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguistics:
Human Language Technologies, pages 217?222.
F. Reali and M.H. Christiansen. 2005. Uncovering the rich-
ness of the stimulus: Structure dependence and indirect
statistical evidence. Cognitive Science, 29(6):1007?1028.
36
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, page 29,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Distributional Learning as a Theory of Language Acquisition
(Extended Abstract)
Alexander Clark
Department of Philosophy
King?s College, London
Strand, London
alexander.clark@kcl.ac.uk
1 Abstract
In recent years, a theory of distributional learning
of phrase structure grammars has been developed
starting with the simple algorithm presented in
(Clark and Eyraud, 2007). These ideas are based
on the classic ideas of American structuralist lin-
guistics (Wells, 1947; Harris, 1954). Since that
initial paper, the algorithms have been extended to
large classes of grammars, notably to the class of
Multiple Context-Free grammars by (Yoshinaka,
2011).
In this talk we will sketch a theory of language
acquisition based on these techniques, and con-
trast it with other proposals, such as the semantic
bootstrapping and parameter setting models. This
proposal is based on three recent results: first, a
weak learning result for a class of languages that
plausibly includes all natural languages (Clark and
Yoshinaka, 2013), secondly, a strong learning re-
sult for some context-free grammars, that includes
a general strategy for converting weak learners to
strong learners (Clark, 2013a), and finally a theo-
retical result that all minimal grammars for a lan-
guage will have distributionally definable syntac-
tic categories (Clark, 2013b). We argue that we
now have all of the pieces for a complete and ex-
planatory theory of language acquisition based on
distributional learning and sketch some of the non-
trivial predictions of this theory about the syntax
and syntax-semantics interface.
2 Biography
Alexander Clark is a Lecturer in Logic and Lin-
guistics in the Department of Philosophy at King?s
College London; before that he taught for sev-
eral years in the Computer Science department of
Royal Holloway, University of London. His first
degree was in Mathematics from the University
of Cambridge, and his Ph.D. is from the Univer-
sity of Sussex. He did postdoctoral research at the
University of Geneva. He is currently President
of SIGNLL and chair of the steering committee of
the International Conference on Grammatical In-
ference. His research is on unsupervised learn-
ing in computational linguistics, grammatical in-
ference, and theoretical and mathematical linguis-
tics.
References
Alexander Clark and R?emi Eyraud. 2007. Polynomial
identification in the limit of substitutable context-
free languages. Journal of Machine Learning Re-
search, 8:1725?1745, August.
Alexander Clark and Ryo Yoshinaka. 2013. Distri-
butional learning of parallel multiple context-free
grammars. Machine Learning, pages 1?27.
Alexander Clark. 2013a. Learning trees from strings:
A strong learning algorithm for some context-free
grammars. Journal of Machine Learning Research,
14:3537?3559.
Alexander Clark. 2013b. The syntactic concept lat-
tice: Another algebraic theory of the context-free
languages? Journal of Logic and Computation.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146?62.
R. S. Wells. 1947. Immediate constituents. Language,
23(2):81?117.
R. Yoshinaka. 2011. Efficient learning of multiple
context-free languages with multidimensional sub-
stitutability from positive data. Theoretical Com-
puter Science, 412(19):1821 ? 1831.
29
