Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 385?393,
Beijing, August 2010
Comparing Language Similarity across Genetic
and Typologically-Based Groupings
Ryan Georgi
University of Washington
rgeorgi@uw.edu
Fei Xia
University of Washington
fxia@uw.edu
William Lewis
Microsoft Research
wilewis@microsoft.com
Abstract
Recent studies have shown the poten-
tial benefits of leveraging resources for
resource-rich languages to build tools for
similar, but resource-poor languages. We
examine what constitutes ?similarity? by
comparing traditional phylogenetic lan-
guage groups, which are motivated largely
by genetic relationships, with language
groupings formed by clustering methods
using typological features only. Using
data from the World Atlas of Language
Structures (WALS), our preliminary ex-
periments show that typologically-based
clusters look quite different from genetic
groups, but perform as good or better
when used to predict feature values of
member languages.
1 Introduction
While there are more than six thousand languages
in the world, only a small portion of these lan-
guages have received substantial attention in the
field of NLP. With the increase in use of data-
driven methods, languages with few or no elec-
tronic resources have been difficult to process with
current methods. The morphological tagging of
Russian using Czech resources as done by (Hana
et al, 2004) shows the potential benefit for using
the resources of resource-rich languages to boot-
strap NLP tools for related languages. Projecting
syntactic structures across languages (Yarowsky
and Ngai, 2001; Xia and Lewis, 2007) is another
possible way to harness existing tools, though
such projection is more reliable among languages
with similar syntax.
Studies such as these show the possible bene-
fits of working with similar languages. A crucial
question is how we should define similarity be-
tween languages. While genetically related lan-
guages tend to have similar typological features
as they could inherit the features from their com-
mon ancestor, they could also differ a lot due to
language change over time. On the other hand,
languages with no common ancestor could share
many features due to language contact and other
factors.
It is worth noting that the goals of historical lin-
guistics differ from those of language typology in
that while historical linguistics focuses primarily
on diachronic language change, typology is more
focused on a synchronic survey of features found
in the world?s languages: what typological fea-
tures exist, where they are found, and why a lan-
guage has a feature.
These differences between the concepts of ge-
netic relatedness and language similarities lead us
to the following questions:
Q1. If we cluster languages based only on their
typological features, how do the induced
clusters compare to phylogenetic groupings?
Q2. How well do induced clusters and genetic
families perform in predicting values for ty-
pological features?
Q3. What typological features tend to stay the
same within language families, and what fea-
tures are likely to differ?
These questions are the focus of this study,
and for the experiments, we use information from
World Atlas of Language Structures (Haspelmath
et al, 2005), or WALS.
385
ID# Feature Name Category Feature Values
1 Consonant Inventories Phonology (19) {1:Large, 2:Small, 3:Moderately Small, 4:Moderately Large, 5:Average}
23 Locus of Marking in the Clause Morphology (10) {1:Head, 2:None, 3:Dependent, 4:Double, 5:Other}
30 Number of Genders Nominal Categories (28) {1:Three, 2:None, 3:Two, 4:Four, 5:Five or More}
58 Obligatory Possessive Inflection Nominal Syntax (7) {1:Absent, 2:Exists}
66 The Perfect Verbal Categories (16) {1:None, 2:Other, 3:From ?finish? or ?already?, 4:From Possessive}
81 Order of Subject, Object and Verb Word Order (17) {1:SVO, 2:SOV, 3:No Dominant Order, 4:VSO, 5:VOS, 6:OVS, 7:OSV}
121 Comparative Constructions Simple Clauses (24) {1:Conjoined, 2:Locational, 3:Particle, 4:Exceed}
125 Purpose Clauses Complex Sentences (7) {1:Balanced/deranked, 2:Deranked, 3:Balanced}
138 Tea Lexicon (10) {1:Other, 2:Derived from Sinitic ?cha?, 3:Derived from Chinese ?te?}
140 Question Particles in Sign Languages Sign Languages (2) {1:None, 2:One, 3:More than one}
142 Para-Linguistic Usages of Clicks Other (2) {1:Logical meanings, 2:Affective meanings, 3:Other or none}
Table 1: Sample features and their values used in the WALS database. There are eleven feature cate-
gories in WALS, one feature from each is given here. The numbers in parentheses in the ?Category?
column are the total number of features in that category. Feature values are given with both the integers
that represent them in the database and their description in the form {#:description}.
2 WALS
The WALS project consists of a database that cat-
alogs linguistic features for over 2,556 languages
in 208 language families, using 142 features in 11
different categories.1 Table 1 shows a small sam-
ple of features, one feature from each category in
WALS. Listed are the ID number for each exam-
ple, the feature category, and the possible values
for that feature.
WALS as a resource, however, is primarily de-
signed for surveying the distribution of particu-
lar typological features worldwide, not compar-
ing languages. The authors of WALS compiled
their data from a wide array of primary sources,
but these sources do not always cover the same
sets of features or languages.
If we conceive of the WALS database as a two-
dimensional matrix with languages along one di-
mension and features along the other, then only
16% of the cells in that matrix are filled. An empty
cell in the matrix means the feature value for
the (language, feature) pair is not-specified (NS).
Even well-studied languages could have many
empty cells in WALS, and this kind of data spar-
sity presents serious problems to clustering algo-
rithms that cannot handle unknown values. To
address the data sparsity problem, we experiment
with different pruning criteria to create a new ma-
trix that is reasonably dense for our study.
1Our copy of the database was downloaded from http:
//wals.info in June of 2009 and appears to differ
slightly from the statistics given on the website at the time
of writing. Currently, the WALS website reports 2,650 lan-
guages, with 141 features in use.
2.1 Pruning Methods
Answering questions Q1?Q3 is difficult if there
are too many empty cells in the data. Pruning the
data to produce a smaller but denser subset can be
done by one or more of the following methods.
Prune Languages by Minimum Features
Perhaps the most straightforward method of
pruning is to eliminate languages that fail to con-
tain some minimum number of features. Follow-
ing Daume? (2009), we require languages to have a
minimum of 25 features for the whole-world set,
or 10 features for comparing across subfamilies.
This eliminates many languages that simply do
not have enough features to be adequately repre-
sented.
Prune Features by Minimum Coverage
The values for some features, such as those spe-
cific to sign languages, are provided only for a
very small number of languages. Taking this into
account, in addition to removing languages with a
small number of features, it is also helpful to re-
move features that only cover a small portion of
languages. Again we choose the thresholds se-
lected by Daume? (2009) for pruning features that
do not cover more than 10% of the selected lan-
guages in the whole-world set, and 25% in com-
parisons across subfamilies.
Use a Dense Language Family
Finally, using a well-studied family with a num-
ber of subfamilies can produce data sets with less
sparsity. When clustering methods are used with
this data, the groups correspond to subfamilies
386
Data Set Min Features Min Coverage Grouped By # Langs # Groups # Features Density
Unpruned 0 0% Family 2556 208 142 16.0%
Whole-World 25 10% Family 735 121 139 39.7%
Indo-European 10 25% Subfamily 87 10 64 44.9%
Sino-Tibetan 10 25% Subfamily 96 14 64 38.6%
Table 2: Data sets and pruning options used for this paper. Density = |Filled Cells||Total Cells| ? 100
rather than families. In this study, we choose two
families: Indo-European and Sino-Tibetan.
The resulting data sets after various methods of
pruning can be seen in Table 2.
2.2 Features and Feature Values
Besides dealing with the sparsity of the features,
the actual representation of the features in WALS
needs to be taken into account. As can be seen
in Table 1, features are represented with a range
of discrete integer values. Some features, such
as #58?Obligatory Possessive Inflection?are es-
sentially binary features with values ?Absent?
or ?Exists?. Others, such as #1?Consonant
Inventories?appear to be indices along some di-
mension related to size, ranging from small to
large. Features such as these might conceivably
be viewed as on a continuum where closer dis-
tances between values suggests closer relationship
between languages.
Still other features, such as #81?Order of Sub-
ject, Object, and Verb?have multiple values but
cannot be clearly be treated using distance mea-
sures. It?s unclear how such a distance would vary
between an SOV language and either VSO or VOS
languages.
Binarization
Clustering algorithms use similarity functions,
and some functions may simply check whether
two languages have the same value for a feature.
In these cases, no feature binarization is needed.
If a clustering algorithm requires each data point
(a language in this case) to be presented as a fea-
ture vector, features with more than two categori-
cal values should be binarized. We simply treat a
feature with k possible values as k binary features.
There are other ways to binarize features. For in-
stance, Daume? (2009) chose one feature value as
the ?canonical? value and grouped the other val-
ues into the second value (personal communica-
tion). We did not use this approach as it is not
clear to us which values should be selected as the
?canonical? ones.
3 Experimental Setup
To get a picture of how clustering methods com-
pare to genetic groupings, we looked at three el-
ements: cluster similarity, prediction capability,
and feature selection.
3.1 Clustering
Our first experiment is designed to address ques-
tion Q1: how do induced clusters compare to phy-
logenetic groupings?
Clustering Methods
For clustering, two clustering packages were
used. First, we implemented the k-medoids algo-
rithm, a partitional algorithm similar to k-means,
but using median instead of mean distance for
cluster centers (Estivill-Castro and Yang, 2000).
Second, we used a variety of methods from
the CLUTO (Steinbach et al, 2000) clustering
toolkit: repeated-bisection (rb), a k-means im-
plementation (direct), an agglomerative algo-
rithm (agglo) using UPGMA to produce hierar-
chical clusters, and bagglo, a variant of agglo,
which biases the agglomerative algorithm using
partitional clusters.
Similarity Measures
For similarity measures, we used CLUTO?s
default cosine similarity measure (cos), but
also implemented another similarity mea-
sure shared overlap designed to handle
empty cells. Given two languages A and
B, shared overlap(A,B) is defined to be
# Of Features with Same Values
# Features Both Filled Out in WALS . This measurecan handle language pairs with many empty
cells in WALS as it uses only features with cells
387
a is the number of language pairs found in the same set in both clusterings.
b is the number of language pairs found in different sets in C1, and different sets in C2.
c is the number of language pairs found in the same set in C1, but in different sets in C2.
d is the number of language pairs found in different sets in C1, but the same set in C2.
(a) Variables Used In Calculations
R(C1, C2) =
a + b
a + b + c + d(b) Rand Index
Precision(C1, C2) =
a
a + c(c) Cluster precision
Recall(C1, C2) =
a
a + d(d) Cluster recall
Fscore(C1, C2) =
2 ? (Precision ? Recall)
Precision + Recall(e) Cluster f-score
Figure 1: Formulas for calculating the Rand Index, cluster precision, recall, and f-score of two cluster-
ings C1 and C2. C1 is the system output, C2 is the gold standard.
filled out for both languages, and calculates the
percentage of features with the same values.
3.2 Clustering Performance Metrics
To measure clustering performance, we treat the
genetic families specified in WALS as the gold
standard, although we are not strictly aiming to
recreate them.
Rand Index
The Rand Index (Rand, 1971) is one of the
standard metrics for evaluating clustering results.
It compares pairwise assignments of data points
across two clusterings. For every pair of points
there are four possibilities, as given in Figure 1.
The Rand index is calculated by dividing the num-
ber of matching pairs (a+ b) by the number of all
pairs. This results in a number between 0 and 1
where 1 represents an identical clustering. Unfor-
tunately, as noted by (Daume? and Marcu, 2005),
the Rand Index tends to give disproportionately
greater scores to clusterings with a greater num-
ber of clusters. For example, the Rand Index will
always be 1.0 when each data point belongs to its
own cluster. As a result, we have chosen to cal-
culate metrics other than the Rand index: cluster
precision, recall, and f-score.
Cluster Precision, Recall, and F-Score
Extending the notation in Figure 1, precision
is defined as the proportion of same-set pairs in
the target cluster C1 that are correctly identified
as being in the same set in the gold cluster C2,
while recall is the proportion of all same-set pairs
in the gold cluster C2 that are identified in the tar-
get cluster C1. F-score is calculated as the usual
harmonic mean of precision and recall. As it gives
a more accurate representation of cluster similar-
ity across varying amounts of clusters, we will re-
port cluster similarity using cluster F-score.
3.3 Prediction Accuracy
Our second experiment was to answer the ques-
tion posed in Q2: how do induced clusters and
genetic families compare in predicting the values
of features for languages in the same group?
To answer this question, we measure the accu-
racy of the prediction when both types of groups
are used to predict the values of ?empty? cells. We
used 90% of the filled cells to build clusters, and
then predicted the values of the remaining 10% of
filled cells. The missing cells are filled with the
value that occurs the most times among languages
in the same group. If there are no other languages
in the cluster, or the other languages have no val-
ues for this feature, then the cell is filled with
the most common values for that feature across
all languages in the dataset. Finally, the accuracy
is calculated by comparing these predicted values
with the actual values in the gold standard. We run
10-fold cross validation and report the average ac-
curacy.
In addition to the prediction accuracy for each
method of producing groupings, we calculate the
baseline result where an empty cell is filled with
the most frequent value for that feature across all
the languages in the training data.
3.4 Determining Feature Stability
Finally, we look to answer Q3: what typological
features tend to stay the same within related fam-
ilies? To find an answer, we look again to pre-
diction accuracy. While prediction accuracy can
be averaged across all features, it can also be bro-
ken down feature-by-feature to rank features ac-
cording to how accurately they can be predicted
388
by language families. Features that can be pre-
dicted with high accuracy implies that these fea-
tures are more likely to remain stable within a lan-
guage family than others.
Using prediction accuracies based on the ge-
netic families, we rank features according to their
accuracy and then perform clustering using the top
features to determine if the cluster similarity to the
genetic groups increases when using only the sta-
ble features.
4 Results & Analysis
4.1 Cluster Similarity
The graph in Figure 2(a) shows f-scores of clus-
tering methods with the whole-world set. None
achieve an f-score greater than 0.15, and most
perform even worse when the number of clusters
matches the number of genetic families or sub-
families. This indicates that the induced clusters
based on typological features are very different
from genetic groupings.
The question of similarity between these in-
duced clusters and the genetic families is however
a separate one from how those clusters perform in
predicting typological feature values.
4.2 Prediction Accuracy
To determine the amount of similarity between
languages within clusters, we instead look at pre-
diction accuracy across clustering methods and
the genetic groups. These scores are similar to
those given in Daume? (2009), though not directly
comparable due to small discrepancies in the size
of the data set. As can be seen by the numbers
in Table 3 and the graph in 2(b), despite the lack
of similarity between clustering methods and the
genetic groups, the clustering methods produce
as good or better prediction accuracies. Further-
more, the agglo and bagglo hierarchical clus-
tering methods which are favored for producing
phylogenetically motivated clusters do indeed re-
sult in higher f-score similarity to the genetic clus-
ters than the partitional rb and direct methods,
but produce poorer prediction-accuracy results.
In fact, it is not surprising that some induced
clusters outperform the genetic groupings in pre-
diction accuracy, considering that clustering algo-
rithms often want to maximize the similarity be-
tween languages in the same clusters. Now that
we know similarity between languages does not
necessarily mirror language family membership,
the next question is what features tend to stay the
same among languages in the same language fam-
ilies.
4.3 Feature Selection
Our final experiment was to examine the features
in WALS themselves, and look for features that
appear to vary the least within families, and act as
better predictors of family membership.
In order to do this, we again looked at predic-
tion accuracy information on a feature-by-feature
basis. The results from this experiment are shown
in Table 4, which gives a breakdown of how fea-
tures rank both individually and by category.
Since this table is built upon genetic relation-
ships, it is not surprising that the category for
?Lexicon? appears to be the most reliably stable
category. As noted in (McMahon, 1994), lexi-
cal cognates are often used as good evidence for
determining a shared ancestry. We also find that
word order is rather stable within a family.
We ran one further experiment where, using the
agglo clustering method that provided clusters
most similar to the genetic families previously,
only features that showed accuracies above 50%.
This eliminated 28 features, leaving 111 higher-
scoring features for the whole-world set. Pruning
the features to use only these selected for their sta-
bility within the genetic groupings yielded a very
small increase in f-score similarity, as can be seen
in Figure 3. Although this increase is small, it sug-
gests that more advanced feature selection meth-
ods may be able to reveal language features that
are more resistant to language contact and lan-
guage change.
5 Error Analysis
There are two main reasons for the differences be-
tween induced clusters and genetic groupings.
5.1 Language Similarity vs. Genetic
Relatedness
As mentioned before, language similarity and ge-
netic relatedness are two different concepts. Simi-
389
baseline gold rb agglo bagglo direct k-medoids withsimilarity overlap
k-medoids with
cosine similarity
Whole-World-Set (121 Clusters)
F-Score 0.087 ? 0.080 0.140 0.119 0.089 0.081 0.088
Acc (%) 53.72 63.43 64.33 62.86 61.44 65.47 62.11 63.36
Indo-European Subset (10 Clusters)
F-Score 0.319 ? 0.365 0.377 0.391 0.355 0.352 0.331
Acc (%) 64.27 74.1 71.12 72.26 70.62 74.13 73.36 72.12
Sino-Tibetan Subset (14 Clusters)
F-Score 0.305 ? 0.224 0.340 0.333 0.220 0.285 0.251
Acc (%) 58.08 61.71 63.93 63.74 63.06 65.31 64.55 63.94
Table 3: Comparison of clustering algorithms when the number of clusters is set to the same number of
genetic groupings. The highest number in each row is in boldface.
F-Sc
ore
0.04
0.06
0.08
0.10
0.12
0.14
0.16
Number of Clusters20 40 60 80 100 120 140 160 180 200
(a) F-scores of clustering results
Pred
ictio
n Ac
cura
cy
56
58
60
62
64
66
Number of Clusters20 40 60 80 100 120 140 160 180 200
CLUTO-rbCLUTO-agglo CLUTO-baggloCLUTO-directKmedoid-overlap Kmedoid-cosine Gold
(b) Prediction accuracy
Figure 2: Comparison of the performances of different clustering methods using the whole-world data
set. The number of groups in the gold standard (i.e., genetic grouping) is shown as a vertical dashed
line in 2(a) and 2(b), and the prediction accuracy of the gold standard as a horizontal solid line in 2(b).
F-Sc
ore
0.09
0.10
0.11
0.12
0.13
0.14
0.15
0.16
Number of Clusters20 40 60 80 100 120 140 160 180 200
agglo - all featuresagglo - predictive features
Figure 3: F-scores of the agglo clustering
method when using all the features vs. only fea-
tures whose prediction accuracy by the genetic
grouping is higher than 50%.
lar languages might not be genetically related and
dissimilar languages might be genetically related.
An example is given in Table 5. Persian and En-
glish are both Indo-European languages, but look
very different typologically; in contrast, Finnish
and English are not genetically related but they
look more similar typologically. While English
and Persian are related, they have been diverg-
ing in geographically distant areas for thousands
of years. Thus, the fact that English appears to
share more features with a geographically closer
Finnish is expected.
5.2 WALS as the Dataset
Perhaps the biggest challenge we encounter in this
project has been the dataset itself. WALS has cer-
tain properties that complicate the task.
Data Sparsity and Shared Features
While the previous example shows unrelated
languages can be quite similar typologically, our
clustering methods put two closely related lan-
guages, Eastern and Western Armenian, into dif-
390
Breakdown by Feature Category Breakdown By Feature: Top 10 Breakdown by Feature: Bottom 10
Category Accuracy Feature Acc C V Feature Acc C V
Whole-World Set
Lexicon 75.0% (136) M-T Pronouns 94.0% 230 3 (1) Consonant Inventories 32.6% 561 5
Word Order 68.6% (18) Absence of Common Consonants 93.7% 565 6 (133) Number of Basic Color Categories 33.3% 119 7
Phonology 65.9% (11) Front Rounded Vowels 91.1% 560 4 (23) Locus of Marking in the Clause 33.9% 236 5
Complex Sentences 64.0% (73) The Optative 89.6% 319 2 (71) The Prohibitive 34.6% 495 4
Nominal Syntax 63.2% (137) N-M Pronouns 87.9% 230 3 (22) Inflectional Synthesis of the Verb 35.1% 145 7
Verbal Categories 61.9% (6) Uvular Consonants 85.0% 565 4 (56) Conjunctions and Universal Quantifiers 38.2% 116 3
Simple Clauses 60.5% (130) Finger and Hand 84.4% 591 2 (117) Predicative Possession 39.4% 240 5
Nominal Categories 59.1% (115) Negative Indefinite Pronouns 84.2% 206 4 (92) Position of Polar Question Particles 40.0% 775 6
Morphology 53.9% (19) Presence of Uncommon Consonants 83.0% 565 7 (38) Indefinite Articles 40.4% 473 5
Other 41.3% (58) Obligatory Possessive Inflection 81.4% 244 2 (50) Asymmetrical Case-Marking 40.7% 261 6
Indo-European Subset
Lexicon 86.4% (130) Finger and Hand 100.0% 35 2 (3) Consonant-Vowel Ratio 30.6% 31 5
Morphology 83.1% (118) Predicative Adjectives 100.0% 29 3 (92) Position of Polar Question Particles 34.6% 47 6
Word Order 79.6% (18) Absence of Common Consonants 100.0% 31 6 (78) Coding of Evidentiality 36.0% 23 6
Simple Clauses 76.6% (107) Passive Constructions 100.0% 19 2 (1) Consonant Inventories 42.4% 31 5
Nominal Categories 70.4% (88) Order of Demonstrative and Noun 97.2% 66 6 (2) Vowel Quality Inventories 44.4% 31 3
Phonology 66.7% (89) Order of Numeral and Noun 95.7% 64 4 (84) Order of Object, Oblique, and Verb 47.8% 20 6
Verbal Categories 62.1% (27) Reduplication 95.2% 20 3 (16) Weight Factors in Weight-Sensitive
Stress Systems
51.1% 53 7
(7) Glottalized Consonants 93.9% 31 8 (70) The Morphological Imperative 55.3% 53 5
(93) Position of Interrogative Phrases in Con-
tent Questions
93.9% 44 3 (44) Gender Distinctions in Independent Per-
sonal Pronouns
56.5% 19 6
(5) Voicing and Gaps in Plosive Systems 93.8% 31 5 (37) Definite Articles 59.2% 46 5
Sino-Tibetan Subset
Lexicon 100.0% (130) Finger and Hand 100.0% 8 2 (77) Semantic Distinctions of Evidentiality 9.1% 18 3
Word Order 67.7% (82) Order of Subject and Verb 100.0% 99 3 (78) Coding of Evidentiality 17.7% 18 6
Morphology 63.8% (119) Nominal and Locational Predication 100.0% 13 2 (4) Voicing in Plosives and Fricatives 20.7% 26 4
Simple Clauses 60.9% (86) Order of Genitive and Noun 100.0% 73 3 (1) Consonant Inventories 22.2% 26 5
Verbal Categories 60.7% (129) Hand and Arm 100.0% 8 2 (14) Fixed Stress Locations 25.0% 4 7
Nominal Categories 55.8% (18) Absence of Common Consonants 100.0% 26 6 (15) Weight-Sensitive Stress 25.0% 4 8
Phonology 50.7% (93) Pos. of Interr. Phrases in Content Q?s 100.0% 79 3 (38) Indefinite Articles 31.7% 36 5
(85) Order of Adposition and Noun Phrase 97.5% 79 5 (120) Zero Copula for Predicate Nominals 37.5% 13 2
(95) Relationship b/t Object and Verb and Ad-
position and Noun Phrase
96.3% 76 5 (2) Vowel Quality Inventories 42.9% 26 3
(48) Person Marking on Adpositions 93.3% 14 4 (3) Consonant-Vowel Ratio 46.7% 26 5
Table 4: Prediction accuracy figures derived from genetic groupings for each dataset and broken down
by WALS feature category and feature. Ordering is by descending accuracy for the top 10 features,
and by increasing accuracy for the bottom 10 features. The ?C? and ?V? columns give the number
of languages in the set that a feature appears in, and the number of possible values for that feature,
respectively.
ferent clusters. A quick review shows that the rea-
son for this mistake is due to a lack of shared fea-
tures in WALS. Table 6 shows that very few fea-
tures are specified for both languages. The data
sparsity problem and the distribution of empty
cells adversely affect clustering results.
Notice that in this example, the features whose
values are filled for both languages actually have
identical feature values. While using shared over-
lap as a similarity measure can capture the simi-
larity between these two languages, this measure
biases clustering toward features with fewer cells
filled out. The only way out of errors like this, it
seems, is to obtain more data.
There are a few other typological databases
that might be drawn upon to define a more com-
plete set of data: PHOIBLE, (Moran and Wright,
2009), ODIN (Lewis, 2006), and the AUTOTYP
database (Nichols and Bickel, 2009). Using these
databases to fill in the gaps in data may be the only
way to fully address these issues.
The Feature Set in WALS
The features in WALS are not systematically
chosen for full typological coverage; rather, the
contributors to WALS decide what features they
want to work on based on their expertise. Also,
some features in WALS overlap; for example, one
WALS feature looks at the order between subject,
verb, and object, and another feature checks the
order between verb and object. As a result, the
feature set in WALS might not be a good represen-
tative of the properties of the languages covered in
the database.
6 Conclusion & Further Work
By comparing clusters derived from typological
features to genetic groups in the world?s lan-
guages, we have found two interesting results.
First, the induced clusters look very different from
genetic grouping and this is partly due to the de-
sign of WALS. Second, despite the differences, in-
duced clusters show similar, or even greater levels
391
ID: Feature Name English Finnish Persian
2: Vowel Quality Invento-
ries
Large (7-14) Large (7-14) Average (5-6)
6: Uvular Consonants None None Uvular stops only
11: Front Rounded Vow-
els
None High and Mid None
27: Reduplication No productive redupli-
cation
No productive redupli-
cation
Productive full and partial
reduplication
37: Definite Articles Definite word distinct
from demonstrative
No definite or indefinite
article
No definite, but indefinite
article
53: Ordinal Numerals First, second, three-th First, second, three-th First/one-th, two-th,
three-th
81: Order of Subject, Ob-
ject and Verb
SVO SVO SOV
85: Order of Adposition
and Noun Phrase
Prepositions Postpositions Prepositions
87: Order of Adjective
and Noun
Adjective-Noun Adjective-Noun Noun-Adjective
124: ?Want? Complement
Subjects
Subject left implicit Subject left implicit Subject expressed overtly
Number of Features 139 135 128
Cosine Similarity to Eng 1.00 0.56 0.42
Shared Overlap with Eng 1.00 0.56 0.44
Table 5: A selection of ten features from English, Finnish, and Persian. Same feature values in each
row are in boldface. Despite the genetic relation between English and Persian, similarity metrics place
English closer to Finnish than Persian.
ID# Feature Name Armenian (Eastern) Armenian (Western)
1 Consonant Inventories Small ?
27 Reduplication Full Reduplication Only Full Reduplication Only
33 Coding of Nominal Plurality ? Plural suffix
48 Person Marking on Adj. None ?
81 Order of Subj. Obj., and V ? SOV
86 Order of Adposition and Noun Phrase Postpositions Postpositions
100 Alignment of Verbal Person Marking Accusative ?
129 Hand and Arm ? Identical
Number of Features 85 33
Cosine Similarity 0.22
Shared Overlap 1.00
Table 6: Comparison of features between Eastern and Western Armenian. Same feature values in each
row are in boldface. Empty cells are shown as ???.
of typological similarity than genetic grouping as
indicated by the prediction accuracy.
While these initial findings are interesting, us-
ing WALS as a dataset for this purpose leaves a lot
to be desired. Subsequent work that supplements
the typological data in WALS with the databases
mentioned in ?5.2 would help alleviate the data
sparsity and feature selection problems.
Another useful follow-up would be to perform
application-oriented evaluations. For instance,
evaluating the performance of syntactic projection
methods between languages determined to have
similar syntactic patterns, or using similar mor-
phological induction techniques on morphologi-
cally similar languages. With the development
of large typological databases such as WALS, we
hope to see more studies that take advantage of
resources for resource-rich languages when devel-
oping tools for typologically similar, but resource-
poor languages.
Acknowledgment This work is supported by
the National Science Foundation Grant BCS-
0748919. We would also like to thank Emily Ben-
der, Tim Baldwin, and three anonymous reviewers
for helpful comments.
392
References
Daume?, III, Hal and Daniel Marcu. 2005. A Bayesian
Model for Supervised Clustering with the Dirich-
let Process Prior. Journal of Machine Learning Re-
search, 6:1551?1577.
Daume?, III, Hal. 2009. Non-Parametric Bayesian
Areal Linguistics. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL), pages
593?601, Boulder, Colorado, June.
Estivill-Castro, Vladimir and Jianhua Yang. 2000.
A fast and robust general purpose clustering algo-
rithm. In Proc. of Pacific Rim International Con-
ference on Artificial Intelligence, pages 208?218.
Springer.
Hana, Jiri, Anna Feldman, and Chris Brew. 2004. A
Resource-light Approach to Russian Morphology:
Tagging Russian using Czech resources. In Pro-
ceedings of EMNLP 2004, Barcelona, Spain.
Haspelmath, Martin, Matthew S. Dryer, David Gil, and
Bernard Comrie. 2005. The World Atlas of Lan-
guage Structures. Oxford University Press, Oxford,
England.
Lewis, William D. 2006. ODIN: A Model for Adapt-
ing and Enriching Legacy Infrastructure. In Pro-
ceedings of the e-Humanities Workshop, held in co-
operation with e-Science 2006: 2nd IEEE Interna-
tional Conference on e-Science and Grid Comput-
ing, Amsterdam.
McMahon, April M. S. 1994. Understanding lan-
guage change. Cambridge University Press, Cam-
bridge; New York, NY, USA.
Moran, Steven and Richard Wright. 2009. Phonetics
Information Base and Lexicon (PHOIBLE). Online:
http://phoible.org.
Nichols, Johanna and Balthasar Bickel. 2009.
The AUTOTYP genealogy and geography database:
2009 release. http://www.uni-leipzig.
de/?autotyp.
Rand, William M. 1971. Objective criteria for the
evaluation of clustering methods. Journal of the
American Statistical Association, 66(336):846?850.
Steinbach, Michael, George Karypis, and Vipin Ku-
mar. 2000. A comparison of document clustering
techniques. In Proceedings of Workshop at KDD
2000 on Text Mining.
Xia, Fei and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452?459,
Rochester, New York.
Yarowsky, David and Grace Ngai. 2001. Inducing
multilingual pos taggers and np bracketers via ro-
bust projection across aligned corpora. In Proc. of
the Second meeting of the North American Chapter
of the Association for Computational Linguistics on
Language technologies (NAACL-2001), pages 1?8,
Morristown, NJ, USA.
393
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 306?311,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Enhanced and Portable Dependency Projection AlgorithmsUsing Interlinear Glossed Text
Ryan Georgi
University of Washington
Seattle, WA 98195, USA
rgeorgi@uw.edu
Fei Xia
University of Washington
Seattle, WA 98195, USA
fxia@uw.edu
William D. Lewis
Microsoft Research
Redmond, WA 98052, USA
wilewis@microsoft.com
Abstract
As most of the world?s languages are
under-resourced, projection algorithms
offer an enticing way to bootstrap the
resources available for one resource-
poor language from a resource-rich lan-
guage by means of parallel text and
word alignment. These algorithms,
however, make the strong assumption
that the language pairs share common
structures and that the parse trees will
resemble one another. This assump-
tion is useful but often leads to errors
in projection. In this paper, we will
address this weakness by using trees
created from instances of Interlinear
Glossed Text (IGT) to discover pat-
terns of divergence between the lan-
guages. We will show that this method
improves the performance of projection
algorithms significantly in some lan-
guages by accounting for divergence be-
tween languages using only the partial
supervision of a few corrected trees.
1 Introduction
While thousands of languages are spoken
in the world, most of them are considered
resource-poor in the sense that they do not
have a large number of electronic resources
that can be used to build NLP systems. For
instance, some languages may lack treebanks,
thus making it difficult to build a high-quality
statistical parser.
One common approach to address this prob-
lem is to take advantage of bitext between a
resource-rich language (e.g., English) and a
resource-poor language by projecting informa-
tion from the former to the latter (Yarowsky
and Ngai, 2001; Hwa et al, 2004). While pro-
jection methods can provide a great deal of in-
formation at minimal cost to the researchers,
they do suffer from structural divergence be-
tween the language-poor language (aka target
language) and the resource-rich language (aka
source language).
In this paper, we propose a middle ground
between manually creating a large-scale tree-
bank (which is expensive and time-consuming)
and relying on the syntactic structures pro-
duced by a projection algorithm alone (which
are error-prone).
Our approach has several steps. First, we
utilize instances of Interlinear Glossed Text
(IGT) following Xia and Lewis (2007) as seen
in Figure 1(a) to create a small set of parallel
dependency trees through projection and then
manually correct the dependency trees. Sec-
ond, we automatically analyze this small set
of parallel trees to find patterns where the cor-
rected data differs from the projection. Third,
those patterns are incorporated to the projec-
tion algorithm to improve the quality of pro-
jection. Finally, the features extracted from
the projected trees are added to a statisti-
cal parser to improve parsing quality. The
outcome of this work are both an enhanced
projection algorithm and a better parser for
resource-poor languages that require a mini-
mal amount of manual effort.
2 Previous Work
For this paper, we will be building upon
the standard projection algorithm for depen-
dency structures as outlined in Quirk et al
(2005) and illustrated in Figure 1. First, a
sentence pair between resource-rich (source)
and resource-poor (target) languages is word
aligned [Fig 1(a)]. Second, the source sen-
tence is parsed by a dependency parser for
the source language [Fig 1(b)]. Third, sponta-
306
siwA ne pAnI se GadZe ko BarA
Sita filled the clay-pot with water
Sita erg water with clay-pot acc filled
(a) An Interlinear Glossed Text (IGT) instance in Hindiand word alignment between the gloss line and theEnglish translation.
Sita
filled
the
clay-pot with
water
(b) Dependency parse of English translation.
siwA
BarA
the
GadZe se
pAnI
(c) English words are replaced with Hindi words andspontaneous word ?the? are removed from the tree.
siwA
BarA
GadZese
pAnIne ko
(d) Siblings in the tree are reordered based on the wordorder of the Hindi sentence and spontaneous Hindiwords are attached as indicated by dotted lines. Thewords pAnI and se are incorrectly inverted, as indi-cated by the curved arrow.
Figure 1: An example of projecting a depen-
dency tree from English to Hindi.
neous (unaligned) source words are removed,
and the remaining words are replaced with
corresponding words in the target side [Fig
1(c)]. Finally, spontaneous target words are
re-attached heuristically and the children of a
head are ordered based on the word order in
the target sentence [Fig 1(d)]. The resulting
tree may have errors (e.g., pAni should depend
on se in Figure 1(d)), and the goal of this study
is to reduce common types of projection errors.
In Georgi et al (2012a), we proposed a
method for analyzing parallel dependency cor-
pora in which word alignment between trees
was used to determine three types of edge con-
figurations: merged, swapped, and spon-
taneous. Merged alignments were those in
which multiple words in the target tree aligned
to a single word in the source tree, as in Figure
2. Swapped alignments were those in which
a parent node in the source tree aligned to a
child in the target tree and vice-versa. Finally,
spontaneous alignments were those for which
a word did not align to any word on the other
side. These edge configurations could be de-
tected from simple parent?child edges and the
alignment (or lack of) between words in the
language pairs. Using these simple, language-
agnostic measures allows one to look for diver-
gence types such as those described by Dorr
(1994).
Georgi et al (2012b) described a method
in which new features were extracted from
the projected trees and added to the feature
vectors for a statistical dependency parser.
The rationale was that, although the projected
trees were error-prone, the parsing model
should be able to set appropriate weights of
these features based on how reliable these fea-
tures were in indicating the dependency struc-
ture. We started with the MSTParser (Mc-
Donald et al, 2005) and modified it so that the
edges from the projected trees could be used
as features at parse time. Experiments showed
that adding new features improved parsing
performance.
In this paper, we use the small training cor-
pus built in Georgi et al (2012b) to improve
the projection algorithm itself. The improved
projected trees are in turn fed to the statistical
parser to further improve parsing results.
3 Enhancements to the projection
algorithm
We propose to enhance the projection algo-
rithm by addressing the three alignment types
discussed earlier:
1. Merge: better informed choice for head
for multiply-aligned words.2. Swap: post-projection correction of fre-
quently swapped word pairs.3. Spontaneous: better informed attach-
ment of target spontaneous words.
The detail of the enhancements are ex-
plained below.
3.1 Merge Correction
?Merged? words, or multiple words on the tar-
get side that align to a single source word, are
problematic for the projection algorithm be-
cause it is not clear which target word should
be the head and which word should be the
307
rAma buxXimAna lagawA hE
Ram intelligent seem be-Pres
?Ram seems intelligent?
seems
VBZ
Ram
NNP
intelligent
JJ
lagawA
seems
ram
Ram
buxXimAna
intelligent
hE
be-Pres
Figure 2: An example of merged alignment,
where the English word seems align to two
Hindi words hE and lagawA. Below the IGT
are the dependency trees for English and
Hindi. Dotted arrows indicate word align-
ment, and the solid arrow indicates that hE
should depend on lagawA.
dependent. An example is given in Figure 2,
where the English word seems align to two
Hindi words hE and lagawA.
On the other hand, from the small amount
of labeled training data (i.e., a set of hand-
corrected tree pairs), we can learn what kind
of source words are likely to align to multiple
target words, and which target word is likely to
the head. The process is illustrated in Figure
3. In this example, the target words tm and
tn are both aligned with the source word siwhose POS tag is POSi, and tm appears before
tn in the target sentence. Going through theexamples of merged alignments in the training
data, we keep a count for the POS tag of the
source word and the position of the head on
the target side.1 Based on these counts, our
system will generate rules such as the ones in
Figure 3(c) which says if a source word whose
POS is POSi aligns to two target words, theprobability of the right target word depending
on the left one is 75%, and the probability of
the left target word depending on the right one
is 25%. We use maximum likelihood estimate
(MLE) to calculate the probability.
The projection algorithm will use those rules
to handle merged alignment; that is, when a
source word aligns to multiple target words,
the algorithm determines the direction of de-
pendency edge based on the direction prefer-
ence stored in the rules. In addition to rules for
1We use the position of the head, not the POS tag ofthe head, because the POS tags of the target words arenot available when running the projection algorithm onthe test data.
s
i
POS
i
t
m
t
n
(a) Alignment between a source word and two targetwords, and one target word tm is the parent of theother word tn.
t
m
t
n
t
o
... t
p
(b) Target sentence showing the ?left? dependency be-tween tm and tn.
POSi ? left 0.75
POSi ? right 0.25
(c) Rules for handling merged alignment
Figure 3: Example of merged alignment and
rules derived from such an example
an individual source POS tag, our method also
keeps track of the overall direction preference
for all the merged examples in that language.
For merges in which the source POS tag is un-
seen or there are no rules for that tag, this
language-wide preference is used as a backoff.
3.2 Swap Correction
An example of swapped alignment is in Figure
4(a), where (sj , si) is an edge in the sourcetree, (tm, tn) is an edge in the target tree, and
sj aligns to tn and si aligns to tm. Figure1(d) shows an error made by the projection
algorithm due to swapped alignment. In order
to correct such errors, we count the number
of (POSchild, POSparent) dependency edges inthe source trees, and the number of times that
the directions of the edges are reversed on the
target side. Figure 4(b) shows a possible set of
counts resulting from this approach. Based on
the counts, we keep only the POS pairs that
appear in at least 10% of training sentences
and the percentage of swap for the pairs are
no less than 70%.2 We say that those pairs
trigger a swap operation.
At the test time, swap rules are applied as a
post-processing step to the projected tree. Af-
ter the projected tree is completed, our swap
handling step checks each edge in the source
tree. If the POS tag pair for the edge triggers
2These thresholds are set empirically.
308
si
POS
i
t
m
t
n
s
j
POS
j
(a) A swapped alignment between source words sj and
si and target words tm and tn.
POS Pair Swaps Total %
(POSi, POSj) ? 16 21 76(POSk, POSl) ? 1 1 100(POSn, POSo) ? 1 10 10
(b) Example set of learned swap rules. Swaps counts thenumber of times the given (child, parent) pair is seenin a swap configuration in the source side, and totalis the number of times said pair occurs overall.
Figure 4: Example swap configuration and col-
lected statistics.
j
l m n
o p
h
i k l
m n
o p
h
i k
j
Figure 5: Swap operation: on the left is the
original tree; on the right is the tree after
swapping node l with its parent j.
a swap operation, the corresponding nodes in
the projected tree will be swapped, as illus-
trated in Figure 5.
3.3 Spontaneous Reattachment
Target spontaneous words are difficult to han-
dle because they do not align to any source
word and thus there is nothing to project to
them. To address this problem, we collect two
types of information from the training data.
First, we keep track of all the lexical items
that appear in the training trees, and the rel-
ative position of their head. This lexical ap-
proach may be useful in handling closed-class
words which account for a large percentage of
spontaneous words. Second, we use the train-
ing trees to determine the favored attachment
direction for the language as a whole.
At the test time, for each spontaneous word
in the target sentence, if it is one of the words
for which we have gathered statistics from the
training data, we attach it to the next word
in the preferred direction for that word. If the
word is unseen, we attach it using the overall
language preference as a backoff.
3.4 Parser Enhancements
In addition to above enhancements to the pro-
jection algorithm itself, we train a dependency
parser on the training data, with new features
from the projected trees following Georgi et al
(2012b). Furthermore, we add features that
indicate whether the current word appears in
a merge or swap configuration. The results
of this combination of additional features and
improved projection is shown in Table 1(b).
4 Results
For evaluation, we use the same data sets as
in Georgi et al (2012b), where there is a small
number (ranging from 46 to 147) of tree pairs
for each of the eight languages. The IGT
instances for those tree pairs come from the
Hindi Treebank (Bhatt et al, 2009) and the
Online Database of Interlinear Text (ODIN)
(Lewis and Xia, 2010).
We ran 10-fold cross validation and reported
the average of 10 runs in Table 1. The top ta-
ble shows the accuracy of the projection algo-
rithm, and the bottom table shows parsing ac-
curacy of MSTParser with or without adding
features from the projected trees. In both ta-
bles, the Best row uses the enhanced projec-
tion algorithm. The Baseline rows use the
original projection algorithm in Quirk et al
(2005) where the word in the parentheses in-
dicates the direction of merge. The Error Re-
duction row shows the error reduction of the
Best system over the best performing baseline
for each language. The No Projection row in
the second table shows parsing results when
no features from the projected trees are added
to the parser, and the last row in that table
shows the error reduction of the Best row over
the No Projection row.
Table 1 shows that using features from the
projected trees provides a big boost to the
quality of the statistical parser. Furthermore,
the enhancements laid out in Section 3 im-
prove the performance of both the projection
algorithm and the parser that uses features
from projected trees. The degree of improve-
ment may depend on the properties of a par-
ticular language pair and the labeled data we
309
(a) The accuracies of the original projection algorithm (the Baselin rows) and the enhanced algorithm (the Bestrow) on eight language pairs. For each language, the best performing baseline is in italic. The last row showsthe error reduction of the Best row over the best performing baseline, which is calculated by the formula
ErrorRate = Best?BestBaseline100?BestBaseline ? 100
YAQ WLS HIN KKN GLI HUA GER MEX
Best 88.03 94.90 77.44 91.75 87.70 90.11 88.71 93.05
Baseline (Right) 87.28 89.80 57.48 90.34 86.90 79.31 88.03 89.57
Baseline (Left) 84.29 89.80 68.11 88.93 76.98 79.54 88.03 89.57
Error Reduction 5.90 50.00 29.26 14.60 6.11 51.66 5.68 33.37
(b) The parsing accuracies of the MSTParser with or without new features extracted from projected trees. Thereare two error reduction rows: one is with respect to the best performing baseline for each language, the otheris with respect to No Projection where the parser does not use features from projected trees.
YAQ WLS HIN KKN GLI HUA GER MEX
Best 89.28 94.90 81.35 92.96 81.35 88.74 92.93 93.05
Baseline (Right) 88.28 94.22 78.03 92.35 80.95 87.59 90.48 92.43
Baseline (Left) 87.88 94.22 79.64 90.95 80.95 89.20 90.48 92.43
No Projection 66.08 91.32 65.16 80.75 55.16 72.22 62.72 73.03
Error Reduction (BestBaseline) 8.53 11.76 8.40 7.97 2.10 -4.26 25.74 8.19
Error Reduction (No Projection) 68.39 41.24 46.47 63.43 58.41 59.47 81.04 74.23
Table 1: System performance on eight languages: Yaqui (YAQ), Welsh (WLS), Hindi (HIN),
Korean (KKN), Gaelic (GLI), Hausa (HUA), German (GER), and Malagasy (MEX).
have for that language pair. For instance,
swap is quite common for the Hindi-English
pair because postpositions depend on nouns
in Hindi whereas nouns depend on preposi-
tions in English. As a result, the enhancement
for the swapped alignment alone results in a
large error reduction, as in Table 2. This ta-
ble shows the projection accuracy on the Hindi
data when each of the three enhancements is
turned on or off. The rows are sorted by de-
scending overall accuracy, and the row that
corresponds to the system labeled ?Best? in
Table 1 is in bold.
5 Conclusion
Existing projection algorithms suffer from the
effects of structural divergence between lan-
guage pairs. We propose to learn common di-
vergence types from a small number of tree
pairs and use the learned rules to improve pro-
jection accuracy. Our experiments show no-
table gains for both projection and parsing
when tested on eight language pairs. As IGT
data is available for hundreds of languages
through the ODIN database and other sources,
one could produce a small parallel treebank
for a language pair after spending a few hours
manually correcting the output of a projec-
tion algorithm. From the treebank, a bet-
ter projection algorithm and a better parser
can be built automatically using our approach.
Spont Swap Merge Direction Accuracy
X X Left 78.07
X X Informed 77.44
X Left 76.69
X Informed 76.06
X Left 69.49
X Informed 68.96
Left 68.11
Informed 67.58
X X Right 66.32
X Right 64.97
X Right 58.84
Right 57.48
Table 2: Projection accuracy on the Hindi
data, with the three enhancements turning
on or off. The ?spont? and ?swap? columns
show a checkmark when the enhancements
are turned on. The merge direction indicates
whether a left or right choice was made as a
baseline, or whether the choice was informed
by the rules learned from the training data.
While the improvements for some languages
are incremental, the scope of coverage for this
method is potentially enormous, enabling the
rapid creation of tools for under-resourced lan-
guages of all kinds at a minimal cost.
Acknowledgment
This work is supported by the National Sci-
ence Foundation Grant BCS-0748919. We
would also like to thank the reviewers for help-
ful comments.
310
References
Rajesh Bhatt, Bhuvana Narasimhan, Martha
Palmer, Owen Rambow, Dipti Misra
Sharma, and Fei Xia. A multi-
representational and multi-layered treebank
for Hindi/Urdu. In ACL-IJCNLP ?09: Pro-
ceedings of the Third Linguistic Annotation
Workshop. Association for Computational
Linguistics, August 2009.
Bonnie Jean Dorr. Machine translation di-
vergences: a formal description and pro-
posed solution. Computational Linguistics,
20:597?633, December 1994.
R Georgi, F Xia, and W D Lewis. Measur-
ing the Divergence of Dependency Struc-
tures Cross-Linguistically to Improve Syn-
tactic Projection Algorithms. In Proceedings
of the Sixth International Conference on
Language Resources and Evaluation (LREC
2012), Istanbul, Turkey, May 2012a.
Ryan Georgi, Fei Xia, and William D Lewis.
Improving Dependency Parsing with Inter-
linear Glossed Text and Syntactic Projec-
tion. In Proceedings of the 24th Interna-
tional Conference on Computational Lin-
guistics (COLING 2012), Mumbai, India,
December 2012b.
Rebecca Hwa, Philip Resnik, Amy Weinberg,
Clara Cabezas, and Okan Kolak. Bootstrap-
ping parsers via syntactic projection across
parallel texts. Natural Language Engineer-
ing, 1(1):1?15, 2004.
William D Lewis and Fei Xia. Developing
ODIN: A Multilingual Repository of Anno-
tated Language Data for Hundreds of the
World?s Languages. 2010.
R. McDonald, F. Pereira, K. Ribarov, and
J. Haji?. Non-projective dependency parsing
using spanning tree algorithms. Proceedings
of the conference on Human Language Tech-
nology and Empirical Methods in Natural
Language Processing, pages 523?530, 2005.
Chris Quirk, Arul Menezes, and Colin Cherry.
Dependency treelet translation: Syntacti-
cally informed phrasal SMT. In Proceed-
ings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics. Mi-
crosoft Research, 2005.
Fei Xia and William D Lewis. Multilin-
gual Structural Projection across Interlin-
ear Text. In Human Language Technologies:
The Annual Conference of the North Amer-
ican Chapter of the Association for Compu-
tational Linguistics (NAACL), 2007.
David Yarowsky and Grace Ngai. Inducing
multilingual POS taggers and NP bracketers
via robust projection across aligned corpora.
In Second meeting of the North American
Association for Computational Linguistics
(NAACL), Stroudsburg, PA, 2001. Johns
Hopkins University.
311
