Obtaining Japanese Lexical Units for Semantic Frames
from Berkeley FrameNet Using a Bilingual Corpus
Toshiyuki Kanamaru
Kyoto University
Yoshida Nihonmatsu-cho, Sakyo-ku
Kyoto, 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Masaki Murata Kow Kuroda Hitoshi Isahara
National Institute of Information and
Communications Technology (NICT)
3-5 Hikaridai, Seikacho, Sorakugun
Kyoto, 619-0289, Japan
{murata,kuroda,isahara}@nict.go.jp
Abstract
An attempt was made to semi-automatically ob-
tain ?lexical units? (LUs) for Japanese from
the English LUs defined in the semantic frame
database provided by Berkeley FrameNet (BFN)
using an English-Japanese bilingual corpus.
This task was a prerequisite to building a com-
plete database of semantic frames for Japanese.
In the task, a Japanese word is first translated
into an English word or phrase, E. E is one
of the lexical units that evoked a particular se-
mantic frame, F , in the BFN database. When
other lexical units of F are translated back into
Japanese, this defines a candidate set of F for
the lexical units of F in Japanese. The via-
bility of the proposed method was tested on a
Japanese verb (X-ga Y -wo) osou (roughly mean-
ing ?X attack(s) Y ,? ?X hit(s) Y ,? ?X surprise(s)
Y ? in English, showing that it is a relatively pol-
ysemous word). The resulting translation was
compared to semantic descriptions provided by
IPAL and Nihongo Goi-Taikei (A Japanese Lex-
icon), two well-known language resources for
Japanese, and also by the Frame Oriented Con-
cept Analysis of Language (FOCAL). The com-
parison revealed that FOCAL, BFN, Goi Taikei,
and IPAL provided finer-grained descriptions in
this specific order.
1 Introduction
Making use of deep semantics in information pro-
cessing is one of the major problems confronting
today?s NLP community. More and more NLP
researchers are realizing that they need seman-
tic/lexical resources that go beyond such ones as
WordNet (Fellbaum, 1998) that only specify hier-
archical semantic relationships. One of the cru-
cial reasons for this is that raw linguistic data
embodies semantic associations that are difficult
to capture in terms of such hierarchical relation-
ships, one of which is the so-called ?semantic
field? effect, a class of associative relationships
among words (or concepts). To deal with these
issues, deeper semantics are needed with descrip-
tions that incorporate ontological inferences. Let
us assume that X attacked Y is to be interpreted.1
This is a complex situation. In interpreting The
man attacked a bank, it may be necessary to spec-
ify (by inference) that the subject used a weapon
(e.g., a gun) and his purpose was to obtain money
(illegally), whereas in interpreting The wolf at-
tacked a flock of sheep, it may be necessary to
specify that the subject never used a weapon and
its purpose was to eat one or two individual sheep
(rather than the entire flock) after killing them.
Relevant inferences are clearly situation-based, or
?case-based? in the sense of Case-based Reason-
ing (Kolodner, 1993), and difficult to specify in
terms of the lexical semantic descriptions avail-
able in resources such as WordNet (Fellbaum,
1998) which don?t specify associative relation-
ships among concepts, including the relationships
between ROBBER (e.g., a man) and WAREHOUSE
OF VALUABLES (e.g., a bank, museum, jewelry
shop), and the one between a PREDATOR (e.g., a
wolf) and its PREY (e.g., sheep, rabbit). Thus, the
NLP community has a critical need for resources
that encode this kind of information.
Along with PropBank (Kingsbury and Palmer,
2002; Ellsworth et al, 2004), Berkeley FrameNet
1One of the anonymous reviewers told us that it was un-
clear how ontological inferences of this sort are related to
BFN?s frame definitions. The question boils down to the
question of definition, i.e., what kind of information we need
to define semantic frames to encode, and as we will see later,
this is exactly the question addressed by FOCAL claiming
that BFN frames are too coarse-grained to be used as an ef-
fective knowledge-base for ontological inferences.
11
(BFN) (Baker et al, 1998) is an ongoing research
project that is attempting to meet the demand for
resources that encode deeper lexical semantics by
providing a semantic frame lexicon (sometimes
called the ?FrameNet?) and a corpus annotated
for semantic information encoded in terms of se-
mantic frames.
Thus far, BFN has produced ?a lexical database
that currently contains more than 8,900 lexical
units, more than 6,100 of which are fully anno-
tated, in more than 625 semantic frames, exem-
plified in more than 135,000 annotated sentences?
(cited from the FrameNet web page). Other
ongoing projects, i.e., the German FrameNet
or ?SALSA? (Erk et al, 2003), the Spanish
FrameNet (Subirats and Petruck, 2003), and the
Japanese FrameNet (Ohara et al, 2003), are try-
ing to build lexical resources that are compatible
with the BFN, but for Japanese at least, no data
has been released in a usable form, except for a
few annotation examples for verbs of motion.
In sum, no useful resource exists for frame-
based description/analysis of Japanese. This is
one of the reasons that we attempted the task in
this paper, along with our efforts to assess the use-
fulness of the database provided by BFN.
The anonymous reviewers of our paper pointed
out that there have been some similar projects
and other methodologies that have tried to trans-
late BFN into other languages automatically, such
as BiFrameNet (Chen and Fung, 2004) and Ro-
mance FrameNet2, and that it would have been
better to include the comparison against them.
BiFrameNet presented an automatic approach
to constructing a bilingual semantic network us-
ing the Chinese HowNet, which is a Chinese
ontology. While it is an interesting approach,
we have not compared their results with ours,
mainly because they seem to have used differ-
ent resources and had somewhat different goals,
along with the space consideration.
No papers are released, let alne being avail-
able to us, related to the Romance FrameNet
project for the time being. We couldn?t help
putting a comparison with it on hold.3
2http://ic2.epfl.ch/?pallotta/rfn/
3One of the anonymous reviewers criticized us for failing
to mention Romance FrameNet project in our paper; it is just
unreasonable. The project was announced on June 1 on the
2 Proposed Procedure
We used a bilingual corpus (Utiyama and Isahara,
2003) to examine which semantic frames of BFN
contained LUs relevant to the Japanese verb osou.
JFN, for example, used a mono-lingual corpus to
construct the semantic frames. In cases like this,
the construction might be inefficient because they
have to construct all semantic frames by them-
selves. But this affects on the reliability of the
frames identified and described. This risk of arbi-
trary description can be reduced by using a bilin-
gual corpus, if it is of high-quality.
2.1 Identifying English equivalents of ?osou?
We chose Japanese-English alignments from the
bilingual corpus in which the Japanese text con-
tained osou, i.e., the target verb. We obtained 135
alignments from the corpus.
The bilingual corpus is consists of two subcor-
pra. One subcorpus is made of one-to-one align-
ments. Another is of one-to-many alignments. In
the latter, one Japanese sentence is aligned with
several English sentences.
In the first case, it was straightforward to spec-
ify an English word or phrase that translated the
target verb, osou. In the second case, however, it
is not. So, we singled out an English sentence that
corresponds to a Japanese sentence that contained
osou. In this process, the identification of osou?s
English translations was done manually.
After this procedure, the following five verbs
were identified as English translations of osou:
assault, attack, hit, pound, and strike4.
2.2 Identifying relevant semantic frames
Based on these five verbs, we extracted seman-
tic frames using FrameSQL (Sato, 2003). Seman-
tic frames with LUs that included any of the five
verbs were chosen from the BFN semantic frame
database (referred to here as BFN).
Corpora Mailing List, just one week before the submission
deadline. This means that we had little chance to know about
the project unless we were ?insiders.?
4There were a few other verbs or constructions that
served as English translations of osou in the alignments: for
example, besiege, engulf, feel pain, occur, hurt, kill, rob,
shoot, stab, suffer, wreak on were used as its translations.
But we filtered out those less frequent items (whose fre-
quency is less than 3) for purposes of simplicity.
12
Based on Frame Semantics (Fillmore, 1982),
BFN posits that a semantic frame is an organi-
zation of ?semantic roles,? which BFN terms as
?Frame Elements? (FEs). Usually, LUs are in-
stantiations or lexical realizations of FEs. Thus,
an LU in a frame, F , is a word, or phrase, that, ac-
cording to the assumptions of Frame Semantics,
?evokes? frame F . The definition of the ?Attack?
frame in the BFN database is used in Figure 1to
illustrate the procedure. As indicated, assault, at-
tack and strike are listed as LUs of the ?Attack?
frame.
After manually examining all the semantic
frames thus obtained, the five BFN frames were
recognized as relevant to the various senses of the
target word osou: 1. ?Attack?; 2. ?Cause harm?
3. ?Experience bodily harm? 4. ?Cause impact?
5. ?Impact?
Semantic frames in the BFN database are sup-
posedly related to one another. There are vari-
ous relationships, some of which are sometimes
encoded by establishing explicit ?frame-to-frame
relations? (such as ?is used? relation) between
two frames. Using this information, we obtained
the following relationships between the five
frames: 1. ?Attack?; 2. ?Cause harm?, is used:
?Experience bodily harm?; 3. ?Cause impact?,
uses: ?Impact?
2.3 Identifying relevant frame-evoking LUs
in English
Each semantic frame has a number of FEs, each of
which has lexical realizations, which called LUs.
In the work reported here, only verbal LUs were
selected as relevant from the English LUs made
available in the BFN database.5 Admittedly, there
5 On this point, we recognize a certain kind of discrep-
ancy between the theory and the practice in the BFN frame-
work. If a LU is, according to its defintion, a lexical realiza-
tion of a certain FE of a certain frame, more nominals should
be identified and listed as LUs. For example, in Jack or-
dered a hamburger at McDonald?s, hamburger is a noun that
evokes the ?Cooking creation? frame. While the ?Selling?
frame is evoked by order.v, this means that, according the
definition of LU, hamburger.n needs to be identified as an
LU of the ?Cooking creation? frame; more specifically, it is
an LU that instantiates the ?Food? FE of the frame. It is ob-
vious that the QUALIA STRUCTURE (Pustejovsky, 1995) of
hamburger.n contains information of this sort. We suspect
that this aspect of ?frame-evocation by nominals? does not
seem to be properly recognized and coded, and that BFN?s
current practice of mostly identifying predicates as LUs is
somewhat misleading, if we could say so, because it con-
are a few nominal LUs in certain frames in the
BFN, but we ignored them because they found
them to be less relevant to our specific task.
After identifying all the relevant LUs for the
three frames above, we obtained all the English
verbs that translated the senses of the target word
osou identified in terms of Frame Semantics.
For example, the relevant LUs for the ?Attack?
frame are the following verbs: ambush, assault,
attack, charge, invade, jump, lay, set, storm, and
strike
As was the case with the ?Attack? frame, we
extracted the relevant LUs for the ?Cause harm?
and ?Cause impact? frames. We manually
merged the extracted LUs, and obtained 93 ver-
bal LUs relevant to the Japanese verb osou.
2.4 Obtaining LU candidates for Japanese
FEs
Table 1: 15 most frequently occurring nouns
Noun Freq.
jiken (incident) 39
boukou (criminal assault) 32
josei (woman) 28
taiho (arrest) 23
hikoku (accused, defendant) 21
yougi (charge, suspicion) 20
kougeki (attack) 20
shounen (boy) 14
tero (terrorism) 14
shougai (injury) 13
higai (damage, harm) 12
kenkei (prefectural police department) 12
manshon (apartment) 12
butai (military unit) 10
fujo (girl and woman) 10
Using the bilingual corpus again, we gathered
alignments that had English texts containing the
English LUs specified in the way previously de-
scribed. We obtained 262 alignments. This proce-
dure defined a set of Japanese sentences contain-
ing Japanese words or phrases that were natural
translations of the LUs in the BFN.
ceals the fact that there can be, and actually are, many kinds
of frame-evoking effects. BFN has been concentrating on
identifying LUs for ?governors,? not LUs for the entire set of
FEs, for whatever reason. In this respect, it is crucial to note
that not all frame-evokers are frame-governors: hamburger.n
clearly evokes the ?Cooking creation? frame, but there the
noun does not govern the ?Cooking creation? frame. Ar-
guably, it is unreasonable and even gratuitous to posit the
?Hamburger? frame to make hamburger.n a governor.
13
Attack
Definition:
An Assailant physically attacks a Victim (which is usually but not always sentient), causing or intending to cause the Victim
physical injury. The Weapon used by the Assailant may also be mentioned, in addition to the usual Place, Time, Purpose, and
Reason. Sometimes a location is used metonymically to stand for the Assailant or the Victim, and in such cases the Place FE
will be annotated on a second FE layer.
As soon as he stepped out of the bar he was SET upon by four men in ski-masks.
Is he INVADING Iraq just to cover other shortcomings?
Then Jon-O?s forces AMBUSHED them on the left flank from a line of low hills.
FEs:
Core:
Assailant [Asl] The person (or other self-directed entity) that is attempting physical harm to the Victim.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Victim [Vic] This FE is the being or entity that is injured by the Assailant?s attack.
The mysterious fighter ATTACKED the guardsmen with a sabre.
Lexical Units
ambush.n, ambush.v, assail.v, assault.n, assault.v, attack.n, attack.v, charge.n, charge.v, fall.v, incursion.n, invade.v, inva-
sion.n, jump.v, lay ((into)).v, offensive.n, onset.n, onslaught.n, raid.v, set.v, storm. v, strike.n, strike.v
Created by infinity on Fri Nov 22 14:05:22 PST 2002
Figure 1: BFN definition of ?Attack? frame (partial)
It should be noted, however, that there is no es-
tablished method of recognizing these units au-
tomatically; they are part of a text without being
marked as such. To solve this problem, we hy-
pothesized that their statistical properties in the
texts could be used to pick them up; i.e., we as-
sumed that these LUs were relatively specific to
these types of texts and would appear at higher
frequencies than usual in the collected text.
We collected nouns with higher frequencies un-
der this assumption using a KH Coder 6.
The results were sorted according to the parts
of speech. The high-frequency nouns thus ob-
tained are listed in Table 1.
This provided little information about the se-
mantic classification of the nouns because there
was no indication of the LUs that they instan-
tiated. Semantic groupings are latent, how-
ever. This meant that we were able to ?clus-
ter? the nouns based on certain generic proper-
ties to obtain an initial approximation of these
groupings. We used a tool called msort (stand-
ing for ?meaning sort?) (Murata et al, 2001) to
establish generic, domain-independent semantic
6The KH Coder is a free analyzer that uses a combination
of ChaSen (Matsumoto et al, 1999) and MySQL. This is
freely available at http://khc.sourceforge.net/.
groupings.78
Nouns occurring more than three times were
obtained, as shown below:9
human dansei (man), danshi (boy), josei (woman), fujo
(woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), . . .
organization kokka (country), gaikoku (foreign country),
kokusai (international), sekai (world), . . .
product yakubutsu (drug), manshon (apartment), heya
(room), keesu (case), naifu (knife), shoujuu (rifle), . . .
7msort sorts a given set of nouns based on their encod-
ings in a Japanese thesaurus Bunrui Goi-hyou (National Lan-
guage Research Institute, 1964).
8One of the anonymous reviewers commented on this
?domain-independence? with a critical tone, questioning the
validity of the proposed method. This evaluation is clearly
based on a misunderstanding: the semantic association, or
conceptual dependence, between the ?Assailant? and the
?Victim? FEs is already encoded when we collected only
sentences whose main verbs are osou (in Japanese texts) or
its translations (in English texts). What we have done with
msort is to get subgroupings given a larger semantic group-
ing of ?harm-causing? at a more generic level. Based on
our coding experience, we are sure that subclassfication of a
given semantic class is based on ?semantic types? rather than
semantic roles. To give proper subgroupings of the events
that the ?Attack? frame is relevant, it is necessary to know
whether an ?Assailant? is a human ([+human, +animate,
. . . ]) or an animal ([?human, +animate, . . . ]), or whether
a ?Victim? is a human ([+human, +animate, . . . ]) or an
animal ([?human, +animate, . . . ]). If we insist that such
subclassifications in terms of semantic types into messy de-
tails are irrelevant, we are committing what we meant by
?mere generalizations for generalizations,? failing to recog-
nized what is really needed in NLP tasks.
9The listings ending with ?. . . ? are partial.
14
body part itai (body), soshiki (organization)
plant dansei (man), josei (woman), soshiki (tissue)
space genba (field), chiiki (region), mokuteki (purpose),
hokubu (northern area), shinai (city center)
amount gruupu (group)
relation jijou (circumstances), keesu (case), jitai (matter),
jiken (incident), ryakushiki (informality), kankei (rela-
tionship), mokuteki (purpose), genkou (current), . . .
activity jisatsu (suicide), satsugai (slaying), shougai (in-
jury), juushou (serious injuries), ishiki (conscious-
ness), utagai (doubt), yougi (suspicion), sousa (inves-
tigation), sousaku (search), shirabe (investigation), . . .
2.5 Identifying LUs for Japanese FEs
Based on the generic semantic groupings pro-
duced by msort, we classified nouns into sub-
classes by intution, so that they corresponded to
the FEs of the BFN frames in the following way:
Recall that a semantic frame is a collection of
semantic roles, or FEs. In the case of ?Attack?,
the frame has two ?core? FEs, i.e., ?Assailant?
and ?Victim?, and some other ?peripheral? or
?noncore? FEs such as ?Place?, ?Time?, and
?Weapon?. Thus, ?Attack? denotes a situation
in which an agent recognizable as an ?Assailant?
causes (or tries to cause) some ?Harm? or ?Injury?
to someone or a group of people recognizable as a
?Victim? at some ?Place? and ?Time?, sometimes
using an item recognizable as a ?Weapon?.
This means that all we need to do is to clas-
sify the nouns in Table 1 into semantic classes
such as ?Assailant?, ?Victim?, ?Place?, ?Time?, or
?Weapon?, with appropriate subclasses where hu-
man assailants are distinguished from nonhuman
assailants.10 The groupings provided by msort
turned out to be useful for this purpose.11
Using this procedure, the nouns obtained on a
frequency-basis for ?Attack? were classified into
the two core FEs, as follows:
10It is important to note that the target data selection pro-
cedure of BFN is biased. For example, they put aside a num-
ber of problematic cases like metaphorical expressions, and
this is clearly reflected in the current frame definitions. We
repeated noticed that metaphorically extended senses of a
word were systematically dropped in the current release of
BFN. For illustration, the sense of attack.n in heart attack
is not described in BFN. Descriptive ?gaps? of this sort are
clearly undesirable; some specific kinds of mapping prob-
lems between English LUs provided in BFN and Japanese
LUs arise from this.
11We were sometimes unable to identify an FE for a noun
class based solely on the output of msort. In these cases, we
looked at its usage in the corpus to determine its FE.
? ?Assailant?: dansei (man), goutou (burglary/burglar,
robbery/robber), heishi (soldier), hikoku (accused per-
son), butai (military unit), kyoudan (religious group)
? ?Victim?: danshi (boy), josei (woman), fujo (girl and
woman), joshi (girl), danji (young boy), joji (young
girl), youjo (infant girl), shounen (boy), shoujo (girl),
aite (opponent), nihonjin (Japanese), . . .
2.6 Advantages of proposed method
Using msort turned out to be more beneficial
than anticipated when it came to selecting non-
core FEs. msort helped to determine noncore
FEs correctly to a certain extent. The ?Attack?
frame, for example, includes noncore FEs such as
?Place?, ?Time?, ?Purpose?, and ?Reason? in ad-
dition to its core FEs, ?Assailant? and ?Victim?.
msort automatically groups naifu (knife), raifuru
(rifle), and pisutoru (pistol) into the ?product?
category, which corresponds to the ?Weapon? FE.
Similarly, it automatically groups chiiki (Regional
site), hokubu (northern area), and shinai (Inner
city) into the ?location? category, which corre-
sponds to ?Place?. Thus, part of the FE assign-
ment task can be done automatically using msort.
The procedure also produced some interesting
results. For example, the proposed method auto-
matically specifies a set of lexical items (or lex-
ical units) that clearly have the frame-evocation
effect but that are not properly identified as frame
elements of a semantic frame in BFN, either in
terms of core FEs or peripheral FEs (= noncore
FEs). The semantic groupings that were thus au-
tomatically identified are enumerated below:
1. Names denoting an act(ion) of N (N suru (or sareru))
(?(make) do N?): ranbou (violence), boukou (crimi-
nal assault), bouryoku (violence), jikkou (execution),
shuugeki (assault), kougeki (attack)
2. Names denoting a state of affairs N (V shita + N) (N
that S V ): satsugai (slaying), shougai (injury), goutou
(burglary/burglar, robbery/robber), satsujin (murder),
sasshou (killing and wounding)
3. Result ((Y ni) V shite, N wo owaseta) (?did V , and in-
flicted N to Y ): juushou (serious injuries)
4. Parts of the compound words: kyoushuu (assault
force) (a part of ?assault? force)
5. LUs of crime-related frames resulting from ?Attack?:
utagai (doubt), yougi (charge, suspicion), sousa (in-
vestigation), sousaku (search), shirabe (investigation),
kentou (investigation), hanketsu (judgement), . . .
A second look at the lexical items in 1 above
confirmed that most of these words or phrases can
15
be seen as LUs that realize, in Japanese, some of
the FEs of BFN?s ?Attack? frame.12 As sets of
lexical items were not classified automatically, we
had to determine all classifications manually.
2.7 Overall results
When the procedure was applied to ?Attack?,
?Cause harm? and ?Cause impact?, the following
Japanese LUs for their major FEs were specified:
1. Core FEs of ?Attack?:
?Assailant?: dansei (man), goutou (burglary/burglar, rob-
bery/robber), heishi (soldier), hikoku (accused
person), . . .
?Victim?: danshi (boy), josei (woman), fujo (girls and
women), joshi (girl), danji (young boy), . . .
2. Noncore FEs of ?Attack?:
?Place?: genba (field), chiiki (region), hokubu (northern
part), shinai (city center)
?Weapon?: naifu (knife), shoujuu (rifle), tanjuu (pistol)
3. Core FEs of ?Cause harm?:
?Body part? : senaka (back)
4. Core FEs of ?Cause impact?:
?Impactee?: doru (dollar), shijou (market), ginkou (bank),
shokoku (some countries)
?Impactor?: saigai (disaster), jishin (earthquake), fukyou
(depression), dageki (damage)
3 Comparison with other resources
To evaluate our results, we compared them with
other Japanese resources and methods for anal-
ysis, i.e., IPAL (IPA, 1987) and Nihongo Goi
Taikei (a Japanese lexicon) (hereafter called Goi
Taikei) (Ikehara et al, 1997), which are widely
used lexical resources, and semantic frame anal-
ysis by FOCAL (Nakamoto et al, to appear;
Kuroda et al, 2004), which is a recent frame-
work being developed with the aim of provid-
ing BFN-style semantic annotation and analy-
sis for Japanese independent of the Japanese
FrameNet (Ohara et al, 2003).
3.1 Comparison with Goi Taikei descriptions
Goi Taikei contains detailed information on the
predicate-argument structure classified according
to usage. Its semantic description of osou is given
below:
12For the reason of this argument, see note 5 above.
(1) 20 zokusei henka (property change) (motion)
N1 ga N2 wo osou
N1 strike N2
N1 (1270 shimpai (concern) 1262 kanashimi (sorrow)
2056 sainann (disaster) 2359 kishou (atmospheric
phenomena) 1000 tyuushou (abstract)) N2 (2 gutai
(object))
(2) 23 shintai dousa (physical motion) (motion)
N1 ga N2 wo osou
N1 attack N2
N1 (3 shutai (subject) 535 doubutsu (animal) 2416 by-
ouki (disease)) N2 (2 gutai (object))
(3) 23 shintai dousa (physical motion)
31 kanjou dousa (affective motion) (motion)
N1 ga N2 no fui wo osou
N1 surprise N2
N1 (4 hito (man) 1001 tyuushoubutsu (abstruc-
tion/abstraction?) 1235 koto (event)) N2 (4 hito
(man))
The word meanings were classified from the
properties of osou for nouns related to surface
cases of the verb. When we compared the frames
in BFN and the description provided by Goi
Taikei, and examined how the BFN frames corre-
sponded to the Goi Taikei definitions, we obtained
the following relationships:
Table 2: BFN/Goi-Taikei correspondences
Attack (2) 23 shintai dousa (physical motion)
Cause harm (1) 20 zokusei henka (property change)
Cause impact (1) 20 zokusei henka (property change)
First, we did not obtain the meaning ?An unex-
pected event occurred? like (3) in the Goi Taikei.
It was difficult to extract words whose meanings
described a manner of action, such as fui wo (by
surprise) using this method. It was also insuffi-
cient to extract only co-occurring nouns from sen-
tences related to verbs. As might be expected,
there was a close relationship between (2) and the
?Attack? frame. However, we were unable to find
?Assailant?s such as sickness in the BFN FEs. Fi-
nally, the ?Cause impact? frame and (1) were very
similar, except that assailant in (1) includes feel-
ings such as worry or sadness.
There was a good correlation between the se-
mantic frame constructed from BFN and the one
from Goi Taikei. With this method, however, we
met difficulties in extracting frames that did not
appear on the surface, such as ?manner of action?.
16
3.2 Comparison with IPAL descriptions
We compared the frames we obtained with the
definitions from the IPA Lexicon (IPA, 1987). Be-
low is an excerpt from the description of osou
from IPAL:
? Caption: osou001001 Semantic definition: An unde-
sirable thing unexpectedly occurs to someone.
Sentence valence pattern: N1 -ga N2 -wo
Noun phrase 1: bouto (rioter), goutou (burglary),
kuma (bear), sentouki (fighter plane), boufuu (wind
storm), jishinn (earthquake), ekibyou (plague), keizai
kiki (economic crisis)
Noun phrase 2: tabibito (traveler), fune (ship), nin-
gen (human)/kokudo (national land), kuni (country),
kouban (police box)
Example 1: Boufuu ga fune wo osotta. (A stormy wind
struck a ship.)
? Caption: osou001002
Semantic definition: Undesirable feelings and physio-
logical phenomena happening suddenly.
Sentence pattern: N1 -ga N2 -wo
Noun phrase 1: takamaru fuann (increased anxiety),
shi no kyoufu (fear of death), iyana kimochi (unpleas-
ant feelings)/ hageshii hiroukan (acute tiredness), ne-
muke (drowsiness)
Noun phrase 2: kare (he)
Example 1: Nemuke ga totsuzen kare wo osotta.
(Drowsiness fell upon him suddenly.)
Example 2: Kanojo ha fuann ni osowareta. (She be-
came uneasy suddenly.)
The IPAL description of osou identifies its two
senses13 We compared the BFN frames and the
IPAL descriptions (in terms of predicate frames)
and obtained the following correspondences:
Table 3: BFN/IPAL correspondences
Attack osou001001
Cause harm osou001001
Cause impact osou001001
All of the frames obtained from BFN seemed to
be classified into the first meaning in IPAL, e.g.,
there were no BFN frames in which ?Assailant?
recognized ?sickness.? With IPAL definitions,
it was difficult to distinguish the difference be-
tween The bear attacked the traveler and *An eco-
nomic crisis attacked the traveler, the latter of
which sounds unnatural and quite odd, whereas
we can do it with BFN definitions: the former
13A term, ?predicate frame,? is used in the IPAL to char-
acterize semantic properties of a predicate. While the idea
of predicate frames is somewhat related to semantic frames,
predicate frames are not defined as semantic frames in the
sense of Frame Semantics/BFN.
can be classified as an expression in the ?Attack?
frame, whereas the latter can not. The reason
for this is probably that BFN frames successfully
specify the semantic interdependence between the
?Assailant? and ?Victim? roles, whereas such in-
terdependece is not encoded in the IPAL descrip-
tions. We believe this is one of the strengths of
frame-based semantic description.
BFN definitions are not detailed enough, how-
ever. They face problems when we try to ac-
count for the constrast between The shark at-
tacked the swimmer and ?*The shark attacked the
bank, for example. The latter sentences doesn?t
makes sense unless it is reinterpreted some way,
while it is straightforward to interpret the first sen-
tence against a predatory situation.
In interpreting the second, there is a clear con-
flict or ?competition? between two strong read-
ings: one interpretation (reading 1) is against the
situation of ?Predation?, where the shark is inter-
preted as a ?Predator? and the bank as a ?Prey?.
Another (reading 2) is against the situation of
?Bank Robbery?, where the shark is interpreted
as a ?Bank Robber? and the bank as a ?Warehouse
of Valuables? (or simply as a ?Bank?). If reading
2 wins out, an implicit ?type coercion? (Puste-
jovsky, 1995) takes place to the shark so that the
referent of the shark is switched to a human who
acts as a ?Robber? with a nickname ?shark.? If
reading 1 wins out, by contrast, another kind of
implicit type coercion takes place to the bank so
that the referent of the bank is switched to an ani-
mal (an instance of fish, dolphin, or whale) which
acts as a ?Prey?, being called ?the bank? for some
unclear reasons. The preference of the reinter-
pretation for reading 2 over the other can be ac-
counted for if we are allowed to say that to find
someone being called ?shark? is more likely than
to find some animal being called ?bank.?
What this suggests is this: pieces of semantic
information that would account for ?selectional
restrictions? of this sort are not specified in the
BFN definitions (yet). Therefore, it can be said
that the frames constructed from BFN do not
classify all meanings of osou in the same way
IPAL does not, but these frames specify some
finer-grained, selectional aspects of osou?s lexical
meaning than the IPAL description. As we will
see in the next section, this is one of the strong
17
motivations that a framework called FOCAL has
tried to extend the BFN.
3.3 Comparison with FOCAL descriptions
FOCAL is a theoretical framework for semantic
analysis and annotation. Its development has been
strongly influenced by BFN, but it also tries to
extend BFN?s scope of semantic analysis to the
next stage.
In the case of X-ga Y-wo osou, FOCAL recog-
nizes 15 frames in total, listed in Table 4, specify-
ing their hierarchical organization.14
These frames are identified and classified based
on the semantic co-variations between ?Harm
Cause(r))? X , a special case of ?Cause(r)?,
and ?Harm Experiencer? Y , a special case of
?Experiencer?. This is important to note that FO-
CAL puts more emphasis on the specification of
the semantic co-variation between X and Y in
terms of semantic features because they are cru-
cial characteristics of a semantic frame, which are
not captured in the Goi Taikei and IPAL descrip-
tions, and are not clearly encoded even in the BFN
description.
In FOCAL, frames are defined as idealized
models of situations such as Robbery, Predation,
assuming that human understanding is situation-
based. The descriptive task of FOCAL, then, is
to recognize situations and give adequately de-
tailed descriptions to them. Given R is a set of
situation-specific roles {r1, . . . , rn}, which are
called semantic roles in BFN. Semantic frames
are useful only if they serves as specifications of
the co-variations among such Rs.
For example, F06, as a subclass of the ?Attack?
class event is defined as follows:
Definition of F06: Attack(R) = Attack(Predator(X),
Prey(Y ))
= Hunt(Hunter(X), Target(Y ), Purpose(Z))
where Z = Eat(Eater(X), Food(Y ), Purpose(Z?));
where Z? = Satisfy (r1(Z), Hunger)
There seems to be no English noun that names r1.
These are the frames that account for more or
less all possible readings of X-ga Y -wo osou. The
14 Space limitation disallowed us to show that the 15
frames thus recognized are nearly optimal to exhaustively
specify all the situations against which the senses of osou are
determined. This was confirmed by multivariate analyses on
psychological experiments (Nakamoto et al, to appear). We
regret this because the result would surely have answered the
question from one of the anonymous reviewers.
Table 4: 15 FOCAL frames with groups G1?G5
G1 F01 harm to Y caused by conflict between
groups X and Y
G1 F02 harm to Y caused by X?s invasion
G1 F03 harm to Y caused by X?s robbery
G1 F04 harm to Y caused by X?s violence
G1 F05 harm to Y caused by X?s raping
G2 F06 harm to Y caused by X?s preying attack
G2 F07 harm to Y caused by X?s nonpreying attack
(e.g., X?s defense)
G3 F08 harm to Y due to an unexpected accident X
G3 F09 harm to Y caused by a natural phenomenon
X (on a smaller scale, e.g., gust)
G3 F10 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., earthquake, flood)
G3 F11 harm to Y caused by a natural phenomenon
X (on a larger scale, e.g., spread of an
epidemic)
G4 F12 harm to Y caused by a social phenomenon X
G5 F13 harm to Y caused by a disease X
(nontemporary, e.g., cancer)
G5 F14 harm to Y caused by a disease symptom X
(temporary, e.g., heart attack)
G5 F15 harm to Y caused by a bad feeling X
(temporary, e.g., drowsiness)
validity of this claim was confirmed through psy-
chological experiments, and reported in (Kuroda
et al, 2004; Nakamoto et al, to appear). The
BFN identifies 3 frames relevant to the semantics
of osou, while FOCAL uses a total of 15 frames
to determine the range of situations against which
people understand the sentences whose main verb
is osou.
The 3 BFN frames have been compared with
the 15 frames below to assess how well they cor-
respond to one another:
Table 5: BFN/FOCAL correspondences
Attack Part of G1 F01?F05
Cause harm [UNCLEAR] [UNCLEAR]
Cause impact [UNCLEAR] [UNCLEAR]
[UNCLEAR] G5 F13?F15
This comparison revealed several differences.
First, FOCAL specifies situations that the
?Attack? frame applies to in much greater de-
tail, although its descriptions are based on se-
mantic frames like BFN?s descriptions are. This
is mainly because FOCAL identifies frames in
terms of conceivable differences in the ?pur-
poses,? or ?intended effects? of the ?Harm
18
Cause(r)?15, of which BFN?s ?Assailant? is a spe-
cial case. This suggests that BFN frames can be
further elaborated according to the subclassifica-
tion of ?Assailant? in terms of its purpose.16
The same is conversely true of ?Cause harm?
and ?Cause impact? frames. These BFN frames
need to be generalized so that they include nonhu-
man, nonintentional agents, which is not done in
the current BFN. Better matches would be found
if the ?Cause harm? and ?Cause impact? frames
were further classified according to the properties
of the ?Harm causer? and ?Impactor? just as in the
?Attack? frame.
While FOCAL explicitly groups the F01?F05
frames into G1 and combines it with another
group, G2, to yield a more general semantic class
{G1, G2}, it is not clear whether BFN captures
this hybrid class, since the hierarchical relation-
ships among frames are not sufficiently specified.
In fact, the comparison with FOCAL revealed
that BFN does not classify the ?Assailant? types in
as much detail as FOCAL does. According to FO-
CAL?s assumptions, it is ?Assailant??s ?Purpose?
(including the ?null? value) that defines the differ-
ences in otherwise similar situations. To identify
such subtle differences is exactly what humans
are very good at and computers are not. Speci-
fication of information of this kind is one of the
serious demands arising from many of the NLP
tasks.
To conclude, we noted that the granularity
of the semantic descriptions provided by BFN,
IPAL, Goi Taikei, and FOCAL had the following
hierarchy: FOCAL > BFN ? Goi Taikei > IPAL
This suggests that, while BFN is clearly useful
for a variety of purposes, its semantic descrip-
tions are not detailed enough, particularly when
dealing with the polysemy of relatively frequent
words like osou in Japanese or hit in English.
While our result is only suggestive at best, let
15This is not the same as BFN?s ?Harm causer? role,
which is much more specific than ?Harm Cause(r)? in FO-
CAL?s sense.
16The question of ?where to stop,? addressed by one of
the anonymous reviewers, would have been answered if we
had enough space to show that those 15 frames/situations are
nearly optimal to account for all the semantic classifications
reflected in selectional restrictions, as explained in note 14.
Clearly, we do not need to identify all semantically possible
subclassifications; we just need to identify psychologically
real subclassifications.
us make a brief comment on some methodologi-
cal aspects of the BFN framework.
Overall, BFN definitions for semantic frames
are much more oriented or even ?biased? for de-
scriptions of activities intended and caused by
human, volitional agents. In fact, BFN took a
methodological decision not to include metaphor-
ical uses and other ?problematic? uses of words
for ease of lexicon-building, thereby sacrificing
its descriptive range, causing a problem with bi-
ased data coverage, as far as we could see. In
the case of osou, for example, there were clearly
many examples in which harm is not caused by
a human, i.e., cases described by FOCAL frame
clusters G2: F06?F07, G3: F08?F11, G4: F12,
and G5: F13?F15. Therefore, as far as we are
concerned with the viability of the frame-based
description of situations that can be expressed us-
ing osou in Japanese, the current status of the
BFN database is only partially successful in that it
successfully captures the class of situations spec-
ified by G1.
4 Conclusion
We proposed a new translation-like method using
BFN to find Japanese LUs that corresponded to
English LUs in BFN semantic frames. We eval-
uated a technique of identifying Japanese LUs
based on English LUs using a bilingual corpus.
We evaluated the results by comparing them with
other Japanese language resources and analyses,
IPAL, Goi Taikei, and FOCAL. The comparison
revealed that FOCAL, BFN, Goi Taikei, and IPAL
provided finer-grained descriptions in this specific
order.
Our method allowed us to easily find Japanese
LUs that corresponded to LUs in BFN seman-
tics and at the same level of granularity as BFN.
Even if all the relevant sentenceswere not manu-
ally examined when the semantic frame was con-
structed, we were able to collect several members
of FEs. Our method also automatically specified
a set of lexical titems that clearly had the frame-
evocation effect but that were not properly iden-
tified as Frame Elements of a semantic frame in
BFN.
There are several problems still remaining that
need to be addressed. Because the bilingual cor-
pus used was a newspaper corpus, the target se-
19
mantic domains were limited. There is therefore
a possibility that we failed to identify certain se-
mantic frames. We plan to do further experiments
using a greater number of bilingual corpora with
a wider domain coverage.
In the comparison of the analyses by BFN and
by FOCAL, only one target verb osou is used in
this work. Clearly, this is insufficient and our re-
sult is only suggestive at best. To draw a realis-
tic conclusion, we will definitely need to examine
more target words and make the comparison more
reliable.
References
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project. In
Proceedings of the COLING-ACL ?98, Montreal;
Canada.
Benfung Chen and Pascale Fung. 2004. Biframenet:
Bilingual frame semantics resource construction by
cross-lingual induction. In Proceedings of the 20th
International Conference on Computational Lin-
guistics (COLING 2004).
Michael Ellsworth, Katrin Erk, Paul Kingsbury, and
Sebastian Pado?. 2004. PropBank, SALSA, and
FrameNet: How design determines product. In
Proceedings of the LREC 2004 Workshop on Build-
ing Lexical Resources from Semantically Annotated
Corpora, Lisbon.
Katrin Erk, Andrea Kowalski, Sebastian Pado?, and
Manfred Pinkal. 2003. Towards a resource for
lexical semantics: A large German corpus with ex-
tensive semantic annotation. In Proceedings of the
ACL-03.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Charles J. Fillmore. 1982. Frame semantics. In Lin-
guistic Society of Korea, editor, Linguistics in the
Morning Calm, pages 111?137, Seoul. Hanshin.
Satoru Ikehara, Mahahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,
Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997.
Goi-Taikei: A Japanese Lexicon. Iwanami Shoten,
Tokyo. (in Japanese, 5 volumes/CDROM).
IPA, 1987. IPA Lexicon of the Japanese Language for
Computers: Basic Verbs. Information-Technology
Promotion Agency. (in Japanese).
Paul Kingsbury and Martha Palmer. 2002. From Tree-
Bank to PropBank. In Proceedings of the 3rd In-
ternational Conference on Language Resources and
Evaluation (LREC-2002).
Kolodner, Janet. L. 2004. Case-Based Reasoning.
Morgan Kauffman.
Kow Kuroda, Keiko Nakamoto, Toshiyuki Kanamaru,
Masahiro Tatsuoka, and Hajime Nozawa. 2004.
A scope of concept analysis based on ?seman-
tic frames?: Berkeley FrameNet and Beyond. In
Conference Handbook of the 5th Meeting of The
Japanese Cognitive Linguistics Association, pages
133?153. (in Japanese).
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma
Takaoka, and Masayuki Asahara, 1999. Japanese
Morphological Analysis System ChaSen version
2.2.1. NAIST Technical Report NAIST-IS-TR. (in
Japanese).
Masaki Murata, Kyoko Kanzaki, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara. 2001. Meaning sort
? three examples: dictionary construction, tagged
corpus construction, and information presentation
system ?. In Alexander Gelbukh, editor, Compu-
tational Linguistics and Intelligent Text Processing,
Second International Conference, CICLing 2001,
Mexico City, February 2001 Proceedings, pages
305?318. Springer Publisher.
Keiko Nakamoto, Kow Kuroda, and Hajime Nozawa.
to appear. Defining the feature rating task as
a(nother) powerful method to explore sentence
meanings: With a special interest with how they are
mentally represented. In Japanese Journal of Cog-
nitive Psychology. (in Japanese).
National Language Research Institute. 1964. Bunrui
Goihyo (Word List by Semantic Principles). Syuei
Shuppan. (in Japanese).
Pustejovsky, James. 1995. The Generative Lexicon.
MIT Press.
Kyoko Hirose Ohara, Seiko Fujii, Hiroaki Saito, Shun
Ishizaki, Toshio Ohori, and Ryoko Suzuki. 2003.
The Japanese FrameNet project: A preliminary re-
port. In Proceedings of Pacific Association for
Computational Linguistics, pages 249?254.
Hiroaki Sato. 2003. FrameSQL: A software tool for
FrameNet. In ASIALEX ?03 Tokyo Proceedings,
pages 251?258. Asian Association of Lexicogra-
phy.
Carlos Subirats and Miriam R. L. Petruck. 2003. Sur-
prise: Spanish FrameNet. Presentation at Work-
shop on Frame Semantics, International Congress
of Linguists. July 29, 2003, Prague, Czech Repub-
lic.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Proceedings of the Annual
Meeting of the ACL-03, pages 72?79. ACL-2003.
20
Non-Factoid Japanese Question Answering through Passage Retrieval
that Is Weighted Based on Types of Answers
Masaki Murata and Sachiyo Tsukawaki
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
{murata,tsuka}@nict.go.jp
Qing Ma
Ryukoku University
Otsu, Shiga, 520-2194, Japan
qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru
Kyoto University
Yoshida-Nihonmatsu-Cho, Sakyo
Kyoto, 606-8501 Japan
kanamaru@hi.h.kyoto-u.ac.jp
Hitoshi Isahara
National Institute of Information and
Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We constructed a system for answering non-
factoid Japanese questions. We used var-
ious methods of passage retrieval for the
system. We extracted paragraphs based on
terms from an input question and output
them as the preferred answers. We classified
the non-factoid questions into six categories.
We used a particular method for each cate-
gory. For example, we increased the scores
of paragraphs including the word ?reason?
for questions including the word ?why.? We
participated at NTCIR-6 QAC-4, where our
system obtained the most correct answers
out of all the eight participating teams. The
rate of accuracy was 0.77, which indicates
that our methods were effective.
1 Introduction
A question-answering system is an application de-
signed to produce the correct answer to a question
given as input. For example, when ?What is the
capital of Japan?? is given as input, a question-
answering system may retrieve text containing sen-
tences like ?Tokyo is Japan?s capital and the coun-
try?s largest and most important city?, and ?Tokyo
is also one of Japan?s 47 prefectures?, from Web-
sites, newspaper articles, or encyclopedias. The sys-
tem then outputs ?Tokyo? as the correct answer.
We believe question-answering systems will become
a more convenient alternative to other systems de-
signed for information retrieval and a basic compo-
nent of future artificial intelligence systems. Numer-
ous researchers have recently been attracted to this
important topic. These researchers have produced
many interesting studies on question-answering sys-
tems (Kupiec, 1993; Ittycheriah et al, 2001; Clarke
et al, 2001; Dumis et al, 2002; Magnini et al, 2002;
Moldovan et al, 2003). Evaluation conferences and
contests on question-answering systems have also
been held. In particular, the U.S.A. has held the Text
REtrieval Conferences (TREC) (TREC-10 commit-
tee, 2001), and Japan has hosted the Question-
Answering Challenges (QAC) (National Institute of
Informatics, 2002) at NTCIR (NII Test Collection
for IR Systems ) 3. These conferences and contests
have aimed at improving question-answering sys-
tems. The researchers who participate in these create
question-answering systems that they then use to an-
swer the same questions, and each system?s perfor-
mance is then evaluated to yield possible improve-
ments.
We addressed non-factoid question answering in
NTCIR-6 QAC-4. For example, when the question
was ?Why are people opposed to the Private Infor-
mation Protection Law?? the system retrieved sen-
tences based on terms appearing in the question and
output an answer using the retrieved sentences. Nu-
merous studies have addressed issues that are in-
volved in the answering of non-factoid questions
(Berger et al, 2000; Blair-Goldensohn et al, 2003;
727
Xu et al, 2003; Soricut and Brill, 2004; Han et al,
2005; Morooka and Fukumoto, 2006; Maehara et
al., 2006; Asada, 2006).
We constructed a system for answering non-
factoid Japanese questions for QAC-4. We used
methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions
including the word ?why.? We performed exper-
iments using the NTCIR-6 QAC-4 data collection
and tested the effectiveness of our methods.
2 Categories of Non-Factoid Questions
We used six categories of non-factoid questions in
this study. We constructed the categories by con-
sulting the dry run data in QAC-4.
1. Definition-oriented questions (Questions that
require a definition to be given in response.)
e.g., K-1 to wa nandesuka? (What is K-1?)
2. Reason-oriented questions (Questions that re-
quire a reason to be given in response.)
e.g., kojin jouhou hokogou ni hantai shiteiru
hito wa doushite hantai shiteiru no desuka?
(Why are people opposed to the Private Infor-
mation Protection Law?)
3. Method-oriented questions (Questions that re-
quire an explanation of a method to be given in
response.)
e.g., sekai isan wa donoyouni shite kimeru no
desuka?? (How is a World Heritage Site deter-
mined?)
4. Degree-oriented questions (Questions that re-
quire an explanation of the degree of something
to be given in response.)
5. Change-oriented questions (Questions that re-
quire a description of things that change to be
given in response.)
e.g., shounen hou wa dou kawari mashitaka?
(How was the juvenile law changed?)
6. Detail-oriented questions (Questions that re-
quire a description of the particulars or details
surrounding a sequence of events to be given in
response.)
e.g., donoyouna keii de ryuukyuu oukoku wa ni-
hon no ichibu ni natta no desuka? (How did
Ryukyu come to belong to Japan?)
3 Question-answering Systems in this
Study
The system has three basic components:
1. Prediction of type of answer
The system predicts the answer to be a partic-
ular type of expression based on whether the
input question is indicated by an interrogative
pronoun, an adjective, or an adverb. For exam-
ple, if the input question is ?Why are people
opposed to the Private Information Protection
Law??, the word ?why? suggests that the an-
swer will be an expression that describes a rea-
son.
2. Document retrieval
The system extracts terms from the input ques-
tion and retrieves documents by using these
terms. Documents that are likely to contain
the correct answer are thus gathered during the
retrieval process. For example, for the input
question ?Why are people opposed to the Pri-
vate Information Protection Law??, the system
extracts ?people,? ?opposed,? ?Private,? ?Infor-
mation,? ?Protection,? and ?Law? as terms and
retrieves the appropriate documents based on
these.
3. Answer detection
The system separates the retrieved documents
into paragraphs and retrieves those that contain
terms from the input question and a clue ex-
pression (e.g., ?to wa? (copula sentence) for the
definition sentence). The system outputs the re-
trieved paragraphs as the preferred answer.
3.1 Prediction of type of answer
We used the following rules for predicting the type
of answer. We constructed the rules by consulting
the dry run data in QAC-4.
728
1. Definition-oriented questions Questions in-
cluding expressions such as ?to wa nani,?
?donna,? ?douiu,? ?douitta,? ?nanimono,?
?donoyouna mono,? ?donna mono,? and ?douiu
koto? (which all mean ?what is?) are rec-
ognized by the system as being definition-
oriented questions.
2. Reason-oriented questions Questions including
expressions such as ?naze? (why), ?naniyue?
(why), ?doushite? (why), ?nani ga riyuu de?
(what is the reason), and ?donna riyuu de?
(what reason), are recognized by the system as
being reason-oriented questions.
3. Method-oriented questions Questions includ-
ing expressions such as ?dou,? ?dousureba,?
?douyatte,? ?dono youni shite,? ?ikani shite,?
?ikani,? and ?donnna houhou de? (which all
mean ?how?) are recognized by the system as
being method-oriented questions.
4. Degree-oriented questions Questions including
expressions such as ?dorekurai? (how much),
?dorekurai no? (to what extent), and ?dono
teido? (to what extent), are recognized by the
system as being degree-oriented questions.
5. Change-oriented questions Questions includ-
ing expressions such as ?naniga chigau? (What
is different), ?donoyuni kawaru? (How is ...
changed), and ?dokoga kotonaru? (What is dif-
ferent), are recognized by the system as being
change-oriented questions.
6. Detail-oriented questions Questions including
expressions such as ?dono you na keii,? ?dono
you na ikisatsu,? and ?dono you na nariyuki?
(which all mean ?how was?) are recognized by
the system as being detail-oriented questions.
3.2 Document retrieval
Our system extracts terms from a question by using
the morphological analyzer, ChaSen (Matsumoto et
al., 1999). The analyzer first eliminates preposi-
tions, articles, and similar parts of speech. It then
retrieves documents by using the extracted terms.
The documents are retrieved as follows:
We first retrieve the top k
dr1
documents with the
highest scores calculated using the equation
Score(d)
=
?
term t
?
?
?
tf(d, t)
tf(d, t) + kt
length(d) + k
+
? + k
+
? log
N
df(t)
?
?
?
,
(1)
where d is a document, t is a term extracted from
a question, and tf(d, t) is the frequency of t oc-
curring in d. Here, df(t) is the number of docu-
ments in which t appears, N is the total number
of documents, length(d) is the length of d, and ?
is the average length of all documents. Constants
k
t
and k
+
are defined based on experimental re-
sults. We based this equation on Robertson?s equa-
tion (Robertson and Walker, 1994; Robertson et al,
1994). This approach is very effective, and we have
used it extensively for information retrieval (Murata
et al, 2000; Murata et al, 2001; Murata et al, 2002).
The question-answering system uses a large number
for k
t
.
We extracted the top 300 documents and used
them in the next procedure.
3.3 Answer detection
In detecting answers, our system first generates can-
didate expressions for them from the extracted docu-
ments. We use two methods for extracting candidate
expressions. Method 1 uses a paragraph as a candi-
date expression. Method 2 uses a paragraph, two
continuous paragraphs, or three continuous para-
graphs as candidate expressions.
We award each candidate expression the follow-
ing score.
Score(d)
= ?mint1?T log
?
t2?T3
(2dist(t1, t2)df(t2)
N
)
+ 0.00000001 ? length(d)
= maxt1?T
?
t2?T3
log
N
2dist(t1, t2) ? df(t2)
+ 0.00000001 ? length(d)
(2)
729
T3 = {t|t ? T, 2dist(t1, t)df(t)
N
? 1}, (3)
where d is a candidate expression, T is the set of
terms in the question, dist(t1, t2) is the distance
between t1 and t2 (defined as the number of char-
acters between them with dist(t1, t2) = 0.5 when
t1 = t2), and length(d) is the number of charac-
ters in a candidate expression. The numerical term,
0.00000001 ? length(d), is used for increasing the
scores of long paragraphs.
For reason-oriented questions, our system uses
some reason terms such as ?riyuu? (reason),
?gen?in? (cause), and ?nazenara? (because) as terms
for Eq. 2 in addition to terms from the input ques-
tion. This is because we would like to increase the
score of a document that includes reason terms for
reason-oriented questions.
For method-oriented questions, our system uses
some method terms such as ?houhou? (method),
?tejun? (procedure), and ?kotoniyori? (by doing) as
terms for second document retrieval (re-ranking) in
addition to terms from the input question.
For detail-oriented questions, our system uses
some method terms such as ?keii? (a detail, or a se-
quence of events), ?haikei? (background), and ?rek-
ishi? (history) as terms for second document re-
trieval (re-ranking) in addition to terms from the in-
put question.
For degree-oriented questions, when candidate
paragraphs include numerical expressions, the score
(Score(d)) is multiplied by 1.1.
For definition-oriented questions, the system first
extracts focus expressions. When the question in-
cludes expressions such as ?X-wa?, ?X-towa?, ?X-
toiunowa?, and ?X-tte?, X is extracted as a fo-
cus expression. The system multiplies the score,
(Score(d)), of the candidate paragraph having ?X-
wa?, ?X-towa or something by 1.1. When the can-
didate expression includes focus expressions having
modifiers (including modifier clauses and modifier
phrases), the modifiers are used as candidate expres-
sions, and the scores of the candidate expressions are
multiplied by 1.1.
Below is an example of a candidate expression
that is a modifier clause in a sentence.
Table 1: Results
Method Correct A B C D
Method 1 57 18 42 10 89
Method 2 77 5 67 19 90
(There were a total of 100 questions.)
Question sentence: sekai isan jouyaku to
wa dono youna jouyaku desu ka?
(What is the Convention concerning the
Protection of the World Cultural and Nat-
ural Heritage?)
Sentence including answers:
1972 nen no dai 17 kai yunesuko soukai de
saitaku sareta sekai isan jouyaku ....
(Convention concerning the Pro-
tection of the World Cultural
and Natural Heritage, which
was adopted in 1972 in the 17th gen-
eral assembly meeting of the UN Educational,
Scientific and Cultural Organization.)
Finally, our system extracts candidate expressions
having high scores, (Score(d)s), as the preferred
output. Our system extracts candidate expressions
having scores that are no less than the highest score
multiplied by 0.9 as the preferred output.
We constructed the methods for answer detection
by consulting the dry run data in QAC-4.
4 Experiments
The experimental results are listed in Table 1. One
hundred non-factoid questions were used in the ex-
periment. The questions, which were generated by
the QAC-4 organizers, were natural and not gener-
ated by using target documents. The QAC-4 orga-
nizers checked four or fewer outputs for each ques-
tion. Methods 1 and 2 were used to determine what
we used as answer candidate expressions (Method 1
uses one paragraph as a candidate answer. Method
2 uses one paragraph, two paragraphs, or three para-
graphs as candidate answers.).
?A,? ?B,? ?C,? and ?D? are the evaluation criteria.
?A? indicates output that describes the same content
as that in the answer. Even if there is a supplemen-
tary expression in the output, which does not change
730
the content, the output is judged to be ?A.? ?B? in-
dicates output that contains some content similar to
that in the answer but contains different overall con-
tent. ?C? indicates output that contains part of the
same content as that in the answer. ?D? indicates
output does not contain any of the same content as
that in the answer. The numbers for ?A,? ?B,? ?C,?
and ?D? in Table 1 indicate the number of questions
where an output belongs to ?A,? ?B,? ?C,? and ?D?.
?Correct? indicates the number of questions where
an output belongs to ?A,? ?B,? or ?C?. The evalu-
ation criteria ?Correct? was also used officially at
NTCIR-6 QAC-4.
We found the following.
? Method 1 obtained higher scores in evaluation
A than Method 2. This indicates that Method 1
can extract a completely relevant answer more
accurately than Method 2.
? Method 2 obtained higher scores in evaluation
?Correct? than Method 1. The rate of accuracy
for Method 2 was 0.77 according to evaluation
?Correct?. This indicates that Method 2 can ex-
tract more partly relevant answers than Method
1. When we want to extract completely relevant
answers, we should use Method 1. When we
want to extract more answers, including partly
relevant answers, we should use Method 2.
? Method 2 was the most accurate (0.77) of those
used by all eight participating teams. We could
detect paragraphs as answers including input
terms and the key terms related to answer types
based the methods discussed in Section 3.3.
Our system obtained the best results because
our method of detecting answers was the most
effective.
Below is an example of the output of Method 1,
which was judged to be ?A.?
Question sentence:
jusei ran shindan wa douiu baai ni okon-
awareru noka?
(When is amniocentesis performed on a
pregnant woman?)
System output:
omoi idenbyou no kodono ga umareru no
wo fusegu.
(To prevent the birth of children with seri-
ous genetic disorders )
Examples of answers given by organizers:
omoi idenbyou
(A serious genetic disorder)
omoi idenbyou no kodomo ga umareru
kanousei ga takai baai
(To prevent the birth of children with seri-
ous genetic disorders.)
5 Conclusion
We constructed a system for answering non-factoid
Japanese questions. An example of a non-factoid
question is ?Why are people opposed to the Pri-
vate Information Protection Law?? We used vari-
ous methods of passage retrieval for the system. We
extracted paragraphs based on terms from an input
question and output them as the preferred answers.
We classified the non-factoid questions into six cat-
egories. We used a particular method for each cate-
gory. For example, we increased the scores of para-
graphs including the word ?reason? for questions in-
cluding the word ?why.? We participated at NTCIR-
6 QAC-4, where our system obtained the most cor-
rect answers out of all the eight participating teams.
The rate of accuracy was 0.77, which indicates that
our methods were effective.
We would like to apply our method and system to
Web data in the future. We would like to construct a
sophisticated system that can answer many kinds of
complicated queries such as non-factoid questions
based on a large amount of Web data.
Acknowledgements
We are grateful to all the organizers of NTCIR-6
who gave us the chance to participate in their con-
test to evaluate and improve our question-answering
system. We greatly appreciate the kindness of all
those who helped us.
731
References
Yoshiaki Asada. 2006. Processing of definition type
questions in a question answering system. Master?s
thesis, Yokohama National University. (in Japanese).
AdamBerger, Rich Caruana, David Cohn, Dayne Freitag,
and Vibhu Mittal. 2000. Bridging the lexical chasm:
Statistical approaches to answer-finding. In Proceed-
ings of the 23rd annual international ACM SIGIR con-
ference on Research and development in information
retrieval (SIGIR-2000), pages 192?199.
Sasha Blair-Goldensohn, Kathleen R. McKeown, and
Andrew Hazen Schlaikjer. 2003. A hybrid approach
for qa track definitional questions. In Proceedings
of the 12th Text Retrieval Conference (TREC-2003),
pages 185?192.
Charles L. A. Clarke, Gordon V. Cormack, and
Thomas R. Lynam. 2001. Exploiting redundancy
in question answering. In Proceedings of the 24th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
Susan Dumis, Michele Banko, Eric Brill, Jimmy Lin, and
Andrew Ng. 2002. Web question answering: Is more
always better? In Proceedings of the 25th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Kyoung-Soo Han, Young-In Song, Sang-Bum Kim, and
Hae-Chang Rim. 2005. Phrase-based definitional
question answering using definition terminology. In
Lecture Notes in Computer Science 3689, pages 246?
259.
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and
Adwait Ratnaparkhi. 2001. IBM?s Statistical Ques-
tion Answering System. In TREC-9 Proceedings.
Julian Kupiec. 1993. MURAX: A robust linguistic ap-
proach for question answering using an on-line ency-
clopedia. In Proceedings of the Sixteenth Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Hideyuki Maehara, Jun?ichi Fukumoto, and Noriko
Kando. 2006. A BE-based automated evaluation
for question-answering system. IEICE-WGNLC2005-
109, pages 19?24. (in Japanese).
Bernardo Magnini, Matto Negri, Roberto Prevete, and
Hristo Tanev. 2002. Is it the right answer? Exploiting
web redundancy for answer validation. In Proceed-
ings of the 41st Annual Meeting of the Association for
Computational Linguistics.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis sys-
tem ChaSen version 2.0 manual 2nd edition.
Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mi-
hai Surdeanu. 2003. Performance issues and er-
ror analysis in an open-domain question answering
system. ACM Transactions on Information Systems,
21(2):133?154.
Kokoro Morooka and Jun?ichi Fukumoto. 2006. Answer
extraction method for why-type question answering
system. IEICE-WGNLC2005-107, pages 7?12. (in
Japanese).
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara. 2000.
Japanese probabilistic information retrieval using lo-
cation and category information. The Fifth Interna-
tional Workshop on Information Retrieval with Asian
Languages, pages 81?88.
Masaki Murata, Masao Utiyama, Qing Ma, Hiromi
Ozaku, and Hitoshi Isahara. 2001. CRL at NTCIR2.
Proceedings of the Second NTCIR Workshop Meeting
on Evaluation of Chinese & Japanese Text Retrieval
and Text Summarization, pages 5?21?5?31.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002.
High performance information retrieval using many
characteristics and many techniques. Proceedings of
the Third NTCIR Workshop (CLIR).
National Institute of Informatics. 2002. Proceedings of
the Third NTCIR Workshop (QAC).
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-Poisson model for
probabilistic weighted retrieval. In Proceedings of the
Seventeenth Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
Radu Soricut and Eric Brill. 2004. Automatic question
answering: Beyond the factoid. In In Proceedings
of the Human Language Technology and Conference
of the North American Chapter of the Association for
Computational Linguistics (HLT-NAACL-2004), pages
57?64.
TREC-10 committee. 2001. The tenth text retrieval con-
ference. http://trec.nist.gov/pubs/trec10/t10 proceed-
ings.html.
Jinxi Xu, Ana Licuanan, and Ralph Weischedel. 2003.
TREC 2003 QA at BBN: answering definitional ques-
tions. In Proceedings of the 12th Text Retrieval Con-
ference (TREC-2003), pages 98?106.
732
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 587?594,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Machine-Learning-Based Transformation of Passive Japanese Sentences
into Active by Separating Training Data into Each Input Particle
Masaki Murata
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
murata@nict.go.jp
Tamotsu Shirado
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
shirado@nict.go.jp
Toshiyuki Kanamaru
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
kanamaru@nict.go.jp
Hitoshi Isahara
National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
isahara@nict.go.jp
Abstract
We developed a new method of transform-
ing Japanese case particles when trans-
forming Japanese passive sentences into
active sentences. It separates training data
into each input particle and uses machine
learning for each particle. We also used
numerous rich features for learning. Our
method obtained a high rate of accuracy
(94.30%). In contrast, a method that did
not separate training data for any input
particles obtained a lower rate of accu-
racy (92.00%). In addition, a method
that did not have many rich features for
learning used in a previous study (Mu-
rata and Isahara, 2003) obtained a much
lower accuracy rate (89.77%). We con-
firmed that these improvements were sig-
nificant through a statistical test. We
also conducted experiments utilizing tra-
ditional methods using verb dictionar-
ies and manually prepared heuristic rules
and confirmed that our method obtained
much higher accuracy rates than tradi-
tional methods.
1 Introduction
This paper describes how passive Japanese sen-
tences can be automatically transformed into ac-
tive. There is an example of a passive Japanese
sentence in Figure 1. The Japanese suffix reta
functions as an auxiliary verb indicating the pas-
sive voice. There is a corresponding active-voice
sentence in Figure 2. When the sentence in Fig-
ure 1 is transformed into an active sentence, (i) ni
(by), which is a case postpositional particle with
the meaning of ?by?, is changed into ga, which is
a case postpositional particle indicating the sub-
jective case, and (ii) ga (subject), which is a
case postpositional particle indicating the subjec-
tive case, is changed into wo (object), which is
a case postpositional particle indicating the objec-
tive case. In this paper, we discuss the transfor-
mation of Japanese case particles (i.e., ni ? ga)
through machine learning.1
The transformation of passive sentences into ac-
tive is useful in many research areas including
generation, knowledge extraction from databases
written in natural languages, information extrac-
tion, and answering questions. For example, when
the answer is in the passive voice and the ques-
tion is in the active voice, a question-answering
system cannot match the answer with the question
because the sentence structures are different and
it is thus difficult to find the answer to the ques-
tion. Methods of transforming passive sentences
into active are important in natural language pro-
cessing.
The transformation of case particles in trans-
forming passive sentences into active is not easy
because particles depend on verbs and their use.
We developed a new method of transforming
Japanese case particles when transforming pas-
sive Japanese sentences into active in this study.
Our method separates training data into each in-
put particle and uses machine learning for each in-
put particle. We also used numerous rich features
for learning. Our experiments confirmed that our
method was effective.
1In this study, we did not handle the transformation of
auxiliary verbs and the inflection change of verbs because
these can be transformed based on Japanese grammar.
587
inu ni watashi ga kama- reta.
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 1: Passive sentence
inu ni watashi ga kama- reta.
ga wo
(dog) (by) (I) subjective-case postpositional particle (bite) passive voice
(I was bitten by a dog.)
Figure 3: Example in corpus
inu ga watashi wo kanda.
(dog) subject (I) object (bite)
(Dog bit me.)
Figure 2: Active sentence
2 Tagged corpus as supervised data
We used the Kyoto University corpus (Kurohashi
and Nagao, 1997) to construct a corpus tagged for
the transformation of case particles. It has ap-
proximately 20,000 sentences (16 editions of the
Mainichi Newspaper, from January 1st to 17th,
1995). We extracted case particles in passive-
voice sentences from the Kyoto University cor-
pus. There were 3,576 particles. We assigned a
corresponding case particle for the active voice to
each case particle. There is an example in Figure
3. The two underlined particles, ?ga? and ?wo?
that are given for ?ni? and ?ga? are tags for case
particles in the active voice. We called the given
case particles for the active voice target case par-
ticles, and the original case particles in passive-
voice sentences source case particles. We created
tags for target case particles in the corpus. If we
can determine the target case particles in a given
sentence, we can transform the case particles in
passive-voice sentences into case particles for the
active voice. Therefore, our goal was to determine
the target case particles.
3 Machine learning method (support
vector machine)
We used a support vector machine as the basis
of our machine-learning method. This is because
support vector machines are comparatively better
than other methods in many research areas (Kudoh
and Matsumoto, 2000; Taira and Haruno, 2001;
Small Margin Large Margin
Figure 4: Maximizing margin
Murata et al, 2002).
Data consisting of two categories were classi-
fied by using a hyperplane to divide a space with
the support vector machine. When these two cat-
egories were, positive and negative, for example,
enlarging the margin between them in the train-
ing data (see Figure 42), reduced the possibility of
incorrectly choosing categories in blind data (test
data). A hyperplane that maximized the margin
was thus determined, and classification was done
using that hyperplane. Although the basics of this
method are as described above, the region between
the margins through the training data can include
a small number of examples in extended versions,
and the linearity of the hyperplane can be changed
to non-linear by using kernel functions. Classi-
fication in these extended versions is equivalent
to classification using the following discernment
function, and the two categories can be classified
on the basis of whether the value output by the
function is positive or negative (Cristianini and
Shawe-Taylor, 2000; Kudoh, 2000):
2The open circles in the figure indicate positive examples
and the black circles indicate negative. The solid line indi-
cates the hyperplane dividing the space, and the broken lines
indicate the planes depicting margins.
588
f(x) = sgn
(
l
?
i=1
?iyiK(xi,x) + b
)
(1)
b =
maxi,y
i
=?1
bi + mini,y
i
=1
bi
2
bi = ?
l
?
j=1
?jyjK(xj,xi),
where x is the context (a set of features) of an in-
put example, xi indicates the context of a training
datum, and yi (i = 1, ..., l, yi ? {1,?1}) indicates
its category. Function sgn is:
sgn(x) = 1 (x ? 0), (2)
?1 (otherwise).
Each ?i (i = 1, 2...) is fixed as a value of ?i that
maximizes the value of L(?) in Eq. (3) under the
conditions set by Eqs. (4) and (5).
L(?) =
l
?
i=1
?
i
?
1
2
l
?
i,j=1
?
i
?
j
y
i
y
j
K(x
i
,x
j
) (3)
0 ? ?
i
? C (i = 1, ..., l) (4)
l
?
i=1
?
i
y
i
= 0 (5)
Although function K is called a kernel function
and various functions are used as kernel functions,
we have exclusively used the following polyno-
mial function:
K(x,y) = (x ? y + 1)d (6)
C and d are constants set by experimentation. For
all experiments reported in this paper, C was fixed
as 1 and d was fixed as 2.
A set of xi that satisfies ?i > 0 is called a sup-
port vector, (SVs)3, and the summation portion of
Eq. (1) is only calculated using examples that are
support vectors. Equation 1 is expressed as fol-
lows by using support vectors.
f(x) = sgn
?
?
?
i:x
i
?SV
s
?iyiK(xi,x) + b
?
?(7)
b =
bi:y
i
=?1,x
i
?SV
s
+ bi:y
i
=1,x
i
?SV
s
2
bi = ?
?
i:x
i
?SV
s
?jyjK(xj ,xi),
3The circles on the broken lines in Figure 4 indicate sup-
port vectors.
Table 1: Features
F1 part of speech (POS) of P
F2 main word of P
F3 word of P
F4 first 1, 2, 3, 4, 5, and 7 digits of category number
of P5
F5 auxiliary verb attached to P
F6 word of N
F7 first 1, 2, 3, 4, 5, and 7 digits of category number
of N
F8 case particles and words of nominals that have de-
pendency relationship with P and are other than
N
F9 first 1, 2, 3, 4, 5, and 7 digits of category num-
ber of nominals that have dependency relationship
with P and are other than N
F10 case particles of nominals that have dependency
relationship with P and are other than N
F11 the words appearing in the same sentence
F12 first 3 and 5 digits of category number of words
appearing in same sentence
F13 case particle taken by N (source case particle)
F14 target case particle output by KNP (Kurohashi,
1998)
F15 target case particle output with Kondo?s method
(Kondo et al, 2001)
F16 case patterns defined in IPAL dictionary (IPAL)
(IPA, 1987)
F17 combination of predicate semantic primitives de-
fined in IPAL
F18 predicate semantic primitives defined in IPAL
F19 combination of semantic primitives of N defined
in IPAL
F20 semantic primitives of N defined in IPAL
F21 whether P is defined in IPAL or not
F22 whether P can be in passive form defined in
VDIC6
F23 case particles of P defined in VDIC
F24 type of P defined in VDIC
F25 transformation rule used for P and N in Kondo?s
method
F26 whether P is defined in VDIC or not
F27 pattern of case particles of nominals that have de-
pendency relationship with P
F28 pair of case particles of nominals that have depen-
dency relationship with P
F29 case particles of nominals that have dependency
relationship with P and appear before N
F30 case particles of nominals that have dependency
relationship with P and appear after N
F31 case particles of nominals that have dependency
relationship with P and appear just before N
F32 case particles of nominals that have dependency
relationship with P and appear just after N
589
Table 2: Frequently occurring target case particles in source case particles
Source case particle Occurrence rate Frequent target case Occurrence rate
particles in in
source case particles source case particles
ni (indirect object) 27.57% (493/1788) ni (indirect object) 70.79% (349/493)
ga (subject) 27.38% (135/493)
ga (subject) 26.96% (482/1788) wo (direct object) 96.47% (465/482)
de (with) 17.17% (307/1788) ga (subject) 79.15% (243/307)
de (with) 13.36% (41/307)
to (with) 16.11% (288/1788) to (with) 99.31% (286/288)
wo (direct object) 6.77% (121/1788) wo (direct object) 99.17% (120/121)
kara (from) 4.53% ( 81/1788) ga (subject) 49.38% ( 40/ 81)
kara (from) 44.44% ( 36/ 81)
made (to) 0.78% ( 14/1788) made (to) 100.00% ( 14/ 14)
he (to) 0.06% ( 1/1788) ga (subject) 100.00% ( 1/ 1)
no (subject) 0.06% ( 1/1788) wo (direct object) 100.00% ( 1/ 1)
Support vector machines are capable of han-
dling data consisting of two categories. Data con-
sisting of more than two categories is generally
handled using the pair-wise method (Kudoh and
Matsumoto, 2000).
Pairs of two different categories (N(N-1)/2
pairs) are constructed for data consisting of N cat-
egories with this method. The best category is de-
termined by using a two-category classifier (in this
paper, a support vector machine4 is used as the
two-category classifier), and the correct category
is finally determined on the basis of ?voting? on
the N(N-1)/2 pairs that result from analysis with
the two-category classifier.
The method discussed in this paper is in fact a
combination of the support vector machine and the
pair-wise method described above.
4 Features (information used in
classification)
The features we used in our study are listed in Ta-
ble 1, where N is a noun phrase connected to the
4We used Kudoh?s TinySVM software (Kudoh, 2000) as
the support vector machine.
5The category number indicates a semantic class of
words. A Japanese thesaurus, the Bunrui Goi Hyou (NLRI,
1964), was used to determine the category number of each
word. This thesaurus is ?is-a? hierarchical, in which each
word has a category number. This is a 10-digit number that
indicates seven levels of ?is-a? hierarchy. The top five lev-
els are expressed by the first five digits, the sixth level is ex-
pressed by the next two digits, and the seventh level is ex-
pressed by the last three digits.
6Kondo et al constructed a rich dictionary for Japanese
verbs (Kondo et al, 2001). It defined types and characteris-
tics of verbs. We will refer to it as VDIC.
case particle being analyzed, and P is the phrase?s
predicate. We used the Japanese syntactic parser,
KNP (Kurohashi, 1998), for identifying N, P, parts
of speech and syntactic relations.
In the experiments conducted in this study, we
selected features. We used the following proce-
dure to select them.
? Feature selection
We first used all the features for learning. We
next deleted only one feature from all the fea-
tures for learning. We did this for every fea-
ture. We decided to delete features that would
make the most improvement. We repeated
this until we could not improve the rate of ac-
curacy.
5 Method of separating training data
into each input particle
We developed a new method of separating train-
ing data into each input (source) particle that uses
machine learning for each particle. For example,
when we identify a target particle where the source
particle is ni, we use only the training data where
the source particle is ni. When we identify a tar-
get particle where the source particle is ga, we use
only the training data where the source particle is
ga.
Frequently occurring target case particles are
very different in source case particles. Frequently
occurring target case particles in all source case
particles are listed in Table 2. For example, when
ni is a source case particle, frequently occurring
590
Table 3: Occurrence rates for target case particles
Target case Occurrence rate
particle Closed Open
wo (direct object) 33.05% 29.92%
ni (indirect object) 19.69% 17.79%
to (with) 16.00% 18.90%
de (with) 13.65% 15.27%
ga (subject) 11.07% 10.01%
ga or de 2.40% 2.46%
kara (from) 2.13% 3.47%
Other 2.01% 1.79%
target case particles are ni or ga. In contrast, when
ga is a source case particle, a frequently occurring
target case particle is wo.
In this case, it is better to separate training data
into each source particle and use machine learn-
ing for each particle. We therefore developed this
method and confirmed that it was effective through
experiments (Section 6).
6 Experiments
6.1 Basic experiments
We used the corpus we constructed described in
Section 2 as supervised data. We divided the su-
pervised data into closed and open data (Both the
closed data and open data had 1788 items each.).
The distribution of target case particles in the data
are listed in Table 3. We used the closed data to
determine features that were deleted in feature se-
lection and used the open data as test data (data
for evaluation). We used 10-fold cross validation
for the experiments on closed data and we used
closed data as the training data for the experiments
on open data. The target case particles were deter-
mined by using the machine-learning method ex-
plained in Section 3. When multiple target parti-
cles could have been answers in the training data,
we used pairs of them as answers for machine
learning.
The experimental results are listed in Tables 4
and 5. Baseline 1 outputs a source case particle
as the target case particle. Baseline 2 outputs the
most frequent target case particle (wo (direct ob-
ject)) in the closed data as the target case particle
in every case. Baseline 3 outputs the most fre-
quent target case particle for each source target
case particle in the closed data as the target case
particle. For example, ni (indirect object) is the
most frequent target case particle when the source
case particle is ni, as listed in Table 2. Baseline 3
outputs ni when the source case particle is ni. KNP
indicates the results that the Japanese syntactic
parser, KNP (Kurohashi, 1998), output. Kondo in-
dicates the results that Kondo?s method, (Kondo et
al., 2001), output. KNP and Kondo can only work
when a target predicate is defined in the IPAL dic-
tionary or the VDIC dictionary. Otherwise, KNP
and Kondo output nothing. ?KNP/Kondo + Base-
line X? indicates the use of outputs by Baseline
X when KNP/Kondo have output nothing. KNP
and Kondo are traditional methods using verb dic-
tionaries and manually prepared heuristic rules.
These traditional methods were used in this study
to compare them with ours. ?Murata 2003? indi-
cates results using a method they developed in a
previous study (Murata and Isahara, 2003). This
method uses F1, F2, F5, F6, F7, F10, and F13 as
features and does not have training data for any
source case particles. ?Division? indicates sepa-
rating training data into each source particle. ?No-
division? indicates not separating training data for
any source particles. ?All features? indicates the
use of all features with no features being selected.
?Feature selection? indicates features are selected.
We did two kinds of evaluations: ?Eval. A? and
?Eval. B?. There are some cases where multiple
target case particles can be answers. For example,
ga and de can be answers. We judged the result to
be correct in ?Eval. A? when ga and de could be
answers and the system output the pair of ga and
de as answers. We judged the result to be correct
in ?Eval. B? when ga and de could be answers and
the system output ga, de, or the pair of ga and de
as answers.
Table 4 lists the results using all data. Table 5
lists the results where a target predicate is defined
in the IPAL and VDIC dictionaries. There were
551 items in the closed data and 539 in the open.
We found the following from the results.
Although selection of features obtained higher
rates of accuracy than use of all features in the
closed data, it did not obtain higher rates of accu-
racy in the open data. This indicates that feature
selection was not effective and we should have
used all features in this study.
Our method using all features in the open data
and separating training data into each source parti-
cle obtained the highest rate of accuracy (94.30%
in Eval. B). This indicates that our method is ef-
591
Table 4: Experimental results
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 58.67% 61.41% 62.02% 64.60%
Baseline 2 33.05% 33.56% 29.92% 30.37%
Baseline 3 84.17% 88.20% 84.17% 88.20%
KNP 27.35% 28.69% 27.91% 29.14%
KNP + Baseline 1 64.32% 67.06% 67.79% 70.36%
KNP + Baseline 2 48.10% 48.99% 45.97% 46.48%
KNP + Baseline 3 81.21% 84.84% 80.82% 84.45%
Kondo 39.21% 40.88% 39.32% 41.00%
Kondo + Baseline 1 65.27% 68.57% 67.34% 70.41%
Kondo + Baseline 2 54.87% 56.54% 53.52% 55.26%
Kondo + Baseline 3 78.08% 81.71% 78.30% 81.88%
Murata 2003 86.86% 89.09% 87.86% 89.77%
Our method, no-division + all features 89.99% 92.39% 90.04% 92.00%
Our method, no-division + feature selection 91.28% 93.40% 90.10% 92.00%
Our method, division + all features 91.22% 93.79% 92.28% 94.30%
Our method, division + feature selection 92.06% 94.41% 91.89% 93.85%
Table 5: Experimental results on data that can use IPAL and VDIC dictionaries
Method Closed data Open data
Eval. A Eval. B Eval. A Eval. B
Baseline 1 57.71% 58.98% 58.63% 58.81%
Baseline 2 37.39% 37.39% 37.29% 37.29%
Baseline 3 84.03% 86.57% 86.83% 88.31%
KNP 74.59% 75.86% 75.88% 76.07%
Kondo 76.04% 77.50% 78.66% 78.85%
Our method, no-division + all features 94.19% 95.46% 94.81% 94.81%
Our method, division + all features 95.83% 96.91% 97.03% 97.03%
fective.
Our method that used all the features and did
not separate training data for any source particles
obtained an accuracy rate of 92.00% in Eval. B.
The technique of separating training data into each
source particles made an improvement of 2.30%.
We confirmed that this improvement has a signifi-
cance level of 0.01 by using a two-sided binomial
test (two-sided sign test). This indicates that the
technique of separating training data for all source
particles is effective.
Murata 2003 who used only seven features and
did not separate training data for any source par-
ticles obtained an accuracy rate of 89.77% with
Eval. B. The method (92.00%) of using all fea-
tures (32) made an improvement of 2.23% against
theirs. We confirmed that this improvement had
a significance level of 0.01 by using a two-sided
binomial test (two-sided sign test). This indicates
that our increased features are effective.
KNP and Kondo obtained low accuracy rates
(29.14% and 41.00% in Eval. B for the open data).
We did the evaluation using data and proved that
these methods could work well. A target predicate
in the data is defined in the IPAL and VDIC dictio-
naries. The results are listed in Table 5. KNP and
Kondo obtained relatively higher accuracy rates
(76.07% and 78.85% in Eval. B for the open data).
However, they were lower than that for Baseline 3.
Baseline 3 obtained a relatively high accuracy
rate (84.17% and 88.20% in Eval. B for the open
data). Baseline 3 is similar to our method in terms
of separating the training data into source parti-
cles. Baseline 3 separates the training data into
592
Table 6: Deletion of features
Deleted Closed data Open data
features Eval. A Eval. B Eval. A Eval. B
Acc. Diff. Acc. Diff. Acc. Diff. Acc. Diff.
Not deleted 91.22% ? 93.79% ? 92.28% ? 94.30% ?
F1 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F2 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F3 91.11% -0.11% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F4 91.50% 0.28% 94.13% 0.34% 91.72% -0.56% 93.68% -0.62%
F5 91.22% 0.00% 93.62% -0.17% 91.95% -0.33% 93.96% -0.34%
F6 91.00% -0.22% 93.51% -0.28% 92.23% -0.05% 94.24% -0.06%
F7 90.66% -0.56% 93.18% -0.61% 91.78% -0.50% 93.90% -0.40%
F8 91.22% 0.00% 93.79% 0.00% 92.39% 0.11% 94.24% -0.06%
F9 91.28% 0.06% 93.62% -0.17% 92.45% 0.17% 94.07% -0.23%
F10 91.33% 0.11% 93.85% 0.06% 92.00% -0.28% 94.07% -0.23%
F11 91.50% 0.28% 93.74% -0.05% 92.06% -0.22% 93.79% -0.51%
F12 91.28% 0.06% 93.62% -0.17% 92.56% 0.28% 94.35% 0.05%
F13 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F14 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.41% 0.11%
F15 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F16 91.39% 0.17% 93.90% 0.11% 92.34% 0.06% 94.30% 0.00%
F17 91.22% 0.00% 93.79% 0.00% 92.23% -0.05% 94.24% -0.06%
F18 91.16% -0.06% 93.74% -0.05% 92.39% 0.11% 94.46% 0.16%
F19 91.33% 0.11% 93.90% 0.11% 92.28% 0.00% 94.30% 0.00%
F20 91.11% -0.11% 93.68% -0.11% 92.34% 0.06% 94.35% 0.05%
F21 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
F22 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F23 91.28% 0.06% 93.79% 0.00% 92.28% 0.00% 94.24% -0.06%
F24 91.22% 0.00% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F25 89.54% -1.68% 92.11% -1.68% 90.04% -2.24% 92.39% -1.91%
F26 91.16% -0.06% 93.74% -0.05% 92.28% 0.00% 94.30% 0.00%
F27 91.22% 0.00% 93.68% -0.11% 92.23% -0.05% 94.18% -0.12%
F28 90.94% -0.28% 93.51% -0.28% 92.11% -0.17% 94.13% -0.17%
F29 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.30% 0.00%
F30 91.16% -0.06% 93.74% -0.05% 92.23% -0.05% 94.24% -0.06%
F31 91.28% 0.06% 93.85% 0.06% 92.28% 0.00% 94.24% -0.06%
F32 91.22% 0.00% 93.79% 0.00% 92.28% 0.00% 94.30% 0.00%
source particles and uses the most frequent tar-
get case particle. Our method involves separating
the training data into source particles and using
machine learning for each particle. The fact that
Baseline 3 obtained a relatively high accuracy rate
supports the effectiveness of our method separat-
ing the training data into source particles.
6.2 Experiments confirming importance of
features
We next conducted experiments where we con-
firmed which features were effective. The results
are listed in Table 6. We can see the accuracy rate
for deleting features and the accuracy rate for us-
ing all features. We can see that not using F25
greatly decreased the accuracy rate (about 2%).
This indicates that F25 is particularly effective.
F25 is the transformation rule Kondo used for P
and N in his method. The transformation rules in
Kondo?s method were made precisely for ni (indi-
rect object), which is particularly difficult to han-
dle. F25 is thus effective. We could also see not
using F7 decreased the accuracy rate (about 0.5%).
F7 has the semantic features for N. We found that
the semantic features for N were also effective.
6.3 Experiments changing number of
training data
We finally did experiments changing the number
of training data. The results are plotted in Figure
5. We used our two methods of all features ?Di-
vision? and ?Non-division?. We only plotted the
593
Figure 5: Changing number of training data
accuracy rates for Eval. B in the open data in the
figure. We plotted accuracy rates when 1, 1/2, 1/4,
1/8, and 1/16 of the training data were used. ?Divi-
sion?, which separates training data for all source
particles, obtained a high accuracy rate (88.36%)
even when the number of training data was small.
In contrast, ?Non-division?, which does not sepa-
rate training data for any source particles, obtained
a low accuracy rate (75.57%), when the number of
training data was small. This indicates that our
method of separating training data for all source
particles is effective.
7 Conclusion
We developed a new method of transform-
ing Japanese case particles when transforming
Japanese passive sentences into active sentences.
Our method separates training data for all input
(source) particles and uses machine learning for
each particle. We also used numerous rich features
for learning. Our method obtained a high rate of
accuracy (94.30%). In contrast, a method that did
not separate training data for all source particles
obtained a lower rate of accuracy (92.00%). In ad-
dition, a method that did not have many rich fea-
tures for learning used in a previous study obtained
a much lower accuracy rate (89.77%). We con-
firmed that these improvements were significant
through a statistical test. We also undertook ex-
periments utilizing traditional methods using verb
dictionaries and manually prepared heuristic rules
and confirmed that our method obtained much
higher accuracy rates than traditional methods.
We also conducted experiments on which fea-
tures were the most effective. We found that
Kondo?s transformation rule used as a feature in
our system was particularly effective. We also
found that semantic features for nominal targets
were effective.
We finally did experiments on changing the
number of training data. We found that our
method of separating training data for all source
particles could obtain high accuracy rates even
when there were few training data. This indicates
that our method of separating training data for all
source particles is effective.
The transformation of passive sentences into ac-
tive sentences is useful in many research areas
including generation, knowledge extraction from
databases written in natural languages, informa-
tion extraction, and answering questions. In the
future, we intend to use the results of our study for
these kinds of research projects.
References
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
IPA. 1987. (Information?Technology Promotion Agency,
Japan). IPA Lexicon of the Japanese Language for Com-
puters IPAL (Basic Verbs). (in Japanese).
Keiko Kondo, Satoshi Sato, and Manabu Okumura. 2001.
Paraphrasing by case alternation. Transactions of Infor-
mation Processing Society of Japan, 42(3):465?477. (in
Japanese).
Taku Kudoh and Yuji Matsumoto. 2000. Use of support vec-
tor learning for chunk identification. CoNLL-2000, pages
142?144.
Taku Kudoh. 2000. TinySVM: Support Vector Machines.
http://cl.aist-nara.ac.jp/?taku-ku//software/TinySVM/
index.html.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Univer-
sity text corpus project. 3rd Annual Meeting of the Asso-
ciation for Natural Language Processing, pages 115?118.
(in Japanese).
Sadao Kurohashi, 1998. Japanese Dependency/Case Struc-
ture Analyzer KNP version 2.0b6. Department of Infor-
matics, Kyoto University. (in Japanese).
Masaki Murata and Hitoshi Isahara, 2003. Conversion of
Japanese Passive/Causative Sentences into Active Sen-
tences Using Machine Learning, pages 115?125. Springer
Publisher.
Masaki Murata, Qing Ma, and Hitoshi Isahara. 2002. Com-
parison of three machine-learning methods for Thai part-
of-speech tagging. ACM Transactions on Asian Language
Information Processing, 1(2):145?158.
NLRI. 1964. Bunrui Goi Hyou. Shuuei Publishing.
Hirotoshi Taira and Masahiko Haruno. 2001. Feature se-
lection in svm text categorization. In Proceedings of
AAAI2001, pages 480?486.
594
Trend Survey on Japanese Natural Language Processing Studies
over the Last Decade
Masaki Murata?, Koji Ichii?, Qing Ma?,?, Tamotsu Shirado?,
Toshiyuki Kanamaru?,?, and Hitoshi Isahara?
?National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
{murata,qma,shirado,kanamaru,isahara}@nict.go.jp
?Port and Airport Research Institute
Nagase 3-1-1, Yokosuka, Kanagawa 239-0826, Japan, ichii@pari.go.jp
?Ryukoku University, Otsu 520-2194, Japan, qma@math.ryukoku.ac.jp
?Kyoto University, Yoshida-Nihonmatsu, Sakyo, Kyoto 606-8501, Japan
kanamaru@hi.h.kyoto-u.ac.jp
Abstract
Using natural language processing, we
carried out a trend survey on Japanese
natural language processing studies that
have been done over the last ten years.
We determined the changes in the num-
ber of papers published for each re-
search organization and on each re-
search area as well as the relationship
between research organizations and re-
search areas. This paper is useful
for both recognizing trends in Japanese
NLP and constructing a method of sup-
porting trend surveys using NLP.
1 Introduction
We conducted a trend survey on Japanese nat-
ural language processing studies that have been
done over the last ten years. We used biblio-
graphic information from journal papers and an-
nual conference papers of the Association for
Natural Language Processing, Japan (The Asso-
ciation for Natural Language Processing, 1995-
2004; The Association for Natural Language Pro-
cessing, 1994-2003). Just ten years have passed
since the association was established. Therefore,
we can use the bibliographic information from the
past ten years. In this study, we investigated what
kinds of studies have been presented in journal
papers and annual conference papers on the Asso-
ciation for Natural Language Processing, Japan.
We first digitized documents listed in the bibli-
ographic information and then extracted various
pieces of useful information for the trend survey.
Figure 1: Change in the number of papers
We also examined the changes in the number of
papers put up by each Japanese research orga-
nization and the changes in the number of pa-
pers written on specific research areas. More-
over, we examined the relationship between each
Japanese research organization and each research
area. This study is useful for trend surveys of
studies performed by members of in the Associa-
tion for Natural Language Processing, Japan.
2 Trend survey on NLP research studies
We show the changes in the number of journal
papers and conference papers in Figure 1. Jour-
nal papers are reviewed, but conference papers are
not reviewed in the association. In comparing the
journal papers and conference papers, we found
that the number of conference papers was much
larger than that of journal papers. We also found
that although both types of papers decreased in
number at some point, they both demonstrate an
upward trend.
Conference papers have a temporal peak in the
fourth year and a temporal drop in the sixth year,
250
Figure 2: Change in the number of journal papers
by each research organization (The two numbers in
the parentheses indicate the total number of papers
and the average value of published years.)
while journal papers have a peak in the sixth year
and a drop in the eighth year. The temporal peak
and drop of the journal papers occurred just two
years after the peak and drop of the conference
papers. We presume this is because journal papers
need more time for reviewing and publishing, and
because journal papers are presented later than
conference papers for studies performed at the
same time.
3 Trend survey on research
organizations
Next, we investigated the change in the number
of papers put out by each research organization.
The results are represented in contour in Figures
2 and 3. The height in contour (the depth of a
black color) indicates the number of papers. We
calculated the average (we call it average value)
of the average, the mode, and the median of the
published years by using the data of the number
of papers performed by each research organiza-
tion. In the figures, each research organization is
listed in ascending order of the average value. We
added the total number of papers and the average
value to each research organization in the figures.
Therefore, research organizations that had many
papers in the earlier years are displayed higher
on the list, while research organizations that had
Figure 3: Change in the number of conference pa-
pers by each research organization
many papers in the later years are displayed lower.
Here, we displayed only research organizations
that had many total papers. If a research orga-
nization?s name was changed during the ten-year
period, we used the name that had the most usage
on published papers for displaying it.1
From these figures, we can see that ATR and
CRL (NICT) put out many journal papers, and
NTT, ATR, Tokyo Institute of Technology, CRL,
and the University of Tokyo put out many confer-
ence papers. We also found that while NTT and
ATR had many papers in the earlier years, CRL
and the Univ. of Tokyo had many papers in the
later years. We can expect that because CRL and
the Univ. of Tokyo demonstrate an upward ten-
dency, their quantity of papers will continue to in-
crease in the future. Using these figures, we can
see very easily in which reference year each re-
search organization put out many papers.
4 Trend survey on research areas
Next, we investigated the change in the number
of papers in each research area. The results are in
Figures 4, 5, and 6. (Because the volume of data
for conference papers was large, it was divided
into two figures.). For journal papers, the height
1When we counted the frequency of a research organiza-
tion whose name was changed, we used all the names of it
including old and new names.
251
Figure 4: Change in the number of journal papers
in each research area
in contour indicates the number of papers. For
conference papers, the height in contour indicates
the base two logarithm of the number of papers
added by one. Using the same method as that de-
scribed above, we calculated the average of the
average, mode, and median of the years papers
were published using the data of the number of
papers in each research area. In the figures, each
research area is displayed in ascending order of
the average value. We added the total number of
papers and the average value to each research area
in the figures. Here, we divided the title of each
paper into words using ChaSen software (Mat-
sumoto et al, 1999), and we treaded each word as
a research area. A paper with a particular word in
Figure 5: Change in the number of conference pa-
pers in each research area (part I)
its title was categorized in the research area indi-
cated by the word. Wemanually eliminated words
that were not indicative of a research area, for ex-
ample, ?teki? (of) and ?kenkyu? (study).
From these figures, it is clear that the research
areas of ?Japanese? and ?analysis? were studied
in an especially large number of papers. We
also found that for journal papers, because the
research areas of ?verb?, ?noun?, ?disambigua-
tion?, ?probability?, ?corpus?, and ?polysemic?
were displayed higher on the list, these areas were
studied thoroughly in the earlier years. Likewise,
we found that the research areas of ?morphol-
ogy?, ?dependency?, ?dialogue?, and ?speech?
were studied thoroughly in the sixth year and the
252
Figure 6: Change in the number of conference pa-
pers in each research area (part II)
research areas of ?summarization?, ?retrieval?,
?translation? and so on were studied well in the
later years. Special journal issues on ?summariza-
tion? were published in the sixth and ninth years,
so the research area of ?summarization? was rep-
resented in many papers in those years. We can
expect that because the research area of ?transla-
tion? demonstrates an upward tendency, the num-
ber of papers on this topic will continue to in-
crease in the future.
In terms of conference papers, we found that
the research areas of ?bilingual?, ?morphology?,
?probability?, ?dictionary?, ?statistics?, and so on
were studied well in the earlier years. In the lower
part of the figures, such research areas as ?re-
Figure 7: Change in the number of conference pa-
pers at each research organization in the research
area of ?translation?
Figure 8: Change in the number of conference pa-
pers in each research area in the research area of
?translation?
trieval?, ?summarization?, ?question? and ?para-
phrase? are found. Thus, we can see that these
research areas were studied thoroughly in recent
years. We can see very easily in which reference
years each research area was studied using these
figures.
5 Trend survey using part of data
Although we have focused on using all the data
in the trend survey so far, we can narrow down
the survey by looking only at a certain part of
the data. For example, when we want to exam-
253
Figure 9: Relationship between research organizations and research areas in journal papers (The name
of each research organization is given a ??? symbol.)
ine a trend survey on translation in more detail,
all we have to do is to extract papers on transla-
tion and use them for a trend survey. We carried
out a trend survey on machine translation in this
manner. We first extracted papers whose titles in-
cluded the word ?translation? and then performed
the same investigations as in Sections 3 and 4.
The results are in Figures 7 and 8. The height in
contour (the depth of a color) indicates the num-
ber of papers. From Figure 7, we can see that
NTT had many papers in the earlier years, and
ATR had many papers in later years. From Figure
7, we can also see that studies on translation of-
ten dealt with specific topics such as ?semantics?,
?knowledge? and ?dictionary? in earlier years and
?support?, ?example?, and ?retrieval? in more re-
cent years.
6 Relationship between research
organizations and research topics
Finally, we investigated the various research ar-
eas that research organizations studied more fre-
quently during the ten-year period. Here, we
show only the results for journal papers. We used
the same method as in the previous sections for
extracting research organizations and research ar-
eas from the data. We counted the cooccurrent
frequency of each research organization and each
research area. We then constructed a cross table
in this manner and then performed the dual scal-
ing method (Weller and Romney, 1990; Ueda et
al., 2003). The result is depicted in Figure 9. The
dual scaling method displays the relationship be-
tween research organizations and research areas.
In Figure 9, ?translation? appears in the lower
left quadrant, ?learning? appears in the lower
right quadrant, ?statistics? and ?retrieval? appear
in the upper right quadrant, and ?noun? and ?sen-
tence? appear in the upper left quadrant. In the
vicinity around these words, the research areas
and organizations relating to them appear. For ex-
ample, in the upper right quadrant, Hitachi and
University of Tokushima appear near ?statistics?
and ?retrieval?, which were frequent study topics
for them. Similarly, ?summarization? appears in
the near upper right area of the source origin and
is surrounded by JAIST, Toyohashi University of
Technology, and Tokyo Institute of Technology.,
indicating it was a frequent topic of study at those
institutions. We can easily see which research
topics were primarily studied by each organiza-
tion using this figure.
Also in Figure 9, research areas on numeri-
cals such as ?probability? and ?learning? appear
254
on the right side. Therefore, we can interpret the
figure as depicting quantitative research topics on
the right side and qualitative research topics on
the left side. Research areas using complicated
processing such as ?learning? and ?translation?
appear in the lower area and research areas deal-
ing with theory such as ?probability?, ?grammar?,
?sentence?, and ?noun? appear in the upper area.
Therefore, we can interpret the figure as depict-
ing theoretical research topics in the upper area
and research topics using complicated processing
in the lower area.
7 Conclusion
In this paper, we described a trend survey carried
out on Japanese natural language processing stud-
ies done over the last ten years. We were able to
investigate trend surveys on research areas very
easily by treating divided words in titles by a mor-
phological analyzer as the indications of research
areas. We displayed the changes in the number of
papers put out by each research organization and
written on specific research topics. We also dis-
played the relationship between research organi-
zations and research areas using the dual scaling
method. The simple methods we used that are de-
scribed here made it possible to show many useful
results.
This paper has the following two significant ef-
fects:
 This paper explained a trend survey on
Japanese natural language processing. By
reading it, we can understand the trends in
research on Japanese natural language pro-
cessing. For example, we can find out
which research areas were studied more of-
ten and we can see which research organiza-
tions were involved in studying natural lan-
guage processing. We can also see which re-
search organization studied a particular re-
search area most often over the ten-year pe-
riod.
 We used natural language processing to
carry out the trend survey described here.
For example, we automatically detected the
indication of a research area from words
used in titles by using a morphological ana-
lyzer. In addition, we displayed words that
were extracted by the morphological ana-
lyzer in several ways to display the results
of the trend survey effectively. The methods
used in this paper would be useful in other
trend surveys.
In short, this paper is useful for recognizing trends
in Japanese NLP and for constructing methods of
supporting trend surveys using NLP.
In the future, we would like to perform an in-
ternational trend survey on natural language pro-
cessing using international conference and jour-
nal papers such as IJCNLP, ACL, and the Journal
of Computational Linguistics. We would also like
to do trend surveys on other topics such as AI, bi-
ology, politics, and sociology.
The kinds of investigations we did can easily be
altered to do many other kinds of investigations
as well. For example, we can use the dual scal-
ing method by investigating the relationship be-
tween the reference years and the research organi-
zations/areas. We can also use the representation
in contour for the relationship between research
organizations and research areas. Although we
showed the data in ascending order of the aver-
age value of the published years, we could show
the data in different order, for example, the or-
der of the total number of papers or the order of
the location, i.e., showing similar research orga-
nizations/areas that are located near each other by
clustering research organizations/areas using their
cooccurrent words. We would like to continue
to study these kinds of support methods for trend
surveys in the future.
References
The Association for Natural Language Processing. 1994-
2003. Journal of Natural Language Processing.
The Association for Natural Language Processing. 1995-
2004. Proceedings of the Annual Meeting of The Associ-
ation for Natural Language Processing.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshi-
taka Hirano, Hiroshi Matsuda, and Masayuki Asahara.
1999. Japanese morphological analysis system ChaSen
version 2.0 manual 2nd edition.
Taichiro Ueda, Masao Karita, and Kazue Honda. 2003. Jis-
sen Workshop Excel Tettei Katsuyou Tahenryou Kaiseki.
Shuuwa System. (in Japanese).
Susan C. Weller and A. Kimball Romney. 1990. Metric
Scaling : Correspondence Analysis (Quantitative Appli-
cations in the Social Sciences). SAGE Publications.
255
Proceedings of the Workshop on Information Extraction Beyond The Document, pages 1?11,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Development of an automatic trend exploration system
using the MuST data collection
Masaki Murata1
murata@nict.go.jp
Qing Ma3,1
3qma@math.ryukoku.ac.jp
Toshiyuki Kanamaru1,4
1kanamaru@nict.go.jp
Hitoshi Isahara1
isahara@nict.go.jp
1National Institute of Information
and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun,
Kyoto 619-0289, Japan
3Ryukoku University
Otsu, Shiga, 520-2194, Japan
Koji Ichii2
ichiikoji@hiroshima-u.ac.jp
Tamotsu Shirado1
shirado@nict.go.jp
Sachiyo Tsukawaki1
tsuka@nict.go.jp
2Hiroshima University
1-4-1 Kagamiyama, Higashi-hiroshima,
Hiroshima 739-8527, Japan
4Kyoto University
Yoshida-nihonmatsu-cho, Sakyo-ku,
Kyoto, 606-8501, Japan
Abstract
The automatic extraction of trend informa-
tion from text documents such as news-
paper articles would be useful for explor-
ing and examining trends. To enable this,
we used data sets provided by a workshop
on multimodal summarization for trend in-
formation (the MuST Workshop) to con-
struct an automatic trend exploration sys-
tem. This system first extracts units, tem-
porals, and item expressions from news-
paper articles, then it extracts sets of ex-
pressions as trend information, and finally
it arranges the sets and displays them in
graphs. For example, when documents
concerning the politics are given, the sys-
tem extracts ?%? and ?Cabinet approval
rating? as a unit and an item expression in-
cluding temporal expressions. It next ex-
tracts values related to ?%?. Finally, it
makes a graph where temporal expressions
are used for the horizontal axis and the
value of percentage is shown on the ver-
tical axis. This graph indicates the trend
of Cabinet approval rating and is useful
for investigating Cabinet approval rating.
Graphs are obviously easy to recognize
and useful for understanding information
described in documents. In experiments,
when we judged the extraction of a correct
graph as the top output to be correct, the
system accuracy was 0.2500 in evaluation
A and 0.3334 in evaluation B. (In evalua-
tion A, a graph where 75% or more of the
points were correct was judged to be cor-
rect; in evaluation B, a graph where 50%
or more of the points were correct was
judged to be correct.) When we judged
the extraction of a correct graph in the top
five outputs to be correct, accuracy rose to
0.4167 in evaluation A and 0.6250 in eval-
uation B. Our system is convenient and ef-
fective because it can output a graph that
includes trend information at these levels
of accuracy when given only a set of doc-
uments as input.
1 Introduction
We have studied ways to automatically extract
trend information from text documents, such as
newspaper articles, because such a capability will
be useful for exploring and examining trends. In
this work, we used data sets provided by a work-
shop on multimodal summarization for trend in-
formation (the MuST Workshop) to construct an
automatic trend exploration system. This system
firsts extract units, temporals, and item expres-
sions from newspaper articles, then it extract sets
of expressions as trend information, and finally it
arranges the sets and displays them in graphs. For
example, when documents concerning the politics
1
are given, the system extracts ?%? and ?Cabinet
approval rating? as a unit and an item expression
including temporal expressions. It next extracts
values related to ?%?. Finally, it makes a graph
where temporal expressions are used for the hor-
izontal axis and the value of percentage is shown
on the vertical axis. This graph indicates the trend
of Cabinet approval rating and is useful for inves-
tigating Cabinet approval rating. Graphs are obvi-
ously easy to recognize and useful for understand-
ing information described in documents.
2 The MuST Workshop
Kato et al organized the workshop on multimodal
summarization for trend information (the MuST
Workshop) (Kato et al, 2005). In this work-
shop, participants were given data sets consisting
of newspaper documents (editions of the Mainichi
newspaper from 1998 and 1999 (Japanese docu-
ments)) that included trend information for vari-
ous domains. In the data, tags for important ex-
pressions (e.g. temporals, numerical expressions,
and item expressions) were tagged manually.1 The
20 topics of the data sets (e.g., the 1998 home-run
race to break the all-time Major League record,
the approval rating for the Japanese Cabinet, and
news on typhoons) were provided. Trend infor-
mation was defined as information regarding the
change in a value for a certain item. A change in
the number of home runs hit by a certain player or
a change in the approval rating for the Cabinet are
examples of trend information. In the workshop,
participants could freely use the data sets for any
study they chose to do.
3 System
3.1 Structure of the system
Our automatic trend exploration system consists
of the following components.
1. Component to extract important expressions
First, documents related to a certain topic are
given to the system, which then extracts im-
portant expressions that will be used to ex-
tract and merge trend information. The sys-
tem extracts item units, temporal units, and
item expressions as important expressions.
1We do not use manually provided tags for important ex-
pressions because our system automatically extracts impor-
tant expressions.
Here, important expressions are defined as
expressions that play important roles in a
given document set. Item expressions are de-
fined as expressions that are strongly related
to the content of a given document set.
1a. Component to extract important item
units
The system extracts item units that will
be used to extract and merge trend infor-
mation.
For example, when documents concern-
ing the home-run race are given, ?hon?
or ?gou? (the Japanese item units for the
number of home runs) such as in ?54
hon? (54th home run) are extracted.
1b. Component to extract important tempo-
ral units
The system extracts temporal units that
will also be used to extract and merge
trend information.
For example, the system extracts tempo-
ral units such as ?nichi? (day), ?gatsu?
(month), and ?nen? (year). In Japanese,
temporal units are used to express dates,
such as in ?2006 nen, 3 gatsu, 27 nichi?
for March 27th, 2006.
1c. Component to extract important item
expressions
The system extracts item expressions
that will also be used to extract and
merge trend information.
For example, the system extracts expres-
sions that are objects for trend explo-
ration, such as ?McGwire? and ?Sosa?
as item expressions in the case of docu-
ments concerning the home-run race.
2. Component to extract trend information sets
The system identifies the locations in sen-
tences where a temporal unit, an item unit,
and an item expression that was extracted by
the component to extract important expres-
sions appear in similar sentences and extracts
sets of important expressions described by
the sentences as a trend information set. The
system also extracts numerical values appear-
ing with item units or temporal units, and
uses the connection of the numerical values
and the item units or temporal units as nu-
merical expressions or temporal expressions.
2
For example, in the case of documents con-
cerning the home-run race, the system ex-
tracts a set consisting of ?item expression:
McGwire?, ?temporal expression: 11 day?
(the 11th), and ?numerical expression: 47
gou? (47th home run) as a trend information
set.
3. Component to extract and display important
trend information sets
The system gathers the extracted trend infor-
mation sets and displays them as graphs or by
highlighting text displays.
For example, for documents concerning
the home-run race, the system displays as
graphs the extracted trend information sets
for ?McGwire? . In these graphs, temporal
expressions are used for the horizontal axis
and the number of home runs is shown on the
vertical axis.
3.2 Component to extract important
expressions
The system extracts important expressions that
will be used to extract trend information sets. Im-
portant expressions belong to one of the following
categories.
? item units
? temporal units
? item expressions
We use ChaSen (Matsumoto et al, 1999), a
Japanese morphological analyzer, to extract ex-
pressions. Specifically, we use the parts of
speeches in the ChaSen outputs to extract the ex-
pressions.
The system extracts item units, temporal units,
and item expressions by using manually con-
structed rules using the parts of speeches. The
system extracts a sequence of nouns adjacent to
numerical values as item units. It then extracts
expressions from among the item units which in-
clude an expression regarding time or date (e.g.,
?year?, ?month?, ?day?, ?hour?, or ?second?) as
temporal units. The system extracts a sequence of
nouns as item expressions.
The system next extracts important item units,
temporal units, and item expressions that play im-
portant roles in the target documents.
The following three methods can be used to ex-
tract important expressions. The system uses one
of them. The system judges that an expression
producing a high value from the following equa-
tions is an important expression.
? Equation for the TF numerical term in Okapi
(Robertson et al, 1994)
Score =
?
i?Docs
TF
i
TF
i
+
l
i
?
(1)
? Use of total word frequency
Score =
?
i?Docs
TF
i
(2)
? Use of total frequency of documents where a
word appears
Score =
?
i?Docs
1 (3)
In these equations, i is the ID (identification
number) of a document, Docs is a set of document
IDs, TF
i
is the occurrence number of an expres-
sion in document i, l is the length of document i,
and ? is the average length of documents inDocs.
To extract item expressions, we also applied a
method that uses the product of the occurrence
number of an expression in document i and the
length of the expression as TF
i
, so that we could
extract longer expressions.
3.3 Component to extract trend information
sets
The system identifies the locations in sentences
where a temporal unit, an item unit, and an item
expression extracted by the component to extract
important expressions appears in similar sentences
and extracts sets of important expressions de-
scribed by the sentences as a trend information
set. When more than one trend information set
appears in a document, the system extracts the one
that appears first. This is because important and
new things are often described in the beginning of
a document in the case of newspaper articles.
3.4 Component to extract and display
important trend information sets
The system gathers the extracted trend informa-
tion sets and displays them in graphs or as high-
lighted text. In the graphs, temporal expressions
3
are used for the horizontal axis and numerical ex-
pressions are used for the vertical axis. The system
also displays sentences used to extract trend infor-
mation sets and highlights important expressions
in the sentences.
The system extracts multiple item units, tempo-
ral units, and item expressions (through the com-
ponent to extract important expressions) and uses
these to make all possible combinations of the
three kinds of expression. The system extracts
trend information sets for each combination and
calculates the value of one of the following equa-
tions for each combination. The system judges
that the combination producing a higher value rep-
resents more useful trend information. The fol-
lowing four equations can be used for this purpose,
and the system uses one of them.
? Method 1 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? S
1
? S
2
? S
3
(4)
? Method 2 ? Use both the frequency of trend
information sets and the scores of important
expressions
M = Freq ? (S
1
? S
2
? S
3
)
1
3 (5)
? Method 3 ? Use the frequency of trend in-
formation sets
M = Freq (6)
? Method 4 ? Use the scores of important ex-
pressions
M = S
1
? S
2
? S
3
(7)
In these equations, Freq is the number of trend
information sets extracted as described in Section
3.3, and S1, S2, and S3 are the values of Score as
calculated by the corresponding equation in Sec-
tion 3.2.
The system extracts the top five item units, the
top five item expressions, and the top three tem-
poral units through the component to extract im-
portant expressions and forms all possible combi-
nations of these (75 combinations). The system
then calculates the value of the above equations for
these 75 combinations and judges that a combina-
tion having a larger value represents more useful
trend information.
4 Experiments and Discussion
We describe some examples of the output of our
system in Sections 4.1, 4.2, and 4.3, and the re-
sults from our system evaluation in Section 4.4.
We made experiments using Japanese newspaper
articles.
4.1 Extracting important expressions
To extract important expressions we applied the
equation for the TF numerical term in Okapi and
the method using the product of the occurrence
number for an expression and the length of the
expression as TF
i
for item expressions. We did
experiments using the three document sets for ty-
phoons, the Major Leagues, and political trends.
The results are shown in Table 1.
We found that appropriate important expres-
sions were extracted for each domain. For ex-
ample, in the data set for typhoons, ?typhoon?
was extracted as an important item expression and
an item unit ?gou? (No.), indicating the ID num-
ber of each typhoon, was extracted as an im-
portant item unit. In the data set for the Major
Leagues, the MuST data included documents de-
scribing the home-run race between Mark McG-
wire and Sammy Sosa in 1998. ?McGwire? and
?Sosa? were properly extracted among the higher
ranks. ?gou? (No.) and ?hon? (home run(s)), im-
portant item units for the home-run race, were
properly extracted. In the data set for political
trends, ?naikaku shiji ritsu? (cabinet approval rat-
ing) was properly extracted as an item expression
and ?%? was extracted as an item unit.
4.2 Graphs representing trend information
We next tested how well our system graphed the
trend information obtained from the MuST data
sets. We used the same three document sets as in
the previous section. As important expressions in
the experiments, we used the item unit, the tempo-
ral unit, and the item expression with the highest
scores (the top ranked ones) which were extracted
by the component to extract important expressions
using the method described in the previous sec-
tion. The system made the graphs using the com-
ponent to extract trend information sets and the
component to extract and display important trend
information sets. The graphs thus produced are
shown in Figs. 1, 2, and 3. (We used Excel to draw
these graphs.) Here, we made a temporal axis for
each temporal expression. However, we can also
4
Table 1: Examples of extracting important expressions
Typhoon
item units temporal units item expressions
gou nichi taihuu
(No.) (day) (typhoon)
me-toru ji gogo
(meter(s)) (o?clock) (afternoon)
nin jigoro higai
(people) (around x o?clock) (damage)
kiro fun shashin setsumei
(kilometer(s)) (minute(s)) (photo caption)
miri jisoku chuushin
(millimeter(s)) (per hour) (center)
Major League
item units temporal units item expressions
gou nichi Maguwaia
(No.) (day) (McGwire)
hon nen honruida
(home run(s)) (year) (home run)
kai gatsu Ka-jinarusu
(inning(s)) (month) (Cardinals)
honruida nen buri Ma-ku Maguwaia ichiruishu
(home run(s)) (after x year(s) interval) (Mark McGwire, the first baseman)
shiai fun So-sa
(game(s)) (minute(s)) (Sosa)
Political Trend
item units temporal units item expressions
% gatsu naikaku shiji ritsu
(%) (month) (cabinet approval rating)
pointo gen nichi Obuchi naikaku
(decrease of x point(s)) (day) (Obuchi Cabinet)
pointo zou nen Obuchi shushou
(increase of x point(s)) (year) (Prime Minister Obuchi)
dai kagetu shijiritsu
(generation) (month(s)) (approval rating)
pointo bun no kitai
(point(s)) (divided) (expectation)
5
Figure 1: Trend graph for the typhoon data set
Figure 2: Trend graph for the Major Leagues data
set
display a graph where regular temporal intervals
are used in the temporal axis.
For the typhoon data set, gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as the top ranked item unit, temporal unit, and
item expression. The system extracted trend in-
formation sets using these, and then made a graph
where the temporal expression (day) was used for
the horizontal axis and the ID numbers of the ty-
phoons were shown on the vertical axis. The
MuST data included data for September and Octo-
ber of 1998 and 1999. Figure 1 is useful for seeing
when each typhoon hit Japan during the typhoon
season each year. Comparing the 1998 data with
that of 1999 reveals that the number of typhoons
increased in 1999.
For the Major Leagues data set, gou (No.), nichi
(day), and Maguwaia (McGwire) were extracted
with the top rank. The system used these to make
a graph where the temporal expression (day) was
used for the horizontal axis and the cumulative
number of home runs hit by McGwire was shown
on the vertical axis (Fig. 2). The MuST data
included data beginning in August, 1998. The
graph shows some points where the cumulative
number of home runs decreased (e.g., September
Figure 3: Trend graph for the political trends data
set
4th), which was obviously incorrect. This was be-
cause our system wrongly extracted the number of
home runs hit by Sosa when this was given close
to McGwire?s total.
In the political trends data set, %, gatsu
(month), and naikaku shiji ritsu (cabinet approval
rating) were extracted with the top rankings. The
system used these to make a graph where the
temporal expression (month) was used for the
horizontal axis and the Cabinet approval rating
(Japanese Cabinet) was shown as a percentage on
the vertical axis. The MuST data covered 1998
and 1999. Figure 2 shows the cabinet approval
rating of the Obuchi Cabinet. We found that the
overall approval rating trend was upwards. Again,
there were some errors in the extracted trend infor-
mation sets. For example, although June was han-
dled correctly, the system wrongly extracted May
as a temporal expression from the sentence ?in
comparison to the previous investigation in May?.
4.3 Sentence extraction and highlighting
display
We then tested the sentence extraction and high-
lighting display with respect to trend information
using the MuST data set; in this case, we used
the typhoon data set. As important expressions,
we used the item unit, the temporal unit, and the
item expression extracted with the highest scores
(the top ranked ones) by the component to extract
important expressions using the method described
in the previous section. Gou (No.), nichi (day),
and taihuu (typhoon) were respectively extracted
as an item unit, a temporal unit, and an item ex-
pression. The system extracted sentences includ-
ing the three expressions and highlighted these ex-
pressions in the sentences. The results are shown
in Figure 4. The first trend information sets to ap-
6
Sept. 16, 1998 No. 5
Large-scale and medium-strength Typhoon No. 5 made landfall near Omaezaki in Shizuoka Pre-
fecture before dawn on the 16th, and then moved to the northeast involving the Koshin, Kantou,
and Touhoku areas in the storm.
Sept. 21, 1998 No. 8
Small-scale Typhoon No. 8 made landfall near Tanabe City in Wakayama Prefecture around 4:00
p.m. on the 21st, and weakened while tracking to the northward across Kinki district.
Sept. 22, 1998 No. 7
Typhoon No. 7 made landfall near Wakayama City in the afternoon on the 22nd, and will hit the
Kinki district.
Sept. 21, 1998 No. 8
The two-day consecutive landfall of Typhoon No. 8 on the 21st and Typhoon No. 7 on the 22nd
caused nine deaths and many injuries in a total of six prefectures including Nara, Fukui, Shiga,
and so on.
Oct. 17, 1998 No. 10
Medium-scale and medium-strength Typhoon No. 10 made landfall on Makurazaki City in
Kagoshima Prefecture around 4:30 p.m. on the 17th, and then moved across the West Japan area
after making another landfall near Sukumo City in Kochi Prefecture in the evening.
Aug. 20, 1999 No. 11
The Meteorological Office announced on the 20th that Typhoon No. 11 developed 120 kilometers
off the south-southwest coast of Midway.
Sept. 14, 1999 No. 16
Typhoon No. 16, which developed off the south coast in Miyazaki Prefecture, made landfall near
Kushima City in the prefecture around 5:00 p.m. on the 14th.
Sept. 15, 1999 No. 16
Small-scale and weak Typhoon No. 16 became extratropical in Nagano Prefecture and moved out
to sea off Ibaraki Prefecture on the 15th.
Sept. 24, 1999 No. 18
Medium-scale and strong Typhoon No. 18 made landfall in the north of Kumamoto Prefecture
around 6:00 a.m. on the 24th, and after moving to Suo-Nada made another landfall at Ube City
in Yamaguchi Prefecture before 9:00 p.m., tracked through the Chugoku district, and then moved
into the Japan Sea after 10:00 p.m.
Sept. 25, 1999 No. 18
Typhoon No. 18, which caused significant damage in the Kyushu and Chugoku districts, weakened
and made another landfall before moving into the Sea of Okhotsk around 10:00 a.m. on the 25th.
Figure 4: Sentence extraction and highlighting display for the typhoon data set
7
pear are underlined twice and the other sets are
underlined once. (In the actual system, color is
used to make this distinction.) The extracted tem-
poral expressions and numerical expressions are
presented in the upper part of the extracted sen-
tence. The graphs shown in the previous section
were made by using these temporal expressions
and numerical expressions.
The extracted sentences plainly described the
state of affairs regarding the typhoons and were
important sentences. For the research being done
on summarization techniques, this can be consid-
ered a useful means of extracting important sen-
tences. The extracted sentences typically describe
the places affected by each typhoon and whether
there was any damage. They contain important
descriptions about each typhoon. This confirmed
that a simple method of extracting sentences con-
taining an item unit, a temporal unit, and an item
expression can be used to extract important sen-
tences.
The fourth sentence in the figure includes infor-
mation on both typhoon no.7 and typhoon no.8.
We can see that there is a trend information set
other than the extracted trend information set (un-
derlined twice) from the expressions that are un-
derlined once. Since the system sometimes ex-
tracts incorrect trend information sets, the high-
lighting is useful for identifying such sets.
4.4 Evaluation
We used a closed data set and an open data set
to evaluate our system. The closed data set was
the data set provided by the MuST workshop or-
ganizer and contained 20 domain document sets.
The data sets were separated for each domain.
We made the open data set based on the MuST
data set using newspaper articles (editions of the
Mainichi newspaper from 2000 and 2001). We
made 24 document sets using information retrieval
by term query. We used documents retrieved by
term query as the document set of the domain for
each query term.
We used the closed data set to adjust our system
and used the open data set to calculate the evalua-
tion scores of our system for evaluation.
We judged whether a document set included the
information needed to make trend graphs by con-
sulting the top 30 combinations of three kinds of
important expression having the 30 highest values
as in the method of Section 3.4. There were 19
documents including such information in the open
data. We used these 19 documents for the follow-
ing evaluation.
In the evaluation, we examined how accurately
trend graphs could be output when using the top
ranked expressions. The results are shown in Table
2. The best scores are described using bold fonts
for each evaluation score.
We used five evaluation scores. MRR is the av-
erage of the score where 1/r is given as the score
when the rank of the first correct output is r (Mu-
rata et al, 2005b). TP1 is the average of the pre-
cision in the first output. TP5 is the average of
the precision where the system includes a correct
output in the first five outputs. RP is the average
of the r-precision and AP is the average of the av-
erage precision. (Here, the average means that the
evaluation score is calculated for each domain data
set and the summation of these scores divided by
the number of the domain data sets is the average.)
R-precision is the precision of the r outputs where
r is the number of correct answers. Average pre-
cision is the average of the precision when each
correct answer is output (Murata et al, 2000). The
r-precision indicates the precision where the recall
and the precision have the same value. The preci-
sion is the ratio of correct answers in the system
output. The recall is the ratio of correct answers
in the system output to the total number of correct
answers.
Methods 1 to 4 in Table 2 are the methods used
to extract useful trend information described in
Section 3.4. Use of the expression length means
the product of the occurrence number for an ex-
pression and the length of the expression was used
to calculate the score for an important item ex-
pression. No use of the expression length means
this product was not used and only the occurrence
number was used.
To calculate the r-precision and average preci-
sion, we needed correct answer sets. We made the
correct answer sets by manually examining the top
30 outputs for the 24 (= 4? 6) methods (the com-
binations of methods 1 to 4 and the use of Equa-
tions 1 to 3 with or without the expression length)
and defining the useful trend information among
them as the correct answer sets.
In evaluation A, a graph where 75% or more of
the points were correct was judged to be correct.
In evaluation B, a graph where 50% or more of the
points were correct was judged to be correct.
8
Table 2: Experimental results for the open data
Evaluation A Evaluation B
MRR TP1 TP5 RP AP MRR TP1 TP5 RP AP
Use of Equation 1 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1360 0.1162 0.5522 0.4211 0.7368 0.1968 0.1565
Method 2 0.3847 0.3158 0.4211 0.1360 0.1150 0.5343 0.4211 0.6316 0.1880 0.1559
Method 3 0.3557 0.2632 0.4211 0.1360 0.1131 0.5053 0.3684 0.6316 0.1805 0.1541
Method 4 0.3189 0.2632 0.4211 0.1125 0.0973 0.4492 0.3158 0.6316 0.1645 0.1247
Use of Equation 2 and the expression length
Method 1 0.3904 0.3158 0.4737 0.1422 0.1154 0.5746 0.4211 0.7368 0.2127 0.1674
Method 2 0.3877 0.3158 0.4737 0.1422 0.1196 0.5544 0.4211 0.7368 0.2127 0.1723
Method 3 0.3895 0.3158 0.5263 0.1422 0.1202 0.5491 0.4211 0.7895 0.2127 0.1705
Method 4 0.2216 0.1053 0.3684 0.0846 0.0738 0.3765 0.2105 0.5789 0.1328 0.1043
Use of Equation 3 and the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
Use of Equation 1 and no use of the expression length
Method 1 0.3789 0.3158 0.4737 0.1294 0.1152 0.5456 0.4211 0.7368 0.2002 0.1627
Method 2 0.3750 0.3158 0.4211 0.1294 0.1137 0.5215 0.4211 0.6842 0.2002 0.1621
Method 3 0.3333 0.2632 0.4211 0.1119 0.1072 0.4798 0.3684 0.6842 0.1763 0.1552
Method 4 0.2588 0.1053 0.4737 0.1269 0.0872 0.3882 0.1579 0.6842 0.1833 0.1189
Use of Equation 2 and no use of the expression length
Method 1 0.3277 0.2105 0.4737 0.1134 0.0952 0.4900 0.2632 0.7895 0.1779 0.1410
Method 2 0.3662 0.2632 0.4737 0.1187 0.1104 0.5417 0.3684 0.7368 0.1831 0.1594
Method 3 0.3504 0.2632 0.4737 0.1187 0.1116 0.5167 0.3684 0.7368 0.1884 0.1647
Method 4 0.1877 0.0526 0.3684 0.0775 0.0510 0.3131 0.1053 0.5263 0.1300 0.0879
Use of Equation 3 and no use of the expression length
Method 1 0.3855 0.3158 0.4737 0.1335 0.1155 0.5452 0.4211 0.7368 0.1943 0.1577
Method 2 0.3847 0.3158 0.4211 0.1335 0.1141 0.5256 0.4211 0.6316 0.1855 0.1555
Method 3 0.3570 0.2632 0.4737 0.1335 0.1124 0.4979 0.3684 0.6842 0.1780 0.1524
Method 4 0.3173 0.2632 0.4737 0.1256 0.0962 0.4652 0.3684 0.6316 0.1777 0.1293
9
From the experimental results, we found that
the method using the total frequency for a word
(Equation 2) and the length of an expression was
best for calculating the scores of important expres-
sions.
Using the length of an expression was impor-
tant. (The way of using the length of an expres-
sion was described in the last part of Section 3.2.)
For example, when ?Cabinet approval rating? ap-
pears in documents, a method without expression
lengths extracts ?rating?. When the system ex-
tracts trend information sets using ?rating?, it ex-
tracts wrong information related to types of ?rat-
ing? other than ?Cabinet approval rating?. This
hinders the extraction of coherent trend informa-
tion. Thus, it is beneficial to use the length of an
expression when extracting important item expres-
sions.
We also found that method 1 (using both the fre-
quency of the trend information sets and the scores
of important expressions) was generally the best.
When we judged the extraction of a correct
graph as the top output in the experiments to be
correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
In terms of the evaluation scores for the 24 original
data sets (these evaluation scores were multiplied
by 19/24), when we judged the extraction of a cor-
rect graph as the top output in the experiments to
be correct, our best system accuracy was 0.3158 in
evaluation A and 0.4211 in evaluation B.When we
judged the extraction of a correct graph in the top
five outputs to be correct, the best accuracy rose to
0.5263 in evaluation A and 0.7895 in evaluation B.
Our system is convenient and effective because it
can output a graph that includes trend information
at these levels of accuracy when given only a set
of documents as input.
As shown in Table 2, the best values for RP
(which indicates the precision where the recall and
the precision have the same value) and AP were
0.2127 and 0.1705, respectively, in evaluation B.
This RP value indicates that our system could
extract about one out of five graphs among the cor-
rect answers when the recall and the precision had
the same value.
5 Related studies
Fujihata et al (Fujihata et al, 2001) developed a
system to extract numerical expressions and their
related item expressions by using syntactic infor-
mation and patterns. However, they did not deal
with the extraction of important expressions or
gather trend information sets. In addition, they did
not make a graph from the extracted expressions.
Nanba et al (Nanba et al, 2005) took an
approach of judging whether the sentence rela-
tionship indicates transition (trend information)
or renovation (revision of information) and used
the judgment results to extract trend information.
They also constructed a system to extract nu-
merical information from input numerical units
and make a graph that includes trend information.
However, they did not consider ways to extract
item numerical units and item expressions auto-
matically.
In contrast to these systems, our system auto-
matically extracts item numerical units and item
expressions that each play an important role in a
given document set. When a document set for
a certain domain is given, our system automati-
cally extracts item numerical units and item ex-
pressions, then extracts numerical expressions re-
lated to these, and finally makes a graph based
on the extracted numerical expressions. When a
document set is given, the system automatically
makes a graph that includes trend information.
Our system also uses an original method of pro-
ducing more than one graphs and selecting an ap-
propriate graph among them using Methods 1 to 4,
which Fujihata et al and Namba et al did not use.
6 Conclusion
We have studied the automatic extraction of trend
information from text documents such as newspa-
per articles. Such extraction will be useful for ex-
ploring and examining trends. We used data sets
provided by a workshop on multimodal summa-
rization for trend information (the MuST Work-
shop) to construct our automatic trend exploration
system. This system first extracts units, tempo-
rals, and item expressions from newspaper arti-
cles, then it extracts sets of expressions as trend
information, and finally it arranges the sets and
displays them in graphs.
In our experiments, when we judged the extrac-
tion of a correct graph as the top output to be cor-
rect, the system accuracy was 0.2500 in evaluation
10
A and 0.3334 in evaluation B. (In evaluation A, a
graph where 75% or more of the points were cor-
rect was judged to be correct; in evaluation B, a
graph where 50% or more of the points were cor-
rect was judged to be correct.) When we judged
the extraction of a correct graph in the top five out-
puts to be correct, we obtained accuracy of 0.4167
in evaluation A and 0.6250 in evaluation B. Our
system is convenient and effective because it can
output a graph that includes trend information at
these levels of accuracy when only a set of docu-
ments is provided as input.
In the future, we plan to continue this line of
study and improve our system. We also hope to
apply the method of using term frequency in doc-
uments to extract trend information as reported by
Murata et al (Murata et al, 2005a).
References
Katsuyuki Fujihata, Masahiro Shiga, and Tatsunori
Mori. 2001. Extracting of numerical expressions
by constraints and default rules of dependency struc-
ture. Information Processing Society of Japan,
WGNL 145.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. MuST: A workshop on multimodal
summarization for trend information. Proceedings
of the Fifth NTCIR WorkshopMeeting on Evaluation
of Information Access Technologies: Information
Retrieval, Question Answering and Cross-Lingual
Information Access.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, and Masayuki
Asahara. 1999. Japanese morphological analysis
system ChaSen version 2.0 manual 2nd edition.
Masaki Murata, Kiyotaka Uchimoto, Hiromi Ozaku,
Qing Ma, Masao Utiyama, and Hitoshi Isahara.
2000. Japanese probabilistic information retrieval
using location and category information. The Fifth
International Workshop on Information Retrieval
with Asian Languages, pages 81?88.
Masaki Murata, Koji Ichii, Qing Ma, Tamotsu Shirado,
Toshiyuki Kanamaru, and Hitoshi Isahara. 2005a.
Trend survey on Japanese natural language process-
ing studies over the last decade. In The Second In-
ternational Joint Conference on Natural Language
Processing, Companion Volume to the Proceedings
of Conference including Posters/Demos and Tutorial
Abstracts.
Masaki Murata, Masao Utiyama, and Hitoshi Isahara.
2005b. Use of multiple documents as evidence with
decreased adding in a Japanese question-answering
system. Journal of Natural Language Processing,
12(2).
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
Information Processing Society of Japan, WGNL
168, pages 67?74.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M. Gatford. 1994. Okapi at TREC-3.
In TREC-3.
11
