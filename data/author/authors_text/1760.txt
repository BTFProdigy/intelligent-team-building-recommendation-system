A Morphologically Sensitive Clustering Algorithm for Identifying
Arabic Roots
Anne N. DE ROECK
Department of Computer Science
University of Essex
Colchester, CO4 3SQ, U.K.
deroe@essex.ac.uk
Waleed AL-FARES
Computer Science Department
College of Business Studies,
Hawaly, Kuwait
al-fareswaleed@usa.net
Abstract
We present a clustering algorithm for Arabic
words sharing the same root. Root based
clusters can substitute dictionaries in
indexing for IR. Modifying Adamson and
Boreham (1974), our Two-stage algorithm
applies light stemming before calculating
word pair similarity coefficients using
techniques sensitive to Arabic morphology.
Tests show a successful treatment of infixes
and accurate clustering to up to 94.06% for
unedited Arabic text samples, without the
use of dictionaries.
Introduction
Canonisation of words for indexing is an
important and difficult problem for Arabic IR.
Arabic is a highly inflectional language with
85% of words derived from tri-lateral roots (Al-
Fedaghi and Al-Anzi 1989). Stems are derived
from roots through the application of a set of
fixed patterns. Addition of affixes to stems
yields words. Words sharing a root are
semantically related and root indexing is
reported to outperform stem and word indexing
on both recall and precision (Hmeidi et al1997).
However, Arabic morphology is excruciatingly
complex (the Appendix attempts a brief
introduction), and root identification on a scale
useful for IR remains problematic.
Research on Arabic IR tends to treat automatic
indexing and stemming separately. Al-Shalabi
and Evans (1998) and El-Sadany and Hashish
(1989) developed stemming algorithms. Hmeidi
et al(1997) developed an information retrieval
system with an index, but does not explain the
underlying stemming algorithm. In Al-Kharashi
and Evans (1994), stemming is done manually
and the IR index is built by manual insertion of
roots, stems and words.
Typically, Arabic stemming algorithms operate
by ?trial and error?. Affixes are stripped away,
and stems ?undone?, according to patterns and
rules, and with reference to dictionaries. Root
candidates are checked against a root lexicon. If
no match is found, affixes and patterns are re-
adjusted and the new candidate is checked. The
process is repeated until a root is found.
Morpho-syntactic parsers offer a possible
alternative to stemming algorithms. Al-Shalabi
and Evans (1994), and Ubu-Salem et al(1999)
develop independent analysers. Some work
builds on established formalisms such a DATR
(Al-Najem 1998), or KIMMO. This latter strand
produced extensive deep analyses. Kiraz (1994)
extended the architecture with multi-level tape,
to deal with the typical interruption of root letter
sequences caused by broken plural and weak
root letter change. Beesley (1996) describes the
re-implementation of earlier work as a single
finite state transducer between surface and
lexical (root and tag) strings. This was refined
(Beesley 1998) to the current on-line system
capable of analysing over 70 million words.
So far, these approaches have limited scope for
deployment in IR. Even if substantial, their
morpho-syntactic coverage remains limited and
processing efficiency implications are often
unclear. In addition, modern written Arabic
presents a unique range of orthographic
problems. Short vowels are not normally written
(but may be). Different regional spelling
conventions may appear together in a single text
and show interference with spelling errors.
These systems, however, assume text to be in
perfect (some even vowelised) form, forcing the
need for editing prior to processing. Finally, the
success of these algorithms depends critically on
root, stem, pattern or affix dictionary quality,
and no sizeable and reliable electronic
dictionaries exist. Beesley (1998) is the
exception with a reported 4930 roots encoded
with associated patterns, and an additional affix
and non-root stem lexicon1. Absence of large
and reliable electronic lexical resources means
dictionaries would have to be updated as new
words appear in the text, creating a maintenance
overhead. Overall, it remains uncertain whether
these approaches can be deployed and scaled up
cost-effectively to provide the coverage required
for full scale IR on unsanitised text.
Our objective is to circumvent morpho-
syntactic analysis of Arabic words, by using
clustering as a technique for grouping words
sharing a root. In practise, since Arabic words
derived from the same root are semantically
related, root based clusters can substitute root
dictionaries for indexing in IR and furnish
alternative search terms. Clustering works
without dictionaries, and the approach removes
dictionary overheads completely. Clusters can
be implemented as a dimension of the index,
growing dynamically with text, and without
specific maintenance. They will accommodate
effortlessly a mixture of regional spelling
conventions and even some spelling errors.
1 Clustering and Arabic.
To our knowledge, there is no application of
automatic root-based clustering to Arabic, using
morphological similarity without dictionary.
Clustering and stemming algorithms have
mainly been developed for Western European
languages, and typically rely on simple heuristic
rules to strip affixes and conflate strings. For
instance, Porter (1980) and Lovins (1968)
confine stemming to suffix removal, yet yield
acceptable results for English, where roots are
relatively inert. Such approaches exploit the
morphological frugality of some languages, but
do not transfer to heavily inflected languages
such as Arabic.
In contrast, Adamson and Boreham (1974)
developed a technique to calculate a similarity
co-efficient between words as a factor of the
number of shared sub-strings. The approach
(which we will call Adamson?s algorithm for
short) is a promising starting point for Arabic
                                            
1 Al-Fedaghi and Al-Anzi (1989) estimate there are
around 10,000 independent roots.
clustering because affix removal is not critical to
gauging morphological relatedness.
In this paper, we explain the algorithm, apply
it to raw modern Arabic text and evaluate the
result. We explain our Two-stage algorithm,
which extends the technique by (a) light
temming and (b) refinements sensitive to
Arabi  morphology. We show how the
adaptation increased successful clustering of
both the original and new evaluation data.
2 Data Description
We focus on IR, so experiments use modern,
unedited Arabic text, with unmarked short
vowels (Stalls and Knight 1998). In all we
constructed five data sets. The first set is
controlled, and was designed for testing on a
broad spectrum of morphological variation. It
contains selected roots with derived words
chosen for their problematic structure, featuring
infixes, root consonant changes and weak letters.
It also includes superficially similar words
belonging to different roots, and examples of
hamza as a root consonant, an affix and a silent
sign. Table 1 gives details.
Table 1: Cluster size for 1st data set
root size root size
ktb wrote 49 HSL obtained 7
qwm straightened38 s?aL asked 6
mr passed 26 HSd cultivated5
wSL linked 11 shm shared 4
r?as headed 10
Data sets two to four contain articles extracted
from Al-Raya (1997), and the fifth from Al-
Watan (2000), both newspapers from Qatar.
Following Adamson, function words have been
removed. The sets have domain bias with the
second (575 words) and the fourth (232 words)
drawn randomly from the economics and the
third (750 words) from the sports section. The
fifth (314 words) is a commentary on political
history. Sets one to three were used to varying
extents in refining our Two-stage algorithm. Sets
four and five were used for evaluation only.
Electronically readable Arabic text has only
recently become available on a useful scale,
hence our experiments were run on short texts.
On the other hand, the coverage of the data sets
allows us to verify our experiments on
demanding samples, and their size lets us verify
correct clustering manually.
3. Testing Adamson?s Algorithm
3.1 The Algorithm
Adamson and Boreham (1974) developed a
technique expressing relatedness of strings as a
factor of shared sub-strings. The algorithm drags
an n-sized window across two strings, with a 1
character overlap, and removes duplicates. The
strings' similarity co-efficient (SC) is calculated
by Dice?s equation: SC (Dice) = 2*(number of
shared unique n-grams)/(sum of unique n-grams
in each string)
Table 2: Adamson's Algorithm Illustrated
String 2-grams Unique 2-grams
phosphorus ph ho os sp ph
ho or ru us
ph ho os sp or ru
us (7)
phosphate ph ho os sp ph
ha at te
ph ho os sp ha at
te (7)
Shared unique 2-grams ph ho os sp (4)
SC (Dice) = 2(4)/(7+7) = 0.57
After the SC for all word pairs is known, the
single link clustering algorithm is applied. A
similarity (or dissimilarity) threshold is set. The
SC of pairs is collected in a matrix. The
threshold is applied to each pair?s SC to yield
clusters. A cluster absorbs a word as long as its
SC to another cluster item exceeds the threshold
(van Rijsbergen 1979). Similarity to a single
item is sufficient. Cluster size is not pre-set.
3.2 Background Assumptions
This experiment tests Adamson's algorithm on
Arabic data to assess its ability to cluster words
sharing a root. Each of the data sets was
clustered manually to provide an ideal
benchmark. This task was executed by a native
Arabic speaker with reference to dictionaries.
Since we are working with very small texts, we
sought to remove the effects of sampling in the
tests. To assess Adamson?s algorithm?s potential
for clustering Arabic words, we preferred to
compare instances of optimal performance. We
varied the SC to yield, for each data set, the
highest number of correct multi-word clusters.
Note that the higher the SC cut-off, the less
likely that words will cluster together, and the
more single word clusters will appear. This has
the effect of growing the number of correct
clusters because the proportion of correct single
word clusters will increase. As a consequence,
for our purposes, the number of correct multi-
word clusters (and not just correct clusters) are
an important indicator of success.
A correct multi-word cluster covers at least
two words and is found in the manual
benchmark. It contains all and only those words
in the data set which share a root. Comparison
with a manual benchmark inevitably introduces
a subjective element. Also, our evaluation
measure is the percentage of correct benchmark
clusters retrieved. This is a ?recall? type
indicator. Together with the strict definition of
correct cluster, it cannot measure cluster quality.
Finer grained evaluation of cluster quality would
be needed in an IR context.
However, our main concern is comparing
algorithms. The current metrics aim for a
conservative gauge of how Adamson?s
algorithm can yield more exact clusters from a
full range of problematic data.
Table 3: Adamson's Algorithm Test Results
Data set Set 1 Set 2 Set 3 Set 4 Set 5
Benchmark:
   Total Manual Clusters(A)9 267 337 151 190
          Multi-word (B) 9 130 164 50 63
         Single word (C) 0 137 173 101 127
SC cut-off2 0.50 0.54 0.75 0.58-0.60 0.61-0.66
Test:(% of Benchmark)
   Correct Clusters (% of A)11.11% 56.55% 60.83% 70.86% 74.21%
          Multi-word (% of B) 11.11% 38.46% 21.95% 40% 34.92%
          Single word (% of C)0.0% 73.72% 97.69% 86.14% 93.70%
                                            
2 Ranges rather than specific values are given where
cut-offs between the lower and higher value do not
alter cluster distribution.
Our interpretation of correct clustering is
stringent and therefore conservative, adding to
the significance of our results. Cluster quality
will be reviewed informally.
3.3 Adamson?s Arabic Test Results
Table 3 shows results for Adamson?s
algorithm. The figures for the first data set have
to be suitably interpreted. The set deliberately
did not include single word clusters.
The results suggest that the algorithm is very
successful at identifying single word clusters but
performs poorly on multi-word clusters. The
high success rate for single word clusters is
partly due to the high SC cut-off, set to yield as
many correct multi-word clusters as possible.
In terms of quality, however, only a small
proportion of multi-word clusters were found to
contain infix derivations (11.11%, 4.76%, 0.0%
4.35% and 9.09% for each data set respectively),
as opposed to other variations. In other words,
strings sharing character sequences in middle
position cluster together more successfully. Infix
recognition is a weak point in this approach.
Whereas the algorithm is successful for
English, it is no surprise that it should not
perform equally well on Arabic. Arabic words
tend to be short and the chance of words derived
from different roots sharing a significant
proportion of characters is high (eg Khbr (news)
vs Khbz (bread)). Dice?s equation assumes the
ability to identify an uninterrupted sequence of
root consonants. The heavy use of infixes runs
against this. Similarly, affixes cause interference
(see 4.1.1).
4 The Two-Stage Algorithm.
The challenge of root based clustering for
Arabic lies in designing an algorithm which will
ive relevance to root consonants only. Using
Adamson?s algorithm as a starting point, we
devised a solution by introducing and testing a
number of successive refinements based on the
morphological knowledge and the first three
data sets. The rationale motivating these
refinements is given below.
4.1 Refinements
4.1.1Affixes and light stemming:
The high incidence of affixes keeps accurate
cluster formation low, because it increases the
SC among words derived from different roots,
and lowers the SC between derivations of the
same root using different affixes, as illustrated in
tables 4 and 5. Following Popovic and Willet
(1992), we introduced stemming to minimise the
effect of affixes. We found empirically that light
stemming, removing a small number of obvious
affixes, gave better results than heavy stemming
aimed at full affix stripping. Heavy stemming
brought the risk of root consonant loss (eg
t?amyn (insurance) from root amn (sheltered):
heavy stemming: t?am, light stemming: t?amn).
Light stemming, on the other hand, does little
more than reducing word size to 3 or 4
characters.
 4.1.2Weak letters, infixes and ?cross?:
Weak letters (alif, waw, ya) occur freely as
root consonants as well as affixes. Under
derivation, their form and location may change,
or they may disappear. As infixes, they interfere
with SC, causing failure to cluster (table 6).
Their effects were reduced by a method we refer
to as ?cross?. It adds a bi-gram combining the
letters occurring before and after the weak letter.
Table 4: Inflected words from different roots: ?Lm (learned) and arb (arabised)
String Unique 2-grams with affixes Unique 2-grams without affixes
aL?aLmyh (the universal) aL L? ?a Lm my yh  (6) ?a Lm (2)
aL?rbyh (the Arabic)  aL L? ?r rb by yh  (6) ?r rb (2)
SC (Dice) 2(3)/(6+6) = 0.50 2(0)/(2+2) = 0
Table 5: Inflected words from the same root: mrr (passed)
String Unique 2-grams with affixes Unique 2-grams without affixes
mstmr (continuous) ms st tm mr (4) mr (1)
mr (passed) mr (1) mr (1)
SC (Dice) 2(1)/(4+1) = 0.40 2(1)/(1+1) = 1.0
Table 6: Infix derivation from root wqf (stopped) - post light stemming
String Unique 2-grams without cross Unique di-grams with cross
qaf qa af (2) qa af qf (3)
wqf wq qf (2) wq qf (2)
SC (Dice) 2(0)/(2+2) = 0 2(1)/(2+3) = 0.4
4.1.3Suspected affixes and differential
weighting:
Our objective is to define an algorithm which
gives suitable precedence to root consonants.
Light stemming, however does not remove all
affixes. Whereas fool proof affix detection is
problematic due to the overlap between affix and
root consonants, affixes belong to a closed class
and it is possible to identify ?suspect? letters
which might be part of an affix.
Following Harman (1991) we explored the
idea of assigning differential weights to sub-
strings. Giving equal weight of 1 to all
substrings equates the evidence contributed by
all letters, whether they are root consonants or
not. Suspected affixes, however, should not be
allowed to affect the SC between words on a par
with characters contributing stronger evidence.
We conducted a series of experiments with
differential weightings, and determined
empirically that 0.25 weight for strings
containing weak letters, and 0.50 for strings
containing suspected non-weak letter affixes
gave the best SC for the first three data sets.
4.1.4Substring boundaries:
N-gram size can curtail the significance of
word boundary letters (Robertson and Willet
1992). To give them opportunity to contribute
fully to the SC, we introduced word boundary
blanks (Harman 1991).
Also, the larger the n-gram, the greater its
capacity to mask the shorter substring which can
contain important evidence of similarity between
word pairs (Adamson and Boreham 1974). Of
equal importance is the size of the sliding
overlap between successive n-grams (Adams
1991).
Table 7: Blank insertion with ?cross?
String Unique 2-grams (no)
qaf *q qa af qf f* (5)
wqf *w wq *q qf f* (5)
SC (Dice) 2(3)/(5+5) =  0.60
The problem is to find the best setting for n-
gram and overlap size to suit the language. We
sought to determine settings experimentally. Bi-
grams with single character overlap and blank
insertion (* in the examples) at word boundaries
raised the SC for words sharing a root in our
three data sets, and lowered the SC for words
belonging to different roots.
4.1.5SC formula:
Dice?s equation boosts the importance of
unique shared substrings between word pairs, by
doubling their evidence. As we argued earlier,
since Arabic words tend to be short, the relative
impact of shared substrings will already be
dramatic. We replaced the Dice metric with the
Jaccard formula below to reduce this effect (see
van Rijsbergen 1979). SC (Jac) = shared unique
n-grams/(sum of unique n-grams in each string -
shared unique n-grams)
4.2 The Two-stage Algorithm
The Two-stage algorithm is fully implemented.
Words are first submitted to light stemming to
remove obvious affixes. The second stage is
bas d on Adamson?s algorithm, modified as
described above. From the original, we retained
bi-grams with a one character overlap, but
inserted word boundary blanks. Unique bi-grams
are isolated and cross is implemented. Each bi-
gram is assigned a weight (0.25 for bi-grams
containing weak letters; 0.5 for bi-grams
containing potential non-weak letter affixes; 1
for all other bi-grams). Jaccard?s equation
computes a SC for each pair of words. We
retained the single-link clustering algorithm to
ensure comparability.
4.3 Testing the Two-stage Algorithm
Table 8 shows the results of the Two-stage
algorithm for our data sets. The maximally
effective cut of point for all sets lies closer.
Figures for the first set have to be treated with
caution. The perfect clustering is explained by
the text?s perfect spelling and by the sample
containing exactly those problematic phenomena
on which we wanted to concentrate.
Table 8: Two-stage Algorithm Test Results
Data set Set 1 Set 2 Set 3 Set 4 Set 5
Benchmark:
   Total Manual Clusters (A) 9 267 337 151 190
          Multi-word (B) 9 130 164 50 63
          Single word (C) 0 137 173 101 127
SC cut-off 0.42-0.66 0.54 0.54 0.53-0.540.62-0.66
Test:  (% of Benchmark)
   Correct Clusters (% of A) 100% 88.05% 86.94% 94.04% 86.84%
          Multi-word (% of B) 100% 85.39% 82.93% 94% 74.60%
          Single word (% of C) - 90.51% 90.75% 94.06% 92.91%
The algorithm deals with weak letter mutation,
and infix appearance and disappearance in
words sharing a root (eg the root qwm and its
derived words, especially the role of Hamza as
an infix in one of its variations). Even though
the second and third data sets informed the
modifications to a limited extent, their results
show that the improvements stood up to free
text. For the second data set, the Two-stage
algorithm showed 31.5% improvement over
Adamson?s algorithm. Importantly, it discovered
84.13% of the multi-word clusters containing
words with infixes, an improvement of 79.37%.
The values for single word clustering are close
and the modifications preserved the strength of
Adamson?s algorithm in keeping single word
clusters from mixing, because we were able to
maintain a high SC threshold.
On the third data set, the Two-stage algorithm
showed an 26.11% overall improvement, with
84% successful multi-word clustering of words
with infixes (compare 0% for Adamson). The
largest cluster contained 14 words. 10 clusters
counted as unsuccessful because they contained
one superficially similar variation belonging to a
different root (eg TwL (lengthened) and bTL (to
be abolished)). If we allow this error margin, the
success rate of multi-word clustering rises to
90%. Since our SC cut-off was significantly
lower than in Adamson?s base line experiment,
we obtained weaker results for single word
clustering.
The fourth and fifth data sets played no role in
the development of our algorithm and were used
for evaluation purposes only. The Two-stage
algorithm showed an 23.18% overall
improvement in set four. It successfully built all
clusters containing words with infixes (100% -
compare with 4.35% for Adamson?s algorithm),
an improvement of 95.65%. The two-stage
algorithm again preserved the strength of
Adamson at distinguishing single word clusters,
i  spite of a lower SC cut-off.
The results for the fifth data set are particularly
important because the text was drawn from a
differ nt source and domain. Again, significant
improvements in multi and single word
clustering are visible, with a slightly higher SC
cut-off. The algorithm performed markedly
better at identifying multi-word clusters with
infixes (72.72% - compare with 9.09% for
Adamson).
The results suggest that the Two-stage
algorithm preserves the strengths of Adamson
and Boreham (1994), whilst adding a marked
advantage in recognising infixes. The outcome
of the evaluation on fourth and fifth data sets are
very encouraging and though the samples are
small, they give a strong indication that this kind
of approach may transfer well to text from
different domains on a larger scale.
5 Two-stage Algorithm Limitations
Weak letters can be root consonants, but our
differential weighting technique prevents them
from contributing strong evidence, whereas non-
weak letters featuring in affixes, are allowed to
contribute full weight. Modifying this
arrangement would interfere with successful
clustering (eg after light stemming: t is a root
consonant in ntj (produced) and an infix in Ltqy
(from root Lqy - encountered). These limitations
are a result of light stemming.
Although the current results are promising,
evaluation was hampered by the lack of a
sizeable data set to verify whether our solution
would scale up.
Conclusion
We have developed, successfully, an automatic
classification algorithm for Arabic words which
share the same root, based only on their
morphological similarities. Our approach works
on unsanitised text. Our experiments show that
algorithms designed for relatively uninflected
languages can be adapted for highly inflected
languages, by using morphological knowledge.
We found that the Two-stage algorithm gave a
significant improvement over Adamson?s
algorithm for our data sets. It dealt successfully
with infixes in multi-word clustering, an area
where Adamson?s algorithm failed. It matched
the strength of Adamson in identifying single
word clusters, and sometimes did better. Weak
letters and the overlap between root and affix
consonants continue to cause interference.
Nonetheless, the results are promising and
suggest that the approach may scale up
Future work will concentrate on two issues.
The light stemming algorithm and the
differential weighting may be modified to
improve the identification of affixes. The extent
to which the algorithm can be scaled up must be
tested on a large corpus.
Acknowledgements
Our thanks go to the Kuwait State's Public
Authority for Applied Education and Training,
for the supporting research studentship, and to
two anonymous referees for detailed, interesting
and constructive comments.
Appendix - Arabic in a Nutshell
The vast majority of Arabic words are derived
from 3 (and a few 4) letter roots via a complex
morphology. Roots give rise to stems by the
application of a set of fixed patterns. Addition of
affixes to stems yields words.
Table 9: Stem Patterns
Root Pattern Stem
ktb wrote fa?L katb writer
mf?wL mktwb document
qtL killed fa?L qatL killer
mf?wL mqtwL corpse
Table 9 shows examples of stem derivation
from 3-letter roots. Stem patterns are formulated
as variations on the characters f?L (pronounced
as f'l - ? is the symbol for ayn, a strong glottal
stop), where each of the successive consonants
matches a character in the bare root (for ktb, k
matches f, t matches ? and b matches L). Stems
follow the pattern as directed. As the examples
show, each pattern has a specific effect on
meaning. Several hundred patterns exist, but on
average only about 18 are applicable to each
root (Beesley 1998).
The language distinguishes between long and
short vowels. Short vowels affect meaning, but
are not normally written. However, patterns may
involve short vowels, and the effects of some
patterns are indistinguishable in written text.
Readers must infer the intended meaning.
Affixes may be added to the word, either under
derivation, or to mark grammatical function. For
instance, walktab breaks down as w (and) + al
(the) + ktab (writers, or book, depending on the
voweling). Other affixes function as person,
number, gender and tense markers, subject and
direct object pronouns, articles, conjunctions and
prepositions, though some of these may also
occur as separate words (eg wal (and the)).
Arabic morphology presents some tricky NLP
problems. Stem patterns ?interdigitate? with root
consonants, which is difficult to parse. Also, the
long vowels a (lif), w (waw) and y (ya) can
occur as root consonants, in which case they are
considered to be weak letters, and the root a
weak root. Under certain circumstances, weak
letters may change shape (eg waw into ya) or
di appear during derivation. Long vowels also
occur as affixes, so identifying them as affix or
root consonant is often problematic.
The language makes heavy use of infixes as
well as prefixes and suffixes, all of which may
be consonants or long vowels. Apart from
breaking up root letter sequences (which tend to
be short), infixes are easily confused with root
consonants, whether weak or not. The problem
for affix detection can be stated as follows: weak
root consonants are easily confused with long
vowel affixes; consonant affixes are easily
confused with non-weak letter root consonants.
Erroneus stripping of affixes will yield the
wrong root.
Arabic plurals are difficult. The dual and some
plurals are formed by suffixes, in which case
they are called external plurals. The broken, or
internal plural, however, changes the internal
structure of the word according to a set of
patterns. To illustrate the complexity, masculine
plurals take a -wn or -yn suffix, as in mhnds
(engineer), mhndswn. Female plurals add the -at
suffix, or change word final -h to -at, as in
mdrsh (teacher), mdrsat. Broken plurals affect
root characters, as in mal (fund from root mwl),
amwal, or wSL (link from root wSL), ?aySaL.
The examples are rife with long vowels (weak
letters?). They illustrate the degree of
interference between broken plural patterns and
other ways of segmenting words.
Regional spelling conventions are common:
eg. three versions of word initial alif occur. The
most prominent orthographic problem is the
behaviour of hamza, (?), a sign written over a
carrier letter and sounding a lenis glottal stop
(not to be confused with ayn). Hamza is not
always pronounced. Like any other consonant, it
can take a vowel, long or short. In word initial
position it is always carried by alif, but may be
written above or below, or omitted. Mid-word it
is often carried by one of the long vowels,
depending on rules whose complexity often
gives rise to spelling errors. At the end of words,
it may be carried or written independently.
Hamza is used both as a root consonant and an
affix, and is subject to the same problems as
non-weak letter consonants, compounded by
unpredictable orthography: identical words may
have differently positioned hamzas and would
be considered as different strings.
References
Adams, E. (1991)  A Study of Trigrams and their
feasibility as Index Terms in a full text Information
Retrieval System. PhD Thesis, George Washington
University, USA.
Adamson, George W. and J. Boreham (1974)  The
use of an association measure based on character
structure to identify semantically related pairs of
words and document titles. Information Storage
and Retrieval,. Vol 10, pp 253-260
Al-Fedaghi Sabah S. and Fawaz Al-Anzi (1989)  A
new algorithm to generate Arabic root-pattern
forms. Proceedings of the 11th National Computer
Conference, King Fahd University of Petroleum &
Minerals, Dhahran, Saudi Arabia., pp04-07
Al-Kharashi, I. and M. Evens (1994)  Comparing
words, stems, and roots as Index terms in an
Arabic Information Retrieval system. Journal of the
American Society for Information Science, 45/8,
pp. 548-560
Al-Naj m, Salah R. (1998). An Explanation of
Computational Arabic Morphology. DATR
Documentation Report, University of Sussex.
Al-Raya (1997) Newspaper. Quatar.
Al-Shalabi, R. and M. Evens (1998)  A
Computational Morphology System for Arabic.
Proceedings of COLING-ACL, New Brunswick,
NJ.
Al-Watan (2000) Newspaper. Qatar.
Beesley, K.B. (1996)  Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pp 89-94.
Beesley, K.B. (1998)  Arabic Morphological Analysis
on the Internet. Proceedings of the 6th International
Conference and Exhibition on Multi-Lingual
Computing, Cambridge.
El-Sadany, T. and M. Hashish (1989)  An Arabic
morphological system. IBM System Journal, 28/4
Harman, D. (1991)  How effective is suffixing?
Journal of the American Society for Information
Science, 42/1, pp 7-15.
Hmeidi, I., Kanaan, G. and M. Evens (1997)  Design
and Implementation of Automatic Indexing for
Information Retrieval with Arabic Documents.
Journal of the American Society for Information
Science, 48/10, pp. 867-881.
Kiraz, G. (1994)  Multi-tape two-level Morphology: a
case study in Semitic non-linear morphology.
Proceedings of COLING-94, pp180-186.
Lovins, J.B. (1968)  Development of a Stemming
Algorithm. Mechanical Translation and
Computational Linguistics, 11/1.
Popovic, M. and P. Willet (1992)  The effectiveness
of stemming for natural language access to Sloven
textual data. Journal of the American Society for
Information Science, 43/5, pp. 384-390.
Porter, M.F. (1980)  An Algorithm for suffix
stripping. Program, 14 /3, pp 130-137
Stalls, B. and Knight, K. (1998)  Translating names
and technical terms in Arabic text. Proceedings of
COLING-ACL, New Brunswick, NJ, 1998
van Rijsbergen, C. J. (1979)  Information Retrieval.
Butterworths, London.
Robertson, A. and Willett, P.(1992)  Searching for
historical word-forms in a database of 17th-century
English text using spelling-correction methods. 15th
Annual International Conference SIGIR.
Ubu-Salem H., Al-Omari M., and M. Evens (1999)
Stemming methodologies over individual query
words for an Arabic information retrieval system.
Journal of the American Society for Information
Science. 50/6, pp 524-529.
Identifying Broken Plurals in Unvowelised Arabic Text 
 
Abduelbaset Goweder 
University of Essex 
Dept. of Computer 
Science 
Wivenhoe Park,  
Colchester  CO4 3SQ, 
UK 
agowed@essex.ac.uk 
Massimo Poesio 
University of Essex 
Dept. of Computer 
Science 
Wivenhoe Park, 
Colchester  CO4 3SQ, 
UK 
poesio@essex.ac.uk 
Anne De Roeck 
The Open University 
Dept. of Computing 
Walton Hall, Milton 
Keynes 
Buckinghamshire, MK7 
6AA, UK 
A.DeRoeck@open.ac.uk 
Jeff Reynolds 
University of Essex 
Dept. of Computer 
Science 
Wivenhoe Park, 
Colchester  CO4 3SQ, 
UK 
reynt@essex.ac.uk 
 
Abstract 
Irregular (so-called broken) plural identification 
in modern standard Arabic is a problematic issue 
for information retrieval (IR) and language 
engineering applications, but their effect on the 
performance of IR has never been examined. 
Broken plurals (BPs) are formed by altering the 
singular (as in English: tooth  teeth) through 
an application of interdigitating patterns on 
stems, and singular words cannot be recovered 
by standard affix stripping stemming techniques. 
We developed several methods for BP detection, 
and evaluated them using an unseen test set. We 
incorporated the BP detection component into a 
new light-stemming algorithm that conflates both 
regular and broken plurals with their singular 
forms. We also evaluated the new light-stemming 
algorithm within the context of information 
retrieval, comparing its performance with other 
stemming algorithms. 
1. Introduction 
Broken plurals constitute ~10% of texts in large 
Arabic corpora (Goweder and De Roeck, 2001), and 
~41% of plurals (Boudelaa and Gaskell, 2002). 
Detecting broken plurals is therefore an important 
issue for light-stemming algorithms developed for 
applications such as information retrieval, yet the 
effect of broken plural identification on the 
performance of information retrieval systems has 
not been examined. We present several methods for 
BP detection, and evaluate them using an unseen 
test set containing 187,309 words. We also 
developed a new light-stemming algorithm 
incorporating a BP recognition component, and 
evaluated it within an information retrieval context, 
comparing its performance with other stemming 
algorithms.  
We give a brief overview of Arabic in Section 2. 
Several approaches to BP detection are discussed in 
Section 3, and their evaluation in Section 4. In 
Section 5, we present an improved light stemmer 
and its evaluation. Finally in Section 6, our 
conclusions are summarised.  
2. Arabic Morphology and its Number 
System 
Arabic is a heavily inflected language. Its 
grammatical system is traditionally described in 
terms of a root-and-pattern structure, with about 
10,000 roots (Ali, 1988). Roots such as drs () 
and ktb () are listed alphabetically in standard 
Arabic dictionaries like the Wehr-Cowan (Beesley, 
1996). The root is the most basic verb form. Roots 
are categorized into: triliteral, quadriliteral, or rarely 
pentaliteral. Most words are derived from a finite set 
of roots formed by adding diacritics1 or affixes 
(prefixes, suffixes, and infixes) through an 
application of fixed patterns which are templates to 
help in deriving inflectional and derivational forms 
of a word.  Theoretically, several hundreds of 
Arabic words can be derived from a single root. 
Traditional Arab grammarians describe Arabic 
morphology in terms of patterns associated with the 
basic root f3l (	, ?to do?)- where f, 3, and l are like 
wildcards in regular expressions: the letter f (
 
,?pronounced fa?) represents the first consonant 
(sometimes called a radical), the letter 3 ( , 
?pronounced ain?) represents the second, and the 
letter l ( , ?pronounced lam?) represents the third 
                                                                
1
 Special characters which are superscript or subscript marks 
added to the word. 
respectively. Adding affixes to the basic root f3l 
(	, ?to do?) allows additional such patterns to be 
formed. For instance, adding the letter Alef () as a 
prefix to the basic root f3l (	, ?to do?) we get the 
pattern Af3l (	) which is used to form words such 
as: anhr (, ?rivers?), arjl (, ?legs?), and asqf 
(, ?ceilings?). Some examples of the word 
patterns are Yf3l (), Mf3Wl (), Af3Al (	E-Assessment using Latent Semantic Analysis in the Computer Science 
Domain: A Pilot Study 
Pete Thomas, Debra Haley, Anne deRoeck, Marian Petre 
Computing Research Centre, Department of Computing 
The Open University, Walton Hall, Milton Keynes, UK MK7 6AA 
P.G.Thomas;D.T.Haley;A.Deroeck;M.Petre [at] open.ac.uk 
 
Abstract 
Latent Semantic Analysis (LSA)  is a statistical 
Natural Language Processing (NLP) technique for 
inferring meaning from a text. Existing LSA-based 
applications focus on formative assessment in 
general domains. The suitability of LSA for 
summative assessment in the domain of computer 
science is not well known.  The results from the 
pilot study reported in this paper encourage us to 
pursue further research in the use of LSA in the 
narrow, technical domain of computer science. 
This paper explains the theory behind LSA, 
describes some existing LSA applications, and 
presents some results using LSA for automatic 
marking of short essays for a graduate class in 
architectures of computing systems.  
1 Introduction 
This paper describes a pilot study undertaken to 
investigate the feasibility of using Latent Semantic 
Analysis (LSA) for automatic marking of short 
essays in the domain of computer science. These 
short essays are free-form answers to exam 
questions - not multiple choice questions (MCQ). 
Exams in the form of MCQs, although easy to 
mark, do not provide the opportunity for deeper 
assessment made possible with essays.  
This study employs LSA in several areas that are 
under-researched. First, it uses very small corpora 
? less than 2,000 words compared to  about 11 
million words in one of the existing, successful 
applications (Wade-Stein & Kintsch, 2003). 
Second, it involves the specific, technical domain 
of computer science. LSA research usually 
involves more heterogeneous text with a broad 
vocabulary. Finally, it focuses on summative 
assessment where the accuracy of results is 
paramount. Most LSA research has involved 
formative assessment for which more general 
evaluations are sufficient. 
The study investigates one of the shortcomings 
of LSA mentioned by Manning and Sch?tze (1999, 
p. 564). They report that LSA has high recall but 
low precision. The precision declines because of 
spurious co-occurrences. They claim that LSA 
does better on heterogeneous text with a broad 
vocabulary.  Computer science is a technical 
domain with a more homogeneous vocabulary, 
which results, possibly, in fewer spurious co-
occurrences. A major question of this research is 
how LSA will behave when the technique is 
stretched by applying it to a narrow domain.  
Section 2 gives the history of LSA and explains 
how it works. Section 3 describes several existing 
LSA applications related to e-assessment. Section 
4 provides the motivation for more LSA research 
and reports on a pilot study undertaken to assess 
the feasibility of using LSA for automatic marking 
of short essays in the domain of computer science. 
Section 5 lists several open issues and areas for 
improvement that future studies will address. 
Finally, Section 6 summarises the paper. 
2 What is Latent Semantic Analysis? 
 ?Latent Semantic Analysis is a theory and 
method for extracting and representing the 
contextual-usage meaning of words by statistical 
computations applied to a large corpus of text? 
(Landauer, Foltz & Laham, 1998). It is a 
statistical-based natural language processing (NLP) 
method for inferring meaning from a text1. It was 
developed by researchers at Bellcore as an 
information retrieval technique (Deerwester, 
Dumais, Furnas, Landauer & Harshman, 1990) in 
the late 1980s. The earliest application of LSA was 
Latent Semantic Indexing (LSI) (Furnas, et al, 
1988; Deerwester, et al, 1990). LSI provided an 
advantage over keyword-based methods in that it 
could induce associative meanings of the query 
(Foltz, 1996) rather than relying on exact matches.  
Landauer and Dumais (1997) promoted LSA as 
a model for the human acquisition of knowledge. 
They developed their theory after creating an 
information retrieval tool and observing 
unexpected results from its use. They claimed that 
                                                     
1 The researchers originally used the term LSI (Latent 
Semantic Indexing) to refer to the method. The 
information retrieval community continues to use the 
term LSI. 
LSA solves Plato?s problem, that is, how do people 
learn so much when presented with so little? Their 
answer is the inductive process: LSA ?induces 
global knowledge indirectly from local co-
occurrence data in a large body of representative 
text? (Landauer & Dumais, 1997).  
From the original application for retrieving 
information, the applications of LSA have evolved 
to systems that more fully exploit its ability to 
extract and represent meaning. Recent applications 
based on LSA compare a sample text with a pre-
existing, very large corpus to judge the meaning of 
the sample.  
To use LSA, researchers amass a suitable corpus 
of text. They create a term-by-document matrix 
where the columns are documents and the rows are 
terms (Deerwester, et al, 1990). A term is a 
subdivision of a document; it can be a word, 
phrase, or some other unit. A document can be a 
sentence, a paragraph, a textbook, or some other 
unit. In other words, documents contain terms. The 
elements of the matrix are weighted word counts of 
how many times each term appears in each 
document. More formally, each element, aij in an i 
x j matrix is the weighted count of term i in 
document j. 
LSA decomposes the matrix into three matrices 
using Singular Value Decomposition (SVD), a 
well-known technique (Miller, 2003) that is the 
general case of factor analysis. Deerwester et. al., 
(1990) describe the process as follows.  
 
Let t = the number of terms, or rows 
      d =  the number of documents, or columns 
      X = a t by d matrix 
 
Then, after applying SVD, X = TSD, where 
 
m = the number of dimensions, m <= min(t,d) 
T =  a t by m matrix 
S = an m by m diagonal matrix, i.e., only 
diagonal entries have non-zero values 
D =  an m by d matrix 
 
LSA reduces S, the diagonal matrix created by 
SVD, to an appropriate number of dimensions k, 
where k << m, resulting in S'. The product of TS'D 
is the least-squares best fit to X, the original matrix 
(Deerwester, et al, 1990).  
The literature often describes LSA as analyzing 
co-occurring terms. Landauer and Dumais (1997) 
argue it does more and explain that the new matrix 
captures the ?latent transitivity relations? among 
the terms. Terms not appearing in an original 
document are represented in the new matrix as if 
they actually were in the original document 
(Landauer & Dumais, 1997). LSA?s ability to 
induce transitive meanings is considered especially 
important given that Furnas et. al. (1982) report 
fewer than 20% of paired individuals will use the 
same term to refer to the same common concept.  
LSA exploits what can be named the transitive 
property of semantic relationships: If A?B and 
B?C, then A?C (where ? stands for is 
semantically related to). However, the similarity to 
the transitive property of equality is not perfect. 
Two words widely separated in the transitivity 
chain can have a weaker relationship than closer 
words. For example, LSA might find that copy ? 
duplicate ? double ? twin ? sibling. Copy and 
duplicate are much closer semantically than copy 
and sibling. 
Finding the correct number of dimensions for the 
new matrix created by SVD is critical; if it is too 
small, the structure of the data is not captured. 
Conversely, if it is too large, sampling error and 
unimportant details remain, e.g., grammatical 
variants (Deerwester, et al, 1990; Miller, 2003; 
Wade-Stein & Kintsch, 2003). Empirical work 
involving very large corpora shows the correct 
number of dimensions to be about 300 (Landauer 
& Dumais, 1997; Wade-Stein & Kintsch, 2003).  
Creating the matrices using SVD and reducing 
the number of dimensions, often referred to as 
training the system, requires a lot of computing 
power; it can take hours or days to complete the 
processing (Miller, 2003). Fortunately, once the 
training is complete, it takes just seconds for LSA 
to evaluate a text sample (Miller, 2003).  
3 Using LSA for assessment 
3.1 Types of assessment 
Electronic feedback, or e-assessment, is an 
important component of e-learning. LSA, with its 
ability to provide immediate, accurate, 
personalised, and content-based feedback, can be 
an important component of an e-learning 
environment.  
Formative assessment provides direction, focus, 
and guidance concurrent with the learner engaging 
in some learning process. E-assessment can 
provide ample help to a learner without requiring 
added work by a human tutor. A learner can 
benefit from private, immediate, and convenient 
feedback.  
Summative assessment, on the other hand, 
happens at the conclusion of a learning episode or 
activity. It evaluates a learner?s achievement and 
communicates that achievement to interested 
parties. Summative assessment using LSA shares 
the virtues of formative assessment and can 
produce more objective grading results than those 
that can occur when many markers are assessing 
hundreds of student essays.  
The applications described in the next section 
use LSA to provide formative  assessment. Section 
4 discusses a pilot study that focuses on summative 
assessment. 
3.2 Existing applications 
Much work is being done in the area of using 
LSA to mark essays automatically and to provide 
content-based feedback. One of the great 
advantages of automatic assessment of essays is its 
ability to provide helpful, immediate feedback to 
the learner without burdening the teacher. This 
application is particularly suited to distance 
education, where opportunities for one-on-one 
tutoring are infrequent or non-existent (Steinhart, 
2001). Existing systems include Apex (Lemaire & 
Dessus, 2001), Autotutor (Wiemer-Hastings, 
Wiemer-Hastings & Graesser, 1999), Intelligent 
Essay Assessor (Foltz, Laham & Landauer, 1999), 
Select-a-Kibitzer (Miller, 2003), and Summary 
Street (Steinhart, 2001; Wade-Stein & Kintsch, 
2003). They differ in details of audience addressed, 
subject domain, and advanced training required by 
the system (Miller, 2003). They are similar in that 
they are LSA-based, web-based, and provide 
scaffolding, feedback, and unlimited practice 
opportunities without increasing a teacher?s 
workload (Steinhart, 2001). All of them claim that 
LSA correlates as well to human markers as human 
markers correlate to one another. See (Miller, 
2003) for an excellent analysis of these systems. 
4 E-Assessment pilot study  
Although research using Latent Semantic 
Analysis (LSA) to assess essays automatically has 
shown promising results (Chung & O'Neil, 1997; 
Foltz, et al, 1999; Foltz, 1996; Lemaire & Dessus, 
2001; Landauer, et al, 1998; Miller, 2003; 
Steinhart, 2001; Wade-Stein & Kintsch, 2003), not 
enough research has been done on using LSA for 
instructional software (Lemaire & Dessus, 2001). 
Previous studies involved both young students and 
university-age students, and several different 
knowledge domains. An open question is how LSA 
can be used to improve the learning of university-
age, computer science students. This section offers 
three characteristics that distinguish this research 
from existing research involving the use of LSA to 
analyse expository writing texts and reports on a 
pilot study to determine the feasibility of using 
LSA to mark students? short essay answers to 
exam questions. 
4.1 Focuses of the experiment 
This subsection describes three facets of the 
experiment that involve under-researched areas, in 
the cases of the domain and the type of assessment, 
and an unsolved research question in the case of 
the appropriate dimension reduction value for 
small corpora. 
The study involves essays written by computer 
science (CS) students. CS, being a technical 
domain, has a limited, specialist vocabulary. Thus, 
essays written for CS exams are thought to have a 
more restricted terminology than do the expository 
writing texts usually analysed by LSA researchers. 
Nevertheless, the essays are written in English 
using a mixture of technical terms and general 
terms. Will LSA produce valid results?  
Accuracy is paramount in summative 
assessment. Whereas formative assessment can be 
general and informative, summative assessment 
requires a high degree of precision. Can LSA 
produce results with a high degree of correlation 
with human markers? 
The consensus among LSA researchers, who 
customarily use very large corpora, is that the 
number of dimensions that produces the best result 
is about 300. But because this study involved  just 
17 graded samples, the number of reduced 
dimensions has to be less than 17. Can LSA work 
with many fewer dimensions than 300? A broader 
question is whether LSA can work with a small 
corpus in a restricted domain. 
4.2 The Data 
The data for this experiment consisted of 
answers from six students to three questions in a 
single electronic exam held at the Open University 
in April 2002. The answers are free-form short 
essays. The training corpus for each question 
comprised 16 documents consisting of student 
answers to the same question and a specimen 
solution. Table 1 gives the average size (in words) 
of both the student answers graded by LSA and the 
corpus essays. 
 
  
Question 
A 
Question 
B 
Question 
C 
Corpus 
documents 112 35 131 
Student 
answers 108 31 88 
Table 1: Average document size 
 
The corpus training documents had been marked 
previously by three trained human markers. The 
average marks were assigned to each corpus 
document. To provide a standard on which to 
judge the LSA results, each of the answers from 
the six students was marked by three human 
markers and awarded the average mark. 
4.3 The LSA Method 
The following steps were taken three times, once 
for each question on the exam.  
? Determine the words, or terms, in the corpus 
documents after removing punctuation and 
stop words. (No attempt has yet been made 
to deal with synonyms or word forms, such 
as plurals, via stemming.) 
? Construct a t x d term frequency matrix M, 
where t is the number of terms  in the corpus 
and d is the number of documents ? 17 in 
this experiment. Each entry tfij is the number 
of times term i appears in document j. 
? Weight each entry tfij in M using the simple 
weighting scheme: 1 + log(tfij). 
? Perform singular value decomposition of the 
weighted term frequency matrix resulting in 
Mweighted = TSDT. 
? Choose an optimum dimension, k, to reduce 
Mweighted.  (see the next subsection for details) 
? Compute B = SDT - the reduced weighted 
frequency document 
? Construct a vector, a, of weighted term 
frequencies in a student-answer document. 
? Compute the reduced student-answer vector 
a' = aTST 
? Determine the corpus document that best 
matches the student-answer by comparing a' 
with the column vectors in B. 
? Award the student-answer the mark 
associated with the most similar corpus 
document using the cosine similarity 
measure. 
4.4 Determining the optimum dimension 
reduction (k) 
? This experiment reduced the SVD matrices 
using k = 2 .. number of corpus documents ? 
1, or k = 2 .. 16. For each value of k, the 
LSA method produced a mark for each 
student-answer. 
? The experiment compared the six LSA 
marks for the student-answers with the 
corresponding average human mark using 
Euclidean distance. 
? The experiment revealed that, for this 
corpus, k = about 10 gave the best matches 
across the three questions. 
 
4.5 Results 
The four graphs below show the results obtained. 
Question A
0
1
2
3
4
5
6
7
1 2 3 4 5 6
Student
P
o
in
ts
 A
w
ar
de
d
Graded by Human
Graded by LSA
 
Figure 1: LSA marks for question A 
Question B
0
1
2
3
4
5
6
7
1 2 3 4 5 6
Student
P
o
in
ts
 A
w
ar
d
ed Graded by Human
Graded by LSA
 
Figure 2: LSA marks for question B 
Question C
0
1
2
3
4
5
6
7
8
1 2 3 4 5 6
Student
P
oi
nt
s 
A
w
ar
de
d
Graded by Human
Graded by LSA
 
Figure 3: LSA marks for question C 
Total
0
2
4
6
8
10
12
14
16
1 2 3 4 5 6
Student
P
o
in
ts
 A
w
ar
d
ed
Graded by Human
Graded by LSA
 
Figure 4: LSA marks for total 
4.6 Discussion  
This experiment investigated the feasibility of 
using LSA to assess short essay answers. The 
results shown in Figures 1 ? 3 suggest that LSA-
marked answers were similar to human-marked 
answers in 83% (15 of 18) of the answers tested.  
LSA seemed to work well on five of the six 
student-answers for Question A, all the answers for 
Question B, and four of the six answers for 
Question C. For the three clearly incorrect 
answers, LSA gave a higher score than did the 
human markers for the answer to question A and 
one higher mark and one lower mark than did the 
human markers for the answers to question C.   
To quantify these visual impressions, the study 
used the Spearman?s rho statistical test for each of 
the three questions. Only one of the three questions 
shows a statistical correlation between LSA and 
human marks: question B shows a statistical 
correlation significant at the 95% level.   
These results, while unacceptable for a real-
world application, are encouraging given the 
extremely small corpus size of only 17 documents, 
or about 2,000 words for questions A and C and 
about 600 words for question B. This pilot study 
solidified our understanding of how to use LSA, 
the importance of a large corpus, and how to 
approach further research to improve the results 
and increase the applicability of the results of this 
pilot study. 
5 A roadmap for further research 
5.1 The corpus 
LSA results depend on both corpus size and 
corpus content.  
5.1.1 Corpus size 
Existing LSA research stresses the need for a 
large corpus. The corpora for the pilot study 
described in this paper were very small. In 
addition, the documents are too few in number to 
be  representative of the student population. An 
ideal corpus would provide documents that give a 
spread of marks across the mark range and a 
variety of answers for each mark. Future studies 
will use a larger corpus. 
5.1.2 Corpus content 
Wiemer-Hastings, et. al (1999) report that size is 
not the only important characteristic of the corpus. 
Not surprisingly, the composition of the corpus 
effects the results of essay grading by LSA. In 
addition to specific documents directly related to 
their essay questions, Wiemer-Hastings, et. al used 
more general documents. They found the best 
composition to be about 40% general documents 
and 60% specific documents. 
The corpora used for this pilot study comprised 
only specific documents - the human marked short 
essays. Future work will involve adding sections of 
text books to enlarge and enrich the corpus with 
more general documents.   
5.2 Weighting function 
The pilot study used local weighting - the most 
basic form of term weighting. Local weighting is 
defined as tfij (the number of times term i is found 
in document j) dampened by the log function: local 
weighting = 1 + log (tfij ). This dampening reflects 
the fact that a term that appears in a document x 
times more frequently than another term is not x 
times more important.  
The study selected this simple weighting 
function  to provide a basis on which to compare 
more sophisticated functions in future work. Many 
variations of weighting functions exist; two are 
described next. 
5.2.1 Log-entropy 
Dumais (1991) recommended using log-entropy 
weighting, which is local weighting times global 
weighting. Global weighting is defined as 1 ? the 
entropy or noise. Global weighting attempts to 
quantify the fact that a term appearing in many 
documents is less important than a term appearing 
in fewer documents. 
 
The log-entropy term weight for term i in doc j =  
( ) ( )
??
??
?
?
??
??
?
? ?
??+
?
numdocs
gf
tf
gf
tf
tf i
ij
i
ij
ij log
log
11log  
where  
ijtf  ? term frequency ? the frequency of term i in 
document j 
igf  ? global frequency ? the total number of 
times term i occurs in the whole collection 
5.2.2 tfidf 
Sebastiani (2002) claims the most common 
weighting is tfidf, or term frequency inverse 
document frequency. 
 
( )jk dttfidf ,  = ( ) ( )kjk tTr
Tr
dt
#
log,# ?  
where #( tk, dj ) denotes the number of times tk 
occurs in dj
#Tr(tk) denotes the document frequency of term tk, 
that is, the number of documents in Tr in which tk 
occurs. 
 
Future studies will examine the effects of 
applying various term weighting functions. 
5.3 Similarity measures 
The pilot study used two different similarity 
measures. It used the cosine measure to compare 
the test document with the corpus documents. It 
used Euclidean distance to choose k, the number of 
reduced dimensions that produced the best results 
overall. Other measures exist and will be tried in 
future studies. 
Ljungstrand and Johansson (1998) define the 
following similarity measures: 
Inner product (dot) measure: 
M( X, Y ) =  ?
=
n
i
ii yx
1
Cosine measure: 
M( X, Y ) = 
??
?
==
=
n
i
i
n
i
i
n
i
ii
yx
yx
11
1  
Manhattan distance measure: 
M( X, Y ) = ?
=
?
n
i
ii yx
1
 
Euclidean distance measure (2-norm): 
M( X, Y ) = ( )?
=
?
n
i
ii yx
1
2  
m-norm measure: 
M( X, Y ) = ( ) mn
i
m
ii yx
1
1
??
???
?? ?
=
,  m ? N 
Where X = (x1,x2,...,xn) and Y = (y1,y2,...,yn) are two 
n-dimensional vectors. 
Figure 5. Similarity Measures 
5.4 Corpus pre-processing 
Removing stop words is one type of pre-
processing performed for this study. Explicitly 
adding synonym knowledge and stemming are two 
additional ways of preparing the corpus that future 
research will consider. Stemming involves 
conflating word forms to a common string, e.g., 
write, writing, writes, written, writer would be 
represented in the corpus as writ.  
5.5 Dimension reduction 
Choosing the appropriate dimension, k, for 
reducing the matrices in LSA is a well known open 
issue. The current consensus is that k should be 
about 300. No theory yet exists to suggest the 
appropriate value for k. Currently, researchers 
determine k by  empirically testing various values 
of k and selecting the best one. The only heuristic 
says that k << min(terms, documents). An 
interesting result from the study reported in this 
paper is that even though k had to be less than 17, 
the number of documents in our corpora and thus 
much less than the recommended value of 300, 
LSA produced statistically significant results for 
one of the three questions tested.  
Future studies will continue to investigate the 
relationship among k, the size of the corpus, the 
number of documents in the corpus, and the type of 
documents in the corpus. 
6 Summary 
This paper introduced and explained LSA and 
how it can be used to provide e-assessment by both 
formative and summative assessment. It provided 
examples of existing research that uses LSA for e-
assessment. It reported the results of a pilot study 
to determine the feasibility of using LSA to assess 
automatically essays written in the domain of 
computer science. Although just one of the three 
essay questions tested showed that LSA marks 
were statistically correlated to the average of three 
human marks, the results are promising because 
the experiment used very small corpora. 
Future studies will attempt to improve the results 
of LSA by increasing the size of the corpora, 
improving the content of the corpora, 
experimenting with different weighting functions 
and similarity measures, pre-processing the corpus, 
and using various values of k for dimension 
reduction. 
7 Acknowledgements 
The work reported in this study was partially supported by 
the European Community under the Innovation Society 
Technologies (IST) programme of the 6th Framework 
Programme for RTD - project ELeGI, contract IST-002205. 
This document does not represent the opinion of the European 
Community, and the European Community is not responsible 
for any use that might be made of data appearing therein. 
8 References 
Chung, G., & O'Neil, G. (1997). Methodological 
approaches to online scoring of essays (Center 
for the Study of Evaluation, CRESST No. 461).  
Los Angeles. 
Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T. K., & Harshman, R. (1990). 
Indexing by Latent Semantic Analysis. Journal 
of the American Society for Information Science, 
41(6), 391-407. 
Dumais, S. T. (1991). Improving the retrieval of 
information from external sources. Behavioral 
Research Methods, Instruments & Computers, 
23(2), 229-236. 
Foltz, P. W. (1996). Latent semantic analysis for 
text-based research. Behavior Research Methods, 
Instruments and Computers, 28(2), 197-202. 
Foltz, P. W., Laham, D., & Landauer, T. K. (1999). 
Automated Essay Scoring: Applications to 
Educational Technology. In Proceedings of 
EdMedia '99. 
Furnas, G. W., Deerwester, S., Dumais, S. T., 
Landauer, T. K., Harshman, R. A., Streeter, L. 
A., et al (1988). Information retrieval using a 
singular value decomposition model of latent 
semantic structure. ACM, pp. 465-480. 
Furnas, G. W., Gomez, L. M., Landauer, T. K., & 
Dumais, S. T. (1982). Statistical semantics: How 
can a computer use what people name things to 
guess what things people mean when they name 
things? In Proceedings of the SIGCHI 
Conference on Human Factors in Computing 
Systems (pp. 251-253). ACM. 
Landauer, T. K., & Dumais, S. T. (1997). A 
solution to Plato's problem: The Latent Semantic 
Analysis theory of acquisition, induction and 
representation of knowledge. Psychological 
Review, 104(2), 211-240. 
Landauer, T. K., Foltz, P. W., & Laham, D. (1998). 
An introduction to Latent Semantic Analysis. 
Discourse Processes, 25, 259-284. 
Lemaire, B., & Dessus, P. (2001). A system to 
assess the semantic content of student essays. 
Journal of Educational Computing Research, 
24(3), 305-320. 
Ljungstrand, P., & Johansson, H. (1998, May). 
Intranet indexing using semantic document 
clustering. Retrieved 5/4/2004, from 
http://www.handels.gu.se/epc/archive/00002294/
01/ljungstrand.IA7400.pdf. 
Manning, C., & Sch?tze, H. (1999). Foundations 
of Statistical Natural Language Processing. 
Cambridge, Massachusetts: MIT Press. 
Miller, T. (2003). Essay assessment with Latent 
Semantic Analysis. Journal of Educational 
Computing Research, 28. 
Sebastiani, F. (2002, March). Machine Learning in 
Automated Text Categorization. ACM 
Computing Surveys, 34(1), 1-47. 
Steinhart, D. J. (2001). Summary Street: An 
intelligent tutoring system for improving student 
writing through the use of Latent Semantic 
Analysis. Unpublished doctoral dissertation, 
University of Colorado, Boulder, Department of 
Psychology. 
Wade-Stein, D., & Kintsch, E. (2003). Summary 
Street: Interactive computer support for writing 
(Tech Report from the Institute for Cognitive 
Science). University of Colorado, USA. 
Wiemer-Hastings, P., Wiemer-Hastings, K., & 
Graesser, A. C. (1999). Improving an intelligent 
tutor's comprehension of students with Latent 
Semantic Analysis. In S. Lajoie & M. Vivet 
(Eds.), Artificial Intelligence in Education. 
Amsterdam: IOS Press. 
 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 48?55, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Bayesian mixture model for term re-occurrence and burstiness
Avik Sarkar1, Paul H Garthwaite2, Anne De Roeck1
1 Department of Computing, 2 Department of Statistics
The Open University
Milton Keynes, MK7 6AA, UK
{a.sarkar, p.h.garthwaite, a.deroeck}@open.ac.uk
Abstract
This paper proposes a model for term re-
occurrence in a text collection based on
the gaps between successive occurrences
of a term. These gaps are modeled using
a mixture of exponential distributions. Pa-
rameter estimation is based on a Bayesian
framework that allows us to fit a flexi-
ble model. The model provides measures
of a term?s re-occurrence rate and within-
document burstiness. The model works
for all kinds of terms, be it rare content
word, medium frequency term or frequent
function word. A measure is proposed to
account for the term?s importance based
on its distribution pattern in the corpus.
1 Introduction
Traditionally, Information Retrieval (IR) and Statis-
tical Natural Language Processing (NLP) applica-
tions have been based on the ?bag of words? model.
This model assumes term independence and homo-
geneity of the text and document under considera-
tion, i.e. the terms in a document are all assumed
to be distributed homogeneously. This immediately
leads to the Vector Space representation of text. The
immense popularity of this model is due to the ease
with which mathematical and statistical techniques
can be applied to it.
The model assumes that once a term occurs in a
document, its overall frequency in the entire doc-
ument is the only useful measure that associates a
term with a document. It does not take into consid-
eration whether the term occurred in the beginning,
middle or end of the document. Neither does it con-
sider whether the term occurs many times in close
succession or whether it occurs uniformly through-
out the document. It also assumes that additional
positional information does not provide any extra
leverage to the performance of the NLP and IR ap-
plications based on it. This assumption has been
shown to be wrong in certain applications (Franz,
1997).
Existing models for term distribution are based on
the above assumption, so they can merely estimate
the term?s frequency in a document or a term?s top-
ical behavior for a content term. The occurrence of
a content word is classified as topical or non-topical
based on whether it occurs once or many times in
the document (Katz, 1996). We are not aware of any
existing model that makes less stringent assumptions
and models the distribution of occurrences of a term.
In this paper we describe a model for term re-
occurrence in text based on the gaps between succes-
sive occurrences of the term and the position of its
first occurrence in a document. The gaps are mod-
eled by a mixture of exponential distributions. Non-
occurrence of a term in a document is modeled by
the statistical concept of censoring, which states that
the event of observing a certain term is censored at
the end of the document, i.e. the document length.
The modeling is done in a Bayesian framework.
The organization of the paper is as follows. In
section 2 we discuss existing term distribution mod-
els, the issue of burstiness and some other work that
demonstrates the failure of the ?bag of words? as-
48
sumption. In section 3 we describe our mixture
model, the issue of censoring and the Bayesian for-
mulation of the model. Section 4 describes the
Bayesian estimation theory and methodology. In
section 5 we talk about ways of drawing infer-
ences from our model, present parameter estimates
on some chosen terms and present case studies for a
few selected terms. We discuss our conclusions and
suggest directions for future work in section 6.
2 Existing Work
2.1 Models
Previous attempts to model a term?s distribution pat-
tern have been based on the Poisson distribution. If
the number of occurrences of a term in a document
is denoted by k, then the model assumes:
p(k) = e??
?k
k!
for k = 0, 1, 2, . . . Estimates based on this model
are good for non-content, non-informative terms, but
not for the more informative content terms (Manning
and Schu?tze, 1999).
The two-Poisson model is suggested as a variation
of the Poisson distribution (Bookstein and Swanson,
1974; Church and Gale, 1995b). This model as-
sumes that there are two classes of documents as-
sociated with a term, one class with a low average
number of occurrences and the other with a high av-
erage number of occurrences.
p(k) = ?e??1
?k
1
k!
+ (1 ? ?)e??2
?k
2
k!
,
where ? and (1 ? ?) denote the probabilities of a
document in each of these classes. Often this model
under-estimates the probability that a term will oc-
cur exactly twice in a document.
2.2 Burstiness
Burstiness is a phenomenon of content words,
whereby they are likely to occur again in a text af-
ter they have occurred once. Katz (1996) describes
within-document burstiness as the close proximity of
all or some individual instances of a word within a
document exhibiting multiple occurrences.
He proposes a model for within-document bursti-
ness with three parameters as:
? the probability that a term occurs in a document
at all (document frequency)
? the probability that it will occur a second time
in a document given that it has occurred once
? the probability that it will occur another time,
given that it has already occurred k times
(where k > 1).
The drawbacks of this model are: (a) it cannot han-
dle non-occurrence of a term in a document; (b) the
model can handle only content terms, and is not suit-
able for high frequency function words or medium
frequency terms; and (c) the rate of re-occurrence of
the term or the length of gaps cannot be accounted
for. We overcome these drawbacks in our model.
A measure of burstiness was proposed as a binary
value that is based on the magnitude of average-term
frequency of the term in the corpus (Kwok, 1996).
This measure takes the value 1 (bursty term) if the
average-term frequency value is large and 0 other-
wise. The measure is too naive and incomplete to
account for term burstiness.
2.3 Homogeneity Assumption
The popular ?bag of words? assumption for text
states that a term?s occurrence is uniform and ho-
mogeneous throughout. A measure of homogeneity
or self-similarity of a corpus can be calculated, by
dividing the corpus into two frequency lists based
on the term frequency and then calculating the ?2
statistic between them (Kilgarriff, 1997). Various
schemes for dividing the corpus were used (De
Roeck et al, 2004a) to detect homogeneity of terms
at document level, within-document level and by
choosing text chunks of various sizes. Their work
revealed that homogeneity increases by nullifying
the within document term distribution pattern and
homogeneity decreases when chunks of larger size
are chosen as it incorporates more document struc-
ture in it. Other work based on the same method-
ology (De Roeck et al, 2004b) reveals that even
very frequent function words do not distribute ho-
mogeneously over a corpus or document. These (De
Roeck et al, 2004a; De Roeck et al, 2004b) provide
evidence of the fact that the ?bag of words? assump-
tion is invalid. Thus it sets the platform for a model
49
that defies the independence assumption and consid-
ers the term distribution pattern in a document and
corpus.
3 Modeling
3.1 Terminology and Notation
We build a single model for a particular term in a
given corpus. Let us suppose the term under consid-
eration is x as shown in Figure 1. We describe the
notation for a particular document, i in the corpus.
Figure 1: The document structure and the gaps be-
tween terms
? d
i
denotes the number of words in document i
(i.e. the document length).
? n
i
denotes the number of occurrences of term
x in document i.
? w
i1
denotes the position of the first occurrence
of term x in document i.
? w
i2
, . . . , w
in
i
denotes the successive gaps be-
tween occurrences of term x in document i.
? w
in
i
+1
denotes the gap for the next occurrence
of x, somewhere after the document ends.
? cen
i
is the value at which observation w
in
i
+1
is censored, as explained in section 3.2.2.
3.2 The Model
We suppose we are looking through a document,
noting when the term of interest occurs. Our model
assumes that the term occurs at some low underly-
ing base rate 1/?
1
but, after the term has occurred,
then the probability of it occurring soon afterwards
is increased to some higher rate 1/?
2
. Specifically,
the rate of re-occurrence is modeled by a mixture of
two exponential distributions. Each of the exponen-
tial components is described as follows:
? The exponential component with larger mean
(average), 1/?
1
, determines the rate with which
the particular term will occur if it has not oc-
curred before or it has not occurred recently.
? The second component with smaller mean
(average), 1/?
2
, determines the rate of re-
occurrence in a document or text chunk given
that it has already occurred recently. This com-
ponent captures the bursty nature of the term in
the text (or document) i.e. the within-document
burstiness.
The mixture model is described as follows:
?(w
ij
) = p?
1
e??1wij + (1 ? p)?
2
e??2wij
for j ? {2, . . . , n
i
}. p and (1 ? p) denote respec-
tively, the probabilities of membership for the first
and the second exponential distribution.
There are a few boundary conditions that the
model is expected to handle. We take each of these
cases and discuss them briefly:
3.2.1 First occurrence
The model treats the first occurrence of a term dif-
ferently from the other gaps. The second exponen-
tial component measuring burstiness does not fea-
ture in it. Hence the distribution is:
?
1
(w
i1
) = ?
1
e??1wi1
3.2.2 Censoring
Here we discuss the modeling of two cases that
require special attention, corresponding to gaps that
have a minimum length but whose actual length is
unknown. These cases are:
? The last occurrence of a term in a document.
? The term does not occur in a document at all.
We follow a standard technique from clinical tri-
als, where a patient is observed for a certain amount
of time and the observation of the study is expected
in that time period (the observation might be the
time until death, for example). In some cases it hap-
pens that the observation for a patient does not occur
in that time period. In such a case it is assumed that
the observation would occur at sometime in the fu-
ture. This is called censoring at a certain point.
50
In our case, we assume the particular term would
eventually occur, but the document has ended before
it occurs so we do not observe it. In our notation we
observe the term n
i
times, so the (n
i
+ 1)
th time the
term occurs is after the end of the document. Hence
the distribution of w
in
i
+1
is censored at length cen
i
.
If cen
i
is small, so that the nth
i
occurrence of the
term is near the end of the document, then it is not
surprising that w
in
i
+1
is censored. In contrast if cen
i
is large, so the nth
i
occurrence is far from the end
of the document, then either it is surprising that the
term did not re-occur, or it suggests the term is rare.
The information about the model parameters that is
given by the censored occurrence is,
Pr(w
in
i
+1
> cen
i
) =
?
?
cen
i
?(x)dx
= pe??1ceni + (1 ? p)e??2ceni ; where,
cen
i
= d
i
?
n
i
?
j=1
w
ij
Also when a particular term does not occur in a
document, our model assumes that the term would
eventually occur had the document continued indef-
initely. In this case the first occurrence is censored
and censoring takes place at the document length. If
a term does not occur in a long document, it suggests
the term is rare.
3.3 Bayesian formulation
Our modeling is based on a Bayesian approach (Gel-
man et al, 1995). The Bayesian approach differs
from the traditional frequentist approach. In the fre-
quentist approach it is assumed that the parameters
of a distribution are constant and the data varies.
In the Bayesian approach one can assign distrib-
utions to the parameters in a model. We choose
non-informative priors, as is common practice in
Bayesian applications. So we put,
p ? Uniform(0, 1), and
?
1
? Uniform(0, 1)
To tell the model that ?
2
is the larger of the two ?s,
we put ?
2
= ?
1
+ ?, where ? > 0, and
? ? Uniform(0, 1)
Also cen
i
depends on the document length d
i
and
the number of occurrences of the term in that doc-
ument, n
i
. Fitting mixture techniques is tricky and
Figure 2: Bayesian dependencies between the para-
meters
requires special methods. We use data augmenta-
tion to make it feasible to fit the model using Gibbs
Sampling (section 4.2). For details about this, see
Robert (1996) who describes in detail the fitting of
mixture models in MCMC methods (section 4.2).
4 Parameter Estimation
4.1 Bayesian Estimation
In the Bayesian approach of parameter estimation,
the parameters are uncertain, and it is assumed that
they follow some distribution. In our case the para-
meters and the data are defined as:
~
? = {p, ?
1
, ?
2
} denote the parameters of the model.
~W = {w
i1
, . . . , w
in
i
, w
in
i
+1
} denotes the data.
Hence based on this we may define the following:
? f(~?) is the prior distribution of ~? as assigned
in section 3.3. It summarizes everything we
know about ~? apart from the data ~W .
? f( ~W |~?) is the likelihood function. It is our
model for the data ~W conditional on the para-
meters ~?. (As well as the observed data, the
likelihood also conveys the information given
by the censored values)
? f(~?| ~W ) is the posterior distribution of ~?,
given ~W . It describes our beliefs about the pa-
rameters given the information we have.
51
Deriving the density function for a parameter set ~?
after observing data ~W , can be achieved by using
Bayes Theorem as:
f(~?| ~W ) =
f( ~W |~?)f(~?)
f( ~W )
(1)
where f( ~W ) is simply a normalizing constant, inde-
pendent of ~?. It can be computed in terms of the
likelihood and prior as:
f( ~W ) =
?
f( ~W |~?)f(~?)d~?
Hence equation 1 is reduced to:
f(~?| ~W ) ? f( ~W |~?)f(~?)
So, once we have specified the posterior density
function f(~?| ~W ), we can obtain the estimates of the
parameters ~? by simply averaging the values gener-
ated by f(~?| ~W ).
4.2 Gibbs Sampling
The density function of ?
i
, f(?
i
|
~W ) can be ob-
tained by integrating f(~?| ~W ) over the remaining
parameters of ~?. But in many cases, as in ours, it is
impossible to find a closed form solution of f(?
i
).
In such cases we may use a simulation process
based on random numbers, Markov Chain Monte
Carlo (MCMC) (Gilks et al, 1996). By generating
a large sample of observations from the joint distri-
bution f(~?, ~W ), the integrals of the complex dis-
tributions can be approximated from the generated
data. The values are generated based on the Markov
chain assumption, which states that the next gener-
ated value only depends on the present value and
does not depend on the values previous to it. Based
on mild regularity conditions, the chain will gradu-
ally forget its initial starting point and will eventu-
ally converge to a unique stationary distribution.
Gibbs Sampling (Gilks et al, 1996) is a popular
method used for MCMC analysis. It provides an ele-
gant way for sampling from the joint distributions of
multiple variables: sample repeatedly from the dis-
tributions of one-dimensional conditionals given the
current observations. Initial random values are as-
signed to each of the parameters. And then these val-
ues are updated iteratively based on the joint distri-
bution, until the values settle down and converge to
a stationary distribution. The values generated from
the start to the point where the chain settles down are
discarded and are called the burn-in values. The pa-
rameter estimates are based on the values generated
thereafter.
5 Results
Parameter estimation was carried out using Gibb?s
Sampling on the WinBUGS software (Spiegelhalter
et al, 2003). Values from the first 1000 iteration
were discarded as burn-in. It had been observed that
in most cases the chain reached the stationary distri-
bution well within 1000 iterations. A further 5000 it-
erations were run to obtain the parameter estimates.
5.1 Interpretation of Parameters
The parameters of the model can be interpreted in
the following manner:
?
??
1
= 1/?
1
is the mean of an exponential dis-
tribution with parameter ?
1
.
??
1
measures the
rate at which this term is expected in a running
text corpus. ??
1
determines the rarity of a term
in a corpus, as it is the average gap at which
the term occurs if it has not occurred recently.
Thus, a large value of ??
1
tells us that the term
is very rare in the corpus and vice-versa.
? Similarly, ??
2
measures the within-document
burstiness, i.e. the rate of occurrence of a term
given that it has occurred recently. It measures
the term re-occurrence rate in a burst within
a document. Small values of ??
2
indicate the
bursty nature of the term.
? p? and 1 ? p? denote, respectively, the probabil-
ities of the term occurring with rate??
1
and ??
2
in the entire corpus.
Table 1 presents some heuristics for drawing in-
ference based on the values of the parameter esti-
mates.
5.2 Data
We choose for evaluation, terms from the Associ-
ated Press (AP) newswire articles, as this is a stan-
dard corpus for language research. We picked terms
which had been used previously in the literature
(Church and Gale, 1995a; Church, 2000; Manning
52
?
1
small ?
1
large

?
2
small frequently occur-
ring and common
function word
topical content
word occurring in
bursts

?
2
large comparatively
frequent but well-
spaced function
word
infrequent and scat-
tered function word
Table 1: Heuristics for inference, based on the para-
meter estimates.
and Schu?tze, 1999; Umemura and Church, 2000)
with respect to modeling different distribution, so as
to present a comparative picture. For building the
model we randomly selected 1% of the documents
from the corpus, as the software (Spiegelhalter et al,
2003) we used is Windows PC based and could not
handle enormous volume of data with our available
hardware resources. As stated earlier, our model can
handle both frequent function terms and rare content
terms. We chose terms suitable for demonstrating
this. We also used some medium frequency terms to
demonstrate their characteristics.
5.3 Parameter estimates
Table 2 shows the parameter estimates for the cho-
sen terms. The table does not show the values of
1 ? p? as they can be obtained from the value of p?. It
has been observed that the value??
1
/??
2
is a good in-
dicator of the nature of terms, hence the rows in the
table containing terms are sorted on the basis of that
value. The table is divided into three parts. The top
part contains very frequent (function) words. The
second part contains terms in the medium frequency
range. And the bottom part contains rarely occurring
and content terms.
5.4 Discussion
The top part of the table consists of the very fre-
quently occurring function words occurring fre-
quently throughout the corpus. These statements are
supported by the low values of ??
1
and ??
2
. These
values are quite close, indicating that the occurrence
of these terms shows low burstiness in a running text
chunk. This supports our heuristics about the value
of ??
1
/??
2
, which is small for such terms. Moder-
ate, not very high values of p? also support this state-
ment, as the term is then quite likely to be gener-
Term p ?
1

?
2

?
1
/

?
2
the 0.82 16.54 16.08 1.03
and 0.46 46.86 45.19 1.04
of 0.58 38.85 37.22 1.04
except 0.67 21551.72 8496.18 2.54
follows 0.56 80000.00 30330.60 2.64
yet 0.51 10789.81 3846.15 2.81
he 0.51 296.12 48.22 6.14
said 0.03 895.26 69.06 12.96
government 0.60 1975.50 134.34 14.71
somewhat 0.84 75244.54 4349.72 17.30
federal 0.84 2334.27 102.57 22.76
here 0.94 3442.34 110.63 31.12
she 0.73 1696.35 41.41 40.97
george 0.88 17379.21 323.73 53.68
bush 0.71 3844.68 53.48 71.90
soviet 0.71 4496.40 59.74 75.27
kennedy 0.78 14641.29 99.11 147.73
church 0.92 11291.78 70.13 161.02
book 0.92 17143.84 79.68 215.16
vietnam 0.92 32701.11 97.66 334.86
boycott 0.98 105630.08 110.56 955.42
noriega 0.91 86281.28 56.88 1516.82
Table 2: Parameter estimates of the model for some
selected terms, sorted by the??
1
/??
2
value
ated from either of the exponential distributions (the
has high value of p?, but since the values of ? are
so close, it doesn?t really matter which distribution
generated the observation). We observe compara-
tively larger values of ??
1
for terms like yet, follows
and except since they have some dependence on the
document topic. One may claim that these are some
outliers having large values of both??
1
and ??
2
. The
large value of??
1
can be explained, as these terms are
rarely occurring function words in the corpus. They
do not occur in bursts and their occurrences are scat-
tered, so values of??
2
are also large (Table 1). Inter-
estingly, based on our heuristics these large values
nullify each other to obtain a small value of??
1
/??
2
.
But since these cases are exceptional, they find their
place on the boundary region of the division.
The second part of the table contains mostly non-
topical content terms as defined in the literature
(Katz, 1996). They do not describe the main topic
of the document, but some useful aspects of the doc-
ument or a nearby topical term. Special attention
may be given to the term george, which describes
the topical term bush. In a document about George
Bush, the complete name is mentioned possibly only
once in the beginning and further references to it are
made using the word bush, leading to bush being as-
53
signed as a topical term, but not george. The term
government in the group refers to some newswire
article about some government in any state or any
country, future references to which are made us-
ing this term. Similarly the term federal is used
to make future references to the US Government.
As the words federal and government are used fre-
quently for referencing, they exhibit comparatively
small values of??
2
. We were surprised by the occur-
rence of terms like said, here and she in the second
group, as they are commonly considered as func-
tion words. Closer examination revealed the details.
Said has some dependence on the document genre,
with respect to the content and reporting style. The
data were based on newswire articles about impor-
tant people and events. It is true, though unfor-
tunate, that the majority of such people are male,
hence there are more articles about men than women
(he occurs 757, 301 times in 163, 884 documents as
the 13th most frequent term in the corpus, whereas
she occurs 164, 030 times in 48, 794 documents as
the 70th frequent term). This explains why he has
a smaller value of ??
1
than she. But the ??
2
values
for both of them are quite close, showing that they
have similar usage pattern. Again, newswire articles
are mostly about people and events, and rarely about
some location, referenced by the term here. This ex-
plains the large value of??
1
for here. Again, because
of its usage for referencing, it re-occurs frequently
while describing a particular location, leading to a
small value of??
2
. Possibly, in a collection of ?travel
documents?, here will have a smaller value of??
1
and
thus occur higher up in the list, which would allow
the model to be used for characterizing genre.
Terms in the third part, as expected, are topical
content terms. An occurrence of such a term de-
fines the topic or the main content word of the doc-
ument or the text chunk under consideration. These
terms are rare in the entire corpus, and only appear
in documents that are about this term, resulting in
very high values of ??
1
. Also low values of ??
2
for
these terms mean that repeat occurrences within the
same document are quite frequent; the characteris-
tic expected from a topical content term. Because of
these characteristics, based on our heuristics these
terms have very high values of??
1
/??
2
, and hence are
considered the most informative terms in the corpus.
5.5 Case Studies
Here we study selected terms based on our model.
These terms have been studied before by other re-
searchers. We study these terms to compare our
findings with previous work and also demonstrate
the range of inferences that may be derived from our
model.
5.5.1 somewhat vrs boycott
These terms occur an approximately equal num-
ber of times in the AP corpus, and inverse doc-
ument frequency was used to distinguish between
them (Church and Gale, 1995a). Our model also
gives approximately similar rates of occurrence (??
1
)
for these two terms as shown in Table 2. But the re-
occurrence rate, ??
2
, is 110.56 for boycott, which is
very small in comparison with the value of 4349.72
for somewhat. Hence based on this, our model as-
signs somewhat as a rare function word occurring in
a scattered manner over the entire corpus. Whereas
boycott is assigned as a topical content word, as it
should be.
5.5.2 follows vrs soviet
These terms were studied in connection with fit-
ting Poisson distributions to their term distribution
(Manning and Schu?tze, 1999), and hence determin-
ing their characteristics1 . In our model, follows has
large values of both ??
1
and ??
2
(Table 2), so that it
has the characteristics of a rare function word. But
soviet has a large??
1
value and a very small??
2
value,
so that it has the characteristics of a topical content
word. So the findings from our model agree with the
original work.
5.5.3 kennedy vrs except
Both these terms have nearly equal inverse doc-
ument frequency for the AP corpus (Church, 2000;
Umemura and Church, 2000) and will be assigned
equal weight. They used a method (Kwok, 1996)
based on average-term frequency to determine the
nature of the term. According to our model, the??
2
value of kennedy is very small as compared to that
for except. Hence using the??
1
/??
2
measure, we can
correctly identify kennedy as a topical content term
1The original study was based on the New York Times, ours
on the Associated Press corpus
54
and except as an infrequent function word. This is in
agreement with the findings of the original analysis.
5.5.4 noriega and said
These terms were studied in the context of an
adaptive language model to demonstrate the fact that
the probability of a repeat occurrence of a term in a
document defies the ?bag of words? independence
assumption (Church, 2000). The deviation from in-
dependence is greater for content terms like noriega
as compared to general terms like said. This can be
explained in the context of our model as said has
small values of??
1
and ??
2
, and their values are quite
close to each other (as compared to other terms, see
Table 2). Hence said is distributed more evenly in
the corpus than noriega. Therefore, noriega defies
the independence assumption to a much greater ex-
tent than said. Hence their findings (Church, 2000)
are well explained by our model.
6 Conclusion
In this paper we present a model for term re-
occurrence in text based on gaps between succes-
sive occurrences of a term in a document. Parameter
estimates based on this model reveal various charac-
teristics of term use in a collection. The model can
differentiate a term?s dependence on genre and col-
lection and we intend to investigate use of the model
for purposes like genre detection, corpus profiling,
authorship attribution, text classification, etc. The
proposed measure of ??
1
/??
2
can be appropriately
adopted as a means of feature selection that takes
into account the term?s occurrence pattern in a cor-
pus. We can capture both within-document bursti-
ness and rate of occurrence of a term in a single
model.
References
A. Bookstein and D.R Swanson. 1974. Probabilistic
models for automatic indexing. Journal of the Ameri-
can Society for Information Science, 25:312?318.
K. Church and W. Gale. 1995a. Inverse document fre-
quency (idf): A measure of deviation from poisson.
In Proceedings of the Third Workshop on Very Large
Corpora, pages 121?130.
K. Church and W. Gale. 1995b. Poisson mixtures. Nat-
ural Language Engineering, 1(2):163?190.
K. Church. 2000. Empirical estimates of adaptation: The
chance of two noriega?s is closer to p/2 than p2. In
COLING, pages 173?179.
Anne De Roeck, Avik Sarkar, and Paul H Garthwaite.
2004a. Defeating the homogeneity assumption. In
Proceedings of 7th International Conference on the
Statistical Analysis of Textual Data (JADT), pages
282?294.
Anne De Roeck, Avik Sarkar, and Paul H Garthwaite.
2004b. Frequent term distribution measures for
dataset profiling. In Proceedings of the 4th Interna-
tional conference of Language Resources and Evalua-
tion (LREC), pages 1647?1650.
Alexander Franz. 1997. Independence assumptions con-
sidered harmful. In Proceedings of the eighth confer-
ence on European chapter of the Association for Com-
putational Linguistics, pages 182?189.
A. Gelman, J. Carlin, H.S. Stern, and D.B. Rubin. 1995.
Bayesian Data Analysis. Chapman and Hall, London,
UK.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. 1996.
Markov Chain Monte Carlo in Practice. Interdisci-
plinary Statistics Series. Chapman and Hall, London,
UK.
Slava M. Katz. 1996. Distribution of content words and
phrases in text and language modelling. Natural Lan-
guage Engineering, 2(1):15?60.
A Kilgarriff. 1997. Using word frequency lists to mea-
sure corpus homogeneity and similarity between cor-
pora. In Proceedings of ACL-SIGDAT Workshop on
very large corpora, Hong Kong.
K. L. Kwok. 1996. A new method of weighting query
terms for ad-hoc retrieval. In SIGIR, pages 187?195.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Christian. P. Robert. 1996. Mixtures of distributions: in-
ference and estimation. In W.R. Gilks, S. Richardson,
and D.J. Spiegelhalter, editors, Markov Chain Monte
Carlo in Practice, pages 441?464.
D.J. Spiegelhalter, A. Thomas, N. G. Best, and D. Lunn.
2003. Winbugs: Windows version of bayesian infer-
ence using gibbs sampling, version 1.4.
K. Umemura and K. Church. 2000. Empirical term
weighting and expansion frequency. In Empirical
Methods in Natural Language Processing and Very
Large Corpora, pages 117?123.
55
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 113?118,
Prague, June 2007. c?2007 Association for Computational Linguistics
SVO triple based Latent Semantic Analysis for recognising textual
entailment
Gaston Burek Christian Pietsch
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, MK7 6AA, UK
{g.g.burek,c.pietsch,a.deroeck}@open.ac.uk
Anne De Roeck
Abstract
Latent Semantic Analysis has only recently
been applied to textual entailment recogni-
tion. However, these efforts have suffered
from inadequate bag of words vector repre-
sentations. Our prototype implementation
for the Third Recognising Textual Entail-
ment Challenge (RTE-3) improves the ap-
proach by applying it to vector represen-
tations that contain semi-structured repre-
sentations of words. It uses variable size
n-grams of word stems to model indepen-
dently verbs, subjects and objects displayed
in textual statements. The system perfor-
mance shows positive results and provides
insights about how to improve them further.
1 Introduction
The Third Recognising Textual Entailment Chal-
lenge (RTE-3) task consists in developing a system
for automatically determining whether or not a hy-
pothesis (H) can be inferred from a text (T), which
could be up to a paragraph long.
Our entry to the RTE-3 challenge is a system
that takes advantage of Latent Semantic Analysis
(LSA) (Landauer and Dumais, 1997). This numer-
ical method for reducing noise generated by word
choices within texts is extensively used for docu-
ment indexing and word sense disambiguation. Re-
cently, there have also been efforts to use techniques
from LSA to recognise textual entailment (Clarke,
2006; de Marneffe et al, 2006). However, we argue
that these efforts (like most LSA approaches in the
past) suffer from an inadequate vector representation
for textual contexts as bags of words. In contrast,
we have applied LSA to vector representations of
semi-structured text. Our representation takes into
account the grammatical role (i.e. subject, verb or
object) a word occurs in.
Within this system report, we describe and dis-
cuss our methodology in section 2, our current im-
plementation in section 3, and system results in sec-
tion 4. We conclude in section 5 with a discussion
of the results obtained and with the presentation of
possible steps to improve our system?s performance.
2 Methodology for detecting Textual
Entailment
2.1 Textual entailment formalisation
Our approach addresses the problem of the semantic
gap that exists between low level linguistic entities
(words) and concepts. Concepts can be described
by means of predicate-argument structures or by a
set of alternative natural language realisations. In
this work we use terminology co-occurrence infor-
mation to identify when different spans of text have
common semantic content even if they do not share
vocabulary. To achieve this, we use variable size n-
grams to independently model subject, verb and ob-
ject, and capture semantics derived from grammati-
cal structure. In order to detect textual entailment we
measure the semantic similarity between n-grams in
each T?H pair.
2.2 Using n-grams to align SVOs
To align subjects, verbs and objects within H and T,
we build the set of all n-grams for T, and do the same
113
for H. Section 3.5 describes this process in more de-
tail.
2.3 Deriving word senses with Latent Semantic
Analysis
Our approach is based on the assumption that a
word sense can be derived from the textual contexts
in which that words occurs. This assumption was
formalised in the Distributional Hypothesis (Harris,
1954).
We implemented a vector space model (Salton et
al., 1975) to capture word semantics from linguis-
tic (i.e. grammatical role) and contextual (i.e. fre-
quency) information about each word. To avoid high
matrix sparsity our vector space model uses second
order co-occurrence (Widdows, 2004, p. 174).
We assumed that the corpus we generated the vec-
tor space model from has a probabilistic word distri-
bution that is characterised by a number of seman-
tic dimensions. The LSA literature seems to agree
that optimal number of dimensions is somewhere
between two hundred and one thousand depending
on corpus and domain. As specified by LSA we ap-
plied Singular Value Decomposition (SVD) (Berry,
1992) to identify the characteristic semantic dimen-
sions.
The resulting model is a lower dimensional pro-
jection of the original model that captures indi-
rect associations between the vectors in the original
model. SVD reduces the noise in word categori-
sations by producing the best approximation to the
original vector space model.
3 Implementation
3.1 Development data set
The development data set consists of eight hundred
T?H pairs, half of them positive. By positive pair
we mean a T?H pair in which T entails H. All other
pairs we call negative. Each T?H pair belongs to a
particular sub-task. Those sub-tasks are Information
Extraction (IE), Information Retrieval (IR), Ques-
tion Answering (QA) and Summarisation (SUM). In
the current prototype, we ignored annotations about
sub-tasks.
3.2 Corpus analysis
3.2.1 Corpora used
The only knowledge source we used in our imple-
mentation was a parsed newswire corpus (Reuters
News Corpus) (Lewis et al, 2004). To derive con-
textual information about the meaning of words con-
stituting the SVOs, we analysed the Reuters corpus
as explained below.
3.2.2 SVO triple extraction
For parsing the corpus, we used Minipar1 because
of its speed and because its simple dependency triple
output format (-t option) contains word stems and
the grammatical relations between them. A simple
AWK script was used to convert the parse results into
Prolog facts, one file for each sentence. A straight-
forward Prolog program then identified SVOs in
each of these fact files, appending them to one big
structured text file.
Our algorithm currently recognises intransitive,
transitive, ditransitive, and predicative clauses. In-
transitive clauses are encoded as SVOs with an
empty object slot. Transitive clauses result in a fully
instantiated SVOs. Ditransitive clauses are encoded
as two different SVOs: the first containing subject,
verb and direct object; the second triple containing
the subject again, an empty verb slot, and the indi-
rect object. Predicatives (e.g. ?somebody is some-
thing?) are encoded just like transitive clauses.
In this first prototype, we only used one word
(which could be a multi-word expression) for sub-
ject, verb and object slot respectively. We realise
that this approach ignores much information, but
given a large corpus, it might not be detrimental to
be selective.
3.2.3 SVO Stemming and labeling
To reduce the dimensionality of our vector space
model we stem the SVOs using Snowball2. Then,
we calculate how many times stems co-occur as sub-
ject, verb or object with another stem within the
same SVO instance.
1Minipar can be downloaded from http://www.cs.ualberta.
ca/?lindek/minipar.htm. It is based on Principar, which is de-
scribed in Lin (1994).
2Snowball is freely available from http://snowball.tartarus.
org/. The English version is based on the original Porter Stem-
mer (Porter, 1980).
114
To keep track of the grammatical role (i.e. subject,
verb, object) of the words we stem them and label
the stems with the corresponding role.
3.3 Building vector spaces to represent stem
semantics
From the corpus, we built a model (S,V,O) of the
English (news) language consisting of three matri-
ces: S for subjects, V for verbs, and O for objects.
We built the three stem-to-stem matrices from
labeled stem co-occurrences within the extracted
triples. The entries to the matrices are the frequen-
cies of the co-occurrence of each labeled stem with
itself or with another labeled stem. In our current
prototype, due to technical restrictions explained in
Section 3.4, each matrix has 1000 rows and 5000
columns.
Columns of matrix S contain entries for stems la-
beled as subject, columns of matrix V contain en-
tries for stems labeled as verb, and columns of ma-
trix O contain entries for stems labeled as object.
The frequency entries of each matrix correspond to
the set of identically labeled stems with the highest
frequency.
Rows of the three matrices contain entries corre-
sponding to the same set of labeled stems. Those la-
beled stems are the ones with the highest frequency
in the set of all labeled stems. Of these, 347 stems
are labeled as subject, 356 are labeled as verb, and
297 are labeled as object. Each row entry is the fre-
quency of co-occurrence of two labeled stems within
the same triple.
Finally, each column entry is divided by the num-
ber of times the labeled stem associated with that
column occurs within all triples.
3.4 Calculating the singular value
decomposition
We calculated the Singular Value Decompositions
(SVDs) for S, V and O. Each SVD of a matrix A is
defined as a product of three matrices:
A = U ? S ? V ? (1)
SVD is a standard matrix operation which is sup-
ported by many programming libraries and com-
puter algebra applications. The problem is that only
very few can handle the large matrices required for
real-world LSA. It is easy to see that the memory re-
quired for representing a full matrix of 64 bit float-
ing point values can easily exceed what current hard-
ware offers. Fortunately, our matrices are sparse, so
a library with sparse matrix support should be able
to cope. Unfortunately, these are hard to find out-
side the Fortran world. We failed to find any Java li-
brary that can perform SVD on sparse matrices.3 We
finally decided to use SVDLIBC, a modernised ver-
sion of SVDPACKC using only the LAS2 algorithm.
In pre-tests with a matrix derived from a different
text corpus (18371 rows ? 3469 columns, density
0.73%), it completed the SVD task within ten min-
utes on typical current hardware. However, when
we try to use it for this task on a matrix S of dimen-
sion 5000 ? 5000 (density 1.4%), SVDLIBC did not
terminate4. In theory, there is a Singular Value De-
composition for every given matrix, so we assume
this is an implementation flaw in either SVDLIBC or
GCC. With no time left to try Fortran alternatives,
we resorted to reducing the size of our three matri-
ces to 1000 ? 5000, thus losing much information
in our language model.
3.5 Looking for evidence of H in T using
variable size n-grams
3.5.1 Building variable size n-grams
OurMinipar triple extraction algorithm is not able
to handle SVOs that are embedded within other
SVOs (as e.g. in ?Our children play a new game that
involves three teams competing for a ball.?). There-
fore, in order to determine if SVOs displayed in H
are semantically similar to any of those displayed in
T, we generate all n-grams of all lengths for each T
and H: one set for subjects, one for verbs and another
one for objects.
Example: ?The boy played tennis.?
Derived n-grams: the; the boy; the boy played; the
boy played tennis; boy; boy played; boy played
3The popular JAMA library and the related Jampack library
have no sparse matrix support at all. MTJ and Colt do support
sparse matrices but cannot perform SVD on them without first
converting them to full matrices.
4We tried various hardware / operating system / compiler
combinations. On Linux systems, SVDLIBC would abort after
about 15 minutes with an error message ?imtqlb failed to con-
verge?. On Solaris and Mac OS X systems, the process would
not terminate within several days.
115
tennis; played; played tennis; tennis.
We use n-grams to model subjects, verbs and ob-
jects of SVOs within T and H.
3.5.2 How to compare n-grams
We generate three vector representations for each
n-gram. To do this, we add up columns from the
Reuters Corpus derived matrices. To build the first
vector representation, we use the S matrix, to build
the second vector we use the V matrix, and to build
the third vector we use the O matrix. Each of
the three representations is the result of adding the
columns corresponding to each stem within the n-
gram.
To calculate the semantic similarity between n-
grams, we fold the three vector representations of
each n-gram into one of the dimensionally reduced
matrices S200, V200 or O200. Vector representation
originating from the S matrix are folded into S200.
We proceed analogously for vector representations
originating from V200 and O200. We apply equation
2 to fold vectors from Gr where r ? {S,V,O}. G
is a matrix which consists of all the vector represen-
tations of all the n-grams modeling T or H. Sr200 and
U r200 are SVD results reduced to 200 dimensions.
Gr? ? U r200 ? (S
r
200)
?1 = Gr200 (2)
For each T?H pair we calculate the dot product
between the G matrices for H and T as expressed in
equation 3
textGr200 ?
hypothesis Gr?200 = O
r (3)
The resulting matrix Or contains the dot product
similarity between all pairs of n-grams within the
same set. Finally, for each T?H pair we obtain three
similarity values s, v, o by selecting the entry of Or
with the highest value.
3.5.3 Scoring
Now we have calculated almost everything we
need to venture a guess about textual entailment.
For each T?H pair, we have three scores s, v, o
for for subject, verb and object slot respectively. The
remaining task is to combine them in a meaningful
way in order to make a decision. This requires some
amount of training which in the current prototype
is as simple as computing six average values: s?p,
s? v? o?
positive 0.244 5.05 ? 10?7 0.323
negative 0.196 4.76 ? 10?7 0.277
Table 1: Values computed for s?p, v?p, o?p, s?n, v?n, o?n
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8 0.9
 0  100  200  300  400  500  600  700  800
d
o
t
 
p
r
o
d
u
c
t
 
s
i
m
i
l
a
r
i
t
y
n-grams
Figure 1: Subject similarities s = maxOS for all
H?T pairs
v?p, o?p are the average scores of subject, verb and
object slots over those T?H pairs for which textual
entailment is known to hold. Conversely, s?n, v?n, o?n
are the averages for those pairs that do not stand in
a textual entailment relation. The (rounded) values
were determined are shown in table 1.
Note that the average values for non-entailment
are always lower than the average values for entail-
ment, which indicates that our system indeed tends
to discriminate correctly between these cases.
The very low values for the verb similarities (fig-
ure 3) compared to subject similarities (figure 1) and
object similarities (figure 2) remind us that before
we can combine slot scores, they should be scaled
to a comparable level. This is achieved by divid-
ing each slot score by its corresponding average. Ig-
noring the difference between positive and negative
pairs for a moment, the basic idea of our scoring al-
gorithm is to use the following threshold:
s
s?
+
v
v?
+
o
o?
= 3 (4)
 0 0.1
 0.2 0.3
 0.4 0.5
 0.6 0.7
 0.8
 0  100  200  300  400  500  600  700  800
d
o
t
 
p
r
o
d
u
c
t
 
s
i
m
i
l
a
r
i
t
y
n-grams
Figure 2: Object similarities o = maxOO for all
H?T pairs
116
 0 2e-07
 4e-07 6e-07
 8e-07 1e-06
 1.2e-06 1.4e-06
 1.6e-06
 0  100  200  300  400  500  600  700  800
d
o
t
 
p
r
o
d
u
c
t
 
s
i
m
i
l
a
r
i
t
y
n-grams
Figure 3: Verb similarities v = maxOV for all H?T
pairs
At this point we observed that scaling the verb
similarities so much seemed to make results worse.
It seems to be necessary to introduce weights:
?
s
s?
+ ?
v
v?
+ ?
o
o?
= ? + ?+ ? (5)
Without loss of generality, we may assume ? = 1:
?
s
s?
+
v
v?
+ ?
o
o?
= ? + 1 + ? (6)
The complete scoring formula with both positive
and negative scores is shown below. We assumed
that the weights ? and ? are the same in the positive
and in the negative case, so ? = ?p = ?n and ? =
?p = ?n.
?
s
s?p
+
v
v?p
+?
o
o?p
+?
s
s?n
+
v
v?n
+?
o
o?n
= 2(?+1+?)
(7)
At this point, some machine learning over the de-
velopment data set should be performed in order to
determine optimal values for ? and ?. For lack of
time, we simply performed a dozen or so of test runs
and finally set ? = ? = 3.
Our entailment threshold is thus simplified:
3
s
s?p
+
v
v?p
+ 3
o
o?p
+ 3
s
s?n
+
v
v?n
+ 3
o
o?n
= q (8)
If q > 14, our prototype predicts textual entail-
ment. Otherwise, it predicts non-entailment.
4 Results
Using the scoring function described in section
3.5.3, our system achieved an overall accuracy of
0.5638 on the development dataset. Table 2 shows
results for the system run on the test dataset. On this
unseen dataset, the overall accuracy decreased only
all IE IR QA SUM
accuracy 0.5500 0.4950 0.5750 0.5550 0.5750
av. prec. 0.5514 0.4929 0.5108 0.5842 0.6104
Table 2: Results on the test set
slightly to 0.5500. We take this as a strong indica-
tion that the thresholds we derived from the develop-
ment dataset work well on other comparable input.
Results show that our system has performed signifi-
cantly above the 0.5 baseline that would result from
a random decision.
As shown in section 3.5.3, the values in the three
similarity plots (see figures 1, 2 and 3) obtained with
the development set seem to be scattered around the
means. Therefore it seems that the threshold values
used to the decide whether or not T entails H do not
fully reflect the semantics underlying textual entail-
ment.
The nature of the SVD calculations do not allow
us directly to observe the performance of the vari-
able size n-grams in independently aligning subject,
verb and objects from T and from H. Nevertheless
we can infer from figures 1, 2 and 3 that many of
the values shown seem to be repeated. These value
configurations can be observed in the three horizon-
tal lines. These lines better visible in figures 2 and
3 are the effect of (a) many empty vectors resulting
from the rather low number of stems represented by
columns in our Reuters-derived matrices S, V and
O, and (b) the effect of folding the n-gram vector
representations into reduced matrices with two hun-
dred dimensions.
5 Conclusion
Even though our system was developed from scratch
in a very short period of time, it has already out-
performed other LSA-based approaches to recognis-
ing textual entailment (Clarke, 2006), showing that
it is both feasible and desirable to move away from
a bag-of-words semantic representation to a semi-
structured (here, SVO) semantic representation even
when using LSA techniques.
Our system displays several shortcomings and
limitations owing to its immature implementation
state. These will be addressed in future work, and
we are confident that without changing its theoret-
ical basis, this will improve performance dramati-
117
cally. Envisaged changes include:
? using larger matrices as input to SVD
? using the complete Reuters corpus, and adding
Wikinews texts
? performing corpus look-up for unknown words
? extracting larger chunks from S and O slots
? using advanced data analysis and machine
learning techniques to improve our scoring
function
In addition, our approach currently does not take
into consideration the directionality of the entail-
ment relationship between the two text fragments. In
cases where T1 entails T2 but T2 does not entail T1,
our approach will treat (T1, T2) and (T2, T1) as the
same pair. We expect to correct this misrepresenta-
tion by evaluating the degree of specificity of words
composing the SVOs in asymmetric entailment rela-
tionships where the first text fragment is more gen-
eral than the second one. For that purpose, one can
use term frequencies as an indicator of specificity
(Spa?rck Jones, 1972).
Obviously, system performance could be further
improved by taking a hybrid approach as e.g. in de
Marneffe et al (2006), but we find it more instruc-
tive to take our pure LSA approach to its limits first.
6 Acknowledgements
We are grateful to Prof. Donia Scott, head of the Nat-
ural Language Generation group within the Centre
for Research in Computing of the Open University,
who made us aware of the RTE-3 Challenge and en-
couraged us to participate.
References
[Berry1992] M. W. Berry. 1992. Large-scale sparse sin-
gular value computations. The International Journal
of Supercomputer Applications, 6(1):13?49, Spring.
[Clarke2006] Daoud Clarke. 2006. Meaning as context
and subsequence analysis for entailment. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
[de Marneffe et al2006] Marie-Catherine de Marneffe,
Bill MacCartney, Trond Grenager, Daniel Cer, Anna
Rafferty, and Christopher D. Manning. 2006. Learn-
ing to distinguish valid textual entailments. In Pro-
ceedings of the Second PASCAL Challenges Workshop
on Recognising Textual Entailment, Venice, Italy.
[Harris1954] Zelig S. Harris. 1954. Distributional struc-
ture. WORD, 10:146?162. Reprinted in J. Fodor and J.
Katz, The structure of language: Readings in the phi-
losophy of language, pp. 33?49, Prentice-Hall, 1964.
[Landauer and Dumais1997] T. K. Landauer and S. T. Du-
mais. 1997. A solution to Plato?s Problem. The Latent
Semantic Analysis theory of the acquisition, induction
and representation of knowledge. Psychological Re-
view, 104(2):211?240.
[Lewis et al2004] D. D. Lewis, Y. Yang, T. Rose, and
F. Li. 2004. RCV1: A new benchmark collection
for text categorization research. Journal of Machine
Learning Research, 5:361?397.
[Lin1994] Dekang Lin. 1994. PRINCIPAR ? an effi-
cient, broad-coverage, principle-based parser. In Proc.
COLING-94, pages 42?488, Kyoto, Japan.
[Porter1980] M. F. Porter. 1980. An algorithm for suffix
stripping. Program, 14(3):130?137.
[Salton et al1975] G. Salton, A. Wong, and C. S. Yang.
1975. A vector space model for automatic indexing.
Commun. ACM, 18(11):613?620.
[Spa?rck Jones1972] Karen Spa?rck Jones. 1972. A statis-
tical interpretation of term specificity and its applica-
tion in retrieval. Journal of Documentation, 28(1):11?
21. Reprinted 2004 in 60(5):493?502 and in Spa?rck
Jones (1988).
[Spa?rck Jones1988] Karen Spa?rck Jones. 1988. A statis-
tical interpretation of term specificity and its applica-
tion in retrieval. Document retrieval systems, pages
132?142.
[Widdows2004] DominicWiddows. 2004. Geometry and
Meaning. Number 172 in CSLI Lecture Notes. Uni-
versity of Chicago Press.
118
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1218?1226,
Beijing, August 2010
A Methodology for Automatic Identification of Nocuous Ambiguity 
 
Hui Yang1            Anne de Roeck1            Alistair Willis1            Bashar Nuseibeh1, 2 
1Department of Computing, The Open University 
2Lero, University of Limerick 
 {h.yang, a.deroeck, a.g.willis, b.nuseibeh}@open.ac.uk 
 
Abstract 
Nocuous ambiguity occurs when a lin-
guistic expression is interpreted differ-
ently by different readers in a given con-
text. We present an approach to auto-
matically identify nocuous ambiguity 
that is likely to lead to misunderstand-
ings among readers. Our model is built 
on a machine learning architecture. It 
learns from a set of heuristics each of 
which predicts a factor that may lead a 
reader to favor a particular interpretation. 
An ambiguity threshold indicates the ex-
tent to which ambiguity can be tolerated 
in the application domain. Collections of 
human judgments are used to train heu-
ristics and set ambiguity thresholds, and 
for evaluation. We report results from 
applying the methodology to coordina-
tion and anaphora ambiguity. Results 
show that the method can identify nocu-
ous ambiguity in text, and may be wid-
ened to cover further types of ambiguity. 
We discuss approaches to evaluation. 
1 Introduction 
Traditional accounts of ambiguity have generally 
assumed that each use of a linguistic expression 
has a unique intended interpretation in context, 
and attempted to develop a model to determine it 
(Nakov and Hearst, 2005; Brill and Resnik, 
1994). However, disambiguation is not always 
appropriate or even desirable (Poesio and Art-
stein, 2008). Ambiguous text may be interpreted 
differently by different readers, with no consen-
sus about which reading is the intended one. At-
tempting to assign a preferred interpretation may 
therefore be inappropriate. Misunderstandings 
among readers do occur and may have undesir-
able consequences. In requirements engineering 
processes, for example, this results in costly im-
plementation errors (Boyd et al, 2005).  
Nonetheless, most text does not lead to sig-
nificant misinterpretation. Our research aims to 
establish a model that estimates how likely an 
ambiguity is to lead to misunderstandings. Our 
previous work on nocuous ambiguity (Chantree 
et al, 2006; Willis et al, 2008) cast ambiguity 
not as a property of a text, but as a property of 
text in relation to a set of stakeholders. We drew 
on human judgments - interpretations held by a 
group of readers of a text ? to establish criteria 
for judging the presence of nocuous ambiguity. 
An ambiguity is innocuous if it is read in the 
same way by different people, and nocuous oth-
erwise. The model was tested on co-ordination 
ambiguity only. 
In this paper, we implement, refine and extend 
the model. We investigate two typical ambiguity 
types arising from coordination and anaphora. 
We extend the previous work (Willis et al, 
2008) with additional heuristics, and refine the 
concept of ambiguity threshold. We experiment 
with alternative machine learning algorithms to 
find optimal ways of combining the output of the 
heuristics. Yang et al (2010a) describes a com-
plete implementation in a prototype tool running 
on full text. Here we present our experimental 
results, to illustrate and evaluate the extended 
methodology. 
The rest of the paper is structured as follows. 
Section 2 introduces the methodology for auto-
matic detection of nocuous ambiguity. Sections 
3 and 4 provide details on how the model is ap-
plied to coordination and anaphora ambiguity. 
Experimental setup and results are reported in 
Section 5, and discussed in Section 6. Section 7 
reports on related work. Conclusions and future 
work are found in Section 8.          
1218
2 Methodology for Nocuous Ambiguity 
Identification 
This section describes the main ideas underpin-
ning our model of ambiguity. We distinguish 
between structural and interpretative aspects. 
The former captures the fact that text may have 
structure (i.e. syntax) which, in principle, per-
mits multiple readings. These are relatively 
straightforward to identify from the linguistic 
constructs present in the text. The latter ac-
knowledges that if text is interpreted in the same 
way by different readers, it has a low risk of be-
ing misunderstood. Modelling interpretive as-
pects requires access to human judgments about 
texts. Our approach has three elements, which 
we describe in turn: collection of human judg-
ments; heuristics that model those judgments, 
and a machine learning component to train the 
heuristics.  
 
Human judgments. We define an ambiguity as 
nocuous if it gives rise to diverging interpreta-
tions. Wasow et al (2003) suggests that ambigu-
ity is always a product of the meaning that peo-
ple assign to language, and thus a subjective 
phenomenon. We capture individual interpreta-
tions of instances of ambiguity by surveying par-
ticipants, asking them for their interpretation. 
We use this information to decide whether, 
given some ambiguity threshold, a particular 
instance is seen as innocuous or nocuous de-
pending on the degree of dissent between judges. 
A key concept in determining when ambiguity 
is nocuous is the ambiguity threshold. Different 
application areas may need to be more or less 
tolerant of ambiguity (Poesio and Artstein, 2008). 
For instance, requirements documents describing 
safety critical systems should seek to avoid mis-
understandings between stakeholders. Other 
cases, such as cookbooks, could be less sensitive. 
Willis et al (2008)?s general concept of ambigu-
ity threshold sought to implement a flexible tol-
erance level to nocuous ambiguity. Given an 
instance of ambiguous text, and a set of judg-
ments as to the correct interpretation, the cer-
tainty of an interpretation is the percentage of 
readers who assign that interpretation to the text. 
For example, in Table 1 below (sec. 3.1), the 
certainty of the two interpretations, HA and LA 
of expression (a) are 12/17=71% and 1/17=5.9% 
respectively. Here, an expression shows nocuous 
ambiguity if none of the possible interpretations 
have a certainty exceeding the chosen threshold. 
Later in this section, we will describe further 
experiments with alternative, finer grained ap-
proaches to setting and measuring thresholds, 
that affect the classifier?s behaviour. 
 
Heuristics. Heuristics capture factors that may 
favour specific interpretations. Each heuristic 
embodies a hypothesis, drawn from the literature, 
about a linguistic phenomenon signifying a pre-
ferred reading. Some use statistical information 
(e.g., word distribution information obtained 
from a generic corpus, the BNC 1 , using the 
Sketch Engine2). Others flag the presence of sur-
face features in the text, or draw on semantic or 
world knowledge extracted from linguistic re-
sources like WordNet3 or VerbNet4. 
 
Machine learning (ML). Individual heuristics 
have limited predictive power: their effective-
ness lies in their ability to operate in concert. 
Importantly, the information they encapsulate 
may be interdependent. We harness this by using 
ML techniques to combine the outputs of indi-
vidual heuristics. ML is an established method 
for recognizing complex patterns automatically, 
making intelligent decisions based on empirical 
data, and learning of complex and nonlinear re-
lations between data points. Our model uses su-
pervised learning ML techniques, deducing a 
function from training data, to classify instances 
of ambiguity into nocuous or innocuous cases. 
The classifier training data consists of pairs of 
input objects (i.e. vectors made up of heuristics 
scores) and desired outputs (i.e. the class labels 
determined by the distribution of human judg-
ments as captured by thresholds). To select an 
appropriate ML algorithm for the nocuity classi-
fier, we tested our datasets (described in later 
sections) on several algorithms in the WEKA5 
package (e.g., decision tree, J48, Naive Bayes, 
SVM, Logistic Regression, LogitBoost, etc.)  
To train, and validate, a nocuity classifier for 
a particular form of ambiguity, we build a data-
set of judgments, and select heuristics that model 
                                                 
1
 http://www.natcorp.ox.ac.uk/ 
2
 http://sketchengine.co.uk/ 
3
 http://wordnet.princeton.edu/ 
4
 http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 
5
 http://www.cs.waikato.ac.nz/~ml/index.html 
1219
the information underlying the human judge-
ments about a preferred interpretation.  
We validated the approach on two forms of 
ambiguity. Sections 3 and 4 discuss how the 
methodology is applied to forms of coordination 
and anaphoric ambiguity, and evaluate the per-
formance of the final classifiers.                       
3 Automatic Identification of Nocuous 
Coordination Ambiguity 
Our previous work on nocuous ambiguity has 
focused on coordination ambiguity: a common 
kind of structural ambiguity. A coordination 
structure connects two words, phrases, or clauses 
together via a coordination conjunction (e.g., 
?and?, ?or?, etc) as in the following examples:  
 
(1) They support a typing system for architec-
tural components and connectors.  
(2) It might be rejected or flagged for further 
processing. 
 
     In (1), the coordination construction ?architec-
tural components and connectors? consists of a 
near conjunct (NC) (i.e. ?components?), a far 
conjunct (FC) (i.e. ?connectors?), and the at-
tached modifier (M) (i.e. ?architectural?). This 
construction allows two bracketings correspond-
ing to high modifier attachment ([architectural 
[components and connectors]]) or low modifier 
attachement ([[architectural components] and 
connector]). Our aim is to refine Chantree et al
(2006) and Willis et al(2008), hence our focus is 
on the two phenomena they treated: modification 
in noun phrase coordination (as in (1)) and in 
verb phrase coordination (as in (2)).   
     We implemented the heuristics described in 
the earlier work, and introduced two further ones 
(local document collocation frequency, and se-
mantic similarity). We used the Chantree et al
(2006) dataset of human judgments, but em-
ployed the LogitBoost algorithm for implement-
ing the nocuity classifier (rather than the Logis-
tic Regression equation). The following subsec-
tions give more detail. 
3.1 Building a dataset 
Coordination instances. Our dataset was col-
lected and described by Chantree et al (2006). It 
contains 138 coordination instances gathered 
from a set of requirement documents. Noun 
compound conjunctions account for the majority 
(85.5%) of cases (118 instances). Nearly half of 
these arose as a result of noun modifiers, while 
there are 36 cases with adjective and 18 with 
preposition modifiers. 
 
Human judgment collection. The coordination 
instances containing potential ambiguity were 
presented to a group of 17 computing profes-
sionals including academic staff or research stu-
dents. For each instance, the judges were asked 
to select one of three options: high modifier at-
tachment (HA), low modifier attachment (LA), 
or ambiguous (A). Table 1 shows the judgment 
count for two sample instances. In instance (a) in 
table 1, the certainty of HA is 12/17=71%, and 
the certainty of LA is 1/17=6%. Instance (b) was 
judged mainly to be ambiguous.  
 
 
 Judgments 
 HA LA A 
(a) security and privacy requirements 12 1 4 
(b) electrical characteristics and interface 4 4 9 
Table 1. Judgment count for the sample instances (HA=high at-
tachment; LA=low attachment; and A=Ambiguous) 
 
We set an ambiguity threshold, ?, to determine 
whether the distribution of interpretations is 
nocuous or innocuous with respect to that par-
ticular ?. If the certainty of neither interpretation, 
HA or LA, exceeds the threshold ?, we say this 
is an instance of nocuous coordination. Other-
wise it is innocuous. Here, (a) displays nocuous 
ambiguity for ?>71%. 
0
10
20
30
40
50
60
70
80
90
100
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ambiguity Thresholds 
Am
bi
gu
iti
e
s 
(%
)
Inno
Nocu
 
Figure 1. Proportions of interpretations at different ambiguity 
thresholds in the coordination instances 
Figure 1 shows the systematic relationship be-
tween ambiguity threshold and the incidence of 
nocuous ambiguity in the dataset. Low thresh-
olds can be satisfied with a very low certainty 
scores resulting in few instances being consid-
ered nocuous. At high thresholds, almost all in-
stances are classified as nocuous unless the 
judges report a consensus interpretation.  
1220
3.2 Heuristics to predict Nocuity 
Each heuristic tests a factor favouring a high or 
low modifier attachment (HA or LA). We im-
plemented and extended Willis et al (2008). 
 
Coordination matching favours HA when the 
head words of near and far conjuncts are fre-
quently found coordinated in a general corpus 
like BNC, suggesting they may form a single 
syntactic unit. 
 
Distribution similarity measures how often two 
words are found in the same contexts. It favours 
HA where it detects a strong distributional simi-
larity between the headwords of the two con-
juncts, suggesting these form a syntactic unit 
(Kilgariff 2003).  
 
Collocation frequency favours LA when the 
modifier is collocated much more frequently 
with the headword of the near conjunct than the 
far conjunct, in the document, or in the BNC. 
 
Morphology favours HA when the conjunct 
headwords share a morphological marker (suf-
fix) (Okumura and Muraki 1994).  
 
Semantic similarity favours HA when the con-
junct headwords display strong similarity in the 
taxonomic structure in WordNet6.  
3.3 Nocuity classification 
To train, and test, the nocuity classifier, each 
ambiguity training/test instance is represented as 
an attribute-value vector, with the values set to 
the score of a particular heuristic. The class label 
of each instance (nocuous (Y) or innocuous (N) 
at a given ambiguity threshold) is determined by 
the certainty measure as discussed earlier. We 
selected the LogitBoost algorithm for building 
the classifier, because it outperformed other can-
didates on our training data than. To determine 
whether a test instance displays nocuity or not, 
we presented its feature vector to the classifier, 
and obtained a predicted class label (Y or N). 
4 Automatic Identification of Nocuous 
Anaphora Ambiguity 
An anaphor is an expression referring to an an-
tecedent, usually a noun phrase (NP) found in 
                                                 
6
 Implemented by the NLP tool - Java WordNet Similarity Library. 
http://nlp.shef.ac.uk/result/software.html 
the preceding text. Anaphora ambiguity occurs 
when there are two or more candidate antece-
dents, as in example (3). 
 
(3) The procedure shall convert the 24 bit image to 
an 8 bit image, then display it in a dynamic window. 
 
In this case, both of the NPs, ?the 24 bit im-
age? and ?an 8 bit image?, are considered poten-
tial candidate antecedents of the anaphor ?it?. 
Anaphora ambiguity is difficult to handle due 
to contextual effects spread over several sen-
tences. Our goal is to determine whether a case 
of anaphora ambiguity is nocuous or innocuous, 
automatically, by using our methodology.  
4.1 The building of the Dataset 
Anaphora instances. We collected 200 anaph-
ora instances from requirements documents from 
RE@UTS website 7 . We are specifically con-
cerned with 3rd person pronouns, which are 
widespread in requirements texts. The dataset 
contains different pronoun types. Nearly half  
the cases (48%) involve subject pronouns, al-
though pronouns also occurred in objective and 
possessive positions (15% and 33%, respec-
tively).  Pronouns in prepositional phrases (e.g., 
?under it?) are rarer (4% - only 8 instances).  
 
Human judgment collection. The instances 
were presented to a group of 38 computing pro-
fessionals (academic staff, research students, 
software developers). For each instance, the 
judges were asked to select the antecedent from 
the list of NP candidates. Each instance was 
judged by at least 13 people. Table 2 shows an 
example of judgment counts, where 12 out of 13 
judges committed to ?supervisors? as the antece-
dent of ?they?, whereas 1 chose ?tasks?.   
 
1. Supervisors may only modify tasks they supervise to the 
agents they supervise.  
 Response 
Percent 
Response 
Count 
(a) supervisors 
(b) tasks 
92.3% 
7.7% 
12 
1 
Table 2. Judgment count for an anaphora ambiguity instance. 
 
Ambiguity threshold. Given an anaphor, the 
interpretation certainty of a particular NP candi-
date is calculated as the percentage of the judg-
ments for this NP against the total judgments for 
the instance. For example, consider the example 
in Table 2. The certainty of the NP ?supervisors? 
                                                 
7
 http://research.it.uts.edu.au/re/ 
1221
is 12/13=92.3% and the certainty of the NP 
?tasks? is 1/13=7.7%. Thus, at an ambiguity 
threshold of, for instance, ? = 0.8, the ambiguity 
in Table 2 is innocuous because the agreement 
between the judges exceeds the threshold. 
Figure 2 shows the relationship between am-
biguity threshold and occurrence of nocuous 
ambiguity. As in Figure 1, the number of nocu-
ous ambiguities increases with threshold ?. For 
high thresholds (e.g., ??0.9), more than 60% of 
instances are classified as nocuous. Below 
threshold (??0.4), fewer than 8 cases are judged 
nocuous. Also, comparing Figures 1 and 2 would 
appear to suggest that, in technical documents, 
anaphora ambiguity is less likely to lead to mis-
understandings than coordination.  
0
10
20
30
40
50
60
70
80
90
100
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ambiguity Thresholds
Am
bi
gu
iti
e
s 
(%
)
Inno
Nocu
Figure 2. Proportions of interpretations at different ambiguity 
thresholds in the anaphora instances. 
4.2 Antecedent Preference Heuristics 
Drawing on the literature on anaphoric reference, 
we developed 12 heuristics of three types: re-
lated to linguistic properties of text components, 
to context and discourse information, or to sta-
tistical information drawn from standard corpora. 
Yang et al (2010b) gives more detail. A heuris-
tic marks candidate antecedents which it favours, 
or disfavours. For instance, heuristics favour 
definite NPs as antecedents, candidate NPs 
which agree in number and syntactic role with 
the anaphor, and those which share a syntactic 
collocation pattern in the text. They also favour 
those which respect the semantic constraints 
(e.g., animacy) propagated from subcategorisa-
tion information, and reward proximity to the 
anaphor. They disfavour candidate antecedents 
that occur in prepositional phrases, and those 
occupying a syntactic role distinct from the ana-
phor. Note: not all NPs are marked by all heuris-
tics, and some heuristics are interdependent.   
4.3 Nocuous Ambiguity Identification 
Unlike coordination ambiguity, where judges 
chose for high or low modifier attachment, 
anaphora have scope over a variable set of po-
tential antecedents, depending on each particular 
instance. To accommodate this, we developed an 
antecedent classifier which assigns a weighted 
antecedent tag to each NP candidate associated 
with an instance. Tag information is used subse-
quently to predict the whether the instance dis-
plays nocuous ambiguity. 
The antecedent classifier is built using the Na-
ive Bayes algorithm within the WEKA package 
and is trained to return three classes of candidate 
antecedent: positive (Y), questionable (Q), or 
negative (N). In an innocuous case, a candidate 
NP will be classed as Y if its interpretation cer-
tainty exceeds the threshold set by ?, and tagged 
as N otherwise; in a nocuous case, it will be 
classed as N if its certainty is 0%, and classified 
as Q otherwise.  
 
1. The LPS operational scenarios represent sequences of activi-
ties performed by operations personnel as they relate to the LPS 
software. 
 Response Label 
(a) the LPS operational scenarios 
(b) sequences of activities 
(c) activities 
(d) operations personnel 
33.3% 
66.7% 
0% 
0% 
Q 
Q 
N 
N 
Table 3. The determination of antecedent label for the NP candi-
dates in a NOCUOUS ambiguity case (? =0.8) 
 
2. Testing performed to demonstrate to the acquirer that a 
CSCI system meets its specified requirements. 
 Response 
Percent 
Class 
Label 
(a) Testing 
(b) the acquirer 
(c) a CSCI system 
0% 
16.7% 
83.3% 
N 
N 
Y 
Table 4. The determination of antecedent label for the NP candi-
dates in a INNOCUOUS ambiguity case (? =0.8) 
 
Antecedent Class Label  
Y Q N 
? = 0.5 181 54 623 
? = 0.6 160 99 599 
? = 0.7 137 149 572 
? = 0.8 107 209 542 
? = 0.9 77 261 520 
? = 1.0 41 314 503 
Table 5. The distribution of three antecedent class label at different 
ambiguity thresholds 
 
Table 3 and 4 illustrate antecedent labels for 
NP antecedent candidates in a nocuous and in-
nocuous case. Candidates (a) and (b) in Table 3 
are labeled Q because their certainty falls below 
the threshold (? = 0.8). For the same threshold, 
candidate (c) in Table 4 is tagged as Y. Table 5 
1222
shows the distribution of tags at certainty thresh-
olds ? ? 0.5 for all (858) candidate antecedents 
in our sample. 
Our intended application is a system to alert 
experts to risk of misunderstandings. This sug-
gests we should emphasise recall even at the ex-
pense of some precision (Berry et al 2003). We 
developed two versions of the algorithm that 
determines whether an instance is nocuous or not, 
depending on the contribution made by its ante-
cedent candidates tagged Y. We relax constraints 
by introducing two concepts: a weak positive 
threshold W
Y
 and a weak negative threshold WN 
set at 0.5 and 0.4, respectively8. The rationale for 
weak thresholds is that antecedent preference 
reflects a spectrum with Y (high), Q (medium), 
and N (low). Weak positive and negative thresh-
olds act as buffers to the Q area. Antecedent NPs 
that fall in the W
Y
 or WN buffer area are treated 
as possible false negative (FN) for the classifica-
tion of the label Q. An antecedent tag Y/N is la-
beled as weak positive or negative depending on 
these thresholds. The algorithm for identifying 
nocuous ambiguity is given in Figure 3. It treats 
as innocuous those cases where the antecedent 
label list contains one clear Y candidate, whose 
certainty exceeds all others by a margin.  
 
Given an anaphora ambiguity instance with multiple potential NPs, 
the antecedent classifier returns a label list, },,,{ 21 nrrrR K= , for 
individual NPs. 
 
Parameters:  
1) W
Y
 - the threshold for the weak positive label. The label Y is 
viewed as weak positive when the positive prediction score ri < WY 
2) W
N
 - the threshold for the weak negative label. The label N is 
viewed as weak negative when the negative prediction score ri < 
W
N
 
 
Procedure: 
if the label list R contains  
         (one Y, no Q, one or more N ) 
    or  
         (no Y, one Q, one or more N but not weak negative ) 
    or  
        (one Y but not weak positive, any number of Q or N)    
then 
         the ambiguity is INNOCUOUS 
else 
         the ambiguity is NOCUOUS          
Figure 3. The algorithm for nocuous ambiguity identification 
5 Experiments and Results 
In all experiments, the performance was evalu-
ated using 5-fold cross-validation, using  stan-
                                                 
8
 Weak positive and negative thresholds are set experimentally. 
dard measures of Precision (P), Recall (R), F-
measure (F), and Accuracy. We use two naive 
baselines: BL-1 assumes that all ambiguity in-
stances are innocuous; BL-2 assumes that they 
are all nocuous. For fair comparison against the 
baselines, for both forms of ambiguity, we only 
report the performance of our ML-based models 
when the incidence of nocuous ambiguities falls 
between 10% ~ 90% of the set (see Figures 1 
and 2). We first report our findings for the iden-
tification of nocuous coordination ambiguities 
and then discuss the effectiveness of our model 
in distinguishing possible nocuous ambiguities 
from a set of ambiguity instances.    
5.1 Nocuous Coordination Ambiguity Iden-
tification 
Willis et al(2008) demonstrated the ability of 
their approach to adapt to different thresholds by 
plotting results against the two na?ve base lines. 
Since we extended and refined their approach 
described we plot our experimental results (CM-
1), for comparison, using the same measures, 
against their evaluation data (CM-2), in Figure 4.   
0
10
20
30
40
50
60
70
80
90
100
40 45 50 55 60 65 70 75 80
Ambiguity Threshold (%)
Ac
cu
ra
cy
 (%
) BL-1
BL-2
CM-1
CM-2
Figure 4. The performance comparison of the ML-based models, 
CM-1 and CM-2, to the two baseline models, BL-1 and BL-2, in 
nocuous coordination ambiguity identification.  
 
Our CM-1 model performed well with an ac-
curacy of above 75% on average at all ambiguity 
threshold levels. As expected, at very high and 
very low thresholds, we did not improve on the 
naive baselines (which have perfect recall and 
hence high accuracy). The CM-1 model dis-
played its advantage when the ambiguity thresh-
old fell in the range between 0.45 and 0.75 (a 
significantly wider range than reported for CM-2 
Willis et al(2008)). CM-1 maximum improve-
ment was achieved around the 58% crossover 
point where the two na?ve baselines intersect and  
our model achieved around 21% increased accu-
1223
racy. This suggests that the combined heuristics 
do have strong capability of distinguishing 
nocuous from innocuous ambiguity at the weak-
est region of the baseline models. 
Figure 4 also shows that, the CM-1 model 
benefitted from the extended heuristics and the 
LogitBoost algorithm with an increased accuracy 
of around 5.54% on average compared with CM-
2.  This suggests that local context information 
and semantic relationships between coordinating 
conjuncts provide useful clues for the identifica-
tion of nocuous ambiguity. Furthermore, the 
LogitBoost algorithm is more suitable for deal-
ing with a numeric-attribute feature vector than 
the previous Logistic Regression algorithm.  
5.2 Nocuous Anaphora Ambiguity Identifi-
cation 
We report on two implementations: one with 
weak thresholds (AM-1) and one without (AM-
2). We compare both approaches using the base-
lines, BL-1 and BL-2 (in Figure 5). It shows that 
AM-1 and AM-2 achieve consistent improve-
ments on baseline accuracy at high thresholds 
(??0.75). Here also, the improvement maximises 
around the 83% threshold point where the two 
baselines intersect. However, the ML-based 
models perform worse than BL-1 at the lower 
thresholds (0.5???0.7). One possible explanation 
is that, at low thresholds, performance is affected 
by lack of data for training of the Q class label, 
an important indicator for nocuous ambiguity 
(see Table 5). This is also consistent with the 
ML models performing well at higher thresh-
olds, when enough nocuous instances are avail-
able for training. 
0
10
20
30
40
50
60
70
80
90
100
50 55 60 65 70 75 80 85 90 100
Ambiguity Threshold (%)
Ac
cu
ra
cy
 
(%
)
BL-1
BL-2
AM-1
AM-2
 
Figure 5. The performance comparison of the ML-based models, 
AM-1 and AM-2, to the two baseline models, BL-1 and BL-2, in 
nocuous anaphora ambiguity identification.  
     
 Figure 5 further shows that the model with 
weak thresholds (AM-1) did not perform as well 
as the model without weak thresholds (AM-2) on 
accuracy. Although both models perform much 
better than the baselines on precision (more ex-
perimental results are reported in Yang et al 
(2010b)), the actual precisions for both models 
are relatively low, ranging from 0.3 ~ 0.6 at dif-
ferent thresholds. When the AM-1 model at-
tempts to discover more nocuous instances using 
weak thresholds, it also introduces more false 
positives (innocuous instances incorrectly 
classed as nocuous). The side-effect of introduc-
ing false positives for AM-1 is to lower accu-
racy. However, the AM-1 model outperforms 
both AM-2 and BL-2 models on F-measure 
(Figure 6), with an average increase of 5.2 and 
3.4 percentage points respectively. This reveals 
that relaxing sensitivity to the ambiguity thresh-
old helps catch more instances of nocuous 
anaphora ambiguity.             
10
15
20
25
30
35
40
45
50
55
60
50 55 60 65 70 75 80 85 90 100
Ambiguity Threshold (%)
F-
m
e
a
su
re
 
(%
) BL-2
AM-1
AM-2
Figure 6. The performance comparison of the ML-based models, 
AM-1 and AM-2, to the baseline model BL-2 (na?ve nocuous) 
6 Discussions 
We presented judges with sentences containing 
ambiguities without any surrounding context, 
even though contextual information (e.g., dis-
course focus) clearly contributes to interpreta-
tion. This is a weakness in our data collection 
technique. Besides contextual information, van 
Deemter?s Principle of Idiosyncratic Interpreta-
tion (1998) suggests that some factors, including 
the reader?s degree of language competence, can 
affect perceptions of ambiguity. Similarly, fa-
miliarity with a domain, including tacit specialist 
information (Polanyi, 1966), and the extent to 
which this is shared by a group, will have an ef-
fect on the extent to which stakeholders arrive at 
diverging interpretations. 
In our case, we extracted instances from re-
quirements documents covering several techni-
1224
cal domains. Judgements are sensitive to the 
backgrounds of the participants, and the extent 
to which stakeholder groups share such a back-
ground. Also, we used several large, generic NL 
resources, including the BNC and WordNet. The 
performance of several heuristics would change 
if they drew on domain specific resources. Dif-
ferent interpretations may be compatible, and so 
not necessarily contribute to misunderstanding.  
Finally, we used different machine learning 
algorithms to tackle different types of ambiguity 
instances: LogitBoost for coordination ambigu-
ity and Naive Bayes for anaphora ambiguity. 
The main reason is that coordination heuristics 
returned numeric values, whereas the anaphora 
heuristics were Boolean. Our method assumes 
tailoring of the ML algorithm to the choice of 
heuristic. These limitations indicate that the 
methodology has a high degree of flexibility, but 
also that it has several interdependent compo-
nents and background assumptions that have to 
be managed if an application is to be developed. 
7 Related Work 
Many researchers have remarked on the fact that 
some ambiguities are more likely than others to 
lead to misunderstandings, and suggested classi-
fying them accordingly. Poesio (1996) discussed 
cases where multiple readings are intended to 
coexist, and distinguished between language in-
herent and human disambiguation factors from a 
philosophical perspective. His notion of ?per-
ceived ambiguity? suggests that human percep-
tions are what actually cause an ambiguity to be 
misunderstood. Van Deemter?s (2004) ?vicious 
ambiguity? refers to an ambiguity that has no 
single, strongly preferred interpretation. He pro-
posed quantifying ?viciousness? using probabili-
ties taken from corpus data. Van Rooy (2004) 
defined a notion of ?true ambiguity?: a sentence 
is truly ambiguous only if there are at least two 
interpretations that are optimally relevant. These 
last two approaches rely on probability analysis 
of language usage, and not directly on human 
perception, which we believe to be the key to 
evaluating ambiguity. Our work differs in that it 
takes into account the distribution of interpreta-
tions arrived at by a group of human judges en-
gaged with a text. Our model treats ambiguity 
not as a property of a linguistic construct or a 
text, or a relation between a text and the percep-
tions of a single reader, but seeks to understand 
the mechanisms that lead to misunderstandings 
between people in a group or process. 
    Poesio et al(2006) have pointed out that dis-
ambiguation is not always necessary; for in-
stance, in some complex anaphora cases, the fi-
nal interpretation may not be fully specified, but 
only ?good enough?. Our work does not attempt 
disambiguation. It seeks to highlight the risk of 
multiple interpretations (whatever those are).   
8 Conclusions and Future Work 
We have presented a general methodology for 
automatically identifying nocuous ambiguity 
(i.e. cases of ambiguity where there is a risk that 
people will hold different interpretations) rela-
tive to some tolerance level set for such a risk. 
The methodology has been implemented in a 
ML based architecture, which combines a num-
ber of heuristics each highlighting factors which 
may affect how humans interpret ambiguous 
constructs. We have validated the methodology 
by identifying instances of nocuous ambiguity in 
coordination and anaphoric constructs. Human 
judgments were collected in a dataset used for 
training the ML algorithm and evaluation. Re-
sults are encouraging, showing an improvement 
of approximately 21% on accuracy for coordina-
tion ambiguity and about 3.4% on F-measure for 
anaphora ambiguity compared with naive base-
lines at different ambiguity threshold levels. We 
showed, by comparison with results reported in 
Willis et al(2008) that the methodology can be 
fine tuned, and extended to other ambiguity 
types, by including different heuristics.  
Our method can highlight the risk of different 
interpretations arising: this is not a task a single 
human could perform, as readers typically have 
access only to their own interpretation and are 
not routinely aware that others hold a different 
one. Nonetheless, our approach has limitations, 
particularly around data collection, and for 
anaphora ambiguity at low thresholds. We en-
visage further work on the implementation of 
ambiguity tolerance thresholds 
Several interesting issues remain to be inves-
tigated to improve our system?s performance and 
validate its use in practice. We need to explore 
how to include different and complex ambiguity 
types (e.g., PP attachment and quantifier scop-
1225
ing), and investigate whether these are equally 
amenable to a heuristics based approach.  
Acknowledgement  
This work is supported financially by UK EPSRC for 
the MaTREx project (EP/F068859/1), and Irish SFI 
for the grant 03/CE2/I303_1. 
References 
Daniel M. Berry, Erik Kamsties, and Michael M. 
Krieger.  2003. From Contract Drafting to Soft-
ware Specification: Linguistic Sources of Ambigu-
ity. Technical Report, School of Computer Sci-
ence, University of Waterloo.  
Stephen Boyd, Didar Zowghi, and Alia Farroukh.  
2005. Measuring the Expressiveness of a Con-
strained Natural Language: An Empirical Study. In 
Proceedings of the 13th IEEE International Con-
ference on Requirements Engineering (RE?05), 
Washington, DC, pages 339-52. 
Eric Brill and Philip Resnik.  1994. A Rule-Based 
Approach to Prepositional Phrase Attachment Dis-
ambiguation. In Proceedings of the 15th Interna-
tional Conference on Computational Linguistics, 
pages 1198-204. 
Francis Chantree, Bashar Nuseibeh, Anne de Roeck, 
and Alistair Willis.  2006. Identifying Nocuous 
Ambiguities in Natural Language Requirements. 
In Proceedings of 14th IEEE International Re-
quirements Engineering Conference (RE'06), Min-
neapolis, USA, pages 59-68. 
Adam Kilgarriff.  2003. Thesauruses for Natural Lan-
guage Processing. In Proceedings of NLP-KE, 
pages 5-13. 
Preslav Nakov and Marti  Hearst.  2005. Using the 
Web as an Implicit Training Set: Application to 
Structural Ambiguity Resolution. In Proceedings 
of HLT-NAACL?05, pages 835-42. 
Akitoshi Okumura and Kazunori Muraki.  1994. 
Symmetric Pattern Matching Analysis for English 
Coordinate Structures. In Proceedings of the 4th 
Conference on Applied Natural Language Proc-
essing, pages 41-46. 
Massimo Poesio. 1996. Semantic Ambiguity and Per-
ceived Ambiguity In Semantic Ambiguity and Un-
derspecification edited by K. van Deemter and S. 
Peters, pages 159-201. 
Massimo Poesio and Ron Artstein. 2008. Introduction 
to the Special Issue on Ambiguity and Semantic 
Judgements. Research on Language & Computa-
tion 6: 241-45. 
Massimo Poesio, Patick Sturt, Ron Artstein, and Ruth 
Filik. 2006. Underspecification and Anaphora: 
Theoretical Issues and Preliminary Evidence. Dis-
course Processes 42(2): 157-75. 
Michael Polanyi.  1966. The Tacit Dimension. RKP, 
London. 
Kees van Deemter. 1998. Ambiguity and Idiosyn-
cratic Interpretation. Journal of Semantics 15(1): 
5-36. 
Kees van Deemter. 2004. Towards a Probabilistic 
Version of Bidirectional Ot Syntax and Semantics. 
Journal of Semantics 21(3): 251-80. 
Robert van Rooy. 2004. Relevance and Bidirectional 
Ot. In Optimality Theory and Pragmatic, edited by 
R. Blutner and H. Zeevat, pages 173-210. 
Thomas Wasow, Amy Perfors, and David Beaver. 
2003. The Puzzle of Ambiguity. In Morphology 
and the Web of Grammar: Essays in Menory of 
Steven G. Lapointe, edited by O. Orgun and P. 
Sells. 
Alistair Willis, Francis Chantree, and Anne De 
Roeck. 2008. Automatic Identification of Nocuous 
Ambiguity. Research on Language & Computa-
tion 6(3-4): 1-23. 
Hui Yang, Alistair Willis, Anne de Roeck, and Ba-
shar Nuseibeh. 2010a. Automatic Detection of 
Nocuous Coordination Ambiguities in Natural 
Language Requirements. In Proceedings of the 
25th IEEE/ACM International Conference on 
Automated Software Engineering Conference 
(ASE?10). (In press) 
Hui Yang, Anne de Roeck, Alistair Willis, and Ba-
shar Nuseibeh. 2010b. Extending Nocuous Ambi-
guity Analysis for Anaphora in Natural Language 
Requirements. In Proceedings of the 18th Interna-
tional Requirements Engineering Conference 
(RE?10). (In press) 
 
1226
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 97?105,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A Generalised Hybrid Architecture for NLP
Alistair Willis
Department of Computing
The Open University,
Milton Keynes, UK
a.g.willis@open.ac.uk
Hui Yang
Department of Computing
The Open University,
Milton Keynes, UK
h.yang@open.ac.uk
Anne De Roeck
Department of Computing
The Open University,
Milton Keynes, UK
a.deroeck@open.ac.uk
Abstract
Many tasks in natural language process-
ing require that sentences be classified from
a set of discrete interpretations. In these
cases, there appear to be great benefits in
using hybrid systems which apply multiple
analyses to the test cases. In this paper, we
examine a general principle for building hy-
brid systems, based on combining the re-
sults of several, high precision heuristics.
By generalising the results of systems for
sentiment analysis and ambiguity recogni-
tion, we argue that if correctly combined,
multiple techniques classify better than sin-
gle techniques. More importantly, the com-
bined techniques can be used in tasks where
no single classification is appropriate.
1 Introduction
The success of hybrid NLP systems has demon-
strated that complex linguistic phenomena and
tasks can be successfully addressed using a com-
bination of techniques. At the same time, it is
clear from the NLP literature, that the perfor-
mance of any specific technique is highly depen-
dent on the characteristics of the data. Thus, a
specific technique which performs well on one
dataset might perform very differently on another,
even on similar tasks, and even if the two datasets
are taken from the same domain. Also, it is possi-
ble that the properties affecting the effectiveness
of a particular technique may vary within a single
document (De Roeck, 2007).
As a result of this, for many important NLP
applications there is no single technique which
is clearly to be preferred. For example, recent
approaches to the task of anaphora resolution
include syntactic analyses (Haghighi and Klein,
2009), Maximum Entropy models (Charniak and
Elsner, 2009) and Support Vector Machines (Yang
et al, 2006; Versley et al, 2008). The perfor-
mance of each of these techniques varies depend-
ing upon the particular choice of training and test
data.
This state of affairs provides a particular op-
portunity for hybrid system development. The
overall performance of an NLP system depends
on complex interactions between the various phe-
nomena exhibited by the text under analysis, and
the success of a given technique can be sensitive
to the different properties of that text. In partic-
ular, the text?s or document?s properties are not
generally known until the document comes to be
analysed. Therefore, there is a need for systems
which are able to adapt to different text styles at
the point of analysis, and select the most appropri-
ate combination of techniques for the individual
cases. This should lead to hybridising techniques
which are robust or adaptive in the face of varying
textual styles and properties.
We present a generalisation of two hybridi-
sation techniques first described in Yang et al
(2012) and Chantree et al (2006). Each uses
hybrid techniques in a detection task: the first is
emotion detection from suicide notes, the second
is detecting nocuous ambiguity in requirements
documents. The distinguishing characteristic of
both tasks is that a successful solution needs to
accommodate uncertainty in the outcome. The
generalised methodology described here is partic-
ularly suited to such tasks, where as well as se-
lecting between possible solutions, there is a need
to identify a class of instances where no single so-
lution is most appropriate.
97
2 Hybridisation as a Solution to
Classification Tasks
The methodology described in this paper pro-
poses hybrid systems as a solution to NLP tasks
which attempt to determine an appropriate inter-
pretation from a set of discrete alternatives, in par-
ticular where no one outcome is clearly prefer-
able. One such task is nocuous ambiguity detec-
tion. For example, in sentence (1), the pronoun he
could refer to Bill, John or to John?s father.
(1) When Bill met John?s father, he was pleased.
Here, there are three possible antecedents for he,
and it does not follow that all human readers
would agree on a common interpretation of the
anaphor. For example, readers might divide be-
tween interpreting he as Bill or as John?s father.
Or perhaps a majority of readers feel that the
sentence is sufficiently ambiguous that they can-
not decide on the intended interpretation. These
are cases of nocuous ambiguity (Chantree et al,
2006), where a group of readers do not interpret a
piece of text in the same way, and may be unaware
that the misunderstanding has even arisen.
Similarly, as a classification task, sentiment
analysis for sentences or fragments may need
to accommodate instances where multiple senti-
ments can be identified, or possibly none at all.
Example (2) contains evidence of both guilt and
love:
(2) Darling wife, ? I?m sorry for everything.
Hybrid solutions are particularly suited to such
tasks, in contrast to approaches which use a single
technique to select between possible alternatives.
The hybrid methodology proposed in this paper
approaches such tasks in two stages:
1. Define and apply a set of heuristics, where
each heuristic captures an aspect of the phe-
nomenon and estimates the likelihood of a
particular interpretation.
2. Apply a combination function to either com-
bine or select between the values contributed
by the individual heuristics to obtain better
overall system performance.
The model makes certain assumptions about
the design of heuristics. They can draw on a mul-
titude of techniques such as a set of selection fea-
tures based on domain knowledge, linguistic anal-
ysis and statistical models. Each heuristic is a
partial descriptor of an aspect of a particular phe-
nomenon and is intended as an ?expert?, whose
opinion competes against the opinion offered by
other heuristics. Heuristics may or may not be in-
dependent. The crucial aspect is that each of the
heuristics should seek to maximise precision or
complement the performance of another heuristic.
The purpose of step 2 is to maximise the contri-
bution of each heuristic for optimal performance
of the overall system. Experimental results anal-
ysed below show that selecting an appropriate
mode of combination helps accommodate dif-
ferences between datasets and can introduce ad-
ditional robustness to the overall system. The
experimental results also show that appropriate
combination of the contribution of high precision
heuristics significantly increases recall.
For the tasks under investigation here, it proves
possible to select combination functions that al-
low the system to identify behaviour beyond clas-
sifying the subject text into a single category. Be-
cause the individual heuristics are partial descrip-
tions of the whole language model of the text, it
is possible to reason about the interaction of these
partial descriptions, and identify cases where ei-
ther none, or many, of the potential interpretations
of the text are possible. The systems use either a
machine learning technique or a voting strategies
to combine the individual heuristics.
In sections 3 and 4, we explore how the pre-
viously proposed solutions can be classed as in-
stances of the proposed hybridisation model.
3 Case study: Sentiment Analysis
Following Pang et al (2002) and the release of the
polarity 2.0 dataset, it is common for sentiment
analysis tasks to attempt to classify text segments
as either of positive or negative sentiment. The
task has been extended to allow sentences to be
annotated as displaying both positive and negative
sentiment (Wilson et al, 2009) or indicating the
degree of intensity (Thelwall et al, 2010).
The data set used for the 2011 i2b2 shared chal-
lenge (Pestian et al, 2012) differs from this model
by containing a total of 15 different sentiments to
classify the sentences. Each text fragment was
labelled with zero, one or more of the 15 senti-
ments. For example, sentence (2) was annotated
with both Love and Guilt. The fragments varied
between phrases and full sentences, and the task
aims to identify all the sentiments displayed by
98
each text fragment.
In fact, several of the proposed sentiments were
identified using keyword recognition alone, so the
hybrid framework was applied only to recognise
the sentiments Thankfulness, Love, Guilt, Hope-
lessness, Information and Instruction; instances
of the other sentiments were too sparse to be reli-
ably classified with the hybrid system. A keyword
cue list of 984 terms was manually constructed
from the training data based on their frequency in
the annotated set; no other public emotion lexicon
was used. This cue list was used both to recognise
the sparse sentiments, and as input to the CRF.
3.1 Architecture
An overview of the architecture is shown in figure
1. Heuristics are used which operate at the word
level (Conditional Random Fields), and at the
sentence level (Support Vector Machine, Naive
Bayes and Maximum Entropy). These are com-
bined using a voting strategy that selects the most
appropriate combination of methods in each case.
Input
text
?
Preprocess
text
?
Negation
detection
? ?
Combine
values
?
Token level Sentence level
classifier classifiers
CRF SVM
NB
ME
Figure 1: Architecture for sentiment classification task
The text is preprocessed using the tokeniser,
POS tagger and chunker from the Genia tagger,
and parsed using the Stanford dependency parser.
This information, along with a negation recog-
niser, is used to generate training vectors for the
heuristics. Negation is known to have a major ef-
fect on sentiment interpretation (Jia et al, 2009).
3.2 Sentiment recognition heuristics
The system uses a total of four classifiers for each
of the emotions to be recognised. The only token-
level classification was carried out using CRFs
(Lafferty et al, 2001) which have been success-
fully used on Named Entity Recognition tasks.
However, both token- and phrase-level recogni-
tion are necessary to capture cases where sen-
tences convey more than one sentiment. The
CRF-based classifiers were trained to recognise
each of the main emotions based on the main key-
word cues and the surrounding context. The CRF
is trained on the set of features shown in figure 2,
and implemented using CRF++1.
Feature Description
Words word, lemma, POS tag, phrase
chunk tag
Context 2 previous words and 2 following
words with lemma, POS tags and
chunk tags
Syntax Dependency relation label and
the lemma of the governer word
in focus
Semantics Is it negated?
Figure 2: Features used for CRF classifier
Three sentence-level classifiers were trained
for each emotion, those being Naive Bayes and
Maximum Entropy learners implemented by the
MALLET toolkit2, and a Support Vector Machine
model implemented using SVM light3 with the
linear kernel. In each case, the learners were
trained using a feature vector using the two fea-
ture vectors as shown in figure 3.
Feature vector Description
Words word lemmas
Semantics negation terms identified by
the negative term lexicon,
and cue terms from the emo-
tion term lexicon
Figure 3: Features used for sentence-level classifiers
A classifier was built for each of the main emo-
tions under study. For each of the six emotions,
four learners were trained to identify whether the
text contains an instance of that emotion. That is,
an instance of text receives 6 groups of results,
and each group contains 4 results obtained from
different classifiers estimating whether one par-
ticular emotion occurs. The combination func-
tion predicts the final sentiment(s) exhibited by
the sentence.
1http://crfpp.sourceforge.net/
2http://mallet.cs.umass.edu/
3http://svmlight.joachims.org/
99
3.3 Combination function
To combine the outputs of the heuristics, Yang et
al. (2012) use a voting model. Three different
combination methods are investigated:
Any If a sentence is identified as an emotion in-
stance by any one of the ML-based models, it
is considered a true instance of that emotion.
Majority If a sentence is identified as an emotion
instance by two or more of the ML-based
models, it is considered a true instance of
that emotion.
Combined If a sentence is identified as an emo-
tion instance by two or more of the ML-
based models or it is identified as an emo-
tion instance by the ML-based model with
the best precision for that emotion, it is con-
sidered a true instance of that emotion.
This combined measure reflects the intuition
that where an individual heuristic is reliable for a
particular phenomenon, then that heuristic?s vote
should be awarded a greater weight. The preci-
sion scores of the individual heuristics is shown
in table 1, where the heuristic with the best preci-
sion for that emotion is highlighted.
Emotion CRF NB ME SVM
Thankfulness 60.6 58.8 57.6 52.6
Love 76.2 68.5 77.6 76.9
Guilt 58.1 46.8 35.3 58.3
Hopelessness 73.5 63.3 68.7 74.5
Information 53.1 41.0 48.1 76.2
Instruction 76.3 63.6 70.9 75.9
Table 1: Precision scores (%) for individual heuristics
3.4 Results
Table 2 reports the system performance on 6 emo-
tions by both individual and combined heuristics.
In each case, the best performer among the four
individual heuristics is highlighted. As can be
seen from the table, the Any combinator and the
Combined combinators both outperform each of
the individual classifiers. This supports the hy-
pothesis that hybrid systems work better overall.
3.5 Additional comments
The overall performance improvement obtained
by combining the individual measures raises the
question of how the individual elements interact.
Table 3 shows the performance of the combined
systems on the different emotion classes. For
each emotion, the highest precision, recall and f-
measure is highlighted.
As we would have expected, the Any strategy
has the highest recall in all cases, while the Major-
ity strategy, with the highest bar for acceptance,
has the highest precision for most cases. The
Any and Combined measures appear to be broadly
comparable: for the measures we have used, it ap-
pears that the precision of the individual classi-
fiers is sufficiently high that the combination pro-
cess of improving recall does not impact exces-
sively on the overall precision.
A further point of interest is that table 2 demon-
strates that the Naive Bayes classifier often re-
turns the highest f-score of the individual classi-
fiers, even though it never has the best precision
(table 1). This supports our thesis that a success-
ful hybrid system can be built from multiple clas-
sifiers with high precision, rather than focussing
on single classifiers which have the best individ-
ual performance (the Combined strategy favours
the highest precision heuristic).
4 Nocuous ambiguity detection
It is a cornerstone of NLP that all text contains
a high number of potentially ambiguous words or
constructs. Only some of those will lead to misun-
derstandings, where two (or more) participants in
a text-mediated interchange will interpret the text
in different, and incompatible ways, without real-
ising that this is the case. This is defined as nocu-
ous ambiguity (Willis et al, 2008), in contrast to
innocuous ambiguity, where the text is interpreted
in the same way by different readers, even if that
text supports different possible analyses.
The phenomenon of nocuous ambiguity is par-
ticularly problematic in high stake situations. For
example, in software engineering, a failure to
share a common interpretation of requirements
stated in natural language may lead to incorrect
system implementation and the attendant risk of
system failure, or higher maintenance costs. The
systems described by Chantree et al (2006) and
Yang et al (2010a) aim not to resolve ambigu-
100
Individual heuristics Hybrid models
Emotion CRF NB ME SVM Any Majority Combined
Thankfulness 59.5 59.6 61.9 60.3 63.9 63.0 64.2
Love 63.7 69.3 66.5 61.5 72.0 70.3 71.0
Guilt 35.3 40.5 27.7 37.8 46.3 29.9 45.8
Hopelessness 63.2 64.1 59.9 57.0 67.3 65.4 67.3
Information 42.3 47.7 43.7 43.4 50.2 45.5 47.8
Instruction 65.7 65.7 63.4 58.8 72.1 65.4 72.0
Table 2: F-scores (%) for individual and combined heuristics (sentiment analysis)
Any Majority Combined
P R F P R F P R F
Thankfulness 52.6 81.6 63.9 60.6 65.7 63.0 55.0 77.1 64.2
Love 68.7 75.6 72.0 77.9 64.0 70.3 74.6 67.7 71.0
Guilt 46.6 46.2 46.3 50.0 21.4 29.9 50.5 41.9 45.8
Hopelessness 64.1 70.8 67.3 80.3 55.2 65.4 66.3 68.4 67.3
Information 40.9 64.9 50.2 49.9 41.8 45.5 45.2 50.7 47.8
Instruction 68.5 76.1 72.1 80.8 54.9 65.4 70.3 73.7 72.0
Table 3: Precision, recall and F-scores (%) for the combined systems (sentiment analysis)
ous text in requirements, but to identify where in-
stances of text might display nocuous ambiguity.
These systems demonstrate how, for hybrid
systems, the correct choice of combination func-
tion is crucial to how the individual heuristics
work together to optimise overall system perfor-
mance.
4.1 Nocuous Ambiguity: Coordination
Chantree et al (2006) focus on coordination at-
tachment ambiguity, which occurs when a mod-
ifier can attach to one or more conjuncts of a
coordinated phrase. For example, in sentence
(3), readers may divide over whether the modi-
fier short attaches to both books and papers (wide
scope), or only to books (narrow scope).
(3) I read some short books and papers.
In each case, the coordination involves a near
conjunct, (books in (3)), a far conjunct, (papers)
and a modifier (short). The modifier might also
be a PP, or an adverb in the case where a VP con-
tains the conjunction. In disambiguation, the task
would be to identify the correct scope of the mod-
ifier (i.e. which of two possible bracketings is the
correct one). For nocuous ambiguity detection,
the task is to identify to what extent people inter-
pret the text in the same way, and to flag the in-
stance as nocuous if they diverge relative to some
threshold.
4.1.1 The dataset
17 human judgements were collected for each
of 138 instances of sentences exhibiting coor-
dination ambiguity drawn from a collection of
software requirements documents. The majority
of cases (118 instances) were noun compounds,
with some adjective and some preposition modi-
fiers (36 and 18 instances respectively). Partici-
pants were asked to choose between wide scope
or narrow scope modifier attachment, or to indi-
cate that they experienced the example as ambigu-
ous. Each instance is assigned a certainty for wide
and narrow scope modification reflecting the dis-
tribution of judgements. For instance, if 12 judges
favoured wide scope for some instance, 3 judges
favoured narrow scope and 1 judge thought the
instance ambiguous, then the certainty for wide
scope is 71% (12/17), and the certainty for nar-
row scope is 18% (3/17).
A key concept in nocuous ambiguity is that of
an ambiguity threshold, ? . For some ? :
? if at least ? judges agree on the interpretation
101
of the text, then the ambiguity is innocuous,
? otherwise the ambiguity is nocuous.
So for ? = 70%, at least 70% of the judges must
agree on an interpretation. Clearly, the higher ?
is set, the more agreement is required, and the
greater the number of examples which will be
considered nocuous.
4.1.2 Selectional heuristics
A series of heuristics was developed, each cap-
turing information that would lead to a preference
for either wide or narrow scope modifier attach-
ment. Examples from Chantree et al (2006) pro-
pose seven heuristics, including the following:
Co-ordination Matching If the head words
of the two conjuncts are frequently co-
ordinated, this is taken to predict wide
modifier scope.
Distributional Similarity If the head words of
the two conjuncts have high distributional
similarity (Lee, 1999), this is taken to pre-
dict wide modifier scope.
Collocation Frequency If the head word of the
near conjunct has a higher collocation with
the modifier than the far conjunct, this is
taken to predict narrow modifier scope.
Morphology If the conjunct headwords have
similar morphological markers, this is taken
to predict wide modifier scope (Okumura
and Muraki, 1994).
As with the sentiment recognition heuristics
(section 3.2), each predicts one interpretation of
the sentence with high precision, but potentially
low recall. Recall of the system is improved by
combining the heuristics, as described in the next
section. Note that for the first three of these
heuristics, Chantree et al (2006) use the British
National Corpus4, accessed via the Sketch Engine
(Kilgarriff et al, 2004), although a domain spe-
cific corpus could potentially be constructed.
4.1.3 Combining the heuristics
Chantree et al (2006) combine the heuristics
using the logistic regression algorithms contained
in the WEKA machine learning package (Witten
and Frank, 2005). The regression algorithm was
4http://www.natcorp.ox.ac.uk/
trained against the training data so that the text
was interpreted as nocuous either if there was ev-
idence for both wide and narrow modifier scope
or if there was no evidence for either.
This system performed reasonably for mid-
range ambiguity thresholds (around 50% < ? <
80%; for high and low thresholds, naive base-
lines give very high accuracy). However, in sub-
sequent work, Yang et al (2010b) have demon-
strated that by combining the results in a similar
way, but using the LogitBoost algorithm, signifi-
cant improvements can be gained over the logis-
tic regression approach. Their paper suggests that
LogitBoost provides an improvement in accuracy
of up to 21% in the range of interest for ? over
that of logistic regression.
We believe that this improvement reflects that
LogitBoost handles interacting variables better
than logistic regression, which assumes a linear
relationship between individual variables. This
supports our hybridisation method, which as-
sumes that the individual heuristics can interact.
In these cases, the heuristics bring into play dif-
ferent types of information (some structural, some
distributional, some morphological) where each
relies on partial information and favours one par-
ticular outcome over another. It would be unusual
to find strong evidence of both wide and narrow
scope modifier attachment from a single heuristic
and the effect of one heuristic can modulate, or
enhance the effect of another. This is supported by
Chantree et al?s (2006) observation that although
some of the proposed heuristics (such as the mor-
phology heuristic) perform poorly on their own,
their inclusion in the regression model does im-
prove the overall performance of the system
To conclude, comparing the results of Chantree
et al (2006) and Yang et al (2010b) demonstrates
that the technique of combining individual, high
precision heuristics is a successful one. However,
the combination function needs careful consider-
ation, and can have as large an effect on the final
results as the choice of the heuristics themselves.
4.2 Nocuous Ambiguity: Anaphora
As example (1) demonstrates, nocuous ambigu-
ity can occur where there are multiple possible
antecedents for an anaphor. Yang et al (2010a)
have addressed the task of nocuous ambiguity de-
tection for anaphora in requirements documents,
in sentences such as (4), where the pronoun it has
102
three potential antecedents (italicised).
(4) The procedure shall convert the 24 bit image
to an 8 bit image, then display it in a dynamic
window.
As with the coordination task, the aim is to
identify nocuous ambiguity, rather than attempt to
disambiguate the sentence.
4.2.1 The dataset
The data set used for the anaphora task con-
sisted of 200 sentences collected from require-
ments documents which contained a third person
pronoun and multiple possible antecedents. Each
instance was judged by at least 13 people.
The concept of ambiguity threshold, ? , remains
central to nocuous ambiguity for anaphora. The
definition remains the same as in section 4.1.1, so
that an anaphor displays innocuous ambiguity if
there is an antecedent that at least ? judges agree
on, and nocuous ambiguity otherwise. So if, say,
75% of the judges considered an 8 bit image to
be the correct antecedent in (4), then the sentence
would display nocuous ambiguity at ? = 80%,
but innocuous ambiguity at ? = 70%.
For innocuous cases, the potential antecedent
NP with certainty of at least ? is tagged as Y,
and all other NPs are tagged as N. For nocuous
cases, potential antecedents with ? greater than 0
are tagged as Q (questionable), or are tagged N
otherwise (? = 0, ie. unselected).
4.2.2 Selectional Heuristics
The approach to this task uses only one selec-
tion function (Naive Bayes), but uses the output
to support two different voting strategies. Twelve
heuristics (described fully in Yang et al (2010a))
fall broadly into three types which signal the like-
lihood that the NP is a possible antecedent:
linguistic such as whether the potential an-
tecedent is a definite or indefinite NP
contextual such as the potential antecedent?s re-
cency, and
statistical such as collocation frequencies.
To treat a sentence, the classifier is applied to
each of the potential antecedents and assigns a
pair of values: the first is the predicted class of
the antecedent (Y, N or Q), and the second is the
associated probability of that classification.
Given a list of class assignments to potential an-
tecedents with associated probabilities, a weak
positive threshold, WY , and a weak negative
threshold, WN :
if the list of potential antecedents contains:
one Y, no Q, one or more N
or
no Y, one Q, one or more N but no weak
negatives
or
one strong positive Y , any number of Q or N
then
the ambiguity is INNOCUOUS
else
the ambiguity is NOCUOUS
where a classification Y is strong positive if its
associated probability is greater than WY , and a
classification N is weak negative if its associated
probability is smaller than WN .
Figure 4: Combination function for nocuous anaphora
detection with weak thresholds
4.2.3 The combination function
As suggested previously, the choice of com-
bination function can strongly affect the system
performance, even on the same set of selectional
heuristics. Yang et al (2010a) demonstrate two
different combination functions which exploit the
selectional heuristics in different ways. Both
combination functions use a voting strategy.
The first voting strategy states that a sentence
exhibits innocuous ambiguity if either:
? there is a single antecedent labelled Y, and all
others are labelled N, or
? there is a single antecedent labelled Q, and
all others are labelled N.
The second strategy is more sophisticated, and
depends on the use of weak thresholds: intu-
itively, the aim is to classify the text as innocu-
ous if is (exactly) one clearly preferred antecedent
among the alternatives. The combination function
is shown in figure 4. The second clause states
that a single potential antecedent labelled Q can
be enough to suggest innocuous ambiguity if all
the alternatives are N with a high probability.
103
Model without Model with
weak thresholds weak thresholds
? P R F P R F
0.50 27.2 55.0 45.7 24.1 95.0 59.7
0.60 33.9 67.5 56.3 30.9 97.5 68.1
0.70 45.1 76.2 66.9 43.9 98.4 78.8
0.80 58.0 85.0 77.7 56.1 97.9 85.5
0.90 69.1 88.6 83.9 67.4 98.4 90.1
1.0 82.2 95.0 92.1 82.0 99.4 95.3
Table 4: Precision, Recall and f-measure (%) for the
two combination functions (anaphora)
Task Selectional
heuristics
Combination
functions
Sentiment CRF Voting
analysis NB - any
SVM - majority
ME - combined
Nocuous 3 distributional logistic
ambiguity metrics regression
(coordin-
ation) 4 others LogitBoost
Nocuous NB Voting
ambiguity
(anaphora) Voting
(+ threshold)
Table 5: Hybridisation approaches used
The performance of the two voting strategies
is shown in table 4. It is clear that the improved
overall performance of the strategy with weak
thresholds is due to the improved recall when the
functions are combined; the precision is compa-
rable in both cases. Again, this shows the desired
combinatorial behaviour; a combination of high
precision heuristics can yield good overall results.
5 Conclusion
The hybridised systems we have considered are
summarised in table 5. This examination suggests
that hybridisation can be a powerful technique for
classifying linguistic phenomena. However, there
is currently little guidance on principles regarding
hybrid system design. The studies here show that
there is room for more systematic study of the de-
sign principles underlying hybridisation, and for
investigating systematic methodologies.
This small scale study suggests several prin-
ciples. First, the sentiment analysis study has
shown that a set of heuristics and a suitable com-
bination function can outperform the best individ-
ually performing heuristic or technique. In partic-
ular, our results suggest that hybrid systems of the
kind described here are most valuable when there
is significant interaction between the various lin-
guistic phenomena present in the text. This occurs
both with nocuous ambiguity (where competition
between the different interpretations creates dis-
agreement overall), and with sentiment analysis
(where a sentence can convey multiple emotions).
As a result, hybridisation is particularly power-
ful where there are multiple competing factors, or
where it is unclear whether there is sufficient evi-
dence for a particular classification.
Second, successful hybrid systems can be built
using multiple heuristics, even if each of the
heuristics has low recall on its own. Our case
studies show that with the correct choice of hy-
bridisation functions, high precision heuristics
can be combined to give good overall recall while
maintaining acceptable overall precision.
Finally, the mode of combination matters. The
voting system is successful in the sentiment anal-
ysis task, where different outcomes are not exclu-
sive (the presence of guilt does not preclude the
presence of love). On the other hand, the log-
itBoost combinator is appropriate when the dif-
ferent interpretations are exclusive (narrow modi-
fier scope does preclude wide scope). Here, logit-
Boost can be interpreted as conveying the degree
of uncertainty among the alternatives. The coor-
dination ambiguity case demonstrates that the in-
dividual heuristics do not need to be independent,
but if the method of combining them assumes in-
dependence, the benefits of hybridisation will be
lost (logistic regression compared to LogitBoost).
This analysis has highlighted the interplay be-
tween task, heuristics and combinator. Currently,
the nature of this interplay is not well understood,
and we believe that there is scope for investigating
the broader range of hybrid systems that might be
applied to different tasks.
Acknowledgments
The authors would like to thank the UK Engi-
neering and Physical Sciences Research Coun-
cil who funded this work through the MaTREx
project (EP/F068859/1), and the anonymous re-
viewers for helpful comments and suggestions.
104
References
Francis Chantree, Bashar Nuseibeh, Anne De Roeck,
and Alistair Willis. 2006. Identifying nocuous
ambiguities in natural language requirements. In
Proceedings of 14th IEEE International Require-
ments Engineering conference (RE?06), Minneapo-
lis/St Paul, Minnesota, USA, September.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL
?09), pages 148?156.
Anne De Roeck. 2007. The role of data in NLP:
The case for dataset profiling. In Nicolas Nicolov,
Ruslan Mitkov, and Galia Angelova, editors, Re-
cent Advances in Natural Language Processing IV,
volume 292 of Current Issues in Linguistic Theory,
pages 259?266. John Benjamin Publishing Com-
pany, Amsterdam.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152?1161, Singapore, August.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009.
The effect of negation on sentiment analysis and
retrieval effectiveness. In The 18th ACM Confer-
ence on Information and Knowledge Management
(CIKM?09), Hong Kong, China, November.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The sketch engine. Technical Re-
port ITRI-04-08, University of Brighton.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning (ICML-2001),
pages 282?289.
Lillian Lee. 1999. Measures of distributional simi-
larity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics,
pages 25?32, College Park, Maryland, USA, June.
Association for Computational Linguistics.
Akitoshi Okumura and Kazunori Muraki. 1994. Sym-
metric pattern matching analysis for english coor-
dinate structures. In Proceedings of the 4th Con-
ference on Applied Natural Language Processing,
pages 41?46.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86, Philadelphia, July.
John P. Pestian, Pawel Matykiewicz, Michelle Linn-
Gust, Brett South, Ozlem Uzuner, Jan Wiebe,
K. Bretonnel Cohen, John Hurdle, and Christopher
Brew. 2012. Sentiment analysis of suicide notes:
A shared task. Biomedical Informatics Insights,
5(Suppl 1):3?16.
Mike Thelwall, Kevan Buckley, Georgios Paltoglou,
Di Cai, and Arvid Kappas. 2010. Sentiment in
short strength detection informal text. Journal of
the American Society for Information Science &
Technology, 61(12):2544?2558, December.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference sys-
tems based on kernels methods. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 961?968,
Manchester, August.
Alistair Willis, Francis Chantree, and Anne DeRoeck.
2008. Automatic identification of nocuous ambigu-
ity. Research on Language and Computation, 6(3-
4):355?374, December.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
Ian H. Witten and Eibe Frank. 2005. Data mining:
Practical machine learning tools and techniques.
Morgan Kaufmann, 2nd edition.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the ACL, pages 41?
48, Sydney, July.
Hui Yang, Anne De Roeck, Vincenzo Gervasi, Al-
istair Willis, and Bashar Nuseibeh. 2010a. Ex-
tending nocuous ambiguity analysis for anaphora
in natural language requirements. In 18th Interna-
tional IEEE Requirements Engineering Conference
(RE?10), Sydney, Australia, Oct.
Hui Yang, Anne De Roeck, Alistair Willis, and Bashar
Nuseibeh. 2010b. A methodology for automatic
identification of nocuous ambiguity. In 23rd Inter-
national Conference on Computational Linguistics
(COLING 2010), Beijing, China.
Hui Yang, Alistair Willis, Anne De Roeck, and Bashar
Nuseibeh. 2012. A hybrid model for automatic
emotion recognition in suicide notes. Biomedical
Informatics Insights, 5(Suppl. 1):17?30, January.
105
