Coling 2010: Poster Volume, pages 656?664,
Beijing, August 2010
Unsupervised Discriminative Language Model Training
for Machine Translation using Simulated Confusion Sets
Zhifei Li and Ziyuan Wang and Sanjeev Khudanpur and Jason Eisner
Center for Language and Speech Processing
Johns Hopkins University
zhifei.work@gmail.com,{zwang40, khudanpur, eisner}@jhu.edu
Abstract
An unsupervised discriminative training
procedure is proposed for estimating a
language model (LM) for machine trans-
lation (MT). An English-to-English syn-
chronous context-free grammar is derived
from a baseline MT system to capture
translation alternatives: pairs of words,
phrases or other sentence fragments that
potentially compete to be the translation
of the same source-language fragment.
Using this grammar, a set of impostor
sentences is then created for each En-
glish sentence to simulate confusions that
would arise if the system were to process
an (unavailable) input whose correct En-
glish translation is that sentence. An LM
is then trained to discriminate between
the original sentences and the impostors.
The procedure is applied to the IWSLT
Chinese-to-English translation task, and
promising improvements on a state-of-
the-art MT system are demonstrated.
1 Discriminative Language Modeling
A language model (LM) constitutes a crucial com-
ponent in many tasks such as machine translation
(MT), speech recognition, information retrieval,
handwriting recognition, etc. It assigns a pri-
ori probabilities to word sequences. In general,
we expect a low probability for an ungrammat-
ical or implausible word sequence. The domi-
nant LM used in such systems is the so-called
n-gram model, which is typically derived from a
large corpus of target language text via maximum
likelihood estimation, mitigated by some smooth-
ing or regularization. Due to the Markovian as-
sumptions implicit in n-gram models, however,
richer linguistic and semantic dependencies are
not well captured. Rosenfeld (1996) and Khu-
danpur and Wu (2000) address such shortcom-
ing by using maximum entropy models with long-
span features, while still working with a locally
normalized left-to-right LM. The whole-sentence
maximum entropy LM of Rosenfeld et al (2001)
proposes a globally normalized log-linear LM in-
corporating several sentence-wide features.
The n-gram as well as the whole-sentence
model are generative or descriptive models of
text. However, in a task like Chinese-to-English
MT, the de facto role of the LM is to discriminate
among the alternative English translations being
contemplated by the MT system for a particular
Chinese input sentence. We call the set of such
alternative translations a confusion set. Since a
confusion set is typically a minuscule subset of
the set of all possible word sequences, it is ar-
guably better to train the LM parameters so as to
make the best candidate in the confusion set more
likely than its competitors, as done by Roark et al
(2004) for speech recognition and by Li and Khu-
danpur (2008) for MT. Note that identifying the
best candidate requires supervised training data?
bilingual text in case of MT?which is expensive
in many domains (e.g. weblog or newsgroup) and
for most language pairs (e.g. Urdu-English).
We propose a novel discriminative LM in this
paper: a globally normalized log-linear LM that
can be trained in an efficient and unsupervised
manner, using only monolingual (English) text.
The main idea is to exploit (translation) un-
certainties inherent in an MT system to de-
rive an English-to-English confusion grammar
(CG), illustrated in this paper for a Hiero sys-
tem (Chiang, 2007). From the bilingual syn-
chronous context-free grammar (SCFG) used in
Hiero, we extract a monolingual SCFG, with rules
of the kind, X ? ?strong tea, powerful tea? or
656
X ? ?in X1, in the X1?. Thus our CG is also an
SCFG that generates pairs of English sentences
that differ from each other in ways that alterna-
tive English hypothesis considered during transla-
tion would differ from each other. This CG is then
used to ?translate? each sentence in the LM train-
ing corpus into what we call its confusion set ? a
set of other ?sentences? with which that sentence
would likely be confused by the MT system, were
it to be the target translation of a source-language
sentence. Sentences in the training corpus, each
paired with its confusion set, are then used to train
a discriminative LM to prefer the training sen-
tences over the alternatives in their confusion sets.
Since the monolingual CG and the bilingual
Hiero grammar are both SCFGs, the confusion
sets are isomorphic with translation hypergraphs
that are used by supervised discriminative train-
ing. The confusion sets thus simulate the super-
vised case, with a key exception: lack of any
(Chinese) source-language information. There-
fore, only target-side ?language model? probabil-
ities may be estimated from confusion sets.
We carry out this discriminative training proce-
dure, and empirically demonstrate promising im-
provements in translation quality.
2 Discriminative LM Training
2.1 Whole-sentence Maximum Entropy LM
We aim to train a globally normalized log-linear
language model p?(y) of the form
p?(y) = Z?1 ef(y)?? (1)
where y is an English sentence, f(y) is a vector
of arbitrary features of y, ? is the (weight) vec-
tor of model parameters, and Z def= ?y? ef(y?)?? is
a normalization constant. Given a set of English
training sentences {yi}, the parameters ? may be
chosen to maximize likelihood, as
?? = argmax
?
?
i
p?(yi). (2)
This is the so called whole-sentence maximum
entropy (WSME) language model1 proposed by
1Note the contrast with the maximum entropy n-gram
LM (Rosenfeld, 1996; Khudanpur and Wu, 2000), where the
normalization is performed for each n-gram history.
Rosenfeld et al (2001). Training the model of
(2) requires computing Z, a sum over all possible
word sequences y? with any length, which is com-
putationally intractable. Rosenfeld et al (2001)
approximate Z by random sampling.
2.2 Supervised Discriminative LM Training
In addition to the computational disadvantage, (2)
also has a modeling limitation. In particular, in
a task like MT, the primary role of the LM is to
discriminate among alternative translations of a
given source-language sentence. This set of alter-
natives is typically a minuscule subset of all pos-
sible target-language word sequences. Therefore,
a better way to train the global log-linear LM,
given bilingual text {(xi, yi)}, is to generate the
real confusion set N (xi) for each input sentence
xi using a specific MT system, and to adjust ? to
discriminate between the reference translation yi
and y? ? N (xi) (Roark et al, 2004; Li and Khu-
danpur, 2008).
For example, one may maximize the condi-
tional likelihood of the bilingual training data as
?? = argmax
?
?
i
p?(yi |xi) (3)
= argmax
?
?
i
ef(xi,yi)???
y??N (xi) ef(xi,y
?)?? ,
which entails summing over only the candidate
translations y? of the given input xi. Furthermore,
if the features f(xi, y) are depend on only the out-
put y, i.e. on the English-side features of the bilin-
gual text, the resulting discriminative model may
be interpreted as a language model.
Finally, in a Hiero style MT system, if f(xi, y)
depends on the target-side(s) of the bilingual rules
used to construct y from xi, we essentially have a
syntactic LM.
2.3 Unsupervised Discriminative Training
using Simulated Confusion Sets
While the supervised discriminative LM training
has both computational and modeling advantages
over the WSME LM, it relies on bilingual data,
which is expensive to obtain for several domains
and language pairs. For such cases, we propose
a novel discriminative language model, which is
657
still a global log-linear LM with the modeling ad-
vantage and computational efficiency of (3) but re-
quires only monolingual text {yi} for training ?.
Specifically, we propose to modify (3) as
?? = argmax
?
?
i
p?(yi | N (yi)) (4)
= argmax
?
?
i
ef(yi)???
y??N (yi) ef(y
?)?? ,
where N (yi) is a simulated confusion set for yi
obtained by applying a confusion grammar to yi,
as detailed in Section 3. Our hope is that N (yi)
resembles the actual confusion set N (xi) that an
MT system would generate if it were given the in-
put sentence xi.
Like (3), the maximum likelihood training of
(4) does not entail the expensive computation of a
global normalization constant Z, and is therefore
very efficient. Unlike (3) however, where the input
xi for each output yi is needed to create N (xi),
the model of (4) can be trained in an unsupervised
manner with only {yi}.
3 Unsupervised Discriminative Training
of the Language Model for MT
The following is thus the proposed procedure for
unsupervised discriminative training of the LM.
1. Extract a confusion grammar (CG) from the
baseline MT system.
2. ?Translate? each English sentence in the LM
training corpus, using the CG as an English-
to-English translation model, to generate a
simulated confusion set.
3. Train a discriminative language model on the
simulated confusion sets, using the corre-
sponding original English sentences as the
training references.
The trained model may then be used for actual MT
decoding. We next describe each step in detail.
3.1 Extracting a Confusion Grammar
We assume a synchronous context free grammar
(SCFG) formalism for the confusion grammar
(CG). While the SCFG used by the MT system
is bilingual, the CG we extract will be monolin-
gual, with both the source and target sides being
English. Some example CG rules are:
X ? ? strong tea , powerful tea ? ,
X ? ?X0 at beijing , beijing ?s X0 ? ,
X ? ?X0 of X1 , X0 of the X1 ? ,
X ? ?X0 ?s X1 , X1 of X0 ? .
Like a regular SCFG, a CG contains rules with
different ?arities? and reordering of the nontermi-
nals (as shown in the last example) capturing the
confusions that the MT system encounters when
choosing word senses, reordering patterns, etc.
3.1.1 Extracting a Confusion Grammar from
the Bilingual Grammar
The confusion grammar is derived from the MT
system?s bilingual grammar. In Hiero, the bilin-
gual rules are of the form X ? ?c, e?, where
both c and e may contain (a matched number of)
nonterminal symbols. For every c which appears
on the source-side of two different Hiero rules
X ? ?c, e1? and X ? ?c, e2?, we extract two CG
rules, X ? ?e1, e2? and X ? ?e2, e1?, to capture
the confusion the MT system would face were it
to encounter c in its input. For each Hiero rule
X ? ?c, e?, we also extract X ? ?e, e?, the iden-
tity rule. Therefore, if a pattern c appears with |E|
different translation options, we extract |E|2 dif-
ferent CG rules from c. In our current work, the
rules of the CG are unweighted.
3.1.2 Test-set Specific Confusion Grammars
If the bilingual grammar contains all the rules
that are extractable from the bilingual training cor-
pus, the resulting confusion grammar is likely to
be huge. As a way of reducing computation, the
bilingual grammar can be restricted to a specific
test set, and only rules used by the MT system for
translating the test set used for extracting the CG.2
To economize further, one may extract a CG
from the translation hypergraphs that are gener-
ated for the test-set. Recall that a node in a hy-
pergraph corresponds to a specific source (Chi-
nese) span, and the node has many incident hy-
peredges, each associated with a different bilin-
2Test-set specific CGs are of course only practical for off-
line applications.
658
gual rule. Therefore, all the bilingual rules asso-
ciated with the incoming hyperedges of a given
node translate the same Chinese string. At each
hypergraph node, we extract CG rules to represent
the competing English sides as described above.
Note that even though different rules associated
with a node may have different ?arity,? we extract
CG rules only from pairs of bilingual rules that
have the same arity.
A CG extracted from only the bilingual rule
pairs incident on the same node in the test hy-
pergraphs is, of course, much smaller than a CG
extracted from the entire bilingual grammar. It
is also more suitable for our task, since the test
hypergraphs have already benefited from a base-
line n-gram LM and pruning, removing all confu-
sions that are easily resolved (rightly or wrongly)
by other system components.
3.2 Generating Simulated Confusion Sets
For each English sentence y in the training cor-
pus, we use the extracted CG to produce a simu-
lated confusion set N (y). This is done like a reg-
ular MT decoding pass, because we can treat the
CG as a Hiero style ?translation? grammar3 for an
English-to-English translation system.
Since the CG is an SCFG, the confusion set
N (y) generated for a sentence y is a hypergraph,
encoding not only the alternative sentences y? but
also the hierarchical derivation tree for each y?
from y (e.g., which phrase in y has been re-
placed with what in y?). As usual, many differ-
ent derivation trees d may correspond to the same
string/sentence y? due to spurious ambiguity. We
use D(y) to denote the set of derivations d, which
is a hypergraph representation of N (y).
Figure 1 presents an example confusion hy-
pergraph for the English sentence y =?a cat on
the mat,? containing four alternative hypotheses:
3To make sure that we produce at least one derivation tree
for each y, we need to add to the CG the following two glue
rules, as done in Hiero (Chiang, 2007).
S ? ?X0 , X0 ? ,
S ? ?S0 X1 , S0 X1 ? .
We also add an out of vocabulary rule X ? ?word, oov? for
each word in y and set the cost of this rule to a high value so
that the OOV rule will get used only when the CG does not
know how to ?translate? the word.
X ? ? a cat , the cat ?
X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ?
X ? ?X0 on X1 , X1 on X0 ?
X ? ?X0 on X1 , X1 of X0 ?
S ? ?X0 , X0 ?
(a) An example confusion grammar.
a
0
  cat
1             
       on
2
         the
3
 mat
4
S??X0,X0?
X
0,5
X
0,2
X
3,5
X ? ? a cat , the cat ? X ? ? the mat , the mat ?
X ? ?X0 on X1 , X0 X1 ?
X ? ?X0 on X1 , X0 ?s X1 ? X ? ?X0 on X1 , X1 of X0 ?
X ? ?X0 on X1 , X1 on X0 ?
S
0,5
(b) An example hypergraph generated by the confusion
grammar of (a) for the input sentence ?a cat on the mat.?
Figure 1: Example confusion grammar and simulated
confusion hypergraph. Given an input sentence y = ?a cat
on the mat,? the confusion grammar of (a) generates a hyper-
graph D(y) shown in (b), which represents the confusion set
N (y) containing four alternative sentences y?.
N (y) = { ?the cat the mat,? ?the cat ?s the mat,?
?the mat of the cat,? ?the mat on the cat?}.
Notice that each competitor y? ? N (y) can be
regarded as the result of a ?round-trip? translation
y ? x ? y?, in which we reconstruct a possible
Chinese source sentence x that our Hiero bilin-
gual grammar could translate into both y and y?.4
We will train our LM to prefer y, which was ac-
tually observed. Our CG-based round-trip forces
x? y? to use the same hierarchical segmentation
of x as y ? x did. This constraint leads to effi-
cient training but artificially reduces the diversity
4This is because of the way we construct our CG from the
Hiero grammar. However, the identity and glue rules in our
CG allow almost any portion of y to be preserved untrans-
lated through the entire y ? x ? y? process. Much of y
will necessarily be preserved in the situation where the CG is
extracted from a small test set and hence has few non-identity
rules. See (Li, 2010) for further discussion.
659
ofN (y). In other recent work (Li et al, 2010), we
have taken the round-trip view more seriously, by
imputing likely source sentences x and translating
them back to separate, weighted confusion forests
N (y), without any same-segmentation constraint.
3.3 Confusion-based Discriminative Training
With the training sentences yi and their simulated
confusion sets N (yi) ? represented as hyper-
graphs D(yi)) ? we can perform the discrimi-
native training using any of a number of proce-
dures such as MERT (Och, 2003) or MIRA as
used by Chiang et al (2009). In our paper, we
use hypergraph-based minimum risk (Li and Eis-
ner, 2009),
?? = argmin
?
?
i
Risk?(yi) (5)
= argmin
?
?
i
?
d?D(yi)
L(Y(d), yi)p?(d |D(yi)),
where L(y?, yi) is the loss (e.g negated BLEU) in-
curred by producing y? when the true answer is yi,
Y(d) is the English yield of a derivation d, and
p?(d |D(yi)) is defined as,
p?(d |D(yi)) = e
f(d)??
?
d?D(yi) ef(d)??
, (6)
where f(d) is a feature vector over d. We will
specify the features in Section 5, but in general
they should be defined such that the training will
be efficient and the actual MT decoding can use
them conveniently.
The objective of (5) is differentiable and thus
we can optimize ? by a gradient-based method.
The risk and its gradient on a hypergraph can
be computed by using a second-order expectation
semiring (Li and Eisner, 2009).
3.3.1 Iterative Training
In practice, the full confusion set N (y) defined
by a confusion grammar may be too large and we
have to perform pruning when training our model.
But the pruning itself may depend on the model
that we aim to train. How do we solve this circu-
lar dependency problem? We adopt the following
procedure. Given an initial model ?, we generate a
hypergraph (with pruning) for each y, and train an
optimal ?? of (5) on these hypergraphs. Then, we
use the optimal ?? to regenerate a hypergraph for
each y, and do the training again. This iterates un-
til convergence. This procedure is quite similar to
the k-best MERT (Och, 2003) where the training
involves a few iterations, and each iteration uses a
new k-best list generated using the latest model.
3.4 Applying the Discriminative LM
First, we measure the goodness of our language
model in a simulated task. We generate simulated
confusion sets N (y) for some held out English
sentences y, and test how well p?(d |D(y)) can
recover y from N (y). This is merely a proof of
concept, and may be useful in deciding which fea-
tures f(d) to employ for discriminative training.
The intended use of our model is, of course, for
actual MT decoding (e.g., translating Chinese to
English). Specifically, we can add the discrimina-
tive model into an MT pipeline as a feature, and
tune its weight relative to other models in the MT
system, including the baseline n-gram LM.
4 Related and Similar Work
The detailed relation between the proposed pro-
cedure and other language modeling techniques
has been discussed in Sections 1 and 2. Here, we
review two other methods that are related to our
method in a broader context.
4.1 Unsupervised Training of Global
Log-linear Models
Our method is similar to the contrastive estimation
(CE) of Smith and Eisner (2005) and its succes-
sors (Poon et al, 2009). In particular, our confu-
sion grammar is like a neighborhood function in
CE. Also, our goal is to improve both efficiency
and accuracy, just as CE does. However, there
are two important differences. First, the neigh-
borhood function in CE is manually created based
on human insights about the particular task, while
our neighborhood function, generated by the CG,
is automatically learnt (e.g., from the bilingual
grammar) and specific to the MT system being
used. Therefore, our neighborhood function is
more likely to be informative and adaptive to the
task. Secondly, when tuning ?, CE uses the maxi-
mum likelihood training, but we use the minimum
660
risk training of (5). Since our training uses a task-
specific loss function, it is likely to perform better
than maximum likelihood training.
4.2 Paraphrasing Models
Our method is also related to methods for train-
ing paraphrasing models (Quirk et al, 2004; Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Madnani et al, 2007). Specifically, the
form of our confusion grammar is similar to that
of the paraphrase model they use, and the ways
of extracting the grammar/model are also similar
as both employ a second language (e.g., Chinese
in our case) as a pivot. However, while a ?trans-
lation? rule in a paraphrase model is expected to
contain a pair of phrases that are good alterna-
tives for each other, a confusion rule in our CG
is based on an MT system processing unseen test
data and contains pairs of phrases that are typi-
cally bad (and only rarely good) alternatives for
each other.
The motivation and goal are also different. For
example, the goal of Bannard and Callison-Burch
(2005) is to extract paraphrases with the help of
parallel corpora. Callison-Burch et al (2006) aim
to improve MT quality by adding paraphrases in
the translation table, while Madnani et al (2007)
aim to improve the minimum error rate training by
adding the automatically generated paraphrases
into the English reference sets. In contrast, our
motivation is to train a discriminative language
model to improve MT (by using the confusion
grammar to decide what alternatives the model
should learn to discriminate).
5 Experimental Results
We have applied the confusion-based discrimina-
tive language model (CDLM) to the IWSLT 2005
Chinese-to-English text translation task5 (Eck and
Hori, 2005). We see promising improvements
over an n-gram LM for a solid Joshua-based
baseline system (Li et al, 2009).
5.1 Data Partitions for Training & Testing
Four kinds of data are used for CDLM training:
5This is a relatively small task compared to, say, the NIST
MT tasks. We worked on it for a proof-of-concept. Having
been successful, we are now investigating larger MT tasks.
# sentencesData Usage ZH EN
Set1 TM & LM training 40k 40k
Set2 Min-risk training 1006 1006?16
Set3 CDLM training ? 1006?16
Set4 Test 506 506?16
Table 1: Data sets used. Set1 contains translation-equivalent
Chinese-English sentence pairs, while for each Chinese sen-
tence in Set2 and Set4, there are 16 English translations. Set3
happens to be the English side of Set2 due to lack of ad-
ditional in-domain English text, but this is not noteworthy;
Set3 could be any in-domain target-language text corpus.
Set1 a bilingual training set on which 10 individ-
ual MT system components are trained,
Set2 a small bilingual, in-domain set for tuning
relative weights of the system components,
Set3 an in-domain monolingual target-language
corpus for CDLM training, and
Set4 a test set on which improvements in MT per-
formance is measured.
We partition the IWSLT data into four such sub-
sets as listed in Table 1.
5.2 Baseline MT System
Our baseline translation model components are
estimated from 40k pairs of utterances from the
travel domain, called Set1 in Table 1. We use a 5-
gram language model with modified Kneser-Ney
smoothing (Chen and Goodman, 1998), trained on
the English side of Set1, as our baseline LM.
The baseline MT system comprises 10 com-
ponent models (or ?features?) that are standard
in Hiero (Chiang, 2007), namely the baseline
language model (BLM) feature, three baseline
translation model features, one word-insertion
penalty (WP) feature, and five arity features ?
three to count how many rules with an arity of
zero/one/two are used in a derivation, and two
to count how many times the unary and binary
glue rules are used in a derivation. The rela-
tive weights of these 10 features are tuned via
hypergraph-based minimum risk training (Li and
Eisner, 2009) on the bilingual data Set2.
The resulting MT system gives a BLEU score of
48.5% on Set4, which is arguably a solid baseline.
661
5.3 Unsupervised Training of the CDLM
We extract a test-set specific CG from the hyper-
graphs obtained by decoding Set2 and Set4, as de-
scribed in Section 3.1.2. The number of rules in
the bilingual grammar and the CG are about 167k
and 1583k respectively. The CG is used as the
?translation? model to generate confusion hyper-
graphs for sentences in Set3.
Two CDLMs, corresponding to different fea-
ture sets f(d) in equation (6), were trained.
Only n-gram LM Features: We consider a
CDLM with only two features f(d): a base-
line LM feature (BLM) that equals the 5-
gram probability of Y(d) and a word penalty
feature (WP) equal to the length of Y(d).
Target-side Rule Bigram Features6: For each
CG rule used in d, we extract counts of bi-
grams that appear on the target-side of the
CG rule. For example, if the confusion rule
X ? ?X0 of X1 , X0 of the X1 ? is used in
d, the bigram features in f(d) whose counts
are incremented are: ?X of,? ?of the? and
?the X .?7 Note that the indices on the non-
terminals in the rule have been removed. To
avoid very rare features, we only consider
the 250 most freqent terminal symbol (En-
glish words) in the English of Set1 and map
all other terminal symbols into a single class.
Finally, we replace the identities of words
with their dominant POS tags. These restric-
tions result in 525 target-side rule bigram
(TsRB) features f(d) in the model of (6).
For each choice of the feature vector f(d), be it
2- or 527-dimensional, we use the training proce-
dure of Section 3.3.1 to iteratively minimize the
objective of (5) and get the CDLM parameter ??.
Note that each English sentence in Set3 has 15
other paraphrases. We generate a separate confu-
sion hypergraph D(y) for each English sentence
y, but for each such hypergraph we use both y
and its 15 paraphrases as ?reference translations?
when computing the risk L(Y(d), {y}) in (5).8
6Note that these features are novel in MT.
7With these target-side rule-based features, our LM is es-
sentially a syntactic LM, not just an LM on English strings.
8We take unfair advantage of this unusual dataset to com-
5.4 Results on Monolingual Simulation
We first probe how our novel CDLM performs as
a language model itself. One usually uses the per-
plexity of the LM on some unseen text to measure
its goodness. But since we did not optimize the
CDLM for likelihood, we instead examine how
it performs in discriminating between a good En-
glish sentence and sentences with which the MT
system may confuse that sentence. The test is per-
formed as follows. For each test English sentence
y of Set4, the confusion grammar defines a full
confusion set N (y) via a hypergraph D(y). We
use a LM to pick the most likely y? from N (y),
and then compute its BLEU score by using y and
its 15 paraphrase sentences as references. The
higher the BLEU, the better is the LM in picking
out a good translation from N (y).
Table 2 shows the results9 under a regular n-
gram LM and the two CDLMs described in Sec-
tion 5.3.
The baseline LM (BLM) entails no weight op-
timization a la (5) on Set3. The CDLM with the
BLM and word pentaly (WP) features improves
over the baseline LM. Compared to either of them,
the CDLM with the target-side rule bigram fea-
tures (TsRB) performs dramatically better.
5.5 Results on MT Test Data
We now examine how our CDLM performs during
actual MT decoding. To incorporate the CDLM
into MT decoding, we add the log-probability (6)
of a derivation d under the CDLM as an additional
bat an unrelated complication?a seemingly problematic in-
stability in the minimum risk training procedure.
As an illustration of this problem, we note that in super-
vised tuning of the baseline MT system (|f(d)|=10) with
500 sentences from Set2, the BLEU score on Set4 varies from
38.6% to 44.2% to 47.8% if we use 1, 4 and 16 reference
translations during the supervised training respectively. We
choose a system tuned on 16 references on Set2 as our base-
line. In order not to let the unsupervised CDLM training
suffer from this unrelated limitation of the tuning procedure,
we give it too the benefit of being able to compute risk on
Set3 using y plus its 15 paraphrases.
We wish to emphasize that this trait of Set3 having 15
paraphrases for each sentence is otherwise unnecessary, and
does not detract much from the main claim of this paper.
9Note that the scores in Table 2 are very low compared to
scores for actual translation from Chinese shown in Table 3.
This is mainly because in this monolingual simulation, the
LM is the only model used to rank the y? ? N (y). Said dif-
ferently, y? is being chosen in Table 2 entirely for its fluency
with no consideration whatsoever for its adequacy.
662
LM used for Features used BLEU
rescoring BLM WP TsRB on Set4
Baseline LM X 12.8
CDLM X X 14.2
CDLM X X X 25.3
Table 2: BLEU scores in monolingual simulations. Rescor-
ing the confusion sets of English sentences created using the
CG shows that the CDLM with TsRB features recovers hy-
potheses much closer to the sentence that generated the con-
fusion set than does the baseline n-gram LM.
Model used Features used BLEU
for rescoring 10 models TsRB on Set4
Joshua X 48.5
+ CDLM X X 49.5
Table 3: BLEU scores on the test set. The baseline MT sys-
tem has ten models/features, and the proposed system has
one additional model, the CDLM. Note that for the CDLM,
only the TsRB features are used during MT decoding.
feature, on top of the 10 features already present
in baseline MT system (see Section 5.2). We then
(re)tune relative weights for these 11 features on
the bilingual data Set2 of Table 1.
Note that the MT system also uses the BLM and
WP features whose weights are now retuned on
Set2. Therefore, when integrating a CDLM into
MT decoding, it is mathematically equivalent to
use only the TsRB features of the CDLM, with
the corresponding weights as estimated alongside
its ?own? BLM and WP features during unsuper-
vised discriminative training on Set3.
Table 3 reports the results. A BLEU score im-
provement of 1% is seen, reinforcing the claim
that the unsupervised CDLM helps select better
translations from among the system?s alternatives.
5.6 Goodness of Simulated Confusion Sets
The confusion set N (y) generated by applying
the CG to an English sentence y aims to simulate
the real confusion set that would be generated by
the MT system if the system?s input was the Chi-
nese sentence whose English translation is y. We
investigate, in closing, how much the simulated
confusion set resembles to the real one. Since
we know the actual input-output pairs (xi, yi) for
Set4, we generate two confusion sets: the simu-
lated set N (yi) and the real one N (xi).
One way to measure the goodness of N (yi) as
a proxy for N (xi), is to extract the n-gram types
n-gram Precision Recall
unigram 36.5% 48.2%
bigram 10.1% 12.8%
trigram 3.7% 4.6%
4-gram 2.0% 2.4%
Table 4: n-gram precision and recall of simulated con-
fusion sets relative to the true confusions when translating
Chinese sentences. The n-grams are collected from k-best
strings in both cases, with k = 100. The precision and recall
change little when varying k.
witnessed in the two sets, and compute the ratio of
the number of n-grams in the intersection to the
number in their union. Another is to measure the
precision and recall of N (yi) relative to N (xi).
Table 4 presents such precision and recall fig-
ures. For convenience, the n-grams are collected
from the 100-best strings, instead of the hyper-
graph D(yi) and D(xi). Observe that the sim-
ulated confusion set does a reasonably good job
on the real unigram confusions but the simulation
needs improving for higher order n-grams.
6 Conclusions
We proposed a novel procedure to discrimina-
tively train a globally normalized log-linear lan-
guage model for MT, in an efficient and unsu-
pervised manner. Our method relies on the con-
struction of a confusion grammar, an English-to-
English SCFG that captures translation alterna-
tives that an MT system may face when choosing
a translation for a given input. For each English
training sentence, we use this confusion gram-
mar to generate a simulated confusion set, from
which we train a discriminative language model
that will prefer the original English sentence over
sentences in the confusion set. Our experiments
show that the novel CDLM picks better alterna-
tives than a regular n-gram LM from simulated
confusion sets, and improves performance in a
real Chinese-to-English translation task.
7 Acknowledgements
This work was partially supported by the National
Science Foundation via grants No? SGER-0840112
and RI-0963898, and by the DARPA GALE pro-
gram. The authors thank Brian Roark and Dami-
anos Karakos for insightful discussions.
663
References
Bannard, Colin and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, pages
597?604, Morristown, NJ, USA. Association for
Computational Linguistics.
Callison-Burch, Chris, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 17?
24, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Chen, Stanley F. and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report.
Chiang, David, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Eck, Matthias and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In Proc. of the
International Workshop on Spoken Language Trans-
lation.
Khudanpur, Sanjeev and Jun Wu. 2000. Maximum en-
tropy techniques for exploiting syntactic, semantic
and collocational dependencies in language model-
ing. In Computer Speech and Language, number 4,
pages 355?372.
Li, Zhifei and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
40?51, Singapore, August. Association for Compu-
tational Linguistics.
Li, Zhifei and Sanjeev Khudanpur. 2008. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In AMTA, pages 133?142.
Li, Zhifei, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In WMT09,
pages 26?30.
Li, Zhifei, Ziyuan Wang, Jason Eisner, and Sanjeev
Khudanpur. 2010. Minimum imputed risk training
for machine translation. In review.
Li, Zhifei. 2010. Discriminative training and varia-
tional decoding in machine translation via novel al-
gorithms for weighted hypergraphs. PHD Disserta-
tion, Johns Hopkins University.
Madnani, Nitin, Necip Fazil Ayan, Philip Resnik, and
Bonnie J. Dorr. 2007. Using paraphrases for pa-
rameter tuning in statistical machine translation. In
Proceedings of the Workshop on Statistical Machine
Translation, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL, pages
160?167.
Poon, Hoifung, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, pages 209?217, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
Quirk, Chris, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for para-
phrase generation. In In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 142?149.
Roark, Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Meeting of the Association for Computational Lin-
guistics (ACL?04), Main Volume, pages 47?54,
Barcelona, Spain, July.
Rosenfeld, Roni, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Rosenfeld, Roni. 1996. A maximum entropy approach
to adaptive statistical language modeling. In Com-
puter Speech and Language, number 3, pages 187?
228.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics (ACL 2005), Ann Arbor, Michi-
gan.
664
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 920?929,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Minimum Imputed Risk: Unsupervised Discriminative Training for
Machine Translation
Zhifei Li?
Google Research
Mountain View, CA 94043, USA
zhifei.work@gmail.com
Ziyuan Wang, Sanjeev Khudanpur
Johns Hopkins University
Baltimore, MD 21218, USA
zwang40,khudanpur@jhu.edu
Jason Eisner
Johns Hopkins University
Baltimore, MD 21218, USA
eisner@jhu.edu
Brian Roark
Oregon Health & Science University
Beaverton, Oregon 97006, USA
roark@cslu.ogi.edu
Abstract
Discriminative training for machine transla-
tion has been well studied in the recent past.
A limitation of the work to date is that it relies
on the availability of high-quality in-domain
bilingual text for supervised training. We
present an unsupervised discriminative train-
ing framework to incorporate the usually plen-
tiful target-language monolingual data by us-
ing a rough ?reverse? translation system. Intu-
itively, our method strives to ensure that prob-
abilistic ?round-trip? translation from a target-
language sentence to the source-language and
back will have low expected loss. Theoret-
ically, this may be justified as (discrimina-
tively) minimizing an imputed empirical risk.
Empirically, we demonstrate that augment-
ing supervised training with unsupervised data
improves translation performance over the su-
pervised case for both IWSLT and NIST tasks.
1 Introduction
Missing data is a common problem in statistics when
fitting the parameters ? of a model. A common strat-
egy is to attempt to impute, or ?fill in,? the missing
data (Little and Rubin, 1987), as typified by the EM
algorithm. In this paper we develop imputation tech-
niques when ? is to be trained discriminatively.
We focus on machine translation (MT) as our ex-
ample application. A Chinese-to-English machine
translation system is given a Chinese sentence x and
? Zhifei Li is currently working at Google Research, and
this work was done while he was a PHD student at Johns Hop-
kins University.
asked to predict its English translation y. This sys-
tem employs statistical models p?(y | x) whose pa-
rameters ? are discriminatively trained using bilin-
gual sentence pairs (x, y). But bilingual data for
such supervised training may be relatively scarce for
a particular language pair (e.g., Urdu-English), es-
pecially for some topics (e.g., technical manuals) or
genres (e.g., blogs). So systems seek to exploit ad-
ditional monolingual data, i.e., a corpus of English
sentences y with no corresponding source-language
sentences x, to improve estimation of ?. This is our
missing data scenario.1
Discriminative training of the parameters ? of
p?(y | x) using monolingual English data is a cu-
rious idea, since there is no Chinese input x to trans-
late. We propose an unsupervised training approach,
called minimum imputed risk training, which is con-
ceptually straightforward: First guess x (probabilis-
tically) from the observed y using a reverse English-
to-Chinese translation model p?(x | y). Then train
the discriminative Chinese-to-English model p?(y |
x) to do a good job at translating this imputed x
back to y, as measured by a given performance met-
ric. Intuitively, our method strives to ensure that
probabilistic ?round-trip? translation from a target-
language sentence to the source-language and back
again will have low expected loss.
Our approach can be applied in an application
scenario where we have (1) enough out-of-domain
bilingual data to build two baseline translation sys-
tems, with parameters ? for the forward direction,
and ? for the reverse direction; (2) a small amount
1Contrast this with traditional semi-supervised training that
looks to exploit ?unlabeled? inputs x, with missing outputs y.
920
of in-domain bilingual development data to discrim-
inatively tune a small number of parameters in ?;
and (3) a large amount of in-domain English mono-
lingual data.
The novelty here is to exploit (3) to discrimina-
tively tune the parameters ? of all translation model
components,2 p?(y|x) and p?(y), not merely train a
generative language model p?(y), as is the norm.
Following the theoretical development below, the
empirical effectiveness of our approach is demon-
strated by replacing a key supervised discriminative
training step in the development of large MT sys-
tems ? learning the log-linear combination of sev-
eral component model scores (viewed as features) to
optimize a performance metric (e.g. BLEU) on a set
of (x, y) pairs ? with our unsupervised discrimina-
tive training using only y. One may hence contrast
our approach with the traditional supervised meth-
ods applied to the MT task such as minimum error
rate training (Och, 2003; Macherey et al, 2008), the
averaged Perceptron (Liang et al, 2006), maximum
conditional likelihood (Blunsom et al, 2008), min-
imum risk (Smith and Eisner, 2006; Li and Eisner,
2009), and MIRA (Watanabe et al, 2007; Chiang et
al., 2009).
We perform experiments using the open-source
MT toolkit Joshua (Li et al, 2009a), and show that
adding unsupervised data to the traditional super-
vised training setup improves performance.
2 Supervised Discriminative Training via
Minimization of Empirical Risk
Let us first review discriminative training in the su-
pervised setting?as used in MERT (Och, 2003) and
subsequent work.
One wishes to tune the parameters ? of some
complex translation system ??(x). The function ??,
which translates Chinese x to English y = ??(x)
need not be probabilistic. For example, ? may be
the parameters of a scoring function used by ?, along
with pruning and decoding heuristics, for extracting
a high-scoring translation of x.
The goal of discriminative training is to mini-
mize the expected loss of ??(?), under a given task-
specific loss function L(y?, y) that measures how
2Note that the extra monolingual data is used only for tuning
the model weights, but not for inducing new phrases or rules.
bad it would be to output y? when the correct output
is y. For an MT system that is judged by the BLEU
metric (Papineni et al, 2001), for instance, L(y?, y)
may be the negated BLEU score of y? w.r.t. y. To be
precise, the goal3 is to find ? with low Bayes risk,
?? = argmin
?
?
x,y
p(x, y)L(??(x), y) (1)
where p(x, y) is the joint distribution of the input-
output pairs.4
The true p(x, y) is, of course, not known and,
in practice, one typically minimizes empirical risk
by replacing p(x, y) above with the empirical dis-
tribution p?(x, y) given by a supervised training set
{(xi, yi), i = 1, . . . , N}. Therefore,
?? = argmin
?
?
x,y
p?(x, y)L(??(x), y)
= argmin
?
1
N
N?
i=1
L(??(xi), yi). (2)
The search for ?? typically requires the use of nu-
merical methods and some regularization.5
3 Unsupervised Discriminative Training
with Missing Inputs
3.1 Minimization of Imputed Risk
We now turn to the unsupervised case, where we
have training examples {yi} but not their corre-
sponding inputs {xi}. We cannot compute the sum-
mand L(??(xi), yi) for such i in (2), since ??(xi)
requires to know xi. So we propose to replace
3This goal is different from the minimum risk training of
Li and Eisner (2009) in a subtle but important way. In both
cases, ?? minimizes risk or expected loss, but the expectation
is w.r.t. different distributions: the expectation in Li and Eisner
(2009) is under the conditional distribution p(y |x), while the
expectation in (1) is under the joint distribution p(x, y).
4In the terminology of statistical decision theory, p(x, y) is
a distribution over states of nature. We seek a decision rule
??(x) that will incur low expected loss on observations x that
are generated from unseen states of nature.
5To compensate for the shortcut of using the unsmoothed
empirical distribution rather than a posterior estimate of p(x, y)
(Minka, 2000), it is common to add a regularization term ||?||22
in the objective of (2). The regularization term can prevent over-
fitting to a training set that is not large enough to learn all pa-
rameters.
921
L(??(xi), yi) with the expectation
?
x
p?(x | yi)L(??(x), yi), (3)
where p?(? | ?) is a ?reverse prediction model? that
attempts to impute the missing xi data. We call the
resulting variant of (2) the minimization of imputed
empirical risk, and say that
?? = argmin
?
1
N
N?
i=1
?
x
p?(x | yi)L(??(x), yi) (4)
is the estimate with the minimum imputed risk6.
The minimum imputed risk objective of (4) could
be evaluated by brute force as follows.
1. For each unsupervised example yi, use the re-
verse prediction model p?(? | yi) to impute pos-
sible reverse translations Xi = {xi1, xi2, . . .},
and add each (xij , yi) pair, weighted by
p?(xij | yi) ? 1, to an imputed training set .
2. Perform the supervised training of (2) on the
imputed and weighted training data.
The second step means that we must use ?? to
forward-translate each imputed xij , evaluate the loss
of the translations y?ij against the corresponding true
translation yi, and choose the ? that minimizes the
weighted sum of these losses (i.e., the empirical risk
when the empirical distribution p?(x, y) is derived
from the imputed training set). Specific to our MT
task, this tries to ensure that probabilistic ?round-
trip? translation, from the target-language sentence
yi to the source-language and back again, will have
a low expected loss.7
The trouble with this method is that the reverse
model p? generates a weighted lattice or hyper-
graph Xi encoding exponentially many translations
of yi, and it is computationally infeasible to forward-
translate each xij ? Xi. We therefore investigate
several approximations to (4) in Section 3.4.
6One may exploit both supervised data {(xi, yi)} and unsu-
pervised data {yj} to perform semi-supervised training via an
interpolation of (2) and (4). We will do so in our experiments.
7Our approach may be applied to other tasks as well. For
example, in a speech recognition task, ?? is a speech recognizer
that produces text, whereas p? is a speech synthesizer that must
produce a distribution over audio (or at least over acoustic fea-
tures or phone sequences) (Huang et al, 2010).
3.2 The Reverse Prediction Model p?
A crucial ingredient in (4) is the reverse prediction
model p?(?|?) that attempts to impute the missing xi.
We will train this model in advance, doing the best
job we can from available data, including any out-
of-domain bilingual data as well as any in-domain
monolingual data8 x.
In the MT setting, ?? and p? may have similar pa-
rameterization. One translates Chinese to English;
the other translates English to Chinese.
Yet the setup is not quite symmetric. Whereas ??
is a translation system that aims to produce a single,
low-loss translation, the reverse version p? is rather
a probabilistic model. It is supposed to give an accu-
rate probability distribution over possible values xij
of the missing input sentence xi. All of these val-
ues are taken into account in (4), regardless of the
loss that they would incur if they were evaluated for
translation quality relative to the missing xi.
Thus, ? does not need to be trained to minimize
the risk itself (so there is no circularity). Ideally,
it should be trained to match the underlying condi-
tional distribution of x given y, by achieving a low
conditional cross-entropy
H(X |Y ) = ?
?
x,y
p(x, y) log p?(x | y). (5)
In practice, ? is trained by (empirically) minimiz-
ing ? 1M
?N
j=1 log p?(xj | yj) + 12?2 ???22 on some
bilingual data, with the regularization coefficient ?2
tuned on held out data.
It may be tolerable for p? to impute mediocre
translations xij . All that is necessary is that the (for-
ward) translations generated from the imputed xij
?simulate? the competing hypotheses that we would
see when translating the correct Chinese input xi.
3.3 The Forward Translation System ?? and
The Loss Function L(??(xi), yi)
The minimum empirical risk objective of (2) is
quite general and various popular supervised train-
ing methods (Lafferty et al, 2001; Collins, 2002;
Och, 2003; Crammer et al, 2006; Smith and Eisner,
8In a translation task from x to y, one usually does not make
use of in-domain monolingual data x. But we can exploit x to
train a language model p?(x) for the reverse translation system,
which will make the imputed xij look like true Chinese inputs.
922
2006) can be formalized in this framework by choos-
ing different functions for ?? and L(??(xi), yi). The
generality of (2) extends to our minimum imputed
risk objective of (4). Below, we specify the ?? and
L(??(xi), yi) we considered in our investigation.
3.3.1 Deterministic Decoding
A simple translation rule would define
??(x) = argmax
y
p?(y |x) (6)
If this ??(x) is used together with a loss function
L(??(xi), yi) that is the negated BLEU score9, our
minimum imputed risk objective of (4) is equivalent
to MERT (Och, 2003) on the imputed training data.
However, this would not yield a differentiable ob-
jective function. Infinitesimal changes to ? could re-
sult in discrete changes to the winning output string
??(x) in (6), and hence to the loss L(??(x), yi). Och
(2003) developed a specialized line search to per-
form the optimization, which is not scalable when
the number of model parameters ? is large.
3.3.2 Randomized Decoding
Instead of using the argmax of (6), we assume
during training that ??(x) is itself random, i.e. the
MT system randomly outputs a translation y with
probability p?(y |x). As a result, we will modify
our objective function of (4) to take yet another ex-
pectation over the unknown y. Specifically, we will
replace L(??(x), yi) in (4) with
?
y
p?(y |x)L(y, yi). (7)
Now, the minimum imputed empirical risk objective
of (4) becomes
?? = argmin
?
1
N
N?
i=1
?
x,y
p?(x | yi) p?(y |x)L(y, yi)
(8)
If the loss function L(y, yi) is a negated BLEU, this
is equivalent to performing minimum-risk training
described by (Smith and Eisner, 2006; Li and Eisner,
2009) on the imputed data.10
9One can manipulate the loss function to support other
methods that use deterministic decoding, such as Perceptron
(Collins, 2002) and MIRA (Crammer et al, 2006).
10Again, one may manipulate the loss function to support
other probabilistic methods that use randomized decoding, such
as CRFs (Lafferty et al, 2001).
The objective function in (8) is now differentiable,
since each coefficient p?(y |x) is a differentiable
function of ?, and thus amenable to optimization
by gradient-based methods; we use the L-BFGS al-
gorithm (Liu et al, 1989) in our experiments. We
perform experiments with the syntax-based MT sys-
tem Joshua (Li et al, 2009a), which implements
dynamic programming algorithms for second-order
expectation semirings (Li and Eisner, 2009) to effi-
ciently compute the gradients needed for optimizing
(8).
3.4 Approximating p?(x | yi)
As mentioned at the end of Section 3.1, it is com-
putationally infeasible to forward-translate each of
the imputed reverse translations xij . We propose
four approximations that are computationally feasi-
ble. Each may be regarded as a different approxima-
tion of p?(x | yi) in equations (4) or (8).
k-best. For each yi, add to the imputed training set
only the k most probable translations {xi1, . . . xik}
according to p?(x | yi). (These can be extracted
from Xi using standard algorithms (Huang and Chi-
ang, 2005).) Rescale their probabilities to sum to 1.
Sampling. For each yi, add to the training set k in-
dependent samples {xi1, . . . xik} from the distribu-
tion p?(x | yi), each with weight 1/k. (These can be
sampled from Xi using standard algorithms (John-
son et al, 2007).) This method is known in the liter-
ature as multiple imputation (Rubin, 1987).
Lattice. 11 Under certain special cases it is be pos-
sible to compute the expected loss in (3) exactly
via dynamic programming. Although Xi does con-
tain exponentially many translations, it may use a
?packed? representation in which these translations
share structure. This representation may further-
more enable sharing work in forward-translation, so
as to efficiently translate the entire set Xi and ob-
tain a distribution over translations y. Finally, the
expected loss under that distribution, as required by
equation (3), may also be efficiently computable.
All this turns out to be possible if (a) the poste-
rior distribution p?(x | yi) is represented by an un-
11The lattice approximation is presented here as a theoreti-
cal contribution, and we do not empirically evaluate it since its
implementation requires extensive engineering effort that is be-
yond the main scope of this paper.
923
ambiguous weighted finite-state automaton Xi, (b)
the forward translation system ?? is structured in a
certain way as a weighted synchronous context-free
grammar, and (c) the loss function decomposes in a
certain way. We omit the details of the construction
as beyond the scope of this paper.
In our experimental setting described below, (b) is
true (using Joshua), and (c) is true (since we use a
loss function presented by Tromble et al (2008) that
is an approximation to BLEU and is decomposable).
While (a) is not true in our setting because Xi is a
hypergraph (which is ambiguous), Li et al (2009b)
show how to approximate a hypergraph representa-
tion of p?(x | yi) by an unambiguous WFSA. One
could then apply the construction to this WFSA12,
obtaining an approximation to (3).
Rule-level Composition. Intuitively, the reason
why the structure-sharing in the hypergraphXi (gen-
erated by the reverse system) cannot be exploited
during forward translating is that when the forward
Hiero system translates a string xi ? Xi, it must
parse it into recursive phrases.
But the structure-sharing within the hypergraph of
Xi has already parsed xi into recursive phrases, in a
way determined by the reverse Hiero system; each
translation phrase (or rule) corresponding to a hy-
peredge. To exploit structure-sharing, we can use
a forward translation system that decomposes ac-
cording to that existing parse of xi. We can do that
by considering only forward translations that respect
the hypergraph structure of Xi. The simplest way to
do this is to require complete isomorphism of the
SCFG trees used for the reverse and forward trans-
lations. In other words, this does round-trip impu-
tation (i.e., from y to x, and then to y?) at the rule
level. This is essentially the approach taken by Li et
al. (2010).
3.5 The Log-Linear Model p?
We have not yet specified the form of p?. Following
much work in MT, we begin with a linear model
score(x, y) = ? ? f(x, y) =
?
k
?kfk(x, y) (9)
where f(x, y) is a feature vector indexed by k. Our
deterministic test-time translation system ?? simply
12Note that the forward translation of a WFSA is tractable by
using a lattice-based decoder such as that by Dyer et al (2008).
outputs the highest-scoring y for fixed x. At training
time, our randomized decoder (Section 3.3.2) uses
the Boltzmann distribution (here a log-linear model)
p?(y |x) =
e??score(x,y)
Z(x) =
e??score(x,y)?
y? e??score(x,y
?) (10)
The scaling factor ? controls the sharpness of the
training-time distribution, i.e., the degree to which
the randomized decoder favors the highest-scoring
y. For large ?, our training objective approaches
the imputed risk of the deterministic test-time sys-
tem while remaining differentiable.
In a task like MT, in addition to the input x and
output y, we often need to introduce a latent variable
d to represent the hidden derivation that relates x to
y. A derivation d represents a particular phrase seg-
mentation in a phrase-based MT system (Koehn et
al., 2003) and a derivation tree in a typical syntax-
based system (Galley et al, 2006; Chiang, 2007).
We change our model to assign scores not to an
(x, y) pair but to the detailed derivation d; in partic-
ular, now the function f that extracts a feature vector
can look at all of d. We replace y by d in (9)?(10),
and finally define p?(y|x) by marginalizing out d,
p?(y |x) =
?
d?D(x,y)
p?(d |x) (11)
where D(x, y) represents the set of derivations that
yield x and y.
4 Minimum Imputed Risk vs. EM
The notion of imputing missing data is familiar
from other settings (Little and Rubin, 1987), particu-
larly the expectation maximization (EM) algorithm,
a widely used generative approach. So it is instruc-
tive to compare EM with minimum imputed risk.
One can estimate ? by maximizing the log-
likelihood of the data {(xi, yi), i = 1, . . . , N} as
argmax
?
1
N
N?
i=1
log p?(xi, yi). (12)
If the xi?s are missing, EM tries to iteratively maxi-
mize the marginal probability:
argmax
?
1
N
N?
i=1
log
?
x
p?(x, yi). (13)
924
The E-step of each iteration comprises comput-
ing ?x p?t(x | yi) log p?(x, yi), the expected log-
likelihood of the complete data, where p?t(x | yi) is
the conditional part of p?t(x, yi) under the current
iterate ?t, and the M-step comprises maximizing it:
?t+1 = argmax
?
1
N
N?
i=1
?
x
p?t(x | yi) log p?(x, yi).
(14)
Notice that if we replace p?t(x|yi) with p?(x | yi)
in the equation above, and admit negated log-
likelihood as a loss function, then the EM update
(14) becomes identical to (4). In other words, the
minimum imputed risk approach of Section 3.1 dif-
fers from EM in (i) using an externally-provided and
static p?, instead of refining it at each iteration based
on the current p?t , and (ii) using a specific loss func-
tion, namely negated log-likelihood.
So why not simply use the maximum-likelihood
(EM) training procedure for MT? One reason is
that it is not discriminative: the loss function (e.g.
negated BLEU) is ignored during training.
A second reason is that training good joint models
p?(x, y) is computationally expensive. Contempo-
rary MT makes heavy use of log-linear probability
models, which allow the system designer to inject
phrase tables, linguistic intuitions, or prior knowl-
edge through a careful choice of features. Comput-
ing the objective function of (14) in closed form is
difficult if p? is an arbitrary log-linear model, be-
cause the joint probability p?(xi, yi) is then defined
as a ratio whose denominatorZ? involves a sum over
all possible sentence pairs (x, y) of any length.
By contrast, our discriminative framework will
only require us to work with conditional models.
While conditional probabilities such as p?(x | y) and
p?(y |x) are also ratios, computing their denomina-
tors only requires us to sum over a packed forest of
possible translations of a given y or x.13
In summary, EM would impute missing data us-
ing p?(x | y) and predict outputs using p?(y |x),
both being conditional forms of the same joint
model p?(x, y). Our minimum imputed risk train-
ing method is similar, but it instead uses a pair of
13Analogously, discriminative CRFs have become more pop-
ular than generative HMMs because they permit efficient train-
ing even with a wide variety of log-linear features (Lafferty et
al., 2001).
separately parameterized, separately trained mod-
els p?(x | y) and p?(y |x). By sticking to condi-
tional models, we can efficiently use more sophis-
ticated model features, and we can incorporate the
loss function when we train ?, which should improve
both efficiency and accuracy at test time.
5 Experimental Results
We report results on Chinese-to-English translation
tasks using Joshua (Li et al, 2009a), an open-source
implementation of Hiero (Chiang, 2007).
5.1 Baseline Systems
5.1.1 IWSLT Task
We train both reverse and forward baseline sys-
tems. The translation models are built using the cor-
pus for the IWSLT 2005 Chinese to English trans-
lation task (Eck and Hori, 2005), which comprises
40,000 pairs of transcribed utterances in the travel
domain. We use a 5-gram language model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998), trained on the English (resp. Chi-
nese) side of the bitext. We use a standard train-
ing pipeline and pruning settings recommended by
(Chiang, 2007).
5.1.2 NIST Task
For the NIST task, the TM is trained on about 1M
parallel sentence pairs (about 28M words in each
language), which are sub-sampled from corpora dis-
tributed by LDC for the NIST MT evaluation using a
sampling method implemented in Joshua. We also
used a 5-gram language model, trained on a data set
consisting of a 130M words in English Gigaword
(LDC2007T07) and the bitext?s English side.
5.2 Feature Functions
We use two classes of features fk for discriminative
training of p? as defined in (9).
5.2.1 Regular Hiero Features
We include ten features that are standard in Hi-
ero (Chiang, 2007). In particular, these include
one baseline language model feature, three baseline
translation models, one word penalty feature, three
features to count how many rules with an arity of
925
zero/one/two are used in a derivation, and two fea-
tures to count how many times the unary and binary
glue rules in Hiero are used in a derivation.
5.2.2 Target-rule Bigram Features
In this paper, we do not attempt to discrimina-
tively tune a separate parameter for each bilingual
rule in the Hiero grammar. Instead, we train several
hundred features that generalize across these rules.
For each bilingual rule, we extract bigram fea-
tures over the target-side symbols (including non-
terminals and terminals). For example, if a bilingual
rule?s target-side is ?on the X1 issue of X2? where
X1 and X2 are non-terminals (with a position in-
dex), we extract the bigram features on the, the X ,
X issue, issue of, and of X . (Note that the posi-
tion index of a non-terminal is ignored in the fea-
ture.) Moreover, for the terminal symbols, we will
use their dominant POS tags (instead of the sym-
bol itself). For example, the feature the X becomes
DTX . We use 541 such bigram features for IWSLT
task (and 1023 such features for NIST task) that fire
frequently.
5.3 Data Sets for Discriminative Training
5.3.1 IWSLT Task
In addition to the 40,000 sentence pairs used to
train the baseline generative models (which are used
to compute the features fk), we use three bilingual
data sets listed in Table 1, also from IWSLT, for dis-
criminative training: one to train the reverse model
p? (which uses only the 10 standard Hiero features
as described in Section 5.2.1),14 one to train the for-
ward model ?? (which uses both classes of features
described in Section 5.2, i.e., 551 features in total),
and one for test.
Note that the reverse model ? is always trained us-
ing the supervised data of Dev ?, while the forward
model ? may be trained in a supervised or semi-
supervised manner, as we will show below.
In all three data sets, each Chinese sentence xi
has 16 English reference translations, so each yi is
actually a set of 16 translations. When we impute
data from yi (in the semi-supervised scenario), we
14Ideally, we should train ? to minimize the conditional
cross-entropy (5) as suggested in section 3.2. In the present
results, we trained ? discriminatively to minimize risk, purely
for ease of implementation using well versed steps.
Data set Purpose # of sentencesChinese English
Dev ? training ? 503 503?16
Dev ? training ? 503? 503?16
Eval ? testing 506 506?16
Table 1: IWSLT Data sets used for discriminative
training/test. Dev ? is used for discriminatively training
of the reverse model ?, Dev ? is for the forward model,
and Eval ? is for testing. The star ? for Dev ? empha-
sizes that some of its Chinese side will not be used in the
training (see Table 2 for details).
actually impute 16 different values of xi, by using
p? to separately reverse translate each sentence in
yi. This effectively adds 16 pairs of the form (xi, yi)
to the training set (see section 3.4), where each xi
is a different input sentence (imputed) in each case,
but yi is always the original set of 16 references.
5.3.2 NIST Task
For the NIST task, we use MT03 set (having 919
sentences) to tune the component parameters in both
the forward and reverse baseline systems. Addition-
ally, we use the English side of MT04 (having 1788
sentences) to perform semi-supervised tuning of the
forward model. The test sets are MT05 and MT06
(having 1082 and 1099 sentences, respectively). In
all the data sets, each source sentence has four refer-
ence translations.
5.4 Main Results
We compare two training scenarios: supervised and
semi-supervised. The supervised system (?Sup?)
carries out discriminative training on a bilingual data
set. The semi-supervised system (?+Unsup?) addi-
tionally uses some monolingual English text for dis-
criminative training (where we impute one Chinese
translation per English sentence).
Tables 2 and 3 report the results for the two tasks
under two training scenarios. Clearly, adding unsu-
pervised data improves over the supervised case, by
at least 1.3 BLEU points in IWSLT and 0.5 BLEU in
NIST.
5.5 Results for Analysis Purposes
Below, we will present more results on the IWSLT
data set to help us understand the behavior of the
926
Training scenario Test BLEU
Sup, (200, 200?16) 47.6
+Unsup, 101?16 Eng sentences 49.0
+Unsup, 202?16 Eng sentences 48.9
+Unsup, 303?16 Eng sentences 49.7?
Table 2: BLEU scores for semi-supervised training for
IWSLT task. The supervised system (?Sup?) is trained
on a subset of Dev ? containing 200 Chinese sentences
and 200?16 English translations. ?+Unsup? means that
we include additional (monolingual) English sentences
from Dev ? for semi-supervised training; for each En-
glish sentence, we impute the 1-best Chinese translation.
A star ? indicates a result that is signicantly better than
the ?Sup? baseline (paired permutation test, p < 0.05).
Training scenario Test BLEUMT05 MT06
Sup, (919, 919?4) 32.4 30.6
+Unsup, 1788 Eng sentences 33.0? 31.1?
Table 3: BLEU scores for semi-supervised training for
NIST task. The ?Sup? system is trained on MT03, while
the ?+Unsup? system is trained with additional 1788 En-
glish sentences from MT04. (Note that while MT04 has
1788?4 English sentences as it has four sets of refer-
ences, we only use one such set, for computational ef-
ficiency of discriminative training.) A star ? indicates a
result that is signicantly better than the ?Sup? baseline
(paired permutation test, p < 0.05).
methods proposed in this paper.
5.5.1 Imputation with Different Reverse
Models
A critical component of our unsupervised method
is the reverse translation model p?(x | y). We
wonder how the performance of our unsupervised
method changes when the quality of the reverse sys-
tem varies. To study this question, we used two dif-
ferent reverse translation systems, one with a lan-
guage model trained on the Chinese side of the bi-
text (?WLM?), and the other one without using such
a Chinese LM (?NLM?). Table 4 (in the fully unsu-
pervised case) shows that the imputed Chinese trans-
lations have a far lower BLEU score without the lan-
guage model,15 and that this costs us about 1 English
15The BLEU scores are low even with the language model
because only one Chinese reference is available for scoring.
Data size Imputed-CN BLEU Test-EN BLEUWLM NLM WLM NLM
101 11.8 3.0 48.5 46.7
202 11.7 3.2 48.9 47.6
303 13.4 3.5 48.8 47.9
Table 4: BLEU scores for unsupervised training
with/without using a language model in the reverse
system. A data size of 101 means that we use only
the English sentences from a subset of Dev ? containing
101 Chinese sentences and 101?16 English translations;
for each English sentence we impute the 1-best Chinese
translation. ?WLM? means a Chinese language model
is used in the reverse system, while ?NLM? means no
Chinese language model is used. In addition to reporting
the BLEU score on Eval ?, we also report ?Imputed-CN
BLEU?, the BLEU score of the imputed Chinese sentences
against their corresponding Chinese reference sentences.
BLEU point in the forward translations. Still, even
with the worse imputation (in the case of ?NLM?),
our forward translations improve as we add more
monolingual data.
5.5.2 Imputation with Different k-best Sizes
In all the experiments so far, we used the reverse
translation system to impute only a single Chinese
translation for each English monolingual sentence.
This is the 1-best approximation of section 3.4.
Table 5 shows (in the fully unsupervised case)
that the performance does not change much as k in-
creases.16 This may be because that the 5-best sen-
tences are likely to be quite similar to one another
(May and Knight, 2006). Imputing a longer k-best
list, a sample, or a lattice for xi (see section 3.4)
might achieve more diversity in the training inputs,
which might make the system more robust.
6 Conclusions
In this paper, we present an unsupervised discrimi-
native training method that works with missing in-
puts. The key idea in our method is to use a re-
verse model to impute the missing input from the ob-
served output. The training will then forward trans-
late the imputed input, and choose the parameters of
the forward model such that the imputed risk (i.e.,
16In the present experiments, however, we simply weighted
all k imputed translations equally, rather than in proportion to
their posterior probabilities as suggested in Section 3.4.
927
Training scenario Test BLEU
Unsup, k=1 48.5
Unsup, k=2 48.4
Unsup, k=3 48.9
Unsup, k=4 48.5
Unsup, k=5 48.4
Table 5: BLEU scores for unsupervised training with
different k-best sizes. We use 101?16 monolingual En-
glish sentences, and for each English sentence we impute
the k-best Chinese translations using the reverse system.
the expected loss of the forward translations with
respect to the observed output) is minimized. This
matches the intuition that the probabilistic ?round-
trip? translation from the target-language sentence
to the source-language and back should have low ex-
pected loss.
We applied our method to two Chinese to English
machine translation tasks (i.e. IWSLT and NIST).
We showed that augmenting supervised data with
unsupervised data improved performance over the
supervised case (for both tasks).
Our discriminative model used only a small
amount of training data and relatively few features.
In future work, we plan to test our method in settings
where there are large amounts of monolingual train-
ing data (enabling many discriminative features).
Also, our experiments here were performed on a lan-
guage pair (i.e., Chinese to English) that has quite
rich bilingual resources in the domain of the test
data. In future work, we plan to consider low-
resource test domains and language pairs like Urdu-
English, where bilingual data for novel domains is
sparse.
Acknowledgements
This work was partially supported by NSF Grants
No IIS-0963898 and No IIS-0964102 and the
DARPA GALE Program. The authors thank Markus
Dreyer, Damianos Karakos and Jason Smith for in-
sightful discussions.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. J. Mach. Learn. Res., 7:551?
585.
Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In ACL,
pages 1012?1020.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In In IWSLT.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL,
pages 961?968.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In IWPT, pages 53?64.
Jui-Ting Huang, Xiao Li, and Alex Acero. 2010. Dis-
criminative training methods for language models us-
ing conditional entropy criteria. In ICASSP.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In NAACL, pages 139?146.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In NAACL,
pages 48?54.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In EMNLP, pages
40?51.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar. Zaidan. 2009a.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In WMT09, pages 26?30.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In ACL, pages 593?601.
Zhifei Li, Ziyuan Wang, Sanjeev Khudanpur, and Jason
Eisner. 2010. Unsupervised discriminative language
928
model training for machine translation using simulated
confusion sets. In COLING, pages 556?664.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL, pages 761?
768.
R. J. A. Little and D. B. Rubin. 1987. Statistical Analysis
with Missing Data. J. Wiley & Sons, New York.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the
limited memory bfgs method for large scale optimiza-
tion. Mathematical Programming, 45:503?528.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
EMNLP, pages 725?734.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In NAACL, pages 351?358.
Thomas Minka. 2000. Empirical risk minimization is
an incomplete inductive principle. In MIT Media Lab
note.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: A method for automatic eval-
uation of machine translation. In ACL, pages 311?318.
D. B. Rubin. 1987. Multiple Imputation for Nonresponse
in Surveys. J. Wiley & Sons, New York.
David A. Smith and Jason Eisner. 2006. Minimum
risk annealing for training log-linear models. In ACL,
pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice minimum-Bayes-risk de-
coding for statistical machine translation. In EMNLP,
pages 620?629.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In EMNLP-CoNLL, pages
764?773.
929
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 145?151,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The SDL Language Weaver Systems in the WMT12 Quality Estimation
Shared Task
Radu Soricut Nguyen Bach Ziyuan Wang
SDL Language Weaver
6060 Center Drive, Suite 101
Los Angeles, CA, USA
{rsoricut,nbach,zwang}@sdl.com
Abstract
We present in this paper the system sub-
missions of the SDL Language Weaver
team in the WMT 2012 Quality Estimation
shared-task. Our MT quality-prediction sys-
tems use machine learning techniques (M5P
regression-tree and SVM-regression models)
and a feature-selection algorithm that has been
designed to directly optimize towards the of-
ficial metrics used in this shared-task. The
resulting submissions placed 1st (the M5P
model) and 2nd (the SVM model), respec-
tively, on both the Ranking task and the Scor-
ing task, out of 11 participating teams.
1 Introduction
The WMT 2012 Quality Estimation shared-task fo-
cused on automatic methods for estimating machine
translation output quality at run-time (sentence-level
estimation). Different from MT evaluation met-
rics, quality prediction (QP) systems do not rely
on reference translations and are generally built us-
ing machine learning techniques to estimate quality
scores (Specia et al, 2009; Soricut and Echihabi,
2010; Bach et al, 2011; Specia, 2011).
Some interesting uses of sentence-level MT qual-
ity prediction are the following: decide whether a
given translation is good enough for publishing as-
is (Soricut and Echihabi, 2010), or inform monolin-
gual (target-language) readers whether or not they
can rely on a translation; filter out sentences that
are not good enough for post-editing by professional
translators (Specia, 2011); select the best translation
among options from multiple MT systems (Soricut
and Narsale, 2012), etc.
This shared-task focused on estimating the qual-
ity of English to Spanish automatic translations. The
training set distributed for the shared task comprised
of 1, 832 English sentences taken from the news do-
main and their Spanish translations. The translations
were produced by the Moses SMT system (Koehn et
al., 2007) trained on Europarl data. Translations also
had a quality score derived from an average of three
human judgements of Post-Editing effort using a 1-
5 scale (1 for worse-quality/most-effort, and 5 for
best-quality/least-effort). Submissions were evalu-
ated using a blind official test set of 422 sentences
produced in the same fashion as the training set.
Two sub-tasks were considered: (i) scoring transla-
tions using the 1-5 quality scores (Scoring), and (ii)
ranking translations from best to worse (Ranking).
The official metrics used for the Ranking task were
DeltaAvg (measuring how valuable a proposed rank-
ing is from the perspective of extrinsic values asso-
ciated with the test entries, in this case post-editing
effort on a 1-5 scale; for instance, a DeltaAvg of 0.5
means that the top-ranked quantiles have +0.5 bet-
ter quality on average compared to the entire set), as
well as the Spearman ranking correlation. For the
Scoring task the metrics were Mean-Absolute-Error
(MAE) and Root Mean Squared Error (RMSE). The
interested reader is referred to (Callison-Burch et al,
2012) for detailed descriptions of both the data and
the evaluation metrics used in the shared-task.
The SDL Language Weaver team participated
with two submissions based on M5P and SVM re-
gression models in both the Ranking and the Scoring
145
tasks. The models were trained and used to predict
Post-Editing?effort scores. These scores were used
as-such for the Scoring task, and also used to gener-
ate sentence rankings for the Ranking task by simply
(reverse) sorting the predicted scores. The submis-
sions of the SDL Language Weaver team placed 1st
(the M5P model) and 2nd (the SVM model) on both
the Ranking task (out of 17 entries) and the Scoring
task (out of 19 entries).
2 The Feature Set
Both SDLLW system submissions were created
starting from 3 distinct sets of features: the baseline
feature set (here called BFs), the internal features
available in the decoder logs of Moses (here called
MFs), and an additional set of features that we de-
veloped internally (called LFs). We are presenting
each of these sets in what follows.
2.1 The Baseline Features
The WMT Quality Estimation shared-task defined
a set of 17 features to be used as ?baseline? fea-
tures. In addition to that, all participants had access
to software that extracted the corresponding feature
values from the inputs and necessary resources (such
as the SMT-system?s training data, henceforth called
SMTsrc and SMTtrg). For completeness, we are
providing here a brief description of these 17 base-
line features (BFs):
BF1 number of tokens in the source sentence
BF2 number of tokens in the target sentence
BF3 average source token length
BF4 LM probability of source sentence
BF5 LM probability of the target sentence
BF6 average number of occurrences of the target
word within the target translation
BF7 average number of translations per source word
in the sentence (as given by IBM 1 table thresh-
olded so that Prob(t|s) > 0.2)
BF8 average number of translations per source word
in the sentence (as given by IBM 1 table thresh-
olded so that Prob(t|s) > 0.01) weighted
by the inverse frequency of each word in the
source corpus
BF9 percentage of unigrams in quartile 1 of fre-
quency (lower frequency words) in SMTsrc
BF10 percentage of unigrams in quartile 4 of fre-
quency (higher frequency words) in SMTsrc
BF11 percentage of bigrams in quartile 1 of fre-
quency of source words in SMTsrc
BF12 percentage of bigrams in quartile 4 of fre-
quency of source words in SMTsrc
BF13 percentage of trigrams in quartile 1 of fre-
quency of source words in SMTsrc
BF14 percentage of trigrams in quartile 4 of fre-
quency of source words in SMTsrc
BF15 percentage of unigrams in the source sentence
seen in SMTsrc
BF16 number of punctuation marks in source sen-
tence
BF17 number of punctuation marks in target sentence
These features, together with the other ones we
present here, are entered into a feature-selection
component that decides which feature set to use for
optimum performance (Section 3.2).
In Table 1, we are presenting the performance
on the official test set of M5P and SVM-regression
(SVR) models using only the BF features. The
M5P model is trained using the Weka package 1
and the default settings for M5P decision-trees
(weka.classifiers.trees.M5P). The SVR model is
trained using the LIBSVM toolkit 2. The follow-
ing options are used: ?-s 3? (-SVR) and ?-t 2? (ra-
dial basis function). The following parameters were
optimized via 5-fold cross-validation on the train-
ing data: ?-c cost?, the parameter C of -SVR; ?-g
gamma?, the ? parameter of the kernel function; ?-p
epsilon?, the  for the loss-function of -SVR.
1http://www.cs.waikato.ac.nz/ml/weka/
2http://www.csie.ntu.edu.tw/?cjlin/libsvm/
146
Systems Ranking Scoring
DeltaAvg Spearman MAE RMSE Predict. Interval
17 BFs with M5P 0.53 0.56 0.69 0.83 [2.3-4.9]
17 BFs with SVR 0.55 0.58 0.69 0.82 [2.0-5.0]
best-system 0.63 0.64 0.61 0.75 [1.7-5.0]
Table 1: Performance of the Baseline Features using M5P and SVR models on the test set.
The results in Table 1 are compared against the
?best-system? submission, in order to offer a com-
parison point. The ?17 BFs with SVM? system ac-
tually participated as an entry in the shared-task, rep-
resenting the current state-of-the-art in MT quality-
prediction. This system has been ranked 6th (out of
17 entries) in the Ranking task, and 8th (out of 19
entries) in the Scoring task.
2.2 The Decoder Features
The current Quality Estimation task has been de-
fined as a glass-box task. That is, the prediction
component has access to everything related to the
internal workings of the MT system for which the
quality prediction is made. As such, we have cho-
sen to use the internal scores of the Moses 3 decoder
(available to all the participants in the shared-task)
as a distinct set of features. These features are the
following:
MF1 Distortion cost
MF2 Word penalty cost
MF3 Language-model cost
MF4 Cost of the phrase-probability of source given
target ?(s|t)
MF5 Cost of the word-probability of source given
target ?lex(s|t)
MF6 Cost of the phrase-probability of target given
source ?(t|s)
MF7 Cost of the word-probability of target given
source ?lex(t|s)
MF8 Phrase penalty cost
3http://www.statmt.org/moses/
These features are then entered into a feature-
selection component that decides which feature set
to use for achieving optimal performance.
The results in Table 2 present the performance
on the test set of the Moses features (with an M5P
model), presented against the ?best-system? sub-
mission. These numbers indicate that the Moses-
internal features, by themselves, are fueling a QP
system that surpasses the performance of the strong
?baseline? system. We note here that the ?8 MFs
with M5P? system would have been ranked 4th (out
of 17 entries) in the Ranking task, and 5th (out of 19
entries) in the Scoring task.
2.3 Language Weaver Features
In addition to the features presented until this point,
we have created and tested additional features that
helped our systems achieve improved performance.
In addition to the SMT training corpus, these fea-
tures also use the SMT tuning dev set (henceforth
called Devsrc and Devtrg). These features are the
following:
LF1 number of out-of-vocabulary tokens in the
source sentence
LF2 LM perplexity for the source sentence
LF3 LM perplexity for the target sentence
LF4 geometric mean (?-smoothed) of 1-to-4?gram
precision scores (i.e., BLEU score without
brevity-penalty) of source sentence against the
sentences of SMTsrc used as ?references?
LF5 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against the
sentences of SMTtrg used as ?references?
147
Systems Ranking Scoring
DeltaAvg Spearman-Corr MAE RMSE Predict. Interval
8 MFs with M5P 0.58 0.58 0.65 0.81 [1.8-5.0]
best-system 0.63 0.64 0.61 0.75 [1.7-5.0]
Table 2: Performance of the Moses-based Features with an M5P model on the test set.
LF6 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of source sentence against the
top BLEU-scoring quartile of Devsrc
LF7 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against the
top BLEU-scoring quartile of Devtrg
LF8 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of source sentence against the
bottom BLEU-scoring quartile of Devsrc
LF9 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against the
bottom BLEU-scoring quartile Devtrg
LF10 geometric mean (?-smoothed) of 1-to-4?gram
precision scores of target translation against a
pseudo-reference produced by a second MT
Eng-Spa system
LF11 count of one-to-one (O2O) word alignments
between source and target translation
LF12 ratio of O2O alignments over source sentence
LF13 ratio of O2O alignments over target translation
LF14 count of O2O alignments with Part-of-Speech?
agreement
LF15 ratio of O2O alignments with Part-of-Speech?
agreement over O2O alignments
LF16 ratio of O2O alignments with Part-of-Speech?
agreement over source
LF17 ratio of O2O alignments with Part-of-Speech?
agreement over target
Most of these features have been shown to help
Quality Prediction performance, see (Soricut and
Echihabi, 2010) and (Bach et al, 2011). Some of
them are inspired from word-based confidence esti-
mation, in which the alignment consensus between
the source words and target-translation words are
informative indicators for gauging the quality of a
translation hypothesis. The one-to-one (O2O) word
alignments are obtained from the decoding logs of
Moses. We use the TreeTagger to obtain Spanish
POS tags4 and a maximum-entropy POS tagger for
English. Since Spanish and English POS tag sets
are different, we normalize their fine-grained POS
tag sets into a coarser tag set by mapping the orig-
inal POS tags into more general linguistic concepts
such as noun, verb, adjective, adverb, preposition,
determiner, number, and punctuation.
3 The Models
3.1 The M5P Prediction Model
Regression-trees built using the M5P algo-
rithm (Wang and Witten, 1997) have been previ-
ously shown to give good QP performance (Soricut
and Echihabi, 2010). For these models, the num-
ber of linear equations used can provide a good
indication whether the model overfits the training
data. In Table 3, we compare the performance of
several M5P models: one trained on all 42 features
presented in Section 2, and two others trained on
only 15 and 14 features, respectively (selected using
the method described in Section 3.2). We also
present the number of linear equations (L.Eq.) used
by each model. Aside from the number of features
they employ, these models were trained under
identical conditions: default parameters of the Weka
implementation, and 1527 training instances (305
instances were held-out for the feature-selection
step, from the total 1832 labeled instances available
for the shared-task).
As the numbers in Table 3 clearly show, the set of
4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
148
Systems #L.Eq. Dev Set Test Set
DeltaAvg MAE DeltaAvg MAE
42 FFs with M5P 10 0.60 0.58 0.56 0.64
(best-system) 15 FFs with M5P 2 0.63 0.52 0.63 0.61
14 FFs with M5P 6 0.62 0.50 0.61 0.62
Table 3: M5P-model performance for different feature-function sets (15-FFs ? 42-FFs; 14-FFs ? 42-FFs).
feature-functions that an M5P model is trained with
matters considerably. On both our development set
and the official test set, the 15-FF M5P model out-
performs the 42-FF model (even if 15-FF ? 42-FF).
The 42-FF model would have been ranked 5th (out
of 17 entries) in the Ranking task, and also 5th (out
of 19 entries) in the Scoring task. In comparison, the
15-FF model (feature set optimized for best perfor-
mance under the DeltaAvg metric) was our official
M5P submission (SDLLW M5PBestDeltaAvg), and
ranked 1st in the Ranking task and also 1st in the
Scoring task. The 14-FF model (also a subset of the
42-FF set, optimized for best performance under the
MAE metric) was not part of our submission, but
would have been ranked 2nd on both the Ranking
and Scoring tasks.
The number of linear equations used (see #L.Eq.
in Table 3) is indicative for our results. When using
42 FFs, the M5P model seems to overfit the train-
ing data (10 linear equations). In contrast, the model
trained on a subset of 15 features has only 2 linear
equations. This latter model is less prone to overfit-
ting, and performs well given unseen test data. The
same number for the 14-FF model indicates slight
overfit on the training and dev data: with 6 equa-
tions, this model has the best MAE numbers on the
Dev set, but slightly worse MAE numbers on the
Test score compared to the 15-FF model.
3.2 Feature Selection
As we already pointed out, some of the features of
the entire 42-FF set are highly overlapping and cap-
ture roughly the same amount of information. To
achieve maximum performance given this feature-
set, we applied a computationally-intensive feature-
selection method. We have used the two official
metrics, DeltaAvg and MAE, and a development set
of 305 instances to perform an extensive feature-
selection procedure that directly optimizes the two
official metrics using M5P regression-trees.
The overall space that needs to be explored for 42
features is huge, on the order of 242 possible com-
binations. We performed the search in this space in
several steps. In a first step, we eliminated the obvi-
ously overlapping features (e.g., BF5 and MF3 are
both LM costs of the target translation), and also
excluded the POS-based features (LF14-LF17, see
Section 2.3). This step reduced the overall num-
ber of features to 24, and therefore left us with an
order of 224 possible combinations. Next, we ex-
haustively searched all these combinations by build-
ing and evaluating M5P models. This operation
is computationally-intensive and takes approxima-
tively 60 hours on a cluster of 800 machines. At
the conclusion of this step, we ranked the results
and considered the top 64 combinations. The perfor-
mance of these top combinations was very similar,
and a set of 15 features was selected as the superset
of active feature-functions present in most of the top
64 combinations.
DeltaAvg optim. BF1 BF3 BF4 BF6 BF12
BF13 BF14 MF3 MF4 MF6
LF1 LF10 LF14 LF15 LF16
MAE optim. BF1 BF3 BF4 BF6 BF12
BF14 BF16 MF3 MF4 MF6
LF1 LF10 LF14 LF17
Table 4: Feature selection results.
The second round of feature selection consid-
ers these 15 feature-functions plus the 4 POS-based
feature-functions, for a total of 19 features and there-
fore a space of 219 possible combinations (215 of
these already covered by the first search pass). A
second search procedure was executed exhaustively
149
Dev Set Test Set
SVR Model (C;?;) #S.V. DeltaAvg MAE DeltaAvg MAE
1.0 ; 0.00781; 0.50 695 0.62 0.52 0.60 0.66
1.74; 0.00258; 0.3299 952 0.63 0.51 0.61 0.64
8.0 ; 0.00195; 0.0078 1509 0.64 0.50 0.60 0.68
16.0; 0.00138; 0.0884 1359 0.63 0.51 0.59 0.70
Table 5: SVR-model performance for dev and test sets.
over the set of all the new possible combinations.
In the end, we selected the winning feature-function
combination as our final feature-function sets: 15
features for DeltaAvg optimization and 14 features
for MAE optimization. They are given in Table 4,
using the feature id-s given in Section 2. The perfor-
mance of these two feature-function sets using M5P
models can be found in Table 3.
3.3 The SVM Prediction Model
The second submission of our team consists of rank-
ings and scores produced by a system using an -
SVM regression model (-SVR) and a subset of 19
features. This model is trained on 1,527 training
examples by the LIBSVM package using radial ba-
sis function (RBF) kernel. We have found that the
feature-set obtained by the feature-selection opti-
mization for M5P models described in Section 3.2
does not achieve the same performance for SVR
models on our development set. Therefore, we
have performed our SVR experiments using a hand-
selected set of features: 9 features from the BF fam-
ily (BF1 BF3 BF4 BF6 BF10 BF11 BF12 BF14
BF16); all 8 features from the MF family; and 2 fea-
tures from the LF family (LF1 LF10).
We optimize the three hyper parameters C, ?, and
 of the SVR method using a grid-search method and
measure their performance on our development set
of 305 instances. The C parameter is a penalty fac-
tor: if C is too high, we have a high penalty for non-
separable points and may store many support vec-
tors and therefore overfit the training data; if C is
too low, we may end up with a model that is poorly
fit. The  parameter determines the level of accuracy
of the approximated function; however, getting too
close to zero may again overfit the training data. The
? parameter relates to the RBF kernel: large ? val-
ues give the model steeper and more flexible kernel
functions, while small gamma values give the model
smoother functions. In general, C, , and ? are all
sensitive parameters and instantiate -SVR models
that may behave very differently.
In order to cope with the overfitting issue given
a small amount of training data and grid search op-
timization, we train our models with 10-fold cross
validation and restart the tuning process several
times using different starting points and step sizes.
We select the best model parameters based on a cou-
ple of indicators: the performance on the develop-
ment set and the number of support vectors of the
model. In Table5 we present the performance of dif-
ferent model parameters on both the development
set and the official test set. Our second submis-
sion (SDLLW SVM), which placed 2nd in both the
Ranking and the Scoring tasks, is the entry in bold
font. It was chosen based on good performance on
the Dev set and also a setting of the (C, ?, ) pa-
rameters that provides a number of support vectors
that is neither too high nor too low. As a contrastive
point, the model on the row below it uses 1,509 sup-
port vectors extracted from 1,527 training vectors,
which represents a clear case of overfitting. Indeed,
the performance of this model is marginally better
on the Dev set, but ends up underperforming on the
Test data.
4 Conclusions
The WMT 2012 Quality Estimation shared-task pro-
vided the opportunity for the comparing different
QP systems using shared datasets and standardized
evaluation metrics. Our participation in this shared-
task revealed two important aspects of Quality Pre-
diction for MT that we regard as important for the
future. First, our experiments indicated that the
150
Moses-internal features, by themselves, can fuel a
QP-system that surpasses the performance of the
strong ?baseline? system used in this shared task to
represent state-of-the-art performance in MT qual-
ity prediction. This is a surprising finding, consid-
ering that these decoder-internal features have been
primarily designed to gauge differences in transla-
tion quality when starting from the same source sen-
tence. In contrast, for quality-prediction tasks like
ranking one needs to gauge differences in quality of
translations of different source sentences.
The second aspect relates to the importance of
feature selection. Given the availability and good
scalability of Machine Learning toolkits today, it
is tempting to throw as much features as possible
at this problem and let the built-in mechanisms of
these learning algorithms deal with issues relating
to feature overlapping, training-data overfitting, etc.
However, these learning algorithms have their own
limitations in these regards, and, in conjunction with
the limited availability of the labeled data, can easily
produce models that are underperforming on blind
tests. There is a need for careful engineering of
the models and evaluation of the resulting perfor-
mance in order to achieve optimal performance us-
ing the current state-of-the-art supervised learning
techniques.
References
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan. 2011.
Goodness: A method for measuring machine transla-
tion confidence. In Proceedings of the ACL/HLT, Port-
land, Oregon, USA.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations via
ranking. In Proceedings of ACL.
Radu Soricut and Sushant Narsale. 2012. Combining
quality prediction and system selection for improved
automatic translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman, Mar-
cho Turchi, and Nello Cristianini. 2009. Estimating
the sentence-level quality of machine translation. In
Proceedings of EAMT.
Lucia Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proceed-
ings of EAMT.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Proceedings of
the 9th European Conference on Machine Learning.
151
