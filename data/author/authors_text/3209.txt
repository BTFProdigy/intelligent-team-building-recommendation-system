Acknowledgments in Human-Computer Interaction 
Karen Ward and Peter A. Heeman 
Center for Spoken Language Understanding 
Oregon Graduate Institute of Science & Technology 
20000 NW Walker Road 
Beaverton, Oregon 97006 
wardk@cse, ogi. edu, heeman@cse, ogi. edu 
Abstract 
Acknowledgments are relatively rare in human- 
computer interaction. Are people unwilling to use 
this human convention when talking to a machine, 
or is their scarcity due to the way that spoken-lan- 
guage interfaces are designed? We found that, 
given a simple spoken-language interface that pro- 
vided opportunities for and responded to acknowl- 
edgments ,  about hal f  of our subjects  used 
acknowledgments at least once and nearly 30% 
used them extensively during the interaction. 
1 Introduction 
As our ability to build robust and flexible spoken- 
language interfaces increases, it is worthwhile to 
ask to what extent we should incorporate various 
human-human discourse phenomena into our dia- 
logue models. Many studies have shown that peo- 
ple alter their dialogue techniques when they 
believe that they are talking to a computer (e.g., 
Brennan, 1991), so it is not clear that observations 
of human-human conversation will provide us with 
the guidance we need. At the same time, we cannot 
always look to current systems to determine which 
discourse phenomena should be supported. Cur- 
rent-generation interfaces are still relatively fragile, 
and so designers of spoken-language systems go to 
some effort to structure dialogues and create 
prompts that guide the user toward short, content- 
ful, in-vocabulary responses (e.g., Basson et al 
1996; Cole, et al 1997; Oviatt et al 1994). One 
result of this approach is the suppression of meta- 
dialogue acts such as acknowledgement a d repeti- 
tion. 
The term "acknowledgment" is from Clark and 
Schaefer (1989), who describe ahierarchy of meth- 
ods by which one conversant may signal that 
another's contribution has been understood well 
enough to allow the conversation to proceed. 
Acknowledgments often appear in American 
English conversation as an "okay" or "uh-huh" that 
signals understanding but not necessarily agree- 
ment. These are also called "back channels" or 
"prompts" (e.g., Chu-Carroll & Brown, 1997), 
Closely related to acknowledgments are repeti- 
tions, in which the conversant provides a stronger 
signal that a contribution has been understood by 
repeating part or all of the other's contribution. 
Repetitions are also referred to as "paraphrases" 
(Traum & Heeman, 1996), "echoing" (Swerts et al 
1998), and "demonstration" (Clark & Schaefer, 
1989). Repetitions are often seen when one is con- 
veying complex information, such as when one 
copies an address or telephone number. 
Neither acknowledgments or repetitions con- 
tribute new domain information to the conversa- 
tion, but they serve to assure the speaker that 
information has been conveyed successfully. 
Acknowledgments also play a role in managing 
turn-taking in mixed-initiative dialogue; although 
acknowledgments may preface a new contribution 
by the same speaker (Novick & Sutton, 1994), 
often they occur alone as a single-phrase turn that 
appears to serve the purpose of explicitly declining 
an opportunity to take a turn (Sacks et al 1974). 
Acknowledgments and repetitions are ubiqui- 
tous in many types of human-human conversation. 
In a corpus of problem-solving spoken dialogues, 
for example, Traum and Heeman (1996) found that 
51% of turns began with or consisted of an explicit 
acknowledgment. Given this, one would expect 
that acknowledgments should be modeled in dia- 
logue models for spoken-language systems, and 
280 
indeed some research models are beginning to 
incorporate acknowledgments, e.g., Kita et al
(1996), Aist, (1998), Iwase & Ward (1998). 
Typical human-computer dialogue models are 
structured in ways that suppress the use of 
acknowledgments. In many systems turn-taking is 
completely controlled by one conversant, e.g., the 
user responds to system prompts, which tends to 
eliminate the need for acknowledgments as a turn- 
taking mechanism. In other systems, the use of 
barge-in defeats the common interpretation of an 
acknowledgment; if the user speaks, the system 
contribution is cut off before the user utterance is
interpreted. If that utterance was intended to signal 
that the contribution should continue, the effect is 
exactly the opposite of the one desired. 
Thus, current design practices both discourage 
and render meaningless the standard uses of 
acknowledgments. If these impediments were 
removed, would people choose to use acknowledg- 
ments when interacting with a computer interface? 
2 Experiment 
This study was designed as a pilot to our larger 
investigation into the effects of incorporating 
acknowledgement behavior in dialogue models for 
spoken-language interfaces. Before we attempted 
to compare interfaces with and without acknowl- 
edgement behavior, we wanted to understand 
whether people are willing to use this sort of meta- 
dialogue behavior when interacting with a com- 
puter. 
2.1 Approach 
In this study we hypothesized that subjects will 
choose to use acknowledgments in human-com- 
puter interaction if they are given an interface that 
provides opportuni t ies  for and responds to 
acknowledgments. 
In designing the study, we assumed that it 
would not immediately occur to subjects that they 
could use acknowledgments to a computer. At the 
same time, we did not want to explicitly instruct or 
require subjects to use acknowledgment behavior, 
as that would tell us nothing about their prefer- 
ences. We therefore decided against a comparison/ 
control-group experimental design for this initial 
study and instead focused on creating a situation in 
which subjects would have a reason to use 
acknowledgments, perhaps even gain an advantage 
from doing so, while still keeping the behavior 
optional. 
We decided to focus on a somewhat narrow use 
of acknowledgments. Conversants are especially 
likely to offer acknowledgments and repetitions 
when complex information is being presented, 
especially when the conversant is copying the 
information. While this is certainly explainable in 
terms of mutuality of understanding, this particular 
use of acknowledgment may be viewed from a 
more mechanical standpoint as regulating the pace 
at which information ispresented. This insight sug- 
gested to us that a fruitful task for this study might 
be one in which the subject is asked to write down 
verbally-presented information, as when taking 
messages over the telephone. 
2.2 Task 
We selected the domain of telephone interface to 
email and designed a task in which subjects were 
asked to transcribe items of information from the 
messages. Writing is slow in comparison to speak- 
ing, so we anticipated that subjects would require a 
slower pace of information presentation when they 
were writing. The messages included information 
not asked for on the question list to simulate "unin- 
teresting" material that the subject would want to 
move through at a faster pace. In this way we 
hoped to motivate subjects to try to control the 
pace at which information was presented. 
The email was presented in segments roughly 
corresponding to a long phrase. After each seg- 
ment, the system paused to give the subject ime to 
make notes. If the subject said nothing, the system 
would continue by presenting the next message 
segment. Subjects could accept--and perhaps 
make use of--this delay, or they could reduce it by 
acknowledging the contribution, e.g., "okay," or by 
commanding the system to continue, e.g., "go on." 
The system signalled the possibility of controlling 
the delay by prompting the subject "Are you ready 
to go on?" after the first pause. This prompting was 
repeated for every third pause in which the subject 
said nothing. In this way we hoped to suggest o 
the subjects that they could control the wait time if 
desired without explicitly telling them how to do 
SO. 
On the surface, there is no functional differ- 
ence in system behavior between a subject's use of 
281 
a command to move the system onward (e.g., "go 
on" "next", "continue") and the use of an acknowl- 
edgment ("okay," "uh-huh", or a repetition). In 
either case, the system responds by presenting the 
next message segment, and in fact it eventually 
presents the next segment even if the subject says 
nothing at all. Thus, the design allows the subject 
to choose freely between accepting the system's 
pace (system initiative), or commanding the system 
to continue (user initiative), or acknowledging the 
presentations in a fashion more typical of mixed- 
initiative human conversation. In this way, we 
hoped to understand how the subject preferred to 
interact with the computer. 
2.3 Subjects 
Subjects were told that the study's purpose was to 
assess the understandability and usability of the 
interface, and that their task was to find the 
answers to a list of questions. They were given no 
instructions in the use of the program beyond the 
information that they could talk to it using normal, 
everyday speech. 
The 14 volunteers were native speakers of 
North American English, and most were staff at a 
research university. Ten were female, four were 
male. Ages ranged from 13 to 57. All used comput- 
ers, typically office software and games, but none 
had significant programming experience. Each ses- 
sion lasted about 45 minutes total, and each subject 
was paid $10.00. One subject declined payment. 
2.4 Interface 
As mentioned earlier, one difficulty with recogniz- 
ing acknowledgements in poken-language inter- 
faces is that the use of barge-in tends to defeat he 
purpose of acknowledgments when they occur in 
overlapped speech. We used a Wizard of Oz proto- 
col as a simple way to allow the system to respond 
to such utterances and to provide robustness in 
handling repetitions. 
The wizard's interface was constructed using 
the Rapid Application Developer in the Center for 
Spoken Language Understanding Toolkit (Sutton, 
et al 1998). A simple button panel allowed the 
wizard to select he appropriate response from the 
actions supported by the application. The applica- 
tion functionality was deliberately limited to sug- 
gest realistic abilities for a current spoken- 
language interface. Using messages pre-recorded 
in a synthesized voice, the wizard was able to 
direct he system to: 
? Read a list of all messages. 
? Begin reading aparticular message. 
? Read the next message segment. 
? Repeat he current message segment. 
? Repeat he previous message segment. 
? Ask the subject whether the program should 
continue reading the current message. 
? Ask the subject o what to do next. 
? End the program. 
? Play one of several error and help messages. 
The texts of the email messages were pre- 
sented in phrases of varying lengths, with each 
phrase followed by a pause of about five seconds. 
Preliminary tests showed that the combined 
response time of the wizard and the interface was 
between one and two seconds, and that pauses of 
less than five seconds were not obviously different 
from the normal pace of system response. Five sec- 
onds is a long response time, uncomfortably sofor 
human-human conversation, so we hoped that this 
lengthy pause would encourage the subjects to take 
the initiative in controlling the pace of the interac- 
tion. 
The messages were divided into segments by 
hand. The divisions were intended to simulate a 
phrase-level presentation, although some short 
phrases were combined to make the presentation 
less choppy. An example of one message and its 
division into phrases may be seen in Figure 1. 
Synthesized speech from the Festival speech 
synthesizer (Taylor, et al 1998) was used through- 
out the interface. The message texts were presented 
in a synthesized male voice, while the control por- 
tions of the interface used a synthesized female 
voice. Default pronunciations were used except 
when the default was incorrect, e.g., "read" 
defaulted to the past-tense pronunciation i  all con- 
texts. Also, there was minor use of the SABLE 
markup language (Wouters, et al 1999) to flatten 
the pitch range at the end of phrases in list items; 
the intent was to suggest the prosody of list contin- 
uation rather than the default sentence-final drop. 
282 
Message six is from Jo at teleport dot com, about, please stop by store on your way home. 
I'm going to be late getting home tonight, so would you please stop by the store on your way home? 
We need milk, 
eggs, 
a bunch of spinach, 
fresh ginger, 
green onions, 
maple syrup, 
a pound of coos-coos, 
mild curry powder, 
a pound of coffee, 
and a package of seventy five watt light bulbs. 
Thanks! See you tonight. 
Figure 1. Text of a sample message. 
The subject's list of questions included "What items are you supposed to pick up at the store?" 
To improve the understandability, both voices were 
slowed slightly to 90% of the default speaking rate. 
2.5 Measures 
The central question to be answered is: will the 
subject use acknowledgments in interacting with 
the program? A subject can show one of several 
patterns of response: 
? The subject may make no attempt to control the 
pacing of the interface, instead allowing the 
interaction to proceed via time-outs. 
? The subject may use only commands to control 
the pacing. 
? The subject may use only acknowledgments to 
control the pacing. 
? The subject may use a mixture of commands and 
acknowledgments. 
The determination as to whether a particular utter- 
ance constituted an acknowledgment or a com- 
mand was based primarily on word choice and 
dialogue context; this approach is consistent with 
definitions of this behavior, e.g., Chu-Carroll and 
Brown (1997). For example, "yes" in the context of 
a system inform (a segment of an email message) 
was considered an acknowledgment, but "yes" in 
the context of a system question was not. The 
words "okay," "uh-huh," and "yes" (immediately 
following an inform) were taken as evidence of 
acknowledgments, and phrases such as "go on," 
"continue," next" following an inform were taken 
as evidence of commands. The interpretation was 
confirmed uring the post-experiment i terview by 
questioning the subjects about their word choice. 
2.6 Post-Experiment Interview 
A post-experiment interview was conducted to 
gather subject feedback and to answer subjects' 
questions. The experimenter took notes and thus 
could have introduced bias in the record of 
responses? No tape recording was made. 
The subject was first invited to comment on the 
interface and the interaction in an open-ended fash- 
ion. When the subject had finished, the experi- 
menter asked several specific questions to assess 
their understanding of the interface functionality. 
During this time, the experimenter reminded the 
subjects of the words that they had used most fre- 
quently to prompt the system to continue during 
pauses and asked the subjects why they had 
selected those words. 
Finally, the experimenter explained the true 
purpose and hypothesis of the experiment, verified 
that the subject was unaware that they had been 
interacting with a Wizard-of-Oz interface, and 
asked the subject to comment on the notion of 
using acknowledgments when interacting with a 
computer. The responses to this question, espe- 
cially, must be assumed to be somewhat optimistic, 
as it is likely that at least some subjects would be 
reluctant to disagree with the experimenter. 
3 Results 
Results are summarized in Table l .Because the 
subject pool was not balanced for gender, results 
for male and female subjects are reported sepa- 
rately. Due to the small number of male subjects in 
283 
Table 1. Summary of Acknowledgment Behavior 
Behavior 
Subjects 
Female 
10 subjects 
Male. 
4 subjects 
Total 
(14) 
Used acknowledgment/repetition at least once 4 (40%) 4 (100%) 8 (57%) 
Used acknowledgment/repetition m re than command 3 (30%) 1 (25%) 4 (29%) 
Used acknowledgment bu no commands 1 (i0%) 0 1 (7%) 
Described acknowledgment to computer as strange 2 (20%) 0 2 (14%) 
this pilot study, no tests of statistical significance of 
differences in the rates of acknowledgment behav- 
ior were made. 
Eight of the fourteen subjects used an 
acknowledgment or repetition at least once, and 
four used acknowledgment/repetitions more fre- 
quently than they used commands. Only one sub- 
ject used acknowledgments exclusively, while five 
subjects never used acknowledgments. No subject 
relied exclusively on time-outs to allow the system 
to proceed at its own pace, although one subject 
did use that as her predominant method (42 times, 
while using acknowledgments only six times and 
commands three times). Only one subject used rep- 
etition, and he reported uring the interview that he 
was unaware of having done so. 
It is interesting to note that while all of the 
male subjects in this sample xhibited acknowledg- 
ment behavior at least once, only one preferred 
acknowledgment over command. One of the male 
subjects used acknowledgments only three times, 
in all cases as prefaces to commands. Conversely, 
although a lower percentage of women used 
acknowledgments (40%), a higher percentage of 
them (30%) used acknowledgments in preference 
to commands. Because of the small numbers of 
subjects, however, we do not conclude that these 
differences are significant. 
During the post-experiment i erview, two sub- 
jects (both female) described the idea of using 
acknowledgments to the computer as strange and 
stated that they didn't feel that they would do this 
unless directed tt>---and even then, they would 
regard it as simply an alternate command. Two 
other subjects, both females who had used 
acknowledgments 2-6 times during the task, each 
reported that she had felt silly when she had caught 
herself saying "please" and "okay" to a computer 
but had been pleased when it had worked. The 
remainder of the subjects either expressed no 
strong opinion (two, both female) or expressed a 
positive attitude toward being able to use acknowl- 
edgments when interacting with a computer. Two 
subjects who had not used acknowledgments com- 
mented that they would probably be more likely to 
use human-like conversation if the synthesized 
voice were more human-like. 
Again, this report of the subjects' attitudes 
should be interpreted with caution; at this point in 
the interview they knew the experimenter's hypoth- 
esis and so may have been reluctant to disagree. 
3.1 Other Dialogue Behaviors 
Although we had not formed any hypothesis about 
other dialogue behaviors, we noticed several inter- 
esting dialogue behaviors that we had not antici- 
pated. 
We were surprised at the number of subjects 
who exhibited politeness behavior toward the inter- 
face, either saying "please" when issuing com- 
mands to the computer or responding to the 
program's "good-bye" at the end of the session. 
One subject used "please" throughout the interac- 
tion, but a more common pattern was to use 
"please" at the beginning of the session and to drop 
the behavior as the interface became more familiar. 
Politeness did not seem to be strongly associated 
with a willingness to use acknowledgments, how- 
ever; four of the nine subjects who exhibited 
politeness did not use any acknowledgments in 
their interaction. 
Despite the deliberately-artificial interface, 
several subjects responded at least once to the mes- 
sage content as if they were talking to the message 
284 
System: I could come to your office now or at any of the following times, one thirty 
SUBJECT: continue 
System: three o clock 
SUBJECT: continue 
System: or five fifteen 
SUBJECT: continue 
System: thank you. I look forward to your prompt reply 
SUBJECT: thank you- uh ((laugh)) continue 
Figure 2. Excerpt of transcript. 
Subject hanks the interface. The system is reading the text of one of the messages. 
sender. In the excerpt shown in Figure 2., for exam- 
ple, the subject replied "I'hank you" to the message 
text's "thank you." This did not appear to be a mat- 
ter of misunderstanding the capabilities of the 
interface; the subject later reported that despite the 
synthesized voices she had briefly forgotten that 
she wasn't alking to her secretary. 
Three subjects also made one or more meta- 
comments, e.g., "ah, there it is" when finding a par- 
ticular piece of information. These may have been 
at least partially an artifact of the "treasure hunt" 
nature of the task. When questioned in the post- 
experiment interview, subjects didn't seem aware 
that they'd made these comments. All but one of 
these instances were followed immediately by a 
command, so the wizard responded to the com- 
mand and ignored the meta-comment. The one 
stand-alone meta-comment was treated as an 
unrecognized command (an error message was 
played). 
4 Discussion 
Subjects were provided with three methods for 
controlling the pace at which information was pre- 
sented: silence, command, or acknowledgment/ 
repetition. The majority of the subjects used com- 
mands more than they used acknowledgments, but 
over one half used an acknowledgment or repeti- 
tion at least once during their interaction and 
nearly 30% used acknowledgments in preference 
to commands. This occurred despite the fact that 
subjects were given no reason to think that this 
behavior would be effective: the interface was 
deliberately limited in functionality, and voice syn- 
thesis was used instead of recorded voice to 
emphasize the artificial nature of the interaction. 
Furthermore, the interface did not offer acknowl- 
edgments to the subjects, and the subjects were 
given no instructions uggesting that the interface 
understood acknowledgments. In fact two subjects 
who did use acknowledgments expressed surprise 
that they had worked, and two who had not used 
acknowledgments reported that they would proba- 
bly have used them if they had known it would 
work. 
It is interesting to consider these results in light 
of those reported by Okato et al(1998). They 
describe a Japanese-language Wizard-of-Oz study 
in which the subjects were given some instruction 
on using the system, and in which the system both 
presented and accepted back-channel feedback. 
They found that even when the interface offered 
back channels itself the rate of subject back-chan- 
nels was somewhat lower in human-computer 
interaction than in comparable human-human con- 
versation. This makes the fact that our interface 
elicited acknowledgments without offering them 
even more encouraging. Clearly, some people are 
willing to utilize this human conversational con- 
vention in human-computer dialogue. Our post- 
experiment interviews uggest, however, that some 
people will find the use of acknowledgements 
strange or uncomfortable in human-computer inter- 
action. While self-reports of attitudes toward hypo- 
thetical situations must be treated with some 
caution, it seems reasonable to assume that even 
when such interfaces become available there will 
be users who will prefer to interact with computers 
using commands. 
Will attitudes and conversational behavior 
change as people gain experience with more 
advanced spoken-language interfaces? Despite the 
285 
relatively short duration of this test--most subjects 
completed the task itself in 15-20 minutes--some 
changes in behavior could be observed over the 
course of the dialogue. In particular, politeness 
behaviors were likely to be seen early in the dia- 
logues and then diminish as the subjects became 
more comfortable with their interaction. We specu- 
late that the use of politeness words did not reflect 
a strong underlying politeness toward the computer 
so much as a falling back on human conventions 
when faced with an unfamiliar dialogue situation. 
One subject who had used "please" 21 times dur- 
ing the interaction, for example, simply hung up 
without warning when she had finished. This con- 
trasts, however, with the findings of Nass et al
(1999) that people do offer socially-desirable 
behavior to computers. 
Would a better voice increase the incidence of 
acknowledgment behavior? Several subjects 
thought i would, and even with the current synthe- 
sized voices we saw several examples of subjects 
seemingly forgetting briefly that they were not 
talking to a human. We plan to explore this ques- 
tion in future work. 
4.1 Conclusions and Future Work 
We conducted a preliminary study to examine the 
willingness of subjects to use a particular dialogue 
act, acknowledgment, in human-computer interac- 
tion. Although the number of subjects was small, 
we saw that about half  of our subjects used 
acknowledgements or repetition at least occasion- 
ally to control the pace at which information was 
presented, and about 29% used acknowledgments 
more frequently than they used commands for that 
purpose. 
Our immediate plans include extending this 
study to a larger and gender-balanced group of sub- 
jects so that we can draw firmer quantitative con- 
clusions about the percentage of people who are 
likely to prefer this style of interaction. In particu- 
lar, we cannot conclude from the current study's 
small sample how strong the preference for using 
acknowledgment might be, especially among male 
subjects. Also, in our current study the subject 
achieved no functional benefit in using acknowl- 
edgments. With better estimates of subject prefer- 
ences, we can then proceed to our larger goal of 
comparing the usefulness and user acceptability of 
spoken language dialogue models with and without 
acknowledgment behavior (c.f. Walker, 1993). We 
also plan to explore the effect of the quality of the 
synthesized voice on the incidence of acknowledg- 
ment behavior. 
Acknowledgments 
This work was partially supported by a grant from 
Intel Research Council. The authors gratefully 
acknowledge and thank David G. Novick and the 
anonymous reviewers for their helpful comments 
and suggestions. 
References 
Gregory Aist. 1998. Expanding a Time-Sensitive 
Conversational Architecture for Turn-Taking 
to Handle Content-Driven I terruption. In Pro- 
ceedings of lCSLP 98 Fifth International Con- 
ference on Spoken Language Processing, pages 
413-417. 
Sara Basson, Stephen Springer, Cynthia Fong, 
Hong Leung, Ed Man, Michele Olson, John 
Pitrelli, Ranvir Singh, and Suk Wong. 1996. 
User Participation and Compliance in Speech 
Automated Telecommunications Applications. 
In Proceedings of ICSLP 96 Fourth Interna- 
tional Conference on Spoken Language Pro- 
cessing, pages 1676-1679. 
Susan E. Brennan. 1991. Conversation With and 
Through Computers. User Modeling and User- 
Adapted Interaction. 1:67-86. 
Jennifer Chu-Carroll and Michael K. Brown. 1997. 
Tracking Initiative in Collaborative Dialogue 
Interactions. In Proceedings of the 35th Annual 
Meeting of the Association for Computational 
Linguistics, pages 262-270. 
Herbert H. Clark and Edward F. Schaefer. 1989. 
Contributing to Discourse. Cognitive Science, 
13:259-294. 
Ronald A. Cole, David G. Novick, Pieter J. E. Ver- 
meulen, Stephen Sutton, Mark Fanty, L. F. A. 
Wessels, Jacques Ho de Villiers, Johan Schalk- 
wyk, Brian Hansen and Daniel Bumett. 1997. 
Experiments with a Spoken Dialogue System 
for Taking the U.S. Census. Speech Communi- 
cations, Vol. 23. 
Peter A. Heeman, Michael Johnston, Justin Den- 
ney and Edward Kaiser. 1998. Beyond Struc- 
tured Dialogues: Factoring out Grounding. In 
286 
Proceedings of ICSLP 98 Fifth International 
Conference on Spoken Language Processing, 
pages 863-867. 
Tatsuya Iwase, and Nigel Ward. 1998. Pacing Spo- 
ken Directions to Suit the Listener. In Proceed- 
ings of ICSLP 98 Fifth International 
Conference on Spoken Language Processing, 
Vol. 4, page 1203-1207. 
K. Kita, Y. Fukui, M. Jagata and T. Morimoto. 
1996. Automatic Acquisition of Probabilistic 
Dialogue Models. In Proceedings of lCSLP 96 
Fourth International Conference on Spoken 
Language Processing, pages 196-199. 
Clifford Nass, Youngme Moon and Paul Carney. 
1999. Are Respondents Polite to Computers? 
Social Desirability and Direct Responses to 
Computers. Journal of Applied Social Psychol- 
ogy, 29:5, pages 1093-1110. 
H. Noguchi and Yasuharu Den. 1998. Prosody- 
Based Detection of the Context of Backchan- 
nel Responses. In Proceedings of ICSLP 98 
Fifth International Conference on Spoken Lan- 
guage Processing, Vol. 2, pages 487-490. 
David G. Novick and S. Sutton. 1994. An Empiri- 
cal Model of Acknowledgment for Spoken- 
Language Systems. In Proceedings of the 32nd 
Annual Meeting of the Association for Compu- 
tational Linguistics, pages 96-101. 
Yohei Okato, Keiji Kato, Mikio Yamamoto and 
Shuichi Itahashi. 1998. System-User Interac- 
tion and Response Strategy in Spoken Dia- 
logue System. Proceedings of lCSLP 98 Fifth 
International Conference on Spoken Lan- 
guage Processing, Vol. 2, pages 495-498. 
S. L. Oviatt and P. R. Cohen and M. Wang. 1994. 
Toward Interface Design for Human Language 
Technology: Modality and Structure as Deter- 
minants of Linguistic Complexity. In Speech 
Communication, 15:3-4, pages 283-300. 
H. Sacks, E. Schegloff and G. Jefferson. 1974. A 
Simplest Systematics for the Organization of 
Turn-Taking in Conversation. Language, 
50:696-735. 
Stephen Sutton, Ronald Cole, Jacques de Villiers, 
Johan Schalkwyk, Pieter Vermeulen, Mike 
Macon, Yonghong Yan, Ed Kaiser, Brian Run- 
die, Khaldoun Shobaki, Paul Hosom, Alex 
Kain, Johan Wouters, Dominic Massaro and 
Michael Cohen. 1998. Universal Speech Tools: 
the CSLU Toolkit. In Proceedings of the Inter- 
national Conference on Spoken Language Pro- 
cessing, pages 3221-3224. 
Marc Swerts, Hanae Koiso, Atsushi Shimojima 
and Yasuhiro Katagiri. 1998. On Different 
Functions of Repetitive Utterances. In Pro- 
ceedings of lCSLP 98 Fifth International Con- 
ference on Spoken Language Processing, Vol. 
2, pages 483-487. 
Paul Taylor, Alan W. Black and Richard Caley. 
1998. The Architecture of the Festival Speech 
Synthesis System. In The Third ESCA/ 
COCOSDA Workshop on Speech Synthesis, 
pages 147-151. 
David R. Traum and Peter A. Heeman. 1996. 
Utterance Units and Grounding in Spoken Dia- 
logue. In Proceedings of ICSLP 96 Fourth 
International Conference on Spoken Lan- 
guage Processing, pages 1884-1887. 
Marilyn A. Walker. 1993. Informational Redun- 
dancy and Resource Bounds in Dialogue. Doc- 
toral dissertation, University of Pennsylvania. 
Johan Wouters, Brian Rundle and Michael W. 
Macon. 1996. Authoring Tools for Speech 
Synthesis using the Sable Markup Standard. In 
Proceedings of Eurospeech '99. 
Nigel Ward. 1996. Using Prosodic Clues to Decide 
When to Produce Back-Channel Utterances. In
Proceedings of ICSLP 96 Fourth International 
Conference on Spoken Language Processing, 
page 1724-1727. 
287 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1025?1032
Manchester, August 2008
Switching to Real-Time Tasks in Multi-Tasking Dialogue
Fan Yang and Peter A. Heeman
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
fly,heeman@cslu.ogi.edu
Andrew Kun
Electrical and Computer Engineering
University of New Hampshire
andrew.kun@unh.edu
Abstract
In this paper we describe an empirical
study of human-human multi-tasking dia-
logues (MTD), where people perform mul-
tiple verbal tasks overlapped in time. We
examined how conversants switch from the
ongoing task to a real-time task. We found
that 1) conversants use discourse markers
and prosodic cues to signal task switch-
ing, similar to how they signal topic shifts
in single-tasking speech; 2) conversants
strive to switch tasks at a less disruptive
place; and 3) where they cannot, they ex-
ert additional effort (even higher pitch) to
signal the task switching. Our machine
learning experiment also shows that task
switching can be reliably recognized using
discourse context and normalized pitch.
These findings will provide guidelines for
building future speech interfaces to sup-
port multi-tasking dialogue.
1 Introduction
Existing speech interfaces have mostly been used
to perform a single task. However, we envision
that next-generation speech interfaces will be able
to work with the user on multiple tasks at the same
time, which is especially useful for real-time tasks.
For instance, a driver in a car might use a speech
interface to catch up on emails, while occasionally
checking upcoming traffic conditions, and receiv-
ing navigation instructions.
Several speech interfaces that allow multi-
tasking dialogues have been built (Lemon et al,
2002; Kun et al, 2004). However, these interfaces
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
freely switch between different tasks without much
signaling. Thus the user might be confused about
which task the interface is talking about. Multi-
tasking dialogues, even in the best circumstances,
will be difficult for users, as users need to remem-
ber the details of each task and be aware of task
switching.
In order to build a speech interface that supports
multi-tasking dialogue, there needs to be a set of
conventions for the user and the interface to follow
in switching between tasks. To design such a set,
we propose to start with conventions that are ac-
tually used in human-human conversations, which
are natural for users to follow and probably effi-
cient in problem-solving. Multi-tasking dialogues,
where multiple independent topics overlap with
each other in time, regularly arise in human-human
conversation: for example, a driver and a navigator
in a car might be talking about their summer plans,
while occasionally interjecting road directions or
conversation about what music to listen to.
In order to better understand the human con-
ventions on task switching, we have collected the
MTD corpus (Heeman et al, 2005), which consists
of a set of human-human dialogues where pairs of
conversants have multiple overlapping verbal tasks
to perform: an ongoing task that takes a long time
to finish, and a real-time task that can be done in
a couple of turns but has a time constraint. This
paper is focused on how conversants switch from
the ongoing task to a waiting real-time task.
Previous research suggested the correlation be-
tween task switching and certain discourse con-
text; for example, conversants try to avoid task
switching in the middle of an adjacency pair (Shy-
rokov et al, 2007). In a preliminary study (Hee-
man et al, 2005), we examined the timing when
conversants switched from the ongoing task to a
real-time task using some pilot data, and found that
1025
conversants did not always switch to a real-time
task as soon as it arose, but instead waited for dif-
ferent amounts of time depending on its time con-
straint. In this study, we hypothesize that conver-
sants strive to switch at an opportune place in the
ongoing task, and we examine the discourse con-
text where task switching occurs for evidence to
support this hypothesis.
We are also interested in the cues that conver-
sants use to signal task switching. Although there
is a substantial body of research on how people
signal topic shifts in single-tasking speech (mono-
logue and dialogue), such as using discourse mark-
ers and prosodic cues (see Section 2.2), little re-
search work has been done in investigating task
switching in multi-tasking dialogues. In this study,
we examine discourse markers and prosodic cues
for their correlations with task switching. We also
examine combining these cues to recognize task
switching with machine learning techniques.
In Section 2, we review related literature. In
Section 3, we describe the MTD corpus. In Sec-
tion 4, we examine the discourse contexts in which
task switching occurs. In Section 5, we examine
the use of discourse markers and prosody associ-
ated with task switching. In Section 6, we exam-
ine automatic recognizing task switching with ma-
chine learning techniques. We conclude the paper
in Section 7.
2 Related Research
In this section, we first describe two existing
speech interfaces that allow multi-tasking dia-
logues. These speech interfaces, however, freely
switch between tasks as soon as a new task arises,
and without much signaling. We then review lit-
erature on how people signal topic shifts in single-
tasking speech, which sheds light on our research
of signaling task switching in multi-tasking dia-
logues.
2.1 Speech Interfaces for MTD
Kun et al (2004) developed a system called
Project54, which allows a user to interact with
multiple devices in a police cruiser using speech.
The architecture of Project54 allows for handling
multiple tasks overlapped in time. For example,
when pulling over a vehicle, an officer can first is-
sue a spoken command to turn on the lights and
siren, then issue spoken commands to initiate a
data query, go back to interacting with the lights
and siren (perhaps to change the pattern after the
vehicle has been pulled over), and finally receive
the spoken results of the data query. While the
current implementation of Project54 assumes that
the officer initiates the task switching (e.g. the
one about lights and the one about data query), the
system can initiate task switching too. However,
Project54 does not provide infrastructure for sig-
naling to the officer a system-initiated switch. Any
such signaling would have to be hand-coded by de-
velopers.
Lemon et al (2002) also explored multi-tasking
in a dialogue system. They built a multi-tasking
dialogue system for a human operator to direct
a robotic helicopter on executing multiple tasks,
such as searching for a car and flying to a tower.
The system keeps an ordered set of active dialogue
tasks, and interprets the user utterance in terms of
the most active task for which the utterance makes
sense. Conversely, during the system?s turn of
speaking, it can produce an utterance for any of
the dialogue tasks. Thus the system does not take
into account the user?s cost of task switching. The
system switches to a new task as soon as it arises,
instead of at an opportune place to minimize the
user?s effort. Moreover, the system does not sig-
nal when it switches between tasks. As with the
approach of Kun et al (2004) to multiple devices,
it is unclear whether an actual user will be able to
understand such conversations. The user might be-
come confused about which task the system is on.
2.2 Signaling Topic Shifts in STP
Although speech interfaces have not used cues to
signal task switching, researchers have found vari-
ous cues that people naturally use in single-tasking
speech to signal topic shifts. These cues are a good
starting point from which to study how people sig-
nal task switching in multi-tasking dialogue.
Signaling topic shifts in single-tasking speech is
about signaling the boundary of related discourse
segments that contribute to the achievement of a
task. Two types of cues have been identified for
signaling topic shifts. The first type is discourse
markers (Moser and Moore, 1995; Schiffrin, 1987;
Grosz and Sidner, 1986; Passonneau and Litman,
1997; Bangerter and Clark, 2003). Discourse
markers can be used to signal the start of a new dis-
course segment and its relation to other discourse
segments. For example, ?now? might signal mov-
ing on to the next topic, while ?well? might signal
1026
a negative or unexpected response.
The second type of cue is prosody. In read
speech, Grosz and Hirschberg (1992) studied
broadcast news and found that pause length is the
most important factor that indicates a new dis-
course segment. Ayers (1992) found that pitch
range appears to correlate more closely with hi-
erarchical topic structure in read speech than in
spontaneous speech. In spontaneous monologue,
Butterworth (1972) found that the beginning of a
discourse segment exhibited slower speaking rate;
Swerts (1995), and Passonneau and Litman (1997)
found that pause length correlates with discourse
segment boundaries; Hirschberg and Nakatani
(1996) found that the beginning of a discourse
segment correlates with higher pitch. In human-
human dialogue, similar behavior was observed:
the pitch value tends to be higher for starting a new
discourse segment (Nakajima and Allen, 1993). In
human-computer dialogue, Swerts and Ostendorf
(1995) found that the first utterance of a discourse
segment correlates with slower speaking rate and
longer preceding pause. Clearly, prosody is used
to signal topic shifts in single-tasking speech.
3 The MTD Corpus
In order to fully understand multi-tasking human-
human dialogue, we collected the MTD corpus, in
which pairs of subjects perform overlapping verbal
tasks. Details of the corpus collection can be found
in (Heeman et al, 2005).
3.1 Design of Tasks
Conversants work on two types of tasks via conver-
sation: an on-going task that takes a long time to
finish and a real-time task that just takes a couple
turns to complete but has a time constraint.
In the ongoing task, a pair of players work to-
gether to form as many poker hands as possible,
where a poker hand consists of a full house, flush,
straight, or four of a kind. Each player has three
cards in hand, which the other cannot see (players
are separated so that they cannot see each other.)
Players take turns drawing an extra card and then
discarding one, until they find a poker hand, for
which they earn 50 points; they then start over to
form another poker hand. To discourage players
from simply rifling through the cards to look for a
specific card without talking, one point is deducted
for each picked-up card, and 10 points for a missed
poker hand or incorrect poker hand. To complete
Figure 1: The game display for players
this game, players converse to share card informa-
tion, explore and establish strategies based on the
combined cards in their hands (Toh et al, 2006).
The poker game is played on computers. The
game display, which each player sees, is shown in
Figure 1. The player with four cards can click on
a card to discard it. The card disappears from the
screen, and an extra card is automatically dealt to
the other player. The player with four cards clicks
the ?Done Poker Hand? button to start a new game
once they find a poker hand.
From time to time, the computer generates a
prompt for one player to find out whether the other
has a certain picture on the bottom of the display.
The picture game has a time constraint of 10, 25
or 40 seconds, which is (pseudo) randomly deter-
mined. The players get 5 points for the picture
game if the correct answer is given in time. The
overall goal of the players is to earn as many points
as possible from the two games.
To alert the player to the picture game, two
solid bars flash above and below the player?s cards.
Thus the player will know that there is a wait-
ing picture game without taking the attention away
from the poker game. The color of the flash-
ing bars depends on how much time is remaining:
green for 26-40 seconds, yellow for 11-25 seconds
1027
and red for 0-10 seconds. The player can see the
exact amount of time in the heading for the pic-
ture game. In Figure 1, the player needs to find out
whether the other has a blue circle, with 6 seconds
left.
3.2 Corpus Annotations
We transcribed and annotated ten MTD dialogues
totaling about 150 minutes of conversation. The
dialogues were by five pairs of players, all na-
tive American-English speakers. Each pair par-
ticipated in two sessions and each session lasted
about 15 minutes. During each session, 9 picture
games (3 for each time constraint) were prompted
for each player. Of the total 180 picture game
prompted, 8 were never started by players
1
. Thus
the corpus contains 172 picture games.
The ongoing task can naturally be divided into
individual poker games, in which the players suc-
cessfully complete a poker hand. Each poker game
can be further divided into a sequence of card seg-
ments, in which players discuss which card to dis-
card, or a poker hand is found. In total, there are
105 game segments and 690 card segments in the
corpus. As well, we grouped the utterances in-
volved in each picture game into segments. Fig-
ure 2 shows an excerpt from an MTD dialogue
with these annotations. Here b7 is a game segment
in which players got a poker hand of flush; and
b8, b10, b11, b12 and b14, inside of b7, are card
segments. Also embedded in b7 are b9 and b13,
each of which is an segment for a picture game.
As can be seen, players switched from the ongo-
ing poker-playing to a picture game. After the pic-
ture game was completed, the conversation on the
poker-playing resumed.
4 Where to Switch
In a preliminary study (Heeman et al, 2005), we
found that players did not always switch to a real-
time task as soon as it arose, but instead waited for
different amounts of time depending on the time
constraint of the real-time task. We thus hypoth-
esize that players strive to switch at an opportune
place in the ongoing task (poker-playing). There
are three types of places where a player could sus-
pend the poker playing and switch to a waiting
picture game: (G) immediately after completing
a poker game (at the end of a game), (C) immedi-
1
Although in the post-experiment survey all players re-
ported that they never ignored a picture game on purpose
Figure 2: An excerpt of an MTD dialogue
ately after discarding a card (at the end of a card),
and (E) embedded inside a card segment, where
players are deciding which card to discard. In this
section, we examine where task switching occurs.
4.1 Time Constraint and Place of Switching
We first examine the place of switching under dif-
ferent time constraints. As shown in Table 1, for
the time constraint of 10s, 75% of the task switch-
ing was embedded inside a card segment, 23% at
the end of a card, and 2% at the end of a game;
for the time constraints of 25s and 40s
2
, 46% em-
bedded inside a card segment, 33% at the end of a
card, and 21% at the end of a game. The difference
in the places of switching between the time con-
straint of 10s and 25s/40s is statistically significant
(?
2
(2) = 15.92, p < 0.001). The time constraint
of 10s requires players to start a picture game very
quickly in order to complete it in time. On the
other hand, when given 25s or 40s, players are in
a less hurry to switch. Compared with 10s, when
players had 25s or 40s, the percentage of switch-
ing embedded inside a card segment decreases by
29%, while at the end of a card increases by 10%,
and at the end of a game increases by 19%. These
results suggest that when given more time, players
try to switch at the end of a game or a card.
2
We combined the time constraints of 25s and 40s because
25s seemed to be sufficient for most players.
1028
Table 1: Time constraint and place of switching
E C G Total
10s 42 (75%) 13 (23%) 1 (2%) 56 (100%)
25/40s 54 (46%) 38 (33%) 24 (21%) 116 (100%)
Table 2: Waiting time and place of switching
E C G Total
? 3s 47 (69%) 18 (27%) 3 (4%) 68 (100%)
> 3s 49 (47%) 33 (32%) 22 (21%) 104 (100%)
4.2 Waiting Time and Place of Switching
We next examine the place of task switching from
the perspective of waiting time. Waiting time
refers to the time interval between when a pic-
ture game is prompted to a player and when the
player actually starts the picture game. Our ques-
tion is: if players wait at least a certain amount
of time, where would they switch tasks? We arbi-
trary choose a time amount of 3 seconds. We as-
sume that when the waiting time is shorter than 3s,
the player starts the picture game as soon as he or
she notices it without significant waiting; in other
words, based on human reaction time, if players
are going to respond to it right away, they should
be able to do so within 3s. The results are shown
in Table 2. When the waiting time is shorter than
3s, 69% of the task switching is embedded inside
a card segment, 27% at the end of a card, and only
4% at the end of a game; when longer than 3s, 47%
is embedded inside a card segment, 32% at the end
of a card, and 21% at the end of a game. The dif-
ference in the places of switching is statistically
different (?
2
(2) = 11.88, p = 0.003). When the
waiting time is longer than 3s, the percentage of
switching inside a card decreases by 22%, while
switching at the end of a card increases by 5%, and
at the end of a game increases by 17%. These re-
sults suggest that players wait for the end of a game
or a card to switch to a picture game.
4.3 Discussion
We examined the discourse context of task switch-
ing, and found that 1) when given more time, play-
ers intend to switch to a picture game at the end of
a (poker) game or a card; and (2) if players wait,
they are waiting for the end of a (poker) game or a
card to switch to a picture game. These results sug-
gest that players strive to switch to a picture game
at the end of a (poker) game or a card.
In fact, we also observed that after a picture
game that is at the end of a game, players smoothly
start a new poker game as if nothing had hap-
pened; after a picture game that is at the end of a
card, players might sometimes remind each other
what cards they have in hands; while after a pic-
ture game that is in the middle of a card segment,
players might even repeat or clarify the previous
utterances that were said before the interruption.
It is thus reasonable to assume that switching em-
bedded inside a card segment is the most disrup-
tive, followed by at the end of a card, and at the
end of a game is the least. Our experiment results
hence suggest that players strive to switch to a real-
time task at a less disruptive place in the ongoing
task. This is consistent with Clark and Wilkes-
Gibbs (1986), that conversants try to minimize col-
laborative effort.
5 How to Switch
In Section 2.2, we discussed how people use cer-
tain cues, such as discourse markers and prosody,
to signal topic shifts in single-tasking speech. This
suggests that people might also signal task switch-
ing in multi-tasking dialogues. In this section, we
examine how players signal that they are switch-
ing from the ongoing task to a real-time task with
discourse markers and prosody.
5.1 Task Switching and Discourse Markers
Close examination of the MTD corpus found
that ?oh? was the most frequently used discourse
marker when switching to a picture game. An-
other discourse marker, ?wait? (including ?wait a
minute?), was often used together with ?oh? in the
way of ?oh wait?. Thus we examined the use of
?oh? and ?wait? in switching to a picture game.
Players used the discourse markers ?oh? or
?wait? 14.5% (25/172) of the time in switching to
a picture game. In poker playing, 5.7% (238/4192)
of utterances contain the words ?oh? or ?wait?, and
only 4.6% (32/690) of card segments are initiated
with the two discourse markers (i.e. the first ut-
terance of a card segment has ?oh? or ?wait? at
the very beginning). Players have a statistically
higher percentage of using ?oh? or ?wait? at task
switching than in poker playing (?
2
(1) = 22.89,
p < 0.001) or to initiate a card segment (?
2
(1) =
21.84, p < 0.001).
5.2 Task Switching and Prosody
To understand the prosodic cues in initiating
a topic, traditionally researchers compared the
1029
prosody of the first utterance in each segment with
other utterances (e.g. (Nakajima and Allen, 1993;
Hirschberg and Nakatani, 1996)). This approach
encounters two problems here. First, the words in
an utterance might affect the prosody. For exam-
ple, the duration and energy of ?bat? are usually
larger than ?bit?. Thus a large amount of data are
required to balance out these differences. Second,
in the MTD corpus, players typically switch to a
picture game by using a yes-no question, such as
?do you have a blue circle?, while most forward
utterances (c.f. Core and Allen 1997) in the ongo-
ing task are statements or proposals. As questions
have very different prosody than statements or pro-
posals, a direct comparison is further biased.
Examination of the MTD corpus found that 86%
(148/172) of the picture games were initiated by
?do you have ...? with optional discourse markers
at the beginning. While in the poker game, players
used ?do you have ...? 108 times to ask whether
the other had certain cards, such as ?do you have a
queen?? This observation inspired us to compare
the prosody of the phrase ?do you have? in switch-
ing to a picture game and during poker-playing.
3
This avoids comparing prosody of different words
or of different types of utterances.
We measure pitch, energy (local root mean
squared measurement), and duration of each case
of ?do you have?. We aggregate on each player and
calculate the average values. The results are shown
in Table 3. The second and third columns show the
average pitch of the phrase ?do you have? for task-
switching (SWT) and poker-playing (PKR) respec-
tively. When switching to a picture game, play-
ers? average pitch is statistically higher than poker-
playing (t(9) = 4.15, p = 0.001). In fact, for each
of the ten players, the average pitch of ?do you
have? in switching to a picture game is higher than
in poker-playing. These results show a strong cor-
relation between task switching and higher pitch.
We next examine the correlation between energy
and task switching. The fourth and fifth columns in
Table 3 show the average energy of the phrase ?do
you have? for task switching and poker-playing re-
spectively. We do not find a statistically significant
difference (t(9) = 0.80, p = 0.44). We also exam-
ine the duration of ?do you have?. The sixth and
3
Note that most cases of ?do you have? in poker-playing
are not at the beginning of a card segment. It would have
also been interesting to compare the prosody of ?do you have?
of initiating a picture game and of initiating a card segment.
However, we do not have enough data for the latter.
Table 3: Average prosodic values for each player
Player pitch (Hz) energy duration (s)
SWT PKR SWT PKR SWT PKR
4A 136 123 383 266 0.28 0.38
4B 178 156 466 506 0.32 0.30
5A 164 152 357 367 0.37 0.25
5B 214 182 231 153 0.36 0.28
6A 144 126 414 370 0.32 0.21
6B 122 117 564 496 0.25 0.23
8A 238 199 973 1061 0.36 0.21
8B 150 143 246 180 0.33 0.35
9A 109 102 538 465 0.44 0.59
9B 125 122 702 814 0.33 0.24
Table 4: Pitch (Hz) and place of switching
Player E C & G PKR
4A 137 131 123
4B 180 173 156
5A 167 161 152
5B 219 206 182
6A 146 143 126
6B 124 121 117
8A 245 233 199
8B 152 140 143
9A 110 108 102
9B 130 117 122
seventh columns in Table 3 show the results. We
do not find a statistically significant difference ei-
ther (t(9) = 1.03, p = 0.33). These results do not
support that energy or duration (i.e. speaking rate)
is correlated to task switching.
5.3 Intensity of Signal
To better understand how pitch is used in signaling
task switching, we next examine whether it corre-
lates with place of switching, i.e., switching at the
end of a game, at the end of a card, or embedded
inside a card segment. Because there are relatively
less data for switching at the end of a game (see
Table 1 and 2), we combine switching at the end
of a game and at the end of a card (C & G) as a
category.
Table 4 shows the average pitch of ?do you
have? when switching to a picture game embedded
inside a card segment, at the end of a card or game
segment, and during poker-playing. The difference
between these three conditions is statistically sig-
nificant (F (2, 9) = 15.61, p < 0.001). Switching
embedded inside a card segment has a statistically
higher pitch than switching at the end of a card or
game segment (t(9) = 5.54, p < 0.001), which
in turn has a statistically higher pitch than during
poker-playing (t(9) = 2.91, p = 0.01).
1030
5.4 Discussion
Consistent with previous research on topic shifts in
single-tasking speech, our experiments show that
switching to a real-time task correlates with the
use of certain discourse markers and prosodic vari-
ations. It is not surprising that ?oh? and ?wait? cor-
relate with task switching. Task switching involves
a sudden change of the conversation topic, and pre-
vious research found that conversants use ?oh? to
mark a change of state in orientation or awareness
(Heritage, 1984). ?Wait? is used to mark a discon-
tinuity in the ongoing topic, which is also required
by task switching. Thus people may use these dis-
course markers to signal switching to a real-time
task. In terms of prosodic variations, we find that
task switching correlates with higher pitch. This
suggests that pitch is used to signal switching to a
real-time task.
Our experiments have also shown that pitch cor-
relates to place of switching. As discussed in Sec-
tion 4.3, task switching embedded inside a card
segment is the most disruptive, switching at the
end of a card is less, and at the end of a game is the
least. Our results show that switching embedded
in a card segment has a higher pitch than switch-
ing at the end of a card or a game, which in turn has
a higher pitch than non-switching (poker-playing).
This suggests that the degree of disruptiveness cor-
responds to the value of pitch: the more disruptive
place to switch, the higher is the pitch.
From our results we speculate that pitch is used
to divert the hearer from the ongoing task, sig-
naling an unexpected event (c.f. (Sussman et al,
2003)). When task switching is more disruptive,
the speaker uses higher pitch; probably because the
hearer has a stronger expectation of the next utter-
ance to be in the context of poker-playing. The
use of higher pitch servers as a cue that the hearer
should suspend the ongoing context and interpret
the utterance in a new context. According to the
theory of least collaborative effort, the effort of
raising the pitch by the speaker is probably to re-
duce the effort of recognizing and processing the
task switching by the hearer (Clark and Wilkes-
Gibbs, 1986).
6 Machine Learning Experiment
In the previous sections, we showed the correlation
of various cues with task switching. In this sec-
tion, we conduct a machine learning experiment to
determine whether we can reliably recognize task
switching using these cues. For the reasons given
in Section 5.2, we limit our experiment to the 256
cases of ?do you have?, 148 for task switching
and 108 for poker playing. We train a decision
tree classifier (C4.5) to discriminate task switching
from poker playing. We use 5-fold cross validation
to evaluate the performance. We use decision tree
learning because its output is interpretable and we
have found its performance comparable to other
discriminative classifiers for this task.
The feature set includes 1) discourse context:
whether the utterance before ?do you have? is the
end of a poker game, the end of a card segment,
or in the middle of a card segment
4
; 2) cue word:
whether the ?do you have? follows the cue word
?oh? or ?wait?; and 3) normalized pitch: the pitch
of ?do you have? divided by the average pitch of
the speaker during the dialogue.
The decision tree learning obtains an accuracy
of 83% in identifying whether a ?do you have? ini-
tiates a task switching or belongs to poker playing;
and the recall, precision, and F measure for task
switching are 90%, 82%, and 86% respectively. As
a baseline, if we blindly assume that all cases of
?do you have? are for task switching, we have an
accuracy of 58%. Thus decision tree learning with
the three features has 43% relative error reduction
over the baseline.
To examine the structure of the decision tree, we
build a single tree from all 256 cases of ?do you
have?. We find that the decision tree first examines
the normalized pitch; if it is greater than 1.085, it is
a task-switch. Otherwise, if the discourse context
is at the end of a game, then it is for task switch-
ing; if the discourse context is embedded in a card
segment, it is for poker playing; if the discourse
context is at the end of a card: if normalized pitch
is higher than 0.975 then it is for task switching,
otherwise for poker playing. Interestingly, the fea-
ture of cue word is not used in the tree.
The performance and structure of the learned
tree suggest that discourse context and normalized
pitch are useful features for discriminating task
switching.
7 Conclusion
In this paper we have described an empirical study
of human-human multi-tasking dialogues, where
people perform multiple verbal tasks overlapped
4
Card and game segments can be determined fairly accu-
rately from the mouse clicks even without the speech.
1031
in time. We first examined the place of task
switching, i.e. where players suspend the ongoing
task and switch to a real-time task. Our analysis
showed that people strive to switch at a less dis-
ruptive place. We then examined the cues to signal
task switching. We found that task switching cor-
relates with certain discourse markers and prosodic
variations. More interestingly, the more disruptive
the switching is, the higher is the pitch. We thus
speculate that pitch is used by the speaker to help
the listener be aware of task switching and under-
stand the utterance. Finally, our machine learn-
ing experiment showed that discourse context and
pitch are useful features to reliably identify task
switching.
Acknowledgement
This work was funded by the National Science
Foundation under IIS-0326496.
References
Ayers, Gayle M. 1992. Discourse functions of pitch
range in spontaneous and read speech. Presented at
the Linguistic Society of America Annual Meeting.
Bangerter, Adrian and Herbert H. Clark. 2003. Nav-
igating joint projects with dialogue. Cognitive Sci-
ence, 27:195?229.
Butterworth, Brian. 1972. Hesitation and semantic
planning in speech. Journal of Psycholinguistic Re-
search, 4:75?87.
Clark, Herbert H. and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognitive Sci-
ence, 22:1?39.
Core, Mark G. and James F. Allen. 1997. Coding
dialogues with the DAMSL annotation scheme. In
Working Notes: AAAI Fall Symposium on Commu-
nicative Action in Humans and Machines, pages 28?
35, Cambridge.
Grosz, Barbara J. and Julia Hirschberg. 1992. Some
intonational characteristics of discourse structure. In
Proceedings of 2nd ICSLP, pages 429?432.
Grosz, Barbara J. and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Heeman, Peter A., Fan Yang, Andrew L. Kun, and
Alexander Shyrokov. 2005. Conventions in
human-human multithreaded dialogues: A prelimi-
nary study. In Proceedings of IUI (short paper ses-
sion), pages 293?295, San Diego CA.
Heritage, John. 1984. A change-of-state token and as-
pects of its sequential placement. In Atldnson, J. M.
and J. Heritage, editors, Structures of social action:
Studies in conversation analysis, chapter 13, pages
299?345. Cambridge University Press.
Hirschberg, Julia and Christine H. Nakatani. 1996. A
prosodic analysis of discourse segments in direction-
giving monologues. In Proceedings of 34th ACL,
pages 286?293.
Kun, Andrew L., W. Thomas Miller, and William H.
Lenharth. 2004. Computers in police cruisers.
IEEE Pervasive Computing, 3(4):34?41, October-
December.
Lemon, Oliver, Alexander Gruenstein, Alexis Battle,
and Stanley Peters. 2002. Multi-tasking and collab-
orative activities in dialogue systems. In Proceed-
ings of 3rd SIGdial, Philadelphia PA.
Moser, Megan and Johanna D. Moore. 1995. Inves-
tigating cue selection and placement in tutorial dis-
course. In Proceedings of 33rd ACL, pages 130?135.
Nakajima, Shin?ya and James F. Allen. 1993. A study
on prosody and discourse structure in cooperative
dialogues. Technical report, Rochester, NY, USA.
technical report.
Passonneau, Rebecca J. and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103?139.
Schiffrin, Deborah. 1987. Discourse Markers. Cam-
bridge University Press.
Shyrokov, Alexander, Andrew Kun, and Peter Hee-
man. 2007. Experiments modeling of human-
human multi-threaded dialogues in the presence of
a manual-visual task. In Proceedings of 8th SIGdial,
pages 190?193.
Sussman, E., I. Winkler, and E. Schrg?oer. 2003. Top-
down control over involuntary attention switching in
the auditory modality. Psychonomic Bulletin & Re-
view, 10(3):630?637.
Swerts, Marc and Mari Ostendorf. 1995. Discourse
prosody in human-machine interactions. In Proceed-
ings of ESCA workshop on spoken dialogue systems:
theories and applications, pages 205?208, Visgo
Denmark.
Swerts, Marc. 1995. Combining statistical and pho-
netic analyses of spontaneous discourse segmenta-
tion. In Proceedings of the 12th ICPhS, volume 4,
pages 208?211.
Toh, Siew Leng, Fan Yang, and Peter A. Heeman.
2006. An annotation scheme for agreement analy-
sis. In Proceedings of 9th ICSLP, pages 201?204,
Pittsburgh PA.
1032
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 1011?1018, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Learning Mixed Initiative Dialog Strategies
By Using Reinforcement Learning On Both Conversants
Michael S. English and Peter A. Heeman
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
Beaverton OR, 97006, USA
menglish6@gmail.com and heeman@cslu.ogi.edu
Abstract
This paper describes an application of re-
inforcement learning to determine a dia-
log policy for a complex collaborative task
where policies for both the system and a
proxy for a user of the system are learned
simultaneously. With this approach a use-
ful dialog policy is learned without the
drawbacks of other approaches that re-
quire significant human interaction. The
specific task that the agents were trained
on was chosen for its complexity and re-
quirement that both conversants bring task
knowledge to the interaction, thus ensur-
ing its collaborative nature. The results of
our experiment show that you can use re-
inforcement learning to create an effective
dialog policy, which employs a mixed ini-
tiative strategy, without the drawbacks of
large amounts of data or significant human
input.
1 Introduction
The problem of developing a dialog manager can be
expressed as the task of building a specific dialog
policy for the dialog system to follow as it interacts
with the user. A dialog policy can be thought of as an
enumeration of all of the states a dialog system can
be in, and the corresponding action to take from each
of those states. Thus a policy completely specifies
the behavior of a dialog manager.
Most conventional approaches to accomplishing
this task seek to directly model human interactions
in some manner. These techniques include hand-
crafting a policy, using a Wizard-of-Oz approach in
an iterative manner and inducing a policy from a
human-human dialog corpus. All three approaches
have shortcomings that make them less than ideal for
developing dialog systems. The approach of hand-
crafting of a dialog policy is problematic as it is
difficult to predict how a user with interact with it,
making it difficult to craft an optimal policy. To get
around this, an iterative approach can be used, with
a Wizard taking the place of the system. However, it
is still difficult to train a wizard, and it is difficult to
explore many different strategies in order to find the
optimal one. Human-human dialog can be used for
policy generation, as this should represent optimal
behavior to accomplish a task. However, computers
are not capable of behaving exactly as a human. In
addition, humans might not interact with a computer
as they would another person.
Recently a number of researchers have proposed
using reinforcement learning to alleviating the prob-
lems encountered with more conventional methods
of developing dialog policies. With the development
of a good policy evaluation function, reinforcement
learning can effectively and quickly explore a large
policy space. There is the additional benefit that it
will learn a policy that is optimal for the capabilities
of the system.
The main drawback of reinforcement learning ap-
proaches is that they require some form of conver-
sational partner to train the system against. Con-
ventionally, these partners have taken the form of a
human (Walker, 2000; Singh et al, 2002) or a simu-
lated user (Levin et al, 2000; Scheffler and Young,
2002; Georgila et al, 2005). These two types of con-
versational partners limit the complexity and diver-
sity of policies that can be generated by reinforce-
ment learning. These two approaches to training
partners limit the whole system to the abilities of
the partners themselves. For a human partner we
1011
run into the significant time and effort problems that
were present in Wizard-of-Oz and handcrafting pol-
icy development. With a simulated user the system
is limited by the complexity and flexibility of the
simulated user, which itself can require a large de-
gree of handcrafting by its creator.
In this paper, we propose a solution to the con-
versational partner problem of generating a dialog
policy with reinforcement learning. We have taken a
complex collaborative task and used reinforcement
learning, applied to both participants, to develop a
dialog policy for the task. By training both agents
simultaneously we are able to avoid the uncertain-
ties of creating a user to train against, as well as the
time and data limitations of training directly against
humans. Our training approach allows us to avoid
these conventional drawbacks even while applying
reinforcement learning to complex tasks.
Section 2 provides a brief overview of previous
work in using reinforcement learning for dialog sys-
tems. Sections 3 and 4 describe the dialog task and
its specification as a reinforcement-learning prob-
lem. Section 5 and 6 present the results of this ex-
periment and a discussion of them.
2 Related Work
A number of researchers have explored using re-
inforcement learning to create a policy for a dia-
log system. Walker (2000) trained a dialog system,
ELVIS, to learn a dialog strategy for supporting spo-
ken language access to a user?s email. The main
function of ELVIS is to provide verbal summaries of
email folders. This summary could consist of simple
statements about the number of messages or a more
detailed description of current emails.
Reinforcement learning is used to determine the
best settings for a variety of properties of the sys-
tem. For example, the system must learn to choose
between email reading styles of reading back the full
email first, reading a summary of the email first, or
prompting the user with the two choices of reading
styles. The system also learns whether it is better to
take a mixed initiative or a system initiative strategy
when interacting with the user.
To enable the learning process, ELVIS utilized
human users as its conversational partner. Users per-
formed a set of tasks with ELVIS, with each run us-
ing different state-property values, which were ran-
domly chosen for that dialog. In order to support hu-
mans as a training partner Walker restricted the pol-
icy space so that it would only contain policies that
were capable of accomplishing the available system
tasks. Thus, during training the users would not be
faced with a system that simply could not perform
the tasks asked of it.
ELVIS was trained with a Q-learning approach
that sought to determine the expected utility at each
state, where utility was a subjective function involv-
ing such variables as task completion and user sat-
isfaction. The state variables utilized in the training
process were (a) whether the user?s name is known,
(b) what the initiative style is, (c) the task progress,
and (d) what the user?s current goal is. Given these
state variables, ELVIS was able to learn the best
style to adopt in responding to the user?s requests at
various points in the dialog. One major shortcoming
of the conversational partner used with ELVIS is its
reliance upon human interaction for training. This
shortcoming is somewhat mitigated by the fact that
the learning problem was one of fitting together pre-
existing policy components, but would be severely
limiting if the goal was to learn a complete dialog
policy. The amount of data necessary for learning a
complete policy makes direct human interaction in
the learning process unrealistic.
Levin et al (2000) tackles a slightly different
reinforcement-learning task. She is learning a pol-
icy to use in a dialog system built from a small set
of atomic actions. This system is trained to provide
a verbal interface to an airline flight database. This
system is able to provide users with a way to find
flights that meet a dynamic set of criteria. The di-
alog agent?s state consists of information regarding
the departure city, destination city, flight date, etc.
Levin takes a useful approach in reducing the size
of true state space by simply tracking when a partic-
ular state variable has a value rather than including
the specific value in the state. For instance during
a dialog when the system determines that the de-
parture city is New York it does not distinguish this
from when it has determined that the departure city
is Chicago.
To converse with the dialog agent during rein-
forcement learning, Levin uses a ?simulated user.?
The simulated user is created from a corpus of hu-
man dialogs with a prior airline system. In de-
1012
veloping this user Levin makes the simplifying as-
sumption that a user?s response is based solely on
the previous prompt. Then the specific probabilities
for each user response are determined by examin-
ing the corpus for exchanges that match the possible
prompts for the new dialog agent as well as hand
crafting some of the probabilities. During the actual
learning the agent used Monte Carlo training with
exploring starts in order to fully explore the state
space.
The ?simulated user? method of supplying the
conversational partner seems difficult and not partic-
ularly applicable to tasks where a dialog corpus does
not already exist, but Kearns and Singh (1998) indi-
cates that the accuracy of the transition probabilities
for the probabilistic user is not critical for the dialog
agent to learn an optimal strategy. While this experi-
ment does allow for the dialog agent to learn a com-
plex strategy, the notion of learning against a sim-
ulated user limits the space of policies that will be
considered during training. Training against a con-
versational partner that is a model of a human au-
tomatically prejudices the system towards policies
that we would be inclined towards building by hand
and precludes the sincere exploration of all possible
policies.
3 Task Specification
For our experiment we use the task presented in
Yang and Heeman (2004), which is a modification
of the DesignWorld task of Walker (1995). The task
requires 2 conversants to agree on 5 pieces of furni-
ture to place in a room. Both conversants know all
of the furniture items that can be chosen, which dif-
fer by color, type and point value. Each conversant
also has private preferences about which furniture
items it wants in the room; such as ?if there is a red
couch in the room, I also want a lamp in the room?.
Each preference has a score. As this is a collabora-
tive task, the conversants have the goal of finding the
5 furniture items that have the highest score, where
the score is the sum of the point value of each of
the 5 chosen furniture items less the scores for any
violated preferences of either conversant.
The conversational agents work to achieve their
goal by performing the following actions: propose,
accept, reject, inform, and release turn. If there
is not a current proposal, either agent can propose
an item, which makes that item into the current pro-
posal. If there is a current proposal, the other conver-
sant can accept it or reject it. Accepting an item re-
sults in that item being included in the task solution
and removes it as the current proposal. Rejecting
a proposed item removes it as the current proposal.
When an item has been rejected it remains a valid
choice for future proposals. In addition to accept-
ing or rejecting a proposal, either conversant may
inform the other conversant of preferences that are
violated by the current proposal. A preference is vi-
olated by the current proposal if the addition of that
proposed item to the solution set would cause the
solution set to violate the preference. When a con-
versant informs of a violated preference, that prefer-
ence becomes mutually known and so affects future
decisions by both participants. Only preferences that
are not known by the other conversant are commu-
nicated. For turn taking, we include the action re-
lease turn, which the conversant that currently has
the turn can perform to signal that it is relinquishing
the turn (cf. Traum and Hinkelman, 1992). Note that
after a release turn, the other agent must make the
next move, which could itself be a release turn. The
inclusion of this action allows conversants to per-
form multiple actions in a row, such as a reject, an
inform, and a propose. Our approach to turn tak-
ing differs slightly from Yang and Heeman, as they
make it an implicit part of other actions.
In order to successfully utilize these actions in a
dialog, some reasoning effort is required of the con-
versants. Conversants must be able to determine
what preferences are violated by a pending proposal
and which of the remaining items makes the best
proposal. In order to keep the reasoning effort man-
ageable, we follow Yang and Heeman and use a
greedy algorithm to pick the item that results in the
best score for the item plus the set of items already
accepted. The conversants do not consider interac-
tions with the items that will be subsequently added
to the plan. Conversants using this greedy approach
can construct a plan that is very close to optimal.
4 Learning Specification
4.1 Agent Specification
In order to apply reinforcement learning to this task
we must formalize the conversants as reinforcement
1013
learning agents, specifying their state and actions,
as well as the environment they will interact in. In
order to reduce the size of the state space for this
task we simplified the representation of the state in
a manner similar to that done by Levin (2004). We
formulated the state of the dialog agents with many
of the more specific details of the actual state of the
task removed. For instance the agent state does not
include specific information about the furniture item
that is the pending proposal, rather the agent?s state
only indicates that there is a pending proposal.
The state specification for each agent includes
the following binary variables: Pending-Proposal,
I-Proposed, Violated-Preference, Prior-Violated-
Preferences, and Better-Alternative. Pending-
Proposal indicates whether an item has been pro-
posed but not accepted or rejected. I-Proposed in-
dicates if the agent made the most recent proposal.
Violated-Preference indicates that the pending pro-
posal has caused one or more violations of the
conversant?s private preferences. Prior-Violated-
Preferences indicates whether the conversant had
one or more violated preferences when the pending
proposal was made. This variable allows the agent
to remember what its original response to a proposal
was, even after it may have shared all of its prefer-
ences that were violated (thus creating a state where
it no longer has any violated personal preferences).
Better-Alternative indicates that the agent thinks it
knows an item that would achieve a better score than
the item currently proposed.
The actions from Section 3 can be sequenced in
a number of different orders, leading to different
policies. Unlike Yang and Heeman, who compared
handcrafted policies, we use reinforcement learning
to learn policy pairs, one part of the pair for the sys-
tem, and the other for the simulated user. We have
restricted the space of policies that can be learned.
First, we reduce the space by only considering le-
gal sequences of actions. For example, if there is a
pending proposal, another item cannot be proposed.
Second, after 5 items have been accepted, the dialog
is automatically ended. Third, to keep the space of
dialog policies small, we force an inform to inform
of all violated preferences at once.
The Reinforcement Learning states and actions of
our dialog agents capture a subset of the true state
of the dialog. Our agents do not have the ability to
distinguish between, or develop distinct policies in
response to, the proposal of a blue chair versus a red
desk. Since our formulation of the dialog agents do
not encode specific information about items or pref-
erences, the dialog environment must maintain these
details. This extra information that must include the
currently proposed item, what each agent?s private
and currently violated preferences are, what pref-
erences are shared between each agent, what items
have been accepted as part of the task solution, and
what items are still available for selection. This tech-
nique of generalizing the state space is the same as
the one used by Levin (2000), and allows us to keep
the state space at a manageable size for our task.
4.2 Reinforcement Learning
For our Reinforcement Learning algorithm we chose
to use an on-policy Monte Carlo method (Sutton and
Barto, 1998). Our chosen task is naturally episodic
since the two agents agreeing upon five items indi-
cates task completion and thus the end of the dialog,
which constitutes one learning episode. We also im-
posed a limit of 500 interactions per dialog in order
to ensure that each learning episode was finite even
if the task was not successfully completed. For
some state-action pairs our task does not allow the
accurate specification of the resulting state. In fact,
due to the way that our state representation simpli-
fies the true task environment an action choice for
many states will necessarily lead to different states
depending upon the task environment. For instance,
proposing an item will sometimes lead to that items
acceptance and sometimes it will be rejected. Given
this uncertainty our learning approach necessarily
had to learn the expected rewards of actions instead
of states.
At the end of each dialog the interaction is given
a score based on the evaluation function and that
score is used to update the dialog policy of both
agents. The state-action history for each agent is
iterated over separately and the score from the re-
cent dialog is averaged in with the expected return
from the existing policy. We chose not to include
any discounting factor to the dialog score as we pro-
gressed back through the dialog history. The deci-
sion to equally weight each state-action pair in the
dialog history was made because an action?s contri-
bution to the dialog score is not dependent upon its
1014
proximity to the end of the task. An action that ac-
cepts a proposed item at the beginning of the dialog
should be rewarded as much as an action that accepts
a proposed item later in the same dialog.
In order for the learning agents to obtain a large
enough variety of experiences to fully explore the
state space some exploration technique must be
used. We chose to use e-greedy action selection in
order to achieve this goal. With this approach the
dialog agent makes an on policy action choice with
probability 1-e and a random valid action choice the
rest of the time.
Training both agents simultaneously causes each
agent to learn its policy as an optimal response to the
opposing agent. This can create problems in the ini-
tial stages of training as each agent has an immature
policy that is based on little experience. In this situ-
ation each of the agents will associate weights with
state action pairs based on action choices of the op-
posing agent that are themselves not well developed.
As training progresses the eccentricities of the ini-
tial immature policies are perpetuated and the learn-
ing process does not converge on an effective dialog
policy for either agent.
In order to combat the problem of converging to
an effective policy we divided up the agent training
process into multiple epochs. Each epoch is com-
posed of a number of training episodes. The initial
epsilon value is set to a large value and for each suc-
cessive epoch the epsilon value for action selection
is decreased. With an initially high epsilon value
the agents are able to develop a policy that is ini-
tially weighted more heavily towards a response to
random action selection than the immature policy of
the other agent. As the epsilon value decreases, each
agent slowly adjusts its learning to be weighted more
heavily towards a response to the other agent?s pol-
icy. This approach allows the agents to develop a
minimally coherent dialog policy before beginning
to rely too heavily upon the response of the oppos-
ing agent.
Utilizing this strategy of continuously decreasing
epsilon values we were able to get both agents to
converge to an effective and coherent dialog policy.
The initial epsilon value was set to 80
4.3 Objective Function
In the reinforcement learning process the objective
function provides the dialog agents with feed-back
on the success of each dialog. The specification of
this function requires input from a human. For our
learning specification we crafted a simple function
that attempted to model a human perception of a di-
alog?s quality. Our objective function is linear com-
bination of the solution quality (S) and the dialog
length (L), taking the form:
o(S, I) = w1S ? w2L
where w1 and w2 are positive constants. As higher
values for S and lower values for L indicate better
dialogs, we subtract w2L from w1S. Instead of at-
tempting to hand pick the constants in the objective
function, we explored the effects of different values,
which we report in Section 5.2.
For our experiment we trained the dialog agents
for 200 epochs, where each epoch consisted of 200
training episodes. After the training the agents, we
then had them perform 5000 dialogs with 100% on-
policy action selection (i.e. strictly following the
learned policy). The results of these 5000 dialogs
were then combined to obtain an average plan score
and average number of interactions for the policy of
the agents. These two values are then combined ac-
cording to the objective function to obtain a numeric
score for the learned policy.
5 Results
In this section, we present the results of the dialog
policies that we learned. We first present 3 baseline
policies to which we will compare the performance
of our learned policies. We will then present results
varying the weights in the objective function in com-
parison to the baseline policies. As we are learning
a pair of policies?one for the system and one rep-
resenting the user?we explore how well the system
policy does against handcrafted ones, that will repre-
sent what a user might do, rather than test it against
its learned counter-part.
5.1 Baseline Policies
In order to provide comparative data to evaluate the
effectiveness of our approach, we will compare the
performance of the policies learned for the system
and user against several pairs of handcrafted poli-
1015
cies. The first pair implement the unrestricted ini-
tiative strategy of Yang and Heeman. Here, one con-
versant, A, proposes an item and then the other, B,
informs A of any violated preferences. B then pro-
poses an alternative and A informs B of any violated
preferences. The process repeats until an item is pro-
posed that does not violate any of the other agent?s
preferences. The second pair of policies implement
the restricted initiative policy of Yang and Heeman,
in which A proposes an item and B informs A of
any violated preferences. However, the conversants
do not switch roles: it is always A who proposes
items and B that informs of preferences and accepts.
These two policies represent successful handcrafted
pairs of dialog policies. The third pair represents a
minimum performance: A proposes an item and B
simply accepts it. This is repeated for all 5 items,
with A making all of the proposals. This policy
is an un-collaborative approach, which represents
how well A can do on its own.
5.2 Impact of Weights on Learned Policy
We first explore the ability of the reinforcement
learning algorithm to learn a dialog policy pair that
is optimal with respect to the objective function. The
only important aspect of the weights is the ratio be-
tween the two: w2/w1. We varied the ratio from
0.1 to 0.5 in increments of 0.02. For each weight
setting, we learned 66 policy pairs, and tested each
policy pair on 1000 different task configurations. We
compared the average objective function score of the
learned policy pairs with the baseline restricted pol-
icy pair (cf. Scheffler and Young, 2002). Figure 1
shows the percentage of the learned policies that per-
form at least as well as the unrestricted policy pair
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.0
2
0.0
6 0.1 0.1
4
0.1
8
0.2
2
0.2
6 0.3 0.3
4
0.3
8
0.4
2
0.4
6
w2/w1
Pe
rc
en
t
Figure 1: Percentage of learned policies performing
better than unrestricted baseline pair.
at each weight setting. Interestingly, it is clear that
there is a lack of convergence in the learning pro-
cess, no weight ratio learns a good policy 100% of
the time. Additionally, we see that as the weight
ratio increases (putting more emphasis on shorter
dialogs), the ability of the algorithm to learn good
policies decreases. As the objective function gives
this aspect more weight, it is more difficult for the
objective function to learn the importance of solu-
tion quality. We think this lack of convergence is
due to learning both the system and a simulated user
at the same time, which is a more difficult reinforce-
ment learning problem than just learning the policy
for the system against a fixed user.
5.3 Lack of Convergence
To better understand the lack of convergence, we ex-
plore when a single weight is chosen for the objec-
tive function. For this analysis, we restricted our-
selves to the objective function having a ratio for
w2/w1 of 0.1, one of the best performing weights
from section 5.2. For this setting, we learned a num-
ber of policy pairs, each learned from a different se-
quence of task configurations. We then tested each
policy pair on 1000 task configurations, in which ac-
tions are selected strictly according to the learned
policy. This gives us 1000 dialogs for each policy
pair. We then computed the average objective func-
tion score for each policy pair and plotted them as a
histogram in Figure 2. As can be seen, at this weight
setting, 63% of the learned policies achieved an ob-
jective function score around 44.8. However, the
rest achieved a performance substantially less than
this. Hence, the reinforcement learning procedure
does not always converge on an optimal solution.
To better understand why reinforcement learning
is not always converging, we examined the compo-
nents of the objective function score: solution qual-
ity and dialog length. Figure 3 uses the same x-axis
as Figure 2: average objective function score. The
y-axis plots the average solution quality and average
dialog length. We see that at this weight ratio, all
learned dialogue pairs are very consistent in solution
quality, but that the difference in objective function
scores is mainly due to differences in dialog length.
This is consistent with our earlier observation that
the reinforcement learning strategy sometimes dis-
proportionately favors shorter dialog length.
1016
05
10
15
20
25
30
35
40
45
43.82 43.93 44.05 44.16 44.27 44.39 44.50 44.61 44.73 44.84
Objective Function Score
In
st
an
ce
s
Figure 2: Average objective function scores for poli-
cies learned with w2/w1 = 0.1.
0
5
10
15
20
25
30
35
40
45
50
43.82 43.93 44.05 44.16 44.27 44.39 44.50 44.61 44.73 44.84
Objective Function Score
Solution Quality Dialog Length
Figure 3: Variation of solution quality and dialogue
length versus objective function score for policies
learned with w2/w1 = 0.1.
5.4 Consistency of Policies
For the weight ratio of 0.1, the reinforcement learn-
ing algorithm usually finds a good policy pair. To
further improve the likelihood of this happening, we
could learn multiple policy pairs, and then pick the
best performing one. In this section, we compare
learned policies chosen in this way against the re-
stricted baseline pairs. We learned 10 sets of 10 dia-
logue pairs. We then ran each on 1000 task configu-
rations and chose the best performing policy pair in
each set. We then ran the resulting 10 policy pairs
on another set of 1000 task configurations. Table 1
gives the average objective function score for each of
the 10 learned policy pairs and the 3 baseline pairs.
From the table, we see that the learned policy pair
performs almost as well as the restricted policy pair,
for both solution quality and dialog length.
5.5 Robustness of Learned Policies
All of the results so far have used the learned pol-
icy for the system interacting with the corresponding
policy that was learned for the user. However, there
Objective Solution Dialog
Function Quality Length
Learned Policies 44.90 46.71 18.17
Restricted 45.04 46.89 18.44
Unrestricted 44.40 46.80 24.07
Uncollaborative 32.52 33.62 11.00
Table 1: Comparison of Learned Policies
is no guarantee that a real user will behave like the
learned policy. Thus, the true test of our approach
is to run the learned system policy against actual
users. The problem with testing our policies against
actual users is that there are a number of aspects
of dialog that we have not modeled, such as non-
understandings, misunderstandings, and even pars-
ing sentences into the action specification and gener-
ating sentences from the action specification. Thus,
as a simplification we tested our learned system pol-
icy on the handcrafted baseline policies.
For the weight ratio of 0.1, we learned 10 sets of
10 pairs of policies and choose the best policy pair
from each set. For each of the 10 policy pairs, we ran
the system policy against the 6 individual policies
from the 3 baseline policy pairs. We changed the
hand-crafted policies slightly from Yang and Hee-
man so that the policies would not fail if they en-
countered unexpected input. For example, for the
restricted policy for A (the conversant who proposes
but never informs), if the learned policy proposes an
item, A always rejects it. For the restricted policy
for B (the conversant who informs but never pro-
poses), if the learned policy releases the turn when
there is not an item proposed, B simply releases the
turn back to the learned policy.
Figure 4 shows the resulting average objective
function scores on 1000 dialog runs. For each base-
line policy, we show the performance with the pol-
icy pair, and then with each side of the baseline pol-
icy interacting with the learned policy. We see that
although the performance of the learned policy is
not as good as with the handcrafted pair, the perfor-
mance is close, with the major shortcoming being
a general increase in dialog length. Thus, the poli-
cies that we have learned our robust against different
strategies a user might want to use.
1017
Solution Quality
0
5
10
15
20
25
30
35
40
45
50
Uncollaborative Unrestricted Restricted
Learned with Baseline A Learned with Baseline B Baseline Pair
Dialog Length
0
5
10
15
20
25
30
35
40
45
50
Uncollaborative Unrestricted Restricted
Learned with Baseline A Learned with Baseline B
Baseline Pair
Figure 4: Learned dialogue policies interacting with
baseline policies.
6 Conclusion
In this paper, we proposed using reinforcement for
learning a dialog strategy for the system. Our ap-
proach differs from past research in that we learn
the system policy in conjunction with learning a user
policy. This approach of learning the user policy al-
lows us to minimize human involvement, as neither
a training corpus must be collected nor a simulated
user built. Thus, the only human input required for
this approach was to define the domain task and to
define success in that domain. While our training
approach did not always find an effective policy, we
overcame this obstacle by carefully choosing a ra-
tio for the weights in the objective function and by
running the learning algorithm multiple times. Our
approach resulted in learned system and user dia-
log policies that achieved comparable performance
with handcrafted system and user policy pairs. Fur-
thermore, the learned system policies were robust.
When the learned system policies ?conversed? with
the handcrafted user policies, the resulting dialogs
had comparable solution quality to what the hand-
crafted system and user policies achieved together.
Even with the lack of convergence our approach
could be applied to more complicated domains in or-
der to learn an effective dialog policy. Our approach
would be especially useful in situations where there
are no existing corpora of human-human interac-
tions for the domain or as a way to provide a check
against a policy based on human intuition. In most
situations where the domain requires significant col-
laboration between the dialog system and the user,
training both the system and a user simultaneously
will prove to be much less costly and labor intensive
approach.
7 Acknowledgments
The authors thank John Moody and Fan Yang for
helpful discussions. Partial funding for this research
was provided by the National Science Foundation
under grant IIS-0326496. The first author is now at
Google.
References
K. Georgila, J. Henderson, and O. Lemon. 2005. Learn-
ing user simulations for information state update dia-
logue systems. In Eurospeech, Lisbon Portugal.
M. Kearns and S. Singh. 1998. Finite-sample conver-
gence rates for q-learning and indirect algorithms. In
NIPS, Denver CO.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1):11?23.
K. Scheffler and S. J. Young. 2002. Automatic learn-
ing of dialogue strategy using dialogue simulation and
reinforcement learning. In HLT, pages 12?18.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue managment with reinforcement
learning: Experiments with the njfun system. Journal
of Artificial Intelligence Research, 16:105?133.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press, Cambridge MA.
D. Traum and E. Hinkelman. 1992. Conversation acts in
task-oriented spoken dialogue. Computational Intelli-
gence, 8(3):575?599.
M. Walker. 1995. Testing collaborative strategies by
computational simulation: Cognitive and task effects.
Knowledge-Based Systems, 8:105?116.
M. Walker. 2000. An application of reinforcement learn-
ing to dialog strategy selection in a spoken dialogue
system. Journal of Artificial Intelligence Research.
F. Yang and P. Heeman. 2004. Using computer simu-
lation to compare two models of mixed-initiative. In
ICSLP.
1018
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 20?21,
Vancouver, October 2005.
DialogueView: an Annotation Tool for Dialogue
Fan Yang and Peter A. Heeman
Center for Spoken Langauge Understanding
OGI School of Science & Engineering
Oregon Health & Science University
20000 NW Walker Rd., Beaverton OR, U.S.A. 97006
{fly, heeman}@cslu.ogi.edu
1 Introduction
There is growing interest in collecting and annotating cor-
pora of language use. Annotated corpora are useful for
formulating and verifying theories of language interac-
tion, and for building statistical models to allow a com-
puter to naturally interact with people.
A lot of annotation tools have been built or are be-
ing built. CSLU Toolkit (Sutton et al, 1998) and Emu
(Cassidy and Harrington, 2001) are built for words tran-
scription or speech events (such as accent); DAT is built
for coding dialogue acts using the DAMSL scheme (Core
and Allen, 1997); Nb is built for annotating hierarchical
discourse structure (Flammia, 1998); annotation toolkits,
such as Mate (McKelvie et al, 2001), AGTK (Bird et al,
2001), and Nite (Carletta et al, 2003), are built for users
to create their own tools. In this demo, we will present a
novel tool, DialogueView, for annotating speech repairs,
utterance boundaries, utterance tags, and hierarchical dis-
course structure altogether.
The annotation tool, DialogueView, consists of three
views: WordView, UtteranceView, and BlockView.
These three views present different abstractions of a di-
alogue, which helps users better understand what is hap-
pening in the dialogue. WordView shows the words time-
aligned with the audio signal. UtteranceView shows the
dialogue as a sequence of utterances. It abstracts away
from the exact timing of the words and can even skip
words, based on WordView annotations, that do not im-
pact the progression of the dialogue. BlockView shows
the dialogue as a hierarchy of discourse blocks, and ab-
stracts away from the exact utterances that were said. An-
notations are done at the view that is most appropriate for
what is being annotated. The tool allows users to eas-
ily navigate among the three views and it automatically
updates all views when changes are made in one view.
DialogueView makes use of multiple views to present
different abstractions of a dialogue to users. Abstraction
helps users focus on what is important for different an-
notation tasks. For example, for annotating speech re-
pairs, utterance boundaries, and overlapping and aban-
doned utterances, WordView provides the exact timing
information. For coding speech act tags and hierarchi-
cal discourse structure, UtteranceView shows a broader
context and hides such low-level details.
In this presentation, we will show how DialogueView
helps users annotate speech repairs, utterance boundaries,
utterance tags, and hierarchical discourse blocks. Re-
searchers studying dialogue might want to use this tool
for annotating these aspects of their own dialogues. We
will also show how the idea of abstraction in Dialogue-
View helps users understand and annotate a dialogue. Al-
though DialogueView focuses on spoken dialogue, we
feel that abstraction can be used in annotating mono-
logues, multi-party, and multi-modal interaction, with
any type of annotations, such as syntactic structure, se-
mantics and co-reference. Researchers might want to
adopt the use of abstraction in their own annotation tools.
2 WordView
The first view is WordView, which takes as input two au-
dio files (one for each speaker), the words said by each
speaker and the start and stop times of each word (in
XML format), and shows the words time-aligned with the
audio signal. This view is ideal for seeing the exact tim-
ing of speech, especially overlapping speech. Users can
annotate speech repairs, utterance boundaries, and utter-
ance tags in WordView.
WordView gives users the ability to select a region of
the dialogue and to play it. Users can play each speaker
channel individually or both combined. Furthermore, Di-
alogueView allows users to aurally verify their speech re-
pair annotations. WordView supports playing a region
of speech but with the annotated reparanda and editing
terms skipped over. We have found this useful in decid-
ing whether a speech repair is correctly annotated. If one
has annotated the repair correctly, the edited speech will
sound fairly natural.
3 UtteranceView
The annotations in WordView are utilized in building the
next view, UtteranceView. This view shows the utter-
ances of two speakers as if it were a script for a movie.
To derive a single ordering of the utterances of the two
20
speakers, we use the start time of each utterance as anno-
tated in WordView. We refer to this process as linearizing
the dialogue (Heeman and Allen, 1995). The order of the
utterances should show how the speakers are sequentially
adding to the dialogue, and is our motivation for defin-
ing utterances as being small enough so that they are not
affected by subsequent speech of the other speaker.
Users can annotate utterance tags in UtteranceView be-
sides WordView. WordView is more suitable for tags that
depend on the exact timing of the words, or a very lo-
cal context, such as whether an utterance is abandoned
or incomplete, or whether there is overlap speech. Utter-
anceView is more suitable for tags that relate the utter-
ance to other utterances in the dialogue, such as whether
an utterance is an answer, a statement, a question, or an
acknowledgment. Whether an annotation tag can be used
in WordView or UtteranceView (or both) is specified in
the configuration file. Which view a tag is used in does
not affect how it is stored in the annotation files (also in
XML format).
In UtteranceView, users can annotate hierarchical
groupings of utterances. We call each grouping a block,
and blocks can have other blocks embedded inside of
them. Each block is associated with a summary, which
users need to fill in. Blocks can be closed; when a block is
closed, it is replaced by its summary, which is displayed
as if it were said by the speaker who initiated the block.
Just as utterances can be tagged, so can discourse blocks.
The block tags scheme is also specified in the configura-
tion file.
UtteranceView supports two types of playback. The
first playback simply plays both channels mixed, which is
exactly what is recorded. The second playback is slightly
different. It takes the linearization into account and dy-
namically builds an audio file in which each utterance
in turn is concatenated together, and a 0.5 second pause
is inserted between each utterance. This gives the user
an idealized rendition of the utterances, with overlapping
speech separated. By comparing these two types of play-
backs, users can aurally check if their linearization of the
dialogue is correct.
Users can use the configuration file to customize Utter-
anceView. Typically, UtteranceView gives users a clean
display of what is going on in a dialogue. This clean
display removes reparanda and editing terms in speech
repairs, and it also removes abandoned speech, which
has no contributions to the conversation.1 UtteranceView
also supports adding texts or symbols to an utterance
based on the tags, such as adding ??? after a question,
?...? after an incomplete utterance, and ?+? at both the
beginning and end of an overlapping utterance to signal
the overlap. (c.f. Childes scheme (MacWhinney, 2000)).
1Note that these clean processes are optional. Users can
specify them in the configuration file.
4 BlockView
In addition to WordView and UtteranceView, we are ex-
perimenting with a third view, which we call BlockView.
This view shows the hierarchical structure of the dis-
course by displaying the summary and intention (DSP)
for each block, indented appropriately. BlockView gives
a very concise view of the dialogue. It is also convenient
for navigating in the dialogue. By highlighting a line and
then pressing Sync, the user can see the corresponding
part of the dialogue in UtteranceView and WordView.
5 Availability
DialogueView is written in Incr Tcl/Tk. We also use the
snack package for audio support; hence DialogueView
supports audio file formats of WAV, MP3, AU, and oth-
ers (see http://www.speech.kth.se/snack/ for the complete
list). DialogueView has been tested on Microsoft Win-
dows (2000 and XP) and Redhat Enterprise Linux.
DialogueView is freely available for research and
educational use. Users should first install a stan-
dard distribution of Tcl/Tk, such as ActiveTcl from
http://www.tcl.tk, and then download DialogueView from
http://www.cslu.ogi.edu/DialogueView. The distribution
also includes some examples of annotated dialogues.
References
Steven Bird et al 2001. Annotation tools based on the anno-
tation graph API. In Proceedings of ACL/EACL 2001 Work-
shop on Sharing Tools and Resources for Research and Edu-
cation.
Jean Carletta et al 2003. The NITE XML toolkit: flexible an-
notation for multi-modal language data. Behavior Research
Methods, Instruments, and Computers, April. Special Issue
on Measuring Behavior.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level an-
notation in the Emu speech database management system.
Speech Communication, 33:61?77.
Mark G. Core and James F. Allen. 1997. Coding dialogues with
the DAMSL annotation scheme. In Proceedings of AAAI Fall
1997 Symposium.
Giovanni Flammia. 1998. Discourse Segmentation Of Spo-
ken Dialogue: An Empirical Approach. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
Peter A. Heeman and James Allen. 1995. Dialogue transcrip-
tion tools. Trains Technical Note 94-1, URCS, March.
Brian MacWhinney. 2000. The CHILDES Project: Tools for
Analyzing Talk. Mahwah, NJ:Lawrence Erlbaum Associates,
third edition.
D. McKelvie, et al 2001. The MATE Workbench - An anno-
tation tool for XML coded speech corpora. Speech Commu-
nication, 33(1-2):97?112. Special issue, ?speech Annotation
and Corpus Tools?.
Stephen Sutton et al. 1998. Universal speech tools: The CSLU
toolkit. In Proceedings of 5th ICSLP, Australia.
21
Proceedings of NAACL HLT 2007, pages 17?24,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Avoiding and Resolving Initiative Conflicts in Dialogue?
Fan Yang and Peter A. Heeman
Center for Spoken Language Understanding
OGI School of Science & Engineering
Oregon Health & Science University
{fly,heeman}@cslu.ogi.edu
Abstract
In this paper, we report on an empirical study
on initiative conflicts in human-human conver-
sation. We examined these conflicts in two
corpora of task-oriented dialogues. The re-
sults show that conversants try to avoid initia-
tive conflicts, but when these conflicts occur,
they are efficiently resolved by linguistic de-
vices, such as volume.
1 Introduction
Current computer dialogue systems tend to be system-
initiative. Although there are some mixed-initiative sys-
tems that allow the user to make a request or state a goal,
such systems are limited in how they follow natural ini-
tiative behavior. An example is where the system always
releases the turn whenever the user barges in. However,
in a complex domain where the computer system and hu-
man user are collaborating on a task, the computer sys-
tem might need to interrupt the human user, or might
even need to fight with the human user over the turn.
Thus the next generation of computer dialogue systems
need a better model of initiative (Horvitz, 1999). In what
situations can the system try to take initiative from the
user? What devices can the system use to fight for ini-
tiative? We propose examining human-human conversa-
tion to answer these questions. Once we understand the
conventions people adopt in negotiating initiative, we can
implement them in a computer dialogue system to create
natural interactivity.
In this research work, we examined two corpora of
human-human conversation: the Trains corpus (Heeman
and Allen, 1995) and the MTD corpus (Heeman et al,
2005). The research purpose is to understand conver-
sants? behavior with initiative conflicts, which we define
a situation where both conversants try to direct the con-
versation at the same time, but one of them fails. We
?This work was funded by the National Science Foundation
under IIS-0326496.
found that (1) conversants try to avoid initiative con-
flicts; and (2) initiative conflicts, when they occur, are
efficiently resolved by linguistic devices, such as volume.
In Section 2, we review related research work on mod-
eling initiative and turn-taking. Dialogue initiative and
turn-taking are two intertwined research topics. When
conversants fight to show initiative, they are also fighting
for the turn to speak. In Section 3, we describe the two
corpora and their annotations. In Section 4, we define
initiative conflict and give an example. In Section 5, we
present the evidence that conversants try to avoid initia-
tive conflicts. In Section 6, we present evidence that ini-
tiative conflicts are efficiently resolved by linguistic de-
vices. We discuss our findings in Section 7 and future
work in Section 8.
2 Related Research
2.1 Initiative Models
Researchers have been investigating how people man-
age dialogue initiative in their conversation. Whittaker
and Stenton (1988) proposed rules for tracking initiative
based on utterance types; for example, statements, pro-
posals, and questions show initiative, while answers and
acknowledgements do not. Smith (1993) proposed four
different initiative strategies with differing amounts of
control by the system. Chu-Carrol and Brown (1998)
distinguished dialogue initiative from task initiative, and
proposed an evidential model of tracking both of them.
Cohen et al (1998) proposed presenting initiative in dif-
ferent strengths. Some researchers related initiative to
discourse structure. Walker and Whittaker (1990) found
a correlation between initiative switches and discourse
segments. Strayer et al (2003) proposed the restricted
initiative model in which the initiator of a discourse seg-
ment, who introduces the discourse segment purpose, is
in control of the segment and shows most of the initia-
tive. These models allowed the possibility that multiple
conversants will want to show initiative at the same time;
however, none of them addressed initiative conflicts.
Guinn (1998) studied another type of initiative, task
initiative, which is about directing the problem-solving
17
of a domain goal. Guinn proposed that the person who
is more capable of coordinating the current goal is the
person who should be leading the dialogue. Initiative
switches between conversants as goals get pushed and
popped from the problem-solving stack. However, be-
cause conversants only have incomplete information, ini-
tiative conflicts might occur when conversants overesti-
mate their own capability or underestimate the other?s.
Guinn proposed a negotiation model to resolve these con-
flicts of task initiative. Conversants negotiate by inform-
ing each other of positive and negative information of
their plans to achieve the goal. By comparing each other?s
plan, the conversant whose plan has the higher probabil-
ity of success takes initiative. Guinn?s research on con-
flicts of task initiative, however, has little bearing on con-
flicts of dialogue initiative. For dialogue initiative, very
often, one of the conversants just gives up the attempt
very quickly, without giving a justification. As stated by
Haller and Fossum (1999):?... conflicts are often simple
clashes that result from both participants trying to take
the initiative at the same time. Such conflicts do not nec-
essarily require complex negotiation to resolve. Often,
unwritten rules based on factors like social roles, personal
assertiveness, and the current locus of control play a part
in determining who will give away.? However, Haller and
Fossum did not further investigate how conversants effi-
ciently resolve conflicts of dialogue initiative.
2.2 Turn-Taking and Initiative
Turn-taking in conversation is highly related to initiative.
Conversants have to possess the turn in order to show ini-
tiative. When conversants are fighting for initiative, they
are also fighting for the turn to speak. Thus the mech-
anisms of turn-taking might share some similarity with
initiative. On the other hand, turn-taking is different from
initiative; for example, an answer takes a turn, but an-
swering does not show initiative.
Turn-taking in conversation has been discussed in lin-
guistics literature. Duncan (1974) examined cues (ges-
ture, acoustic, and linguistic) that conversants use to sig-
nal turn-taking or turn-releasing. A model based on these
signals was created to account for conversants? turn-
taking behavior. In this model, miscues are the cause of
overlapping speech: for example, the hearer misrecog-
nizes the speaker?s cue to keep the turn, or the speaker
fails to properly signal.
Sacks et al (1974) proposed a set of rules for turn-
taking: the current speaker can select somebody else to
speak; otherwise, hearers can self-select to speak; oth-
erwise, the speaker can self-select to speak. This model
suggested that overlapping speech results from either the
hearer waiting too long to speak, or the speaker not wait-
ing long enough.
Schegloff (2000) examined overlapping speech in de-
tail in human conversation. He concluded that (1) fights
for turn are often accompanied with sudden acoustic al-
teration, such as louder volume, higher pitch, and faster
or slower speaking rate; (2) the vast majority of fights for
turn are resolved very quickly; (3) fights for turn are re-
solved through an interactive procedure, e.g. syllable by
syllable negotiation, using devices such as volume, pitch,
and speaking rate. However, his analysis only consisted
of a few examples; no statistical evidence was given. It
is thus unclear whether his conclusions represent human
conventions of initiative conflict, or are occasional behav-
ior that would only occur under special circumstances.
3 Corpora and Annotations
To understand human behavior in initiative conflicts, we
examined two corpora, the Trains corpus and the MTD
corpus. These two corpora have very different domain se-
tups. The distinct behavior seen in each corpus will help
inform us how domain settings affect initiative, while the
common behavior will help inform us the cross-domain
human conventions.
3.1 The Trains Corpus
The Trains corpus is a collection of human-human task-
oriented dialogues, in which two participants work to-
gether to formulate a plan involving the manufacture and
transportation of goods. One participant, the user, has a
goal to solve; and the other participant, the system, knows
the detailed domain information including how long it
takes to ship and manufacture goods.
We annotated eight Trains dialogues totaling about
45 minutes using the tool DialogueView (Yang et al,
2007). We tagged each utterance with a simplified
DAMSL scheme (Core and Allen, 1997). Utterances
were tagged as forward or backward functions, stalls, or
non-contributions. Forward functions include statements,
questions, checks and suggestions. Backward functions
include agreements, answers, acknowledgments, repeti-
tions and completions. Examples of stalls are ?um? and
?let?s see?, used by a conversant to signal uncertainty of
what to say next or how to say it. Non-contributions in-
clude abandoned and ignored utterances. The flow of
the dialog would not change if non-contributions were
removed.
Hierarchical discourse structure was annotated follow-
ing Strayer et al (2003). To determine whether a group
of utterances form a discourse segment, we took into ac-
count whether there exists a shared goal introduced by
one of the conversants (cf. Grosz and Sidner, 1986).
3.2 The MTD Corpus
The MTD corpus contains dialogues in which a pair of
participants play two games via conversation: an ongoing
18
game that takes a relatively long time to finish and an
interruption game that can be done in a couple turns but
has a time constraint. Both games are done on computers.
Players are separated so that they cannot see each other.
In the ongoing game, the two players work together to
assemble a poker hand of a full house, flush, straight, or
four of a kind. Each player has three cards in hand, which
the other cannot see. Players take turns drawing an extra
card and then discarding one until they find a poker hand,
for which they earn 50 points. To discourage players from
simply rifling through the cards to look for a specific card
without talking, one point is deducted for each picked-up
card, and ten points for a missed or incorrect poker hand.
To complete this game, players converse to share card
information, and explore and establish strategies based
on the combined cards in their hands.
From time to time, the computer generates a prompt
for one player to start an interruption game to find out
whether the other player has a certain picture on the
screen. The interruption game has a time constraint of
10, 25, or 40 seconds, which is (pseudo) randomly deter-
mined. Players get five points for the interruption game
if the correct answer is given in time. Players are told to
earn as many points as possible.
We annotated six MTD dialogues totaling about 90
minutes. Utterances were segmented based on player?s
intention so that each utterance has only one dialogue
act that is to share information, explore strategies, sug-
gest strategies, or maintain an established strategy (Toh
et al, 2006). We applied the same simplified DAMSL
scheme on utterance tag annotations. Figure 1 shows an
annotated excerpt of an MTD dialogue. We grouped ut-
terances into blocks. Block b21 is a game block in which
conversants completed a poker hand. Blocks b22 and b23
are two card blocks in which conversants picked up a
new card, discussed what they had in hand, and chose
a card to discard. Block b24 is an interruption segment
in which conversants switched their conversation to the
interruption game. No claim is made that the game and
card blocks are discourse segments according to Grosz
and Sidner?s definition (1986).
4 Defining Initiative Conflicts
An initiative conflict occurs when a conversant?s attempt
to show initiative fails because someone else is show-
ing initiative at the same time. Following Whittaker
and Stenton (1988), we use utterance tags to determine
whether an utterance shows initiative: forward functions
show initiative while others do not. Non-contributions
are viewed as failed attempt to show initiative. Thus we
identify initiative conflicts as overlapping utterances that
involve either a forward function and a non-contribution
or two non-contributions.
Figure 2 gives an example of an initiative conflict from
Figure 1: An excerpt of an MTD dialogue
the MTD corpus. The top conversant says ?that?s pair of
threes and pair of fours?, which ends at time point A. Af-
ter a short pause, at time B, the bottom conversant asks
?how many threes do you have?, which is overlapped by
the top conversant?s second utterance ?I?ll drop? at time
C. The top conversant then abandons the attempt of show-
ing initiative at time D. Hence the bottom speaker is the
winner of this initiative conflict.
We use the term preceding-pause to refer to the time
interval between the end of the previous utterance and
the first utterance that is involved in the overlap (from A
to B in Figure 2). Offset refers to the interval between
the start times of the two overlapped utterances (from B
to C). Duration refers to the time interval from the begin-
ning of overlap till the end of overlap (from C to D).
In the Trains corpus, there are 142 cases of overlap-
ping speech, 28 of which are initiative conflicts. Of the
remaining, 96 cases involve a backward function (e.g. an
acknowledgment overlapping the end of an inform), and
10 cases involve a stall. The remaining 8 cases are other
types of overlap, such as a collaborative completion, or
conversants talking about the same thing: for example,
one saying ?we are a bit early? and the other saying ?we
are a little better?.
In the MTD corpus, there are 383 cases of overlapping
speech, 103 of which are initiative conflicts. Of the re-
maining, 182 cases involve a backward function, 21 cases
involve a stall, and 77 cases are others. Initiative conflicts
19
Figure 2: An illustration of an initiative conflict
are more frequent in the MTD corpus (103 cases in 90
min) than in the Trains corpus (28 cases in 45 min).
There are three cases in the Trains and thirteen cases in
the MTD corpus where the preceding-pause is negative,
i.e. the first overlapped utterance is started before the
other conversant finishes the previous utterance. Some-
times the hearer starts a little bit early to take the turn. If
the original speaker does not intend to release the turn,
a conflict arises. Because these cases involve three ut-
terances, we exclude them from our current analysis and
save them for future research.1 This leaves 25 cases in
the Trains corpus and 90 cases in the MTD corpus for
analyzing initiative conflicts.
5 Avoiding Initiative Conflicts
In this section, we show that conversants try to avoid ini-
tiative conflicts by examining both the offset of initiative
conflicts and the urgency levels.
5.1 Offset of Initiative Conflicts
The offset of an initiative conflict indicates where the
conflict happens. A short offset indicates that the conflict
happens at the beginning of an utterance, while a long
offset indicates an interruption in the middle.
Figure 3 shows the cumulative distribution function
(CDF) for offsets for both corpora individually. The mean
offset is 138ms for the Trains corpus, and 236ms for
the MTD corpus. In comparison to the average length
of forward utterances (2596ms in the Trains corpus and
1614ms in the MTD corpus), the offset is short. More-
over, in the Trains corpus, 88% of offsets are less than
300ms (and 80% less than 200ms); in the MTD corpus,
75% of offsets are less than 300ms. Thus most initiative
conflicts happen at the beginning of utterances.
1These cases of negative value preceding-pause are in fact
very interesting. They seem to contradict with Sacks et
al. (1974)?s model that the hearer has priority to self select to
speak. If Sacks et al is correct, the speaker should wait a cer-
tain amount of time in order not to overlap with the hearer, but
in these cases we see that the speaker self-selects to speak with-
out taking into account whether the hearer self-selects to speak
or not.
0 500 1000 15000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Offset (ms)
Perc
enta
ge
A: Trains
0 500 1000 15000
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Offset (ms)
Perc
enta
ge
B: MTD
Figure 3: CDF plot for offsets of initiative conflicts
Few initiative conflicts have offsets longer than 500ms.
There is one instance in the Trains corpus and eleven in
the MTD corpus. Four cases are because the second con-
versant has something urgent to say. For example, when
an interruption game is timing out, conversants would in-
terrupt, sometimes in the middle of an utterance, which
results in a long offset. Another six cases are due to mis-
cues. Figure 4 shows an example. Conversant B said ?I
have two aces? with end-of-utterance intonation, paused
for about half a second, and then added ?and a seven?.
The ending intonation and the pause probably misled
conversant A to believe that B had finished, and thus A
started a new forward utterance, which overlapped with
B?s extension. A?s utterance was then quickly abandoned.
In these cases, it is ambiguous whether B?s utterance ?I
have two aces ... and a seven? should be further chopped
into two utterances. The final two cases are intrusions,
with an example shown in Figure 5. Conversant A cut in
probably because he was confident with his decision and
wanted to move on to the next card. In such cases, the
intruder might be perceived as being rude.
20
B: I have two aces and a seven
A: I have .
Figure 4: Long offset: miscue
B: well let?s just
A: it?s no help I think it goes away
Figure 5: Long offset: intrusion
The preponderance of short offsets provides evidence
that conversants try to avoid initiative conflicts. When A
detects that B is talking, A should not attempt to show
initiative until the end of B?s utterance in order to avoid
conflicts, unless there is an urgent reason. If conversants
did not take into account whether someone else is speak-
ing before attempting initiative, we would see a lot of in-
trusions in the middle of utterances, which in fact rarely
happen in the two corpora. As we have shown, initiative
conflicts tend to happen at the beginning of utterances.
Thus initiative conflicts occur mainly due to unintentional
collision, i.e. both conversants happen to start speaking
almost at the same time. The fact that the offset of most
initiative conflicts is within 300ms confirms this.2
5.2 Urgency Level and Initiative Conflicts
To further support the hypothesis that conversants avoid
initiative conflicts except for urgent reasons, we exam-
ined the MTD corpus for the correlation between the ur-
gency levels of the interruption game and initiative con-
flicts. For the urgency level of 10 seconds, conversants
started 33 interruption games, 8 of which were intro-
duced via initiative conflicts. For 25 seconds, conversants
started 36 interruption games, 5 introduced via initiative
conflicts. For 40 seconds, conversants started 33 interrup-
tion games, 3 introduced via initiative conflicts. Thus the
percentages of initiative conflicts for the three urgency
levels are 24% for 10 seconds, 14% for 25 seconds, and
9% for 40 seconds. The urgency level of 10 seconds
requires conversants to start the interruption game very
quickly in order to complete it in time. On the other hand,
the urgency level of 40 seconds allows conversants ample
time to wait for the best time to start the game (Heeman
et al, 2005). Thus we see the percentage of initiative
conflicts decreases as it becomes less urgent to the inter-
ruption game. These results suggest that conversants try
to avoid initiative conflicts if they can, unless there is an
urgent reason.
6 Resolving Initiative Conflicts
In this section, we present evidence that initiative con-
flicts, if they occur, are resolved very quickly using sim-
ple devices.
2This 300ms might be related to human reaction time.
0 500 1000 1500 2000 2500 3000 3500 40000
0.2
0.4
0.6
0.8
1
Length(ms)
Perc
enta
ge
A: Trains
Duration of initiative conflictsLength of forward utterances
0 500 1000 1500 2000 2500 3000 3500 40000
0.2
0.4
0.6
0.8
1
Length(ms)
Perc
enta
ge
B: MTD
Duration of initiative conflictsLength of forward utterances
Figure 6: CDF plot for durations of initiative conflicts
together with lengths of forward utterances
6.1 Duration of Initiative Conflicts
The duration of an initiative conflict, as defined in Sec-
tion 4, indicates how quickly the conflict is resolved. Fig-
ure 6 shows the cumulative distribution function of dura-
tions of initiative conflicts and the lengths of forward ut-
terances in the two corpora. The mean duration is 328ms
in the Trains corpus and 427ms in the MTD corpus. From
Figure 6 we see that the duration is much shorter than the
length of forward utterances, which have the mean length
of 2596ms in the Trains corpus and 1614ms in the MTD
corpus. The difference between duration of initiative con-
flicts and length of forward utterances is statistically sig-
nificant (p < 10?5, ttest). On average, the duration of
initiative conflicts is about 1/8 the length of forward ut-
terances in the Trains corpus and about 1/4 in the MTD
corpus. The short durations suggest that initiative con-
flicts are resolved very quickly.
According to Crystal and House (1990), the average
length of CVC syllable is about 250ms. Thus on aver-
age, the length of initiative conflicts is about one to two
syllables.3 In fact, 96% of conflicts in the Trains corpus
and 73% in the MTD corpus are resolved within 500ms.
These observations are consistent with one of Schelogff?s
(2000) claims about turn-taking conflicts, that they usu-
ally last less than two syllables to resolve.
6.2 Resolution of Initiative Conflicts
From our definition of initiative conflict, at least one of
the speakers has to back off. For expository ease, we re-
3It would be interesting to examine the length of initiative
conflicts based on syllable. However currently we do not have
syllable-level alignment for the two corpora. We leave this for
future research.
21
fer to the person who gets the turn to contribute as the
winner, and the other who fails as the yielder. There are
two cases in the Trains corpus and three cases in the MTD
corpus in which both speakers abandoned their incom-
plete utterances, paused for a while, and then one of them
resumed talking. These five cases are treated as ties: no
winners or yielders, and are excluded from our analysis
here.
Given how quickly initiative conflicts are resolved, we
examined whether the resolution process might be depen-
dent on factors presented before the conflict even begins,
namely who was speaker in the previous utterance, and
who was interrupted. If we predict that the conversant
who spoke prior to the conflict (speaker of u262 in Fig-
ure 2) loses, we get 55% accuracy in the Trains corpus
and 61% accuracy in the MTD corpus. If we predict
the conversant who spoke first in the overlap (speaker of
u263 in Figure 2) wins, we get 60% accuracy in the Trains
corpus and 53% accuracy in the MTD corpus. These low
percentages suggest that they are not robust predictors.
We next examined how conversants resolve the con-
flicts using devices such as volume, pitch, and others.
6.2.1 Volume
For a stretch of speech, volume is calculated as the mean
energy of the spoken words. For each initiative conflict,
we calculated each conversant?s volume during the over-
lap, and then normalized it with respect to the conver-
sant?s volume throughout the whole conversation.4 We
refer to this as relative volume. In the Trains corpus, the
average relative volume of the winner is 1.06; the average
relative volume of the yielder is 0.93. The difference is
statistically significant (P < 0.01, anova). In the MTD
corpus, the average relative volume of the winner is 1.12;
the average relative volume of the yielder is 0.98. The dif-
ference is also statistically significant (p < 10?6, anova).
These results show that the winner is the one speaking at
a higher relative volume.
To strengthen our argument, we also calculated volume
ratio as the relative volume of the winner divided by the
yielder. The average volume ratio in the Trains corpus is
1.16 and in the MTD corpus is 1.18. If a classifier always
chooses the speaker with higher relative volume to be the
winner, we achieve about 79% accuracy in both corpora,
which is a 29% absolute improvement over random pre-
diction. These results further confirm that the conversant
who speaks at a higher relative volume wins the initiative
conflicts.
Given the importance of volume in the resolution pro-
cess, we examined whether it has an impact on the du-
ration of initiative conflicts. Figure 7 plots the relation
4Normalization is necessary particularly as conversants
heard each other via headsets, and the microphones were not
calibrated to have exactly the same gains.
0 200 400 600 800 1000 1200 1400 16000.6
0.8
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
Duration of initiative conflicts (ms)
Volu
me 
ratio
 False prediction
Figure 7: Volume ratio and duration of conflicts
between volume ratio and duration of conflicts for all
the cases in the two corpora. For reference, the dot-
ted line divides the data points into two groups: under
the line are what volume ratio fails to predict the win-
ner, and above the line are success. If we look at the
points where volume ratio succeeds, we see that when
duration of initiative conflicts is long, volume ratio tends
to be small: in fact, the average volume ratio for initiative
conflicts shorter than 600ms is 1.27; for long than 600ms
is 1.13; and the difference is statistically significant (ttest,
p < 0.01).
To further understand how volume is used in the reso-
lution procedure, we examined how volume changes dur-
ing the overlap. For initiative conflicts whose duration is
longer than 600ms, we cut the overlapped speech evenly
in half, and calculated the relative volume for each half
individually. For the first half, the average relative vol-
ume of the winner is 1.03, and the yielder is 1.02. The
difference is not statistically significant (p = 0.93, paired
ttest). For the second half, the average relative volume of
the winner is 1.20, and the yielder is 1.02. The difference
is statistically significant (p < 0.001, paired ttest). The
fact that these long initiative conflicts are not resolved in
the first half is probably partially due to the close relative
volume.
We then calculated volume increment as subtracting the
relative volume of the first half from the second half. The
average volume increment of the winner is 0.17; the aver-
age volume increment of the yielder is 0. The difference
is statistically significant (p < 0.001, paired ttest). These
results show that the range of volume increment during
the overlap by the winner is larger than the yielder. The
behavior of increasing volume during overlap to win the
fight suggests that conversants use volume as a device to
resolve initiative conflicts.
22
6.2.2 Pitch
We used the tool WaveSurfer (Sjo?lander and Beskow,
2000) to extract the f0 from the audio files. We calcu-
lated relative pitch similarly as we did for volume.
In the Trains corpus, the average relative pitch of the
winner is 1.02; the average relative pitch of the yielder
is 0.96. The difference is not statistically significant
(P = 0.54, anova). In the MTD corpus, the average
relative pitch of the winner is 1.09; the average relative
pitch of the yielder is 0.98. The difference is statistically
significant (p < 0.001, anova). If we choose the speaker
with higher pitch to be the winner, we achieve about 65%
accuracy in the Trains corpus and 62% in the MTD cor-
pus. These results suggest that pitch alone is not robust
for predicting the winner of initiative conflicts, at least
not as predictive as volume, although we do see the ten-
dency of higher pitch by the winner.
We also examined pitch range in the window of 100ms
and 300ms respectively. We calculated the pitch range
of the overlapping speech and then normalized it with
respect to the conversant?s pitch range throughout the
whole conversation. We did not see a significant corre-
lation between pitch range and the winner of initiative
conflicts. Thus pitch does not seem to be a device for
resolving initiative conflicts.
6.2.3 Role of Conversants
Human-computer dialogues often have a user interact-
ing with a system, in which the two have very different
roles. Hence, we investigated whether the conversant?s
role has an effect in how initiative conflicts are resolved.
We focused on the Trains corpus due to both its rich dis-
course structure and the difference in the roles that the
system and the use have.
In the Trains corpus, if we predict that the initiator of
a discourse segment wins the conflicts, we get 65% ac-
curacy. In system-initiated segments, the system wins all
eight conflicts; however, in user-initiated segments, the
user only wins seven and system wins eight. The user
does not have an advantage during initiative conflicts in
its segments. Moreover, if the initiator had an advantage,
we would expect the system to have fought more strongly
in the user-initiated segments in order to win. However,
we do not see that the relative volume of the system win-
ning in user-initiated segments is statistically higher than
in system-initiated segments in this small sample size
(p = 0.9, ttest). The initiator does not seem to have a
privileged role in the resolution process.
From the above analysis, we see that the system wins
the conflicts 16 out of 23 times. Thus if we predict that
the system always wins the conflicts, we achieve 70%
accuracy. This is not surprising because the system has
all the domain information, and is more experienced in
solving goals. If the system and user want to speak at
the same time, both would know that the system proba-
bly has a more significant contribution. That the system
wins most of the initiative conflicts agrees with Guinn
(1998) that capability plays an important role in deter-
mining who to show initiative next.
7 Discussion
In this paper, we present our empirical study of human
behavior in initiative conflicts. Our first finding is that
conversants try to avoid initiative conflicts. The conse-
quence of initiative conflicts is that at least one of the
conversants would have to back off, which makes their
effort of contributing in vain. Moreover, the effort of
resolving initiative conflicts is overhead to the dialogue.
According to the theory of least collaborative effort by
Clark and Wilkes-Gibbs (1986), it only makes sense for
conversants to interrupt when the loss of not interrupting
is higher than the cost of an initiative conflict. Thus the
theory of least collaborative effort is consistent with our
conclusion that most initiative conflicts are unintentional
collisions, except where conversants interrupt in the mid-
dle of an utterance for urgency reasons.
The second finding of our research is that initiative
conflicts, when they occur, are efficiently resolved. We
found that volume plays an important role: the louder
speaker wins. We also show how conversants change
their volume to resolve initiative conflicts. Conversants
probably identify their eagerness of speaking, confidence
in what they want to say, and capability of achieving the
current goal by means of volume, which resolves the ini-
tiative conflicts very quickly.
Domain settings obviously have an impact on conver-
sants? initiative behavior. There are more frequent initia-
tive conflicts in the MTD corpus than in the Trains cor-
pus. Moreover, the roles of the conversants also affect
their initiative behavior as we found that the system wins
more initiative conflicts in the Trains corpus. In a teacher-
student conversation, one would expect to see that the
teacher interrupts the student more often than vice versa,
but also that the teacher wins more initiative conflicts.
Capability, culture, and social relationship probably are
some underlying elements that influence when and under
what conditions conversants would seek initiative, while
volume is a device for resolving initiative conflicts.
8 Future Work
In this paper we focused on initiative conflicts in dialogue
where two conversants cannot see each other. In face-to-
face conversation, there might be other cues, such as eye-
contact, head-nodding, and hand gesture, that conversants
use in initiative conflicts. Moreover, in a multi-party con-
versation, a conversant might talk to different people on
different topics, and get interrupted from time to time,
23
which leads to an initiative conflict involving multiple
speakers. In our future work, we plan to examine ini-
tiative conflicts in face-to-face multi-party conversation,
such as the ICSI corpus (Shriberg et al, 2004).
Inspired by the findings on human behavior of initia-
tive conflicts, we speculate that conversants might also
have a mechanism to even minimize unintentional ini-
tiative conflicts, which probably includes devices such
as volume, pause, and other prosodic features. The
speaker uses these devices, as opposed to explicitly in-
forming each other of their knowledge to evaluate capa-
bility (Guinn, 1998), to implicitly signal his or her ea-
gerness, confidence and capability. The hearer then com-
pares his or her own eagerness with the speaker?s, and
decides whether to just make an acknowledgement (al-
lowing the speaker to continue the lead) or to take over
the initiative when taking the turn to speak. In our future
work, we plan to build an initiative model to capture this
negotiation process.
References
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User
Adapted Interaction, 8:215?253.
Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognitive Science,
22:1?39.
Robin Cohen, C. Allaby, C. Cumbaa, M. Fitzgerald,
K. Ho, B. Hui, C. Latulipe, F. Lu, N. Moussa, D. Poo-
ley, A. Qian, and S. Siddiqi. 1998. What is initiative?
User Modeling and User Adapted Interaction, 8:171?
214.
Mark G. Core and James F. Allen. 1997. Coding dia-
logues with the DAMSL annotation scheme. In Work-
ing Notes: AAAI Fall Symposium on Communicative
Action in Humans and Machines, pages 28?35, Cam-
bridge.
Thomas H. Crystal and Arthur S. House. 1990. Articula-
tion rate and the duration of syllables and stress groups
in connected speech. Journal of Acoustical Society of
America, 88:101?112.
Starkey Duncan. 1974. On the structure of speaker-
auditor interaction during speaking turns. Language
in Society, 2:161?180.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Curry I. Guinn. 1998. An analysis of initiative selection
in collaborative task-oriented discourse. User Model-
ing and User Adapted Interaction, 8:255?314.
Susan Haller and Timothy Fossum. 1999. Using pro-
tocols to model mixed initiative interaction. In Pro-
ceedings of AAAI Workshop on Mixed Initiative Intel-
ligence.
Peter A. Heeman and James F. Allen. 1995. The Trains
spoken dialogue corpus. CD-ROM, Linguistics Data
Consortium.
Peter A. Heeman, Fan Yang, Andrew L. Kun, and
Alexander Shyrokov. 2005. Conventions in human-
human multithreaded dialogues: A preliminary study.
In Proceedings of Intelligent User Interface (short pa-
per session), pages 293?295, San Diego CA.
Eric Horvitz. 1999. Principles of mixed-initiative user
interfaces. In Proceedings of CHI, pages 159?166,
Pittsburgh PA.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systematics for the organization of
turn-taking for conversation. Language, 50(4):696?
735.
Emanuel A. Schegloff. 2000. Overlapping talk and
the organization of turn-taking for conversation. Lan-
guage in Society, 29:1?63.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.
2004. The ICSI meeting recorder dialog act corpus. In
Proceedings of the 5th SIGdial Workshop on Discourse
and Dialogue.
Ka?re Sjo?lander and Jonas Beskow. 2000. WaveSurfer:
An open source speech tool. In Proceedings of ICSLP,
pages 4:464?467, Beijing China.
Ronnie W. Smith. 1993. Effective spoken natural lan-
guage dialogue requires variable initiative behavior:
an empirical study. In AAAI93 Fall Symposium On
Human-Computer Collaboration.
Susan E. Strayer, Peter A. Heeman, and Fan Yang. 2003.
Reconciling control and discourse structure. In J. Van
Kuppevelt and R. W. Smith, editors, Current and New
Directions in Discourse and Dialogue, chapter 14,
pages 305?323. Kluwer Academic Publishers.
Siew Leng Toh, Fan Yang, and Peter A. Heeman. 2006.
An annotation scheme for agreement analysis. In Pro-
ceedings of INTERSPEECH, Pittsburgh PA.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse seg-
mentation. In Proceedings of 28th ACL.
Steve Whittaker and Phil Stenton. 1988. Cues and con-
trol in expert-client dialogue. In Proceedings of 28th
ACL, pages 123?130.
Fan Yang, Peter A. Heeman, Kristy Hollingshead, and
Susan E. Strayer. 2007. Dialogueview: Annotating
dialogues in multiple views with abstraction. Natural
Language Engineering. To appear.
24
Proceedings of NAACL HLT 2007, pages 268?275,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Combining Reinforcement Learning with Information-State Update Rules?
Peter A. Heeman
Center for Spoken Language Understanding
Oregon Health & Science University
Beaverton OR, 97006, USA
heeman@cslu.ogi.edu
Abstract
Reinforcement learning gives a way to
learn under what circumstances to per-
form which actions. However, this ap-
proach lacks a formal framework for spec-
ifying hand-crafted restrictions, for speci-
fying the effects of the system actions, or
for specifying the user simulation. The in-
formation state approach, in contrast, al-
lows system and user behavior to be spec-
ified as update rules, with preconditions
and effects. This approach can be used
to specify complex dialogue behavior in
a systematic way. We propose combining
these two approaches, thus allowing a for-
mal specification of the dialogue behavior,
and allowing hand-crafted preconditions,
with remaining ones determined via rein-
forcement learning so as to minimize dia-
logue cost.
1 Introduction
Two different approaches have become popular for
building spoken dialogue systems. The first is the
symbolic reasoning approach. Speech actions are
defined in a formal logic, in terms of the situations
in which they can be applied, and what effect they
will have on the speaker?s and the listener?s mental
state (Cohen and Perrault, 1979; Allen and Perrault,
1980). One of these approaches is the information
state (IS) approach (Larsson and Traum, 2000). The
knowledge of the agent is formalized as the state.
The IS state is updated by way of update rules,
which have preconditions and effects. The precondi-
tions specify what must be true of the state in order
?The author wishes to thank Fan Yang and Michael En-
glish for helpful conversations. Funding from the National
Science Foundation under grant IIS-0326496 is gratefully ac-
knowledged.
to apply the rule. The effects specify how the state
changes as a result of applying the rule. At a mini-
mum, two sets of update rules are used: one set, un-
derstanding rules, specify the effect of an utterance
on the agent?s state and a second set, action rules,
specify which speech action can be performed next.
For example, a precondition for asking a question is
that the agent does not know the answer to the ques-
tion. An effect of an answer to a question is that the
hearer now knows the answer. One problem with
this approach is that although necessary precondi-
tions for speech actions are easy to code, there are
typically many speech actions that can be applied at
any point in a dialogue. Determining which one is
the optimal one is a daunting task for the dialogue
designer.
The second approach for building spoken dia-
logue systems is to use reinforcement learning (RL)
to automatically determine what action to perform
in each different dialogue state so as to minimize
some cost function (e.g. Walker, 2000; Levin et al,
2000). The problem with this approach, however, is
that it lacks the framework of IS to specify the man-
ner in which the internal state is updated. Further-
more, sometimes no preconditions are even speci-
fied for the actions, even though they are obvious
to the dialogue designer. Thus RL needs to search
over a much larger search space, even over dialogue
strategies that do not make any sense. This not only
substantially slows down the learning procedure, but
also increases the chance of being caught in a locally
optimal solution, rather than the global optimal. Fur-
thermore, this large search space will limit the com-
plexity of the domains to which RL can be applied.
In this paper, we propose combining IS and RL.
IS update rules are formulated for both the system
and the simulated user, thus allowing RL to use a
rich formalism for specifying complex dialogue pro-
cessing. The preconditions on the action rules of
the system, however, only need to specify the neces-
268
sary preconditions that are obvious to the dialogue
designer. Thus, preconditions on the system?s ac-
tions might not uniquely identify a single action that
should be performed in a given state. Instead, RL
is used to determine which of the applicable actions
minimizes a dialogue cost function.
In the rest of the paper, we first present an exam-
ple domain. Section 3 gives an overview of apply-
ing RL to dialogue strategy and Section 4 gives an
overview of IS. Section 5 demonstrates that IS can
be used for simulating a dialogue between the sys-
tem and a user. Section 6 demonstrates how IS can
be used with RL. Section 7 gives results on using
hand-crafted preconditions specified in the IS update
rules to simplify learning dialogue strategies with
RL. Section 8 gives concluding comments.
2 Flight Information Application
To illustrate our proposed approach, we use the
flight information domain, similar to that of Levin
et al (2000). The goal of the system is to display
a short list of flights that meets the user?s require-
ments. The user is assumed to have a flight in mind,
in terms of its destination, origin, airline, departure
time, and number of stops. The user might be flexi-
ble on some of the parameters. It is assumed that the
user will not change his or her mind depending on
what flights are found.
In this paper, we are focusing on dialogue man-
agement issues, and so we use a semantic represen-
tation for both the input and output of the system.
The system can ask the user the value of parame-
ter p with ?askconstraint p?, and the user will an-
swer with ?constrain p v?, where v is the user?s pre-
system askconstraint from
user constrain from miami
system askconstraint to
user constrain to sacramento
system askconstraint departure
user constrain departure 6pm
system dbquery miami sacremento - 6pm
system askconstraint airline
user constrain airline united
system dbquery miami sacremento united ...
system askrelax departure
user relax departure yes
system dbquery miami sacremento united ...
system output {918 11671 13288}
system finish
Figure 1: Sample dialogue
ferred value of the parameter.1 The system can ask
whether the user is flexible on the values for parame-
ter p with ?askrelax p?, and the user will answer with
?relax p a?, where a is either ?yes? or ?no?. The sys-
tem can do a database query, ?dbquery?, to determine
whether any flights match the current parameters. If
no flights exactly match, ?dbquery? will check if any
flights match according to the relaxed restrictions,
by ignoring parameters that the system knows the
user is flexible on. The system can display the found
flights with ?output?. It can also quit at any time. A
sample dialogue is given in Fig. 1.
3 Reinforcement Learning (RL)
Given a set of system actions, a set of states, and a
cost function that measures the quality of a dialogue,
RL searches for an optimal dialogue policy (Sutton
and Barto, 1998; Levin et al, 2000).
Cost Function: The cost function assesses how
good a dialogue is: the lower the cost, the better the
dialogue. RL uses the cost function to provide feed-
back in its search for an optimal strategy. The cost
function is specified by the dialogue designer, and
can take into account any number of factors, typi-
cally including dialogue length and solution quality.
System Actions: RL takes as input a finite number
of actions, and for each state, learns which action is
best to perform. The dialogue designer decides what
the actions will be, both in terms of how much to
combine into a single action, and how specific each
action should be.
State Variables: RL learns what system action to
perform in each state. The RL states are defined in
terms of a set of state variables: different values for
the variables define the different states that can exist.
The state variables need to include all information
that the dialogue designer thinks will be relevant in
determining what action to perform next. Any infor-
mation that is thought to be irrelevant is excluded in
order to keep the search space small.
Transitions: RL treats a dialogue as a succession
of states, with actions causing a transition from one
state to the next. The transition thus encompasses
the effect of the system making the speech act,
1In contrast to Levin, over-answering by the user is not al-
lowed. The system also does not have a general greeting, to
which the user can answer with any of the flight parameters.
269
the user?s response to the system?s speech act, and
the system?s understanding of the user?s response.
Hence, the transition incorporates a user simulation.
In applying RL to dialogue policies, the transition
from a state-action pair to the next state is usually
modeled as a probability distribution, and is not fur-
ther decomposed (e.g. Levin et al, 2000).
Policy Exploration: RL searches the space of po-
lices by determining Q for each state-action pair s-
a, which is the minimal cost to get to the final state
from state s starting with action a. From the Q val-
ues, a policy can be determined: for each state s,
choose the action a that has the maximum Q value.
Q is determined in an iterative fashion. The cur-
rent estimates for Q for each state-action are used to
determine the current dialogue policy. The policy,
in conjunction with the transition probabilities, are
used to produce a dialogue run, which is a sequence
of state-action pairs, each pair having an associated
cost to get to the next state-action pair. Thus, for a
dialogue run, the cost from each state-action pair to
the final state can be determined. These costs are
used to revise the Q estimates.
To produce a dialogue run, the ?-greedy method
is often used. In this approach, with probability ?,
an action other than the action specified by the cur-
rent policy is chosen. This helps ensure that new
estimates are obtained for all state-action pairs, not
just ones in the current policy. Typically, a number
of dialogue runs, an epoch, are made before the Q
values and dialogue policy are updated. With each
successive epoch, a better dialogue policy is used,
and thus the Q estimates will approach their true val-
ues, which in turn, ensures that the dialogue policy
is approaching the optimal one.
3.1 Flight Information Task in RL
To illustrate how RL learns a dialogue policy, we use
the flight information task from Section 2.
Actions: The system actions were given in Section
2. The queries for the destination, origin, airline, de-
parture time, number of stops are each viewed as dif-
ferent actions so that RL can reason about the indi-
vidual parameters. There are also 5 separate queries
for checking whether each parameter can be relaxed.
There is also a database query to determine which
flights match the current parameters. This is in-
cluded as an RL action, even though it is not to the
user, so that RL can decide when it should be per-
formed. There is also an output and a finish action.
State Variables: We use the following variables
for the RL state. The variable ?fromP? indicates
whether the origin has been given by the user and
the variable ?fromR? indicates whether the user has
been asked if the origin can be relaxed, and if so,
what the answer is. Similar variables are used for the
other parameters. The variable ?dbqueried? indicates
whether the database has been queried. The variable
?current? indicates whether no new parameters have
been given or relaxed since the last database query.
The variable ?NData? indicates the number of items
that were last returned from the database quantized
into 5 groups: none, 1-5, 6-12, 13-30, more than 30).
The variable ?outputP? indicates whether any flights
have been given to the user. Note that the actual val-
ues of the parameters are not included in the state.
This helps limit the size of the search space, but pre-
cludes the values of the parameters from being used
in deciding what action to perform next.
Cost Function: Our cost function is the sum of
four components. Each speech action has a cost of
1. A database query has a cost of 2 plus 0.01 for each
flight found. Displaying flights to the user costs 0 for
5 or fewer flights, 8 for 12 or fewer flights, 16 for 30
or fewer flights, and 25 for 30 or more flights. The
last cost is the solution cost. This cost takes into ac-
count whether the user?s preferred flight is even in
the database, and if so, whether it was shown to the
user. The solution cost is zero if appropriate infor-
mation is given to the user, and 90 points otherwise.
3.2 Related Work in RL
In the work of Levin, Pieraccini, and Eckert (2000),
RL was used to choose between all actions. Actions
that resulted in infelicitous speech act sequences
were allowed, such as asking the value of a parame-
ter that is already known, asking if a parameter can
be relaxed when the value of the parameter is not
even known, or displaying values when a database
query has not yet been performed.
In other work, RL has been used to choose among
a subset of the actions in certain states (Walker,
2000; Singh et al, 2002; Scheffler and Young, 2002;
English and Heeman, 2005). However, no for-
mal framework is given to specify which actions to
choose from.
270
Furthermore, none of the approaches used a for-
mal specification for updating the RL variables after
a speech action, nor for expressing the user simula-
tion. As RL is applied to more complex tasks, with
more complex speech actions, this will lead to diffi-
culty in encoding the correct behavior.
Georgila, Henderson, and Lemon (2005) advo-
cated the use of IS to specify the dialogue context
for learning user simulations needed in RL. How-
ever, they did not combine hand-crafted with learned
preconditions, and it is unclear whether they used IS
to update the dialogue context,
4 Information State (IS)
IS has been concerned with capturing how to up-
date the state of a dialogue system in order to build
advanced dialogue systems (Larsson and Traum,
2000). For example, it has been used to build sys-
tems that allow for both system and user initiative,
over answering, confirmations, and grounding (e.g.
(Bohlin et al, 1999; Matheson et al, 2000)). It uses
a set of state variables, whose values are manipu-
lated by update rules, run by a control strategy.
State Variables: The state variables specify the
knowledge of the system at any point in the dia-
logue. This is similar to the RL variables, except that
they must contain everything that is needed to com-
pletely specify the action that the system should per-
form, rather than just enough information to choose
between competing actions. A number of stan-
dard variables are typically used to interface to other
modules in the system. The variable ?lastMove? has
the semantic representation of what was last said, ei-
ther by the user or the system and ?lastSpeaker? in-
dicates who spoke the last utterance. Both are read-
only. The variable ?nextMove? is set by the action
rules to the semantic representation of the next move
and ?keepTurn? is set to indicate whether the current
speaker will keep the turn to make another utterance.
Update Rules: Update rules have preconditions
and effects. The preconditions specify what must
be true of the state in order to apply the rule. The ef-
fects specify how the state should be updated. In this
paper, we will use two types of rules. Understand-
ing rules will be used to update the state to take into
account what was just said, by both the user and the
system. Action rules determine what the system will
say next and whether it will keep the turn.
Control Strategy: The control strategy specifies
how the update rules should be processed. In our ex-
ample, the control strategy specifies that the under-
standing rules are processed first, and then the action
rules if the system has the turn. The control strategy
also specifies which rules should be applied: (a) just
the first applicable rule, (b) all applicable rules, or
(c) randomly choose one of the applicable rules.
Although there is a toolkit available for building
IS systems (Larsson and Traum, 2000), we built a
simple version in Tcl. Update rules are written using
Tcl code, which allows for simple interpretation of
the rules. The state is saved as Tcl variables, and
thus allows strings, numbers, booleans, and lists.
4.1 Flight Information Example in IS
We now express the flight information system with
the IS approach. This allows for a precise formaliza-
tion of the actions, both the conditions under which
they should be performed and their effects.
The IS state variables are similar to the RL ones
given in Section 3. Instead of the variable ?fromP?,
it includes the variable ?from?, which has the actual
value of the parameter if known, and ?? otherwise.
The same is true for the destination, airline, depar-
ture time, and number of stops. Instead of the RL
variable ?NData? and ?outputP, ?results? holds the
actual database and ?output? holds the actual flights
displayed to the user.
Figure 2 displays the system?s understanding
rules, which are used to update the state variables
after an utterance is said. Although it is common
practice in IS to use understanding rules even for
one?s own utterances, the example application is
simple enough to do without this. Understanding
rules are thus only used for understanding the user?s
utterances: giving a parameter value or specifying
whether a parameter can be relaxed. As can be seen,
any time the user specifies a new parameter or re-
laxes a parameter, ?current? is set to false.
Figure 3 gives the action rules for the system.
Rules for querying the destination, departure, and
number of stops are not shown; neither are the rules
for querying whether the destination, origin, airline,
and number of stops can be relaxed. The effects of
the rules show how the state is updated if the rule
is applied. For most of the rules, this is simply to
271
set ?nextMove? and ?keepTurn? appropriately. The
?dbquery? action is more complicated: it runs the
database query and updates ?results?. It then updates
the variables ?queriedDB?, and ?current? appropri-
ately. Note that the actions ?dbquery? and ?output?
specify that the system wants to keep the turn.
The preconditions of the update rules specify the
exact conditions under which the rule can be ap-
plied. The preconditions on the understanding rules
are straightforward, and simply check the user?s re-
sponse. The preconditions on the action rules are
more complex. We divide the preconditions into the
4 groups given below, both to simplify the discus-
sion of the preconditions, and because we use these
groupings in Section 7.
Speech Acts: Some of the preconditions cap-
ture the conditions under which the action can be
performed felicitously (Cohen and Perrault, 1979;
Allen and Perrault, 1980). Only ask the value of
a parameter if you do not know its value. Only ask
if a parameter can be relaxed if you know the value
of the parameter. Only output the data if it is still
current and more than one flight was found. These
preconditions are labeled as ?sa? in Fig. 3.
Application Restrictions: These preconditions
enforce the specification of the application. For
our application, the system should only output data
once: once data is output, the system should end
the conversation. These preconditions are labeled
as ?app? in Fig. 3.
Partial Strategy: These preconditions add addi-
tion constraints that seem reasonable: ask the ?to?,
?from?, and ?departure? parameters first; never relax
the ?to? and ?from?; and only ask whether ?airline?
and ?stops? can be relaxed if the database has been
Understand Answer to Constrain Question
Pre: [lindex $lastMove 0] == ?constrain?
Eff: set [lindex LastMove 1] [lindex LastMove 2]
set current 0
Understand Yes Answer to Relax
Pre: [lindex lastMove 0] == ?relax?
[lindex lastMove 2] == ?yes?
Eff: set [lindex lastMove 1]R yes
set current 0
Understand No Answer to Relax
Pre: [lindex lastMove 0] == ?relax?
[lindex lastMove 2] == ?no?
Eff: set [lindex lastMove 1]R no
Figure 2: Understanding Rules for System
queried. Furthermore, the system may only output
data if (a) the number of flights is between 1 and
5, or (b) the number of flights is greater than 5 and
?airline? and ?stops? have both been asked. These
preconditions are labeled as ?ps? in Fig. 3.
Baseline: The last group of preconditions (to-
gether with the previous preconditions) uniquely
identify a single action to perform in each state, and
Ask Origin of Flight
Pre: $from == ?? sa
$output == ?? app
Eff: set nextMove ?askconstraint from?
set keepTurn false
Ask Airline of Flight
Pre: $airline == ?? sa
$output == ?? app
$departure != ?? ps
$queriedDB == true base
$current == true base
[llength $results] > 5 base
Eff: set nextMove ?askconstraint to?
set keepTurn false
Ask Whether Departure Time can be Relaxed
Pre: $departure != ?? sa
$departureR == ?? sa
$output != ?? app
$queriedDB == true base
$current == true base
$results == {} base
Eff: set nextMove ?askrelax from?
set keepTurn false
Query the Database
Pre: $current == false sa
$output == ?? app
$departure != ?? ps
Eff: set results [DBQuery $from $to $airline ...]
set queriedDB true
set current true
set nextMove dbquery
set keepTurn true
Output Results to User
Pre: $current == true sa
$results != {} sa
$output == ?? app
[llength $results] < 6 || ([llength $results] > 5 ps
&& $airline != ?? && $stops != ??)
Eff: set nextMove ?output $results?
set output $results
Finish
Pre: $output != ?? app
Eff: set nextMove finish
Quit
Pre: $output == ?? app
$current == true app
$results == {} app
$airline != ?? || $airlineR != ?? base
$stops != ?? || $stopsR != ?? base
Eff: set nextMove finish
Figure 3: Action Rules for System
272
thus completely specifies a strategy. These are la-
beled as ?base? in Fig. 3. The strategy that we give
is based on the optimal strategy found by Levin et
al. (2000). After the system asks the values for the
?from?, ?to?, and ?departure? variables, it then per-
forms a database query. If there are between 1 and 5
flights found, they are displayed to the user. If there
are more than 5, the system asks the value of ?air-
line? if unknown, otherwise, ?number of stops?. If
there are 0 items, it tries to relax one of ?departure?,
?airline?, and ?stops?, in that order (but not ?from?
or ?to?). Any time new information is gained, such
as a parameter value or a parameter is relaxed, the
database is requeried, and the process repeats.
5 Implementing the Simulated User
Normally, with IS, the system is run against an ac-
tual user, and so no state variables nor update rules
are coded for the user. To allow the combination of
IS with RL, we need to produce dialogues between
the system and a simulated user. As the IS approach
is very general, we will use it for implementing the
simulated user as well. In this way, we can code the
user simulation with a well-defined formalism, thus
allowing complex user behaviors. Hence, two sepa-
rate IS instantiations will be used: one for the system
and one for the user. The system?s rules will update
the system?s state variables, and the user?s rules will
update the user?s state variables; but the two instan-
tiations will be in lock-step with each other.
We built a simulator that runs the system?s rules
against the user?s. The simulator (a) runs the under-
standing rules for the system and the user on the last
utterance; then (b) checks who has the turn, and runs
that agent?s action rules; and then (c) updates ?lastS-
peaker? and ?lastMove?. It repeats these three steps
until the ?finish? speech act is seen.
5.1 Flight Information Task
The user has the variables ?from?, ?to?, ?departure?,
?airline?, and ?stops?, which hold the user?s ideal
flight, and are set before the dialogue begins. The
variables ?fromR?, ?toR?, ?departureR?, ?airlineR?,
and ?stopsR? are also used, and are also set before
the dialogue begins. No other variables are used.
For the flight application, separate update rules
are used for the user. There are two types of queries
Answer Constrain Question
Pre: [lindex $lastMove 0] == ?askconstraint?
Eff: set nextMove ?constraint [lindex $lastmove 1]
[set [lindex $lastmove 1]]?
set haveTurn 0
Answer Relax Question
Pre: [lindex $lastMove 0] == ?askrelax?
Eff: set nextMove ?relax [lindex $lastmove 1]
[set [lindex $lastMove 1]R]?
set haveTurn 0
Figure 4: Action Rules for User
to which the user needs to react, namely, ?askcon-
traint? and ?askrelax?. This domain is simple enough
that we do not need separate understanding and ac-
tion rules, and so we encompass all reasoning in the
action rules, shown in Fig. 4. The first rule is for
answering system queries about the value of a pa-
rameter. The second is for answering queries about
whether a parameter can be relaxed.
6 Combining IS and RL
RL gives a way to learn the best action to perform in
any given state. However, RL lacks a formal frame-
work for specifying (a) the effects of the system?s
actions, (b) hand-crafted preconditions of the sys-
tem?s actions, and (c) the simulated user. Hence, we
combine RL and IS to rectify these deficits. IS up-
date rules are formulated for both the system and the
simulated user, as done in Section 5.1. The precon-
ditions on the system?s action rules, however, only
need to specify a subset of the preconditions, ones
that are obvious the dialogue designer. The rest of
the preconditions will be determined by RL, so as to
minimize a cost function. To combine these two ap-
proaches, we need to (a) resolve how the IS and RL
state transitions relate to each other; (b) resolve how
the IS state relates to the RL state; and (c) specify
how utterance costs can be specified in the general
framework of IS.
Transitions: When using IS for both the system
and user simulation, the state transitions for each
are happening in lock-step (Section 5.1). In com-
bining RL and IS, the RL transitions happen at a
courser granularity than the IS transitions, and group
together everything that happens between two suc-
cessive system actions. Thus, the RL states are those
IS states just before a system action.
273
State Variables: For the system, we add all of the
RL variables to the IS variables, and remove any du-
plicates. The RL variables are thus a subset of the IS
variables. Some of the variables might be simplifica-
tions of other variables. For our flight example, we
have the exact values of the origin, destination, air-
line, departure time, and number of stops, as well as
a simplification of each that only indicates whether
the parameter has been given or not.
Rather than have the system?s IS rules update
all of the variables, we allow variables to be de-
clared as either primitive or derived.2 Only primitive
variables are updated by the effects of the update
rules. The derived variables are re-computed from
the primitive ones each time an update rule is ap-
plied. For our flight example, the variables ?fromP?,
?toP?, ?airlineP?, ?departureP?, ?stopsP?, ?outputP?,
and ?NData? are derived variables, and these are up-
dated via a procedure.
As the RL variables are a subset of the IS vari-
ables, the RL states are coarser than the IS states.
We do not allow hand-crafted preconditions in the
system?s action rules to distinguish at the finer gran-
ularity. If they did, we would have an action that is
only applicable in part of an RL state, and not the
rest of it. However, RL needs to find a single action
that will work for the entire RL state, and so that
action should not be considered. To prevent such
problems, the hand-crafted preconditions can only
test the values of the RL variables, and not the full
set of IS variables. Hence, we rewrote the precon-
ditions in the action rules of Fig. 3 to use the RL
variables. This restriction does not apply to the sys-
tem?s understanding rules, nor to the user rules, as
those are not subject to RL.
Cost Function: RL needs to track the costs in-
curred in the dialogue. Rather than leaving this to
be specified in an ad-hoc way, we include state vari-
ables to track the components of the cost. This way,
each update rule can set them to reflect the cost of
the rule. Just as with other interface variables (e.g.
?keepTurn?), these are write-only. For our flight ex-
ample, the output action computes the cost of dis-
playing flights to the user, and the database query ac-
tion computes the cost of doing the database lookup.
2This same distinction is sometimes used in the planning
literature (Poole et al, 1998).
7 Evaluation
To show the usefulness of starting RL with some of
the preconditions hand-crafted, we applied RL using
four different sets of action schemes. The first set,
?none?, includes no preconditions on any of the sys-
tem?s actions. The second through fourth sets cor-
respond to the precondition distinctions in Fig. 3, of
?speech act?, ?application? and ?partial strategy?.
For each set of action schemas, we trained 30 di-
alogue policies using an epoch size of 100. Each di-
alogue was run with the ?-greedy method, with ? set
at 0.15. After certain epochs, we ran the learned pol-
icy 2500 times strictly according to the policy. We
found that policies did not always converge. Hence,
we trained the policies for each set of preconditions
for enough epochs so that the average cost no longer
improved. More work is needed to investigate this
issue.
The results of the simulations are given in Table
1. The first row reports the average dialogue cost
that the 30 learned policies achieved. We see that all
four conditions achieved an average cost less than
the baseline strategy of Fig. 3, which was 17.17. The
best result was achieved by the ?application? precon-
ditions. This is probably because ?partial? included
some constraints that were not optimal, while the
search strategy was not adequate to deal with the
large search space in ?speech acts? and ?none?.
The more important result is in the second row
of Table 1. The more constrained precondition sets
result in significantly fewer states being explored,
ranging from 275 for the ?partial? preconditions, up
to 18,206 for no preconditions. In terms of number
of potential policies explored (computed as the prod-
uct of the number of actions explored in each state),
this ranges from 1058 to 107931. As can be seen, by
placing restrictions on the system actions, the space
that needs to be explored is substantially reduced.
The restriction in the size of the search space af-
fects how quickly RL takes to find a good solution.
Figure 5 shows how the average cost for each set of
None SA App. Partial
Dialogue Cost 16.65 16.95 15.24 15.68
States Explored 18206 5261 4080 275
Policies (log10) 7931 2008 1380 58.7
Table 1: Comparison of Preconditions
274
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 1  10  100  1000  10000
None
Speech Acts
Application
Partial
Figure 5: Average dialogue cost versus epochs
preconditions improved with the number of epochs.
As can be seen, by including more preconditions
in the action definitions, RL is able to find a good
solution more quickly. For the ?partial? precondi-
tions, after 10 epochs, RL achieves a cost less than
17.0. For the ?application? setting, this does not hap-
pen until 40 epochs. For ?speech act?, it takes 1000
epochs, and for ?none?, it takes 3700 epochs. So,
adding hand-crafted preconditions allows RL to con-
verge more quickly.
8 Conclusion
In this paper, we demonstrated how RL and IS can
be combined. From the RL standpoint, this allows
the rich formalism of IS update rules to be used for
formalizing the effects of the system?s speech ac-
tions, and for formalizing the user simulation, thus
enabling RL to be applied to domains that require
complex dialogue processing. Second, use of IS al-
lows obvious preconditions to be easily formulated,
thus allowing RL to search a much smaller space of
policies, which enables it to converge more quickly
to the optimal policy. This should also enable RL to
be applied to complex domains with large numbers
of states and actions.
From the standpoint of IS, use of RL means that
not all preconditions need be hand-crafted. Pre-
conditions that capture how one action might be
more beneficial than another can be difficult to deter-
mine for dialogue designers. For example, knowing
whether to first ask the number of stops or the air-
line, depends on the characteristics of the flights in
the database, and on users? relative flexibility with
these two parameters. The same problems occur
for knowing under which situations to requery the
database or ask for another parameter. RL solves
this issue as it can explore the space of different poli-
cies to arrive at one that minimizes a dialogue cost
function.
References
J. Allen and C. Perrault. 1980. Analyzing intention in
utterances. Artificial Intelligence, 15:143?178.
P. Bohlin, R. Cooper, E. Engdahl, and S. Larsson. 1999.
Information states and dialogue move engines. In Pro-
ceedings of the IJCAI Workshop: Knowledge and Rea-
soning in Practical Dialogue Systems, pg. 25?31.
P. Cohen and C. Perrault. 1979. Elements of a plan-based
theory of speech acts. Cognitive Science, 3(3):177?
212.
M. English and P. Heeman. 2005. Learning mixed ini-
tiative dialog strategies by using reinforcement learn-
ing on both conversants. In HLT and EMNLP, pages
1011?1018, Vancouver Canada, October.
K. Georgila, J. Henderson, and O. Lemon. 2005. Learn-
ing user simulations for information state update dia-
logue systems. In Eurospeech, Lisbon Portugal.
S. Larsson and D. Traum. 2000. Information state and di-
alogue management in the TRINDI dialogue move en-
gine toolkit. Natural Language Engineering, 6:323?
340.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1):11?23.
C. Matheson, M. Poesio, and D. Traum. 2000. Mod-
elling grounding and discourse obligations using up-
date rules. In NAACL, Seattle, May.
D. Poole, A. Mackworth, and R. Goebel. 1998. Com-
putational Intelligence: a logical approach. Oxford
University Press.
K. Scheffler and S. J. Young. 2002. Automatic learning
of dialogue strategy using dialogue simulation and re-
inforcement learning. In HLT, pg. 12?18, San Diego.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue managment with reinforcement
learning: Experiments with the NJfun system. Jour-
nal of Artificial Intelligence Research, 16:105?133.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press, Cambridge MA.
M. Walker. 2000. An application of reinforcement learn-
ing to dialog strategy selection in a spoken dialogue
system for email. Journal of Artificial Intelligence Re-
search, 12:387?416.
275
Reconciling Initiative and Discourse Structure
Susan E. Strayer and Peter A. Heeman
Department of Computer Science and Engineering
OGI School of Science and Engineering
Oregon Health & Science University
20000 NW Walker Rd Beaverton, OR 97006, USA
susan strayer@yahoo.com heeman@cse.ogi.edu
Abstract
In this paper we consider how ini-
tiative is managed in dialogue. We
propose that initiative is subordi-
nate to the intentional hierarchy of
discourse structure. In dialogues
from the TRAINS corpus we nd
that inside a segment initiated by
one speaker, the other speaker only
makes two types of contributions: a
special kind of acknowledgment we
call forward acknowledgments, and
short contributions that add content
to the segment. The proposal has
important implications for dialogue
management: a system only needs
to model intentional structure, from
which initiative follows.
1 Introduction
The dialogue manager of a spoken language
system is responsible for determining what
contributions a system can make and when it
can make them. The question is, what should
the dialogue manager pay attention to in or-
der to accomplish this? Two areas of research
have shaped our understanding of what hap-
pens in dialogue: research in dialogue struc-
ture and in mixed initiative.
Grosz and Sidner (1986) proposed a theory
of discourse structure to account for why an
utterance was said and what was meant by it.
Their theory has three components: linguis-
tic structure, intentional structure and atten-
tional state. Intentions are key to accounting
for discourse structure, dening discourse co-
herence, and \providing a coherent conceptu-
alization of the term `discourse' itself." The
intentional structure describes the purpose of
the discourse as a whole, and the relation-
ship of the purpose of each discourse seg-
ment to the main discourse purpose or other
discourse segment purposes. All utterances
within a segment contribute to the purpose
of that segment. This theory, however, does
not comment on initiative within the segment.
Nor does it specify when and how speakers
should start a segment or end the current one.
Hence, it underspecies what speakers can do
in dialogue.
Research in initiative works to account for
which speaker is driving the conversation at
any given point. For example, in a question-
answer pair, the speaker asking the question
is said to have the initiative (Whittaker and
Stenton, 1988; Walker and Whittaker, 1990;
Novick and Sutton, 1997). Whittaker and
Stenton segmented dialogues where initiative
shifts from one speaker to the other. They
found that initiative "did not alternate from
speaker to speaker on a turn by turn basis,
but that there were long sequences of turns in
which [initiative] remained with one speaker.
In a mixed initiative system, the dialogue
manager needs to track initiative in order to
know when the system should add signicant
content, and when it should let the user take
over. However, no theory has oered a good
account of why a speaker would want to take
the initiative, or keep it once they have it.
In the rest of this paper we rst describe
previous work in discourse structure and in
initiative and describe our coding of them.
Next, we explore the relationship between dis-
course structure and initiative. As previous
studies have found (Whittaker and Stenton,
1988; Walker and Whittaker, 1990), there is
a close correlation between them, but the re-
lationship is not direct. We then explore how
initiative can shift within a subdialogue and
nd two types of contributions that a speaker
can make in a discourse segment: a special
kind of acknowledgment we call forward ac-
knowledgment, and short contributions that
add content to the segment. We propose that
initiative is subordinate to intentional struc-
ture. Additionally, our proposal is better able
to account for question-answer pairs and how
initiative returns to the original speaker after
an embedded subdialogue. It will have impor-
tant implications for dialogue management: a
system only needs to model intentional struc-
ture, from which initiative follows.
2 Discourse Structure and
Initiative Analysis
Our proposal for managing initiative builds
on two main areas of research, discourse struc-
ture and initiative. We start by discussing
the work of Grosz and Sidner (1986), which
ties speaker's intentions to linguistic struc-
ture, then discuss the work of Whittaker, et
al. in initiative. We introduce our coding
of three dialogues in the TRAINS corpus,
a corpus of human-human task-oriented di-
alogues, in which two participants work to-
gether to formulate a plan involving the man-
ufacture and transportation of goods (Allen et
al., 1995; Heeman and Allen, 1995). In these
dialogues, one speaker is the user (u), who has
a goal to solve, and the other speaker is the
system (s), who knows the detailed informa-
tion involved in how long it takes to ship and
manufacture goods.
2.1 Discourse Structure
Discourse structure is used to analyze dia-
logue from the top down, starting with the
purpose of the discourse as a whole, then the
purpose of each discourse segment, in order to
understand how each utterance ts into the
dialogue. The theory of discourse structure
developed by Grosz and Sidner (1986) pro-
poses that discourse structure is made up of
three components: linguistic structure, inten-
tional structure, and attentional state. Our
work focuses on the rst two components.
The linguistic structure is a hierarchical seg-
mentation of the dialogue into discourse seg-
ments. Segment boundaries are identied by
changes in tense and aspect, pause lengths,
speech rate, and discourse markers, such as
\anyway," \by the way," \so," and \rst of
all." The intentional structure is a hierarchy
of segment purposes. Each discourse segment
has a purpose, and the purpose of each seg-
ment contributes to the purpose of its parent.
Intentional structure is key to understanding
what the discourse is about and explains its
coherency.
Subdialogue coding: In our study, the
rst author segmented dialogues into subdi-
alogues based on the purpose of the utter-
ance (Smith and Gordon, 1997; Traum and
Hinkelman, 1992). We established two classes
of subdialogues: task subdialogues, segments
that describe subtasks in the dialogue, and
clarication subdialogues, local segments that
clarify a gap in understanding, either to re-
quest missing information or to supply miss-
ing information to the other speaker. The seg-
ment initiator gives the rst utterance in the
segment and establishes its purpose.
1
The
left side of Figure 1 gives an example of a
discourse segment (or subdialogue) with two
embedded clarication subdialogues, and the
segment initiator for each subdialogue. Gen-
erally, we found the dialogue structure in the
dialogues we analyzed to be quite at, with
few embedded structures. Typically, task
subdialogues occurred at the same level, and
clarication subdialogues were embedded in
a task subdialogue, as seen in the example in
Figure 1.
2.2 Initiative
Initiative is held by the speaker who is driv-
ing the conversation at any given point in the
conversation (Whittaker and Stenton, 1988,
1
The segment initiator corresponds to the initi-
ating conversational participant (ICP) of Grosz and
Sidner's theory. The non-initiator corresponds to the
other conversational participant (OCP).
Segment
Initiator Speaker Utterance Initiative
u u where pick up um one of the tankers there umph- with uh oranges that
will be used
u
s mm-hm
u for the t- u
s s okay to move the oranges we need a boxcar s
u okay
u we'll bring a boxcar from Elmira with the engine three u
s okay
u um so we'll get to Corning with the engine three and the boxcar will get
to Corning and we'll pick up uh
u
u u can a tanker and a boxcar be pulled by an engine u
s yes
u so okay so we'll pick up a tank of uh the tanker of OJ orange- oranges u
s ok- right
Figure 1: Example of Discourse Structure and Initiative Segments (d921-5.2: utt25-utt36).
Walker and Whittaker, 1990; Novick and Sut-
ton, 1997). It has been used to analyze dis-
course from the bottom up, starting with
utterances. We start with adjacency pairs
(Scheglo and Sacks, 1973), which consist of a
rst part, uttered by one of the speakers, and
a second part, uttered by the other. The rst
part sets up expectations for the second part,
and hence the speaker of the rst part can be
viewed as being in control of the dialogue dur-
ing both parts of the adjacency pair. Below
we give the annotation scheme used by Whit-
taker, et al (Whittaker and Stenton, 1988;
Walker and Whittaker, 1990) for annotating
initiative based on utterance type.
Assertions: Declarative utterances used to
state facts. The speaker has initiative,
except when it is a response to a ques-
tion.
Questions: Utterances intended to elicit in-
formation from others. The speaker has
initiative, except when it follows a ques-
tion or command.
Commands: Intended to induce actions in
others. The speaker has initiative.
Prompts: Utterances with no propositional
content (e.g., \yeah," \okay"). These ut-
terances do not exhibit initiative.
Whittaker and Stenton used the initiative
codings as a basis for segmenting dialogues.
They used dialogues between an expert and
a client about diagnosing and repairing soft-
ware faults. They found that not only did
initiative pass back and forth between the
speakers (unlike single-initiative dialogues),
but that initiative often stayed with a speaker
for on average of eight speaker turns.
Whittaker and Stenton (1988) looked at the
correlation of control boundaries to discourse
markers, and Walker and Whittaker (1990)
looked at anaphoric reference. These are the
same kinds of linguistic evidence that Grosz
and Sidner (1986) said marks discourse seg-
ment boundaries. In fact, Walker and Whit-
taker claimed that initiative segments are the
discourse segments of Grosz and Sidner's the-
ory, with the speaker with initiative being the
initiator of the segment, who establishes the
discourse segment purpose. However, they ac-
knowledged that \there can be topic shifts
without change of initiation, change of [ini-
tiative] without topic shift." In fact, when
we look at the dialogue excerpt given in Fig-
ure 1, we see that the initiative segmentation
identied the rst subdialogue, but not the
second. However, Walker and Whittaker did
not specify the relationship between initiative
and discourse structure.
Initiative coding In our study, the rst
author coded initiative using the annotation
scheme of Whittaker and Stenton (1988). The
right side of Figure 1 shows the annotation for
utterances where speakers demonstrate initia-
tive. For utterances where the speaker does
Table 1: Correlation of Segment Boundaries
Initiative vs Segment Initiator Initiative vs
Subdialogue vs Subdialogue Segment Initiator
Boundaries 113 113 46
Hits 47 46 41
Misses 66 67 5
False Positives 35 0 41
Recall 42% 41% 89%
Precision 57% 100% 50%
not demonstrate initiative, initiative is said to
belong to the last speaker that demonstrated
it. Hence, when the system uttered \mm-
hm" in the second utterance, which does
not demonstrate initiative, initiative does not
change from the user. We also show initia-
tive segment boundaries, which are identied
by changes in initiative. Here, we see that
initiative swings back and forth between the
system and the user several times, leading to
three initiative segments.
3 Relationship between Initiative
and Discourse Structure
Walker and Whittaker (1990) suggested that
changes in initiative correspond to changes
in discourse structure, but they did not de-
termine the exact relationship between them.
In this section we analyze the dierences be-
tween initiative segments and discourse struc-
ture for three dialogues from the TRAINS
corpus. We nd that there is a close rela-
tionship, but not a direct one.
3.1 Segment Boundary Comparison
In this section, we compare initiative bound-
aries (where initiative shifts from one speaker
to the next) to subdialogue boundaries (where
a new subdialogue begins) using recall and
precision.
2
An initiative boundary is scored
as a hit if there is a corresponding subdia-
logue boundary. It is scored as a false posi-
tive if there is no subdialogue boundary. A
subdialogue boundary is scored as a miss if
it has no initiative boundary. For example,
2
Recall = Hits / (Hits + Misses )
Precision = Hits / ( Hits + False Positives )
in Figure 1, there are two hits (the bound-
ary between the 3rd and 4th utterances and
the boundary between the 5th and 6th utter-
ances), two misses (the boundary between the
8th and 9th utterances and the boundary be-
tween the 10th and 11th utterances), and no
false positives. The second column of Table
1, \Initiative versus Subdialogues", gives the
results. We see that both recall and precision
are very low for initiative boundaries relative
to discourse boundaries. However, compar-
ing initiative segments to discourse segments
is not fair. The misses in Figure 1 should be
expected since the initiator of the last subdi-
alogue is the same as the higher level subdia-
logue.
To show the eect of the unfairness, we
contrast changes in segment initiator to dis-
course segment boundaries in the third col-
umn of Table 1, \Segment Initiator versus
Subdialogue." Not surprisingly, we obtained
a precision of 100%: by denition, the seg-
ment initiator is only set at the beginning of
each discourse segment. However, we only ob-
tained a recall rate of 41%. This means only
41% of discourse segment boundaries are ini-
tiated by a dierent speaker. We should not
expect these boundaries to have a change in
initiative, since there is no change in segment
initiator. A fair comparison should contrast
changes in initiative only to changes in dis-
course segment initiator. The results of doing
this is shown in the fourth column, \Initia-
tive versus Segment Initiator." Here we see
much better results for recall; however, pre-
cision is still very low. We will return to the
low precision rates in Section 4.
Table 2: Initiative Held by Segment Initiator
Clarication Task
Total Subdialogues Subdialogues
Subdialogues 91 45 46
First utterance 91 (100%) 45 (100%) 46 (100%)
Whole subdialogue 77 (85%) 45 (100%) 32 (70%)
Segment
Initiator Initiative Speaker Utterance
u u u okay so we have to take oranges from Corning and
bring them to Elmira
s right
u u and then back to Bath by
s s + by noon +
u u + mid- + by noon
Figure 2: Forward acknowledgment (d92a-5.2: utt14-18).
3.2 Shifts Within Discourse Segments
Although the recall rate in the last column
of Table 1 is very good, it shows there are
some changes in discourse segment initiator
that are not matched by changes in initia-
tive. The question is, does this happen at the
beginning of the segment, the end, or in the
middle? We looked at how often the segment
initiator is the same as the speaker with ini-
tiative for the rst utterance in each discourse
segment. As seen in Table 2, this does give us
a 100% correct rate, meaning that a discourse
segment can only be initiated by the speaker
also taking the initiative. This is not unex-
pected, since the speaker needs to contribute
something new, otherwise it would not count
as the beginning of a new discourse segment.
However, the initiative does not always stay
with the initiator for the entire segment, as
seen in the last row of Table 2.
4 Reconciling Initiative Inside
Discourse Segments
The rst utterance of each discourse segment
shows perfect agreement between the initiator
of the segment and speaker with initiative, as
seen in Table 2. But what happens during
the course of the segment? In this section we
focus on subdialogues where the non-initiator
makes a contribution and the initiator nishes
the segment.
4.1 Forward Acknowledgments
In the TRAINS corpus there are times
when listeners were so synchronized with the
speaker that could they anticipate what the
speaker was going to say and ll it in be-
fore the he said it. Figure 2 gives an exam-
ple, where the system lled in \by noon" for
the user. Typical acknowledgments are utter-
ances that indicate understanding of an ut-
terance made by the other speaker and do
not contribute content. Our phenomena of
forward acknowledgments also indicate under-
standing, but of what the other speaker is
about to say, even before he says it. By ll-
ing in what the other speaker was about to
say, the speaker indicates understanding and
also moves the conversation forward. In both
examples of forward acknowledgments in the
three dialogues, the initiator nished the ut-
terance of the other speaker.
Forward acknowledgments are coded as
demonstrating initiative, because they add
content. However, this initiative is subordi-
nate to the initiative of the main segment, so
they are show as being embedded in the par-
ent segment.
Segment
Initiator Initiative Speaker Utterance
u u u and then take those uh t- to Dan- and then go to
Dansville
s s so that's + uh let's see and + that's one one more hour
u + and +
u u yeah and we can un- we can
s s drop o at + the +
u u + drop + o th- that boxcar and take well
dr-
u yeah drop o the boxcar + of +
s s + and then + take two empty
ones
u u right two empty ones down to Avon
s + oh +
u u + and + pick up the the bananas
s right
Figure 3: Other contribution (d93-19.5, utt83-utt93)
4.2 Other-Contributions
The rest of the utterances coded with initia-
tive made by the non-initiator of the segment
were more substantial contributions. Here,
the speaker added content to the discourse
segment that is not predicted from the ini-
tiator's speech. We refer to these as other-
contributions, and they often occur where the
two speakers are closely collaborating and are
highly synchronized.
3
In Figure 3, we show a
dialogue excerpt in which the two speakers
are so closely synchronized that they pick up
parts of each others utterances and build on
it. Initiative shifts back and forth between the
two speakers, but, in fact, we think this phe-
nomenon of other-contributions is related to
the phenomena that Schirin (1987) referred
to as shared turns.
4.3 Eect on Initiative
Table 3 shows what happens to initiative af-
ter the non-initiator makes a contribution
demonstrating initiative. There were 25 of
3
It is interesting to note the amount of overlapping
speech (marked with a '+') in these examples of other-
contributions. It might be the case that when speak-
ers are highly synchronized, they are more bound to
loosen the restrictions on turn-taking. This is some-
thing we hope to investigate in the future.
Table 3: After Contribution by Non-Initiator
Contributions by Non-Initiator 25
New subdialogue 7
Embedded subdialogue 1
Initiative returns to segment initiator 14
Initiative held by non-initiator 3
these cases. In 7 of them, the next utter-
ance demonstrating initiative occurred in a
new subdialogue, and in one case it occurred
in an embedded subdialogue. We focus on the
remaining 17 cases where the next utterance
demonstrating initiative occurred within the
same discourse segment. In 14 of these, initia-
tive returned to the segment initiator, usually
in the very next utterance (13 out of 14). In
only 3 cases does initiative stay with the non-
initiator. This result is contrary to previous
theories of initiative (Walker and Whittaker,
1990; Chu-Carroll and Brown, 1997), which
would expect initiative to stay with the non-
initiator.
4.4 Discussion
Forward acknowledgments and other-
contributions are exceptions to the general
rule that initiative tends to reside with the
same speaker. Based on the results of our
small preliminary study, we propose that
initiative is subordinate to the intentional
structure of the discourse theory of Grosz
and Sidner. Initiative is held by the segment
initiator. The non-initiator can make utter-
ances that contribute to the purpose of the
current discourse segment, namely forward
acknowledgments and other-contributions,
but initiative remains with the segment
initiator. Hence, initiative does not need to
be tracked, because it is held by the initiator
of the discourse segment.
This proposal allows either speaker to con-
tribute to the purpose of a discourse segment,
which accounts not only for forward acknowl-
edgments and other-contributions, but also
can account for embedded subdialogues and
the answer to a question in a question-answer
pair. It can be argued that embedded subdi-
alogues initiated by either speaker contribute
to the purpose of its parent subdialogue. For
example, in Figure 1, the subdialogue initi-
ated by the system (s) contributes to the pur-
pose of the parent subdialogue by pointing
out a problem with the user's plan to move
oranges, which the user then adjusts. The
clarication subdialogue initiated by the user
(u) checks that the plan the user is developing
will work, and so it also supports the general
purpose of developing the plan.
The proposal also can also be used to sim-
plify how initiative is used in question-answer
pairs. Whittaker and Stenton (1988) assigned
initiative to the speaker of statements, except
when it was the answer to a question, in which
case it belonged to the speaker asking the
question. In our view, a question-answer pair
is a subdialogue with the initiative belonging
to the questioner. The answer is coded with
initiative, but it is an other-contribution, and
so this initiative is subordinate to the ques-
tioner's initiative.
Other researchers have struggled with
structure in initiative. Chu-Carroll and
Brown (1997) referred to initiative as dialogue
initiative, and proposed a second level, task
initiative, to model who is adding domain
actions. In contrast to our proposal, which
makes initiative subordinate to intentional
structure, they proposed that dialogue ini-
tiative is subordinate to their task initiative.
Hence, their model could incorrectly pre-
dict who has initiative after the non-initiator
makes a contribution. As was shown in Ta-
ble 3, after the non-initiator makes an other-
contribution within a subdialogue, generally
initiative returns to the segment initiator of
the subdialogue, instead of staying with the
non-initiator. Chu-Carroll and Brown also
used task initiative to model how cooperative
a system should be. With novice users, the
system would tend to have task initiative and
thus make domain actions, but not so with ex-
perts. This is similar to Smith and Gordon's
(1997) use of four levels of initiative, which
set how much initiative was given to the sys-
tem and how much was given to the user in
one of four levels. Although a system needs
to reason about how helpful it needs to be, it
is unclear whether this can be done through
a single variable that is tied to dialogue ini-
tiative.
5 Conclusion
In this paper, we have proposed that initia-
tive is subordinate to intentional structure in
dialogue. We have backed up this claim by ex-
amining utterances that demonstrate initia-
tive made by the non-initiator of the discourse
segment. We found that after these utter-
ances, initiative returns to the segment initia-
tor in almost all cases. The reconciliation of
initiative and discourse segments means that
we now understand how initiative and dia-
logue level intentions are related and have a
clearer picture of how both participants can
contribute to discourse intentions.
Based on our results, initiative in itself does
not need to be tracked. Initiative belongs to
the speaker who started the current discourse
segment. Therefore, a dialogue manager only
needs to model intentional structure.
6 Future Work
Our preliminary study was based on a small
set of data coded by one of the authors. Plans
for future work include annotating more dia-
logues with multiple coders to ensure the re-
sults reported here are reproducible. We also
intend to analyze what happens to initiative
after an embedded subdialogue is completed
to verify that initiative stays with the seg-
ment initiator of the parent segment. We also
want to better understand the few cases where
stays with the non-initiator (the three cases in
Table 3).
In addition to more TRAINS dialogues,
we will code dialogues from other corpora,
such as Maptask (Anderson et al, 1991) and
Switchboard (Godfrey et al, 1992). This
will help ensure that we do not introduce id-
iosyncrasies of the TRAINS corpus into our
theory. Rather than just code initiative, we
will use the DAMSL annotation scheme (Core
and Allen, 1997). This scheme annotates
the forward and backward-looking functions of
each utterance, from which initiative can be
derived. Reliable intercoder agreement has
been obtained with this coding scheme. For
coding discourse structure, several schemes
have been proposed (Passonneau and Litman,
1997; Flammia, 1998; Nakatani et al, 1995;
Traum and Nakatani, 1999) ranging from cod-
ing at segmentation on monologues to hier-
archical segmentation of dialogues. We will
use these annotation schemes as a foundation,
and monitor our annotation results to ensure
we achieve good intercoder reliability.
Our theory necessitates that we better un-
derstand the structure of discourse, how it is
built, and the actions and rules that a dis-
course manager can use to aect the discourse
structure. We also need to understand the
reasoning process that determines whether a
participant will make an other-contribution or
start a new subdialogue. Since dialogue is
a collaborative eort (Cohen and Levesque,
1994; Clark and Wilkes-Gibbs, 1986), we also
need to explore how the participants collabo-
rate on the discourse structure.
7 Acknowledgments
The authors gratefully acknowledge funding
from the Intel Research Council. The au-
thors also thank David Traum and members
of the Centers for Spoken Language Under-
standing and Human Computer Communica-
tion at OGI for helpful discussions and com-
ments.
References
J. Allen, L. Schubert, G. Ferguson, P. Heeman,
C. Hwang, T. Kato, M. Light, N. Martin, B.
Miller, M. Poesio, and D. Traum. 1995. The
Trains project: A case study in building a con-
versational planning agent. Journal of Experi-
mental and Theoretical AI, 7:7{48.
A. Anderson, M. Bader, E. Bard, E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAl-
lister, J. Miller, C. Sotillo, H. Thompson, and
R. Weinert. 1991. The HCRC map task corpus.
Language and Speech, 34(4):351{366.
J. Chu-Carroll and M. Brown. 1997. Tracking
initiative in collaborative dialogue interaction.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
Madrid, Spain, July.
H. Clark and D. Wilkes-Gibbs. 1986. Referring
as a collaborative process. Cognition, 22:1{39.
P. Cohen and H. Levesque. 1994. Preliminaries
to a collaborative model of dialogue. Speech
Communications, 15(3{4):265{274, December.
M. Core and J. Allen. 1997. Coding dialogs with
the DAMSL annotation scheme. In Working
notes of the AAAI Fall Symposium on Com-
municative Action in Humans and Machines.
G. Flammia. 1998. Discourse segmentation of
spoken dialogue: an empirical approach. Doc-
toral dissertation, Department of Electrical and
Computer Science, Massachusetts Institute of
Technology.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
the International Conference on Audio, Speech
and Signal Processing (ICASSP), pages 517{
520.
B. Grosz and C. Sidner. 1986. Attention, inten-
tions, and the structure of discourse. Compu-
tational Linguistics, 12(3):175{204.
P. Heeman and J. Allen. 1995. The Trains spo-
ken dialog corpus. CD-ROM, Linguistics Data
Consortium, April.
C. Nakatani, B. Grosz, D. Ahn, and J. Hirschberg.
1995. Instructions for annotating discourse.
Technical Report 21-95, Center for Research
in Computing Technology, Harvard University,
Cambridge MA, September.
D. Novick and S. Sutton. 1997. What is
mixed-initiative interaction? In 1997 AAAI
Spring Symposium on Computational Models
for Mixed Initiative Interaction, pages 24{26,
Stanford University, March. AAAI Press.
R. Passonneau and D. Litman. 1997. Discourse
segmentation by human and automated means.
Computational Linguistics, 103{139.
E. Scheglo and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 7:289{327.
D. Schirin. 1987. Discourse Markers. Cam-
bridge University Press, New York.
R. Smith and S. Gordon. 1997. Eects of vari-
able initiative on linguistic behavior in human-
computer spoken natural language dialogue.
Computational Linguistics, 23(1):141{168.
D. Traum and E. Hinkelman. 1992. Conversation
acts in task-oriented spoken dialogue. Compu-
tational Intelligence, 8(3):575{599. Special Is-
sue on Non-literal language.
D. Traum and C. Nakatani. 1999. A two-level ap-
proach to coding dialogue for discourse struc-
ture: Activities of the 1998 working group
on higher-level structures. In Proceedings of
the ACL'99 Workshop Towards Standards and
Tools for Discourse Tagging, pages 101{108,
June.
M. Walker and S. Whittaker. 1990. Mixed initia-
tive in dialogue: An investigation into discourse
segmentation. In Proceedings of the 28th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 70{78.
S. Whittaker and P. Stenton. 1988. Cues and con-
trol in expert client dialogues. In Proceedings
of the 26th Annual Meeting of the Association
for Computational Linguistics, pages 123{130.
DialogueView: An Annotation Tool for Dialogue
Peter A. Heeman and Fan Yang and Susan E. Strayer
Computer Science and Engineering
OGI School of Science and Engineering
Oregon Health & Science University
20000 NW Walker Rd., Beaverton OR, 97006
heeman@cse.ogi.edu yangf@cse.ogi.edu susan strayer@yahoo.com
Abstract
This paper describes DialogueView, a
tool for annotating dialogues with utter-
ance boundaries, speech repairs, speech
act tags, and discourse segments. The
tool provides several views of the data,
including a word view that is time-
aligned with the audio signal, and an ut-
terance view that shows the dialogue as
if it were a script for a play. The ut-
terance view abstracts away from lower
level details that are coded in the word
view. This allows the annotator to have
a simpler view of the dialogue when cod-
ing speech act tags and discourse struc-
ture, but still have access to the details
when needed.
1 Introduction
There is a growing interest in annotating human-
human dialogue. Annotated dialogues are useful
for formulating and verifying theories of dialogue
and for building statistical models. In addition to
orthographic word transcription, one might want
the following dialogue annotations.
 Annotation of the speech repairs. Speech re-
pairs are a type of disuency where speakers
go back and change or repeat something they
just said.
 Segmentation of the speech of each speaker
into utterance units, with a single ordering of
the utterances. We refer to this as linearizing
the dialogue (Heeman and Allen, 1995a).
 Each utterance tagged with its speech act
function.
 The utterances grouped into hierarchical dis-
course segments.
There are tools that address subsets of the above
tasks. However, we feel that doing dialogue an-
notation is very di?cult. Part of this di?culty
is due to the interactions between the annotation
tasks. An error at a lower level can have a large
impact on the higher level annotations. For in-
stance, there can be ambiguity as to whether an
\okay" is part of a speech repair; this will im-
pact the segmentation of the speech into utter-
ance units and the speech act coding. Sometimes,
it is only by considering the higher level annota-
tions that one can make sense of what is going on
at the lower levels, especially when there is over-
lapping speech. Hence, a tool is needed that lets
users examine and code the dialogue at all lev-
els. The second reason why dialogue annotation
is di?cult is because it is di?cult to follow what
is occurring in the dialogue, especially for coding
discourse structure. A dialogue annotation tool
needs to help the user deal with this overload.
In this paper, we describe our dialogue anno-
tation tool, DialogueView. This tool displays the
dialogue at three dierent levels of abstraction.
The word level shows the words time-aligned with
the audio signal. The utterance level shows the
dialogue as a sequence of utterance, as if it were a
script for a play. It abstracts away from the exact
timing of the words and even skips words that do
not impact the progression of the dialogue. The
block level shows the dialogue as a hierarchy of
discourse segment purposes, and abstracts away
from the exact utterances that were said. Anno-
tations are done at the view that is most appropri-
ate for what is being annotated. The tool allows
the user to easily navigate between the three views
and automatically updates the higher level views
when changes are made in the lower level views.
Because the higher levels abstract away lower level
details, it is easier for the user to understand what
is happening in the dialogue.
1
Yet, the user can
easily refer to the lower level views to see what
1
This approach bears resemblance to the work of
Jonsson and Dahlback (2000) in which they distill
human-human dialogues by removing those parts that
would not occur in human-computer dialogue. They
do this to create training data for their spoken dia-
logue systems.
       Philadelphia, July 2002, pp. 50-59.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
actually occurred when necessary.
In the rest of this paper, we rst discuss other
annotation tools. We then describe the three lev-
els of abstraction in our annotation tool. We then
discuss audio playback. Next, we discuss how the
tool can be customized for dierent annotation
schemes. Finally, we discuss the implementation
of the tool.
2 Existing Tools
There are a number of tools that let users annotate
speech audio les. These include Emu (Cassidy
and Harrington, 2001), SpeechView (Sutton et al,
1998) and Waves+ (Ent, 1993). These tools often
allow multiple text annotation les to be associ-
ated with the waveform and allow users an easy fa-
cility to capture time alignments. For instance, for
the ToBI annotation scheme (Pitrelli et al, 1994),
one can have the word alignment in one text le,
intonation features in a second, break indices in a
third, and miscellaneous features in a fourth. The
audio annotation tools often have powerful signal
analysis packages for displaying such phenomena
as spectrograms and voicing. These tools, how-
ever, do not have any built-in facility to group
words into utterances nor group utterances into
hierarchical discourse segments.
Several tools allow users to annotate higher
level structure in dialogue. The annotation tool
DAT from the University of Rochester (Fergu-
son, 1998) allows users to annotate utterances or
groups of utterances with a set of hard-coded an-
notation tags for the DAMSL annotation scheme
(Allen and Core, 1997; Core and Allen, 1997).
The tool Nb from MIT (Flammia, 1995; Flammia,
1998) allows users to impose a hierarchical group-
ing on a sequence of utterances, and hence anno-
tate discourse structure. Both DAT and Nb take
as input a linearization of the speaker utterances.
Mistakes in this input cannot be xed. Whether
there are errors or not, users cannot see the exact
timing interactions between the speakers' words or
the length of pauses. This simplication can make
it di?cult for the annotator to truly understand
what is happening in the dialogue, especially for
overlapping speech, where speakers ght over the
turn or make back-channels.
The tools Transcriber (Barras et al, 2001) and
Mate (McKelvie et al, 2001) allow multiple views
of the data: a word view with words time-aligned
to the audio signal and an utterance view. How-
ever, Transcriber is geared to single channel data
and has a weak ability to handle overlapping
speech and Mate only allows one view to be shown
at a time. The author of a competing tool has
remarked that \speed and stability of [Mate] are
both still problematic for real annotation. Also,
the highly generic approach increases the initial
eort to set up the tool since you basically have
to write your own tool using the Mate style-sheet
language" (Kipp, 2001). Hence, he developed a
new tool Anvil that he claims is a better trade-o
among generality, functionality, and complexity.
This tool oers multi-modal annotation support,
but like Transcriber, does not allow detailed an-
notation of dialogue phenomena, such as overlap-
ping speech and abandoned speech, and has no
abstraction mechanism.
3 Word View
Our dialogue annotation tool, DialogueView,
gives the user three views of the data. The low-
est level view is called WordView and takes as
input the words said by each speaker and their
start and stop times and shows them time-aligned
with the audio signal. Figure 1 shows an ex-
cerpt from the Trains corpus (Heeman and Allen,
1995b) in WordView. This view is ideal for view-
ing the exact timing of speech, especially overlap-
ping speech. As will be discussed below, we use
it for segmenting the speech into utterances, and
annotating communicative status and speech re-
pairs.
2
These annotations will allow us to build a
simpler representation of what is happening in the
speech for the utterance view, which is discussed
in the next section.
3.1 Utterance Segmentation
As can be seen in Figure 1, WordView shows the
words segmented into utterances, which for our
purpose is simply a grouping of consecutive words
by a single speaker, in which there is minimal
eect from the other speaker. Consider the ex-
change in Figure 1, where the upper speaker says
\and then take the remaining boxcar down to" fol-
lowed by the lower speaker saying \right, to du-",
followed by the upper speaker saying \Corning".
Although one could consider the upper speaker's
speech as one utterance, we preclude that due to
the interaction from the lower speaker. A full def-
inition of `utterance' is beyond the scope of this
paper, and is left to the users to specify in their
annotation scheme.
Utterance boundaries in WordView are only
shown by their start times. The end of the utter-
ance is either the start of the next utterance by the
2
There currently is no support for changing the
word transcription. There are already a number of
tools that do an excellent job at this task. Hence,
adding this capability is a low priority for us.
Figure 1: Utterance Segmentation in WordView
same speaker or the end of the le. With Word-
View, the user can easily move, insert or delete
utterance boundaries. The tool ensures that the
boundaries never fall in the middle of a word by
the speaker.
The start times of the utterances are used to
derive a single ordering of the utterances of the
two speakers. This linearization of the dialogue
captures how the speakers are sequentially adding
to the dialogue. The linearization is used by the
next view to give an abstraction away from the
exact timing of the speech.
3.2 Communicative Status
WordView allows features of the utterances to be
annotated. In practice, however, we only anno-
tate features related to its communicative status,
based on the DAMSL annotation scheme (Allen
and Core, 1997). Below, we give the utterance
tags that we assign in WordView.
Abandoned: The speaker abandoned what
they were saying and it had no impact on the rest
of the dialogue. This mainly happens where one
speaker loses the ght over who speaks next. Fig-
ure 2 gives an example where the upper speaker
tries to take the turn by saying \takes," and than
abandons this eort.
Incomplete: The speaker did not make a com-
plete utterance, either prosodically, syntactically,
or semantically. Unlike the previous category, the
partial utterance did impact the dialogue behav-
ior. Figure 1 gives two examples. The rst is
where the upper speaker says \and than take the
remaining boxcar down to", and the second is
where the lower speaker said \to du-," which was
than completed by the upper speaker.
Overlap: The speech in the utterance overlaps
with the speech from the prior utterance. For
instance, in Figure 1, the lower speaker utters
\okay" during the middle of an utterance, perhaps
to tell the upper speaker that they are understand-
ing everything so far. However, a simple lineariza-
tion would make it seem that the \okay" is an
acknowledgment of the entire utterance, which is
not the case. Hence, we tag the \okay" utterance
with the overlap tag. The next view, Utterance-
View, will take the overlap tag into account in
displaying the utterances, as will be discussed in
Section 4.3.
Not all overlapping speech needs to be anno-
tated with the overlap tag. In Figure 1, the sec-
ond instance of \Corning" overlaps slightly with
the rst instance of \Corning". However, viewing
it sequentially does not alter the analysis of the
exchange.
3.3 Reordering Overlapping Speech
Consider the example in Figure 2. Before the ex-
cerpt shown, the lower speaker had just nished
an utterance and then paused for over a second.
The upper speaker then acknowledged the utter-
ance with \okay", but this happened a fraction of
a second after the bottom speaker started speak-
ing again. A simple linearization of the dialogue
would have the \okay" following the wrong stretch
of speech|\and than th(at)- takes w- what three
hours." A solution to this would be to anno-
Figure 2: Utterance Ordering and Speech Repairs in WordView
tate the \okay" with the overlap tag. However,
this \Okay" is more similar to the overlapping in-
stances of \Corning" in Figure 1. The fact that
\Okay" overlaps with the start of the next utter-
ance is not critical to understanding what is oc-
curring in the dialogue, as long as we linearize
the \Okay" to occur before the other utterance.
WordView allows the user to change the lineariza-
tion by moving the start times of utterances. This
can be done provided that the speaker was silent
in the time interval preceding where the other per-
son started talking.
In summary, overlapping speech can be handled
in three ways. The utterance can be explicitly
tagged as overlapping; the overlap can be ignored
if it is not critical in understanding what is going
on in the dialogue; or the start time of the utter-
ance can be changed so that the overlap does not
need to be tagged.
3.4 Speech Repairs
WordView also allows users to annotate speech
repairs. A speech repair is where a user goes back
and repeats or changes something that was just
said (Heeman and Allen, 1999). Below we give an
example of a speech repair and show its principle
components: reparandum, interruption point, and
editing term.
Example 1
why don't we take
|{z}
reparandum
"
ip
um
|{z}
et
take two boxcars
The reparandum is the speech that is being re-
placed, the interruption point is the end of the
reparandum, and the editing term consists of
words such as \um", \uh", \okay", \let's see" that
help signal the repair.
To annotate a repair, the user highlights a se-
quence of words and then tags it as a reparandum
or an editing term of a repair. The user can also
specify the type of repair. Figure 2 shows how
speech repairs are displayed in WordView. The
words in the reparandum and editing term are un-
derlined and displayed in a special color.
3.5 Speech Repairs and Utterances
Some phenomena can be marked as either a speech
repair, or could be marked using the utterance
tags of incomplete or abandon. This is especially
true for fresh starts (Heeman and Allen, 1999),
where a speaker abandons the current utterance
and starts over. To avoid having multiple ways of
annotating the same phenomena, we impose the
following restrictions in our annotation scheme.
 There cannot be an utterance boundary in-
side of a reparandum, inside of an editing
term, at the interruption point, nor at the
end of the editing term. Hence, something
annotated as a reparandum cannot also be
annotated as an abandoned utterance.
 Abandoned or incomplete utterances cannot
be followed by an utterance by the same
speaker.
 All word fragments must either be the last
word of a reparandum or the last word of an
utterance that is marked as abandoned or in-
complete.
Figure 3: UtteranceView: Segmented Utterances in UtteranceView
 Abandoned or incomplete utterances can end
with an editing term, which would be marked
as the editing term of an abridged repair.
3.6 Summary
There are a number of reasons why we annotate
utterance boundaries, speech repairs, and commu-
nicative status in WordView. Annotating utter-
ance boundaries and overlapping speech requires
the user to take into account the exact timing of
the utterances, which is best done in this view.
Speech repairs also require ne tuned listening to
the speech and have strong interactions with ut-
terance boundaries. Furthermore, all three types
of annotations can be used to build a simpler view
of what is happening in the dialogue, as will be ex-
plained in the next section.
4 Utterance View
The annotations from the word view are used to
build the next view, which we refer to as Utter-
anceView. The dialogue excerpts from Figures 1
and 2 are shown in the utterance view in Figures 3
and 4, respectively. The utterance view abstracts
away from the detailed timing information and in-
dividual words that were spoken. Instead, it fo-
cuses on the sequence of utterances between the
speakers. By removing details that were anno-
tated in the word view, we still preserve the im-
portant details that are needed to annotate speech
act types for the utterances and to annotate dis-
course segments. Of course, if the user wants to
see the exact timing of the words in the utter-
ances, they can examine the word view, as it is
Figure 4: UtteranceView: Reordered and Abandoned Utterances in UtteranceView
displayed alongside the utterance view. There are
also navigation buttons on each view that allow
the user to easily reposition the portion of the di-
alogue in the other view. Furthermore, changes
made in the word view are immediately propa-
gated into the utterance view, and hence the user
will immediately see the impact of their annota-
tions.
4.1 Utterance Ordering
Utterance ordering in the utterance view is deter-
mined by the start times of the utterances as spec-
ied in WordView. As was explained earlier, alter-
ing the start time of an utterance can be used to
simplify some cases of overlapping speech, where
the overlap is not critical to understanding the
role of the utterance in the dialogue. Figure 2
gave the word view of such an example. Rather
than code it as an overlap, we moved the start
time of the \okay" utterance so that it precedes
the overlapping speech by the other speaker. Fig-
ure 4 shows how this looks in the utterance view.
Here, the annotator would view the \okay" as an
acknowledgment that occurred between the two
utterances of the lower speaker.
4.2 Speech Repairs
In the word view, the user annotates the reparan-
dum and editing term of speech repairs. If the
reparandum and editing term are removed, the
resulting utterance reects what the speaker in-
tended to say. Speech repairs do carry informa-
tion.
 Their occurrence can signal a lack of certainty
of the speaker.
 The reparandum of a repair can have an
anaphoric reference, as in \Peter was, well
he was red."
However, removing the reparandum and editing
term of speech repairs from utterances in the ut-
terance view leads to a simpler representation of
what is occurring in the dialogue. Hence, in the
utterance view, we clean up the speech repairs, as
shown in Figures 2 and 4. Figure 2, which shows
the word view, contains the utterance \and then
th(at) that takes w- what three hours"; whereas
Figure 4, which shows the utterance view, con-
tains \and then that takes what three hours." Of
course, a user can always refer to the word view
when annotating in the utterance view if they
want to see the exact speech that was said. In
most cases, we feel that this will not be neces-
sary for annotating speech act tags and discourse
segments.
4.3 Communicative Status
The communicative status coded in the word view
is used in formatting the utterance view. Utter-
ances tagged as overlapping are indented and dis-
played with `+' on either side, as shown in Fig-
ure 3. Utterances tagged as abandoned are not
shown, as can be seen in Figure 4, in which the
abandoned utterance \takes" made by the upper
speaker is not included. Utterances tagged as in-
complete are shown with a trailing \..." as shown
in Figure 3.
4.4 Annotating Utterance Tags
In the utterance view, one can also annotate the
utterances with various tags. For our work, we
use a subset of the DAMSL tags corresponding
to forward and backward functions (Allen and
Core, 1997). Forward functions include state-
ment, request information, and suggestion. Back-
ward functions include answer, acknowledgment,
and agreement. Although these utterance tags
could be annotated in the word view, doing it in
the utterance view allows us to see more context,
which is needed to give the utterance the proper
tags. When necessary, the annotator can easily
refer to the word view to see the exact local con-
text.
4.5 Annotating Blocks of Utterances
In the utterance view, the user can also annotate
hierarchical groupings of utterances.
3
We use the
utterance blocks to annotate discourse structure
(Grosz and Sidner, 1986). This is similar to what
Flammia's tool allows (Flammia, 1995). Rather
than showing it with indentation and color, we
draw boxes around segments. Figure 3 shows a
dialogue excerpt with three utterance blocks in-
side of a larger block. To create a segment, the
user highlights a sequence of utterances and then
presses the \make segment" button. The user can
change the boundaries of the blocks by simply
dragging either the top or bottom edge of the box.
Blocks can also be deleted. The tool ensures that
if two blocks have utterances in common then one
block is entirely contained in the other.
Tags can also be assigned to the blocks. We
have just started using the tool for discourse seg-
mentation, and so we are still rening these tags.
In Grosz and Sidner's theory of discourse struc-
ture (1986), the speaker who initiates the block
does so to accomplish a certain purpose. We have
a tag for annotating this purpose. We also have
tags to categorize the block as a greeting, spec-
ify goal, construct plan, summarize plan, verify
plan, give info, request info, or other (Strayer and
Heeman, 2001).
The utterance view also allows the user to open
or close a block. When a block is open (the de-
fault), all of its utterances and sub-blocks are dis-
3
We do not allow segments to be interleaved. It is
unclear if such phenomena actually occur.
played. When it is closed, its utterances and sub-
blocks are replaced by the single line purpose.
Opening and closing blocks is useful as it allows
the user to control the level of detail that is shown.
Consider the third embedded block shown in Fig-
ure 3, in which the conversants take seven utter-
ances to jointly make a suggestion. After we have
analyzed it, we can close it and just see the pur-
pose. This will make it easier to determine the
segmentation of the blocks that contain it.
We are experimenting with a special type of
dialogue block. Consider the example from the
previous paragraph, in which the conversants
take seven utterances to jointly make a sugges-
tion. This is related to the shared turns of
Schirin (1987), the co-operative completions of
Linell (1998), and the grounding units of Traum
and Nakatani (1999). We are experimenting with
how to support the annotation of such phenom-
ena. We have added a tag to indicate whether the
utterances in the block are being used to build a
single contribution. For these single contributions,
we also supply a concise paraphrase of what was
said. We have found that this paraphrase can be
built from a sequential subset of the words in the
utterances of the block. For instance, the para-
phrase of our example block is \and then take the
remaining boxcar down to Corning."
5 Block View
We are experimenting with a third view of the
dialogue. This view, which we refer to as Block-
View, abstracts away from the individual utter-
ances, and shows the hierarchical structure of the
discourse segments. This gives a very concise view
of the dialogue. The block view is also convenient
for it provides an index to the whole dialogue.
This allows the user to quickly move around the
dialogue.
6 Audio Playback
Each view gives the user the ability to select a re-
gion of the dialogue and to play it. In the word
view, the user can play each speaker channel indi-
vidually or both combined.
4
This ability is espe-
cially useful for overlapping speech, where the an-
notator would want to listen to what each speaker
said individually, as well as hear the timing be-
tween the speaker utterances.
Just as each view provides a visual abstraction
from the previous one, we also do the same with
audio playback. In the word view, which has
4
In order to play each speaker individually, we re-
quire a separate audio channel for each speaker.
wordViewUtt => atmostoneof abandoned incomplete overlap
uttViewUtt => anyof forward backward comment
uttViewUtt.forward => oneof statement question suggestion other
uttViewUtt.backward => oneof agreement understanding answer other
uttViewUtt.comment => other
Figure 5: Sample Specication of Utterance Tags
Figure 6: Sample Utterance Annotation Panel
the speech repairs annotated, the user can play
back the speech cleanly of either speaker, where
the stretches of speech annotated as the reparan-
dum or editing term of a repair are skipped. We
have found this to be of great assistance in verify-
ing if something should be annotated as a repair
or not. It gives us an audio means to verify the
speech repair annotations. If we have annotated
the repair correctly, the edited audio signal should
sound fairly natural.
In formatting the utterance view, we take into
account whether utterances have been marked as
abandoned or overlapped. We provide a special
playback in the utterance view that takes this
into account. We build an audio le in which we
skip over repairs, skip over abandoned speech, and
shorten large silences. If there is overlap that is
not marked as signicant, we linearize it by con-
catenating the utterances together. If the overlap
is marked as signicant, we keep the overlap. We
are nding that this gives us an audio means to
ensure that our markings of abandonment, over-
lap and our linearization is correct.
We are also experimenting with even further
simplifying the audio output. For blocks that have
a paraphrase, and the block is closed, we play the
paraphrase by constructing it from the words said
in the block. For blocks that are closed that do
not have a paraphrase, we use the text-to-speech
engine in the CSLU toolkit (Colton et al, 1996;
Sutton et al, 1997) to say the purpose, as if there
was a narrator.
7 Customizations
Some aspects of the tool are built in, such as the
notion of utterances, speech repairs, and hierar-
chical grouping of utterances into blocks. How-
ever, the annotations of these phenomena and how
they are displayed can be customized through a
conguration le. This allows us to easily exper-
iment as we revise our annotation scheme; to use
domain specic tags; and to make the tool useful
for other researchers who might use dierent tags.
Speech repair tags, utterance tags, and block
tags are specied in the conguration le. Fig-
ure 5 gives a sample of how the annotation
tags for an utterance are specied. The two top
level entries in the gure are \wordViewUtt" and
\uttViewUtt", which specify the utterance anno-
tation tags in WordView and UtteranceView, re-
spectively. The decomposition can be of one of
three types.
atmostoneof: at most one of the attributes can
be specied
oneof: exactly one of the attributes must be spec-
ied
anyof: there is no restriction on which attributes
can be specied
The subcomponents can either be terminals as is
the case for the decomposition of \wordViewUtt",
or can be non-terminals, as is the case for each
of the three subcomponents of \uttViewUtt".
Hence, hierarchical tags are supported. Termi-
nals are assumed to be of binary type, except for
\other", which is assumed to be a string. The
conguration le determines how the annotation
panel is generated. For the annotation scheme
specied in Figure 5, Figure 6 shows the annota-
tion panel that would be automatically generated
for the utterance view.
As we explained earlier, some of the utterance
tags aect how the word view and utterance view
are formatted. Rather than hard code this func-
tionality, it is specied in the conguration le.
We are still experimenting with the best way to
code this functionality. Figure 7 gives an exam-
ple of how we code the utterance tag function-
ality. The rst line indicates that the utterance
wordViewUtt.abandoned do wordView color red
wordViewUtt.abandoned uttView ignore
wordViewUtt.incomplete wordView color yellow
wordViewUtt.incomplete uttView trailsoff
wordViewUtt.overlap wordView color blue
wordViewUtt.overlap uttView overlap
Figure 7: Sample Utterance Display Specication
tag of \abandoned" coded in WordView should
be displayed in red in WordView. The second line
indicates that it should not be displayed in Utter-
anceView.
8 Implementation
DialogueView is written in Tcl/Tk. We also use
utilities from the CSLU Speech Toolkit (Colton
et al, 1996; Sutton et al, 1997), including audio
and wave handling and speech synthesis. We have
rewritten the tool to use an object-oriented exten-
sion of Tcl called Tclpp, designed and developed
by Stefan Simnige. This is allowing us to better
manage the growing complexity of the tool as well
as reuse pieces of the software in our annotation
comparison tool (Yang et al, 2002). It should also
help in expanding the tool so that it can handle
any number of speakers.
9 Conclusion
In this paper, we described a dialogue annotation
tool that we are developing for segmenting dia-
logue into utterances, annotating speech repairs,
tagging speech acts, and segmenting dialogue into
hierarchical discourse segments. The tool presents
the dialogue at dierent levels of abstraction al-
lowing the user to both see in detail what is go-
ing on and see the higher level structure that is
being built. The higher levels not only abstract
away from the exact timing, but also can skip over
words, whole utterances, and even simplify seg-
ments to a single line paraphrase. Along with the
visual presentation, the audio can also be played
at these dierent levels of abstraction. We feel
that these capabilities should help annotators bet-
ter code dialogue.
This tool is still under active development. In
particular, we are currently rening how blocks
are displayed, improving the ability to customize
the tool for dierent tagsets, and improving the
audio playback facilities. As we develop this tool,
we are also doing dialogue annotation, and ren-
ing our scheme for annotating dialogue in order to
better capture the salient features of dialogue and
improve the inter-coder reliability.
10 Acknowledgments
The authors acknowledgment funding from the In-
tel Research Council.
References
James F. Allen and Mark G. Core. 1997. Damsl:
Dialog annotation markup in several layers.
Unpublished Manuscript.
Claude Barras, Edouard Georois, Zhibiao Wu,
and Mark Liberman. 2001. Transcriber: devel-
opment and use of a tool for assisting speech
corpora production. Speech Communications,
33:5{22.
Steve Cassidy and Jonathan Harrington. 2001.
Multi-level annotation in the Emu speech
database management system. Speech Commu-
nications, 33:61{77.
Don Colton, Ron Cole, David G. Novick, and
Stephen Sutton. 1996. A laboratory course
for designing and testing spoken dialogue sys-
tems. In Proceedings of the International Con-
ference on Audio, Speech and Signal Processing
(ICASSP), pages 1129{1132.
Mark G. Core and James F. Allen. 1997. Coding
dialogs with the DAMSL annotation scheme.
In Working notes of the AAAI Fall Symposium
on Communicative Action in Humans and Ma-
chines.
Entropic Research Laboratory, Inc., 1993.
WAVES+ Reference Manual. Version 5.0.
George Ferguson. 1998. DAT: Dialogue annota-
tion tool. Available from www.cs.rochester.edu
in the subdirectory research/speech/damsl.
Giovanni Flammia. 1995. N.b.: A graphical user
interface for annotating spoken dialogue. In
AAAI Spring Symposium on Empirical Meth-
ods in Discourse Interpretation and Generation,
pages pages 40{46, Stanford, CA.
Giovanni Flammia. 1998. Discourse segmenta-
tion of spoken dialogue: an empirical approach.
Doctoral dissertation, Department of Electrical
and Computer Science, Massachusetts Institute
of Technology.
Barbara J. Grosz and Candace L. Sidner. 1986.
Attention, intentions, and the structure of dis-
course. Computational Linguistics, 12(3):175{
204.
Peter A. Heeman and James Allen. 1995a. Dia-
logue transcription tools. Trains Technical Note
94-1, Department of Computer Science, Univer-
sity of Rochester, March. Revised.
Peter A. Heeman and James F. Allen. 1995b. The
Trains spoken dialog corpus. CD-ROM, Lin-
guistics Data Consortium, April.
Peter A. Heeman and James F. Allen. 1999.
Speech repairs, intonational phrases and dis-
course markers: Modeling speakers' utterances
in spoken dialog. Computational Linguistics,
25(4):527{572.
Arne Jonsson and Nils Dahlback. 2000. Distill-
ing dialogues | a method using natural di-
alogue corpora for dialogue systems develop-
ment. In Proceedings of the 6th Applied Natural
Language Processing Conference, pages 44{51,
Seattle.
Michael Kipp. 2001. Anvil: A generic annotation
tool for multimodal dialogue. In Proceedings of
the 7th European Conference on Speech Com-
munication and Technology (Eurospeech).
Per Linell. 1998. Approaching Dialogue: Talk,
Interaction and Contexts in Dialogical Perspec-
tives. John Benjamins Publishing.
David McKelvie, Amy Isard, Andreas Mengel,
Morten Baun Muller, Michael Grosse, and Mar-
ion Klein. 2001. The MATE workbench | an
annotation tool for XML coded speech corpora.
Speech Communications, 33:97{112.
John F. Pitrelli, Mary E. Beckman, and Ju-
lia Hirschberg. 1994. Evaluation of prosodic
transcription labeling reliability in the ToBI
framework. In Proceedings of the 3rd Interna-
tional Conference on Spoken Language Process-
ing (ICSLP-94), Yokohama, September.
Deborah Schirin. 1987. Discourse Markers.
Cambridge University Press, New York.
Susan E. Strayer and Peter A. Heeman. 2001.
Dialogue structure and mixed initiative. In
Second workshop of the Special Interest Group
on Dialogue, pages 153{161, Aalborg Denmark,
September.
Stephen Sutton, Ed Kaiser, Andrew Cronk, and
Ronald Cole. 1997. Bringing spoken language
systems to the classroom. In Proceedings of the
5th European Conference on Speech Commu-
nication and Technology (Eurospeech), Rhodes,
Greece.
S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk,
P. Vermeulen, M. Macon, Y. Yan, E. Kaiser,
R. Rundle, K. Shobaki, P. Hosom, A. Kain,
J. Wouters, M. Massaro, and M. Cohen. 1998.
Universal speech tools: the cslu toolkit. In Pro-
ceedings of the 5th International Conference on
Spoken Language Processing (ICSLP-98), pages
3221{3224, Sydney Australia, November.
David R. Traum and Christine H. Nakatani. 1999.
A two-level approach to coding dialogue for dis-
course structure: Activities of the 1998 working
group on higher-level structures. In Proceed-
ings of the ACL'99 Workshop Towards Stan-
dards and Tools for Discourse Tagging, pages
101{108, June.
Fan Yang, Susan E. Strayer, and Peter A. Hee-
man. 2002. ACT: a graphical dialogue anno-
tation comparison tool. Submitted for publica-
tion.
An Investigation of Interruptions and
Resumptions in Multi-Tasking Dialogues
Fan Yang?
Nuance Communications, Inc.
Peter A. Heeman
Oregon Health & Science University
Andrew L. Kun
University of New Hampshire
In this article we focus on human?human multi-tasking dialogues, in which pairs of con-
versants, using speech, work on an ongoing task while occasionally completing real-time tasks.
The ongoing task is a poker game in which conversants need to assemble a poker hand, and the
real-time task is a picture game in which conversants need to find out whether they have a certain
picture on their displays. We employ empirical corpus studies and machine learning experiments
to understand the mechanisms that people use in managing these complex interactions. First,
we examine task interruptions: switching from the ongoing task to a real-time task. We find
that generally conversants tend to interrupt at a less disruptive context in the ongoing task
when possible. We also find that the discourse markers oh and wait occur in initiating a task
interruption twice as often as in the conversation of the ongoing task. Pitch is also found to be
statistically correlated with task interruptions; in fact, the more disruptive the task interruption,
the higher the pitch. Second, we examine task resumptions: returning to the ongoing task after
completing an interrupting real-time task. We find that conversants might simply resume the
conversation where they left off, but sometimes they repeat the last utterance or summarize the
critical information that was exchanged before the interruption. Third, we apply machine learn-
ing to determine how well task interruptions can be recognized automatically and to investigate
the usefulness of the cues that we find in the corpus studies. We find that discourse context, pitch,
and the discourse markers oh and wait are important features to reliably recognize task interrup-
tions; and with non-lexical features one can improve the performance of recognizing task inter-
ruptions with more than a 50% relative error reduction over a baseline. Finally, we discuss the
implication of our findings for building a speech interface that supports multi-tasking dialogue.
1. Introduction
Existing speech interfaces have mostly been used to perform a single task, where
the user finishes with one task before moving on to the next. We envision that
? Nuance Communications, Inc., 505 First Ave. South, Suite 700, Seattle, WA 98104.
E-mail: fan.yang@nuance.com.
Submission received: 26 July 2009; revised submission received: 22 July 2010; accepted for publication:
13 October 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
next-generation speech interfaces will be able to work with the user on multiple tasks
at the same time, which is especially useful for real-time tasks. For instance, a driver in
a car might use a speech interface to catch up on e-mails, while occasionally checking
upcoming traffic conditions, and receiving navigation instructions; or a police officer
might need to be alerted to a nearby accident while accessing a database during a
routine traffic stop.
Several speech interfaces that allow multi-tasking dialogues have been built (e.g.,
Traum and Rickel 2002; Kun, Miller, and Lenharth 2004; Lemon and Gruenstein 2004;
Larsson 2003). However, it is unclear that the mechanisms of managing multiple ver-
bal tasks in these systems resemble human conventions or do the best to help users
with task switching. For complex domains, the user might be confused about which
task the interface is talking about, or might be confused about where they left off in
a task.
In order to build a speech interface that supports multi-tasking dialogue, we need to
determine a set of conventions that the user and interface can follow in task switching.
We propose to start with conventions that are actually used in human?human speech
conversations, which are natural for users to follow and probably efficient in problem-
solving. Once we understand the human conventions, we can try to implement them in
a dialogue manager and run user studies to verify the effectiveness of such conventions
in human?computer dialogue.
In this article we focus on understanding the human conventions of managing
multiple tasks. Multi-tasking dialogues, where multiple independent topics overlap
with each other in time, regularly arise in human?human conversation: For example,
a driver and a passenger in a car might be talking about their summer plans, while
occasionally interjecting road directions or conversation about what music to listen to.
However, little is known about how people manage multi-tasking dialogues. Given the
scenario where a real-time task with a time constraint arises during the course of an
ongoing task, we are specially interested in two switching behaviors: task interruption,
which is to suspend the ongoing task and switch to a waiting real-time task, and task
resumption, which is to return to the interrupted ongoing task after completing a real-
time task.
The first question we ask is how quickly conversants respond to a real-time task.
Intuitively if the real-time task is very urgent (e.g., the driver is about to miss a turn),
the passenger might want to immediately cut off the ongoing conversation, and notify
the driver of the turn. However, if the real-time task is less urgent, for example, the
driver does not like the music and wants the passenger to load another CD, do conver-
sants still immediately interrupt the ongoing conversation? If conversants do vary how
quickly they interrupt, are there any regularities of where conversants switch from the
ongoing task to the real-time task? We hypothesize that, given the choice, conversants
interrupt the ongoing task where the interruption is less disruptive to the ongoing
task.
The second question we ask is how conversants signal task interruptions. Previous
research showed that conversants signal the start of a new topic in single-tasking speech
(monologue and dialogue) with discourse markers and prosodic cues.We thus hypothe-
size that conversants also use these cues to signal task interruptions. We also investigate
whether conversants vary the intensity of the cues, and under what circumstances.
The third question we ask is what conversants do immediately upon resuming the
ongoing task. Switching to a real-time task causes the ongoing task to be temporarily
suspended. On completing the real-time task and returning to the ongoing task, do
conversants simply continue on fromwhere theywere interrupted?We hypothesize that
76
Yang, Heeman, and Kun Multi-Tasking Dialogues
conversants might sometimes perform certain actions to recover from the interruption.
For example, it is imaginable that conversants might ask where were we at for summer
plans, and then review what was discussed before the interruption.
To answer these questions, we collect the MTD corpus, which consists of a set
of human?human dialogues where pairs of conversants have multiple overlapping
verbal tasks to perform. In our research, we keep things relatively simple by having
conversants talk to each other to play two games on computers. The first game, the
ongoing task, is a poker game in which conversants need to assemble a poker hand,
which usually takes a relatively long time to complete. The second game, the real-time
task, is a picture game in which conversants need to find out whether they have a
certain picture on their displays, which can be done in a couple of turns but has a time
constraint. In Section 3, we describe the task setup and corpus collection. In Section 4,
we examine when and where conversants suspend the ongoing task and switch to the
real-time task. In Section 5, we examine how conversants signal task interruptions. In
Section 6, we examine the behavior of context restoration in task resumptions.
In addition to the three questions we have asked, in Section 7, we use machine
learning to automatically recognize task interruptions. Recognizing task interruptions
is an important component in building speech interfaces that support multi-tasking
dialogue. For example, the speech interface can accordingly switch the language model
when it detects that the user has switched to another task, which should improve
speech recognition performance (Iyer and Ostendorf 1999) and utterance understand-
ing, leading to higher user satisfaction (Walker, Passonneau, and Boland 2001). We run
machine learning experiments to determine how well we can automatically recognize
task interruptions and to understand the utility of the features that we found in our
corpus studies. Finally, we conclude the paper in Section 8. This paper includes and
extends Heeman et al (2005), Yang, Heeman, and Kun (2008), and Yang and Heeman
(2009) with more corpus data, more robust statistical analysis, more machine learning
experiments, and more comprehensive discussions.
2. Related Research
2.1 Existing Systems for Multi-Tasking Dialogues
There is some initial research effort in building speech interfaces to support multi-
tasking dialogue. Kun, Miller, and Lenharth (2004) developed a system called Project54,
which allowed a user to interact with multiple devices in a police cruiser using speech.
The architecture of Project54 allowed for handling multiple tasks overlapped in time.
For example, when pulling over a vehicle, an officer could first issue a spoken command
to turn on the lights and siren, then issue spoken commands to initiate a data query,
go back to interacting with the lights and siren (perhaps to change the pattern after
the vehicle has been pulled over), and finally receive the spoken results of the data
query. This example shows that system responses related to different tasks could be
interleaved: The system responded to the data query after the user had already switched
back to interacting with the lights and siren.
Lemon and Gruenstein (2004) also explored multi-tasking in a speech interface.
They built a speech interface for a human operator to direct a robotic helicopter on
executing multiple tasks, such as searching for a car and flying to a tower. The interface
kept an ordered set of active dialogue tasks, and interpreted the user utterance in terms
of the most active task for which the utterance made sense. Conversely, during the
77
Computational Linguistics Volume 37, Number 1
interface?s turn of speaking, it could produce an utterance for any of the dialogue tasks
and thus intermixed utterances from different tasks.
In Kun, Miller, and Lenharth (2004) or Lemon and Gruenstein (2004), the systems
did not explicitly signal tasks switching, either for task interruptions or for task re-
sumptions, but instead relied on semantic interpretation to determine which task an
utterance belonged to. Larsson (2003) built the GoDis system which hard-coded two
types of signals when resuming an interrupted conversation. The first type of signal was
to use the discoursemarker so to implicitly signal a topic resumption. The second type of
signal was to use the phrase returning to the issue of to explicitly resume an interrupted
topic. For example, when searching for the price of an air ticket with GoDis, the user
could suspend the system?s questionwhen do you want to travel by interjecting a question
do I need a visa. The system, after a short dialogue answering the user?s question about
a visa, would resume the ticket booking by returning to the issue of price.
Traum and his colleagues (Rickel et al 2002; Traum and Rickel 2002) developed the
Mission Rehearsal Exercise system in which the user and virtual humans collaborated
on multiple tasks that could interrupt each other. They created a scenario in which a
lieutenant (the user) was sent to a village for an Army peacekeeping task. However,
on his way, he encountered an auto accident in which his platoon?s vehicle crashed
into a civilian vehicle, injuring a local boy. The boy?s mother and an Army medic were
hunched over him, and a sergeant approached the lieutenant to brief him on the situ-
ation. These multiple virtual humans could interrupt or be involved in conversations
with the lieutenant. The authors proposed and partially implemented a multi-level
dialogue manager, with levels for turn-taking, initiative, grounding, topic management,
negotiation, and rhetorical structure. In their view, topic management included where
one topic is started before an old one is completed. They described how topic shifts
in general can be signaled with cue phrases, such as now and anyways, and with non-
verbal cues.
These researchworks show the usefulness of a spoken dialogue system being able to
handle multiple tasks, and promote a thorough examination of multi-tasking dialogue.
In this article we examine the conventions of task switching in human?human dialogue
as the first step towards understanding the practice of managing tasking switching in a
computer dialogue system.
2.2 Insights from Non-Verbal Task Switching
Research in cognitive science suggests that task interruptions and resumptions are
complicated behavior and warrant investigation. There is extensive research on the
disruptiveness of interruptions, in which individuals switch between multiple manual-
visual tasks. For example, Gillie and Broadbent (1989) found that the length (in time)
of an interruption is not an important factor, but that the real-time task?s complexity
and similarity to the ongoing task contribute to the disruptiveness. On the other hand,
in their study of checklists, Linde and Goguen (1987) found that it is not the number
of interruptions but the length of interruptions that affects the disruptiveness. Cutrell,
Czerwinski, and Hovitz (2001) examined the influence of instant messaging on users
performing ongoing computing tasks, and found that interruptions unrelated to the
ongoing task resulted in longer task resumptions. Although these results do not appear
to always converge on the same conclusions, they suggest that task switching can be
disruptive to users.
Researchers have been trying to minimize the disruptive effect of task switching
in human?computer interaction. McFarlane (1999) explored four alternatives for when
78
Yang, Heeman, and Kun Multi-Tasking Dialogues
to suspend the ongoing task and switch to the interruption, namely, immediate, negoti-
ated, mediated, and scheduled, and found mixed results. Renaud (2000) argued for, and
built, a prototype of a visualization tool to help users restore the context of the ongoing
task when returning from an interruption. Hess and Detweiler (1994) and Gopher,
Greenshpan, and Armony (1996) found that the disruptive effects are reduced as people
gain more experience with interruptions. These studies suggest that it is worthwhile to
investigate how a computer dialogue system should manage task switching.
2.3 Insights from Discourse Structure Research
Research in discourse structure also sheds light on task switching. It is important to
understand the conventions that people use to manage discourse structure as these
might also be used for managing multiple tasks. According to Grosz and Sidner (1986),
the structure of a discourse is a combination of linguistic structure, intentional structure,
and attentional state. The linguistic structure is a hierarchical segmentation of the
dialogue. Each segment has a purpose, which is established by the conversant who
initiates the segment. The purposes come together to form the intentional structure.
The attentional state contains the objects, properties, and relations that are most salient
at any point in the dialogue. The attentional state is claimed to work like a stack. When
a new segment is started, a new focus space is created on top of the attentional stack.
When the segment completes, the focus space is popped off.1
Signaling discourse structure in single-tasking speech is about signaling the bound-
ary of related discourse segments that contribute to the achievement of a discourse
purpose. Two types of cues have been identified. The first type is discourse markers
(Grosz and Sidner 1986; Schiffrin 1987; Moser and Moore 1995; Passonneau and Litman
1997; Bangerter and Clark 2003). Discourse markers can be used to signal the start of a
new discourse segment and its relation to other discourse segments. For example, now
might signal moving on to the next topic, andwellmight signal a negative or unexpected
response.
The second type of cue is prosody. In read speech, Grosz and Hirschberg (1992)
studied broadcast news and found that pause length is the most important factor that
indicates a new discourse segment. Ayers (1992) found that pitch range appears to cor-
relate more closely with hierarchical topic structure in read speech than in spontaneous
speech. In spontaneous monologue, Butterworth (1972) found that the beginning of a
discourse segment exhibits slower speaking rate; Swerts (1995) and Passonneau and
Litman (1997) found that pause length correlates with discourse segment boundaries;
Hirschberg and Nakatani (1996) found that the beginning of a discourse segment corre-
lates with higher pitch. In human?human dialogue, similar behavior has been observed:
The pitch value tends to be higher for starting a new discourse segment (Nakajima and
Allen 1993). In human?computer dialogue, Swerts and Ostendorf (1995) found that the
first utterance of a discourse segment correlates with slower speaking rate and longer
preceding pause. Thus, we are interested in whether discourse markers and prosodic
cues are also used in signaling task interruptions in multi-tasking dialogue.
1 Grosz and Sidner (1986) also briefly talked about interruptions. In their discourse structure theory,
interruptions are modeled as special discourse segments. When a task interruption happens, an
attentional state is created for the real-time task and pushed on top of the discourse stack. There is
an impenetrable separation between the attentional state of the real-time task and the interrupted
ongoing task, so that the real-time task cannot access the ongoing task. When the real-time task is
completed, its attentional state is popped off and the ongoing task becomes salient.
79
Computational Linguistics Volume 37, Number 1
3. The MTD Corpus
In order to better understand multi-tasking human?human dialogue, we collected the
MTD corpus, in which pairs of players perform overlapping verbal tasks.
3.1 Design of Tasks
For the MTD corpus, we decided to have players complete two types of tasks via
conversation: an ongoing task and real-time tasks. The ongoing task needs to build up
significant context that players have to keep in mind. On task resumption, this context
is needed to finish the task, and so might need to be re-established. The task should
also encourage both players to equally participate as we believe that mixed-initiative
will be the conversational mode in future speech interfaces. The real-time task can be
kept simple: It does not build up much context and can be finished in a couple turns.
However, we vary the urgency of this task.
For the ongoing task, a pair of players collaborate to assemble as many poker
hands as possible, where a poker hand consists of a full house, flush, straight, or four
of a kind. Each player initially has three cards in hand, which the other cannot see.
Players take turns drawing an extra card and then discarding one, until they find a
valid poker hand, for which they earn 50 points; they then start over to form another
poker hand. To discourage players from rifling through the cards to look for a specific
one without talking, one point is deducted for each picked-up card, and ten points for a
missed or incorrect poker hand. To complete this game, players converse to share card
information, and explore and establish strategies based on the combined cards in their
hands (Toh, Yang, and Heeman 2006). The poker game is played on computers. The
game display, which each player sees, is shown in Figure 1. The player with four cards
can click on a card to discard it. The card disappears from the screen, and a new card
is automatically dealt to the other player. Once they find a poker hand the player with
four cards clicks the Done Poker Hand button to start a new game.
The real-time task is a picture game. From time to time, the computer prompts one
of the players to determine whether the other has a certain picture on the bottom of the
display. The picture task has a time constraint of 10, 25, or 40 seconds, which is (pseudo)
randomly determined. Two solid bars above and below the player?s cards flash when
there is a pending picture game. This should alert the player to a pending picture game
without taking the attention away from the poker game. The color of the flashing bars
depends on howmuch time remains: green for 26?40 seconds, yellow for 11?25 seconds,
and red for 0?10 seconds. The player can see the exact amount of time left in the heading
of the picture game. In Figure 1, the player needs to find out whether the other player
has a blue circle, with 6 seconds left. The players get 5 points if the correct answer is
given in time. The overall goal of the players is to earn as many points as possible from
the two tasks.
3.2 Corpus Collection
We recruited six pairs of players, who each received US $10 for completing the data
collection. All players were native American English speakers, and had a bachelor?s
degree or higher in computer science or electrical engineering. None of the players were
in our research lab, and there was no evidence that any player knew about our research
program before they participated.
80
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 1
The game display for players.
The data collection for each pair of players lasted about one hour. Players were
separated so that they could not see each other and they talked to each other through
headsets. After a short orientation, the players played the poker game for about 5
minutes to become familiar with the rules. They then had a practice conversation with
both the poker game and the picture game for about 15 minutes, so that they got used to
managing both tasks. Finally, they had two more conversations, each lasting for about
15 minutes. In each conversation, nine picture games, three for each urgency level, were
prompted for each player. In this research, we analyze the last two conversations, but
not the practice one. Thus we have a total of about 180 minutes of conversation from
the six pairs of players.
For each dialogue, we recorded both channels of speech (each in an audio file) and
created a log file. The log file contains all the events of the computer dealer and the
GUI actions of the two players for each task with time-stamps. For the poker game, it
contains information of when a card is dealt or discarded, and information of when a
poker hand is achieved or missed; for the real-time task, it contains each question, the
time it is generated, the answer, and the time it is answered.
A post-experiment survey was conducted in which players were given the follow-
ing questions: (1) Did you ever play poker before you participated in this experiment?
(2) Did you always immediately notice the flashing that signaled a new picture task?
81
Computational Linguistics Volume 37, Number 1
Table 1
Summary statistics of game, card, and picture segments for each pair of players.
R1 R2 R3 R4 R5 R6 Total
Game segments 7 13 39 35 11 15 120
Card segments 40 118 227 225 82 89 781
Picture segments 30 36 36 35 35 36 208
(3) Did you ever purposefully ignore a picture task? (4) How did you make use of the
different urgency levels (40, 25, or 10 seconds)? (5) How did the picture task affect the
poker game? (6) Do you have any other comments? All players had at least some poker
experience. All players reported that they always noticed the bars immediately when
they started to flash, and that they never ignored a real-time task on purpose. Some
players also mentioned that they enjoyed the games.
3.3 Dialogue Segmentation
We segmented each dialogue into utterances using consensus annotations (see Yang and
Heeman [2010] for more details), following the guidelines of the Trains corpus (Heeman
and Allen 1995). We also annotated each utterance as to whether or not it is a trivial
utterance. We define trivial utterances as those that are just a stall (such as uh and um)
or a simple acknowledgement (such as okay, uh-huh, and alright). According to Strayer,
Heeman, and Yang (2003), annotators reached high inter-coder agreement on a similar
annotation scheme.2 There are in total about 4,300 non-trivial utterances in playing the
poker game.
The ongoing task can be naturally divided into individual poker games, in which
the players successfully complete a poker hand. Each poker game can be further divided
into a sequence of card segments, in which players discuss which card to discard, or
players identify a poker hand. In total, there are 120 game segments and 781 card
segments in the corpus. We also group the utterances involved in each picture game
into a segment. Of the 216 prompted picture games, 8 were never started, although
players reported that they never ignored a picture game. Hence we have 208 picture
games. Table 1 shows the statistics for each pair of players (R1, R2, ..., R6).
Figure 2 shows an excerpt from an MTD dialogue with the segmentations. Here b7
is a game segment in which players get a poker hand of a flush; and b8, b10, b11, b12,
and b14, inside of b7, are card segments. Also embedded in b7 are b9 and b13, each of
which is a segment for a picture game. As can be seen, players switch from the ongoing
poker-playing to a picture game. After the picture game is completed, the conversation
on the poker-playing resumes.
Most of the segments can be automatically derived from the log file. For example,
the time a new hand is dealt is usually the start of a new game segment; the time a
new card is dealt is usually the start of a new card segment. We then manually fixed
any mistakes. For example, a mis-generated segment is removed where a player simply
discarded a card without any discussion; and a segment boundary is moved if an
utterance about the card being discarded, typically an acknowledgment, is said after
the new card is dealt.
2 They reported an inter-annotator agreement of 92%, which corresponded to ? = 0.83.
82
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 2
An excerpt of an MTD dialogue.
The game, card, and picture segments are cohesive units of discourse in which the
conversants attempt to complete a domain task, that of winning the card game, deciding
what card to discard, or identifying a picture. Thus they follow Grosz and Sidner?s
(1986) definition of discourse segments.
3.4 Discourse Context
We define discourse context on the task level. We distinguish three types of discourse
context where a player suspends the poker playing and switches to a pending picture
game: (G) immediately after completing a poker game (at the end of a game), (C)
immediately after discarding a card (at the end of a card discussion), and (E) embedded
in a card discussion, where players are deciding which card to discard. Corresponding
to our dialogue segmentations, an interruption at the end of a game is thus a picture
game segment between two poker game segments; an interruption at the end of a card
is a picture segment between two card segments; and an interruption embedded in a
83
Computational Linguistics Volume 37, Number 1
card discussion is a picture segment embedded in a card segment. As shown in Figure 2,
both b9 and b13 are interruptions at the end of a card discussion.
4. Where to Interrupt
In this section, we examine whether players wait for certain discourse contexts in the
poker playing to interrupt with a picture game.
4.1 Response Delay
During poker playing, if a picture game is prompted, the bars around the cards flash in
different colors depending on the amount of time left. It is up to the player to decide
when to start the picture game (by asking the other player whether there is a certain
picture at the bottom of the display). The players can start the picture game as soon
as they notice it, for example, within one second; or they can delay the picture game,
for example, for 35 seconds, if the time constraint allows. We thus examine the response
delay, defined as the time interval between when a picture game is prompted and when
the player starts it, to understand how soon a player responds to a picture game. We are
particularly interested in how players respond to different urgency levels, i.e., whether
players wait longer when they are given more time.
Figure 3 shows the average response delay for each player for the urgency levels of
10 sec (black), 25 sec (gray), and 40 sec (white), with the actual values displayed in the
columns below. There are certainly individual differences. Player 5A seems to respond
to a real-time task as soon as the bars start flashing, regardless of the urgency levels.
In fact, in 17 out of the 18 picture games, 5A has less than three seconds of response
delay; and the longest response delay is only 3.22 seconds. Player 4B also has interesting
behavior: He waits a significant amount of time under the urgency level of 25 sec, but
promptly responds under the urgency level of 40 sec. However, overall the response
delay under the urgency levels of 40 sec (M = 12.5 sec) or 25 sec (M = 9.7 sec) is much
higher than under the urgency level of 10 sec (M = 2.8 sec). The response delay for 40 sec
is significantly higher than for 10 sec, t(11) = 4.2, p < 0.001; as is for 25 sec versus 10 sec,
t(11) = 6.36, p < 0.001. In fact, for question (4) how did you make use of the different urgency
levels (40, 25, or 10 seconds) in the post-experiment survey, all players but 5A answered
that they waited to initiate the picture game when they were given 25 sec or 40 sec (5A
answered ?not really.?) The 10 sec urgency level requires players to start a picture game
very quickly in order to complete it in time. On the other hand, when given 25 sec or
40 sec, players are in less of a hurry to switch.
4.2 Urgency Level and Discourse Context
The results on response delay show that players do not always start the real-time
picture game as soon as the bars start flashing, especially when players are given 25 sec
or 40 sec. Of course there are individual differences: Some players wait longer, some
players wait less time, and one does not even wait. The more interesting question,
however, is if players do not immediately start the picture game, what is the purpose of
delaying the switch to this real-time task? Are players delaying the switch just because
they feel that they have time and thus do not need to rush, or because they want to
interrupt at a certain point in the ongoing task?
84
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 3
Response delay for different urgency levels.
Figure 4
Distribution of discourse contexts for task interruptions under different urgency levels.
We now examine how the urgency level affects where in the discourse context
players interrupt the ongoing task and switch to the real-time task. Because we do not
find a statistically significant difference of response delay under the urgency levels of
25 sec and 40 sec, t(11) = 1.51, p = 0.16, we combine these two urgency levels in this
analysis.3
Figure 4 shows the distribution of the discourse contexts of task interruptions for the
urgency levels. Overall the percentage of embedded interruptions for the 10 sec urgency
level (M = 76%) is significantly higher than for 25/40 sec (M = 47%), t(11) = 4.46, p <
0.001. In fact, all players except 4A have a higher percentage of embedded interruptions
for 10 sec than for 25/40 sec. The percentage of interruptions at the end of a game for
10 sec (M = 3%) is significantly lower than for 25/40 sec (M = 20%), t(11) = 4.16, p <
0.001. In fact, all players have a higher or equal percentage of interruptions at the end
of a game for 25/40 sec than for 10 sec. These results suggest an answer to our question
about why players delay switching to the ongoing task. When players are given more
time, that is, when the picture game is less urgent, players often utilize the additional
3 In fact, we find that under the urgency levels of 25 sec and 40 sec, players behave similarly in terms of
the discourse context of task interruptions. The reason for the lack of difference might be that it takes on
average 90 seconds to complete a poker hand and 14 seconds to complete a card segments. Hence, there
is little to be gained from separately reasoning about the 25 sec versus 40 sec urgency levels. In hindsight,
we should have used a longer time for the lowest urgency level.
85
Computational Linguistics Volume 37, Number 1
time to delay the switch to the real-time task such that this switch would happen at the
end of a game or a card rather than in the middle of a card discussion.
4.3 Response Delay and Discourse Context
In Section 4.2, we find that players tend to interrupt more often at the end of a card or a
poker game when they are given more time. However, players do not necessarily wait
for more time when the picture game is less urgent. For example, player 5A seems to
always start a picture game as soon as the bars start flashing, regardless of how urgent
the picture game is. To better understand the rationale of delaying a prompted picture
game, we next examine the correlation between response delay and the discourse
context where the switch to the real-time task occurs.
We assume that if the response delay is shorter than some amount of time, say t1,
players intend to start the picture game as soon as possible; we also assume that if the
response delay is longer than some other time, say t2, players intend to delay the picture
game. For the window between t1 and t2, it is unclear as to what players are doing due
to individual differences. In this article, we set t1 to 3 seconds and t2 to 6 seconds. From
listening to the dialogues, it seems to us that when players interrupt within 3 seconds,
they intend to do so right away, and when players wait at least 6 seconds, they do not.
These two time points are also consistent with human performance in task switching
(Meiran, Chorev, and Sapir 2000). This gives us 77 cases of interruptions with a response
delay of less than 3 seconds, 88 cases greater than 6 seconds, and 43 cases in between.
We have also examined other time thresholds, and find similar results.
Figure 5 shows the distribution of the discourse contexts of task interruptions
regarding the response delay. Because 5A always starts a picture game as soon as the
bars start flashing, we do not have data for when he waits for more than 6 seconds. We
thus exclude 5A from this analysis. The percentage of embedded interruptions for less
than 3 sec response delay (M = 71%) is significantly higher than for more than 6 sec
response delay (M = 41%), t(10) = 3.54, p = 0.002. The percentage of interruptions at the
end of a game for less than 3 sec response delay (M = 5%) is significantly lower than
for more than 6 sec response delay (M = 23%), t(11) = 3.49, p = 0.003. Compared with
immediately starting a picture game, if players wait for a certain amount of time, they
are more likely to suspend the ongoing task at the end of a poker game or a card than
to suspend the ongoing task in the middle of a card discussion.
Figure 5
Distribution of discourse contexts for task interruptions under different response delays.
86
Yang, Heeman, and Kun Multi-Tasking Dialogues
4.4 Discussion
In our research, we define task-level discourse contexts, and investigate the discourse
contexts where task interruptions of different urgency occur. We first examine the
response delay, and find that players do not always interrupt the poker playing as soon
as a picture game starts flashing, but instead they tend to wait longer for less urgent
picture games. We then examine the correlation between discourse context and urgency
level, and find that when given more time players tend to switch more often to a picture
game at the end of a (poker) game or a card. We finally examine the correlation between
discourse context and response delay, and find that if players wait for at least a certain
amount of time, they tend to switch more often to a picture game at the end of a (poker)
game or a card. These results suggest that players prefer to interrupt at the end of a
game or a card rather than interrupt in the middle of a card discussion. In fact, after the
practice session, player pair R3 explicitly decided that they should try to delay a picture
game until the end of a poker game. In other work, Shyrokov, Kun, and Heeman (2007)
examined the correlation between task interruption and conversational-level discourse
context. Similarly, they found that conversants try to avoid interrupting adjacency
pairs.
Discourse context is probably not the only factor that determines when players
switch tasks. We observed that sometimes players had time but still chose to interrupt
inside a card discussion; or that sometimes players waited past a card segment and then
interrupted inside the new card discussion. One guess is that at certain points in a card
discussion, players have less cognitive load and so switch tasks. Another guess is that
at certain points during poker playing, players get frustrated and decide to switch to a
pending picture game. However, these analyses are beyond the scope of this article.
5. Signaling Task Interruption
In this section, we examine how players signal that they are switching from the ongoing
task to a real-time task. In Section 2.3, we discussed how people use certain cues, such as
discourse markers and prosody, to signal discourse structure in single-tasking speech.
This suggests that peoplemight also signal task interruptions inmulti-tasking dialogues
and might even use similar cues.
5.1 Discourse Markers
First, we examine whether discourse markers co-occur with task interruptions. For this
exploratory study, we treat any word that can serve as a discourse maker and that
precedes a task interruption as a discourse marker, even though their roles in dialogue
are sometimes ambiguous, such as and, now, and okay (Gravano et al 2007). We also
include the fillers uh and um, which were shown to sometimes have a discourse function
(Swerts 1998).
Of the total 208 task interruptions, 76 are initiated with a discourse marker, which
accounts for 36.5%.We list these discoursemarkers in Table 2 grouped by their discourse
function. For their use in task interruptions, column 2 shows the number of occurrences
of each group and column 3 shows the number of players who use them. The first
group consists of oh and wait, which are usually used to signal a sudden or urgent
event (Heritage 1984; Schiffrin 1987; Byron and Heeman 1997). This group has the
most frequently uttered discourse markers in task interruption with 27 occurrences, and
seven players utter them at least once. The second group consists of the fillers uh and
87
Computational Linguistics Volume 37, Number 1
Table 2
List of discourse markers used in task interruptions.
Discourse Markers Total Occurrences Number of Players
oh wait 27 7
um uh 23 10
now okay alright 13 8
and 10 5
OTHERS (so but hey) 3 3
um. This group is uttered by the most players with 23 occurrences. The third group
consists of now, okay, and alright, which can signal the end of the current topic and
moving on to the next (Hirschberg and Litman 1987; Gravano et al 2007). This group
has 13 occurrences by eight players. The word and is uttered 10 times by five players.
Finally there is one occurrence of so, one of but, and one of hey. Interestingly, there are
also two cases of calling the name of the other player, such as Gary do you have a blue
triangle?
We next examine the discourse markers oh and wait in more depth. We choose them
because this group co-occurs most frequently with task interruptions, and because task
interruptions involve starting a new and urgent task, which fits their discourse function.
Verifying whether oh and wait are being used as discourse markers is straightforward.
We manually verified that all 27 instances of oh and wait that initiated a picture game
are discourse markers, and we also identified all usages of oh and wait in poker playing
that are discourse markers. For each player, we calculated the rate of task interruptions
initiated with an oh or wait, and compared it with two baselines: (1) the rate of non-
trivial utterances in poker playing that are initiated with an oh or wait, and (2) the rate
of card segments that are initiated with an oh or wait. The rate of task interruptions
initiatedwith an oh orwait (M = 12.7%) is significantly higher than the rate of utterances
initiatedwith an oh orwait (M = 5.7%), t(11) = 1.80, p = 0.05. It is also higher than the rate
of card segments initiated with an oh orwait (M = 7.1%), which is marginally significant
t(11) = 1.66, p = 0.06. These results suggest that the discourse markers oh and wait are
sometimes used in signaling task interruptions.
5.2 Prosody
To understand the prosodic cues in initiating a topic, traditionally researchers compared
the prosody of the first utterance in each topic with other utterances (e.g., Nakajima and
Allen 1993; Hirschberg and Nakatani 1996). For example, they calculated the average
pitch in the utterance or the first part of the utterance that initiates a topic and found
that it is higher than the other utterances in the topic. This approach encounters two
problems here. First, the words in an utterance might affect the prosody. For example,
the duration and energy of bat are usually larger than bit. Thus a large amount of data are
required to balance out these differences. Second, in the MTD corpus, players typically
switch to a picture game by using a yes?no question, such as do you have a blue circle,
whereas most non-trivial utterances in the ongoing task are statements or proposals. As
questions have very different prosody than statements or proposals, a direct comparison
is further biased.
Examination of the MTD corpus finds that 82% (170/208) of the picture games are
initiated by do you have ... with optional discourse markers at the beginning. While in
88
Yang, Heeman, and Kun Multi-Tasking Dialogues
Figure 6
Average pitch of do you have for task interruptions and poker playing.
the poker game, players use do you have ... 115 times to ask whether the other has certain
cards, such as do you have a queen? This abundance of utterances with identical initial-
wording and speech-act-type inspired us to compare the prosody of the phrase do you
have in switching to a picture game and during poker-playing.4 This avoids comparing
prosody of different words or of different types of utterances.
We measure pitch, energy (local root mean squared measurement), and duration
of each case of do you have. We aggregate on each individual player and calculate the
average values. Figure 6 shows the average pitch of the phrase do you have in task inter-
ruption (INT) and poker-playing (PKR) of each player, with the actual values displayed
in the columns below. For task interruption, players? average pitch is significantly
higher than poker-playing, t(11) = 4.82, p < 0.001. In fact, for each of the 12 players,
the average pitch of do you have in task interruption is higher than in poker-playing.
These results show a strong correlation between task interruption and higher pitch.
We also examine energy and duration (speaking rate) for the phrase do you have in
task interruption and poker-playing. However, we do not find a statistically significant
difference in energy, t(11) = 1.53, p = 0.16, or in duration t(11) = 1.67, p = 0.12.
5.3 Intensity of Cues
To better understand how pitch is used in signaling task interruptions, we next examine
whether it correlates with the discourse context of interruptions, namely, interrupting
at the end of a game, at the end of a card discussion, or embedded in a card discussion.
Because there are relatively fewer data for interrupting at the end of a game, we combine
interruptions at the end of a game and at the end of a card discussion (G/C).
Figure 7 shows the average pitch of do you have when switching to a picture game
embedded in a card discussion, at the end of a game or card discussion, and during
poker-playing (i.e., no task switching involved), with the actual values displayed in the
columns below. The difference between these three conditions is statistically significant,
F(2, 11) = 21.60, p < 0.001. Interruptions embedded in a card discussion has a signifi-
cantly higher pitch than at the end of a game or card discussion, t(11) = 5.74, p < 0.001,
4 It would have been interesting to compare the prosody of utterances that initiate a picture game and
those that initiate a card segment. However, we do not have enough utterances that initiate a card
segment that begin with do you have.
89
Computational Linguistics Volume 37, Number 1
Figure 7
Average pitch of do you have for different discourse contexts.
which in turn has a significantly higher pitch than during poker-playing, t(11) = 3.56,
p = 0.002. These results suggest a statistical correlation between discourse context of
task interruption and intensity of cues.5
5.4 Discussion
We find that discourse markers are sometimes used to mark task interruptions, but for
less than 40%. For the discourse markers oh and wait, we find a statistical correlation
between their use and task interruptions. This result should not be surprising as task
interruptions involve a sudden change of the conversation topic, and previous research
found that conversants use oh to mark a change of state in orientation or awareness.
wait is used to mark a discontinuity in the ongoing topic, which is also required by
task switching. Thus, it seems natural for people to use these discourse markers to
signal switching to a real-time task. The use of the other discourse markers is less clear,
but we have some speculations about their use with task interruptions. The discourse
markers now, okay, and alright tend to start a new topic in single-tasking speech, which
is consistent with initiating a task interruption. The fillers um and uh might be used to
hold the floor giving the player who initiates the picture game extra time to mentally
switch tasks; or they might be used to help mark the switch itself, similar to how they
sometimes mark topic shifts (Swerts 1998). Calling the name of the other player or
saying heymight be used to alert the other player of the task switching.
We also find that players signal task interruptions with prosodic cues. Pitch turns
out to be the most prominent feature. Not only do we find a strong correlation between
higher pitch and task interruption, but we also find a correlation between pitch and
the discourse context of the interruption. Switching embedded in a card segment has a
higher pitch than switching at the end of a card segment or a game, which in turn has
a higher pitch than non-switching (poker-playing). We speculate that pitch, as well as
discourse markers and calling the name of the other player, is used to disengage the
hearer from the ongoing task, signaling an unexpected event (see Section 8.1 for more
discussion).
5 We are also interested in whether players alter their use of discourse markers depending on the place of
interruption. However, perhaps due to a lack of data, we do not find a statistical difference.
90
Yang, Heeman, and Kun Multi-Tasking Dialogues
On the other hand, we do not find a statistically significant correlation between
energy and task interruption, or between speaking rate and task interruption. It would
be interesting to understand why pitch is used yet not other prosodic cues. In another
study, in which we examined initiative conflicts, where both conversants speak at the
same time trying to steer the conversation in different directions, we found that energy
is the dominant device for resolving who wins the conflict (Yang and Heeman 2010).
Probably conversants use different prosodic devices, such as pitch, energy, and speaking
rate, for different conversational functions. Further research is needed to explore this
hypothesis.
Finally, it is also interesting to investigate whether players signal the urgency of the
real-time task. In our task setup, besides the urgency level, which is the time (10 sec,
25 sec, or 40 sec) initially given to the players to complete a picture game, a more
important factor that defines urgency is the remaining time, which is the time left
to complete a picture game when players switch to it. Intuitively, when players start
a picture game, the more time that is remaining to finish the task, the less hurried
they need to be. However, we were not able to find a statistical correlation between
urgency level and pitch, or between remaining time and pitch. Norwas there a statistical
correlation with volume or speaking rate. Our explanation is that our task setup might
not be complicated enough. It only takes a couple utterances to finish a picture game,
and players were able to start the picture task far enough ahead that remaining time
was rarely a factor.
6. Context Restoration
On completing an interrupting picture game, players resume poker playing. Due to
our task setup, players tend to mutually know when a picture game ends. Thus we do
not examine how players signal task resumption, but instead we focus on how players
restore the context of the ongoing task, that is, how players re-establish the conversation
on poker playing after being interrupted by a picture game. We use the same distinction
of discourse contexts as we use in examining task interruptions: (1) restoration in the
middle of a card discussion, which corresponds to the players interrupting embedded
in a card discussion; (2) restoration at the beginning of a card, which corresponds to the
players interrupting after a card discussion, and then resuming to poker playing with
one of the players having a new card; and (3) restoration at the beginning of a game,
which corresponds to the players interrupting at the end of a poker game, and then
resuming to poker playing with the beginning of another poker game.
6.1 Restoration in the Middle of a Card Discussion
We start by investigating context restoration in the middle of a card discussion, because
these have the most context. We explored the corpus to look for signs of context restora-
tion behavior after an embedded interruption, by examining informational redundancy
(Walker 1996) of the first non-trivial utterance after completing a picture game.
Probably due to the simplicity of the picture game, especially that it can be com-
pleted in a couple turns, we find that after completing an embedded picture game
players usually continue poker playing without a clear indication of context restoration.
As shown in Example (1), B suspends his own question in poker playing and interrupts
with a picture game. After the completion of the picture game, A gives the answer to
B?s original question right away: The dialogue on poker playing continues as if the
interruption never happened.
91
Computational Linguistics Volume 37, Number 1
Example 1 (Continuation)
B: what do you have to make a high straight with?
B: you got a red circle?
A: no
A: I have a ten of diamonds and an ace of clubs
We do, however, find two types of utterances at the beginning of a resumption that
are informationally redundant (Walker 1996), as listed here.
Utterance Restatement: The first non-trivial utterance after the interruption is a re-
statement of the last non-trivial utterance before the interruption. This can be
further divided into three sub-categories: A) self-repetition: the player repeats
(part of) his or her own utterance, as shown in Example (2); B) other-repetition:
the player repeats (part of) the other?s utterance, as shown in Example (3); and C)
clarification: the player asks for a repetition with a clarification question, as shown
in Example (4).
Example 2 (Self-Repetition)
B: I have three clubs right now
B: do you have a yellow square?
A: yes
B: I have three clubs
B: do you have any clubs?
Example 3 (Other-Repetition)
B: I have jack and two queens
B: um do you have a yellow plus sign?
A: yes
A: a jack and two queens
A: I have a ten
Example 4 (Clarification)
A: I have a six of clubs a nine of spades and a four of diamonds
B: okay
B: okay how about a uh red cross
A: no
B: okay
B: four diamonds six something?
A: clubs
Card Review: The player re-communicates what cards are in hand, as shown in Exam-
ple (5). We define card review as utterances that inform of all of the cards in the
player?s hand, and where this information has already been communicated.
Example 5 (Card Review)
A: so I got a ten of spades
B: alright
B: and do you have a red circle?
A: um yes
B: I mean no no a blue circle
A: oh yes
A: and okay I have a queen of spades a ten of I mean a queen
of diamonds a ten of spades a king of clubs and a two of clubs
92
Yang, Heeman, and Kun Multi-Tasking Dialogues
For the 115 embedded interruptions, we find 34 cases of utterance restatement (20
self-repetitions, 4 other-repetitions, and 10 clarifications) and 9 cases of card review.
Figure 8 shows the rate of each category, aggregated on each pair of players (R1-R6).
The rate of utterance restatements, calculated as the number of embedded inter-
ruptions that are followed by an utterance restatement divided by the total number
of embedded interruptions, ranges from 22% to 37% among the player pairs. To make
sense of these numbers, we annotate each non-trivial utterance in the poker games to
mark whether it is a restatement of the immediate previous one within a card segment,
and calculate the baseline as the rate of performing utterance restatement without being
interrupted by a picture game. From Figure 8, we see that for all six pairs of players, the
rate of utterance restatement after an embedded interruption is higher than the baseline,
and it is statistically significant, t(5) = 13.52, p < 0.001. This suggests that utterance
restatement after an embedded interruption is not a random behavior, but it is part of
the resumption to the ongoing task.
We next examine card review, which does not seem to be a common behavior in all
player pairs. The pairs R1, R4, and R6 never performed it in resuming poker playing,
and R2 only performed it once. The pairs with the highest rates are R5 and R3, with
26% (6/23) and 17% (2/12), respectively. Interestingly, these two pairs have the lowest
rates of performing utterance restatement, with 22% and 27%, respectively. This might
suggest that card review might be complementary to utterance restatement for context
restoration, although more data are needed to validate this hypothesis.
6.2 Restoration at the Beginning of a Card Segment
For restoration at the beginning of a card segment, we find that players mostly just
continue poker playing without a clear indication of being affected by the interruption,
as illustrated by card segments b10 and b14 in Figure 2. Some players might perform an
act similar to card review, in that they communicate all of the cards in his or her hand.
However, the version here differs as it includes the new card just picked up, which has
not been communicated before. Thus we refer to this act as card review + new card.
Table 3 shows the rate of performing card review + new card after an interruption for
each pair, respectively. The baseline is the rate of performing this action at the beginning
Figure 8
Restoration in the middle of a card discussion.
93
Computational Linguistics Volume 37, Number 1
Table 3
Restoration at the beginning of a card segment.
R1 R2 R3 R4 R5 R6
Card review + new card 0% 19% 9% 8% 0% 42%
(0/2) (3/16) (1/11) (1/13) (0/9) (5/12)
Baseline 0% 6% 1% 3% 0% 5%
(0/31) (5/89) (2/177) (5/177) (0/62) (3/62)
of a card segment (excluding the first card segment of a poker game) not following an
interruption of a picture game. Player pairs R1 and R5 never performed this action at
all. R3 and R4 performed this action only once after interruptions at the end of a card
discussion and have a very low overall rate of performing this action during poker
playing. However, for player pairs that have a high overall rate of using this action (R2
and R6), they have an even higher rate of using this action after an interruption. For R6
in particular, the rate of performing this action after an interruption at the end of a card
segment is significantly higher than the baseline, ?2(1) = 6.81, p = 0.01. This suggests
that if players use card review + new card in conversation, they tend to use it more often
after an interruption at the end of a card segment probably for context restoration.
6.3 Restoration at the Beginning of a Game
For interruptions at the beginning of a poker game, we do not find any behavior
associated with context restoration. This is not surprising because there is really no
context that needs to be carried over to the next game.
6.4 Discussion
In this section, we examine the behavior of context restoration when players complete
an interrupting picture game and resume poker playing. Probably due to the simplicity
of the picture game, we find that players mostly just have a smooth continuation as if
the interruption did not happen. However, we do find that players sometimes make
two types of context restorations?utterance restatements and card reviews?and we
find that players have a higher rate of performing these when returning to the ongoing
task.
Card review seems to be refreshing the critical information needed to complete a
task, while utterance restatement is refreshing the last utterance. Both types of restora-
tion behavior are similar to the informationally redundant units that Walker (1996)
studied. In Walker?s work, she posited a limited memory model in which information
will eventually fade away. This might be the explanation here as well. On resuming to
a task that was discussed several utterances ago, the conversant might feel that some
of the critical information might have been forgotten, and so might use card review to
refresh the information. Conversely, the conversant might feel that just the last utterance
needs to be refreshed. Depending onwhether it is the same conversant who resumes the
ongoing task and who says the last utterance before the interruption, it takes the form
of a self-repetition, other-repetition, or request-repetition if clarification is needed. This
explanation for card review and utterance restatement is consistent with the results of
our post-experiment survey, in which some players reported that they had difficulties
94
Yang, Heeman, and Kun Multi-Tasking Dialogues
remembering the context of poker playing when they were interrupted by a picture
game.
In a more complex domain, conversants will probably perform context restoration
more frequently when returning to an interrupted task (Gillie and Broadbent 1989;
Villing 2010), and might use even higher-level summarization beyond utterance restate-
ment and information review, such as reviewing the agreements or decisions that have
been made so far in the conversation.
7. Recognizing Task Interruption: A Machine Learning Approach
Recognizing task switching is important for a speech interface; for example, the speech
interface can accordingly switch the language model when it detects that the user has
switched to another task. In this section, we describe twomachine learning experiments
of recognizing task interruptions using prosody, discourse context, and discourse mark-
ers. The purpose of the first experiment is to understand how these features contribute
to the automatic identification of task interruptions; here, we only include utterances
that start with do you have for better extracting prosodic features. The purpose of the
second experiment is to investigate how well interruptions can be identified without
using lexical features, as could be used in an actual system.
7.1 Recognizing Task Interruptions on Do You Have Utterances
In the previous sections, we examined players? behavior of task switching in the MTD
corpus. We found that players favor certain discourse contexts in the ongoing task for
task interruptions, and that they signal task interruptions with prosodic cues and some-
times with certain discourse markers (oh and wait). We thus conduct a machine learning
experiment to understand how these features contribute to the automatic identification
of task interruptions. In this experiment, we focus on the 285 cases of do you have, 170
for task interruption and 115 for poker playing. As we argued in Section 5.2, this allows
us to better extract and understand prosodic features of task interruptions.
We extract the following features: 1) discourse context: whether the utterance before
do you have is the end of a poker game, the end of a card segment, or in the middle of a
card segment; 2) oh/wait: whether the discourse marker oh/wait precedes do you have;
3) normalized pitch: the pitch of do you have divided by the average pitch of the speaker
during the dialogue. We refer to these features as the core feature set, which we found
to be correlated with task interruptions (Section 4 and 5). We also include the following
additional features: 4) discourse markers: whether a discourse marker precedes do you
have; 5) normalized energy: the energy of do you have divided by the average energy of
the speaker during the dialogue; and 6) duration: the duration of do you have.
We use a decision tree classifier (C4.5) to discriminate task interruption from poker
playing (Quinlan 1986). C4.5 builds a decision tree by using a top?down, greedy pro-
cedure to (locally) optimize mutual information, and prunes the tree with a confidence
level (of 25%). We use C4.5 because its output is interpretable and we have found its
performance comparable to other discriminative classifiers for this task.
We use three re-sampling methods in training and testing the decision tree learn-
ing, which we refer to as general-leave-one-out, speaker-leave-one-out, and leave-one-
speaker-out. In the general leave-one-out method, each data point is tested with the
decision tree trained on all other data points. This approach allows decision trees to
be built with as much training data as possible, which in our case is 284 data points.
95
Computational Linguistics Volume 37, Number 1
Table 4
Performance for general-leave-one-out.
Accuracy Recall Precision F
Baseline 59.6% 100.0% 59.6% 74.7%
Core features 81.4% 89.4% 81.3% 85.2%
Core + discourse markers 80.7% 88.2% 81.1% 84.5%
Core + energy + duration 80.7% 85.3% 82.9% 84.6%
All features 80.4% 84.7% 82.8% 83.7%
In the speaker-leave-one-out method, each data point is tested with the decision tree
trained on the other data points of the same player. This approach is a speaker-specific
model that evaluates the performance of training a decision tree and testing on the same
speaker. In the leave-one-speaker-out method, each player?s data are tested with the
decision tree trained on the other 11 players. This approach is a speaker-independent
model that evaluates the performance of a learned decision tree on a new speaker.
Table 4 shows the results with the general-leave-one-out method. The decision tree
learning with the core feature set obtains an accuracy of 81.4% in recognizing whether
a do you have initiates a task interruption or belongs to poker playing; and the recall,
precision, and F-score for task interruption are 89.4%, 81.3%, and 85.2%, respectively.
For comparison, we use a naive baseline that assumes that all cases of do you have are
task interruptions, which has an accuracy of 59.6%. Thus we achieve 54.0% relative error
reduction in comparison to the baseline. These results show that our machine learning
approach substantially improves the recognition of task interruptions.
Also from Table 4 we see that there is no improvement by adding more features,
namely, discourse markers, energy and duration, or all of them. This suggests that
these features are not adding more information to this discrimination task, which is
not surprising as we did not find them strongly correlated with task interruption in our
corpus study.
Table 5 shows the results for each player with the general-leave-one-out, the
speaker-leave-one-out, and the leave-one-speaker-out, using the core feature set.
Table 5
Accuracy for the three re-sampling methods.
Player General-leave-one-out Speaker-leave-one-out Leave-one-speaker-out
1A 75.0% 66.7% 75.0%
1B 84.6% 69.2% 73.1%
2A 77.8% 74.1% 77.8%
2B 100.0% 90.0% 95.0%
3A 88.0% 88.0% 88.0%
3B 76.7% 76.7% 72.6%
4A 94.4% 94.4% 94.4%
4B 64.7% 64.7% 64.7%
5A 73.9% 69.6% 73.9%
5B 92.9% 71.4% 92.9%
6A 89.5% 84.2% 78.9%
6B 63.6% 90.9% 63.6%
Mean 81.8% 78.3% 79.2%
96
Yang, Heeman, and Kun Multi-Tasking Dialogues
Overall, all the three reach an accuracy of about 80%, which is much higher than the
baseline performance. The performance with the leave-one-speaker-out (M = 79.2%),
which is a speaker-independent model, is particulary encouraging, because in building
a speech interface, it is not always possible to collect speaker-specific data. On the
other hand, we see that the performance with the speaker-leave-one-out (M = 78.3%)
is slightly lower than the leave-one-speaker-out (M = 79.2%). Although this could
be interpreted as that interruption recognition is a speaker-independent task, we
think that a more viable explanation is that for some players, we do not have enough
data to build speaker-specific decision trees. The general-leave-one-out (M = 81.8%),
which uses the most data for training, out-performs the leave-one-speaker-out and
the speaker-leave-one-out. In fact, the general-leave-one-out can also be viewed as a
naive speaker-adaptive model by simply combining speaker-independent data and
speaker-specific data together for training. We speculate that more improvement can be
achieved by interpolating a speaker-independent model with a speaker-specific model,
which we leave for future work.
Finally, we examine the structure of the decision trees learned. Here, we build a
single tree from all 285 cases of do you have with the core feature set, shown in Figure 9.
In the decision tree, the first query is about pitch. If pitch is low it is for poker playing,
otherwise it queries about oh/wait. If the utterance starts with a oh or wait, it is for task
interruptions, otherwise it queries about discourse context. If the discourse context is at
the end of a game or a card discussion, it is for task interruption, otherwise it queries
pitch again. If pitch is lower than a threshold it is for poker playing, otherwise it is for
task interruptions. The structure of the learned tree and its performance confirm that
discourse context, the discourse markers oh and wait, and normalized pitch are useful
features for recognizing task interruptions.
Figure 9
The learned decision tree.
97
Computational Linguistics Volume 37, Number 1
7.2 Recognizing Task Interruptions on All Utterances
The previous experiment helped us determine which features are useful for recognizing
task interruptions. However, the experiment was based only on utterances that start
with do you have, yet not all task interruptions are initiated with do you have. We
thus conduct a further machine learning experiment on recognizing task interruptions
involving any utterances. We extend our feature set to help make up for not limiting
ourselves to do you have utterances. We purposely do not use any lexical features of
the current utterance so that our approach can be applied before speech recognition is
performed.
We extract the following features for all non-trivial utterances: 1) discourse context:
whether the previous utterance is the end of a poker game, the end of a card segment,
or in the middle of a card segment;6 2) overlap: whether the utterance overlaps with the
previous non-trivial utterances; 3) duration: the length in time of the utterance; 4) nor-
malized pitch: the average normalized pitch of the first 100 msec/200 msec/500 msec
and the whole utterance (four features); 5) normalized energy: the average normalized
energy of the first 100 msec/200 msec/500 msec and the whole utterance (four features);
and 6) pitch range: the pitch range of the first 100 msec/200 msec/500 msec and the
whole utterance (four features). In total we have 15 features.
The data that we have are highly skewed. We have 208 cases of task interruptions
but more than 4,000 non-interrupting utterances. We thus perform down-sampling so
that both classes have the same number of data points. In the first down-sampling,
which we refer to as general down-sampling, we use all 208 cases of task interruptions,
and we randomly select 208 non-interrupting utterances. A concern with the general
down-sampling is that 82% of the task interruptions are do you have questions, and do
you have questions are only about 2.5% of the non-interrupting utterances. It is unclear
whether a classifier trained from such a data set discriminates task interruptions or
discriminates do you have utterances. Thus in the second down-sampling, which we
refer to asDYH down-sampling, we use all 208 cases of task interruptions, and we also
use all 105 cases of non-interrupting do you have utterances, then finally we randomly
select 103 other non-interrupting utterances. The DYH down-sampling, however, still
has imbalanced do you have utterances in the two classes. Thus we further introduce
the Balanced-DYH down-sampling, in which we use all 105 do you have utterances
in the poker playing and 38 other (i.e., non do you have) utterances in task interrup-
tions, and randomly select 105 do you have utterances from task interruptions and
38 other utterances from poker playing. We run the experiments with decision tree
learning (C4.5) (Quinlan 1986) and support vector machine (SVM) (Chang and Lin
2001).
We evaluate the performance using general-leave-one-out. The procedure of
down-sampling and general-leave-one-out is repeated 10 times, and then we calculate
the average performance. Note that in our evaluation, the distribution of task inter-
ruption (which is 50%) is different from the true distribution in the corpus (which is less
than 5%). We adopt some metrics from medical diagnostic tests that do not involve
prior distributions. Sensitivity is defined as TruePositive/(TruePositive+ FalseNegative),
which, in our case, is the recall of task interruptions. It measures the percentage
of task interruptions that the classifier correctly identifies as such. Specificity is
6 Card and game segments could be determined fairly accurately from the mouse clicks even without
the speech.
98
Yang, Heeman, and Kun Multi-Tasking Dialogues
defined as TrueNegative/(TrueNegative+ FalsePositive), which, in our case, is the re-
call of non-interruptions. It measures the percentage of non-interruptions that the
classifier correctly identifies as such. These two metrics can then be combined
using the likelihood ratio, which provides a direct estimate of how much a prediction
will change the odds. The likelihood ratio for a positive result (LR+) is defined as
LR+ = sensitivity/(1? specificity). It tells us how much the odds of a task interruption
increase when the classifier predicts positive (task interruption). The likelihood ratio
for a negative result (LR?) is defined as LR? = specificity/(1? sensitivity). It tells us
howmuch the odds of a task interruption decrease when the classifier predicts negative
(non-interruption).
Table 6 shows the results. If we assume a naive baseline with no knowledge, its
sensitivity and specificity are both 50%, and LR+ and LR? are both 1.0. For all three
down-sampling settings, SVM performs slightly better than C4.5, and both are much
better than the baseline. The result for SVM with general down-sampling shows how
well we can recognize task interruptions for our MTD domain, for which we achieve a
sensitivity of 78.6% and a specificity of 76.9%. For the Balanced-DYH down-sampling,
in which we have the same number of do you have utterances in both the classes, SVM
cannot make use of the features that distinguish do you have from other utterances.
Hence, its result might be more indicative of performance in other domains, where
task interruptions might not be marked by the same introductory words. Even here,
we obtain a sensitivity of 75.3%, a specificity of 75.8%, and 3.11 in LR+ and 3.07 in LR?,
which is more than a 50% relative error reduction over the baseline.
Overall, our results show that non-lexical features are useful for the recognition of
task interruptions. Because the features used in our machine learning experiments do
not require the lexical information of the current utterance, we can make use of the
identification of task interruptions to benefit automatic speech recognition (ASR). For
example, we can build two language models, one for the ongoing task, and one for the
real-time task. For each utterance, we can calculate the likelihood of the utterance being
a task interruption, using the decision tree classifier or the SVM classifier. We can then
use this likelihood to dynamically interpolate the two language models in the speech
decoding. This should be able to improve the accuracy of ASR, which we leave for
future work.
8. Conclusion
In this article we describe a series of empirical studies of human?human multi-tasking
dialogues, where people perform multiple verbal tasks overlapped in time. We first
Table 6
Performance for non-lexical features.
Sensitivity Specificity LR+ LR?
Baseline 50.0% 50.0% 1.0 1.0
C4.5 + general down-sampling 77.5% 75.3% 3.14 3.35
C4.5 + DYH down-sampling 72.9% 73.2% 2.72 2.70
C4.5 + B-DYH down-sampling 69.4% 71.8% 2.46 2.35
SVM + general down-sampling 78.6% 76.9% 3.40 3.59
SVM + DYH down-sampling 78.6% 78.4% 3.64 3.66
SVM + B-DYH down-sampling 75.3% 75.8% 3.11 3.07
99
Computational Linguistics Volume 37, Number 1
examined the discourse context of task interruptions, that is, where conversants sus-
pend the ongoing task and switch to a real-time task. Our analysis shows that people
are more likely to wait until the end of a card or game segment for task switching.
We then examined the cues that people use to signal task interruptions. We find that
task interruptions correlate with certain discourse markers and prosodic variations.
More interestingly, the intensity of pitch depends on the discourse context of the task
interruption. We next conducted an exploratory study on context restoration in task
resumption. We find that when returning to an interrupted task, conversants sometimes
re-synchronize the interrupted ongoing conversation by either restating a previous
utterance or summarizing the critical information. Finally, our machine learning ex-
periments show that discourse context, pitch, and the discourse markers oh and wait
are useful features to reliably recognize task interruptions; and, more importantly, with
non-lexical features one can improve the performance of recognizing task interruptions
with more than a 50% relative error reduction over the baseline.
8.1 Disruptiveness of Task Interruption
In our study on multi-tasking dialogues, we distinguish three types of discourse con-
texts where players suspend the poker player and switch to a picture game. We claim
that these discourse contexts differ in terms of players? engagement andmemory load in
the ongoing task. First, we feel that players are more engaged in the ongoing task during
card discussion. In the middle of a card discussion, players actively share information,
explore different (potential) poker hands, and decide what to discard if no poker hand
is found. Second, we feel that players also have a higher memory load in the middle
of a card discussion. Across poker games, players do not have to remember anything;
across card segments, players need to remember what cards each other has; while inside
of a card discussion, players need to also remember what card is being discussed, and
how far they are into deciding which card to discard.
Engagement can be used to explain the intensity of cues in task interruptions. As we
found in Section 5, when players interrupt in the middle of a card discussion, they use a
higher pitch than in the case when they interrupt at the end of a game or a card, which
is also marked with a higher pitch than non-task-switching (during poker playing).
According to Miyata and Norman (1986), a more intrusive signal is needed to attract
the attention of people heavily engaged in an ongoing task. Sussman, Winkler, and
Schro?ger (2003) found that higher pitch can serve as a more intrusive signal. Thus when
interrupting in the middle of a card discussion, the speaker uses higher pitch probably
because the hearer is more engaged in the ongoing task.
Memory load can explain the context restoration behavior in task resumptions. As
we found in Section 6, after a picture game that is at the end of a game, players smoothly
start a new poker game as if nothing happened; after a picture game that is at the end
of a card segment, players might sometimes use information summary to remind each
other of what cards they have in hand; and after a picture game that is in the middle
of a card segment, players might even repeat or clarify the previous utterance that has
been said before the interruption. These observations are consistent with the memory
load of discourse contexts. If players are interrupted in a discourse context where the
memory load is high, because of the limited working memory, players would need to
spend extra effort to recover the memory after completing the interruptions.
Engagement and memory can also explain our finding on the discourse context
of task interruptions. According to Miyata and Norman (1986), interruptions where
100
Yang, Heeman, and Kun Multi-Tasking Dialogues
people are deeply engaged in the ongoing task, or where people have a high memory
load, should be disruptive. Thus interruptions at the end of a card game are the least
disruptive, with those at the end of a card discussion being more disruptive, and those
embedded inside of a card discussion being the most disruptive. A more disruptive
interruption tends to have a higher cost to the ongoing task. The disruptiveness of inter-
ruptions thus explains players? behavior of delaying the picture game. For task inter-
ruptions, players do not always switch to a real-time task when it is prompted, but
instead they take into account the discourse context of the ongoing task. They strive to
switch to a picture game at the end of a (poker) game or a cardwhen possible. According
to Clark and Wilkes-Gibbs (1986), players would try to minimize their collaborative
effort in dialogue. The reason that players try to avoid interrupting in the middle of a
card discussion probably is because such interruptions have a higher cost to the ongoing
task, i.e. these interruptions are more disruptive. Delaying the switch to the real-time
task is thus used as a tool to reduce the disruptiveness of the switch.
Our studies thus suggest that conversants strive to interrupt at a discourse context
where the cost of interruption is low, but if they interrupt in a more intensive context
they use stronger cues to mark the more disruptive interruption.
8.2 Implication for Speech Interface Design
By understanding people?s conventions in task interruptions and context restoration,
we can implement these conventions into a speech interface to allow natural and
smooth task switching in human-computer dialogue. Based on our findings, we propose
the following principles for building a speech interface that supports multi-tasking
dialogue:
 Minimize the disruptiveness of task switching. Delay task switching till
the user?s engagement and memory load in the ongoing task are low so
that the interruption is less disruptive, while still accomplishing the
interruption task in a timely matter. Minimizing the disruptiveness
reduces the cost of interruptions to the ongoing task.
 Signal task switching. Discourse markers, such as oh and wait, and
prosodic variations, especially high pitch, can be used to signal task
switching. These devices help to disengage the user?s attention from the
ongoing task so that the user is aware of the task switching. Use stronger
cues (e.g., higher pitch) when the task switching is more disruptive
(i.e., when the user is more engaged in the ongoing task).
 Recognize task switching. The speech interface can make use of non-lexical
features, such as contextual information and the user?s pitch, together with
discourse markers if available, to help recognize the user?s initiation of
task interruptions. Recognizing task switching helps the speech interface
to interpret the user?s utterance in the correct context, which should lead
to higher speech recognition accuracy and better language understanding.
 Restore context after an interruption. Utterance restatement and
information summary are two effective devices. Context restoration is
needed, especially after a disruptive interruption where the memory
load in the ongoing task is high, in order to help resolve or prevent
misunderstandings and forgetting.
101
Computational Linguistics Volume 37, Number 1
8.3 Future Work
There are obviously a lot of open questions regarding multi-tasking dialogue that are
not solved in this article. In this research, we only examined a domain where an ongoing
task, rich in context, is interrupted by real-time tasks, which are short and simple in
nature. Psychological research showed that the complexity of the real-time task and its
similarity to the ongoing task play an important role in the disruptiveness of interrup-
tions (Gillie and Broadbent 1989); thus we can vary these factors in future research. First,
we can vary the complexity of the real-time tasks; for example, for some interruptions,
the player needs to find out whether the other player has a combination of pictures, such
as a black square but not a white triangle ( ? ?). This will allow us to examine the
correlation between the length of interruptions and context restoration. Second, we can
use real-time tasks that are less structured, so that people do not mutually know when
it ends. This will allow us to examine whether and how people signal task resumptions.
Third, we can introduce ambiguity between the ongoing task and the real-time task: for
example, to put the card suits (????) into the picture game, where an utterance such
as do you have a heart? can belong to either task. This will allow us to see a wider range
of task switching behavior.
Furthermore, in this research we do not investigate how multi-tasking dialogue
would be affected by a manual-visual task, such as driving. This is an important ques-
tion, because for hands-busy, eyes-busy situations such as driving, speech interfaces
may provide a human?computer interaction modality that interferes the least with the
execution of the manual?visual task (Weng et al 2006; Villing et al 2008). We expect
that the presence of the manual?visual task will even further necessitate a good under-
standing of the natural and efficient human conventions for managing multi-tasking so
as not to adversely affect the manual?visual task.
Finally, it was pointed out that human?computer dialogue is not exactly the same as
human?human dialogue?that is, people might change their behavior when talking to
a computer (Doran et al 2001). It will thus be useful to build an actual speech interface
for multi-tasking dialogue, or perhaps first simulate such a system with Wizard of Oz
experiments, and to examine whether following the principles that we derived from
human?human dialogue does lead to improvements.
Acknowledgments
This work was funded by the National
Science Foundation under grant IIS-0326496.
The authors thank Alex Shyrokov, David
Traum, Elizabeth Shriberg, and members of
CSLU for helpful discussions. The authors
also wish to thank the reviewers for their
constructive comments.
References
Ayers, Gayle M. 1992. Discourse functions
of pitch range in spontaneous and read
speech. Presented at the Linguistic Society
of America Annual Meeting. 9?12 January,
Philadelphia, PA.
Bangerter, Adrian and Herbert H. Clark.
2003. Navigating joint projects with
dialogue. Cognitive Science, 27:195?229.
Butterworth, Brian. 1972. Hesitation and
semantic planning in speech. Journal of
Psycholinguistic Research, 4:75?87.
Byron, Donna K. and P. Heeman. 1997.
Discourse marker use in task-oriented
spoken dialog. In Proceedings of the 5th
EUROSPEECH, pages 2223?2226, Rhodes.
Chang, Chih-Chung and Chih-Jen Lin.
2001. LIBSVM: a library for support
vector machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Clark, Herbert H. and Deanna Wilkes-Gibbs.
1986. Referring as a collaborative process.
Cognitive Science, 22:1?39.
Cutrell, Edward, Mary Czerwinski,
Eric Horvitz. 2001. Notification,
disruption, and memory: Effects of
messaging interruptions on memory
and performance. In Proceedings of
INTERACT, pages 263?269, Tokyo.
102
Yang, Heeman, and Kun Multi-Tasking Dialogues
Doran, Christine, John Aberdeen, Laurie
Damianos, and Lynette Hirschman.
2001. Comparing several aspects of
human?computer and human?human
dialogues. In 2nd SigDial Workshop on
Discourse and Dialogue, pages 1?10,
Aalborg, Denmark.
Gillie, Tony and Donald Broadbent. 1989.
What makes interruptions disruptive? A
study of length, similarity, and complexity.
Psychological Research, 50(4):243?250.
Gopher, Daniel, Yaakov Greenshpan, and
Lilach Armony. 1996. Switching attention
between tasks: Exploration of the
components of executive control and their
development with training. In Proceedings
of the Human Factors and Ergonomics Society
40th Annual Meeting, pages 1060?1064,
Santa Monica, CA.
Gravano, Agustin, Stefan Benus, Julia
Hirschberg, Shira Mitchell, and Illa
Vovsha. 2007. Classification of discourse
functions of affirmative words in spoken
dialogue. In Proceedings of INTERSPEECH,
pages 1613?1616, Antwerp, Belgium.
Grosz, Barbara J. and Julia Hirschberg.
1992. Some intonational characteristics
of discourse structure. In Proceedings
of 2nd International Conference on Spoken
Language Processing, pages 429?432, Banff.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Heeman, Peter A. and James F. Allen. 1995.
The Trains 93 dialogues. Trains Technical
Note 94-2, Department of Computer
Science, University of Rochester.
Heeman, Peter A., Fan Yang, Andrew L.
Kun, and Alexander Shyrokov. 2005.
Conventions in human?human
multithreaded dialogues: A preliminary
study. In Proceedings of Intelligent User
Interface, pages 293?295, San Diego, CA.
Heritage, John. 1984. A change-of-state
token and aspects of its sequential
placement. In J. Maxwell Atkinson and
John Heritage, editors, Structures of Social
Action: Studies in Conversation Analysis.
Cambridge University Press, chapter 13,
pages 299?345.
Hess, Stephen M. and Mark C. Detweiler.
1994. Training to reduce the disruptive
effects of interruptions. In Proceedings of
the Human Factors and Ergonomics Society
38th Annual Meeting, pages 1173?1177,
Nashville, TN.
Hirschberg, Julia and Diane Litman. 1987.
Now let?s talk about now: Identifying cue
phrases intonationally. In Proceedings of the
25th Annual Meeting of the Association for
Computational Linguistics, pages 163?171,
Stanford, California.
Hirschberg, Julia and Christine H. Nakatani.
1996. A prosodic analysis of discourse
segments in direction-giving monologues.
In Proceedings of 34th Annual Meeting of the
Association for Computational Linguistics,
pages 286?293, Santa Cruz, CA.
Iyer, Rukmini M. and Mari Ostendorf. 1999.
Modeling long distance dependence in
language: Topic mixtures versus dynamic
cache models. IEEE Transactions on Speech
and Audio Process, 7(1):30?39.
Kun, Andrew L., W. Thomas Miller, and
William H. Lenharth. 2004. Computers in
police cruisers. IEEE Pervasive Computing,
3(4):34?41.
Larsson, Staffan. 2003. Interactive
communication management in an
issue-based dialogue system. In
Proceedings 7th Workshop on the Semantics
and Pragmatics of Dialogue, pages 75?83,
Saarbru?cken.
Lemon, Oliver and Alexander Gruenstein.
2004. Multithreaded context for
robust conversational interfaces:
Context-sensitive speech recognition and
interpretation of corrective fragments.
ACM Transactions on Computer-Human
Interaction, 11(3):241?267.
Linde, Charlotte and Joseph Goguen. 1987.
Checklist interruption and resumption:
A linguistic study. Technical Report
CR-177460, National Aeronautics and
Space Administration.
McFarlane, Daniel C. 1999. Coordinating
the interruption of people in
human?computer interaction. In
Proceedings of INTERACT, pages 295?303,
Edinburgh, Scotland.
Meiran, Nacshon, Ziv Chorev, and Ayelet
Sapir. 2000. Component processes in
task switching. Cognitive Psychology,
41:211?253.
Miyata, Yoshiro and Donald A. Norman.
1986. Psychological issues in support of
multiple activities. In D. A. Norman and
S. W. Draper, editors, Participant Centered
Design: New Perspectives on Human
Computer Interaction. Lawrence Erlbaum,
Hillsdale, NJ, chapter 13, pages 265?284.
Moser, Megan and Johanna D. Moore. 1995.
Investigating cue selection and placement
in tutorial discourse. In Proceedings of
33rd Annual Meeting of the Association for
Computational Linguistics, pages 130?135,
Cambridge, MA.
103
Computational Linguistics Volume 37, Number 1
Nakajima, Shin?ya and James F. Allen.
1993. A study on prosody and discourse
structure in cooperative dialogues.
TRAINS Technical Note 93-2, University
of Rochester, Rochester, NY.
Passonneau, Rebecca J. and Diane J. Litman.
1997. Discourse segmentation by human
and automated means. Computational
Linguistics, 23(1):103?139.
Quinlan, J. R. 1986. Induction of decision
trees.Machine Learning, 1(1):81?106.
Renaud, Karen. 2000. Expediting rapid
recovery from interruptions by providing
a visualisation of application activity. In
Proceedings of OzCHI, pages 348?355,
Sydney.
Rickel, Jeff, Stacy Marsella, Jonathan Gratch,
Randall Hill, David Traum, and William
Swartout. 2002. Towards a new generation
of virtual humans for interactive
experiences. IEEE Intelligent Systems,
17(4):32?38.
Schiffrin, Deborah. 1987. Discourse Markers.
Cambridge University Press.
Shyrokov, Alexander, Andrew Kun, and
Peter Heeman. 2007. Experiments
modeling of human?human
multi-threaded dialogues in the presence
of a manual?visual task. In Proceedings of
8th SIGdial Workshop on Discourse and
Dialogue, pages 190?193, Antwerp.
Strayer, Susan E., Peter A. Heeman, and
Fan Yang. 2003. Reconciling control and
discourse structure. In J. van Kuppevelt
and R. W. Smith, editors, Current and
New Directions in Discourse and Dialogue.
Kluwer Academic Publishers, Dordrecht,
chapter 14, pages 305?323.
Sussman, E., I. Winkler, and E. Schro?ger.
2003. Top?down control over involuntary
attention switching in the auditory
modality. Psychonomic Bulletin & Review,
10(3):630?637.
Swerts, Marc. 1995. Combining statistical
and phonetic analyses of spontaneous
discourse segmentation. In Proceedings of
the 12th ICPhS, volume 4, pages 208?211,
Stockholm.
Swerts, Marc. 1998. Filled pauses as markers
of discourse structure. Journal of
Pragmatics, 30:485?496.
Swerts, Marc and Mari Ostendorf. 1995.
Discourse prosody in human?machine
interactions. In Proceedings of ESCA
Workshop on Spoken Dialogue Systems:
Theories and Applications, pages 205?208,
Visgo.
Toh, Siew Leng, Fan Yang, and Peter A.
Heeman. 2006. An annotation scheme for
agreement analysis. In Proceedings of 9th
International Conference on Spoken Language
Processing, pages 201?204, Pittsburgh, PA.
Traum, David and Jeff Rickel. 2002.
Embodied agents for multi-party dialogue
in immersive virtual world. In Proceedings
of the First International Joint Conference on
Autonomous Agents and Multi-agent
Systems, pages 766?773, Bologna.
Villing, Jessica. 2010. Now, where was I?
Resumption strategies for an in-vehicle
dialogue system. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, pages 798?805,
Uppsala.
Villing, Jessica, Cecilia Holtelius, Staffan
Larsson, Anders Lindstro?m, Alexander
Seward, and Nina A?berg. 2008.
Interruption, resumption and domain
switching in in-vehicle dialogue. In
Proceedings of the 6th International
Conference on Advances in Natural
Language Processing, pages 488?499,
Berlin.
Walker, Marilyn A. 1996. The effect of
resource limits and task complexity on
collaborative planning in dialogue.
Artificial Intelligence Journal, 85:181?243.
Walker, Marilyn A., Rebecca Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
communicator spoken dialogue systems.
In Proceedings of the Association of
Computational Linguistics, pages 515?522,
Toulouse, France.
Weng, Fuliang, Sebastian Varges, Badri
Raghunathan, Florin Ratiu, Heather
Pon-barry, Brian Lathrop, Qi Zhang,
Harry Bratt, Tobias Scheideck, Kui Xu,
Matthew Purver, and Rohit Mishra.
2006. CHAT: A conversational helper
for automotive tasks. In Proceedings of
9th International Conference on Spoken
Language Processing, pages 1061?1064,
Pittsburgh, PA.
Yang, Fan and Peter A. Heeman. 2009.
Context restoration in multi-tasking
dialogue. In Proceedings of 13th International
Conference on Intelligent User Interfaces,
pages 373?377, Sanibel, FL.
Yang, Fan and Peter A. Heeman. 2010.
Initiative conflicts in task-oriented
dialogue. Computer Speech and Language,
24:175?189.
Yang, Fan, Peter A. Heeman, and Andrew
Kun. 2008. Switching to real-time tasks in
multi-tasking dialogue. In Proceedings of
International Conference on Computational
Linguistics, pages 1025?1032, Manchester.
104

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 177?185,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Importance-Driven Turn-Bidding for Spoken Dialogue Systems
Ethan O. Selfridge and Peter A. Heeman
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, OR, 97006
selfridg@ohsu.edu, heemanp@ohsu.edu
Abstract
Current turn-taking approaches for spoken
dialogue systems rely on the speaker re-
leasing the turn before the other can take it.
This reliance results in restricted interac-
tions that can lead to inefficient dialogues.
In this paper we present a model we re-
fer to as Importance-Driven Turn-Bidding
that treats turn-taking as a negotiative pro-
cess. Each conversant bids for the turn
based on the importance of the intended
utterance, and Reinforcement Learning is
used to indirectly learn this parameter. We
find that Importance-Driven Turn-Bidding
performs better than two current turn-
taking approaches in an artificial collabo-
rative slot-filling domain. The negotiative
nature of this model creates efficient dia-
logues, and supports the improvement of
mixed-initiative interaction.
1 Introduction
As spoken dialogue systems are designed to
perform ever more elaborate tasks, the need
for mixed-initiative interaction necessarily grows.
Mixed-initiative interaction, where agents (both
artificial and human) may freely contribute to
reach a solution efficiently, has long been a focus
of dialogue systems research (Allen et al, 1999;
Guinn, 1996). Simple slot-filling tasks might
not require the flexible environment that mixed-
initiative interaction brings but those of greater
complexity, such as collaborative task comple-
tion or long-term planning, certainly do (Fergu-
son et al, 1996). However, translating this interac-
tion into working systems has proved problematic
(Walker et al, 1997), in part to issues surround-
ing turn-taking: the transition from one speaker to
another.
Many computational turn-taking approaches
seek to minimize silence and utterance overlap
during transitions. This leads to the speaker con-
trolling the turn transition. For example, systems
using the Keep-Or-Release approach will not at-
tempt to take the turn unless it is sure the user
has released it. One problem with this approach
is that the system might have important informa-
tion to give but will be unable to get the turn.
The speaker-centric nature of current approaches
does not enable mixed-initiative interaction and
results in inefficient dialogues. Primarily, these
approaches have been motivated by smooth tran-
sitions reported in the human turn-taking studies
of Sacks et al (1974) among others.
Sacks et al also acknowledge the negotiative
nature of turn-taking, stating that the ?the turn as
unit is interactively determined?(p. 727). Other
studies have supported this, suggesting that hu-
mans negotiate the turn assignment through the
use of cues and that these cues are motivated by
the importance of what the conversant wishes to
contribute (Duncan and Niederehe, 1974; Yang
and Heeman, 2010; Schegloff, 2000). Given
this, any dialogue system hoping to interact with
humans efficiently and naturally should have a
negotiative and importance-driven quality to its
turn-taking protocol. We believe that, by focus-
ing on the rationale of human turn-taking be-
havior, a more effective turn-taking system may
be achieved. We propose the Importance-Driven
Turn-Bidding (IDTB) model where conversants
bid for the turn based on the importance of their
utterance. We use Reinforcement Learning to map
a given situation to the optimal utterance and bid-
ding behavior. By allowing conversants to bid for
the turn, the IDTB model enables negotiative turn-
taking and supports true mixed-initiative interac-
tion, and with it, greater dialogue efficiency.
We compare the IDTB model to current turn-
taking approaches. Using an artificial collab-
orative dialogue task, we show that the IDTB
model enables the system and user to complete
177
the task more efficiently than the other approaches.
Though artificial dialogues are not ideal, they al-
low us to test the validity of the IDTB model be-
fore embarking on costly and time-consuming hu-
man studies. Since our primary evaluation criteria
is model comparison, consistent user simulations
provide a constant needed for such measures and
increase the external validity of our results.
2 Current Turn-Taking Approaches
Current dialogue systems focus on the release-turn
as the most important aspect of turn-taking, in
which a listener will only take the turn after the
speaker has released it. The simplest of these ap-
proaches only allows a single utterance per turn,
after which the turn necessarily transitions to the
next speaker. This Single-Utterance (SU) model
has been extended to allow the speaker to keep the
turn for multiple utterances: the Keep-Or-Release
(KR) approach. Since the KR approach gives the
speaker sole control of the turn, it is overwhelm-
ingly speaker-centric, and so necessarily unnego-
tiative. This restriction is meant to encourage
smooth turn-transitions, and is inspired by the or-
der, smoothness, and predictability reported in hu-
man turn-taking studies (Duncan, 1972; Sacks et
al., 1974).
Systems using the KR approach differ on how
they detect the user?s release-turn. Turn releases
are commonly identified in two ways: either us-
ing a silence-threshold (Sutton et al, 1996), or
the predictive nature of turn endings (Sacks et al,
1974) and the cues associated with them (e.g. Gra-
vano and Hirschberg, 2009). Raux and Eskenazi
(2009) used decision theory with lexical cues to
predict appropriate places to take the turn. Simi-
larly, Jonsdottir, Thorisson, and Nivel (2008) used
Reinforcement Learning to reduce silences be-
tween turns and minimize overlap between utter-
ances by learning the specific turn-taking patterns
of individual speakers. Skantze and Schlangan
(2009) used incremental processing of speech and
prosodic turn-cues to reduce the reaction time of
the system, finding that that users rated this ap-
proach as more human-like than a baseline system.
In our view, systems built using the KR turn-
taking approach suffer from two deficits. First,
the speaker-centricity leads to inefficient dialogues
since the speaker may continue to hold the turn
even when the listener has vital information to
give. In addition, the lack of negotiation forces
the turn to necessarily transition to the listener af-
ter the speaker releases it. The possibility that the
dialogue may be better served if the listener does
not get the turn is not addressed by current ap-
proaches.
Barge-in, which generally refers to allowing
users to speak at any time (Stro?m and Seneff,
2000), has been the primary means to create a
more flexible turn-taking environment. Yet, since
barge-in recasts speaker-centric systems as user-
centric, the system?s contributions continue to be
limited. System barge-in has also been investi-
gated. Sato et al (2002) used decision trees to de-
termine whether the system should take the turn or
not when the user pauses. An incremental method
by DeVault, Sagae, and Traum (2009) found pos-
sible points that a system could interrupt without
loss of user meaning, but failed to supply a rea-
sonable model as to when to use such information.
Despite these advances, barge-in capable systems
lack a negotiative turn-taking method, and con-
tinue to be deficient for reasons similar to those
described above.
3 Importance-Driven Turn-Bidding
(IDTB)
We introduce the IDTB model to overcome the de-
ficiencies of current approaches. The IDTB model
has two foundational components: (1) The impor-
tance of speaking is the primary motivation behind
turn-taking behavior, and (2) conversants use turn-
cue strength to bid for the turn based on this impor-
tance. Importance may be broadly defined as how
well the utterance leads to some predetermined
conversational success, be it solely task comple-
tion or encompassing a myriad of social etiquette
components.
Importance-Driven Turn-Bidding is motivated
by empirical studies of human turn-conflict res-
olution. Yang and Heeman (2010) found an in-
crease of turn conflicts during tighter time con-
straints, which suggests that turn-taking is in-
fluenced by the importance of task completion.
Schlegoff (2000) proposed that persistent utter-
ance overlap was indicative of conversants hav-
ing a strong interest in holding the turn. Walker
and Whittaker (1990) show that people will inter-
rupt to remedy some understanding discrepancy,
which is certainly important to the conversation?s
success. People communicate the importance of
their utterance through turn-cues. Duncan and
178
Niederehe (1974) found that turn-cue strength was
the best predictor of who won the turn, and this
finding is consistent with the use of volume to win
turns found by Yang and Heeman (2010).
The IDTB model uses turn-cue strength to bid
for the turn based on the importance of the utter-
ance. Stronger turn-cues should be used when the
intended utterance is important to the overall suc-
cess of the dialogue, and weaker ones when it is
not. In the prototype described in Section 5, both
the system and user agents bid for the turn after ev-
ery utterance and the bids are conceptualized here
as utterance onset: conversants should be quick
to speak important utterances but slow with less
important ones. This is relatively consistent with
Yang and Heeman (2010). A mature version of
our work will use cues in addition to utterance on-
set, such as those recently detailed in Gravano and
Hirshberg (2009).1
A crucial element of our model is the judgment
and quantization of utterance importance. We use
Reinforcement Learning (RL) to determine impor-
tance by conceptualizing it as maximizing the re-
ward over an entire dialogue. Whatever actions
lead to a higher return may be thought of as more
important than ones that do not.2 By using RL to
learn both the utterance and bid behavior, the sys-
tem can find an optimal pairing between them, and
choose the best combination for a given conversa-
tional situation.
4 Information State Update and
Reinforcement Learning
We build our dialogue system using the Informa-
tion State Update approach (Larsson and Traum,
2000) and use Reinforcement Learning for action
selection (Sutton and Barto, 1998). The system
architecture consists of an Information State (IS)
that represents the agent?s knowledge and is up-
dated using a variety of rules. The IS also uses
rules to propose possible actions. A condensed
and compressed subset of the IS ? the Reinforce-
ment Learning State ? is used to learn which pro-
posed action to take (Heeman, 2007). It has been
shown that using RL to learn dialogue polices is
generally more effective than ?hand crafted? di-
1Our work (present and future) is distinct from some re-
cent work on user pauses (Sato et al, 2002) since we treat
turn-taking as an integral piece of dialogue success.
2We gain an inherent flexibility in using RL since the re-
ward can be computed by a wide array of components. This
is consistent with the broad definition of importance.
alogue policies since the learning algorithm may
capture environmental dynamics that are unat-
tended to by human designers (Levin et al, 2000).
Reinforcement Learning learns an optimal pol-
icy, a mapping between a state s and action a,
where performing a in s leads to the lowest ex-
pected cost for the dialogue (we use minimum
cost instead of maximum reward). An -greedy
search is used to estimate Q-scores, the expected
cost of some state?action pair, where the system
chooses a random action with  probability and the
argminaQ(s, a) action with 1- probability. For
Q-learning, a popular RL algorithm and the one
used here,  is commonly set at 0.2 (Sutton and
Barto, 1998). Q-learning updates Q(s, a) based
on the best action of the next state, given by the
following equation, with the step size parameter
? = 1/
?
N(s, a) where N(s, a) is the number of
times the s, a pair has been seen since the begin-
ning of training.
Q(st, at) = Q(st, at) + ?[costt+1
+ argminaQ(st+1, a)?Q(st, at)]
The state space should be formulated as a
Markov Decision Process (MDP) for Q-learning
to update Q-scores properly. An MDP relies on
a first-order Markov assumption in that the transi-
tion and reward probability from some st, at pair
is completely contained by that pair and is unaf-
fected by the history st?1at?1, st?2at?2, . . .. For
this assumption to be met, care is required when
deciding which features to include for learning.
The RL State features we use are described in the
following section.
5 Domain and Turn-Taking Models
In this section, we show how the IDTB ap-
proach can be implemented for a collaborative
slot filling domain. We also describe the Single-
Utterance and Keep-Or-Release domain imple-
mentations that we use for comparison.
5.1 Domain Task
We use a food ordering domain with two partici-
pants, the system and a user, and three slots: drink,
burger, and side. The system?s objective is to fill
all three slots with the available fillers as quickly
as possible. The user?s role is to specify its de-
sired filler for each slot, though that specific filler
may not be available. The user simulation, while
intended to be realistic, is not based on empirical
data. Rather, it is designed to provide a rich turn-
179
taking domain to evaluate the performance of dif-
ferent turn-taking designs. We consider this a col-
laborative slot-filling task since both conversants
must supply information to determine the intersec-
tion of available and desired fillers.
Users have two fillers for each slot.3 A user?s
top choice is either available, in which case we say
that the user has adequate filler knowledge, or their
second choice will be available, in which we say
it has inadequate filler knowledge. This assures
that at least one of the user?s filler is available.
Whether a user has adequate or inadequate filler
knowledge is probabilistically determined based
on user type, which will be described in Section
5.2.
Table 1: Agent speech acts
Agent Actions
System query slot, inform [yes/no],
inform avail. slot fillers,
inform filler not available, bye
User inform slot filler,
query filler availability
We model conversations at the speech act level,
shown in Table 1, and so do not model the actual
words that the user and system might say. Each
agent has an Information State that proposes possi-
ble actions. The IS is made up of a number of vari-
ables that model the environment and is slightly
different for the system and the user. Shared vari-
ables include QUD, a stack which manages the
questions under discussion; lastUtterance, the pre-
vious utterance, and slotList, a list of the slot
names. The major system specific IS variables
that are not included in the RL State are availSlot-
Fillers, the available fillers for each slot; and three
slotFiller variables that hold the fillers given by the
user. The major user specific IS variables are three
desiredSlotFiller variables that hold an ordered list
of fillers, and unvisitedSlots, a list of slots that the
user believes are unfilled.
The system has a variety of speech actions: in-
form [yes/no], to answer when the user has asked a
filler availability question; inform filler not avail-
able, to inform the user when they have specified
an unavailable filler; three query slot actions (one
for each slot), a query which asks the user for a
filler and is proposed if that specific slot is unfilled;
3We use two fillers so as to minimize the length of train-
ing. This can be increased without substantial effort.
three inform available slot fillers actions, which
lists the available fillers for that slot and is pro-
posed if that specific slot is unfilled or filled with
an unavailable filler; and bye, which is always pro-
posed.
The user has two actions. They can inform the
system of a desired slot filler, inform slot filler, or
query the availability of a slot?s top filler, query
filler availability. A user will always respond with
the same slot as a system query, but may change
slots entirely for all other situations. Additional
details on user action selection are given in Section
5.2.
Specific information is used to produce an in-
stantiated speech action, what we refer to as an
utterance. For example, the speech action inform
slot filler results in the utterance of ?inform drink
d1.? A sample dialogue fragment using the Single-
Utterance approach is shown in Table 2. Notice
that in Line 3 the system informs the user that
their first filler, d1, is unavailable. The user then
asks asks about the availability of its second drink
choice, d2 (Line 4), and upon receiving an affirma-
tive response (Line 5), informs the system of that
filler preference (Line 6).
Table 2: Single-Utterance dialogue
Spkr Speech Action Utterance
1 S: q. slot q. drink
2 U: i. slot filler i. drink d1
3 S: i. filler not avail i. not have d1
4 U: q. filler avail q. drink have d2
5 S: i. slot i. yes
6 U: i. slot filler i. drink d2
7 S: i. avail slot fillers i. burger have b1
Implementation in RL: The system uses RL to
learn which of the IS proposed actions to take. In
this domain we use a cost function based on dia-
logue length and the number of slots filled with an
available filler: C = Number of Utterances + 25 ?
unavailablyFilledSlots. In the present implemen-
tation the system?s bye utterance is costless. The
system chooses the action that minimizes the ex-
pected cost of the entire dialogue from the current
state.
The RL state for the speaker has seven vari-
ables:4 QUD-speaker, the stack of speakers who
have unresolved questions; Incorrect-Slot-Fillers,
4We experimented with a variety of RL States and this one
proved to be both small and effective.
180
a list of slot fillers (ordered chronologically on
when the user informed them) that are unavail-
able and have not been resolved; Last-Sys-Speech-
Action, the last speech action the system per-
formed; Given-Slot-Fillers, a list of slots that the
system has performed the inform available slot
filler action on; and three booleans variables, slot-
RL, that specify whether a slot has been filled cor-
rectly or not (e.g. Drink-RL).
5.2 User Types
We define three different types of users ? Experts,
Novices, and Intermediates. User types differ
probabilistically on two dimensions: slot knowl-
edge, and slot belief strength. We define experts to
have a 90 percent chance of having adequate filler
knowledge, intermediates a 50 percent chance,
and novices a 10 percent chance. These proba-
bilities are independent between slots. Slot belief
strength represents the user?s confidence that it has
adequate domain knowledge for the slot (i.e. the
top choice for that slot is available). It is either
a strong, warranted, or weak belief (Chu-Carroll
and Carberry, 1995). The intuition is that experts
should know when their top choice is available,
and novices should know that they do not know
the domain well.
Initial slot belief strength is dependent on user
type and whether their filler knowledge is ade-
quate (their initial top choice is available). Ex-
perts with adequate filler knowledge have a 70,
20, and 10 percent chance of having Strong, War-
ranted, and Weak beliefs respectfully. Similarly,
intermediates with adequate knowledge have a 50,
25, and 25 percent chance of the respective belief
strengths. When these user types have inadequate
filler knowledge the probabilities are reversed to
determine belief strength (e.g. Experts with inad-
equate domain knowledge for a slot have a 70%
chance of having a weak belief). Novice users al-
ways have a 10, 10, and 80 percent chance of the
respective belief strengths.
The user choses whether to use the query or
inform speech action based on the slot?s belief
strength. A strong belief will always result in an
inform, a warranted belief resulting in an inform
with p = 0.5, and weak belief will result in an in-
form with p = 0.25. If the user is informed of the
correct fillers by the system?s inform, that slot?s
belief strength is set to strong. If the user is in-
formed that a filler is not available, than that filler
is removed from the desired filler list and the belief
remains the same.5
5.3 Turn-Taking Models
We now discuss how turn-taking works for the
IDTB model and the two competing models that
we use to evaluate our approach. The system
chooses its turn action based on the RL state and
we add a boolean variable turn-action to the RL
State to indicate when the system is performing a
turn action or a speech action. The user uses belief
to choose its turn action.
Turn-Bidding: Agents bid for the turn at the
end of each utterance to determine who will speak
next. Each bid is represented as a value between 0
and 1, and the agent with the lower value (stronger
bid) wins the turn. This is consistent with the
use of utterance onset. There are 5 types of bids,
highest, high, middle, low, and lowest, which are
spread over a portion of the range as shown in Fig-
ure 1. The system uses RL to choose a bid and
a random number (uniform distribution) is gener-
ated from that bid?s range. The users? bids are de-
termined by their belief strength, which specifies
the mean of a Gaussian distribution, as shown in
Figure 1 (e.g Strong belief implies a ? = 0.35).
Computing bids in this fashion leads to, on av-
erage, users with strong beliefs bidding highest,
warranted beliefs bidding in the middle, and weak
beliefs bidding lowest. The use of the probabil-
ity distributions allows us to randomly decide ties
between system and user bids.
Figure 1: Bid Value Probability Distribution
Single-Utterance: The Single-Utterance (SU)
approach, as described in Section 2, has a rigid
5In this simple domain the next filler is guaranteed to be
available if the first is not. We do not model this with belief
strength since it is probably not representative of reality.
181
turn-taking mechanism. After a speaker makes a
single utterance the turn transitions to the listener.
Since the turn transitions after every utterance the
system must only choose appropriate utterances,
not turn-taking behavior. Similarly, user agents do
not have any turn-taking behavior and slot beliefs
are only used to choose between a query and an
inform.
Keep-Or-Release Model: The Keep-Or-
Release (KR) model, as described in Section
2, allows the speaker to either keep the turn to
make multiple utterances or release it. Taking the
same approach as English and Heeman (2005),
the system learns to keep or release the turn after
each utterance that it makes. We also use RL
to determine which conversant should begin the
dialogue. While the use of RL imparts some
importance onto the turn-taking behavior, it
is not influencing whether the system gets the
turn when it did not already have it. This is an
crucial distinction between KR and IDTB. IDTB
allows the conversants to negotiate the turn using
turn-bids motivated by importance, whereas in
KR only the speaker determines when the turn
can transition.
Users in the KR environment choose whether to
keep or release the turn similarly to bid decisions.6
After a user performs an utterance, it chooses the
slot that would be in the next utterance. A number,
k, is generated from a Gaussian distribution using
belief strength in the same manner as the IDTB
users? bids are chosen. If k ? 0.55 then the user
keeps the turn, otherwise it releases it.
5.4 Preliminary Turn-Bidding System
We described a preliminary turn-bidding system
in earlier work presented at a workshop (Selfridge
and Heeman, 2009). A major limitation was an
overly simplified user model. We used two user
types, expert and novice, who had fixed bids. Ex-
perts always bid high and had complete domain
knowledge, and the novices always bid low and
had incomplete domain knowledge. The system,
using all five bid types, was always able to out bid
and under bid the simulated users. Among other
things, this situation gives the system complete
control of the turn, which is at odds with the nego-
tiative nature of IDTB. The present contribution is
a more realistic and mature implementation.
6We experimented with a few different KR decision
strategies, and chose the one that performed the best.
6 Evaluation and Discussion
We now evaluate the IDTB approach by compar-
ing it against the two competing models: Single-
Utterance and Keep-Or-Release. The three turn-
taking approaches are trained and tested in four
user conditions: novice, intermediate, expert, and
combined. In the combined condition, one of the
three user types is randomly selected for each dia-
logue. We train ten policies for each condition and
turn-taking approach. Policies are trained using Q-
learning, and ?greedy search for 10000 epochs
(1 epoch = 100 dialogues, after which the Q-scores
are updated) with  = 0.2. Each policy is then
ran over 10000 test dialogues with no exploration
( = 0), and the mean dialogue cost for that pol-
icy is determined. The 10 separate policy values
are then averaged to create the mean policy cost.
The mean policy cost between the turn-taking ap-
proaches and user conditions are shown in Table 3.
Lower numbers are indicative of shorter dialogues,
since the system learns to successfully complete
the task in all cases.
Table 3: Mean Policy Cost for Model and User
condition7
Model Novice Int. Expert Combined
SU 7.61 7.09 6.43 7.05
KR 6.00 6.35 4.46 6.01
IDTB 6.09 5.77 4.35 5.52
Single User Conditions: Single user conditions
show how well each turn-taking approach can op-
timize its behavior for specific user populations
and handle slight differences found in those pop-
ulations. Table 3 shows that the mean policy cost
of the SU model is higher than the other two mod-
els which indicates longer dialogues on average.
Since the SU system must respond to every user
utterance and cannot learn a turn-taking strategy
to utilize user knowledge, the dialogues are neces-
sarily longer. For example, in the expert condition
the best possible dialogue for a SU interaction will
have a cost of five (three user utterances for each
slot, two system utterances in response). This cost
is in contrast to the best expert dialogue cost of
three (three user utterances) for KR and IDTB in-
teractions.
The IDTB turn-taking approach outperforms
the KR design in all single user conditions ex-
7SD between policies ? 0.04
182
cept for novice (6.09 vs. 6.00). In this condi-
tion, the KR system takes the turn first, informs
the available fillers for each slot, and then releases
the turn. The user can then inform its filler eas-
ily. The IDTB system attempts a similar dialogue
strategy by using highest bids but sometimes loses
the turn when users also bid highest. If the user
uses the turn to query or inform an unavailable
filler the dialogue grows longer. However, this is
quite rare as shown by small difference in perfor-
mance between the two models. In all other single
user conditions, the IDTB approach has shorter di-
alogues than the KR approach (5.77 and 4.35 vs.
6.35 and 4.46). A detailed explanation of IDTB?s
performance will be given in Section 6.1.
Combined User Condition: We next measure
performance on the combined condition that
mixes all three user types. This condition is more
realistic than the other three, as it better mimics
how a system will be used in actual practice. The
IDTB approach (mean policy cost = 5.52) outper-
forms the KR (mean policy cost = 6.01) and SU
(mean policy cost = 7.05) approaches. We also
observe that KR outperforms SU. These results
suggest that the more a turn-taking design can be
flexible and negotiative, the more efficient the dia-
logues can be.
Exploiting User bidding differences: It fol-
lows that IDTB?s performance stems from its ne-
gotiative turn transitions. These transitions are dis-
tinctly different than KR transitions in that there is
information inherent in the users bids. A user that
has a stronger belief strength is more likely to be
have a higher bid and inform an available filler.
Policy analysis shows that the IDTB system takes
advantage of this information by using moderate
bids ?neither highest nor lowest bids? to filter
users based on their turn behavior. The distribu-
tion of bids used over the ten learned policies is
shown in Table 4. The initial position refers to
the first bid of the dialogue; final position, the last
bid of the dialogue; and medial position, all other
bids. Notice that the system uses either the low or
mid bids as its initial policy and that 67.2% of di-
alogue medial bids are moderate. These distribu-
tions show that the system has learned to use the
entire bid range to filter the users, and is not seek-
ing to win or lose the turn outright. This behavior
is impossible in the KR approach.
Table 4: Bid percentages over ten policies in the
Combined User condition for IDTB
Position H-est High Mid Low L-est
Initial 0.0 0.0 70.0 30.0 0.0
Medial 20.5 19.4 24.5 23.3 12.3
Final 49.5 41.0 9.5 0.0 0.0
6.1 IDTB Performance:
In our domain, performance is measured by dia-
logue length and solution quality. However, since
solution quality never affects the dialogue cost for
a trained system, dialogue length is the only com-
ponent influencing the mean policy cost.
The primary cause of longer dialogues are un-
available filler inform and query (UFI?Q) utter-
ances by the user, which are easily identified.
These utterances lengthen the dialogue since the
system must inform the user of the available fillers
(the user would otherwise not know that the filler
was unavailable) and then the user must then in-
form the system of its second choice. The mean
number of UFI?Q utterance for each dialogue over
the ten learned policies are shown for all user con-
ditions in Table 5. Notice that these numbers are
inversely related to performance: the more UFI?
Q utterances, the worse the performance. For ex-
ample, in the combined condition the IDTB users
perform 0.38 UFI?Q utterances per dialogue (u/d)
compared to the 0.94 UFI?Q u/d for KR users.
While a KR user will release the turn if its planned
Table 5: Mean number of UFI?Q utterances over
policies
Model Novice Int. Expert Combined
KR 0.0 1.15 0.53 0.94
IDTB 0.1 0.33 0.39 0.38
utterance has a weak belief, it may select that weak
utterance when first getting the turn (either after a
system utterance or at the start of the dialogue).
This may lead to a UFI?Q utterance. The IDTB
system, however, will outbid the same user, result-
ing in a shorter dialogue. This situation is shown
in Tables 6 and 7. The dialogue is the same un-
til utterance 3, where the IDTB system wins the
turn with a mid bid over the user?s low bid. In the
KR environment however, the user gets the turn
and performs an unavailable filler inform, which
the system must react to. This is an instance of
the second deficiency of the KR approach, where
183
Table 6: Sample IDTB dialogue in Combined User
condition; Cost=6
Sys Usr Spkr Utt
1 low mid U: inform burger b1
2 h-est low S: inform burger have b3
3 mid low S: inform side have s1
4 mid h-est U: inform burger b3
5 mid high U: inform drink d1
6 l-est h-est U: inform side s1
7 high mid S: bye
Table 7: Sample KR dialogue in Combined User
condition; Cost=7
Agent Utt Turn-Action
1 U: inform burger b1 Release
2 S: inform burger have b3 Release
3 U: inform side s1 Keep
4 U: inform drink d1 Keep
5 U: inform burger b3 Release
6 S: inform side have s2 Release
7 U: inform side s2 Release
8 S: bye
the speaking system should not have released the
turn. The user has the same belief in both scenar-
ios, but the negotiative nature of IDTB enables a
shorter dialogues. In short, the IDTB system can
win the turn when it should have it, but the KR
system cannot.
A lesser cause of longer dialogues is an instance
of the first deficiency of the KR systems; the lis-
tening user cannot get the turn when it should have
it. Usually, this situation presents itself when the
user releases the turn, having randomly chosen the
weaker of the two unfilled slots. The system then
has the turn for more than one utterance, inform-
ing the available fillers for two slots. However,
the user already had a strong belief and available
top filler for one of those slots, and the system
has increased the dialogue length unnecessarily. In
the combined condition, the KR system produces
0.06 unnecessary informs per dialogue, whereas
the IDTB system produces 0.045 per dialogue.
The novice and intermediate conditions mirror this
(IDTB: 0.009, 0.076 ; KR: 0.019, 0.096 respect-
fully), but the expert condition does not (IDTB:
0.011, KR: 0.0014). In this case, the IDTB system
wins the turn initially using a low bid and informs
one of the strong slots, whereas the expert user ini-
tiates the dialogue for the KR environment and un-
necessary informs are rarer. In general, however,
the KR approach has more unnecessary informs
since the KR system can only infer that one of the
user?s beliefs was probably weak, otherwise the
user would not have released the turn. The IDTB
system handles this situation by using a high bid,
allowing the user to outbid the system as its con-
tribution is more important. In other words, the
IDTB user can win the turn when it should have it,
but the KR user cannot.
7 Conclusion
This paper presented the Importance-Driven Turn-
Bidding model of turn-taking. The IDTB model is
motivated by turn-conflict studies showing that the
interest in holding the turn influences conversant
turn-cues. A computational prototype using Re-
inforcement Learning to choose appropriate turn-
bids performs better than the standard KR and SU
approaches in an artificial collaborative dialogue
domain. In short, the Importance-Driven Turn-
Bidding model provides a negotiative turn-taking
framework that supports mixed-initiative interac-
tions.
In the previous section, we showed that the KR
approach is deficient for two reasons: the speak-
ing system might not keep the turn when it should
have, and might release the turn when it should
not have. This is driven by KR?s speaker-centric
nature; the speaker has no way of judging the
potential contribution of the listener. The IDTB
approach however, due to its negotiative quality,
does not have this problem.
Our performance differences arise from situa-
tions when the system is the speaker and the user
is the listener. The IDTB model also excels in the
opposite situation, when the system is the listener
and the user is the speaker, though our domain is
not sophisticated enough for this situation to oc-
cur. In the future we hope to develop a domain
with more realistic speech acts and a more diffi-
cult dialogue task that will, among other things,
highlight this situation. We also plan on imple-
menting a fully functional IDTB system, using an
incremental processing architecture that not only
detects, but generates, a wide array of turn-cues.
Acknowledgments
We gratefully acknowledge funding from the
National Science Foundation under grant IIS-
0713698.
184
References
J.E Allen, C.I. Guinn, and Horvitz E. 1999. Mixed-
initiative interaction. IEEE Intelligent Systems,
14(5):14?23.
Jennifer Chu-Carroll and Sandra Carberry. 1995. Re-
sponse generation in collaborative negotiation. In
Proceedings of the 33rd annual meeting on Asso-
ciation for Computational Linguistics, pages 136?
143, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can i finish? learning when to respond to incre-
mental interpretation results in interactive dialogue.
In Proceedings of the SIGDIAL 2009 Conference,
pages 11?20, London, UK, September. Association
for Computational Linguistics.
S.J. Duncan and G. Niederehe. 1974. On signalling
that it?s your turn to speak. Journal of Experimental
Social Psychology, 10:234?247.
S.J. Duncan. 1972. Some signals and rules for taking
speaking turns in conversations. Journal of Person-
ality and Social Psychology, 23:283?292.
M. English and Peter A. Heeman. 2005. Learning
mixed initiative dialog strategies by using reinforce-
ment learning on both conversants. In Proceedings
of HLT/EMNLP, pages 1011?1018.
G. Ferguson, J. Allen, and B. Miller. 1996. TRAINS-
95: Towards a mixed-initiative planning assistant.
In Proceedings of the Third Conference on Artificial
Intelligence Planning Systems (AIPS-96), pages 70?
77.
A. Gravano and J. Hirschberg. 2009. Turn-yielding
cues in task-oriented dialogue. In Proceedings of the
SIGDIAL 2009 Conference: The 10th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 253?261. Association for Compu-
tational Linguistics.
C.I. Guinn. 1996. Mechanisms for mixed-initiative
human-computer collaborative discourse. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 278?285. As-
sociation for Computational Linguistics.
P.A. Heeman. 2007. Combining reinforcement learn-
ing with information-state update rules. In Pro-
ceedings of the Annual Conference of the North
American Association for Computational Linguis-
tics, pages 268?275, Rochester, NY.
Gudny Ragna Jonsdottir, Kristinn R. Thorisson, and
Eric Nivel. 2008. Learning smooth, human-like
turntaking in realtime dialogue. In IVA ?08: Pro-
ceedings of the 8th international conference on In-
telligent Virtual Agents, pages 162?175, Berlin, Hei-
delberg. Springer-Verlag.
S. Larsson and D. Traum. 2000. Information state and
dialogue managment in the trindi dialogue move en-
gine toolkit. Natural Language Engineering, 6:323?
340.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1):11 ? 23.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Pro-
ceedings of HLT/NAACL, pages 629?637. Associa-
tion for Computational Linguistics.
H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A
simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
R. Sato, R. Higashinaka, M. Tamoto, M. Nakano, and
K. Aikawa. 2002. Learning decision trees to de-
termine turn-taking by spoken dialogue systems. In
ICSLP, pages 861?864, Denver, CO.
E.A. Schegloff. 2000). Overlapping talk and the orga-
nization of turn-taking for conversation. Language
in Society, 29:1 ? 63.
E. O. Selfridge and Peter A. Heeman. 2009. A bidding
approach to turn-taking. In 1st International Work-
shop on Spoken Dialogue Systems.
G. Skantze and D. Schlangen. 2009. Incremental di-
alogue processing in a micro-domain. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 745?753. Association for Computational Lin-
guistics.
N. Stro?m and S. Seneff. 2000. Intelligent barge-in in
conversational systems. In Sixth International Con-
ference on Spoken Language Processing. Citeseer.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
S. Sutton, D. Novick, R. Cole, P. Vermeulen, J. de Vil-
liers, J. Schalkwyk, and M. Fanty. 1996. Build-
ing 10,000 spoken-dialogue systems. In ICSLP,
Philadelphia, Oct.
M. Walker and S. Whittaker. 1990. Mixed initiative
in dialoge: an investigation into discourse segmen-
tation. In Proceedings of the 28th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 70?76.
M. Walker, D. Hindle, J. Fromer, G.D. Fabbrizio, and
C. Mestel. 1997. Evaluating competing agent
strategies for a voice email agent. In Fifth European
Conference on Speech Communication and Technol-
ogy.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue?. Computer Speech
Language, 24(2):175 ? 189.
185
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 53?61,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Using Reinforcement Learning to Create Communication Channel
Management Strategies for Diverse Users
Rebecca Lunsford
Center for Spoken Lang. Understanding
Oregon Health & Science University
Beaverton, OR, USA
lunsforr@ohsu.edu
Peter Heeman
Center for Spoken Lang. Understanding
Oregon Health & Science University
Beaverton, OR, USA
heemanp@ohsu.edu
Abstract
Spoken dialogue systems typically do not
manage the communication channel, instead
using fixed values for such features as the
amplitude and speaking rate. Yet, the qual-
ity of a dialogue can be compromised if the
user has difficulty understanding the system.
In this proof-of-concept research, we explore
using reinforcement learning (RL) to create
policies that manage the communication chan-
nel to meet the needs of diverse users. To-
wards this end, we first formalize a prelimi-
nary communication channel model, in which
users provide explicit feedback regarding is-
sues with the communication channel, and the
system implicitly alters its amplitude to ac-
commodate the user?s optimal volume. Sec-
ond, we explore whether RL is an appropri-
ate tool for creating communication channel
management strategies, comparing two differ-
ent hand-crafted policies to policies trained
using both a dialogue-length and a novel an-
noyance cost. The learned policies performed
better than hand-crafted policies, with those
trained using the annoyance cost learning an
equitable tradeoff between users with differ-
ing needs and also learning to balance finding
a user?s optimal amplitude against dialogue-
length. These results suggest that RL can be
used to create effective communication chan-
nel management policies for diverse users.
Index Terms: communication channel, spoken di-
alogue systems, reinforcement learning, amplitude,
diverse users
1 Introduction
Both Spoken Dialog Systems (SDS) and Assistive
Technology (AT) tend to have a narrow focus, sup-
porting only a subset of the population. SDS typ-
ically aim to support the ?average man?, ignoring
wide variations in potential users? ability to hear and
understand the system. AT aims to support peo-
ple with a recognized disability, but doesn?t sup-
port those whose impairment is not severe enough
to warrant the available devices or services, or those
who are unaware or have not acknowledged that they
need assistance. However, SDS should be able to
meet the needs of users whose abilities fall within,
and between, the extremes of severly impaired and
perfectly abled.
When aiming to support users with widely differ-
ing abilities, the cause of a user?s difficulty is less
important than adapting the communication channel
in a manner that aids understanding. For example,
speech that is presented more loudly and slowly can
help a hearing-impaired elderly person understand
the system, and can also help a person with no hear-
ing loss who is driving in a noisy car. Although one
user?s difficulty is due to impairment and the other
due to an adverse environment, a similar adaptation
may be appropriate to both.
During human-human communication, speakers
manage the communication channel; implicitly al-
tering their manner of speech to increase the likeli-
hood of being understood while concurrently econo-
mizing effort (Lindblom, 1990). In addition to these
implicit actions, speakers also make statements re-
ferring to breakdowns in the communication chan-
53
nel, explicitly pointing out potential problems or
corrections, (e.g. ?Could you please speak up??)
(Jurafsky et al, 1997).
As for human-computer dialogue, SDS are prone
to misrecognition of users? spoken utterances. Much
research has focused on developing techniques for
overcoming or avoiding system misunderstandings.
Yet, as the quality of automatic speech recognition
improves and SDS are deployed to diverse popula-
tions and in varied environments, systems will need
to better attend to possible human misunderstand-
ings. Future SDS will need to manage the commu-
nication channel, in addition to managing the task,
to aid in avoiding these misunderstandings.
Researchers have explored the use of reinforce-
ment learning (RL) to create dialogue policies that
balance and optimize measures of task success (e.g.,
see (Scheffler and Young, 2002; Levin et al, 2000;
Henderson et al, 2008; Walker, 2000)). Along these
lines, RL is potentially well suited to creating poli-
cies for the subtask of managing the communica-
tion channel, as it can learn to adapt to the user
while continuing the dialogue. In doing so, RL may
choose actions that appear costly at the time, but lead
to better overall dialogues.
Our long term goal is to learn how to manage the
communication channel along with the task, moving
away from just ?what? to say and also focusing on
?how? to say it. For this proof-of-concept, our goals
are twofold: 1) to formalize a communication chan-
nel model that encompasses diverse users, initially
focusing just on explicit user actions and implicit
system actions, and 2) to determine whether RL is
an appropriate tool for learning an effective commu-
nication channel management strategy for diverse
users. To explore the above issues, we use a simple
communication channel model in which the system
needs to determine and maintain an amplitude level
that is pleasant and effective for users with differ-
ing amplitude preferences and needs. As our goal
includes decreasing the amount of potentially an-
noying utterances (i.e., those in which the system?s
amplitude setting is in discord with the user?s op-
timal amplitude), we introduce a user-centric cost
metric, which we have termed annoyance cost. We
then compare hand-crafted policies against policies
trained using both annoyance and more traditional
dialogue-length cost components.
2 Related Work
2.1 How People Manage the Channel
When conversing, speakers implicitly adjust fea-
tures of their speech (e.g., speaking rate, loudness)
to maintain the communication channel. For ex-
ample, speakers produce Lombard speech when in
noisy conditions, produce clear speech to better ac-
commodate a hard of hearing listener, and alter their
speech to more closely resemble the interlocutor?s
(Junqua, 1993; Lindblom, 1990). These changes in-
crease the intelligibility of the speech, thus helping
to maintain the communication channel (Payton et
al., 1994). Research has also shown that speakers
adjust their speaking style when addressing a com-
puter; exhibiting the same speech adaptations seen
during human-human communication (Bell et al,
2003; Lunsford et al, 2006).
In addition to altering their speech implicitly,
speakers also explicitly point out communication
channel problems (Jurafsky et al, 1997). Exam-
ples include; requesting a change in speaking rate or
amplitude (?Could you please speak up??), explain-
ing sources of communication channel interference
(?Oh, that noise is the coffee grinder.?), or asking
their interlocutor to repeat an utterance (?What was
that??). These explicit utterances identify some is-
sue with the communication channel that must be
remedied before continuing the dialogue. In re-
sponse, interlocutors will rewind to a previous point
in the dialogue and alter their speech to ensure they
are understood. This approach, of adapting ones
speech in response to a communication problem, oc-
curs even when conversing with a computer (Stent et
al., 2008).
Both implicit speech alterations and explicit ut-
terances regarding the communication channel of-
ten address issues of amplitude. This is to be
expected, as speaking at an appropriate amplitude
is critical to maintaining an effective communica-
tion channel, with sub-optimal amplitude affecting
listeners? understanding and performance (Baldwin
and Struckman-Johnson, 2002). In addition, Bald-
win (2001) found that audible, but lowered, ampli-
tude can negatively affect both younger and older
subjects? reaction time and ability to respond cor-
rectly while multitasking, and that elderly listeners
are likely to need higher amplitudes than younger
54
listeners to maintain similar performance. Just as
low amplitude can be difficult to understand, high
amplitude can be annoying, and, in the extreme,
cause pain.
2.2 How Systems Manage the Channel
Towards improving listener understanding in a po-
tentially noisy environment, Martinson and Brock
(2007) take advantage of the mobility and sensory
capabilities of a robot. To determine the best course
of action, the robot maintains a noise map of the en-
vironment, measuring the environmental noise prior
to each TTS utterance. The robot then rotates to-
ward the listener, changes location, alters its am-
plitude, or pauses until the noise abates. A similar
technique, useful for remote listeners who may be
in a noisy environment or using a noisy communica-
tion medium, could analyze the signal-to-noise ratio
to ascertain the noise level in the listener?s environ-
ment. Although these techniques may be useful for
adjusting amplitude to compensate for noise in the
listener?s environment, they do not address speech
alterations needed to accommodate users with dif-
ferent hearing abilities or preferences.
Given the need to adapt to individual users, it
seems reasonable that users themselves would sim-
ply adjust volume on their local device. However,
there are issues with this approach. First, man-
ual adjustment of the volume would prove problem-
atic when the user?s hands and eyes are busy, such
as when driving a car. Second, during an ongo-
ing dialogue speakers tend to minimize pauses, re-
sponding quickly when given the turn (Sacks et al,
1974). Stopping to alter the amplitude could re-
sult in longer than natural pauses, which systems
often respond to with increasingly lengthy ?time-
out? responses (Kotelly, 2003), or repeating the same
prompt endlessly (Villing et al, 2008). Third, al-
though we focus on amplitude adaptations in this
paper, amplitude is only one aspect of the commu-
nication channel. A fully functional communication
channel management solution would also incorpo-
rate adaptations of features such as speaking rate,
pausing, pitch range, emphasis, etc. This extended
set of features, because of their number and interac-
tion between them, do not readily lend themselves
to listener manipulation.
3 Reinforcement Learning
RL has been used to create dialogue strategies that
specify what action to perform in each possible
system state so that a minimum dialogue cost is
achieved (Walker, 2000; Levin et al, 2000). To ac-
complish this, RL starts with a policy, namely what
action to perform in each state. It then uses this pol-
icy, with some exploration, to estimate the cost of
getting from each state with each possible action to
the final state. As more simulations are run, RL re-
fines its estimates and its current policy. RL will
converge to an optimal solution as long as assump-
tions about costs and state transitions are met. RL is
particularly well suited for learning dialogue strate-
gies as it will balance opposing goals (e.g., minimiz-
ing excessive confirmations vs. ensuring accurate
information).
RL has been applied to a number of dialogue
scenarios. For form-filling dialogues, in which the
user provides parameters for a database query, re-
searchers have used RL to decide what order to use
when prompting for the parameters and to decrease
resource costs such as database access (Levin et al,
2000; Scheffler and Young, 2002). System misun-
derstanding caused by speech recognition errors has
also been modeled to determine whether, and how,
the system should confirm information (Scheffler
and Young, 2002). However, there is no known work
on using RL to manage the communication channel
so as to avoid user misunderstanding.
User Simulation: To train a dialogue strategy us-
ing RL, some method must be chosen to emulate
realistic user responses to system actions. Training
with actual users is generally considered untenable
since RL can require millions of runs. As such, re-
searchers create simulated users that mimic the re-
sponses of real users. The approach employed to
create these users varies between researchers; rang-
ing from simulations that employ only real user data
(Henderson et al, 2008), to those that model users
with probabilistic simulations based on known re-
alistic user behaviors (Levin et al, 2000). Ai et
al. suggest that less realistic user simulations that al-
low RL to explore more of the dialogue state space
may perform as well or better than simulations that
statistically recreate realistic user behavior (Ai et al,
2007). For this proof-of-concept work, we employ a
55
hand-crafted user simulation that allows full explo-
ration of the state space.
Costs: Although it is agreed that RL is a viable
approach to creating optimal dialogue policies, there
remains much debate as to what cost functions result
in the most useful policies. Typically, these costs in-
clude a measure of efficiency (e.g., number of turns)
and a measure of solution quality (e.g., the user suc-
cessfully completed the transaction) (Scheffler and
Young, 2002; Levin et al, 2000). For manag-
ing the communication channel, it is unclear how
the cost function should be structured. In this work
we compare two cost components, a more traditional
dialogue-length cost versus a novel annoyance cost,
to determine which best supports the creation of use-
ful policies.
4 Communication Channel Model
Based on the literature reviewed in Section 2.1, we
devised a preliminary model that captures essential
elements of how users manage the communication
channel. For now, we only include explicit user ac-
tions, in which users directly address issues with
the communication channel, as noted by Jurafsky
et al (1997). In addition, the users modeled are
both consistent and amenable; they provide feed-
back every time the system?s utterances are too loud
or too soft, and abandon the interaction only when
the system persists in presenting utterances outside
the user?s tolerance (either ten utterances that are too
loud or ten that are too soft).
For this work, we wish to create policies that treat
all users equitably. That is, we do not want to train
polices that give preferential treatment to a subset of
users simply because they are more common. To ac-
complish this, we use a flat rather than normal distri-
bution of users within the simulation, with both the
optimal amplitude and the tolerance range randomly
generated for each user. To represent users with dif-
fering amplitude needs, simulated users are modeled
to have an optimal amplitude between 2 and 8, and
a tolerance range of 1, 3 or 5. For example, a user
may have a optimal amplitude of 4, but be able to
tolerate an amplitude between 2 and 6.
When interacting with the computer, the user re-
sponds with: (a) the answer to the system?s query if
the amplitude is within their tolerance range; (b) too
soft (TS) if below their range; or (c) too loud (TL)
if the amplitude is above their tolerance range. As
a simplifying assumption, TS and TL represent any
user responses that address communication channel
issues related to amplitude. For example, the user
response ?Pardon me?? would be represented by TS
and ?There?s no need to shout!? by TL. With this
user model, the user only responds to the domain
task when the system employs an amplitude setting
within the user?s tolerance range.
For the system, we need to ensure that the sys-
tem?s amplitude range can accommodate any user-
tolerable amplitude. For this reason, the system?s
amplitude can vary between 0 and 10, and is ini-
tially set to 5 prior to each dialogue. In addition to
performing domain actions, the system specifies the
amount the amplitude should change: -2, -1, +0, +1,
+2. Each system communication to the user consists
of both a domain action and the system?s amplitude
change. Thus, the system manages the communica-
tion channel using only implicit actions. If the user
responds with TS or TL, the system will then restate
what it just said, perhaps altering the amplitude prior
to re-addressing the user.
5 Hand-crafted Policies
To help in determining whether RL is an appropriate
tool for learning communication channel manage-
ment strategies, we designed two hand-crafted poli-
cies for comparison. The first handcrafted policy,
termed no-complaints, finds a tolerable amplitude
as quickly as possible, then holds that amplitude for
the remainder of the dialogue. As such, this policy
only changes the amplitude in response to explicit
complaints from the user. Specifically, the policy in-
creases the amplitude by 2 after a TS response, and
drops it by 2 after a TL. If altering the amplitude by
2 would cause the system to return to a setting al-
ready identified as too soft or too loud, the system
uses an amplitude change of 1.
The second policy, termed find-optimal, searches
for the user?s optimal amplitude, then maintains that
amplitude for the remainder of the dialogue. For
this policy, the system first increases the amplitude
by 1 until the user responds with TL (potentially in
response to the system?s first utterance), then de-
creases the amplitude by 1 until the user either re-
56
sponds with TS or the optimal amplitude is clearly
identified based on the previous feedback. An am-
plitude change of 2 is used only when both the op-
timal amplitude is obvious and a change of 2 will
bring the amplitude setting to the optimal ampli-
tude.
6 RL and System Encoding
To learn communication channel management poli-
cies we use RL with system and user actions spec-
ified using Information State Update rules (Hender-
son et al, 2008). Following Heeman (2007), we en-
code commonsense preconditions rather than trying
to learn them, and only use a subset of the informa-
tion state for RL.
Domain Task: We use a domain task that requires
the user to supply 9 pieces of information, excluding
user feedback relating to the communication chan-
nel. The system has a deterministic way of selecting
its actions, thus no learning is needed for the domain
task.
State Variables: For RL, each state is represented
by two variables; AmpHistory and Progress. Am-
pHistory models the user by tracking all previ-
ous user feedback. In addition, it tracks the cur-
rent amplitude setting. The string contains one
slot for each potential amplitude setting (0 through
10), with the current setting contained within ?[]?.
Thus, at the beginning of each interaction, the string
is ?-----[-]-----?, where ?-? represents no
known data. Each time the user responds, the string
is updated to reflect which amplitude settings are too
soft (?<?), too loud (?>?), or within the user?s toler-
ance (?O?). When the user responds with TL/TS,
the system also updates all settings above/below the
current setting. The Progress variable is required
to satisfy the Markov property needed for RL. This
variable counts the number of successful informa-
tion exchanges (i.e., the user did not respond with
TS or TL). As the domain task requires 9 pieces of
information, the Progress variable ranged from 1 to
9.
Costs: Our user model only allows up to 10 utter-
ances that are too soft or too loud. If the cutoff is
reached, the domain task has not been completed, so
a solution quality cost of 100 is incurred. Cutting
off dialogues in this way has the additional benefit
of preventing a policy from looping forever during
testing. During training, to allow the system to bet-
ter model the cost of choosing the same action re-
peatedly, we use a longer cutoff of 1000 utterances
rather than 10.
In addition to solution quality, two different cost
components are utilized. The first, a dialogue-length
cost (DC), assigns a cost of 1 for each user utterance.
The second, an annoyance cost (AC), assigns a cost
calculated as the difference between the system?s
amplitude setting and the user?s optimal amplitude.
This difference is multiplied by 3 when the sys-
tem?s amplitude setting is below the user?s optimal.
This multiplier was chosen based on research that
demonstrated increased response times and errors
during cognitively challenging tasks when speech
was presented below, rather than above, typical con-
versational levels (Baldwin and Struckman-Johnson,
2002). Thus, only utterances at the optimal ampli-
tude have no cost.
7 Results
With the above system and user models, we trained
policies using the two cost functions discussed
above, eight with the DC component and eight us-
ing the AC component. All used Q-Learning and
the ?-greedy method to explore the state space with
? set at 20% (Sutton and Barto, 1998). Dialogue runs
were grouped into epochs of 100; after each epoch,
the current dialogue policy was updated. We trained
each policy for 60,000 epochs. After certain epochs,
we tested the policy on 5000 user tasks.
For our simple domain, the solution quality cost
remained 0 after about the 100th epoch, as all poli-
cies learned to avoid user abandonment. Because of
this, only the dialogue-length cost(DC) and annoy-
ance cost(AC) components are reflected in the fol-
lowing analyses.
7.1 DC-Trained Policies
By 40,000 epochs, all eight DC policies converged
to one common optimal policy. Dialogues resulting
from the DC policies average 9.76 user utterances
long. DC policies start each dialogue using the de-
fault amplitude setting of 5. After receiving the ini-
tial user response, they aggressively explore the am-
plitude range. If the initial user response is TL (or
57
DC AC
AmpHistory System Amp User AmpHistory System Amp User
-----[-]----- Query1 +0 5 TS -----[-]----- Query1 +1 6 TS
<<<<<[<]----- Query1 +2 7 Answer <<<<<<[<]---- Query1 +1 7 Answer
<<<<<<-[0]--- Query2 +0 7 Answer <<<<<<<[0]--- Query2 +1 8 Answer
<<<<<<-[0]--- Query3 +0 7 Answer <<<<<<<0[0]-- Query3 +1 9 Answer
<<<<<<-[0]--- Query4 +0 7 Answer <<<<<<00[0]- Query4 +1 10 TL
<<<<<<-[0]--- Query5 +0 7 Answer <<<<<<<000[>] Query4 -2 8 Answer
<<<<<<-[0]--- Query6 +0 7 Answer <<<<<<<0[0]0> Query5 +0 8 Answer
. . . . . . . . . . . . . . . . . . . . . . . .
dialogue length cost = 10 annoyance cost = 12
Table 1: Comparison of DC (left) and AC (right) interactions with a user who has an optimal amplitude of 8 and a
tolerance range of 3. The policies continue as shown, without changing the amplitude level, until all 9 queries are
answered.
TS), they continue by decreasing (or increasing) the
amplitude by -2 (or +2) until they find a tolerable
volume, in which case they stop. Table 1 illustrates
the above noted aspects of the policy. Additionally,
if the policy receives user feedback that is contrary
to the last feedback (i.e., TS after TL, or TL after
TS), the policy backtracks one amplitude setting. In
addition, if the current amplitude is near the bound-
ary (3 or 7), the policy will change the volume by
-1 or +1 as changing it by -2 or +2 would cause it
to move outside users? amplitude range of 2-8. In
essence, the DC policies are quite straightforward;
aggressively changing the amplitude if the user com-
plains, and assuming the amplitude is correct if the
user does not complain.
7.2 AC-Trained Policies
By 55,000 epochs, AC policies converged to one of
two optimal solutions, with an average annoyance
cost of 7.49. As illustrated in Table 1, the behav-
ior of the AC policies is substantially more complex
than the DC policies. First, the AC policies start
by increasing the amplitude, delivering the first ut-
terance at a setting of 6 or 7. Second, the policies
do not stop exploring after they find a tolerable set-
ting, instead attempting to bracket the user?s toler-
ance range, thus identifying the user?s optimal am-
plitude. Third, AC policies sometimes avoid lower-
ing the amplitude, even when doing so would con-
cretely identify the user?s optimal amplitude. By do-
ing so, the policies potentially incur a cost of 1 for
all following turns, but avoid incurring a one time
cost of 3 or 6. In essence, the AC policies attempt to
find the user?s optimal amplitude but may stop short
as they approach the end of the dialogue, favoring a
slightly too high amplitude over one that might be
too low.
7.3 Comparing AC- and DC- Trained Policies
The costs for the AC and DC trained policy sets can-
not be directly compared as each set used a different
cost function. However, we can compare them using
each others? cost function.
First, we compare the two sets of policies in terms
of average dialogue-length. For example, in Table 1,
following a DC policy results in a dialogue-length
of 10. However, for the same user, following the AC
policy results in a dialogue-length of 11, one utter-
ance longer due to the TL response to Query4.
The average dialogue-length of the DC and AC
policies, averaged across users, is shown in the right-
most two columns of Figure 1. As expected, the DC
policies perform better in terms of dialogue-length,
averaging 9.76 utterances long. However, the AC
policies average 10.32 utterances long, only 0.52 ut-
terances longer. This similarity in length is to be ex-
pected, as system communication outside the user?s
tolerance range impedes progress and is costly using
either cost component.
We also compared the AC and DC policies? aver-
age dialogue-length for users with the same optimal
amplitude (i.e., each column shows the average cost
across users with tolerance ranges of 1, 3 and 5), as
shown in Figure 1. From this figure it is clear that
there is little difference in dialogue-length between
AC and DC policies for users with the same optimal
58
amplitude. In addition, for both policies, the lengths
are similar between users with differing optimal am-
plitudes.
 0
 2
 4
 6
 8
 10
 12
 14
2 3 4 5 6 7 8 Ave
A
ve
ra
ge
 D
ia
lo
gu
e 
Le
ng
th
User?s Optimal Amplitude
AC Policies
DC Policies
Figure 1: Comparison of the dialogue-length between AC
and DC policies for users with differing optimal ampli-
tudes.
Second, we compare the two sets of polices in
terms of annoyance costs. For example, in Table 1,
following the AC policy results in an annoyance cost
of 12. For the same user, following the DC policy re-
sults in an annoyance cost of 36; 9 for Query1 as it is
three below the user?s optimal amplitude, and 3 for
each of the following nine utterances as they are all
one below optimal.
As shown in the rightmost columns of Figure 2,
DC policies average annoyance cost was 13.35, a
substantial 78% increase over the average cost of
7.49 for AC policies. Figure 2 also illustrates that
the AC and DC policies perform quite differently for
users with differing optimal amplitudes. For exam-
ple, users of the DC policies whose optimal is at (5),
or slightly below (4), the system?s default setting (5)
average lower annoyance costs than those using the
AC policies. However, these lowered costs for users
in the mid-range is gained at the expense of users
whose optimal amplitude is farther afield, especially
those users requiring higher amplitude settings. This
substantial difference between users with different
optimal amplitudes is because, for DC policies, the
interaction is often conducted at the very edge of the
users? tolerance. In contrast, the AC policies risk
more intolerable utterances, but use this information
to decrease overall costs by better meeting users?
amplitude needs. As such, users of the AC policies
can expect the majority of the task to be conducted
at, or only one setting above, their optimal ampli-
tude.
 0
 5
 10
 15
 20
 25
 30
 35
2 3 4 5 6 7 8 Ave
A
ve
ra
ge
 A
nn
oy
an
ce
 C
os
t
User?s Optimal Amplitude
AC Policies
DC Policies
Figure 2: Comparison of the annoyance cost between AC
and DC policies for users with differing optimal ampli-
tudes.
7.4 Comparing Hand-crafted and Learned
Policies
Each of the two hand-crafted policies were run with
each user simulation (i.e., optimal amplitude from
2-8 and tolerance ranges of 1, 3, or 5). In addition,
we varied the domain task size, requiring between 4
and 10 pieces of information. DC and AC policies
were also trained for these domain task sizes.
As shown in Figure 3, The no-complain policy?s
annoyance costs ranged from 7.81 for dialogues re-
quiring four pieces of information to 14.67 for those
requiring ten pieces. The cost increases linearly with
the amount of information required, because the no-
complain policy maintains the first amplitude setting
found that does not result in a user response of TS
or TL. This ensures the amplitude setting is toler-
able to the user, but may not be the user?s optimal
amplitude.
In contrast, the find-optimal policy?s annoyance
costs initially increase from 9.67 for four pieces of
information to 12.24 for seven through ten pieces.
The cost does not continue to increase when the
amount of information required is greater than seven
because, for dialogues long enough to allow the sys-
tem to concretely identify the user?s optimal ampli-
tude, the cost is zero for all subsequent utterances.
Figure 3 also includes the mean annoyance cost
for the DC and AC policies. Although one might
expect the DC trained policies to resemble the
no-complain policy, the learned policy performs
slightly better. This difference is because the DC
policies learn the range of users? optimal amplitude
settings (2-8), and do not move the amplitude below
2 or above 8. In contrast, the no-complain policies
59
Figure 3: Average user annoyance costs for hand-crafted,
DC and AC policies across dialogues requiring differing
amounts of information.
behave consistently regardless of the current setting,
and thus will incur costs for exploring settings out-
side the range of users? optimal amplitudes. Simi-
larly, AC policies could be anticipated to closely re-
semble the find-optimal policy. However, the AC
policies average cost is lower than the costs for ei-
ther hand-crafted policy, regardless of the amount
of information required. This difference is, in part,
due to differences in behavior at the ends of the
users? optimal amplitude range, like the DC poli-
cies. However, additional factors include the AC
policies? more varied use of amplitude changes and
their balancing of the remaining duration of the di-
alogue against the cost to perform additional explo-
ration, as discussed in subsection 7.2.
8 Discussion and Future Work
The first objective of this work was to create a model
of the communication channel that takes into ac-
count the abilities and preferences of diverse users.
In this model, each user has an optimal amplitude,
but will answer a system query delivered within a
range around that amplitude, although they find non-
preferred, especially too soft, amplitudes annoying.
When outside the user?s tolerance, the user pro-
vides explicit feedback regarding the communica-
tion channel breakdown. For the system, the model
specifies a composite system action, pairing a do-
main action with a possible communication chan-
nel management action to change the amplitude. By
modeling explicit user actions, and implicit system
actions, this model captures some essential elements
of how people manage the communication channel.
The second objective was to determine whether
RL is appropriate for learning communication chan-
nel management. As expected, the learned policies
found and maintained a tolerable amplitude setting
and eliminated user abandonment. We also com-
pared the learned policies with handcrafted solu-
tions, and found that the learned policies performed
better. This is primarily due to RL?s ability to auto-
matically balance the opposing goals of finding the
user?s optimal amplitude and minimizing dialogue-
length.
An added benefit of RL is that it optimizes the sys-
tem?s behavior for the users on which it is trained.
In this work, we purposely used a flat distribution of
users, which caused RL to find a policy (especially
when using annoyance costs) that does not penal-
ize the outliers, which are usually those with special
needs. In fact, we could modify the user distribution,
or the simulated users? behavior, and RL would op-
timize the system?s behavior automatically.
In this work, we contrasted dialogue length (DC)
against annoyance cost (AC) components. We found
that the AC and DC policies share the objective of
finding an amplitude setting within the user?s tol-
erance range because both incur stepwise costs for
intolerable utterances. But, AC policies further re-
fine this objective by incurring costs for tolerable,
but non-optimal, amplitudes as well. AC policies
are using information that is not explicitly commu-
nicated to the system, but which none-the-less RL
can use while learning a policy.
As this was exploratory work, the user model does
not yet fully reflect expected user behavior. For ex-
ample, as the system?s amplitude decreases, users
may misunderstand the system?s query or fail to re-
spond at all. In future work we will use an enhanced
user model that includes more natural user behavior.
In addition, because we wanted the system to focus
on learning a communication channel management
strategy, the domain task was fixed. In future work,
we will use RL to learn policies that both accom-
plish a more complex domain task, and model con-
nections between domain tasks and communication
channel management. Ultimately, we need to con-
duct user-testing to measure the efficacy of the com-
munication channel management policies. We feel
confident that learned policies trained using a com-
munication channel model which reflects the range
of users? abilities and preferences will prove effec-
tive for supporting all users.
60
References
Hua Ai, Joel R. Tetreault, and Diane J. Litman. 2007.
Comparing user simulation models for dialog strategy
learning. In NAACL-HLT, April.
Carryl L. Baldwin and David Struckman-Johnson. 2002.
Impact of speech presentation level on cognitive task
performance: implications for auditory display design.
Ergonomics, 45(1):62?74.
Carryl L. Baldwin. 2001. Impact of age-related hearing
impairment on cognitive task performance: evidence
for improving existing methodologies. In Human Fac-
tors and Ergonomics Society Annual Meeting; Aging,
pages 245?249.
Linda Bell, Joakim Gustafson, and Mattias Heldner.
2003. Prosodic adaptation in humancomputer inter-
action. In Proceedings of ICPhS 03, volume 1, pages
833?836.
Peter Heeman. 2007. Combining reinforcement learning
with information-state update rules. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics, pages
268?275, Rochester, NY, April.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid reinforcement/supervised learning of
dialogue policies from fixed data sets. Comput. Lin-
guist., 34(4):487?511.
J. C. Junqua. 1993. The lombard reflex and its role
on human listeners and automatic speech recogniz-
ers. The Journal of the Acoustical Society of America,
93(1):510?524, January.
Dan Jurafsky, Liz Shriberg, and Debra Biasca. 1997.
Switchboard: SWBD-DAMSL Coders Manual.
Blade Kotelly. 2003. The Art and Business of Speech
Recognition. Addison-Wesley, January.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1):11?23.
Bjorn Lindblom, 1990. Explaining phonetic variation: A
sketch of the H & H theory, pages 403?439. Kluwer
Academic Publishers.
Rebecca Lunsford, Sharon Oviatt, and Alexander M.
Arthur. 2006. Toward open-microphone engagement
for multiparty interactions. In Proceedings of the 8th
International Conference on Multimodal Interfaces,
pages 273?280, New York, NY, USA. ACM.
Eric Martinson and Derek Brock. 2007. Improv-
ing human-robot interaction through adaptation to the
auditory scene. In HRI ?07: Proceedings of the
ACM/IEEE international conference on Human-robot
interaction, pages 113?120, New York, NY, USA.
ACM.
K. L. Payton, R. M. Uchanski, and L. D. Braida. 1994.
Intelligibility of conversational and clear speech in
noise and reverberation for listeners with normal and
impaired hearing. The Journal of the Acoustical Soci-
ety of America, 95(3):1581?1592, March.
Harvey Sacks, Emanuel A. Schlegoff, and Gail Jefferson.
1974. A simplest sytsematic for the organization of
turn-taking for conversation. Language, 50(4):696?
735, December.
K. Scheffler and S. J. Young. 2002. Automatic learning
of dialogue strategy using dialogue simulation and re-
inforcement learning. In Proceedings of Human Lan-
guage Technology, pages 12?18, San Diego CA.
A. Stent, M. Huffman, and S. Brennan. 2008. Adapt-
ing speaking after evidence of misrecognition: Local
and global hyperarticulation. Speech Communication,
50(3):163?178, March.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction.
Jessica Villing, Cecilia Holtelius, Staffan Larsson, An-
ders Lindstro?m, Alexander Seward, and Nina Aaberg.
2008. Interruption, resumption and domain switching
in in-vehicle dialogue. In GoTAL ?08: Proceedings of
the 6th international conference on Advances in Natu-
ral Language Processing, pages 488?499, Berlin, Hei-
delberg. Springer-Verlag.
Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Aritificial
Intelligence Research, 12:387?416.
61
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 249?252,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Autism and Interactional Aspects of Dialogue
Peter A. Heeman, Rebecca Lunsford, Ethan Selfridge, Lois Black, and Jan van Santen
Center for Spoken Language Understanding
Oregon Health & Science University
heemanp@ohsu.edu
Abstract
Little research has been done to explore
differences in the interactional aspects of
dialogue between children with Autis-
tic Spectrum Disorder (ASD) and those
with typical development (TD). Quantify-
ing the differences could aid in diagnosing
ASD, understanding its nature, and better
understanding the mechanisms of dialogue
processing. In this paper, we report on a
study of dialogues with children with ASD
and TD. We find that the two groups differ
substantially in how long they pause be-
fore speaking, and their use of fillers, ac-
knowledgments, and discourse markers.
1 Introduction
Autism Spectrum Disorders (ASD) form a group
of severe neuropsychiatric conditions whose fea-
tures can include impairments in reciprocal social
interaction and in communication (APA, 2000).
These impairments may take different forms,
ranging from individuals with little or no com-
munication to fully verbal individuals with fluent,
grammatically correct speech. In this latter verbal
group, shortcomings in communication have been
noted, including using and processing social cues
during conversations. This is no surprise, since
negotiating a conversation requires many abilities,
several of which are generally impaired in ASD,
such as generating appropriate prosody (Kanner,
1943) and ?theory of mind? (Baron-Cohen, 2000).
We make a distinction between transactional
and interactional aspects of dialogue (Brown and
Yule, 1983). The transactional aspect refers to
message content and interactional focuses on ex-
pressing social relations and personal attitudes.
In this paper, we focus on surface behaviors
that speakers use to help manage the interaction,
namely turn-taking, and the use of fillers, dis-
course markers, and acknowledgments. One ad-
vantage of these behaviors is that they do not re-
quire complete understanding of the dialogue, and
thus lend themselves to automatic analysis. In
addition, these behaviors are under the speaker?s
control and should be robust to what the other
speaker is doing. We hypothesize that just as in-
teractional aspects in general are affected in ASD,
so are these surface behaviors. However, to our
knowledge, little or no work has been done on this.
Investigating how the interactional aspects of
dialogue are affected in ASD serves several pur-
poses. First, it can help in the diagnostic process.
Currently, diagnosing ASD is subjective. Objec-
tive measures based on dialogue interaction could
improve the reliability of the diagnostic process.
Second, it can help us refine the behavioral phe-
notypes of ASD, which is critical for progress on
the basic science front. Third, it can help us re-
fine therapy for people with ASD to address di-
alogue interaction deficits. Fourth, understand-
ing what dialogue aspects are affected in high-
functioning verbal children with ASD can help de-
termine which aspects of dialogue are primarily
social in nature. For example, do speakers use
fillers to signal that there is a communication prob-
lem, or are fillers a symptom of it (cf. Clark and
Fox Tree, 2002)?
In this paper, we report on a study of interac-
tional aspects of dialogues between clinicians and
children with ASD. The dialogues were recorded
during administration of the Autism Diagnostic
Observation Schedule (Lord et al, 2000), which
is an instrument used to assist in diagnosing ASD.
We compare the performance of these children
with a group of children with typical development
(TD).
2 Data
The data used in this paper was collected dur-
ing administration of the ADOS on 22 TD chil-
dren and 26 with ASD, ranging in age from 4 to
8 years old. The children with ASD were high-
functioning and verbal. The speech of the clini-
cian and child was transcribed into utterance-like
units, with a start and an end time. Activities were
annotated in a separate tier. The transcriptions in-
cluded the punctuation marks ?.?, ?!?, and ??? to
mark syntactically and semantically complete sen-
249
tences, and ?>? to mark incomplete ones. As a sin-
gle audio channel was used, the timing of overlap-
ping speech was marked as best as possible. Each
child on average said 2221 words, 574 utterances,
and 316 turns.
3 Results
Pauses between Turns: We first examine how
long children wait before starting their turn. We
hypothesized that children with ASD would wait
longer on average to respond, either because they
are less aware of (a) the turn-taking cues, (b) the
social obligation to minimize inter-turn pauses, or
(c) they have a slower processing and response
times. For this analysis, we look at all turns in
which there is no overlap between the beginning of
the child?s turn and the clinician?s speech. Data is
available on 4412 pauses for the TD children and
5676 for the children with ASD. The grand means
of the children?s pauses are shown in Table 1 along
with the standard deviations. The TD children?s
average pause length is 0.876s. For the children
with ASD, it is 1.115s, 27.3% longer. This dif-
ference is significant, a-priori independent t-test
t=2.34 (df=39), p<.02 one-tailed.
TD ASD
all 0.876 (0.24) 1.115 (0.45)
after question 0.748 (0.25) 1.005 (0.40)
after non-question 1.076 (0.37) 1.329 (0.74)
Table 1: Pauses before new turns.
We also examine the pauses following ques-
tions by the clinician versus non-questions. Ques-
tions are interesting as they impose a social obli-
gation for the child to respond, and they have
strong prosodic cues at their ending. We identified
questions as utterances transcribed with a ques-
tion mark, which might include rhetorical ques-
tions. After a non-question (e.g., a statement), the
average pause is 1.076s for the TD children and
1.329s for children with ASD. This difference is
not statistically significant by independent t-test,
t<1.6, NS. After a question, the average pause
is 0.748s for the TD children and 1.005s for the
children with ASD, a significant difference by a-
priori independent t-test t=2.72 (df=42), p<.005
one-tailed. The ASD children on average take
34.4% longer to respond. Thus, after a question,
the difference between children with TD and ASD
is more pronounced.
Pauses by Activity: The ADOS includes hav-
ing the child engage in different activities. For
this research, we collapse the activities into three
types: converse is when there is no non-speech
task; describe is when the child is doing a men-
tal task, such as describing a picture; and play is
when the child is interacting with the clinician in
a play session. To better understand the difference
between questions and non-questions, we examine
the pauses in each activity (Table 2).
TD ASD
question non-ques. question non-ques.
converse 0.730 0.30 0.656 0.27 0.890 0.34 0.932 0.88
describe 0.853 0.44 0.879 0.37 1.056 0.51 1.282 1.21
play 0.720 0.34 1.825 0.78 1.289 1.51 1.887 1.37
Table 2: Pauses for each type of activity.
After a question, the TD children tend to re-
spond with similar pauses in each activity (the dif-
ferences in column 2 between activities are not
significant by pairwise paired t-test, all t?s<1.6,
NS). After a question, the child has a social obli-
gation to respond, and this does not seem to be
overridden by whether there is a separate task
they are involved in. Even after a non-question,
conversants have a social obligation to keep the
speaking floor occupied and so to minimize inter-
utterance pauses (Sacks et al, 1974). However, as
seen in the third column, the pauses are affected
by the type of activity, and the differences are
statistically significant by pairwise paired t-test,
(df=21), two-tailed: converse-describe t=2.24,
p<.04; describe-play t=5.68, p<.0001; converse-
play t=6.87, p<.0001. The biggest difference is
with play. Here, it seems that the conversants
physical interaction lessens the social obligation
of maintaining the speaking floor. These findings
are interesting for social-linguistics as it suggests
that the social obligations of turn-taking are al-
tered by the presence of a non-speech task.
We next compare the children with ASD to the
TD children. For the converse activity, we see that
the children with ASD take longer to respond, af-
ter questions and non-questions. The difference
after questions is significant by independent t-test,
t=1.74 (df=46) p<.05, one-tailed, whereas the dif-
ference after non-questions is marginal, t=1.47
(df=28) p<.08. This result could be explained by
the slower processing and response times associ-
ated with ASD.
Just as with the TD children, we see that after
a non-question, the children with ASD take longer
to respond when there is another task. The differ-
ences in pause lengths between converse and play
are significant, by paired t-test, t=2.89 (df=23)
250
p<.009, two-tailed. The difference between de-
scribe and play is marginal, t=2.03 (df=25) p<.06,
and there was no significant difference between
converse and describe, t<1, NS.
After a question, the children with ASD take
longer to respond when there is another task, espe-
cially for play, although the pairwise differences in
pause length between activities are not significant.
This suggests that the children with ASD become
distracted when there is another task, and so be-
come less sensitive to either the question prosody
or the social obligation of questions.
Fillers: We next examine the rate of fillers, at
the beginning of turns, beginning of utterances,
and in the middle of utterances. We look at these
contexts individually as fillers can serve different
roles, such as turn-taking, stalling for time or as
part of a disfluency, and their role is correlated
to their position in a turn. The rates are reported
in Table 3, along with the total number of fillers
within each category. Interestingly, the rate of ?uh?
between children with TD and ASD is similar for
all positions (independent t-test, all g?s<1, NS).
uh um
TD ASD TD ASD
turn init. 1.70% 112 1.84% 159 3.86% 243 1.65% 146
utt. init. 1.31% 43 1.20% 33 2.29% 73 0.52% 10
utt. medial 0.25% 103 0.31% 137 1.03% 492 0.21% 123
Table 3: Rate of fillers.
The more interesting finding, though, is in the
usage of ?um?. Children with ASD use it signifi-
cantly less than the TD children in every position,
from 1/2 the rate in turn-initial position to 1/5 in
utterance-medial position, independent two-tailed
t-test: turn initial t=2.74 (df=38), p<.01; utterance
initial t=2.53 (df=31), p<.02; and utterance me-
dial t=3.94 (df=24), p<.001.
TD ASD
converse 1.76% 569 0.56% 190
describe 1.15% 115 0.33% 31
play 0.96% 124 0.45% 58
Table 4: Use of ?um? by activity.
We also examined the overall usage of ?um? in
each activity (Table 4). The TD children use ?um?
more often in each activity than the children with
ASD, and the differences are statistically signif-
icant by independent two-tailed t-test: converse
t=3.62 (df=29), p<.002; describe t=2.83 (df=27),
p<.01; play t=2.42 (df=33), p<.03. This result
supports the robustness of the findings about ?um?.
Many researchers have speculated on the role
of ?um? and ?uh?. In recent work, Clark and Fox
Tree (2002) argued that they signal a delay, and
that ?um? signals more delay than ?uh?. They view
both as linguistic devices that are planned for, just
as any other word is. Our work suggests that ?um?
and ?uh? arise from different cognitive processes,
and that the process that accounts for ?uh? is not
affected by ASD, while the process for ?um? is.1
Acknowledgments: We next look at the rate of
acknowledgments: single word utterances that are
used to show agreement or understanding. Thus,
the use of acknowledgments requires awareness of
the other person?s desire to ensure mutual under-
standing. As the corpus did not have these words
explicitly marked, we identify a word as an ac-
knowledgment if it meets the following criteria:
(a) it is one of the words listed in Table 5 (based
on Heeman and Allen, 1999); (b) it is first in the
speaker?s turn; and (c) it does not follow a question
by the clinician. The TD children used acknowl-
edgments in 17.42% of their turns that did not fol-
low a question, while the children with ASD did
this only 13.39% of the time (Table 5), a statis-
tically significant difference by a-priori indepen-
dent t-test t=1.78 (df=46), p<.05 one-tailed.
TD ASD
total 17.42% 568 13.39% 459
yeah 7.49% 248 5.87% 215
no 2.78% 78 2.06% 63
mm-hmm 2.06% 75 1.07% 35
mm 0.99% 29 1.35% 42
ok 1.87% 65 0.83% 27
yes 0.92% 32 0.88% 32
right 0.14% 5 0.23% 8
hm 0.73% 21 0.69% 20
uh-huh 0.44% 15 0.42% 17
Table 5: Use of acknowledgments.
Discourse Markers: We next examine dis-
course markers, which are words such as ?well?
and ?oh? that express how the current utterance
relates to the discourse context (Schiffrin, 1987).
We classified a word as a discourse marker if it
was the first word in an utterance and is one of
the words in Table 6 (Heeman and Allen, 1999).
As shown in Table 6, the children with ASD use
discourse markers significantly less than the TD
children in both conditions by a-priori indepen-
dent, one-tailed t-test: turn-initial t=3.24 (df=43)
p<.002; utterance-initial t=4.01 (df=44) p<.0001.
1In Lunsford et al (2010) we investigate the rate and
length of pauses after ?uh? and ?um?. In addition, we veri-
fied the t-tests using Wilcoxon rank sum tests.
251
As can be seen, most of the difference is in the use
of ?and?. The data for the other discourse markers
was sparse, so we compared ?and? against all of the
others combined. The decreased usage of ?and?
in the ASD children is statistically significant
for both conditions by a-priori independent, one-
tailed t-test: turn-initial t=4.47 (df=30), p<.0001;
utterance-initial t=3.79 (df=43), p<.0002. There
is little difference in the use of all of the other dis-
course markers combined, and the difference is not
statistically significant.
Turn Initial Utterance Initial
TD ASD TD ASD
all 19.2% 1290 12.8% 1196 28.7% 2053 19.4% 1330
and 10.7% 731 5.0% 471 19.5% 1419 12.0% 844
then 0.6% 38 1.0% 89 1.5% 97 1.4% 79
but 2.1% 144 1.3% 113 3.6% 238 2.7% 194
well 2.2% 143 2.7% 271 1.1% 74 1.2% 79
oh 2.0% 135 1.8% 160 1.0% 67 1.3% 68
so 1.2% 75 0.7% 60 1.6% 129 0.7% 49
wait 0.2% 9 0.2% 21 0.2% 17 0.2% 15
actually 0.2% 15 0.1% 11 0.2% 12 0.0% 2
not and 8.5% 559 7.8% 725 9.2% 634 7.4% 486
Table 6: Use of discourse markers.
The use of ?and? is also lower in each activity
for the ASD children (Table 7), a significant dif-
ference by a-priori independent one-tailed t-test:
converse t=3.00 (df=41), p<.003; describe t=4.79
(df=38), p<.0001, play t=4.07 (df=30), p<.0002.
TD ASD
converse 13.36% 1139 7.95% 755
describe 21.77% 587 10.76% 339
play 12.97% 424 5.18% 221
Table 7: Use of ?and? in each activity.
One explanation for the decreased usage of
?and? and not the other discourse markers might
be that, of all the discourse markers, ?and? seems
to have the least meaning. It simply signifies
that there is some continuation between the new
speech and the previous context. This might make
it difficult for children with ASD to learn its use. A
second explanation is that the children with ASD
are using ?and? correctly, but simply do not pro-
duce as many utterances that are related to the pre-
vious context (cf. Bishop et al, 2000).
4 Conclusion
In this paper, we examined a number of interac-
tional aspects of dialogue in the speech of children
with ASD and TD. We found that children with
ASD have a lower rate of the filler ?um?, acknowl-
edgments, and the discourse marker ?and.? We also
found that in certain situations, they take longer to
respond. These deficits might prove useful for im-
proved diagnosis of ASD. We also found that chil-
dren with ASD have a lower rate of ?um? but not
of ?uh?, and that only the discourse marker ?and?
seems to be affected. This might prove useful for
both better understanding the nature of ASD as
well as better understanding the role of these phe-
nomena in dialogue. Although the results reported
in this work are preliminary, they do show the po-
tential of our approach. More work is needed to
ensure that our automatic identification of turn-
taking events, discourse markers, and acknowl-
edgments is correct and to explore alternate expla-
nations for the results that we observed.
Acknowledgments
Funding gratefully received from the National In-
stitute of Heath under grants IR21DC010239 and
5R01DC007129, and the National Science Foun-
dation under IIS-0713698. The views herein are
those of the authors and reflect the views neither
of the funding agencies.
References
American Psychiatric Association, Washington DC,
2000. Diagnostic and Statistical Manual of Mental
Disorders, 4th Edition, Text Revision (DSM-IV-TR).
S. Baron-Cohen. 2000. Theory of mind and autism:
A review. In L. M. Glidden, editor, International
Review of Research in Mental Retardation, volume
23: Autism, pages 170?184. Academic Press.
D. Bishop et al 2000. Conversational responsive-
ness in specific language impairment: Evidence of
disproportionate pragmatic difficulties in a subset
of children. Development and Psychopathology,
12(2):177?199.
G. Brown and G. Yule. 1983. Discourse Analysis.
Cambridge University Press.
H. Clark and J. Fox Tree. 2002. Using uh and um in
spontaneous speaking. Cognition, 8:73?111.
P. Heeman and J. Allen. 1999. Speech repairs, in-
tonational phrases and discourse markers: Model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?572.
L. Kanner. 1943. Autistic disturbances of affective
content. Nervous Child, 2:217?250.
C. Lord et al 2000. The autism diagnostic observa-
tion schedule-generic: a standard measure of social
and communication deficits associted with the spec-
trum of autium. Journal of Austism Developmental
Disorders, 30(3):205?223, June.
R. Lunsford et al 2010. Autism and the use of fillers:
differences between ?um? and ?uh?. In 5th Workshop
on Disfluency in Spontaneous Speech, Tokyo.
H. Sacks, E. Schegloff, and G. Jefferson. 1974. A sim-
plest systematics for the organization of turn-taking
for conversation. Language, 50(4):696?735.
D. Schiffrin. 1987. Discourse Markers. Cambridge
University Press, New York.
252
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 110?119,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Stability and Accuracy in Incremental Speech Recognition
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ
{selfridg,heemanp}@ohsu.edu {iker,jdw}@research.att.com
Abstract
Conventional speech recognition ap-
proaches usually wait until the user
has finished talking before returning a
recognition hypothesis. This results in
spoken dialogue systems that are unable
to react while the user is still speaking.
Incremental Speech Recognition (ISR),
where partial phrase results are returned
during user speech, has been used to
create more reactive systems. However,
ISR output is unstable and so prone to
revision as more speech is decoded. This
paper tackles the problem of stability
in ISR. We first present a method that
increases the stability and accuracy of
ISR output, without adding delay. Given
that some revisions are unavoidable,
we next present a pair of methods for
predicting the stability and accuracy of
ISR results. Taken together, we believe
these approaches give ISR more utility for
real spoken dialogue systems.
1 Introduction
Incremental Speech Recognition (ISR) enables a
spoken dialogue system (SDS) to react quicker
than when using conventional speech recogni-
tion approaches. Where conventional methods
only return a result after some indication of user
completion (for example, a short period of si-
lence), ISR returns partial phrase results while
the user is still speaking. Having access to a real-
time stream of user speech enables more natural
behavior by a SDS, and is a foundation for cre-
ating systems which take a more active role in
conversations.
Research by Fink et al(1998) and Skantze
& Schlangen (2009), among others, has demon-
strated the efficacy of ISR but has also drawn
attention to a significant obstacle to widespread
use: partial phrase results are generally unsta-
ble and so, as more speech is decoded, are prone
to revision. For example, the ISR component in
a bus information SDS may return the partial
?leaving from Hills?, where ?Hills? is a neigh-
borhood name. It may then return the revi-
sion ?leaving from Pittsburgh?, which the sys-
tem must handle gracefully. Given this propen-
sity to revise, a Stability Measure (SM) ? like-
lihood of a partial result remaining unchanged
compared to the final result ? is necessary for
optimal incremental system behavior. Further-
more, since a stable partial may still be inaccu-
rate, a Confidence Measure (CM) ? likelihood
of partial correctness ? is also necessary.
Effective ISR enables systems to participate in
more dynamic turn-taking. For instance, these
two measures would enable an SDS to identify
inaccurate recognition results while the user is
still speaking. The SDS could then interrupt
and prompt the user to start again. On the
other hand, ISR allows systems to handle pauses
gracefully. If the SDS recognizes that an utter-
ance is incomplete (though stable and accurate),
it could give the user more time to speak before
reacting.
We present two contributions specific to the
use of ISR. First, we characterize three ap-
proaches to ISR which make different trade-offs
between stability and the number of partials
generated. We then present a novel hybrid ap-
proach that combines their strengths to increase
110
stability without adding latency. However, even
with this method, some partial results are still
later revised. The second contribution of the
paper is to present a pair of methods which pre-
dict the stability and accuracy of each partial
result. These two measures are designed for use
in concert by dialogue systems, which must de-
cide whether to act on each partial result in real
time.
2 Background and Related Work
We now describe modern speech recognition
methodology, the production of partial phrase
results, and the advantages and deficiencies of
ISR. In this we seek only to provide a topical
foundation, and not a comprehensive review.
Most modern speech recognition engines use
Hidden-Markov Models and the Viterbi algo-
rithm to decode words from audio. Decod-
ing employs three models: an acoustic model,
which assigns probabilities to speech audio given
a phone; a lexicon, which specifies phone se-
quences for a word; and a language model, which
specifies the probability of a word sequence. The
aim of the decoding process is to find the N most
probable word sequences given the audio spoken
and these three models.
Two useful but different forms of language
models are commonly used in spoken dialogue
systems. A Rule-based Language Model (RLM)
specifies a list of valid sentences which may be
recognized, usually via expansion rules. By con-
trast, a Statistical Language Model (SLM) spec-
ifies a vocabulary of words, allowing arbitrary
sentences to be formed. Both models specify
probabilities over their respective sets ? RLMs
via whole-sentence probabilities, and SLMs via
probabilities of short word sequences called N-
grams. In an SLM, special word symbols are
used to represent the beginning and end of the
phrase, so the probability of beginning or ending
phrases with words can be modeled.
As speech frames are received, the recognizer
builds up a lattice which compactly describes the
probable sequences of words decoded from the
audio. In conventional turn-based speech recog-
nition, decoding continues until the user finishes
speaking. Once the user has finished, the engine
searches the lattice for the most probable word
sequence and returns this to the dialogue man-
ager. By contrast, in ISR the engine inspects
the lattice as it is being built, and returns partial
results to the dialogue manager as they become
available. A key issue for ISR is that partial
results may later be revised, because as more
speech is received and the lattice is extended, a
different path may become the most probable.
In other words, partial results are unstable in
the sense that they may later be revised. Note
that stability is not the same as accuracy: a par-
tial result may be accurate (correct so far) but
unstable, because it is later revised. Similarly, a
stable result may not be accurate.
In the literature, ISR has been proposed for
dialogue systems to enable them to engage in
more natural, human-like interactions. Stud-
ies have shown that incremental systems react
faster than non-incremental ones, and are well-
liked by users because of their naturalness (Aist
et al, 2007; Skantze and Schlangen, 2009). Aist
et al (2007) found that incremental speech
recognition yielded 20% faster task completion.
Moreover, adding ISR improved users? satisfac-
tion with the interaction; the authors attributed
this improvement to ?naturalness?: ?incremen-
tal systems are more like human-human con-
versation than their non-incremental counter-
parts.? Skantze & Schlangen (2009) observed a
similar trend, finding that an incremental sys-
tem was ?clearly preferred? since it ?was ex-
perienced as more pleasant and human-like?,
though it did not actually outperform the non-
incremental system in a number dictation task.
Some recent work has focused on incremen-
tal natural language understanding (NLU). De-
Vault et al (2009) showed that when using a
relatively small number of semantic possibili-
ties the correct interpretation could be predicted
by early incremental results. Schlangen et al
(2009) demonstrated that an incremental refer-
ence resolver could identify the correct reference
out of 12 more than 50% of the time. This
type of NLU can use context and other infor-
mation to be somewhat resilient to errors, and
word recognition inaccuracies may not yield a
111
change in understanding. In this paper we focus
on improving accuracy and stability at the word
level; we belief that improvements at the word
level are likely to improve performance at the
understanding level, although we do not evalu-
ate this here.
A number of researchers have described meth-
ods for evaluating and improving the stability of
ISR results (Baumann et al, 2009; Fink et al,
1998). Baumann, Atterer, & Schlangen spoke
directly to stability by comparing partial phrase
results against the ?final hypothesis produced
by the ASR?. They show that increasing the
amount of ?right context? ? the amount of
speech after the end of the putative partial result
? increases the stability of the partials. Fink et
al. (1998) also used a right context delay to de-
crease the word error rate of ISR results.
A key limitation of these past efforts to im-
prove stability is that adding right context nec-
essarily incurs delay, which degrades responsive-
ness and erodes the overall benefits of ISR. Fur-
thermore, past work has not addressed the prob-
lem of identifying which partials are likely to be
revised. In this paper, we tackle both of these
problems. We first present a method for im-
proving stability by considering features of the
lattice itself, without incurring the delay asso-
ciated with adding right context. Additionally,
since some partials will still be revised, we then
propose a method of scoring the stability of par-
tial speech recognition results.
3 Three approaches to ISR
We now describe three approaches to ISR: Ba-
sic, Terminal, and Immortal. Basic ISR simply
returns the most likely word sequence observed
after some number of speech frames has been de-
coded (in our case every 3 frames or 30ms). This
is the least restrictive approach, and we believe
is the method used by recent ISR research.
Terminal ISR, a more restrictive approach,
finds a partial result if the most likely path
through the (partially-decoded) lattice ends at
a terminal node in the language model. The in-
tuition is that if a partial result finishes a com-
plete phrase expected by the language model,
it is more likely to be stable. The meaning of
terminal is slightly different for rule-based lan-
guage models (RLMs) and statistical language
models (SLMs). For a rule-based grammar,
the terminal node is simply one that ends a
valid phrase (?Pittsburgh? in ?leaving from Pitts-
burgh?). For an SLM, a terminal node indicates
that the most likely successor state is the spe-
cial end-of-sentence symbol. In other words, in
an SLM Terminal partial result, the language
model assigns the highest probability to ending
the phrase.
A third method, Immortal ISR, is the most
restrictive method (Spohrer et al, 1980). If all
paths of the lattice come together into a node
? called an immortal node ? then the lattice
structure before that node will be unchanged by
any subsequent decoding. This structure guar-
antees that the best word sequence prior to an
immortal node is stable. Immortal ISR operates
identically for both RLMs and SLMs.1
To compare these approaches we evaluate
their performance. Utterances were extracted
from real calls to the Carnegie Mellon ?Lets
Go!? bus information system for Pittsburgh,
USA (Raux et al, 2005; Parent and Eskenazi,
2009). We chose this domain because this cor-
pus is publicly available, and this domain has
recently been used as a test bed for dialogue
systems (Black et al , 2010). The AT&T WAT-
SON speech recognition engine was used, modi-
fied to output partials as described above (Goffin
et al, 2005). We tested these three approaches
to ISR on three different recognition tasks. The
first two tasks used rule-based language models
(RLM), and the third used a statistical language
model (SLM).
The two rule-based language models were de-
veloped for AT&T ?Let?s Go? dialogue sys-
tem, prior to its deployment (Williams et al
, 2010). The first RLM (RLM1) consisted
1The choice of search beam size affects both accuracy
and the number of immortal nodes produced: a smaller
beams yields a sparser lattice with more immortal nodes
and lower accuracy; a larger beam yields a richer lattice
with fewer immortal nodes and higher accuracy. In this
work we used our recognizer?s default beam size, which
allows recognition to run in less than real time and yields
near-asymptotic accuracy for all experiments.
112
of street and neighborhood names, built from
the bus timetable database. The second RLM
(RLM2) consisted of just neighborhood names.
Utterances to test RLM1 and RLM2 were se-
lected from the corpus provided by Carnegie
Mellon to match the expected distribution of
speech at the dialogue states where RLM1 and
RLM2 would be used. RLM1 was evaluated on
a set of 7722 utterances, and RLM2 on 5411 ut-
terances. To simulate realistic use, both RLM
test sets were built so that 80% of utterances
are in-grammar, and 20% are out-of-grammar.
The SLM was a 3-gram trained on a set of 140K
utterances, and is tested on a set of 42620 ut-
terances.
In past work, Raux et al (2005) report word
error rates (WERs) of 60-68% on data from the
same dialogue system, though on a different set
of utterances. By comparison, our SLM yields
a WER of 35%, which gives us some confidence
that our overall recognition accuracy is compet-
itive, and that our results are relevant.
Table 1 provides a few statistics of the LMs
and test sets, including whole-utterance accu-
racy, computed using an exact string match.
Results are analyzed in two groups: All, where
all of the utterances are analyzed, and Multi-
Word (MW), where only utterances whose tran-
scribed speech (what was actually said) has
more than one word. Intuitively, these utter-
ances are where ISR would be most effective.
That said, ISR is beneficial for both short and
long utterances ? for example, ISR systems
can react faster to users regardless of utterance
length.
ISR was run using each of the three ap-
proaches (Basic, Terminal, Immortal) in each of
the three configurations (RLM1, RLM2, SLM).
The mean number of partials per utterance is
shown in Table 2. For all ISR methods, the more
flexible SLM produces more partials than the
RLMs. Also as expected, multi-word utterances
produce substantially more partials per utter-
ance than when looking at the entire utterance
set. The Basic approach produces nearly dou-
ble the number of partials than Terminal ISR
does, and Immortal ISR production highlights
its primary weakness: in many utterances, no
Table 1: Statistics for Recognition Tasks. In all ta-
bles, All refers to all utterances in a test set, and
MW refers to the subset of multi-word utterances in
a test set.
RLM1 RLM2 SLM
Num. Utts All 7722 5411 42620
Num. Utts MW 3213 1748 20396
Words/Utt All 1.7 1.5 2.3
Words/Utt MW 2.8 2.6 3.8
Utt. Acc. All. 50 % 60 % 62 %
Utt. Acc. MW 53 % 56 % 44 %
immortal nodes are found. Given this however,
immortal node occurrence is directly related to
the number of words, as indicted by the greater
number of immortal partials in multi-word ut-
terances.
Stability is assessed by comparing the partial
to the final recognition result. For simplicity, we
restrict our analysis to 1-Best hypotheses. If the
partial 1-Best hypothesis is a prefix (or full ex-
act match) of the final 1-Best hypothesis then it
is considered stable. For instance, if the partial
1-Best hypothesis is ?leaving from Forbes? then
it would be stable if the final 1-Best is ?leaving
from Forbes? or ?leaving from Forbes and Mur-
ray? but not if it is ?from Forbes and Murray? or
?leaving?. Accuracy is assessed similarly except
that the transcribed reference is used instead of
the final recognition result.
We report stability and accuracy in Table 3.
Immortal partials are excluded from stability
since they are guaranteed to be stable. The first
four rows report stability, and the second six
report accuracy. The results show that Termi-
nal Partials are relatively unstable, with 23%-
Table 2: Average Number of Partials per utterance
ISR Group RLM1 RLM2 SLM
Basic All 12.0 9.9 11.6MW 14.6 12.3 29.7
Terminal All 5.4 3.3 6.2MW 6.4 4.1 8.8
Immortal All 0.22 0.32 0.55MW 0.42 0.67 0.63
113
Table 3: Stability and Accuracy Percentages
ISR Group RLM1 RLM2 SLM
Stability
Basic All 10 % 11 % 7 %MW 14 % 15 % 9 %
Terminal All 23 % 31 % 37 %MW 20 % 28 % 36 %
Accuracy
Basic All 9 % 1 % 5 %MW 11 % 13 % 6 %
Terminal All 13 % 21 % 24 %MW 12 % 17 % 21 %
Immortal All 91 % 93 % 55 %MW 90 % 90 % 56 %
37% of partials being stable, and that their sta-
bility drops off when looking at multi-word ut-
terances. SLM stability seems to be somewhat
higher than that of the RLM. Basic partials
are even more unstable (about 10% of partials
are stable), with extremely low stability for the
SLM. Unlike Terminal ISR, their stability grows
when only multi-word utterances are analyzed,
though the maximum is still quite low.
The results also show that partials are always
less accurate than they are stable, indicating
that not all stable partials are accurate. Immor-
tal partials are rare, but when they are found,
they are much more accurate than Terminal or
Basic partials. The RLM accuracy is very high,
and we suspect that immortal nodes are corre-
lated with utterances which are easier to recog-
nize. Terminal ISR is far more accurate than
Basic ISR for all of the utterances, but its im-
provement declines for multi-word RLMs.
We have shown three types of ISR: Basic, Ter-
minal and Immortal ISR. While Basic and Ter-
minal ISR are both highly productive, Terminal
ISR is far more stable and accurate than Basic.
Furthermore, there are far more Basic partials
than Terminal partials, implying that the dia-
logue manager would have to handle more un-
stable and inaccurate partials more often. Given
this, Terminal ISR is a far better ?productive
ISR? than the Basic method. Taking produc-
tion and stability together, there is a double dis-
Table 4: Lattice-Aware ISR (LAISR) Example
1-best Partial Type
yew Terminal
sarah Terminal
baum Terminal
dallas Terminal
downtown Terminal
downtown Immortal
downtown pittsburgh Terminal
downtown pittsburgh Immortal
sociation between Terminal and Immortal ISR.
Terminal partials are over produced and rela-
tively unstable. Furthermore, they are even less
stable when the transcribed reference is greater
than one word. On the other hand, Immortal
partials are stable and quite accurate, but too
rare for use alone. By integrating the Immortal
Partials with the Terminal ones, we may be able
to increase the stability and accuracy overall.
4 Lattice-Aware ISR (LAISR)
We introduce Lattice-Aware ISR (LAISR ?
pronounced ?laser?), that integrates Terminal
and Immortal ISR by allowing both types of par-
tials to be found. The selection procedure works
by first checking for an Immortal partial. If one
is not found then it looks for a Terminal. Re-
dundant partials are returned when the partial
type changes. An example recognition is shown
in Table 4. Notice how the first four partials
are completely unstable. This is very common,
and suppressing this noise is one of the primary
benefits of using more right context. Basic ISR
has even more of this type of noise.
LAISR was evaluated on the three recogni-
tion tasks described above (see Table 5). The
first two rows show the average number of par-
tials per utterance for each task and utterance
group. Unsurprisingly, these numbers are quite
similar to Terminal ISR. The stability percent-
age of LAISR is shown in the second two rows.
For all the utterances, there appears to be a very
slight improvement when compared to Termi-
nal ISR in Table 3. The improvement increases
for MW utterances, with LAISR improving over
114
Table 5: Lattice-Aware ISR Stats
Partials per Utterance
RLM1 RLM2 SLM
All 5.6 3.5 6.7
MW 6.7 4.5 9.6
Stability Percentage
All 24 % 33 % 40 %
MW 24 % 35 % 41 %
Accuracy Percentage
All 15 % 23 % 26 %
MW 16 % 22 % 24 %
Terminal ISR by 4?7 percentage points. This
is primarily because there is a higher occur-
rence of Immortal partials as the utterance gets
longer. Accuracy is reported in the final two
rows. Like the previous ISR methods described,
the accuracy percentage is lower than the sta-
bility percentage. When compared to Terminal
ISR, LAISR accuracy is slightly higher, which
confirms the benefit of incorporating immortal
partials with their relatively high accuracy. To
be useful in practice, it is important to exam-
ine when in the utterance ISR results are be-
ing produced. For example, if most of the par-
tials are returned towards the end of utterances,
than ISR is of little value over standard turn-
based recognition. Figure 1 shows the percent
of partials returned from the start of speech to
the final partial for MW utterances using the
SLM. This figure shows that partials are re-
turned rather evenly over the duration of ut-
terances. For example, in the first 10% of dura-
tion of each utterance, about 10% of all partial
results are returned. Figure 1 also reports the
stability and accuracy of the partials returned.
These numbers grow as decoding progresses, but
shows that mid-utterance results do yield rea-
sonable accuracy: partials returned in the mid-
dle of utterances (50%-60% duration) have an
accuracy of near 30%, compared to final partials
47% percent.
For use in a real-time dialogue system, it is
also important to assess latency. Here we define
latency as the difference in (real-world) time be-
tween (1) when the recognizer receives the last
Figure 1: Percent of LAISR partials returned from
the start of detected speech to the final partial using
the SLM. The percentage of partials returned that
are stable/accurate are also shown.
frame of audio for a segment of speech, and (2)
when the partial that covers that segment of
speech is returned from the recognizer. Mea-
suring latencies of LAISR on each task, we find
that RLM1 has a median of 0.26 seconds and a
mean of 0.41s; RLM2 has a median of 0.60s and
a mean of 1.48s; and SLM has a median of 1.04s
and a mean of 2.10s. Since reducing latency
was not the focus on this work, no speed opti-
mizations have been made, and we believe that
straightforward optimization can reduce these
latencies. For example, on the SLM, simply
turning off N-Best processing reduces the me-
dian latency to 0.55s and the mean to 0.79s.
Human reaction time to speech is roughly 0.20
seconds (Fry, 1975), so even without optimiza-
tion the RLM latencies are not far off human
performance.
In sum, LAISR produces a steady stream
of partials with relatively low latency over the
course of recognition. LAISR has higher stabil-
ity and accuracy than Terminal ISR, but its par-
tials are still quite unstable and inaccurate. This
means that in practice, dialogue systems will
need to make important decisions about which
partials to use, and which to discard. This need
motivated us to devise techniques for predicting
when a partial is stable, and when it is accurate,
which we address next.
115
Table 6: Equal Error Rates: Significant improvements in bold. Basic at p < 0.016, Terminal at p < 0.002,
and LAISR at p < 0.00001
All Multi-Word
Stability Measure (SM) Equal Error Rate
RLM 1 RLM 2 SLM RLM 1 RLM 2 SLM
Basic WATSON Score 13.3 13.3 12.8 15.6 16.4 15.2Regression 10.7 11.3 12.3 13.2 15.2 15.1
Terminal WATSON Score 24.3 29.1 34.4 26.6 26.0 34.1Regression 19.7 26.5 26.5 23.0 24.3 24.7
LAISR WATSON Score 24.7 29.3 35.0 24.0 27.0 35.3Regression 19.2 25.6 25.0 18.4 23.3 22.7
Confidence Measure (CM) Equal Error Rate
Basic WATSON Score 11.3 11.7 9.9 14.1 14.0 11.6Regression 9.8 9.8 9.7 12.3 12.9 11.0
Terminal WATSON Score 15.1 21.1 30.6 15.7 17.4 29.3Regression 11.7 16.8 20.8 12.1 14.5 18.4
LAISR WATSON Score 15.8 21.8 32.3 18.4 19.5 31.8Regression 11.6 16.6 21.0 11.6 14.2 18.7
5 Stability and Confidence Measures
As seen in the previous section, partial speech
recognition results are often revised and inaccu-
rate. In order for a dialogue system to make
use of partial results, measures of both stability
and confidence are crucial. A Stability Measure
(SM) predicts whether the current partial is a
prefix or complete match of the final recogni-
tion result (regardless of whether the final result
is accurate). A Confidence Measure (CM) pre-
dicts whether the current partial is a prefix or
complete match of what the user actually said.
Both are useful in real systems: for example, if
a partial is likely stable but unlikely correct, the
system might interrupt the user and ask them
to start again.
We use logistic regression to learn separate
classifiers for SM and CM. Logistic regression is
appealing because it is well-calibrated, and has
shown good performance for whole-utterance
confidence measures (Williams and Balakrish-
nan, 2009). For this, we use the BXR pack-
age with default settings (Genkin et al, 2011).
For Terminal and Basic ISR we use 11 features:
the raw WATSON confidence score, the individ-
ual features which affect the confidence score,
the normalized cost, the normalized speech like-
lihood, the likelihoods of competing models,
the best path score of word confusion network
(WCN), the length of WCN, the worst probabil-
ity in the WCN, and the length of N-best list.
For LAISR, four additional features are used:
three binary indicators of whether the partial is
Terminal, Immortal or a Terminal following an
Immortal, and one which gives the percentage
of words in the hypothesis that are immortal.
We built stability and confidence measures for
Basic ISR, Terminal ISR, and LAISR. Each of
the three corpora (RLM1, RLM2, SLM) was di-
vided in half to form a train set and test set.
Regression models were trained on all utter-
ances in the train set. The resulting models were
then evaluated on both All and MW utterances.
As a baseline for both measures, we compare
to AT&T WATSON?s existing confidence score.
This score is used in numerous deployed com-
mercial applications, so we believe it is a fair
baseline. Although the existing confidence score
is designed to predict accuracy (not stability),
there is no other existing mechanism for pre-
dicting stability.
We first report ?equal error rate? for the mea-
sures (Table 6). Equal error rate (EER) is the
sum of false accepts and false rejects at the rejec-
116
Figure 2: True accept percentages for stability measure (a) and confidence measure (b), using a fixed false
accept rate of 5%. LAISR yields highest true accept rates, with p < 0.0001 in all cases.
(a) Stability measure (b) Confidence measure
tion threshold for which false accepts and false
rejects are equal. Equal error rate is a widely
used metric to evaluate the quality of scoring
models used for accept/reject decisions. A per-
fect scoring model would yield an EER of 0. For
statistical significance we use ?2 contingency ta-
bles with 1 degree of freedom. It is inappropri-
ate to compare EER across ISR methods, since
the total percentage of stable or accurate par-
tials significantly effects the EER. For example,
Basic ISR has relatively low EER, but this is
because it also has a relatively low number of
stable or accurate partials.
The top six rows of Table 6 show EER for the
Stability Measure (SM). The left three columns
show results on the entire test set (all utterances,
of any length). On the whole, the SM outper-
forms the WATSON confidence scores, and the
greatest improvement is a 10.0 point reduction
in EER for LAISR on the SLM task. The right
three columns show results on only multi-word
(MW) utterances. Performance is similar to the
entire test set, with a maximum EER reduction
of 12.6 percent. The SLM MW performance is
interesting, suggesting that it is easier to pre-
dict stability after at least one word has been
decoded, possibly due to higher probability of
immortal nodes occurring. This suggests there
would be benefit in combining our method with
past work that adds right-context, perhaps us-
ing more context early in the utterance. This
idea is left for future work.
The bottom six rows show results for the Con-
fidence Measure (CM). We see that that even
when comparing our CM against the WATSON
confidence scores, there is significant improve-
ment, with a maximum of 13.1 for LAISR in the
MW SLM task.
The consistent improvement shows that logis-
tic regression is an effective technique for learn-
ing confidence and stability measures. It is most
powerful when combined with LAISR, and only
slightly less so with Terminal. Furthermore,
though the gains are slight, it is also useful with
Basic ISR, which speaks to the generality of the
approach.
While equal error rate is useful for evaluating
discriminative ability, when building an actual
system a designer would be interested to know
how often the correct partial is accepted. To
evaluate this, we assumed a fixed false-accept
rate of 5%, and report the resulting percentage
of partials which are correctly accepted (true-
accepts). Results are shown in Figure 1. LAISR
accepts substantially more correct partials than
other methods, indicating that LAISR would be
more useful in practice. This result also shows
a synergy between LAISR and our regression-
based stability and confidence measures: not
only does LAISR improve the fraction of stable
117
and correct partials, but the regression is able
to identify them better than for Terminal ISR.
We believe this shows the usefulness of the ad-
ditional lattice features used by the regression
model built on LAISR results.
6 Discussion and Conclusion
The adoption of ISR is hindered by the num-
ber of revisions that most partials undergo. A
number of researchers have proposed the use of
right context to increase the stability of par-
tials. While this does increase stability, it mit-
igates the primary gain of ISR: getting a rela-
tively real-time stream of the user?s utterance.
We offer two methods to improve ISR function-
ality: the integration of low-occurring Immortal
partials with higher occurring Terminal partials
(LAISR), and the use of logistic regression to
learn stability and confidence measures.
We find that the integrative approach,
LAISR, outperforms Terminal ISR on three
recognition tasks for a bus timetable spoken dia-
logue system. When looking at utterances with
more than one word this difference becomes even
greater, and this performance increase is due to
the addition of immortal partials, which have
a higher occurrence in longer utterances. This
suggests that as dialogue systems are used to
process multi-phrasal utterances and have more
dynamic turn-taking interactions, immortal par-
tials will play an even larger roll in ISR and par-
tial stability will further improve.
The Stability and Confidence measures both
have lower Equal Error Rates than raw recog-
nition scores when classifying partials. The im-
provement is greatest for LAISR, which benefits
from additional features describing lattice struc-
ture. It also suggests that other incremental fea-
tures such as the length of right context could be
useful for predicting stability. The higher num-
ber of True Accept partials by LAISR indicates
that this method is more useful to a dialogue
manager than Basic or Terminal ISR. Even so,
for all ISR methods there are still more use-
ful stable partials than there are accurate ones.
This suggests that both of these measures are
important to the downstream dialogue manager.
For example, if the partial is predicted to be sta-
ble but not correct, than the agent could possi-
bly interrupt the user and ask them to begin
again.
There are a number of avenues for future
work. First, this paper has examined the word
level; however dialogue systems generally oper-
ate at the intention level. Not all changes at
the word level yield a change in the resulting
intention, so it would be interesting to apply
the confidence measure and stability measures
developed here to the (partial) intention level.
These measures could also be applied to later
stages of the pipeline ? for example, tracking
stability and confidence in the dialogue state re-
sulting from the current partial intention. Fea-
tures from the intention level and dialogue state
could be useful for these measures ? for instance,
indicating whether the current partial intention
is incompatible with the current dialogue state.
Another avenue for future work would be to
apply these techniques to non-dialogue real-time
ASR tasks, such as transcription of broadcast
news. Confidence and stability measures could
be used to determine whether/when/how to dis-
play recognized text to a viewer, or to inform
down-stream processes such as named entity ex-
traction or machine translation.
Of course, an important objective is to eval-
uate our Stability and Confidence Measures
with LAISR in an actual spoken dialogue sys-
tem. ISR completely restructures the conven-
tional turn-based dialogue manager, giving the
agent the opportunity to speak at any mo-
ment. The use of reinforcement learning to make
these turn-taking decisions has been shown in a
small simulated domain by Selfridge and Hee-
man (2010), and we believe this paper builds
a foundation for pursuing these ideas in a real
system.
Acknowledgments
Thanks to Vincent Goffin for help with this
work, and the anonymous reviewers for their
thoughtful suggestions and critique. We ac-
knowledge funding from the NSF under grant
IIS-0713698.
118
References
G. Aist, J. Allen, E. Campana, C. Gallo, S. Stoness,
Mary Swift, and Michael K. Tanenhaus. 2007. In-
cremental understanding in human-computer di-
alogue and experimental evidence for advantages
over nonincremental methods. In Proc. DECA-
LOG, pages 149?154.
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of
speech recognition for incremental systems. In
Proc. NAACL: HLT, pages 380?388.
A. Black, S. Burger, B. Langner, G. Parent, and
M. Eskenazi, 2010. Spoken dialog challenge 2010,
In Proc. Workshop on Spoken Language Technolo-
gies (SLT), Spoken Dialog Challenge 2010 Special
Session.
David DeVault, Kenji Sagae, and David Traum.
2009. Can i finish? learning when to respond to
incremental interpretation results in interactive di-
alogue. In Proc. SIGdial 2009 Conference, pages
11?20,
G.A. Fink, C. Schillo, F. Kummert, and G. Sagerer.
1998. Incremental speech recognition for multi-
modal interfaces. In Industrial Electronics Soci-
ety, 1998. IECON?98 volume 4, pages 2012?2017.
D.B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli.. Cortex volume 11, number 4,
page 355.
A. Genkin, L. Shenzhi, D. Madigan, and DD.
Lewis. 2011. Bayesian logistic regression.
http://www.bayesianregression.org.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-
Tur, A. Ljolje, S. Parthasarathy, M. Rahim,
G. Riccardi, and M. Saraclar. 2005. The AT&T
WATSON speech recognizer. In Proc. of ICASSP,
pages 1033?1036.
G. Parent and M. Eskenazi. 2009. Toward Better
Crowdsourced Transcription: Transcription of a
year of the Let?s Go Bus Information System Data.
Proc. of Interspeech 2005, Lisbon, Portugal.
A. Raux, B. Langner, D. Bohus, A.W. Black, and
M. Eskenazi. 2005. Lets go public! taking a spo-
ken dialog system to the real world. In Proc. of
Interspeech 2005.
D. Schlangen, T. Baumann, and M. Atterer. 2009.
Incremental reference resolution: The task, met-
rics for evaluation, and a Bayesian filtering model
that is sensitive to disfluencies. In Proc. SIGdial,
pages 30?37.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems.
In Proc. of ACL 2010, pages 177?185.
G. Skantze and D. Schlangen. 2009. Incremental
dialogue processing in a micro-domain. In Proc.
EACL 2009, pages 745?753
J.C. Spohrer, PF Brown, PH Hochschild, and
JK Baker. 1980. Partial traceback in continuous
speech recognition. In Proc. of the IEEE Interna-
tional Conference on Cybernetics and Society.
J.D. Williams, I. Arizmendi and A. Conkie.
2010. Demonstration of AT&T ?Let?s Go?: A
production-grade statistical spoken dialog system.
In Proc Demonstration Session at IEEE Workshop
on Spoken Language Technology
J.D. Williams and S. Balakrishnan. 2009. Estimat-
ing probability of correctness for ASR N-Best lists.
In Proc. of SIGdial 2009, pages 132?135.
119
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 113?117,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
A Temporal Simulator for Developing Turn-Taking Methods for
Spoken Dialogue Systems
Ethan O. Selfridge and Peter A. Heeman
Center for Spoken Language Understanding
Oregon Health & Science University
20000 NW Walker Rd., Beaverton, OR, 97006
selfridg@ohsu.edu, heemanp@ohsu.edu
Abstract
Developing sophisticated turn-taking behavior
is necessary for next-generation dialogue sys-
tems. However, incorporating real users into
the development cycle is expensive and cur-
rent simulation techniques are inadequate. As
a foundation for advancing turn-taking behav-
ior, we present a temporal simulator that mod-
els the interaction between the user and the
system, including speech, voice activity de-
tection, and incremental speech recognition.
We describe the details of the simulator and
demonstrate it on a sample domain.
1 Introduction and Background
Effective turn-taking is critical for successful
human-computer interaction. Recently, approaches
have been proposed to improve system turn-taking
behavior that use reinforcement learning (Jonsdot-
tir et al, 2008; Selfridge and Heeman, 2010), de-
cision theory (e.g., Raux and Eskenazi, 2009), and
hard-coded policies (e.g., Skantze and Schlangen ,
2009). Some of these methods model turn-taking
as content-free decisions (Jonsdottir et al, 2008;
Skantze and Schlangen, 2009), while others primar-
ily rely on dialogue context (Selfridge and Heeman,
2010) and lexical cues (e.g., Raux and Eskenazi,
2009). Turn-taking continues to be an area of ac-
tive research and its development is vital for next-
generation dialogue systems, especially as they al-
low for more mixed initiative interaction.
Researchers have turned to simulation since de-
veloping a dialogue system with real users is ex-
pensive, time consuming, and sometimes impossi-
ble. Some turn-taking simulations have been highly
stylized and only model utterance content, failing to
give a realistic model of timing (Selfridge and Hee-
man, 2010). Others have modeled a content-free
form of turn-taking and only attend to timing and
prosodic information (Jonsdottir et al, 2008; Bau-
mann, 2008; Padilha and Carletta, 2002). The for-
mer is insufficient for the training of deployable real-
time systems, and the latter neglect an important as-
pect of turn-taking: semantic information (Gravano
and Hirschberg, 2011).
The overall goal is to develop a simulation en-
vironment to train behavior policies that can be
transferred with minimal modifications to produc-
tion systems. This paper presents some first steps
towards this goal. We describe a temporal simula-
tor that models the timing and content of both user
and system speech, as well as that of incremental
speech recognition (ISR) and voice activity detec-
tion (VAD). We detail the overall temporal simulator
architecture, the design of the individual agents that
simulate dialogue, and an instantiation of a simple
domain. To demonstrate the utility of the simulator,
we implement multiple turn-taking polices and use it
to compare these policies under conditions of vary-
ing reaction time and speech recognition accuracy.
2 Temporal Simulation Framework
We now describe the details of the temporal sim-
ulator. Inspired by the Open Agent Architecture
(Martin et al, 1999), it is composed of a number
of agents, each running as a separate computer pro-
cess. We first describe the time keeping procedure
and then the overall agent communication structure.
113
Time Keeping: To provide a useful training envi-
ronment, the simulator must realistically model, and
run much faster than, ?real-time?. To do this, the
simulator keeps an internal clock that advances to
the next time slice when all agents have been run
for the current time slice. This structure allows the
simulator to run far faster than ?real-time? while sup-
porting realistic communication. This framework is
similar to the clock cycle described by Padilha et al
(2002).
Agent Communication: Agents use messages to
communicate. Messages have three components:
the addressee, the content and a time stamp. Time
stamps dictate when the content is to be processed
and must always be for a future, not the current, time
slice, as the alternative would imply instantaneous
communication and overly complicate the software
architecture. A central hub receives all messages
and passes them to the intended recipient agent at
the appropriate time. At every slice, each agent runs
two procedures: one that retrieves messages and one
that can send messages. If there are multiple mes-
sages intended for the same time slice, the agent
completely processes one before moving to the next.
3 Dialogue Simulator
We use the above temporal simulator to simulate di-
alogue. At present, we focus on dyadic interaction
and have three agents that are run in a strict order at
every time slice: User, ISR, and System. Time slices
are modeled as 10 millisecond (ms) increments, as
this is the time scale that speech recognizers run at.
In general, the User agent sends messages to the
ISR agent that sends messages to the System agent.
The System agent generally sends messages to both
the User agent and the ISR agent. The behavior of
all three agents rely on parameters (Table 1) that
may either be set by hand or estimated from data.
The User and System agents have near identical con-
struction, the primary difference being that the Sys-
tem can misunderstand User speech.
User and System Design: Agent speech is gov-
erned by a number of timing parameters. The Take-
Turn parameter specifies when the agent will begin
speaking the selected utterance. The agent gets the
first word of the utterance, sets the Word Length pa-
rameter, and ?begins? to speak by sending a speech
event message. The agent outputs the word after
the specified Word Length, and sets the Inter-Word
Pause parameter that governs when the next word
will begin. When the agent completes the utter-
ance, it waits until a future time slice to start an-
other (as governed by the Inter-Utterance Pause pa-
rameter). However, if the listening agent interrupts
mid-utterance, the speaking agent stops speaking
and will not complete the utterance. Any dialogue
agent architecture can be used, providing the input
and output fit with the above specifications.
ISR Design: The ISR agent works as both an In-
cremental Speech Recognizer and a VAD. We cur-
rently model uncertainty in recognition but not in
the VAD, though this is certainly a plausible and
worthwhile addition. When the ISR agent receives
the speech event from the User, it sets the VAD
Speech Start parameter that models lag in speech
detection, and the Speech End (no word) parameter
that models situations where the user starts speaking
but stops mid-word and produces an unrecognizable
sound. When the word is received from the User,
the Speech End (word) parameter is set and a par-
tial phrase result is generated based on the probabil-
ity that the word will be correctly recognized. This
probability is then used as the basis for a confidence
score that is packaged with the partial phrase result.
A Recognition Lag parameter governs the time be-
tween User speech and the output of partial phrase
results to the System. The form of ISR we model
recognizes words cumulatively (see Figure 1 for an
example) though the confidence score, at present, is
only for the newly recognized word. The recognizer
will continue to output partials from User words un-
til the User stops speaking or the System sends a
message to stop recognizing. One critical aspect of
ISR which we are not modeling is partial instability,
where partials are revised as recognition progresses.
Partial instability is an area of active research (e.g.
Baumann et al 2009) and, while revisions may cer-
tainly be modeled in the future, we chose not to for
simplicity?s sake. We feel that, at present, the Recog-
nition Lag parameter is sufficient to model the time
for a partial to become stable.
114
Table 1: Parameters and demonstration values (ms)
Conversant Agents
Inter-Word pause (Usr) ? = 200, ? = 100
Inter-Word pause (Sys) 100
Inter-Utt. pause ? = 1000, ? = 500
Word Length 400
Take-Turn (Usr) 500/200
Take-Turn (Sys) 750/100
ISR Agent
Recog. Acc. variable
Recog. Lag 300
VAD
Speech Start 100
Speech End (word) 200
Speech End (no word) 600
4 Simulation demonstration
We now demonstrate the utility of the temporal sim-
ulator by showing that it can be used to evaluate
different turn-taking strategies under conditions of
varying ASR accuracy. This is the first step before
using it to train policies for use in a live dialogue
system.
For this demonstration the conversant agents, the
System and User, are built according to the Infor-
mation State Update approach (Larsson and Traum,
2000), and perform an update for every message as-
sociated with the current time slice. The conver-
sant agents are identical except for individual rule
sets. Four types of rule sets are common across
conversant agents: UNDERSTANDING rules, that up-
date the IS using raw message content; DELIBERA-
TION rules, that update the IS by comparing new in-
formation to old; UTTERANCE rules, that select the
next utterance based on dialogue context; and TURN
Figure 1: Sample dialogue with timing information
rules, that select the time to begin the new utterance
by modifying the Take Turn parameter. Rule sets are
executed in this order with one exception. After the
UNDERSTANDING rules, the System agent has AC-
CEPTANCE rules that use confidence scores to decide
whether to understand the recognition or not.
Temporal Simulation Example: We constructed
a simple credit card domain, similar to Skantze and
Schlangen (2009), where the User says four utter-
ances of four digits each. The System must implic-
itly confirm every number and if it is correct, the
User continues.1 It can theoretically do this at any
time, immediately after the word is recognized, af-
ter an utterance, or after multiple utterances. If the
system says a wrong number the User interrupts the
System with a ?no? and begins the utterance again.
The System has a Non-Understanding (NU) confi-
dence score threshold set at 0.5. After an NU, the
System will not understand any more words and will
either confirm any digits recognized before the NU
or, if there are no words to confirm, will say an NU
utterance (?pardon??). The User says ?yes? to the
final, correct confirmation. To maintain simplicity,
?yes? and ?no? are always accurate. If this were not
the case, there would be a number of dialogues that
were not successful. The User takes the turn in two
ways. It either waits 500 ms after a System utterance
to speak or interrupts 200 ms after the System con-
firms an misrecognized word, which is in line with
human reaction time (Fry, 1975).
We implemented three different turn-taking
strategies: two Fixed and one Context-based. Us-
ing the Fixed strategy the System either uses a Slow
policy, waiting 750 ms after no user speech is de-
tected, or a Fast policy, waiting only 100 ms. The
Fast reaction time results in the System interrupt-
ing the User during an utterance when the inter-word
pause becomes longer than 200 ms. This is because
the VAD Speech End parameter is 100 ms and the
System is waiting for 100 ms of silence after Speech
End. The Slow reaction time results in far less in-
terruptions. The Context-based turn-taking strategy
uses the recognition score to choose its turn-taking
behavior. The motivation is that one would want
1Unlike an explicit confirmation (?I heard five. Is that
right??), an implicit confirm (?Ok, five?) does not necessitate
a strict ?yes? or ?no? response.
115
Figure 2: Mean Time and Interruption for different turn-taking polices and ASR accuracy conditions
to confirm low-confidence recognitions sooner than
those with high confidence. If any unconfirmed re-
sult has scores less than 0.8 then the System uses the
Fast reaction time to try to confirm or reject as soon
as possible. Alternatively, if the results all have high
confidences, it can wait until a longer user pause
(generally between utterances) by using the Slow re-
action time. All parameter values are shown in Table
1.
Figure 1 shows a dialogue fragment of a System
using the Context-based turn-taking policy. Num-
bers are used for the sake of brevity. The start of
a box surrounding a word corresponds to when the
Speech message was sent (from the User agent to the
ISR agent) and the end of the box to when the word
has been completed and recognition lag timer be-
gins. The point of the ISR box refers to the time slice
when the partial phrase result and score were sent to
the System. Note how after the third User word the
System interrupts to confirm the utterance, since the
confidence score of a previous word dropped below
0.8. Also note how the User interrupts the System
after it confirms a wrong number.
Comparing turn-taking policies: We evaluated
the three (two Fixed and one Context-based) turn-
taking policies in two conditions of ASR accuracy:
Low Error, where the probability of correctness was
95%; and High Error, where the probability of cor-
rectness was 75%. We compared the mean dialogue
time (left Figure 2) and the mean number of in-
terruptions per dialogue (right Figure 2). For dia-
logue time, we find that all turn-taking policies per-
form similarly in the Low Error condition. How-
ever, in the High Error condition the Slow reac-
tion time performs much worse since it cannot ad-
dress poor recognitions with the speed of the other
two. For interruption, the Fast and Context-driven
policies have far more than the Slow for the High
Error condition. However, in the Low Error con-
dition the Fast policy interrupts far more than the
Context-driven. Given that natural behavior is one
goal of turn-taking, interruption, while effective at
handling High Error rates, should be minimized.
The Context-based policy provides support for in-
terruption when it is needed (High Error Condition)
and reduces it when it is not (Low Error Condition).
The other policies are either unable to interrupt at all
(Slow), increasing the dialogue time, or due to a lack
the flexibility (Fast), interrupt constantly.
5 Conclusion
We take the first steps towards a simulation approach
that characterizes both the content of conversant
speech as well as its timing. The temporal simula-
tor models conversant utterances, ISR, and the VAD.
The simulator runs quickly (100 times faster than
real-time), and is simple and highly flexible. Us-
ing an example, we demonstrated that the simula-
tor can help understand the ramifications of differ-
ent turn-taking policies. We also highlighted both
the temporal nature of turn-taking ? interruptions,
reaction time, recognition lag...etc. ? and the con-
tent of utterances ? speech recognition errors, con-
fidence scores, and wrong confirmations. Plans for
future work include adding realistic prosodic mod-
eling and estimating model parameters from data.
Acknowledgments
We thank to the reviewers for their thoughtful sug-
gestions and critique. We acknowledge funding
from the NSF under grant IIS-0713698.
116
References
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of speech
recognition for incremental systems. In Proc. NAACL:
HLT, pages 380?388.
T. Baumann. 2008. Simulating spoken dialogue with
a focus on realistic turn-taking. In Proc. of ESSLLI
Student Session.
D. B. Fry. 1975. Simple reaction-times to speech and
non-speech stimuli. Cortex, 11(4):355?360.
A. Gravano and J. Hirschberg. 2011. Turn-taking cues
in task-oriented dialogue. Computer Speech & Lan-
guage, 25(3):601?634.
G.R. Jonsdottir, K.R. Thorisson, and Eric Nivel. 2008.
Learning smooth, human-like turntaking in realtime
dialogue. In Proc. of IVA, pages 162?175.
S. Larsson and D. Traum. 2000. Information state and di-
alogue managment in the trindi dialogue move engine
toolkit. Natural Language Engineering, 6:323?340.
D.L. Martin, Adam J. Cheyer, and Douglas B. Moran.
1999. The open agent architecture: A framework for
building distributed software systems. Applied Ar-
tificial Intelligence: An International Journal, 13(1-
2):91?128.
E. Padilha and J. Carletta. 2002. A simulation of small
group discussion. In Proc. of EDILOG, pages 117?
124.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Proc. of
HLT/NAACL, pages 629?637.
G. Skantze and D. Schlangen . 2009. Incremental di-
alogue processing in a micro-domain. In Proc. of
EACL, pages 745?753.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems. In
Proc. of ACL, pages 177?185.
117
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 275?279,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Integrating Incremental Speech Recognition and POMDP-based Dialogue
Systems
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams1
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ, USA
1Microsoft Research, Redmond, WA, USA
{selfridg,heemanp}@ohsu.edu
iker@research.att.com jason.williams@microsoft.com
Abstract
The goal of this paper is to present a first
step toward integrating Incremental Speech
Recognition (ISR) and Partially-Observable
Markov Decision Process (POMDP) based di-
alogue systems. The former provides sup-
port for advanced turn-taking behavior while
the other increases the semantic accuracy of
speech recognition results. We present an In-
cremental Interaction Manager that supports
the use of ISR with strictly turn-based dia-
logue managers. We then show that using
a POMDP-based dialogue manager with ISR
substantially improves the semantic accuracy
of the incremental results.
1 Introduction and Background
This paper builds toward integrating two distinct
lines of research in spoken dialogue systems: in-
cremental speech recognition (ISR) for input, and
Partially Observable Markov Decision Processes
(POMDPs) for dialogue management.
On the one hand, ISR improves on whole-
utterance speech recognition by streaming results to
the dialogue manager (DM) in real time (Baumann
et al, 2009; Skantze and Schlangen, 2009). ISR
is attractive because it enables sophisticated system
behavior such as interruption and back-channeling.
However, ISR output is particularly error-prone, and
often requires a specialized dialogue manager to be
written (Bu? and Schlangen, 2011; Schlangen and
Skantze, 2009).
1Work done while at AT&T Labs - Research
On the other hand, POMDP-based dialogue man-
agers improve on traditional approaches by (in part)
tracking a distribution over many possible dialogue
states, rather than just one, thereby improving ro-
bustness to speech recognition errors (Williams and
Young, 2007; Thomson and Young, 2010; Young
et al, 2010). The overall aim of combining these
two lines of research is to improve the robustness of
error-prone ISR output.
To our knowledge only one study to date has com-
bined ISR and POMDPs. Lu et al (2011) show
how 1-best ISR hypotheses can be used within a sin-
gle dialogue turn. This work is different than the
present paper, where we use N-Best lists of ISR re-
sults across multiple turns of a dialogue.
Specifically, this paper makes two contributions.
First, as a foundation, we introduce an Incremental
Interaction Manager (IIM) that enables ISR to be
used within the traditional turn-based dialogue man-
agement framework. The IIM confers many, but not
all, of the benefits of ISR without requiring mod-
ification to a traditional dialogue manager. Thus,
in theory, any existing dialogue system architecture
could use ISR with the addition of an IIM. Second,
we show that pairing our IIM with a POMDP-based
dialogue manager yields a substantial improvement
in accuracy for incremental recognition results at the
dialogue level.
The paper is organized as follows. Section 2 de-
scribes the IIM, section 3 describes the POMDP in-
tegration, sections 4 and 5 describe experiments and
results, and section 6 concludes.
275
Table 1: Example IIM operation. P = partial ISR result; A = dialogue action.
Original Copied
ISR IIM DM state DM state DM Action
Prompt: ?Where are you leaving from??
yew Rej. P 0 0 -
ridge Acc. P / Rej. A 0 0 ?I?m sorry...?
mckee Acc. P / Acc. A 0 1 ?Ok, Mckee...?
mckeesport Acc. P / Acc. A 0 2 ?Ok, Mckeesport..?
mckeesport center Acc. P / Rej. A 0 2 ?Ok, Mckeesport..?
Prompt: ?Ok, Mckeesport. Where are you going to??
pitt Acc. P / Rej. A 2 4 ?I?m sorry...?
pittsburgh Acc. P / Acc. A 2 5 ?Ok, Pittsburgh...?
2 Incremental Interaction manager
The Incremental Interaction Manager (IIM) medi-
ates communication between the incremental speech
recognizer and the DM. The key idea is that the
IIM evaluates potential dialogue moves by apply-
ing ISR results to temporary instances of the DM.
The IIM copies the current state of the DM, pro-
vides the copied DM with a recognition result, and
inspects the action that the copied DM would take.2
If the action does not sufficiently advance the dia-
logue (such as re-asking the same question), the ac-
tion is rejected and the copied DM is discarded. If
the action advances the dialogue (such as asking for
or providing new information), then that action is
immediately executed.
The system should gracefully handle revisions
following a premature action execution, and a copy-
ing procedure is a viable solution for any DM. When
a revision is received, a second copy of the original
DM is made and the new ISR result is passed to that
second copy; if that second copy takes an action that
advances the dialogue and is different from the ac-
tion generated by the first copy, then the first action
is terminated, the first copy of the DM is discarded,
the second action is initiated, and the second copy
assumes the position of the first copy. Additional
revisions can be handled by following the same pro-
cedure. Terminating a speech action and immedi-
ately starting another can be jarring (?Say a city /
Ok, Boston...?), which can be mitigated by preced-
2If the DM design does not force a state transition following
a result then the DM supplies the the action without copying.
ing actions with either a sound or simple silence (at
the expense of some response delay). Once recog-
nition is complete, the copied DM is installed as the
new original DM.
Many ISR results can be discarded before passing
them to the DM. First, only incremental results that
could correspond to complete user utterance are con-
sidered: incomplete results are discarded and never
passed to the DM. In addition, ISR results are of-
ten unstable, and it is undesirable to proceed with
an ISR result if it will very likely be revised. Thus
each candidate ISR result is scored for stability (Sel-
fridge et al, 2011) and results with scores below a
manually-set threshold are discarded.
Table 1 shows an example of the recognizer, the
IIM, and the DM. For sake of clarity, stability scores
are not shown. The system asks ?Where are you
leaving from?? and the user answers ?Mckeesport
Center.? The IIM receives five ISR results (called
partials), rejecting the first, yew, because its stabil-
ity score is too low (not shown). With the second,
ridge, it copies the DM, passes ridge to the copy,
and discards the action of the copied DM (also dis-
carded) because it does not advance the dialogue. It
accepts and begins to execute the action generated
by the third partial, mckee. The fourth partial revises
the action, and the fifth action is rejected since it is
the same. The original DM is then discarded and the
copied DM state is installed in its place.
Overall, the IIM enables a turn-based DM to en-
joy many of the benefits of ISR ? in particular, the
ability to make turn-taking decisions with a com-
plete account of the dialogue history.
276
3 Integrating ISR with a POMDP-based
dialogue manager
A (traditional) dialogue manager based on a partially
observable Markov decision process (POMDP DM)
tracks a probability distribution over multiple hid-
den dialogue states called a belief state (Williams
and Young, 2007).3 As such, POMDP DMs read-
ily make use of the entire ASR N-Best list, even
for low-confidence results ? the confidence level of
each N-Best list item contributes proportionally to
the probability of its corresponding hidden state.
It is straightforward to integrate ISR and a
POMDP DM using the IIM. Each item on the N-
Best list of an incremental result is assigned a confi-
dence score (Williams and Balakrishnan, 2009) and
passed to the POMDP DM as if it were a complete
result, triggering a belief state update. Note that this
approach is not predicting future user speech from
partial results (DeVault et al, 2009; Lu et al, 2011),
but rather (tentatively) assuming that partial results
are complete.
The key benefit is that a belief state generated
from an incremental result incorporates all of the
contextual information available to the system from
the start of the dialogue until the moment of that
incremental result. By comparison, an isolated in-
cremental result includes only information from the
current utterance. If the probability models in the
POMDP are estimated properly, belief states should
be more accurate than isolated incremental results.
4 Experimental design
For our experiments we used a corpus of 1037 calls
from real users to a single dialogue system that pro-
vides bus timetable information for Pittsburgh, PA
(a subsequent version of Williams (2011)). This di-
alogue system opened by asking the caller to say a
bus route number or ?I don?t know?; if the system
had insufficient confidence following recognition, it
repeated the question. We extracted the first 3 re-
sponses to the system?s bus route question. Often
the system did not need to ask 3 times; our exper-
imental set contained 1037 calls with one or more
attempts, 586 calls with two or more attempts, and
3It also uses reinforcement learning to choose actions, al-
though in this paper we are not concerned with this aspect.
356 calls with three or more attempts. These utter-
ances were all transcribed, and tagged for the bus
route they contained, if any: 25% contained neither
a route nor ?I don?t know?.
We ran incremental speech recognition on each
utterance using Lattice-Aware Incremental Speech
Recognition (Selfridge et al, 2011) on the AT&T
WATSONSM speech recognizer (Goffin et al, 2005)
with the same rule-based language models used in
the production system. On average, there were
5.78, 5.44, and 5.11 incremental results per utter-
ance (plus an utterance-final result) for the first, sec-
ond, and third attempts. For each incremental result,
we noted its time stamp and interpretation: correct,
if the interpretation was present and correct, other-
wise incorrect. Each incremental result included an
N-Best list, from which we determined oracle accu-
racy: correct if the correct interpretation was present
anywhere on the most recent ISR N-Best list, other-
wise incorrect.
Each incremental result was then passed to the
IIM and POMDP DM. The models in the POMDP
DM were estimated using data collected from a dif-
ferent (earlier) time period. When an incremental
result updated the belief state, the top hypothesis
for the route was extracted from the belief state and
scored for correctness. For utterances in the first at-
tempt, the belief state was initialized to its prior; for
subsequent attempts, it incorporated all of the prior
(whole-turn) utterances. In other words, each at-
tempt was begun assuming the belief state had been
running up to that point.
5 Results and Discussion
We present results by showing instantaneous seman-
tic accuracy for the raw incremental result (base-
line), the top belief state, and oracle. Instantaneous
semantic accuracy is shown with respect to the per-
cent of the total recognition time the partial is rec-
ognized at. An utterance is incorrect if it has no in-
cremental result before a certain percentage.
We show 2 sets of plots. Figure 1 shows only in-
cremental recognition results and excludes the end-
of-utterance (phrase) results; Figure 2 shows incre-
mental recognition results and includes phrase re-
sults. It is useful to view these separately since the
phrase result, having access to all the speech, is sub-
277
Figure 1: Instantaneous semantic accuracy of incremental results, excluding phrase-final results
Figure 2: Instantaneous semantic accuracy of incremental and phrase-final results
stantially more accurate than the incremental results.
Figure 1 shows that the POMDP is more accu-
rate than the raw incremental result (excluding end-
of-phrase results). Its performance gain is minimal
in attempt 1 because the belief is informed only by
the prior. In attempt 2 and 3, the gain is larger
since the belief also benefits from the previous at-
tempts. Since the top POMDP result in subsequent
attempts is sometimes already correct (because it
incorporates past recognitions), the POMDP some-
times meets and occasionally exceeds the oracle dur-
ing the early portions of attempts 2 and 3.
Figure 2 shows that when end-of-phrase recog-
nition results are included, the benefit of the belief
state is limited to the initial portions of the second
and third turns. This is because the POMDP mod-
els are not fit well to the data: the models were
estimated from an earlier version of the system,
with a different user base and different functionality.
Identifying and eliminating this type of mismatch
is an important issue and has been studied before
(Williams, 2011).
Taken as a whole, we find that using belief track-
ing increases the accuracy of partials by over 8%
(absolute) in some cases. Even though the final
phrase results of the 1-best list are more accurate
than the belief state, the POMDP shows better ac-
curacy on the volatile incremental results. As com-
pared to the whole utterance results, incremental re-
sults have lower 1-best accuracy, yet high oracle ac-
curacy. This combination is a natural fit with the
POMDPs belief state, which considers the whole N-
Best list, effectively re-ranking it by synthesizing in-
formation from dialogue history priors.
6 Conclusion
This paper has taken a step toward integrating ISR
and POMDP-based dialogue systems. The Incre-
mental Interaction Manager (IIM) enables a tradi-
tional turn-based DM to make use of incremental
results and enjoy many their benefits. When this
IIM is paired with a POMDP DM, the interpreta-
tion accuracy of incremental results improves sub-
stantially. In the future we hope to build on this work
by incorporating Reinforcement Learning into turn-
taking and dialogue action decisions.
Acknowledgments
Thanks to Vincent Goffin for help with this work,
and to the anonymous reviewers for their comments
and critique. We acknowledge funding from the
NSF under grant IIS-0713698.
278
References
T. Baumann, M. Atterer, and D. Schlangen. 2009.
Assessing and improving the performance of speech
recognition for incremental systems. In Proc. NAACL:
HLT, pages 380?388. Association for Computational
Linguistics.
O. Bu? and D. Schlangen. 2011. Dium?an incremen-
tal dialogue manager that can produce self-corrections.
Proceedings of semdial.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can i finish? learning when to respond to incremental
interpretation results in interactive dialogue. In Pro-
ceedings of the SIGDIAL 2009 Conference, pages 11?
20, London, UK, September. Association for Compu-
tational Linguistics.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T WATSON speech
recognizer. In Proceedings of ICASSP, pages 1033?
1036.
D. Lu, T. Nishimoto, and N. Minematsu. 2011. Decision
of response timing for incremental speech recogni-
tion with reinforcement learning. In Automatic Speech
Recognition and Understanding (ASRU), 2011 IEEE
Workshop on, pages 467?472. IEEE.
D. Schlangen and G. Skantze. 2009. A general, ab-
stract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 710?718. Association for Computational
Linguistics.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2011. Stability and accuracy in incremen-
tal speech recognition. In Proceedings of the SIGdial
2011.
G. Skantze and D. Schlangen. 2009. Incremental dia-
logue processing in a micro-domain. In Proceedings
of the 12th Conference of the European Chapter of
the Association for Computational Linguistics, pages
745?753. Association for Computational Linguistics.
B. Thomson and S. Young. 2010. Bayesian update of di-
alogue state: A pomdp framework for spoken dialogue
systems. Computer Speech & Language, 24(4):562?
588.
J.D. Williams and S. Balakrishnan. 2009. Estimating
probability of correctness for asr n-best lists. In Proc
SIGDIAL, London, United Kingdom.
J.D. Williams and S. Young. 2007. Partially observable
markov decision processes for spoken dialog systems.
Computer Speech & Language, 21(2):393?422.
J.D. Williams. 2011. An empirical evaluation of a sta-
tistical dialog system in public use. In Proceedings of
the SIGDIAL 2011 Conference, pages 130?141. Asso-
ciation for Computational Linguistics.
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hidden
information state model: A practical framework for
pomdp-based spoken dialogue management. Com-
puter Speech & Language, 24(2):150?174.
279
Proceedings of the SIGDIAL 2013 Conference, pages 384?393,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Continuously Predicting and Processing Barge-in
During a Live Spoken Dialogue Task
Ethan O. Selfridge?, Iker Arizmendi?, Peter A. Heeman?, and Jason D. Williams1
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR, USA
?AT&T Labs ? Research, Shannon Laboratory, Florham Park, NJ, USA
1Microsoft Research, Redmond, WA, USA
selfridg@ohsu.edu
Abstract
Barge-in enables the user to provide input
during system speech, facilitating a more
natural and efficient interaction. Stan-
dard methods generally focus on single-
stage barge-in detection, applying the di-
alogue policy irrespective of the barge-in
context. Unfortunately, this approach per-
forms poorly when used in challenging
environments. We propose and evaluate
a barge-in processing method that uses a
prediction strategy to continuously decide
whether to pause, continue, or resume the
prompt. This model has greater task suc-
cess and efficiency than the standard ap-
proach when evaluated in a public spoken
dialogue system.
Index Terms: spoken dialogue systems, barge-in
1 Introduction
Spoken dialogue systems (SDS) communicate
with users with spoken natural language; the op-
timal SDS being effective, efficient, and natural.
Allowing input during system speech, known as
barge-in, is one approach that designers use to
improve system performance. In the ideal use
case, the system detects user speech, switches off
the prompt, and then responds to the user?s utter-
ance. Dialogue efficiency improves, as the sys-
tem receives information prior to completing its
prompt, and the interaction becomes more natu-
ral, as the system demonstrates more human-like
turn-taking behavior. However, barge-in poses a
number of new challenges; the system must now
recognize and process input during its prompt that
may not be well-formed system directed speech.
This is a difficult task and standard barge-in ap-
proaches often stop the prompt for input that will
not be understood, subsequently initiating a clari-
fication sub-dialogue (?I?m sorry, I didn?t get that.
You can say...etc.?). This non-understood barge-in
(NUBI) could be from environmental noise, non-
system directed speech, poorly-formed system di-
rected speech, legitimate speech recognition diffi-
culties (such as acoustic model mismatch), or any
combination thereof.
This paper proposes and evaluates a barge-in
processing method that focuses on handling NU-
BIs. Our Prediction-based Barge-in Response
(PBR) model continuously predicts interpretation
success by applying adaptive thresholds to incre-
mental recognition results. In our view, predicting
whether the recognition will be understood has far
more utility than detecting whether the barge-in
is truly system directed speech as, for many do-
mains, we feel only understandable input has more
discourse importance than system speech. If the
input is predicted to be understood, the prompt is
paused. If it is predicted or found to be NUBI, the
prompt is resumed. Using this method, the sys-
tem may resume speaking before recognition is
complete and will never initiate a clarifying sub-
dialogue in response to a NUBI. The PBR model
was implemented in a public Lets Go! statistical
dialogue system (Raux et al, 2005), and we com-
pare it with a system using standard barge-in meth-
ods. We find the PBR model has a significantly
better task success rate and efficiency.
Table 1 illustrates the NUBI responses produced
by the standard barge-in (Baseline) and PBR mod-
els. After both prompts are paused, the standard
method initiates a clarifying sub-dialogue whereas
PBR resumes the prompt.
We first provide background on Incremental
Speech Recognition and describe the relevant re-
lated work on barge-in. We then detail the
Prediction-based Barge-in Response model?s op-
eration and motivation before presenting a whole-
call and component-wise analysis of the PBR
1Work done while at AT&T Labs - Research
384
Table 1: System response to Non-Understood Barge-In (NUBI)
Baseline Ok, sixty one <NUBI> Sorry, say a bus route like twenty eight x
PBR Ok, sixty one <NUBI> sixty one c. Where are you leaving from?
model. The paper concludes with a discussion of
our findings and implications for future SDS.
2 Background and Related Work
Incremental Speech Recognition: Incremental
Speech Recognition (ISR) provides the real-time
information critical to the PBR model?s continu-
ous predictions. ISR produces partial recognition
results (?partials?) until input ceases and the ?fi-
nal? recognition result is produced following some
silence. As partials have a tendency to be revised
as more audio is processed, stability measures are
used to predict whether a given partial hypothe-
sis will be present in the final recognition result
(McGraw and Gruenstein, 2012; Selfridge et al,
2011). Here, we use Lattice-Aware ISR, which
produces partials after a Voice Activity Detector
(VAD) indicates speech and limits them to be a
complete language model specified phrase or have
guaranteed stability (Selfridge et al, 2011).
Barge-In: Using the standard barge-in model,
the system stops the prompt if barge-in is detected
and applies the dialogue logic to the final recogni-
tion result. This approach assumes that the barge-
in context should not influence the dialogue pol-
icy, and most previous work on barge-in has fo-
cused on detection: distinguishing system directed
speech from other environmental sounds. Cur-
rently, these methods are either based on a VAD
(e.g. (Stro?m and Seneff, 2000)), ISR hypothe-
ses (Raux, 2008), or some combination (Rose and
Kim, 2003). Both approaches can lead to detection
errors: background speech will trigger the VAD,
and partial hypotheses are unreliable (Baumann et
al., 2009). To minimize this, many systems only
enable barge-in at certain points in the dialogue.
One challenge with the standard barge-in model
is that detection errors can initiate a clarifying sub-
dialogue to non-system directed input, as it is un-
likely that this input will be understood (Raux,
2008). Since this false barge-in, which in most
cases is background speech (e.g. the television), is
highly indicative of poor recognition performance
overall, the system?s errant clarifying response can
only further degrade user experience.
Strom and Seneff (2000) provide, to our knowl-
edge, the only mature work that proposed deviat-
ing from the dialogue policy when responding to
a barge-in recognition. Instead of initiating a clar-
ifying sub-dialogue, the system produced a filled-
pause disfluency (?umm?) and resumed the prompt
at the phrase boundary closest to the prompt?s sus-
pension point. However, this model only operated
at the final recognition level (as opposed the incre-
mental level) and, unfortunately, they provide no
evaluation of their approach. An explicit compar-
ison between the approaches described here and
the PBR model is found in Section 3.5.
3 Prediction-based Barge-in Response
The PBR model is characterized by three high-
level states: State 1 (Speaking Prediction), whose
goal is to pause the prompt if stability scores pre-
dict understanding; State 2 (Silent Prediction),
whose goal is to resume the prompt if stability
scores and the incremental recognition rate pre-
dict non-understanding; and State 3 (Completion),
which operates on the final recognition result, and
resumes the prompt unless the recognition is un-
derstood and the new speech act will advance the
dialogue. Here, we define ?advancing the dia-
logue? to be any speech act that does not start a
clarifying sub-dialogue indicating a NUBI. Tran-
sitions between State 1 and 2 are governed by
adaptive thresholds ? repeated resumptions sug-
gest the user is in a noisy environment, so each
resumption increases the threshold required to ad-
vance from State 1 to State 2 and decreases the
threshold required to advance from State 2 to State
1. A high-level comparison of the standard model
and our approach is shown in Figure 1; a complete
PBR state diagram is provided in the Appendix.
3.1 State 1: Speaking Prediction
In State 1, Speaking Prediction, the system is both
speaking and performing ISR. The system scores
each partial for stability, predicting the probability
that it will remain ?stable? ? i.e., will not be later
revised ? using a logistic regression model (Self-
ridge et al, 2011). This model uses a number of
features related to the recognizer?s generic confi-
dence score, the word confusion network, and lat-
tice characteristics. Table 2 shows partial results
385
Table 2: Background noise and User Speech ISR
Background Noise User Utterance
Partial Stab. Scr. Partial Stab. Scr.
one 0.134 six 0.396
two 0.193 sixty 0.542
six 0.127 fifty one 0.428
two 0.078 sixty one a 0.491
and stability scores for two example inputs: back-
ground noise on the left, and the user saying ?sixty
one a? on the right.
State 1 relies on the internal threshold param-
eter, T1. If a partial?s stability score falls below
T1, control remains in State 1 and the partial re-
sult is discarded. If a stability score meets T1, the
prompt is paused and control transitions to State 2.
T1 is initially set to 0 and is adapted as the dialogue
progresses. The adaptation procedure is described
below in Section 3.4. If a final recognition result
is received, control transitions directly to State 3.
Transitioning from State 1 to State 2 is only al-
lowed during the middle 80% of the prompt; oth-
erwise only transitions to State 3 are allowed.1
3.2 State 2: Silent Prediction
Upon entering State 2, Silent Prediction, the
prompt is paused and a timer is started. State 2 re-
quires continuous evidence (at least every T2 ms)
that the ISR is recognizing valid speech and each
time a partial result that meets T1 is received, the
timer is reset. If the timer reaches the time thresh-
old T2, the prompt is resumed and control returns
to State 1. T2 is initially set at 1.0 seconds and is
adapted as the dialogue progresses. Final recogni-
tion results trigger a transition to State 3.
The resumption prompt is constructed using the
temporal position of the VAD specified speech
start to find the percentage of the prompt that was
played up to that point. This percentage is then
reduced by 10% and used to create the resump-
tion prompt by finding the word that is closest to,
but not beyond, the modified percentage. White
space characters and punctuation are used to deter-
mine word boundaries for text-to-speech prompts,
whereas automatically generated word-alignments
are used for pre-recorded prompts.
1We hypothesized that people will rarely respond to the
current prompt during the first 10% of prompt time as over-
laps at the beginning of utterances are commonly initiative
conflicts (Yang and Heeman, 2010). Users may produce
early-onset utterances during the last 10% that should not
stop the prompt as it is not an ?intentional? barge-in.
Figure 1: The Standard Barge-in and PBR Models
3.3 State 3: Completion
State 3, Completion, is entered when a final recog-
nition result is received and determines whether
the current dialogue policy will advance the dia-
logue or not. Here, the PBR model relies on the
ability of the dialogue manager (DM) to produce a
speculative action without transitioning to the next
dialogue state. If the new action will not advance
the dialogue, it is discarded and the recognition
is NUBI. However, if it will advance the dialogue
then it is classified as an Understood Barge-In
(UBI). In the NUBI case, the system either contin-
ues speaking or resumes the current prompt (tran-
sitioning to State 1). In the UBI case, the system
initiates the new speech act after playing a short
reaction sound and the DM transitions to the next
dialogue state. This reaction sound precedes all
speech acts outside the barge-in context but is not
used for resumption or timeout prompts. Note that
by depending solely on the new speech act, our
model does not require access to the DM?s internal
understanding or confidence scoring components.
3.4 Threshold adjustments
States 1 and 2 contain parameters T1 and T2 that
are adapted to the user?s environment. T1 is the
stability threshold used in State 1 and State 2 that
controls how stable an utterance must be before
the prompt should be paused. In quiet environ-
ments ? where only the user?s speech produces
partial results ? a low threshold is desirable as
it enables near-immediate pauses in the prompt.
Conversely, noisy environments yield many spu-
rious partials that (in general) have much lower
stability scores, so a higher threshold is advan-
tageous. T2 is the timing threshold used to re-
sume the prompt during recognition in State 2. In
quiet environments, a higher threshold reduces the
chance that the system will resume its prompt dur-
ing a well-formed user speech. In noisy environ-
386
Figure 2: Example dialogue fragment of PBR Model
ments, a lower threshold allows the system to re-
sume quickly as the NUBI likelihood is greater.
Both T1 and T2 are dependent on the number of
system resumptions, as we view the action of re-
suming the prompt as an indication that the thresh-
old is not correct. With every resumption, the pa-
rameter R is incremented by 1 and, to account for
changing environments, R is decremented by 0.2
for every full prompt that is not paused until it
reaches 0. UsingR, T1 is computed by T1 = 0.17?
R, and T2 by T2 = argmax(0.1, 1? (0.1 ?R)).2
3.5 Method Discussion
The motivation behind the PBR model is both the-
oretical and practical. According to Selfridge and
Heeman (2010), turn-taking is best viewed as a
collaborative process where the turn assignment
should be determined by the importance of the
utterance. During barge-in, the system is speak-
ing and so should only yield the turn if the user?s
speech is more important than its own. For many
domains, we view non-understood input as less
important than the system?s prompt and so, in this
case, the system should not release the turn by
stopping the prompt and initiating a clarifying sub-
dialogue. On the practical side, there is a high
likelihood that non-advancing input is not system
directed, to which the system should neither con-
sume, in terms of belief state updating, nor re-
spond to, in terms of asking for clarification. In
the rare case of non-understood system directed
speech, the user can easily repeat their utterance.
Here, we note that in the event that the user is
backchanneling, the PBR model will behave cor-
rectly and not release the turn.
The PBR approach differs from standard barge-
in approaches in several respects. First, standard
barge-in stops the prompt (i.e., transitions from
State 1 to State 2) if either the VAD or the partial
hypothesis suggests that there is speech; our ap-
proach? using acoustic, language model, and lat-
tice features ? predicts whether the input is likely
to contain an interpretable recognition result. Sec-
2The threshold update values were determined empiri-
cally by the authors.
ond, standard barge-in uses a static threshold; our
approach uses dynamic thresholds that adapt to
the user?s acoustic environment. Parameter adjust-
ments are straightforward since our method auto-
matically classifies each barge-in as NUBI or UBI.
In practice, the prompt will be paused incorrectly
only a few times in a noisy environment, after
which the adaptive thresholds will prevent incor-
rect pauses at the expense of being less responsive
to true user speech. If the noise level decreases,
the thresholds will become more sensitive again,
enabling swifter responses. Finally, with the ex-
ception of Strom and Seneff, standard approaches
always discard the prompt; our approach can re-
sume the prompt if recognition is not understood
or is proceeding poorly, enabling the system to
resume speaking before recognition is complete.
Moreover, resumption yields a natural user expe-
rience as it often creates a repetition disfluency
(?Ok, sixty - sixty one c?), which are rarely no-
ticed by the listener (Martin and Strange, 1968).
An example dialogue fragment is shown in Fig-
ure 2, with the state transitions shown above. Note
the transition from State 2 to State 1, which is the
system resuming speech during recognition. This
recognition stream, produced by non-system di-
rected user speech, does not end until the user says
?repeat? for the last time.
4 Evaluation Results
The PBR model was evaluated during the Spoken
Dialog Challenge 2012-2013 in a live Lets Go!
bus information task. In this task, the public can
access bus schedule information during off hours
in Pittsburgh, PA via a telephonic interaction with
a dialogue system (Raux et al, 2005). The task
can be divided into five sub-tasks: route, origin,
destination, date/time, and bus schedules. The last
sub-task, bus schedules, provides information to
the user whereas the first four gather information.
We entered two systems using the same POMDP-
based DM (Williams, 2012). The first system, the
?Baseline?, used the standard barge-in model with
VAD barge-in detection and barge-in disabled in
387
Figure 3: Estimated success rate for the PBR and Baseline systems. Stars indicate p<0.018 with ?2 test.
a small number of dialogue states that appeared
problematic during initial testing. The second sys-
tem used the PBR model with an Incremental In-
teraction Manager (Selfridge et al, 2012) to pro-
duce speculative actions in State 3. The pub-
lic called both systems during the final weeks of
2011 and the start of 2012. The DM applied a lo-
gistic regression based confidence measure to de-
termine whether the recognition was understood.
Both systems used the AT&T WATSONSM speech
recognizer (Goffin et al, 2005) with the same
sub-task specific rule-based language models and
standard echo cancellation techniques. The beam
width was set to maximize accuracy while still
running faster than real-time. The PBR system
used a WATSON modification to output lattice-
aware partial results.
Call and barge-in statistics are shown in Table
3. Here, we define (potential) barge-in (some-
what imprecisely) as a full recognition that at
some point overlaps with the system prompt, as
determined by the call logs. We show the calls
with barge-in before the bus schedule sub-task was
reached (BI-BS) and the calls with barge-in during
any point of the call (BI All). Since the Baseline
system only enabled barge-in at specific points in
the dialogue, it has fewer instances of barge-in
(Total Barge-In) and fewer barge-in calls. Regret-
fully, due to logging issues with the PBR system,
recognition specific metrics such as Word Error
Rate and true/false barge-in rates are unavailable.
4.1 Estimated Success Rate
We begin by comparing the success rate and
efficiency between the Baseline and PBR sys-
Table 3: Baseline and PBR call/barge-in statistics.
Baseline PBR
Total Calls 1027 892
BI-BS 228 (23%) 345 (39%)
BI All 281 (27%) 483 (54%)
Total Barge-In 829 1388
tems. Since task success can be quite difficult to
measure, we use four increasingly stringent task
success definitions: Bus Times Reached (BTR),
where success is achieved if the call reaches the
bus schedule sub-task; List Navigation (List Nav.),
where success is achieved if the user says ??next?,
?previous?, or ?repeat? ? the intuition being that
if the user attempted to navigate the bus sched-
ule sub-task they were somewhat satisfied with
the system?s performance so far; and Immediate
Exit (BTR2Ex and ListNav2Ex), which further
constrains both of the previous definitions to only
calls that finish directly after the initial visit to the
bus times sub-task. Success rate for the defini-
tions were automatically computed (not manually
labeled). Figure 3 shows the success rate of the
PBR and Baseline systems for all four definitions
of success. It shows, from left to right, Barge-In,
No Barge-In (NBI), and All calls. Here we restrict
barge-in calls to those where barge-in occurred
prior to the bus schedule task being reached.
For the calls with barge-in, a ?2 test finds sig-
nificant differences between the PBR and Base-
line for all four task success definitions. However,
we also found significant differences in the NBI
calls. This was surprising since, when barge-in
is not triggered, both systems are ostensibly the
same. We speculate this could be due to the Base-
line?s barge-in enabling strategy: an environment
that triggers barge-in in the Baseline would always
trigger barge-in in the PBR model, whereas the
converse is not true as the Baseline only enabled
barge-in in some of the states. This means that
there is a potential mismatch when separating the
calls based on barge-in, and so the fairest compar-
ison is using All the calls. This is shown on the far
right of Figure 3. We find that, while the effect is
not as large, there are significant differences in the
success rate for the PBR model for the most and
least stringent success definition, and very strong
trends for the middle two definitions (p < 0.07 for
BTR2Ex and p < 0.054 for List Nav.). Taken as
a whole, we feel this offers compelling evidence
388
Figure 4: Seconds from beginning of dialogue to
reaching the Bus Schedule Information sub-task
that the PBR method is more effective: i.e. yields
higher task completion.
Next, we turn our attention to task efficiency.
For this, we report the amount of clock time from
the beginning of the call to when the Bus Schedule
sub-task was reached. Calls that do not reach this
sub-task are obviously excluded, and PBR times
are adjusted for the reaction sound (explained in
Section 3.3). Task efficiency is reported by cu-
mulative percentage in Figure 4. We find that,
while the NBI call times are nearly identical for
both systems, the PBR barge-in calls are much
faster than the Baseline calls. Here, we do not
feel the previously described mismatch is partic-
ularly problematic as all the calls reached the goal
state and the NBI are nearly identical. In fact, as
more NUBI should actually reduce efficiency, the
potential mismatch only strengthens the result.
Taken together, these results provide substantial
evidence that the PBR model is more effective and
more efficient than the Baseline. In order to ex-
plain PBR?s performance, we explore the effect of
prediction and resumption in isolation.
4.2 State 1: Speaking Prediction
State 1 is responsible for pausing the prompt, the
goal being to pause the prompt for UBI input and
not to pause the prompt for NUBI input. The
prompt is paused if a partial?s stability score meets
or exceeds the T1 threshold. We evaluate the ef-
ficacy of State 1 and T1 by analyzing the statis-
tics of NUBI/UBI input and Paused/Not Paused
(hereafter Continued) prompts. Since resuming
the prompt during recognition affects the recog-
nition outcome, we restrict our analysis to recog-
nitions that do not transition from State 2 back
to State 1. For comparison we show the overall
UBI/NUBI percentages for the Baseline and PBR
systems. This represents the recognition distri-
Table 4: Evaluation of T1, off-line PBR, and Base-
line VAD. For T1 we respectively (?-? split) show
the UBI/NUBI % that are Paused/Continued, the
Paused/Continued % that are UBI/NUBI, and the
percentage over all recognitions
T1 (%) VAD (%)
Paused Continued PBR BL
UBI 72-40-26 28-29-10 36 54
NUBI 61-60-39 39-71-25 64 46
bution for the live Baseline VAD detection and
off-line speculation for the PBR model. Recall
PBR does have VAD activation preceding partial
results and so the off-line PBR VAD shows how
the model would have behaved if it only used the
VAD for detection, as the Baseline does.
Table 4 provides a number of percentages, with
three micro-columns separated by dashes (?-?) for
T1. The first micro-column shows the percent-
age of UBI/NUBI that either Paused or Contin-
ued the prompt (sums to 100 horizontally). The
second micro-column shows the percentage of
Paused/Continued that are UBI/NUBI (sums to
100 vertically). The third micro-column shows
the percentage of each combination (e.g. UBI and
Paused) over all the barge-in recognitions. The
VAD columns show the percentage of UBI/NUBI
that (would) pause the prompt.
We first look at UBI/NUBI percentage that are
Paused/Continued (first micro-column): We find
that 72% of UBI are paused and 28% are Contin-
ued versus 61% of NUBI that are Paused with 39%
Continued. We now look at the Paused/Continued
percentage that are UBI/NUBI (second micro-
column): We find that 40% of Paused are UBI
and 60% are NUBI, whereas 29% of Continued
are UBI and 71% are NUBI. So, while T1 sus-
pends the prompt for the majority of NUBI (not
desirable, though expected since T1 starts at 0),
it has high precision when continuing the prompt.
This reduces the number of times that the prompt
is paused erroneously for NUBI while minimizing
incorrect (UBI) continues. This is clearly shown
by considering all of the recognitions (third micro-
column). We find that PBR erroneously paused
the prompt for 39% of recognitions, as opposed to
64% for the off-line PBR and 46% for the Base-
line. This came at the cost of reducing the number
of correct (UBI) pauses to 26% from 36% (off-line
PBR) and 54% (Baseline VAD).
The results show that the T1 threshold had
389
Figure 5: Secs from Speech Start to Final Result
modest success at discriminating UBI and NUBI;
while continuing the prompt had quite a high
precision for NUBI, the recall was substantially
lower. We note that, since erroneous pauses lead
to resumptions and erroneous continues still lead
to a new speech act, there is minimal cost to these
errors. Furthermore, in our view, reducing the per-
centage of recognitions that pause and resume the
prompt is more critical as these needlessly disrupt
the prompt. In this, T1 is clearly effective, reduc-
ing the percentage from 64% to 39%.
4.3 State 2: Silent Prediction
State 2 governs whether the prompt will remain
paused or be resumed during incremental recogni-
tion. This decision depends on the time parameter
T2, which should trigger resumptions for NUBIs.
Since the act of resuming the prompt during recog-
nition changes the outcome of the recognition, it
is impossible to evaluate how well T2 discrimi-
nated recognition results. However, we can evalu-
ate the effect of that resumption by comparing UBI
percentages between the PBR and Baseline sys-
tems. We first present evidence that T2 is most ac-
tive during longer recognitions, and then show that
longer Baseline recognitions have a lower UBI
percentage than longer PBR recognitions specif-
ically because of T2 resumptions. ?Recognitions?
refer to speech recognition results, with ?longer?
or ?shorter? referring to the clock time between
speech detection and the final recognition result.
We first report the PBR and Baseline response
and recognition time. We separate the PBR barge-
in recognitions into two groups: State 2?State 3,
where the system never transitions from State 2
to State 1, and State 2?State 1, where the sys-
tem resumes the prompt during recognition, tran-
sitioning from State 2 to State 1. The cumulative
percentages of the time from speech detection to
final recognition are shown in Figure 5. We find
that the State 2?State 3 recognitions are far faster
Figure 6: UBI % by minimum recognition time
than the Baseline recognitions, which in turn are
far faster than the State 2?State 1 recognitions.
The difference between PBR and Baseline recog-
nitions implies that T2 has greater activation dur-
ing longer recognitions. Given this, the overall
barge-in response time for PBR should be faster
than the Baseline (as the PBR system is resum-
ing where the Baseline is silent). Indeed this is
the case: the PBR system?s overall mean/median
response time is 1.58/1.53 seconds whereas Base-
line has a mean/median response time of 2.61/1.8
seconds.
The goal of T2 is for the system to resume when
recognition is proceeding poorly, and we have
shown that it is primarily being activated during
longer recognitions. If T2 is functioning properly,
recognition length should be inversely related to
recognition performance, and longer recognitions
should be less likely to be understood. Further-
more, if T2 resumption improves the user?s expe-
rience then longer PBR recognitions should per-
form better than Baseline recognitions of compa-
rable length. Figure 6 presents the UBI percent-
age by the minimum time for recognitions that
reach State 2. We find that, when all recogni-
tions are accounted for (0 second minimum), the
Baseline has a higher rate of UBI. However, as
recognition time increases the Baseline UBI per-
centage decreases (suggesting successful T2 func-
tioning) whereas the PBR UBI percentage actu-
ally increases. Since longer PBR recognitions are
dominated by T2 resumptions, we speculate this
improvement is driven by users repeating or initi-
ating new speech that leads to understanding suc-
cess, as the PBR system is responding where the
Baseline system is silent.
4.4 Resumption
The PBR model relies on resumption to recover
from poor recognitions, either produced in State 2
or State 3. Instead of a resumption, the Baseline
390
Figure 7: Sub-Task Abandonment Rate. NUBI is
different at p < 0.003
system initiates a clarifying sub-dialogue when a
barge-in recognition is not understood. We com-
pare these two behaviors using the call abandon-
ment rate ? the user hangs-up ? of sub-tasks
with and without NUBI. Here, we exclude the Bus
Schedule sub-task as it is the goal state.
Figure 7 shows the call abandonment rate for
sub-tasks that either have or do not have NUBI.
We find that there is a significant difference in
abandoned calls for NUBI sub-tasks between the
two systems (33% vs 48%, p < 0.003 using a ?2
test), but that there is no difference for the calls
that do not have NUBI (7.6% vs 8.4%). This re-
sult shows that prompt resumption is viewed far
more favorably by users than initiating a clarify-
ing sub-dialogue.
5 Discussion and Conclusion
The above results offer strong evidence that the
PBR model increases task success and efficiency,
and we found that all three states contribute to
the improved performance by creating a more ro-
bust, responsive, and natural interaction. T1 pre-
diction in State 1 reduced the number of spurious
prompt suspensions, T2 prediction in State 2 led to
improved understanding performance, and prompt
resumption (States 2 and 3) reduced the number of
abandoned calls.
An important feature of the Prediction-based
Barge-in Response model is that, while it lever-
ages incremental speech processing for barge-in
processing, it does not require an incremental di-
alogue manager to drive its behavior. Since the
model is also domain independent and does not
require access to internal dialogue manager com-
ponents, it can easily be incorporated into any ex-
isting dialogue system. However, one limitation of
the current model is that the prediction thresholds
are hand-crafted. We also believe that substan-
tial improvements can be made by explicitly at-
tempting to predict eventual understanding instead
of using the stability score and partial production
rate as a proxy. Furthermore, the PBR model does
not distinguish between the causes of the non-
understanding, specifically whether the input con-
tained in-domain user speech, out-of-domain user
speech, or background noise. This case is specifi-
cally applicable in domains where system and user
speech are in the same channel, such as interact-
ing via speaker phone. In this context, the system
should be able to initiate a clarifying sub-dialogue
and release the turn, as the system must be more
sensitive to the shared acoustic environment and
so its current prompt may be less important than
the user?s non-understood utterance.
The results challenge a potential assumption re-
garding barge-in: that barge-in indicates greater
user pro-activity and engagement with the task.
One of the striking findings was that dialogues
with barge-in are slower and less successful than
dialogues without barge-in. This suggests that,
for current systems, dialogues with barge-in are
more indicative of environmental difficulty than
user pro-activity. The superior performance of
the PBR model, which is explicitly resistant to
non-system directed speech, implies that domi-
nant barge-in models will have increasingly lim-
ited utility as spoken dialogue systems become
more prevalent and are used in increasingly dif-
ficult environments. Furthermore, within the con-
text of overall dialogue systems, the PBR model?s
performance emphasizes the importance of contin-
uous processing for future systems.
This paper has proposed and evaluated the
Prediction-based Barge-in Response model. This
model?s behavior is driven by continuously pre-
dicting whether a barge-in recognition will be un-
derstood successfully, and combines incremental
speech processing techniques with a prompt re-
sumption procedure. Using a live dialogue task
with real users, we evaluated this model against
the standard barge-in model and found that it led
to improved performance in both task success and
efficiency.
Acknowledgments
Many thanks to Vincent Goffin for help with this
work, and to the anonymous reviewers for their in-
sightful comments and critique. We acknowledge
funding from the NSF under grant IIS-0713698.
391
References
T. Baumann, M. Atterer, and D. Schlangen. 2009. As-
sessing and improving the performance of speech
recognition for incremental systems. In Proc.
NAACL: HLT, pages 380?388. Association for Com-
putational Linguistics.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur,
A. Ljolje, S. Parthasarathy, M. Rahim, G. Riccardi,
and M. Saraclar. 2005. The AT&T WATSON
speech recognizer. In Proceedings of ICASSP, pages
1033?1036.
James G Martin and Winifred Strange. 1968. The per-
ception of hesitation in spontaneous speech. Percep-
tion & Psychophysics, 3(6):427?438.
Ian McGraw and Alexander Gruenstein. 2012. Es-
timating word-stability during incremental speech
recognition. In in Proc. of Interspeech 2012.
A. Raux, B. Langner, D. Bohus, A.W. Black, and
M. Eskenazi. 2005. Lets go public! taking a spo-
ken dialog system to the real world. In in Proc. of
Interspeech 2005.
A. Raux. 2008. Flexible Turn-Taking for Spoken Dia-
log Systems. Ph.D. thesis, CMU.
Richard C Rose and Hong Kook Kim. 2003. A
hybrid barge-in procedure for more reliable turn-
taking in human-machine dialog systems. In Auto-
matic Speech Recognition and Understanding, 2003.
ASRU?03. 2003 IEEE Workshop on, pages 198?203.
IEEE.
E.O. Selfridge and P.A. Heeman. 2010. Importance-
Driven Turn-Bidding for spoken dialogue systems.
In Proc. of ACL 2010, pages 177?185. Association
for Computational Linguistics.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2011. Stability and accuracy in incre-
mental speech recognition. In Proceedings of the
SIGdial 2011.
E.O. Selfridge, I. Arizmendi, P.A. Heeman, and J.D.
Williams. 2012. Integrating incremental speech
recognition and pomdp-based dialogue systems. In
Proceedings of the SIGdial 2012.
Nikko Stro?m and Stephanie Seneff. 2000. Intelligent
barge-in in conversational systems. Procedings of
ICSLP.
Jason D Williams. 2012. A critical analysis of two sta-
tistical spoken dialog systems in public use. In Spo-
ken Language Technology Workshop (SLT), 2012
IEEE, pages 55?60. IEEE.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue?. Computer Speech
Language, 24(2):175 ? 189.
392
A Appendix
This diagram represents the possible operating positions the Prediction-based Barge-in Response
model can be in. If the prompt is complete, the PBR model applies the dialogue policy to the final
recognition result and initiates the on-policy speech act. If the prompt was finished without being paused
it decrements R. In the latter case (barge-in), it operates using the three states as described in Section 2.
When a partial is recognized the Stability Score is computed and compared to the T1 threshold parame-
ter. If the score is below T1 the partial is discarded. Otherwise, if the model is in State 1 (the prompt is
on) the prompt is paused, a timer is started, and control transitions to State 2. If the model is in State 2
the timer is restarted. After transitioning to State 2, control only returns to State 1 if the timer exceeds
T2. At this time, the prompt is resumed and the resumption parameter R is incremented. Control im-
mediately transitions to State 3 if a final recognition result is received. The result is evaluated by the
dialogue manager, and the new speech act is returned. If the speech act indicates the recognition was not
understood successfully, the system either resumes (if in State 1) or continues (if in State 2). In the case
of resumption, R is incremented. If the new speech act indicates understanding success, the new speech
is immediately produced.
393
