On Statistical Parameter Setting 
Damir ?AVAR, Joshua HERRING, 
Toshikazu IKUTA, Paul RODRIGUES 
Linguistics Dept., Indiana University  
Bloomington, IN, 46405 
dcavar@indiana.edu 
Giancarlo SCHREMENTI 
Computer Science, Indiana University 
Bloomington, IN, 47405 
gischrem@indiana.edu 
 
Abstract 
We present a model and an experimental 
platform of a bootstrapping approach to 
statistical induction of natural language 
properties that is constraint based with voting 
components. The system is incremental and 
unsupervised. In the following discussion we 
focus on the components for morphological 
induction. We show that the much harder 
problem of incremental unsupervised 
morphological induction can outperform 
comparable all-at-once algorithms with 
respect to precision. We discuss how we use 
such systems to identify cues for induction in 
a cross-level architecture. 
1 Introduction 
In recent years there has been a growing amount 
of work focusing on the computational modeling 
of language processing and acquisition, implying a 
cognitive and theoretical relevance both of the 
models as such, as well as of the language 
properties extracted from raw linguistic data.1 In 
the computational linguistic literature several 
attempts to induce grammar or linguistic 
knowledge from such data have shown that at 
different levels a high amount of information can 
be extracted, even with no or minimal supervision. 
Different approaches tried to show how various 
puzzles of language induction could be solved. 
From this perspective, language acquisition is the 
process of segmentation of non-discrete acoustic 
input, mapping of segments to symbolic 
representations, mapping representations on 
higher-level representations such as phonology, 
morphology and syntax, and even induction of 
semantic properties. Due to space restrictions, we 
cannot discuss all these approaches in detail. We 
will focus on the close domain of morphology. 
Approaches to the induction of morphology as 
presented in e.g. Schone and Jurafsky (2001) or 
Goldsmith (2001) show that the morphological 
                                                     
1 See Batchelder (1998) for a discussion of these 
aspects. 
properties of a small subset of languages can be 
induced with high accuracy, most of the existing 
approaches are motivated by applied or 
engineering concerns, and thus make assumptions 
that are less cognitively plausible: a. Large corpora 
are processed all at once, though unsupervised 
incremental induction of grammars is rather the 
approach that would be relevant from a 
psycholinguistic perspective; b. Arbitrary decisions 
about selections of sets of elements are made, 
based on frequency or frequency profile rank,2 
though such decisions should rather be derived or 
avoided in general. 
However, the most important aspects missing in 
these approaches, however, are the link to different 
linguistic levels and the support of a general 
learning model that makes predictions about how 
knowledge is induced on different linguistic levels 
and what the dependencies between information at 
these levels are. Further, there is no study focusing 
on the type of supervision that might be necessary 
for the guidance of different algorithm types 
towards grammars that resemble theoretical and 
empirical facts about language acquisition, and 
processing and the final knowledge of language. 
While many theoretical models of language 
acquisition use innateness as a crutch to avoid 
outstanding difficulties, both on the general and 
abstract level of I-language as well as the more 
detailed level of E-language, (see, among others,  
Lightfoot (1999) and Fodor and Teller (2000), 
there is also significant research being done which 
shows that children take advantage of statistical 
regularities in the input for use in the language-
learning task (see Batchelder (1997) and related 
references within). 
In language acquisition theories the dominant 
view is that knowledge of one linguistic level is 
bootstrapped from knowledge of one, or even 
several different levels. Just to mention such 
approaches: Grimshaw (1981), and Pinker (1984) 
                                                     
2 Just to mention some of the arbitrary decisions 
made in various approaches, e.g. Mintz (1996) selects a 
small set of all words, the most frequent words, to 
induce word types via clustering ; Schone and Jurafsky 
(2001) select words with frequency higher than 5 to 
induce morphological segmentation. 
9
assume that semantic properties are used to 
bootstrap syntactic knowledge, and Mazuka (1998) 
suggested that prosodic properties of language 
establish a bias for specific syntactic properties, 
e.g. headedness or branching direction of 
constituents. However, these approaches are based 
on conceptual considerations and psycholinguistc 
empirical grounds, the formal models and 
computational experiments are missing. It is 
unclear how the induction processes across 
linguistic domains might work algorithmically, and 
the quantitative experiments on large scale data are 
missing. 
As for algorithmic approaches to cross-level 
induction, the best example of an initial attempt to 
exploit cues from one level to induce properties of 
another is presented in D?jean (1998), where 
morphological cues are identified for induction of 
syntactic structure. Along these lines, we will 
argue for a model of statistical cue-based learning, 
introducing a view on bootstrapping as proposed in 
Elghamry (2004), and Elghamry and ?avar (2004), 
that relies on identification of elementary cues in 
the language input and incremental induction and 
further cue identification across all linguistic 
levels. 
1.1 Cue-based learning 
Presupposing input driven learning, it has been 
shown in the literature that initial segmenations 
into words (or word-like units) is possible with 
unsupervised methods (e.g. Brent and Cartwright 
(1996)), that induction of morphology is possible 
(e.g. Goldsmith (2001), Schone and Jurafsky 
(2001)) and even the induction of syntactic 
structures (e.g. Van Zaanen (2001)). As mentioned 
earlier, the main drawback of these approaches is 
the lack of incrementality, certain arbitrary 
decisions about the properties of elements taken 
into account, and the lack of integration into a 
general model of bootstrapping across linguistic 
levels. 
As proposed in Elghamry (2004), cues are 
elementary language units that can be identified at 
each linguistic level, dependent or independent of 
prior induction processes. That is, intrinsic 
properties of elements like segments, syllables, 
morphemes, words, phrases etc. are the ones 
available for induction procedures. Intrinsic 
properties are for example the frequency of these 
units, their size, and the number of other units they 
are build of. Extrinsic properties are taken into 
account as well, where extrinsic stands for 
distributional properties, the context, relations to 
other units of the same type on one, as well as 
across linguistic levels. In this model, extrinsic and 
intrinsic properties of elementary language units 
are the cues that are used for grammar induction 
only. 
As shown in Elghamry (2004) and Elghamry and 
?avar (2004), there are efficient ways to identify a 
kernel set of such units in an unsupervised fashion 
without any arbitrary decision where to cut the set 
of elements and on the basis of what kind of 
features. They present an algorithm that selects the 
set of kernel cues on the lexical and syntactic level, 
as the smallest set of words that co-occurs with all 
other words. Using this set of words it is possible 
to cluster the lexical inventory into open and 
closed class words, as well as to identify the 
subclasses of nouns and verbs in the open class. 
The direction of the selectional preferences of the 
language is derived as an average of point-wise 
Mutual Information on each side of the identified 
cues and types, which is a self-supervision aspect 
that biases the search direction for a specific 
language. This resulting information is understood 
as derivation of secondary cues, which then can be 
used to induce selectional properties of verbs 
(frames), as shown in Elghamry (2004). 
The general claim thus is: 
? Cues can be identified in an unsupervised 
fashion in the input. 
? These cues can be used to induce properties of 
the target grammar. 
? These properties represent cues that can be 
used to induce further cues, and so on. 
The hypothesis is that this snowball effect can 
reduce the search space of the target grammar 
incrementally. The main research questions are 
now, to what extend do different algorithms 
provide cues for other linguistic levels and what 
kind of information do they require as supervision 
in the system, in order to gain the highest accuracy 
at each linguistic level, and how does the linguistic 
information of one level contribute to the 
information on another. 
In the following, the architectural considerations 
of such a computational model are discussed, 
resulting in an example implementation that is 
applied to morphology induction, where 
morphological properties are understood to 
represent cues for lexical clustering as well as 
syntactic structure, and vice versa, similar to the 
ideas formulated in D?jean (1998), among others. 
1.2 Incremental Induction Architecture 
The basic architectural principle we presuppose 
is incrementality, where incrementally utterances 
are processed. The basic language unit is an 
utterance, with clear prosodic breaks before and 
after. The induction algorithm consumes such 
utterances and breaks them into basic linguistic 
units, generating for each step hypotheses about 
10
the linguistic structure of each utterance, based on 
the grammar built so far and statistical properties 
of the single linguistic units. Here we presuppose a 
successful segmentation into words, i.e. feeding 
the system utterances with unambiguous word 
boundaries. We implemented the following 
pipeline architecture: 
 
The GEN module consumes input and generates 
hypotheses about its structural descriptions (SD). 
EVAL consumes a set of SDs and selects the set of 
best SDs to be added to the knowledge base. The 
knowledge base is a component that not only stores 
SDs but also organizes them into optimal 
representations, here morphology grammars. 
All three modules are modular, containing a set 
of algorithms that are organized in a specific 
fashion. Our intention is to provide a general 
platform that can serve for the evaluation and 
comparison of different approaches at every level 
of the induction process. Thus, the system is 
designed to be more general, applicable to the 
problem of segmentation, as well as type and 
grammar induction. 
We assume for the input to consist of an 
alphabet: a non-empty set A of n symbols {s1, s2,... 
sn}. A word w is a non-empty list of symbols w = 
[s1,s2,... sn], with s?A. The corpus is a non-empty 
list C of words C = [w1,w2,... wn]. 
In the following, the individual modules for the 
morphology induction task are described in detail. 
1.2.1 GEN 
For the morphology task GEN is compiled from a 
set of basically two algorithms. One algorithm is a 
variant of Alignment Based Learning (ABL), as 
described in Van Zaanen (2001). 
The basic ideas in ABL go back to concepts of 
substitutability and/or complementarity, as 
discussed in Harris (1961). The concept of 
substitutability generally applies to central part of 
the induction procedure itself, i.e. substitutable 
elements (e.g. substrings, words, structures) are 
assumed to be of the same type (represented e.g. 
with the same symbol). 
The advantage of ABL for grammar induction is 
its constraining characteristics with respect to the 
set of hypotheses about potential structural 
properties of a given input. While a brute-force 
method would generate all possible structural 
representations for the input in a first order 
explosion and subsequently filter out irrelevant 
hypotheses, ABL reduces the set of possible SDs 
from the outset to the ones that are motivated by 
previous experience/input or a pre-existing 
grammar. 
Such constraining characteristics make ABL 
attractive from a cognitive point of view, both 
because hopefully the computational complexity is 
reduced on account of the smaller set of potential 
hypotheses, and also because learning of new 
items, rules, or structural properties is related to a 
general learning strategy and previous experience 
only. The approaches that are based on a brute-
force first order explosion of all possible 
hypotheses with subsequent filtering of relevant or 
irrelevant structures are both memory-intensive 
and require more computational effort. 
The algorithm is not supposed to make any 
assumptions about types of morphemes. There is 
no expectation, including use of notions like stem, 
prefix, or suffix. We assume only linear sequences. 
The properties of single morphemes, being stems 
or suffixes, should be a side effect of their 
statistical properties (including their frequency and 
co-occurrence patterns, as will be explained in the 
following), and their alignment in the corpus, or 
rather within words. 
There are no rules about language built-in, such 
as what a morpheme must contain or how frequent 
it should be. All of this knowledge is induced 
statistically. 
In the ABL Hypotheses Generation, a given 
word in the utterance is checked against 
morphemes in the grammar. If an existing 
morpheme LEX aligns with the input word INP, a 
hypothesis is generated suggesting a 
morphological boundary at the alignment 
positions: 
INP (speaks) + LEX (speak) = HYP [speak, s] 
Another design criterion for the algorithm is 
complete language independence. It should be able 
to identify morphological structures of Indo-
European type of languages, as well as 
agglutinative languages (e.g. Japanese and 
Turkish) and polysynthetic languages like some 
Bantu dialects or American Indian languages. In 
order to guarantee this behavior, we extended the 
Alignment Based hypothesis generation with a 
pattern identifier that extracts patterns of character 
sequences of the types: 
1. A ? B ? A 
2. A ? B ? A ? B 
3. A ? B ? A ? C 
This component is realized with cascaded 
regular expressions that are able to identify and 
11
return the substrings that correspond to the 
repeating sequences.3 
All possible alignments for the existing grammar 
at the current state, are collected in a hypothesis 
list and sent to the EVAL component, described in 
the following. A hypothesis is defined as a tuple: 
H = <w, f, g>, with w the input word, f its 
frequency in C, and g a list of substrings that 
represent a linear list of morphemes in w, g = [ 
m1, m2, ... mn ]. 
1.2.2 EVAL 
EVAL is a voting based algorithm that subsumes 
a set of independent algorithms that judge the list 
of SDs from the GEN component, using statistical 
and information theoretic criteria. The specific 
algorithms are grouped into memory and usability 
oriented constraints. 
Taken as a whole, the system assumes two (often 
competing) cognitive considerations. The first of 
these forms a class of what we term ?time-based? 
constraints on learning. These constraints are 
concerned with the processing time required of a 
system to make sense of items in an input stream, 
whereby ?time? is understood to mean the number 
of steps required to generate or parse SDs rather 
than the actual temporal duration of the process. 
To that end, they seek to minimize the amount of 
structure assigned to an utterance, which is to say 
they prefer to deal with as few rules as possible. 
The second of these cognitive considerations forms 
a class of ?memory-based? constraints. Here, we 
are talking about constraints that seek to minimize 
the amount of memory space required to store an 
utterance by maximizing the efficiency of the 
storage process. In the specific case of our model, 
which deals with morphological structure, this 
means that the memory-based constraints search 
the input string for regularities (in the form of 
repeated substrings) that then need only be stored 
once (as a pointer) rather than each time they are 
found. In the extreme case, the time-based 
constraints prefer storing the input ?as is?, without 
any processing at all, where the memory-based 
constraints prefer a rule for every character, as this 
would assign maximum structure to the input. 
Parsable information falls out of the tension 
between these two conflicting constraints, which 
can then be applied to organize the input into 
potential syntactic categories. These can then be 
                                                     
3 This addition might be understood to be a sort of 
supervision in the system. However, as shown in recent 
research on human cognitive abilities, and especially on 
the ability to identify patterns in the speech signal by 
very young infants (Marcus et al 1999) shows that we 
can assume such an ability to be part of the cognitive 
abilities, maybe not even language specific 
used to set the parameters for the internal adult 
parsing system. 
Each algorithm is weighted. In the current 
implementation these weights are set manually. In 
future studies we hope to use the weighting for 
self-supervision.4 Each algorithm assigns a 
numerical rank to each hypothesis multiplied with 
the corresponding weight, a real number between 0 
and 1. 
On the one hand, our main interest lies in the 
comparison of the different algorithms and a 
possible interaction or dependency between them. 
Also, we expect the different algorithms to be of 
varying importance for different types of 
languages. 
Mutual Information (MI) 
For the purpose of this experiment we use a 
variant of standard Mutual Information (MI), see 
e.g. MacKay (2003). Information theory tells us 
that the presence of a given morpheme restricts the 
possibilities of the occurrence of morphemes to the 
left and right, thus lowering the amount of bits 
needed to store its neighbors. Thus we should be 
able to calculate the amount of bits needed by a 
morpheme to predict its right and left neighbors 
respectively. To calculate this, we have designed a 
variant of mutual information that is concerned 
with a single direction of information. 
This is calculated in the following way. For 
every morpheme y that occurs to the right of x we 
sum the point-wise MI between x and y, but we 
relativize the point-wise MI by the probability that 
y follows x, given that x occurs. This then gives us 
the expectation of the amount of information that x 
tells us about which morpheme will be to its right. 
Note that p(<xy>) is the probability of the bigram 
<xy> occurring and is not equal to p(<yx>) which 
is the probability of the bigram <yx> occurring. 
We calculate the MI on the right side of x?G by: 
p(< xy >| x)lg p(< xy >)
p(x)p(y)y?{<xY >}
?  
and the MI on the left of x?G respectively by: 
p(< yx >| x)lg p(< yx >)
p(y) p(x)y?{<Yx>)
?  
One way we use this as a metric, is by summing 
up the left and right MI for each morpheme in a 
                                                     
4 One possible way to self-supervise the weights in 
this architecture is by taking into account the revisions 
subsequent components make when they optimize the 
grammar. If rules or hypotheses have to be removed 
from the grammar due to general optimization 
constraints on the grammars as such, the weight of the 
responsible algorithm can be lowered, decreasing its 
general value in the system on the long run. The 
relevant evaluations with this approach are not yet 
finished. 
12
hypothesis. We then look for the hypothesis that 
results in the maximal value of this sum. The 
tendency for this to favor hypotheses with many 
morphemes is countered by our criterion of 
favoring hypotheses that have fewer morphemes, 
discussed later. 
Another way to use the left and right MI is in 
judging the quality of morpheme boundaries. In a 
good boundary, the morpheme on the left side 
should have high right MI and the morpheme on 
the right should have high left MI. Unfortunately, 
MI is not reliable in the beginning because of the 
low frequency of morphemes. However, as the 
lexicon is extended during the induction procedure, 
reliable frequencies are bootstrapping this 
segmentation evaluation. 
Minimum Description Length (DL) 
The principle of Minimum Description Length 
(MDL), as used in recent work on grammar 
induction and unsupervised language acquisition, 
e.g. Goldsmith (2001) and De Marcken (1996), 
explains the grammar induction process as an 
iterative minimization procedure of the grammar 
size, where the smaller grammar corresponds to the 
best grammar for the given data/corpus. 
The description length metric, as we use it here, 
tells us how many bits of information would be 
required to store a word given a hypothesis of the 
morpheme boundaries, using the so far generated 
grammar. For each morpheme in the hypothesis 
that doesn't occur in the grammar we need to store 
the string representing the morpheme. For 
morphemes that do occur in our grammar we just 
need to store a pointer to that morphemes entry in 
the grammar. We use a simplified calculation, 
taken from Goldsmith (2001), of the cost of storing 
a string that takes the number of bits of 
information required to store a letter of the 
alphabet and multiply it by the length of the string. 
lg(len(alphabet))* len(morpheme) 
We have two different methods of calculating 
the cost of the pointer. The first assigns a variable 
the cost based on the frequency of the morpheme 
that it is pointing to. So first we calculate the 
frequency rank of the morpheme being pointed to, 
(e.g. the most frequent has rank 1, the second rank 
2, etc.). We then calculate: 
floor(lg( freq_ rank) ?1)  
to get a number of bits similar to the way Morse 
code assigns lengths to various letters. 
The second is simpler and only calculates the 
entropy of the grammar of morphemes and uses 
this as the cost of all pointers to the grammar. The 
entropy equation is as follows: 
p(x)lg
1
p(x)x?G
?  
The second equation doesn't give variable 
pointer lengths, but it is preferred since it doesn't 
carry the heavy computational burden of 
calculating the frequency rank. 
We calculate the description length for each GEN 
hypothesis only,5 by summing up the cost of each 
morpheme in the hypothesis. Those with low 
description lengths are favored. 
Relative Entropy (RE) 
We are using RE as a measure for the cost of 
adding a hypothesis to the existing grammar. We 
look for hypotheses that when added to the 
grammar will result in a low divergence from the 
original grammar. 
We calculate RE as a variant of the Kullback-
Leibler Divergence, see MacKay (2003). Given 
grammar G1, the grammar generated so far, and G2 
the grammar with the extension generated for the 
new input increment, P(X) is the probability mass 
function (pmf) for grammar G2, and Q(X) the pmf 
for grammar G1: 
P(x)lg
P(x)
Q(x)x?X
?  
Note that with every new iteration a new element 
can appear, that is not part of G1. Our variant of RE 
takes this into account by calculating the costs for 
such a new element x to be the point-wise entropy 
of this element in P(X), summing up over all new 
elements: 
P(x)lg
1
P(x)x?X
?  
These two sums then form the RE between the 
original grammar and the new grammar with the 
addition of the hypothesis. Hypotheses with low 
RE are favored. 
This metric behaves similarly to description 
length, that is discussed above, in that both are 
calculating the distance between our original 
grammar and the grammar with the inclusion of the 
new hypothesis. The primary difference is RE also 
takes into account how the pmf differs in the two 
grammars and that our variation punishes new 
morphemes based upon their frequency relative to 
the frequency of other morphemes. Our 
implementation of MDL does not consider 
frequency in this way, which is why we are 
including RE as an independent metric. 
Further Metrics 
In addition to the mentioned metric, we take into 
account the following criteria: a. Frequency of 
                                                     
5 We do not calculate the sizes of the grammars with 
and without the given hypothesis, just the amount each 
given hypothesis would add to the grammar, favoring 
the least increase of total grammar size. 
13
morpheme boundaries; b. Number of morpheme 
boundaries; c. Length of morphemes. 
The frequency of morpheme boundaries is given 
by the number of hypotheses that contain this 
boundary. The basic intuition is that the higher this 
number is, i.e. the more alignments are found at a 
certain position within a word, the more likely this 
position represents a morpheme boundary. We 
favor hypotheses with high values for this 
criterion. 
The number of morpheme boundaries indicates 
how many morphemes the word was split into. To 
prevent the algorithm from degenerating into the 
state where each letter is identified as a morpheme, 
we favor hypotheses with low number of 
morpheme boundaries. 
The length of the morphemes is also taken into 
account. We favor hypotheses with long 
morphemes to prevent the same degenerate state as 
the above criterion. 
1.2.3 Linguistic Knowledge 
The acquired lexicon is stored in a hypothesis 
space which keeps track of the words from the 
input and the corresponding hypotheses. The 
hypothesis space is defined as a list of hypotheses: 
Hypotheses space: S = [ H1, H2, ... Hn] 
Further, each morpheme that occurred in the SDs 
of words in the hypothesis space is kept with its 
frequency information, as well as bigrams that 
consist of morpheme pairs in the SDs and their 
frequency.6 
Similar to the specification of signatures in 
Goldsmith (2001), we list every morpheme with 
the set of morphemes it co-occurs. Signatures are 
lists of morphemes. Grammar construction is 
performed by replacement of morphemes with a 
symbol, if they have equal signatures. 
The hypothesis space is virtually divided into 
two sections, long term and short term storage. 
Long term storage is not revised further, in the 
current version of the algorithm. The short term 
storage is cyclically cleaned up by eliminating the 
signatures with a low likelihood, given the long 
term storage. 
2 The experimental setting 
In the following we discuss the experimental 
setting. We used the Brown corpus,7 the child-
                                                     
6 Due to space restrictions we do not formalize this 
further. A complete documentation and the source code 
is available at: http://jones.ling.indiana.edu/~abugi/. 
7 The Brown Corpus of Standard American English, 
consisting of 1,156,329 words from American texts 
printed in 1961 organized into 59,503 utterances and 
compiled by W.N. Francis and H. Kucera at Brown 
University. 
oriented speech portion of the CHILDES Peter 
corpus,8 and Caesar?s ?De Bello Gallico? in Latin.9 
From the Brown corpus we used the files ck01 ? 
ck09, with an average number of 2000 words per 
chapter. The total number of words in these files is 
18071. The randomly selected portion of ?De Bello 
Gallico? contained 8300 words. The randomly 
selected portion of the Peter corpus contains 58057 
words. 
The system reads in each file and dumps log 
information during runtime that contains the 
information for online and offline evaluation, as 
described below in detail. 
The gold standard for evaluation is based on 
human segmentation of the words in the respective 
corpora. We create for every word a manual 
segmentation for the given corpora, used for online 
evaluation of the system for accuracy of hypothesis 
generation during runtime. Due to complicated 
cases, where linguist are undecided about the 
accurate morphological segmentation, a team of 5 
linguists was cooperating with this task. 
The offline evaluation is based on the grammar 
that is generated and dumped during runtime after 
each input file is processed. The grammar is 
manually annotated by a team of linguists, 
indicating for each construction whether it was 
segmented correctly and exhaustively. An 
additional evaluation criterion was to mark 
undecided cases, where even linguists do not 
agree. This information was however not used in 
the final evaluation. 
2.1 Evaluation 
We used two methods to evaluate the 
performance of the algorithm. The first analyzes 
the accuracy of the morphological rules produced 
by the algorithm after an increment of n words. 
The second looks at how accurately the algorithm 
parsed each word that it encountered as it 
progressed through the corpus.  
The morphological rule analysis looks at each 
grammar rule generated by the algorithm and 
judges it on the correctness of the rule and the 
resulting parse. A grammar rule consists of a stem 
and the suffixes and prefixes that can be attached 
to it, similar to the signatures used in Goldsmith 
(2001). The grammar rule was then marked as to 
whether it consisted of legitimate suffixes and 
prefixes for that stem, and also as to whether the 
                                                     
8 Documented in L. Bloom (1970) and available at 
http://xml.talkbank.org:8888/talkbank/file/CHILDES/E
ng-USA/Bloom70/Peter/. 
9 This was taken from the Gutenberg archive at: 
http://www.gutenberg.net/etext/10657. The Gutenberg 
header and footer were removed for the experimental 
run. 
14
stem of the rule was a true stem, as opposed to a 
stem plus another morpheme that wasn't identified 
by the algorithm. The number of rules that were 
correct in these two categories were then summed, 
and precision and recall figures were calculated for 
the trial. The trials described in the graph below 
were run on three increasingly large portions of the 
general fiction section of the Brown Corpus. The 
first trial was run on one randomly chosen chapter, 
the second trial on two chapters, and the third on 
three chapters. The graph shows the harmonic 
average (F-score) of precision and recall. 
 
The second analysis is conducted as the 
algorithm is running and examines each parse the 
system produces. The algorithm's parses are 
compared with the ?correct? morphological parse 
of the word using the following method to derive a 
numerical score for a particular parse. The first 
part of the score is the distance in characters 
between each morphological boundary in the two 
parses, with a score of one point for each character 
space. The second part is a penalty of two points 
for each morphological boundary that occurs in 
one parse and not the other. These scores were 
examined within a moving window of words that 
progressed through the corpus as the algorithm ran. 
The average scores of words in each such window 
were calculated as the window advanced. The 
purpose of this method was to allow the 
performance of the algorithm to be judged at a 
given point without prior performance in the 
corpus affecting the analysis of the current 
window. The following graph shows how the 
average performance of the windows of analyzed 
words as the algorithm progresses through five 
randomly chosen chapters of general fiction in the 
Brown Corpus amounting to around 10,000 words. 
The window size for the following graph was set to 
40 words. 
 
The evaluations on Latin were based on the 
initial 4000 words of ?De Bello Gallico? in a 
pretest. In the very initial phase we reached a 
precision of 99.5% and a recall of 13.2%. This is 
however the preliminary result for the initial phase 
only. We expect that for a larger corpus the recall 
will increase much higher, given the rich 
morphology of Latin, potentially with negative 
consequences for precision. 
The results on the Peter corpus are shown in the 
following table: 
After file precision recall 
01 .9957 .8326 
01-03 .9968 .8121 
01-05 .9972 .8019 
01-07 .9911 .7710 
01-09 .9912 .7666 
We notice a more or less stable precision value 
with decreasing recall, due to a higher number of 
words. The Peter corpus contains also many very 
specific transcriptions and tokens that are indeed 
unique, thus it is rather surprising to get such 
results at all. The following graphics shows the F-
score for the Peter corpus: 
 
3 Conclusion 
The evaluations on two related morphology 
systems show that with a restrictive setting of the 
parameters in the described algorithm, approx 99% 
precision can be reached, with a recall higher than 
60% for the portion of the Brown corpus, and even 
higher for the Peter corpus. 
We are able to identify phases in the generation 
of rules that turn out to be for English: a. initially 
inflectional morphology on verbs, with the plural 
?s? on nouns, and b. subsequently other types of 
morphemes. We believe that this phenomenon is 
purely driven by the frequency of these 
morphemes in the corpora. In the manually 
segmented portion of the Brown corpus we 
identified on the token level 11.3% inflectional 
morphemes, 6.4% derivational morphemes, and 
82.1% stems. In average there are twice as many 
inflectional morphemes in the corpus, than 
derivational. 
Given a very strict parameters, focusing on the 
description length of the grammar, our system 
would need long time till it would discover 
prefixes, not to mention infixes. By relaxing the 
weight of description length we can inhibit the 
15
generation and identification of prefixing rules, 
however, to the cost of precision. 
Given these results, the inflectional paradigms 
can be claimed to be extractable even with an 
incremental approach. As such, this means that 
central parts of the lexicon can be induced very 
early along the time line. 
The existing signatures for each morpheme can 
be used as simple clustering criteria.10 Clustering 
will separate dependent (affixes) from independent 
morphemes (stems). Their basic distinction is that 
affixes will usually have a long signature, i.e. 
many elements they co-occur with, as well as a 
high frequency, while for stems the opposite is 
true.11 Along these lines, morphemes with a similar 
signature can be replaced by symbols, expressing 
the same type information and compressing the 
grammar further. This type information, especially 
for rare morphemes is essential in subsequent 
induction of syntactic structure. Due to space 
limitations, we cannot discuss in detail subsequent 
steps in the cross-level induction procedures. 
Nevertheless, the model presented here provides an 
important pointer to the mechanics of how 
grammatical parameters might come to be set. 
Additionally, we provide a method by which to 
test the roles different statistical algorithms play in 
this process. By adjusting the weights of the 
contributions made by various constraints, we can 
approach an understanding of the optimal ordering 
of algorithms that play a role in the computational 
framework of language acquisition. 
This is but a first step to what we hope will 
eventually finish a platform for a detailed study of 
various induction algorithms and evaluation 
metrics. 
References  
E. O. Batchelder. 1997. Computational evidence for the 
use of frequency information in discovery of the 
infant?s first lexicon. PhD dissertation, CUNY. 
E. O. Batchelder. 1998. Can a computer really model 
cognition? A case study of six computational models 
of infant word discovery. In M. A. Gernsbacher and 
S. J. Derry, editors, Proceedings of the 20th Annual 
Conference of the Cognitive Science Society, pages 
120?125. Lawrence Erlbaum, University of 
Wisconsin-Madison. 
L. Bloom, L. Hood, and P. Lightbown. 1974. Imitation 
in language development: If, when and why. 
Cognitive Psychology, 6, 380?420. 
                                                     
10 Length of the signature and frequency of each 
morpheme are mapped on a feature vector. 
11 This way, similar to the clustering of words into 
open and closed class on the basis of feature vectors, as 
described in Elghamry and ?avar (2004), the 
morphemes can be separated into open and closed class. 
M.R. Brent and T.A. Cartwright. 1996. Distributional 
regularity and phonotactic constraints are useful for 
segmentation. Cognition 61: 93-125. 
H. D?jean. 1998. Concepts et alorithmes pour la 
d?couverte des structures formelles des langues. 
Doctoral dissertation, Universit? de Caen Basse 
Normandie. 
K. Elghamry. 2004. A generalized cue-based approach 
to the automatic acquisition of subcategorization 
frames. Doctoral dissertation, Indiana University. 
K. Elghamry and D. ?avar. 2004. Bootstrapping cues 
for cue-based bootstrapping. Mscr. Indiana 
University. 
J. Fodor and V. Teller. 2000. Decoding syntactic 
parameters: The superparser as oracle. Proceedings of 
the Twenty-Second Annual Conference of the 
Cognitive Science Society, 136-141. 
J. Goldsmith. 2001. Unsupervised learning of the 
morphology of a natural language. Computational 
Linguistics 27(2): 153-198. 
Z.S. Harris. 1961. Structural linguistics. University of 
Chicago Press. Chicago. 
J. Grimshaw. 1981. Form, function, and the language 
acquisition device. In C.L. Baker and J.J. McCarthy 
(eds.), The Logical Problem of Language Acquisition. 
Cambridge, MA: MIT Press. 
D.J.C. MacKay. 2003. Information Theory, Inference, 
and Learning Algorithms. Cambridge: Cambridge 
University Press. 
C.G. de Marcken. 1996. Unsupervised Language 
Acquisition. Phd dissertation, MIT. 
G.F. Marcus, S. Vijayan, S. Bandi Rao, and P.M. 
Vishton. 1999. Rule-learning in seven-month-old 
infants. Science 283:77-80. 
R. Mazuka. 1998. The Development of Language 
Processing Strategies: A cross-linguistic study 
between Japanese and English. Lawrence Erlbaum. 
T.H. Mintz. 1996. The roles of linguistic input and 
innate mechanisms in children's acquisition of 
grammatical categories. Unpublished doctoral 
dissertation, University of Rochester. 
S. Pinker. 1984. Language Learnability and Language 
Development, Harvard University Press, Cambridge, 
MA. 
S. Pinker. 1994. The language instinct. New York, NY: 
W. Morrow and Co. 
P. Schone and D. Jurafsky. 2001. Knowledge-Free 
Induction of Inflectional Morphologies. In 
Proceedings of NAACL-2001. Pittsburgh, PA, June 
2001. 
M.M. Van Zaanen and Pieter Adriaans. 2001. 
Comparing two unsupervised grammar induction 
systems: Alignment-based learning vs. EMILE. Tech. 
Rep. TR2001.05, University of Leeds. 
M.M. Van Zaanen. 2001. Bootstrapping Structure into 
Language: Alignment-Based Learning. Doctoral 
dissertation, The University of Leeds. 
16
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 78?86,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A random forest system combination approach for error detection in
digital dictionaries
Michael Bloodgood and Peng Ye and Paul Rodrigues
and David Zajic and David Doermann
University of Maryland
College Park, MD
meb@umd.edu, pengye@umiacs.umd.edu, prr@umd.edu,
dzajic@casl.umd.edu, doermann@umiacs.umd.edu
Abstract
When digitizing a print bilingual dictionary,
whether via optical character recognition or
manual entry, it is inevitable that errors are
introduced into the electronic version that is
created. We investigate automating the pro-
cess of detecting errors in an XML repre-
sentation of a digitized print dictionary us-
ing a hybrid approach that combines rule-
based, feature-based, and language model-
based methods. We investigate combin-
ing methods and show that using random
forests is a promising approach. We find
that in isolation, unsupervised methods ri-
val the performance of supervised methods.
Random forests typically require training
data so we investigate how we can apply
random forests to combine individual base
methods that are themselves unsupervised
without requiring large amounts of training
data. Experiments reveal empirically that
a relatively small amount of data is suffi-
cient and can potentially be further reduced
through specific selection criteria.
1 Introduction
Digital versions of bilingual dictionaries often
have errors that need to be fixed. For example,
Figures 1 through 5 show an example of an er-
ror that occurred in one of our development dic-
tionaries and how the error should be corrected.
Figure 1 shows the entry for the word ?turfah? as
it appeared in the original print copy of (Qureshi
and Haq, 1991). We see this word has three senses
with slightly different meanings. The third sense
is ?rare?. In the original digitized XML version
of (Qureshi and Haq, 1991) depicted in Figure 2,
this was misrepresented as not being the meaning
Figure 1: Example dictionary entry
Figure 2: Example of error in XML
of ?turfah? but instead being a usage note that fre-
quency of use of the third sense was rare. Figure 3
shows the tree corresponding to this XML repre-
sentation. The corrected digital XML representa-
tion is depicted in Figure 4 and the corresponding
corrected tree is shown in Figure 5.
Zajic et al (2011) presented a method for re-
pairing a digital dictionary in an XML format us-
ing a dictionary markup language called DML. It
remains time-consuming and error-prone however
to have a human read through and manually cor-
rect a digital version of a dictionary, even with
languages such as DML available. We therefore
investigate automating the detection of errors.
We investigate the use of three individual meth-
ods. The first is a supervised feature-based
method trained using SVMs (Support Vector Ma-
chines). The second is a language-modeling
78
.ENTRY
. .? ? ?
.
.SENSE
.USG
.rare
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 3: Tree structure of error
Figure 4: Example of error in XML, fixed
.ENTRY
. .? ? ?
.
.SENSE
.TRANS
.TR
.rare
.
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 5: Tree structure of error, fixed
method that replicates the method presented in
(Rodrigues et al, 2011). The third is a simple
rule inference method. The three individual meth-
ods have different performances. So we investi-
gate how we can combine the methods most effec-
tively. We experiment with majority vote, score
combination, and random forest methods and find
that random forest combinations work the best.
For many dictionaries, training data will not be
available in large quantities a priori and therefore
methods that require only small amounts of train-
ing data are desirable. Interestingly, for automati-
cally detecting errors in dictionaries, we find that
the unsupervised methods have performance that
rivals that of the supervised feature-based method
trained using SVMs. Moreover, when we com-
bine methods using the random forest method, the
combination of unsupervised methods works bet-
ter than the supervised method in isolation and al-
most as well as the combination of all available
methods. A potential drawback of using the ran-
dom forest combination method however is that it
requires training data. We investigated how much
training data is needed and find that the amount
of training data required is modest. Furthermore,
by selecting the training data to be labeled with
the use of specific selection methods reminiscent
of active learning, it may be possible to train the
random forest system combination method with
even less data without sacrificing performance.
In section 2 we discuss previous related work
and in section 3 we explain the three individual
methods we use for our application. In section 4
we explain the three methods we explored for
combining methods; in section 5 we present and
discuss experimental results and in section 6 we
conclude and discuss future work.
2 Related Work
Classifier combination techniques can be broadly
classified into two categories: mathematical and
behavioral (Tulyakov et al, 2008). In the first
category, functions or rules combine normalized
classifier scores from individual classifiers. Ex-
amples of techniques in this category include Ma-
jority Voting (Lam and Suen, 1997), as well as
simple score combination rules such as: sum rule,
min rule, max rule and product rule (Kittler et al,
1998; Ross and Jain, 2003; Jain et al, 2005). In
the second category, the output of individual clas-
sifiers are combined to form a feature vector as
79
the input to a generic classifier such as classifi-
cation trees (P. and Chollet, 1999; Ross and Jain,
2003) or the k-nearest neighbors classifier (P. and
Chollet, 1999). Our method falls into the second
category, where we use a random forest for sys-
tem combination.
The random forest method is described in
(Breiman, 2001). It is an ensemble classifier con-
sisting of a collection of decision trees (called a
random forest) and the output of the random for-
est is the mode of the classes output by the indi-
vidual trees. Each single tree is trained as follows:
1) a random set of samples from the initial train-
ing set is selected as a training set and 2) at each
node of the tree, a random subset of the features is
selected, and the locally optimal split is based on
only this feature subset. The tree is fully grown
without pruning. Ma et al (2005) used random
forests for combining scores of several biometric
devices for identity verification and have shown
encouraging results. They use all fully supervised
methods. In contrast, we explore minimizing the
amount of training data needed to train a random
forest of unsupervised methods.
The use of active learning in order to re-
duce training data requirements without sacri-
ficing model performance has been reported on
extensively in the literature (e.g., (Seung et al,
1992; Cohn et al, 1994; Lewis and Gale, 1994;
Cohn et al, 1996; Freund et al, 1997)). When
training our random forest combination of indi-
vidual methods that are themselves unsupervised,
we explore how to select the data so that only
small amounts of training data are needed because
for many dictionaries, gathering training data may
be expensive and labor-intensive.
3 Three Single Method Approaches for
Error Detection
Before we discuss our approaches for combining
systems, we briefly explain the three individual
systems that form the foundation of our combined
system.
First, we use a supervised approach where we
train a model using SVMlight (Joachims, 1999)
with a linear kernel and default regularization pa-
rameters. We use a depth first traversal of the
XML tree and use unigrams and bigrams of the
tags that occur as features for each subtree to
make a classification decision.
We also explore two unsupervised approaches.
The first unsupervised approach learns rules for
when to classify nodes as errors or not. The rule-
based method computes an anomaly score based
on the probability of subtree structures. Given
a structure A and its probability P(A), the event
that A occurs has anomaly score 1-P(A) and the
event that A does not occur has anomaly score
P(A). The basic idea is if a certain structure hap-
pens rarely, i.e. P(A) is very small, then the oc-
currence of A should have a high anomaly score.
On the other hand, if A occurs frequently, then
the absence of A indicates anomaly. To obtain
the anomaly score of a tree, we simply take the
maximal scores of all events induced by subtrees
within this tree.
The second unsupervised approach uses a reim-
plementation of the language modeling method
described in (Rodrigues et al, 2011). Briefly,
this methods works by calculating the probabil-
ity a flattened XML branch can occur, given a
probability model trained on the XML branches
from the original dictionary. We used (Stolcke,
2002) to generate bigram models using Good Tur-
ing smoothing and Katz back off, and evaluated
the log probability of the XML branches, ranking
the likelihood. The first 1000 branches were sub-
mitted to the hybrid system marked as an error,
and the remaining were submitted as a non-error.
Results for the individual classifiers are presented
in section 5.
4 Three Methods for Combining
Systems
We investigate three methods for combining the
three individual methods. As a baseline, we in-
vestigate simple majority vote. This method takes
the classification decisions of the three methods
and assigns the final classification as the classifi-
cation that the majority of the methods predicted.
A drawback of majority vote is that it does not
weight the votes at all. However, it might make
sense to weight the votes according to factors such
as the strength of the classification score. For ex-
ample, all of our classifiers make binary decisions
but output scores that are indicative of the confi-
dence of their classifications. Therefore we also
explore a score combination method that consid-
ers these scores. Since measures from the differ-
ent systems are in different ranges, we normal-
ize these measurements before combining them
(Jain et al, 2005). We use z-score which com-
80
putes the arithmetic mean and standard deviation
of the given data for score normalization. We then
take the summation of normalized measures as
the final measure. Classification is performed by
thresholding this final measure.1
Another approach would be to weight them by
the performance level of the various constituent
classifiers in the ensemble. Weighting based on
performance level of the individual classifiers is
difficult because it would require extra labeled
data to estimate the various performance lev-
els. It is not clear how to translate the differ-
ent performance estimates into weights, or how
to have those weights interact with weights based
on strengths of classification. Therefore, we did
not weigh based on performance level explicitly.
We believe that our third combination method,
the use of random forests, implicitly cap-
tures weighting based on performance level and
strengths of classifications. Our random forest ap-
proach uses three features, one for each of the in-
dividual systems we use. With random forests,
strengths of classification are taken into account
because they form the values of the three fea-
tures we use. In addition, the performance level
is taken into account because the training data
used to train the decision trees that form the for-
est help to guide binning of the feature values into
appropriate ranges where classification decisions
are made correctly. This will be discussed further
in section 5.
5 Experiments
This section explains the details of the experi-
ments we conducted testing the performance of
the various individual and combined systems.
Subsection 5.1 explains the details of the data we
experiment on; subsection 5.2 provides a sum-
mary of the main results of our experiments; and
subsection 5.3 discusses the results.
5.1 Experimental Setup
We obtained the data for our experiments using
a digitized version of (Qureshi and Haq, 1991),
the same Urdu-English dictionary that Zajic et
al. (2011) had used. Zajic et al (2011) pre-
sented DML, a programming language used to
fix errors in XML documents that contain lexico-
graphic data. A team of language experts used
1In our experiments we used 0 as the threshold.
Recall Precision F1-Measure Accuracy
LM 11.97 89.90 21.13 57.53
RULE 99.79 70.83 82.85 80.37
FV 35.34 93.68 51.32 68.14
Table 1: Performance of individual systems at
ENTRY tier.
DML to correct errors in a digital, XML repre-
sentation of the Kitabistan Urdu dictionary. The
current research compared the source XML doc-
ument and the DML commands to identify the el-
ements that the language experts decided to mod-
ify. We consider those elements to be errors. This
is the ground truth used for training and evalua-
tion. We evaluate at two tiers, corresponding to
two node types in the XML representation of the
dictionary: ENTRY and SENSE. The example de-
picted in Figures 1 through 5 shows an example of
SENSE. The intuition of the tier is that errors are
detectable (or learnable) from observing the ele-
ments within a tier, and do not cross tier bound-
aries. These tiers are specific to the Kitabistan
Urdu dictionary, and we selected them by observ-
ing the data. A limitation of our work is that we do
not know at this time whether they are generally
useful across dictionaries. Future work will be
to automatically discover the meaningful evalua-
tion tiers for a new dictionary. After this process,
we have a dataset with 15,808 Entries, of which
47.53% are marked as errors and 78,919 Senses,
of which 10.79% are marked as errors. We per-
form tenfold cross-validation in all experiments.
In our random forest experiments, we use 12 de-
cision trees, each with only 1 feature.
5.2 Results
This section presents experimental results, first
for individual systems and then for combined sys-
tems.
5.2.1 Performance of individual systems
Tables 1 and 2 show the performance of lan-
guage modeling-based method (LM), rule-based
method (RULE) and the supervised feature-based
method (FV) at different tiers. As can be seen,
at the ENTRY tier, RULE obtains the highest F1-
Measure and accuracy, while at the SENSE tier,
FV performs the best.
81
Recall Precision F1-Measure Accuracy
LM 9.85 94.00 17.83 90.20
RULE 84.59 58.86 69.42 91.96
FV 72.44 98.66 83.54 96.92
Table 2: Performance of individual systems at
SENSE tier.
5.2.2 Improving individual systems using
random forests
In this section, we show that by applying ran-
dom forests on top of the output of individual sys-
tems, we can have gains (absolute gains, not rel-
ative) in accuracy of 4.34% to 6.39% and gains
(again absolute, not relative) in F1-measure of
3.64% to 11.39%. Tables 3 and 4 show our ex-
perimental results at ENTRY and SENSE tiers
when applying random forests with the rule-based
method.2 These results are all obtained from 100
iterations of the experiments with different parti-
tions of the training data chosen at each iteration.
Mean values of different evaluation measures and
their standard deviations are shown in these ta-
bles. We change the percentage of training data
and repeat the experiments to see how the amount
of training data affects performance.
It might be surprising to see the gains in per-
formance that can be achieved by using a ran-
dom forest of decision trees created using only
the rule-based scores as features. To shed light
on why this is so, we show the distribution of
RULE-based output scores for anomaly nodes and
clean nodes in Figure 6. They are well separated
and this explains why RULE alone can have good
performance. Recall RULE classifies nodes with
anomaly scores larger than 0.9 as errors. How-
ever, in Figure 6, we can see that there are many
clean nodes with anomaly scores larger than 0.9.
Thus, the simple thresholding strategy will bring
in errors. Applying random forest will help us
identify these errorful regions to improve the per-
formance. Another method for helping to identify
these errorful regions and classify them correctly
is to apply random forest of RULE combined with
the other methods, which we will see will even
further boost the performance.
2We also applied random forests to our language mod-
eling and feature-based methods, and saw similar gains in
performance.
0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
0
500
1000
1500
output score of rule-based system
o
c
c
u
r
r
e
n
c
e
s
 
 
anomaly
clean
Figure 6: Output anomalies score from RULE
(ENTRY tier).
5.2.3 System combination
In this section, we explore different methods
for combining measures from the three systems.
Table 5 shows the results of majority voting and
score combination at the ENTRY tier. As can
be seen, majority voting performs poorly. This
may be due to the fact that the performances of
the three systems are very different. RULE sig-
nificantly outperforms the other two systems, and
as discussed in Section 4 neither majority voting
nor score combination weights this higher perfor-
mance appropriately.
Tables 6 and 7 show the results of combining
RULE and LM. This is of particular interest since
these two systems are unsupervised. Combin-
ing these two unsupervised systems works better
than the individual methods, including supervised
methods. Tables 8 and 9 show the results for com-
binations of all available systems. This yields the
highest performance, but only slightly higher than
the combination of only unsupervised base meth-
ods.
The random forest combination technique does
require labeled data even if the underlying base
methods are unsupervised. Based on the ob-
servation in Figure 6, we further study whether
choosing more training data from the most error-
ful regions will help to improve the performance.
Experimental results in Table 10 show how the
choice of training data affects performance. It
appears that there may be a weak trend toward
higher performance when we force the selection
of the majority of the training data to be from
ENTRY nodes whose RULE anomaly scores are
82
Training % Recall Precision F1-Measure Accuracy
0.1 78.17( 14.83) 75.87( 3.96) 76.18( 7.99) 77.68( 5.11)
1 82.46( 4.81) 81.34( 2.14) 81.79( 2.20) 82.61( 1.69)
10 87.30( 1.96) 84.11( 1.29) 85.64( 0.46) 86.10( 0.35)
50 89.19( 1.75) 83.99( 1.20) 86.49( 0.34) 86.76( 0.28)
Table 3: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 60.22( 12.95) 69.66( 9.54) 63.29( 7.92) 92.61( 1.57)
1 70.28( 3.48) 86.26( 3.69) 77.31( 1.39) 95.55( 0.25)
10 71.52( 1.23) 91.26( 1.39) 80.18( 0.41) 96.18( 0.07)
50 72.11( 0.75) 91.90( 0.64) 80.81( 0.39) 96.30( 0.06)
Table 4: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(SENSE tier)
larger than 0.9. However, the magnitudes of the
observed differences in performance are within a
single standard deviation so it remains for future
work to determine if there are ways to select the
training data for our random forest combination
in ways that substantially improve upon random
selection.
5.3 Discussion
Majority voting (at the entry level) performs
poorly, since the performance of the three individ-
ual systems are very different and majority voting
does not weight votes at all. Score combination
is a type of weighted voting. It takes into account
the confidence level of output from different sys-
tems, which enables it to perform better than ma-
jority voting. However, score combination does
not take into account the performance levels of
the different systems, and we believe this limits its
performance compared with random forest com-
binations.
Random forest combinations perform the best,
but the cost is that it is a supervised combination
method. We investigated how the amount of train-
ing data affects the performance, and found that a
small amount of labeled data is all that the random
forest needs in order to be successful. Moreover,
although this requires further exploration, there is
weak evidence that the size of the labeled data can
potentially be reduced by choosing it carefully
from the region that is expected to be most error-
ful. For our application with a rule-based system,
this is the high-anomaly scoring region because
although it is true that anomalies are often errors,
it is also the case that some structures occur rarely
but are not errorful.
RULE+LM with random forest is a little bet-
ter than RULE with random forest, with gain of
about 0.7% on F1-measure when evaluated at the
ENTRY level using 10% data for training.
An examination of examples that are marked as
being errors in our ground truth but that were not
detected to be errors by any of our systems sug-
gests that some examples are decided on the ba-
sis of features not yet considered by any system.
For example, in Figure 7 the second FORM is
well-formed structurally, but the Urdu text in the
first FORM is the beginning of the phrase translit-
erated in the second FORM. Automatic systems
detected that the first FORM was an error, how-
ever did not mark the second FORM as an error
whereas our ground truth marked both as errors.
Examination of false negatives also revealed
cases where the systems were correct that there
was no error but our ground truth wrongly indi-
cated that there was an error. These were due to
our semi-automated method for producing ground
truth that considers elements mentioned in DML
commands to be errors. We discovered instances
in which merely mentioning an element in a DML
command does not imply that the element is an er-
ror. These cases are useful for making refinements
to how ground truth is generated from DML com-
mands.
Examination of false positives revealed two
categories. One was where the element is indeed
an error but was not marked as an element in our
ground truth because it was part of a larger error
83
Method Recall Precision F1-Measure Accuracy
Majority voting 36.71 90.90 52.30 68.18
Score combination 76.48 75.82 76.15 77.23
Table 5: LM+RULE+FV (ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 77.43( 15.14) 72.77( 6.03) 74.26( 8.68) 75.32( 6.71)
1 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
10 88.12( 1.12) 84.65( 0.57) 86.34( 0.46) 86.76( 0.39)
50 89.12( 0.62) 87.39( 0.56) 88.25( 0.30) 88.72( 0.29)
Table 6: System combination based on random forest (LM+RULE). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
0.1 65.85( 12.70) 71.96( 7.63) 67.68( 7.06) 93.38( 1.03)
1 80.29( 3.58) 84.97( 3.13) 82.45( 1.36) 96.31( 0.28)
10 82.68( 2.49) 90.91( 2.37) 86.53( 0.41) 97.22( 0.07)
50 83.22( 2.43) 92.21( 2.29) 87.42( 0.35) 97.42( 0.04)
Table 7: System combination based on random forest (LM+RULE). (SENSE tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 91.57( 0.55) 87.77( 0.43) 89.63( 0.23) 89.93( 0.22)
50 92.04( 0.54) 88.85( 0.48) 90.41( 0.29) 90.72( 0.28)
Table 8: System combination based on random forest (LM+RULE+FV). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 86.47( 1.01) 90.67( 1.02) 88.51( 0.26) 97.58( 0.06)
50 86.50( 0.81) 92.04( 0.85) 89.18( 0.30) 97.73( 0.06)
Table 9: System combination based on random forest (LM+RULE+FV). (SENSE tier, mean (std))
Recall Precision F1-Measure Accuracy
50% 85.40( 4.65) 80.71( 3.49) 82.82( 1.57) 82.63( 1.54)
70% 86.13( 3.94) 80.97( 2.64) 83.36( 1.33) 83.30( 1.21)
90% 85.77( 3.61) 81.82( 2.72) 83.65( 1.45) 83.69( 1.35)
95% 85.93( 3.46) 82.14( 2.98) 83.89( 1.32) 83.94( 1.18)
random 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
Table 10: Effect of choice of training data based on rule based method (Mean evaluation measures
from 100 iterations of experiments using RULE+LM at ENTRY tier). We choose 1% of the data for
training and the first column in the table specifies the percentage of training data chosen from Entries
with anomalous score larger than 0.9.
84
Figure 7: Example of error in XML
that got deleted and therefore no DML command
ever mentioned the smaller element but lexicog-
raphers upon inspection agree that the smaller el-
ement is indeed errorful. The other category was
where there were actual errors that the dictionary
editors didn?t repair with DML but that should
have been repaired.
A major limitation of our work is testing how
well it generalizes to detecting errors in other dic-
tionaries besides the Urdu-English one (Qureshi
and Haq, 1991) that we conducted our experi-
ments on.
6 Conclusions
We explored hybrid approaches for the applica-
tion of automatically detecting errors in digitized
copies of dictionaries. The base methods we
explored consisted of a variety of unsupervised
and supervised methods. The combination meth-
ods we explored also consisted of some methods
which required labeled data and some which did
not.
We found that our base methods had differ-
ent levels of performance and with this scenario
majority voting and score combination methods,
though appealing since they require no labeled
data, did not perform well since they do not
weight votes well.
We found that random forests of decision trees
was the best combination method. We hypothe-
size that this is due to the nature of our task and
base systems. Random forests were able to help
tease apart the high-error region (where anoma-
lies take place). A drawback of random forests
as a combination method is that they require la-
beled data. However, experiments reveal empiri-
cally that a relatively small amount of data is suf-
ficient and the amount might be able to be further
reduced through specific selection criteria.
Acknowledgments
This material is based upon work supported, in
whole or in part, with funding from the United
States Government. Any opinions, findings and
conclusions, or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the views of the University of
Maryland, College Park and/or any agency or en-
tity of the United States Government. Nothing
in this report is intended to be and shall not be
treated or construed as an endorsement or recom-
mendation by the University of Maryland, United
States Government, or the authors of the product,
process, or service that is the subject of this re-
port. No one may use any information contained
or based on this report in advertisements or pro-
motional materials related to any company prod-
uct, process, or service or in support of other com-
mercial purposes.
References
Leo Breiman. 2001. Random forests. Machine
Learning, 45:5?32. 10.1023/A:1010933404324.
David A. Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15:201?221.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and
Naftali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28:133?168.
Anil K. Jain, Karthik Nandakumar, and Arun Ross.
2005. Score normalization in multimodal biometric
systems. Pattern Recognition, pages 2270?2285.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. Burges, and Alexander J. Smola, editors, Ad-
vances in Kernel Methods ? Support Vector Learn-
ing, chapter 11, pages 169?184. The MIT Press,
Cambridge, US.
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas.
1998. On combining classifiers. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
20(3):226 ?239, mar.
L. Lam and S.Y. Suen. 1997. Application of majority
voting to pattern recognition: an analysis of its be-
havior and performance. Systems, Man and Cyber-
netics, Part A: Systems and Humans, IEEE Trans-
actions on, 27(5):553 ?568, sep.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
SIGIR ?94: Proceedings of the 17th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 3?12,
85
New York, NY, USA. Springer-Verlag New York,
Inc.
Yan Ma, Bojan Cukic, and Harshinder Singh. 2005.
A classification approach to multi-biometric score
fusion. In AVBPA?05, pages 484?493.
Verlinde P. and G. Chollet. 1999. Comparing deci-
sion fusion paradigms using k-nn based classifiers,
decision trees and logistic regression in a multi-
modal identity verification application. In Proceed-
ings of the 2nd International Conference on Audio
and Video-Based Biometric Person Authentication
(AVBPA), pages 189?193.
Bashir Ahmad Qureshi and Abdul Haq. 1991. Stan-
dard Twenty First Century Urdu-English Dictio-
nary. Educational Publishing House, Delhi.
Paul Rodrigues, David Zajic, David Doermann,
Michael Bloodgood, and Peng Ye. 2011. Detect-
ing structural irregularity in electronic dictionaries
using language modeling. In Proceedings of the
Conference on Electronic Lexicography in the 21st
Century, pages 227?232, Bled, Slovenia, Novem-
ber. Trojina, Institute for Applied Slovene Studies.
Arun Ross and Anil Jain. 2003. Information fusion in
biometrics. Pattern Recognition Letters, 24:2115?
2125.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT ?92: Proceedings of
the fifth annual workshop on Computational learn-
ing theory, pages 287?294, New York, NY, USA.
ACM.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Sergey Tulyakov, Stefan Jaeger, Venu Govindaraju,
and David Doermann. 2008. Review of classi-
fier combination methods. In Machine Learning in
Document Analysis and Recognition, volume 90 of
Studies in Computational Intelligence, pages 361?
386. Springer Berlin / Heidelberg.
David Zajic, Michael Maxwell, David Doermann, Paul
Rodrigues, and Michael Bloodgood. 2011. Cor-
recting errors in digital lexicographic resources us-
ing a dictionary manipulation language. In Pro-
ceedings of the Conference on Electronic Lexicog-
raphy in the 21st Century, pages 297?301, Bled,
Slovenia, November. Trojina, Institute for Applied
Slovene Studies.
86
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 109?115,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
ArCADE: An Arabic Corpus of Auditory Dictation Errors
C. Anton Rytting
Paul Rodrigues
Tim Buckwalter
Valerie Novak
Aric Bills
University of Maryland
7005 52nd Avenue
College Park, MD 20742
{crytting,prr,tbuckwal,
vnovak,abills}@umd.edu
Noah H. Silbert
Communication
Sciences & Disorders
University of Cincinnati
2600 Clifton Avenue
Cincinnati, Ohio
silbernh
@ucmail.uc.edu
Mohini Madgavkar
Independent Researcher
6120 Dhaka Pl. 20189-6120
Dhaka, Bangladesh
mohini.madgavkar
@gmail.com
Abstract
We present a new corpus of word-level lis-
tening errors collected from 62 native En-
glish speakers learning Arabic designed to
inform models of spell checking for this
learner population. While we use the cor-
pus to assist in automated detection and
correction of auditory errors in electronic
dictionary lookup, the corpus can also be
used as a phonological error layer, to be
combined with a composition error layer
in a more complex spell-checking system
for non-native speakers. The corpus may
be useful to instructors of Arabic as a sec-
ond language, and researchers who study
second language phonology and listening
perception.
1 Introduction
Learner corpora have received attention as an im-
portant resource both for guiding teachers in cur-
riculum development (Nesselhauf, 2004) and for
providing training and evaluation material the de-
velopment of tools for computer-assisted language
learning (CALL). One of the most commonly used
technologies in CALL is spell correction. Spell
correction is used for providing automated feed-
back to language learners (cf. Warschauer and
Ware, 2006), automatic assessment (Bestgen and
Granger, 2011), and in providing cleaner input
to downstream natural language processing (NLP)
tools, thereby improving their performance (e.g.
Nagata et al., 2011). However, off-the-shelf spell
correctors developed for native speakers of the tar-
get language are of only limited use for repairing
language learners? spelling errors, since their error
patterns are different (e.g. Hovermale, 2011; Mit-
ton and Okada, 2007; Okada, 2005).
Most learner corpora (and spell correctors) are
understandably focused on learner-written texts.
Thus, they allow a greater understanding (and im-
provement) of learners? writing skills. However,
another important aspect of language learning is
listening comprehension (cf. Field, 2008; Prince,
2012). A better understanding of listening errors
can guide teachers and curriculum development
just as written production errors do. Listening er-
ror data may also be helpful for improving tech-
nologies for listening training tools, by helping
prioritize the most critical pairs of phonemes for
discrimination, and pointing out the most trouble-
some contexts for phoneme discrimination.
Finally, spell correction specifically designed
to correct listening errors may aid listening com-
prehension and vocabulary acquisition. If learn-
ers are unable to hear, recall and record accu-
rately what they heard, they will be less able to
search dictionaries or the Web for more informa-
tion on new vocabulary items they otherwise could
have learned from listening exercises. While data-
driven spelling correction on popular search en-
gines may catch some non-native errors, native er-
rors are likely to ?drown out? any non-native errors
they conflict with due to larger numbers of native
users of these search engines. On the other hand, if
themost common listening and transcription errors
are automatically corrected within a search tool,
learners will have greater success in finding the
new vocabulary items they may have misheard in
speech.
Learner corpora focused on written production
may not have enough samples of phonologically-
based errors to aid in developing such tools, and
109
even in a large corpus, word avoidance strategies
and other biases would make the source unreli-
able for estimating relative magnitudes of listening
problems accurately. It may be more effective to
target listening errors directly, through other tasks
such as listening dictation.
2 Related Work
Tools for language learning and maintenance, and
learner corpora fromwhich to build them, typically
focus on language pairs for which there is a large
market. Learner corpora for native English learn-
ers of low resource languages such as Arabic have
been until recently comparatively rare, and often
too small to be of practical use for the develop-
ment of educational technology. In the past few
years, however, a number of learner corpora for
Arabic have become available, including a corpus
of 19 non-native (mostlyMalaysian) students at Al
Al-Bayt University (Abu al-Rub, 2007); the Ara-
bic InterlanguageDatabase (ARIDA;Abuhakema
et al., 2008, 2009); the Arabic Learners Writ-
ten Corpus from the University of Arizona Center
for Educational Resources in Culture, Language,
and Literacy (CERCLL; Farwaneh and Tamimi,
2012);1 and the Arabic Learner Corpus v1 (Alfaifi
and Atwell, 2013).2
These corpora are all derived from learner writ-
ing samples, such as essays, and as such they con-
tainmany different types of errors, including errors
in morphology, syntax, and word choice. Spelling
errors are also observed, but relatively rarely, and
the relevance of these spelling errors to listening
competence is unclear. Hence, while they are
likely to be useful for many applications in teach-
ing Arabic writing, their usefulness for other pur-
poses, such as examining listening skills and the
effects of learner phonology on spelling, is limited.
Corpora or datasets focused on speaking and lis-
tening skills in Arabic are rarer. One such corpus,
the West Point Arabic Speech Corpus, available
from the LDC, contains one hour of non-native
(learner) speech (LaRocca and Chouairi, 2002)
Sethy et al. (2005) describe a corpus of elicited
Arabic speech, but because none of the partici-
pants had prior exposure to Arabic, its use for un-
1Available from http://l2arabiccorpus.cercll.
arizona.edu/?q=homepage.
2As of February 2014, a second version, with about 130K
words from non-native speakers, is available from http:
//www.arabiclearnercorpus.com/. It also has a small
(three hour) speech component.
derstanding learner Arabic is limited. While there
have been a few studies of Arabic listening skills
(e.g. Huthaily, 2008; Faircloth, 2013), their cov-
erage was not sufficiently broad to make reuse of
their data likely to inform such purposes as the de-
velopment of phoneme discrimination training or
other CALL technology.
3 Motivation
We present here the Arabic Corpus of Auditory
Dictation Errors (ArCADE) version 1, a corpus
of Arabic words as transcribed by 62 native En-
glish speakers learning Arabic. This corpus fills
the current gap in non-native spelling error cor-
pora, and particularly for spelling errors due to lis-
tening difficulties. Unlike error corpora collected
from non-native Arabic writing samples, it is de-
signed to elicit spelling errors arising from percep-
tual errors; it provides more naturalistic data than
is typical in phoneme identification or confusion
studies.
A principal purpose for creating the corpus was
to aid in the development and evaluation of tools
for detecting and correcting listening errors to aid
in dictionary lookup of words learners encountered
in spoken language (cf. Rytting et al., 2010). As
such, it serves as a complementary dataset for the
dictionary search engine?s query logs, since in this
case the intended target of each transcription is
known (rather than having to be inferred, in the
case of query logs). We list three other potential
uses for this corpus in Section 5.
4 Corpus Design and Creation
The ArCADE corpus was created through an
elicitation experiment, similar in structure to an
American-style spelling test. The principal differ-
ence (other than the language) is that in this case,
the participants are expected to be unfamiliar with
thewords, and thus forced to rely onwhat they hear
in the moment, rather than their lexical knowledge.
We selected words from a commonly-used dictio-
nary of Modern Standard Arabic such that the set
of words would contain a complete set of non-glide
consonants in various phonetic contexts.
4.1 Selection of Stimulus Words
Since the corpus was originally collected for a
study focused on the perception of consonants
within the context of real Arabic words, the stim-
ulus set was designed with three purposes in
110
mind: coverage of target sounds, exclusion of ba-
sic words, and brevity (so that participants could
complete the task in one sitting).
In order to differentiate consonants that are rela-
tively unpredictable (and thus test listening ability)
from consonants whose value could be predicted
from non-acoustic cues (such as prior knowledge
of morphological structure), the corpus is anno-
tated for target consonants vs. non-target conso-
nants. A target consonant is defined as a consonant
that should not be predictable (assuming the word
is unknown to the listener) except by the acoustic
cues alone. Glides /w/ and /j/ were not targeted
in the study because orthographic ambiguities be-
tween glides and vowels would complicate the er-
ror analysis.
Each Arabic consonant other than the glides oc-
curs as a target consonant in the stimulus set in six
consonant/vowel/word-boundary contexts: C_V,
V_C, V_V, #_V, V_#, and C_#.3 (The contexts
#_C and C_C are phonotactically illegal in Mod-
ern Standard Arabic.)
Consonants that were judged morphologically
predictable within a word were considered non-
target consonants. These included: (1) non-root
consonants, when Semitic roots were known to the
researchers; (2) consonants participating in a redu-
plicative pattern such as /tamtam/ and /zalzala/;
and (3) Consonants found in doubled (R2=R3)
roots if the two consonants surfaced separately
(e.g., in broken plurals such as /?asnan/).
We excluded words from our stimulus set if
we anticipated that an intermediate Arabic student
would already be familiar with them or would eas-
ily be able to guess their spellings. Items found
in vocabulary lists associated with two commonly-
used introductory textbooks (Al-Kitaab and Alif-
Baa) were excluded (Brustad et al., 2004a,b).
Loanwords from Western languages were also ex-
cluded, as were well-known place names (e.g.,
/?iskotlanda/ = ?Scotland?). Words found only in
colloquial dialects and terms that might be offen-
sive or otherwise distracting (as judged by native
speaker of Arabic) were removed, as well.
In order to keep the stimulus set as short as pos-
sible while maintaining coverage of the full set of
target stimuli consonants in each targeted context,
we chose words with multiple target consonants
whenever possible. The final set of 261words con-
3C = consonant, V = vowel, # = word boundary, and ?_?
(underscore) = location of target consonant.
tained 649 instances of target consonants: one in-
stance of each geminate consonant and between 17
and 50 instances of each singleton consonant (at
least two instances for each of the six contexts),
with a few exceptions.4 Although glides and vow-
els were not specifically targeted, 6 instances of
/w/, 10 instances of /j/, and at least 12 instances of
each of the monophthong vowels (/a/, /i/, /u/, /a:/,
/i:/, /u:/) occur in the stimulus set.
4.2 Recording of the Stimuli
The audio data used in the dictation was recorded
in a sound-proof boothwith a unidirectionalmicro-
phone (Earthworks SR30/HC) equippedwith a pop
filter, and saved as WAV files (stereo, 44.1kHz,
32-bit) with Adobe Audition. The stimuli were
spoken at a medium-fast rate. The audio files were
segmented and normalized with respect to peak
amplitude with Matlab.
The nativeArabic speaker in the audio recording
is of Egyptian and Levantine background, but was
instructed to speak with a neutral (?BBC Arabic?)
accent.
4.3 Participants and Methodology
Seventy-five participants were recruited from six
universities. To be eligible, participants had to be
18 years of age or older, native speakers of En-
glish, and have no known history of speech lan-
guage pathology or hearing loss. Participants were
required to have completed at least two semesters
of university level Arabic courses in order to en-
sure that they were able to correctly write the Ara-
bic characters and to transcribe Arabic speech.
Heritage speakers of Arabic and non-English dom-
inant bilinguals were excluded from the study. The
corpus contains responses from 62 participants.
The mean duration of Arabic study completed was
5.6 semesters (median 4).
Before beginning the experiment, participants
were asked to fill out a biographical questionnaire.
This included questions about language exposure
during childhood and languages studied in a class-
room setting. There were additional questions
about time spent outside of the United States to
ascertain possible exposure to languages not ad-
dressed in previous questions.
4These exceptions include only one instance of a phone
rather than two for the following contexts: (1) /h/ in the con-
text C_#, (2) /f/ in the context V_#, and (3) /z/ in the context
#_V. One geminate consonant, /x:/, was inadvertently omitted
from the stimulus set.
111
Participants wrote their responses to the 261
stimulus words on a response sheet that contained
numbered boxes. They were asked to use Arabic
orthography with full diacritics and short vowels
(fatha, damma, kasra, shadda and sukun). The
shadda (gemination) mark was required in order
to analyze the participants? perception of geminate
consonants; the other diacritics were included so as
to not single out shadda for special attention (since
participants were na?ve to the purpose of the study)
and also to increase the value of the resulting error
corpus for later analysis of short vowels.
4.4 Presentation of the Stumuli
The proctors who ran the experiment supplied an
iPod Touch tablet to each participant, pre-loaded
with a custom stimuli presentation application.
In this custom iPod application, 261 Arabic
words were randomized into 9 stimulus sets. Each
stimulus set was preceded by four practice items
which were not scored; thus each participant saw
265 items. Each touch screen tablet was initialized
by the testers to deliver a specific stimulus set. A
button on the touch screen allowed the participants
to begin the experiment. After a few seconds? de-
lay, the first word was played. A stimulus num-
ber identifying the word appeared in a large font
to aid the participants in recording the word on pa-
per. Participants were given 15 seconds to write
their response, before the tablet automatically ad-
vanced to the next word. Participants were not able
to replay a word.
The participants used noise-canceling head-
phones (Audio-Technica ATH-ANC7 or ATH-
ANC7B) for listening to the audio stimuli. The
experiment was performed in a quiet classroom.
4.5 Data Coding
The participants? handwritten responses were
typed in as they were written, using Arabic Uni-
code characters. Any diacritics (short vowels or
gemination) written by the participants were pre-
served. An automatic post-process was used to en-
sure that the gemination mark was ordered prop-
erly with respect to an adjacent short vowel mark.
The corpus consists of twomain sections: ortho-
graphic and phonemic. The orthographic section is
very simple: each stimulus word is given in its tar-
get orthography (with diacritics) and in each par-
ticipant?s corresponding orthographic transcrip-
tion (including diacritics if the participant provided
them as instructed). The phonemic section is more
elaborate, containing additional fields designed for
a phone level analysis of target consonants. Its
construction is described in further detail below.
Both the orthographic response and the canon-
ical (reference) spelling were automatically con-
verted to a phonemic representation. This conver-
sion normalizes certain orthographic distinctions,
such as various spellings for word-final vowels.
This phonemic representation of the response for
each stimulus item was then compared with the
phonemic representation of the item?s canonical
pronunciation, and each phoneme of the response
was aligned automatically with the most probable
phoneme (or set of equally plausible phonemes)
in the canonical phonemic representation of the
auditory stimulus. This alignment was done via
dynamic programming with a weighted Leven-
shtein edit distance metric. Specifically, weights
were used to favor the alignment of vowels and
glides with each other rather than with non-glide
consonants (since the scope of our original study
was non-glide consonants). Thus substitutions be-
tween short vowels, long vowels, and glides are
given preference over other confusions. This is in-
tended to reduce the ambiguity of the alignments
and to ensure that non-glide consonants are aligned
with non-glide consonants when possible, without
introducing any bias in the non-glide consonants
alignments. When one unique alignment had the
lowest cost, it was used as the alignment for that
item. In some cases, multiple alignments were tied
for minimal cost. In this case, all alignments were
used and assigned equal probability.
Once the least-cost alignment(s) were found be-
tween a response string and the reference string for
an item, the target consonants within the reference
string were then each paired with the correspond-
ing phonemes in the response, and an error cate-
gory (<substitution>, <deletion>, or <match> for
no error) was assigned. In the case of geminate
phonemes, two subtypes of <substitution>were in-
troduced: <gemination> and <degemination>.
Where an entire word had no response, ?NA?
was used to indicate that no edit operation can be
assigned. (A total of 112 items were missing).
Note that insertions were not marked, because
only the 649 instances of target consonants were
analyzed for the phonemic portion of the corpus,
and no other material in each stimulus word (in-
cluding any possible insertion points for additional
material) were annotated for errors. Insertions can
112
be recovered from the orthographic portion of the
corpus.
The coding method described above yielded a
set of 41,121 target consonant records of partici-
pants? responses to target consonants (not count-
ing the 112 non-response items), including 29,634
matches (72.1%) and 11,487 errors (27.9%). At
the word level, there are 16,217 words, of which
8321 (48.2%) contain at least one error in a tar-
geted consonant, and 5969 (37.1%) are spelled
perfectly (excluding diacritics).
5 Potential Uses of the Corpus
In addition to the uses described in Section  3, we
believe the data could be used for several other
uses, such as examining linguistic correlates of
proficiency, developing phonemic training, and in-
vestigating non-native Arabic handwriting.
One potential use of the corpus is to analyze the
errors by individual learners to determine which
sounds are confused only by relatively beginning
learners (after two semesters) and which are con-
fused by beginning and experienced learners alike.
While hard measures of proficiency are not avail-
able for the participants, the language question-
naire includes time of study and self-report mea-
sures of proficiency. To the extent to which these
proxies are reliable, the corpus may lead to the de-
velopment of hypotheses which can be tested in
more targeted studies.
Since the corpus allows quantitative evidence
for the relative difficulty of particular sound pairs
in particular contexts, it may guide the prioritiza-
tion of foci for phonemic discrimination training
and other listening exercises. At the most basic
level, a teacher can take our original audio stimuli
and use them as dictation exercises for beginning
students (who may not be ready for sentence or
paragraph level dictation). It may also form the ba-
sis for automated phonemic discrimination train-
ing, such as Michael et al. (2013). Cf. Bradlow
(2008) for a review.
Since the participants handwrote their re-
sponses, the corpus contains, as a byproduct, a
set of 16,329 words in non-native handwriting and
their digital transcriptions. As Alfaifi and Atwell
(2013) note, this could be used as a corpus of non-
native handwriting for training or evaluating OCR
on L2 Arabic script. If corresponding native tran-
scriptions of the same (or similar) strings were ob-
tained, the corpus could also be used to differenti-
ate native from non-native handwriting (cf. Farooq
et al., 2006; Ramaiah et al., 2013).
6 Limitations and future work
The corpus as it currently stands has some limita-
tions worth noting. First, there is no control set
of native Arabic listeners to provide a comparison
point for distinguishing non-native perceptual er-
rors from acoustic errors that even native speakers
are subject to. Second, the survey does not con-
tain proficiency ratings (except self-report) for the
participants, making direct correlation of particu-
lar confusion patterns with proficiency level more
difficult.
Statistical analysis of the participants? accuracy
at distinguishing Arabic consonants is currently
underway (Silbert et al., in preparation). An inves-
tigation of the utility of the corpus for training and
evaluating spelling correction for L1 English late
learners of Arabic, including the effects of training
corpus size on accuracy, is also in progress.
7 Conclusion
The Arabic Corpus of Auditory Dictation Errors
(ArCADE) version 1 provides a corpus of word-
level transcriptions of Arabic speech by native En-
glish speakers learning Arabic, ideal for the anal-
ysis of within-word listening errors, as well as the
development and evaluation of NLP tools that seek
to aid either in developing listening skill or in com-
pensating for typical non-native deficits in listen-
ing. Since most learner corpora only include writ-
ten composition or spoken production from stu-
dents, this corpus fills a gap in the resources avail-
able for the study of Arabic as a second language.
The corpus, along with the original audio
stimuli and participants? handwriting samples,
is available at http://www.casl.umd.edu/
datasets/cade/arcade/index.html.
Acknowledgments
This material is based on work supported, in whole
or in part, with funding from the United States
Government. Any opinions, findings, and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of the University of Maryland,
College Park and/or any agency or entity of the
United States Government.
113
References
Muhammad Abu al-Rub. 2007. ???????? ??????? ?????
.?????? ???????? ??????? ????? ?????? ??? ??????? ?????? ???
Tahl??l al-akht??? al-kit?b?yah ?ala mustaw?
al-iml?? lad? muta?allim? al-lughah al-?arab?yah
al-n?ti?q?na bi-ghayrih? [Analysis of written
spelling errors among non-native speak-
ing learners of Arabic]. ????????? ?????? ???????
.??????????? Dir?s?t, al-?Ul?m al-Ins?n?yah wa-al-
Ijtim???yah [Humanities and Social Sciences],
34(2). http://journals.ju.edu.jo/
DirasatHum/article/view/1911/1898.
Ghazi Abuhakema, Anna Feldman, and Eileen
Fitzpatrick. 2008. Annotating an Arabic learner
corpus for error. In Proceedings of the Interna-
tional Conference on Language Resources and
Evaluation (LREC 2008). Marrakech, Morocco.
Ghazi Abuhakema, Anna Feldman, and Eileen
Fitzpatrick. 2009. ARIDA: An Arabic inter-
language database and its applications: A pilot
study. Journal of the National Council of Less
Commonly Taught Languages (NCOLCTL),
7:161?184.
Abdullah Alfaifi and Eric Atwell. 2013. Potential
uses of the Arabic Learner Corpus. In Leeds
Language, Linguistics, and Translation PGR
Conference 2013. University of Leeds, Leeds,
UK.
Yves Bestgen and Sylvaine Granger. 2011. Cat-
egorising spelling errors to assess L2 writ-
ing. International Journal of Continuing En-
gineering Education and Life-Long Learning,
21(2/3):235?252.
Ann Bradlow. 2008. Training non-native language
sound patterns. In Phonology and Second Lan-
guage Acquisition, Benjamins, Amsterdam and
Philadelphia, pages 287?308.
Kristin Brustad, Mahmoud Al-Batal, and Abbas
Al-Tonsi. 2004a. Al-Kitaab fii Ta?allum al-
?Arabiyya, volume 1. Georgetown University
Press, Washington, DC, 1st edition.
Kristin Brustad, Mahmoud Al-Batal, and Abbas
Al-Tonsi. 2004b. Alif Baa: Introduction to Ara-
bic Letters and Sounds. Georgetown University
Press, Washington, DC, 2nd edition.
Laura Rose Faircloth. 2013. The L2 Perception
of Phonemic Distinctions in Arabic by English
Speakers. BA Thesis, The College of William
and Mary. https://digitalarchive.wm.
edu/bitstream/handle/10288/18160/
FairclothLauraRose2013Thesis.pdf?
sequence=1.
Faisal Farooq, Liana Lorigo, and Venu Govin-
daraju. 2006. On the accent in handwrit-
ing of individuals. In Tenth Interna-
tional Workshop on Frontiers in Hand-
writing Recognition. La Baule, France.
http://hal.inria.fr/docs/00/11/26/
30/PDF/cr103741695994.pdf.
Samira Farwaneh and Mohammed Tamimi. 2012.
Arabic learners written corpus: A resource for
research and learning. Available from the
University of Arizona Center for Educational
Resources in Culture, Language, and Literacy
web site. http://l2arabiccorpus.cercll.
arizona.edu/?q=homepage.
John Field. 2008. Listening in the Language Class-
room. Cambridge University Press, Cambridge,
UK.
DJ Hovermale. 2011. Erron: A Phrase-Based
Machine Traslation Approach to Customized
Spelling Correction. Ph.D. thesis, The Ohio
State University.
Khaled Yahya Huthaily. 2008. Second Lan-
guage Instruction with Phonological Knowl-
edge: Teaching Arabic to Speakers of English.
Ph.D. thesis, The University of Montana.
Col. Stephen A. LaRocca and Rajaa Chouairi.
2002. West Point Arabic speech corpus. Tech-
nical report, LDC, Philadelphia.
Erica B. Michael, Greg Colflesh, Valerie Karuzis,
Michael Key, Svetlana Cook, Noah H. Silbert,
Christopher Green, Evelyn Browne, C. Anton
Rytting, Eric Pelzl, and Michael Bunting. 2013.
Perceptual training for second language speech
perception: Validation study to assess the ef-
ficacy of a new training regimen (TTO 2013).
Technical report, University of Maryland Cen-
ter for Advanced Study of Language, College
Park, MD.
RogerMitton and Takeshi Okada. 2007. The adap-
tation of an English spellchecker for Japanese
writers. Birbeck ePrints, London. http://
eprints.bbk.ac.uk/archive/00000592.
Ryo Nagata, Edward Whittaker, and Vera Shein-
man. 2011. Creating a manually error-tagged
and shallow-parsed learner corpus. In Proceed-
ings of the 49th Annual Meeting of the Asso-
114
ciation for Computational Linguistics. Associ-
ation for Computational Linguistics, Portland,
OR, pages 1210?1219.
Nadja Nesselhauf. 2004. Learner corpora and their
potential in language teaching. In How to Use
Corpora in Language Teaching, Benjamins,
Amsterdam and Philadelphia, pages 125?152.
Takeshi Okada. 2005. Spelling errors made by
Japanese EFL writers: with reference to errors
occurring at the word-initial and word-final po-
sitions. In Vivian Cook and Benedetta Bassetti,
editors, Second language writing systems, Mul-
tilingual Matters, Clevedon, UK, pages 164?
183.
Peter Prince. 2012. Writing it down: Issues re-
lating to the use of restitution tasks in listening
comprehension. TESOL Journal, 3(1):65?86.
Chetan Ramaiah, Arti Shivram, and Venu
Govindaraju. 2013. A Baysian framework
for modeling accents in handwriting. In
12th International Conference on Docu-
ment Analysis and Recognition (ICDAR).
http://ieeexplore.ieee.org/xpls/abs_
all.jsp?arnumber=6628752.
C. Anton Rytting, Paul Rodrigues, Tim Buckwal-
ter, DavidM. Zajic, Bridget Hirsch, Jeff Carnes,
Nathanael Lynn, Sarah Wayland, Chris Taylor,
Jason White, Charles Blake, Evelyn Browne,
Corey Miller, and Tristan Purvis. 2010. Error
correction for Arabic dictionary lookup. In Sev-
enth International Conference on Language Re-
sources and Evaluation (LREC 2010). Valletta,
Malta.
Abhinav Sethy, Shrikanth Narayanan, Nicolaus
Mote, and W. Lewis Johnson. 2005. Modeling
and automating detection of errors inArabic lan-
guage learner speech. In INTERSPEECH-2005.
pages 177?180.
Noah H. Silbert, C. Anton Rytting, Paul Ro-
drigues, Tim Buckwalter, Valerie Novak, Mo-
hiniMadgavkar, Katharine Burk, andAric Bills.
in preparation. Similarity and bias in non-native
Arabic consonant perception.
Mark Warschauer and Paige Ware. 2006. Auto-
mated writing evaluation: Defining the class-
room research agenda. Language Teaching Re-
search, 10(2):157?180.
115
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 102?106,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The IUCL+ System: Word-Level Language Identification via Extended
Markov Models
Levi King, Eric Baucom, Timur Gilmanov, Sandra K
?
ubler, Daniel Whyatt
Indiana University
{leviking,eabaucom,timugilm,skuebler,dwhyatt}@indiana.edu
Wolfgang Maier
Universita?t Du?sseldorf
maierw@hhu.de
Paul Rodrigues
University of Maryland
prr@umd.edu
Abstract
We describe the IUCL+ system for the shared
task of the First Workshop on Computational
Approaches to Code Switching (Solorio et al.,
2014), in which participants were challenged
to label each word in Twitter texts as a named
entity or one of two candidate languages. Our
system combines character n-gram probabili-
ties, lexical probabilities, word label transition
probabilities and existing named entity recog-
nition tools within a Markovmodel framework
that weights these components and assigns a
label. Our approach is language-independent,
and we submitted results for all data sets
(five test sets and three ?surprise? sets, cov-
ering four language pairs), earning the high-
est accuracy score on the tweet level on two
language pairs (Mandarin-English, Arabic-
dialects 1 & 2) and one of the surprise sets
(Arabic-dialects).
1 Introduction
This shared task challenged participants to perform
word level analysis on short, potentially bilingual Twit-
ter and blog texts covering four language pairs: Nepali-
English, Spanish-English,Mandarin-English andMod-
ern Standard Arabic-Arabic dialects. Training sets
ranging from 1,000 to roughly 11,000 tweets were pro-
vided for the language pairs, where the content of the
tweets was tokenized and labeled with one of six la-
bels. The goal of the task is to accurately replicate
this annotation automatically on pre-tokenized texts.
With an inventory of six labels, however, the task is
more than a simple binary classification task. In gen-
eral, the most common labels observed in the train-
ing data are lang1 and lang2, with other (mainly
covering punctuation and emoticons) also common.
Named entities (ne) are also frequent, and accounting
for them adds a significant complication to the task.
Less common are mixed (to account for words that
may e.g., apply L1 morphology to an L2 word), and
ambiguous (to cover a word that could exist in either
language, e.g., no in the Spanish-English data).
Traditionally, language identification is performed
on the document level, i.e., on longer segments of
text than what is available in tweets. These methods
are based on variants of character n-grams. Seminal
work in this area is by Beesley (1988) and Grefenstette
(1995). Lui and Baldwin (2014) showed that character
n-grams also perform on Twitter messages. One of a
few recent approaches working on individual words is
by King et al. (2014), who worked on historical data;
see also work by Nguyen and Dogruz (2013) and King
and Abney (2013).
Our system is an adaptation of a Markov model,
which integrates lexical, character n-gram, and la-
bel transition probabilities (all trained on the provided
data) in addition to the output of pre-existing NER
tools. All the information sources are weighted in the
Markov model.
One advantage of our approach is that it is language-
independent. We use the exact same architecture for
all language pairs, and the only difference for the indi-
vidual language pairs lies in a manual, non-exhaustive
search for the best weights. Our results show that the
approachworks well for the one language pair with dif-
ferent writing systems (Mandarin-English) as well as
for the most complex language pair, the Arabic set. In
the latter data set, the major difficulty consists in the
extreme skewing with an overwhelming dominance of
words in Modern Standard Arabic.
2 Method
Our system uses an extension of a Markov model to
perform the task of word level language identification.
The system consists of three main components, which
produce named entity probabilities, emission probabil-
ities and label transition probabilities. The outputs of
these three components are weighted and combined in-
side the extended Markov model (eMM), where the
best tag sequence for a given tweet (or sentence) is de-
termined via the Viterbi algorithm.
In the following sections, we will describe these
components in more detail.
2.1 Named Entity Recognition
We regard named entity recognition (NER) as a stand-
alone task, independent of language identification. For
this reason, NER is performed first in our system.
In order to classify named entities in the tweets, we
employ two external tools, Stanford-NER and Twit-
terNLP. Both systems are used in a black box approach,
102
without any attempt at optimization. I.e., we use the
default parameters where applicable.
Stanford NER (Finkel et al., 2005) is a state-of-the-
art named entity recognizer based on conditional ran-
dom fields (CRF), which can easily be trained on cus-
tom data.
1
For all of the four language pairs, we train a
NER model on a modified version of the training data
in which we have kept the label ?ne? as our target la-
bel, but replaced all others with the label ?O?. Thus, we
create a binary classification problem of distinguishing
named entities from all other words. This method is
applicable for all data sets.
For the Arabic data, we additionally employ a
gazetteer, namely ANERgazet (Benajiba and Rosso,
2008).
2
However, we do not use the three classes (per-
son, location, organization) available in this resource.
The second NER tool used in our system is the Twit-
terNLP package.
3
This system was designed specifi-
cally for Twitter data. It deals with the particular dif-
ficulties that Twitter-specific language (due to spelling,
etc.) poses to named entity recognition. The system has
been shown to be very successful: Ritter et al. (2011,
table 6) achieve an improvement of 52% on segmen-
tation F-score in comparison with Stanford NER on
hand-annotated Twitter data, which is mainly due to a
considerably increased recall.
The drawback of using TwitterNLP for our task is
that it was developed for English, and adapting it to
other languages would involve a major redesign and
adaptation of the system. For this reason, we decided
to use it exclusively on the language pairs that include
English. An inspection of the training data showed that
for all language pairs involving English, a majority of
the NEs are written in English and should thus be rec-
ognizable by the system.
TwitterNLP is an IOB tagger. Since we do not dis-
tinguish between the beginning and the rest of a named
entity, we change all corresponding labels to ?ne? in
the output of the NER system.
In testing mode, the NER tools both label each word
in a tweet as either ?O? or ?ne?. We combine the output
such that ?ne? overrides ?O? in case of any disagree-
ments, and pass this information to the eMM. This out-
put is weighted with optimized weights unique to each
language pair that were obtained through 10-fold cross
validation, as discussed below. Thus, the decisions of
the NER systems is not final, but they rather provide
evidence that can be overruled by other system compo-
nents.
2.2 Label Transition Models
The label transition probability component models lan-
guage switches on the sequence of words. It is also
1
See http://nlp.stanford.edu/software/
CRF-NER.shtml.
2
As available from http://users.dsic.upv.es/
grupos/nle/.
3
See https://github.com/aritter/
twitter_nlp.
trained on the provided training data. In effect, this
component consists of unigram, bigram, and trigram
probability models of the sequences of labels found
in the training data. Our MM is second order, thus
the transition probabilities are linear interpolations of
the uni-, bi-, and trigram label transition probabili-
ties that were observed in the training data. We add
two beginning-of-sentence buffer labels and one end-
of-sentence buffer label to assist in deriving the start-
ing and ending probabilities of each label during the
training.
2.3 Emission Probabilities
The emission probability component is comprised of
two subcomponents: a lexical probability component
and a character n-gram probability component. Both
are trained on the provided training data.
Lexical probabilities: The lexical probability com-
ponent consists of a dictionary for each label contain-
ing the words found under that label and their rel-
ative frequencies. Each word type and its count of
tokens are added to the total for each respective la-
bel. After training, the probability of a given label
emitting a word (i.e., P (word|label)) is derived from
these counts. To handle out-of-vocabulary words, we
use Chen-Goodman ?one-count? smoothing, which ap-
proximates the probabilities of unknownwords as com-
pared to the occurrence of singletons (Chen and Good-
man, 1996).
Character n-gram probabilities: The character-
based n-grammodel serves mostly as a back-off in case
a word is out-of-vocabulary, in which case the lexi-
cal probability may not be reliable. However, it also
provides important information in the case of mixed
words, which may use morphology from one language
added to a stem from the other one. In this setting, un-
igrams are not informative. For this reason, we select
longer n-grams, with n ranging between 2 and 5.
Character n-gram probabilities are calculated as fol-
lows: For each training set, the words in that training
set are sorted into lists according to their labels. In
training models for each value of n, n ? 1 buffer char-
acters are added to the beginning and end of each word.
For example, in creating a trigram character model
for the lang1 (English) words in the Nepali-English
training set, we encounter the word star. We first gen-
erate the form $$star##, then derive the trigrams. The
trigrams from all training words are counted and sorted
into types, and the counts are converted to relative fre-
quencies.Thus, using four values of n for a data set
containing six labels, we obtain 24 character n-gram
models for that language pair. Note that because this
component operates on individual words, character n-
grams never cross a word boundary.
In testing mode, for each word and for each value of
n, the component generates a probability that the word
occurred under each of the six labels. These values
103
are passed to the eMM, which uses manually optimized
weights for each value of n to combine the four n-gram
scores for each label into a single n-gram score for each
label. In cases where an n-gram from the test word
was not present in the training data, we use a primitive
variant of LaPlace smoothing, which returns a fixed,
extremely low non-zero probability for that n-gram.
2.4 The Extended Markov Model
Our approach is basically a trigram Markov model
(MM), in which the observations are the words in
the tweet (or blog sentence) and the underlying states
correspond to the sequence of codeswitching labels
(lang1, lang2, ne, mixed, ambiguous,
other). The MM, as usual, also uses starting
and ending probabilities (in our case, derived from
standard training of the label transition model, due
to our beginning- and end-of-sentence buffer labels),
label/state transition probabilities, and probabilities
that the state labels will emit particular observations.
The only difference is that we modify the standard
HMM emission probabilities. We call this resulting
Markov model extended (eMM).
First, for every possible state/label in the sequence,
we linearly interpolate ?lexical (emission) probabil-
ities? P
lex
(the standard emission probabilities for
HMMs) with character n-gram probabilities P
char
.
That is, we choose 0 ? ?
lex
? 1 and 0 ? ?
char
? 1
such that ?
lex
+ ?
char
= 1. We use them to derive
a new emission probability P
combined
= ?
lex
? P
lex
+
?
char
?P
char
. This probability represents the likelihood
that the given label in the hidden layer will emit the lex-
ical observation, along with its corresponding character
n-gram sequence.
Second, only for ne labels in the hidden layer, we
modify the probabilities that they will emit the ob-
served word if that word has been judged by our NER
module to be a named entity. Since the NER compo-
nent exhibits high precision but comparatively low re-
call, we boost the P
combined
(label = ne|word) if the
observedword is judged to be a named entity, but we do
not penalize the regular P
combined
if not. This boosting
is accomplished via linear interpolation and another set
of parameters, 0 ? ?
ne
? 1 and 0 ? ?
combined
? 1
such that ?
ne
+ ?
combined
= 1. Given a positive de-
cision from the NER module, the new probability for
the ne label emitting the observed word is derived as
P
ne+combined
= ?
ne
? 0.80 + ?
combined
? P
combined
,
i.e., we simply interpolate the original probability with
a high probability. All lambda values, as well as the
weights for the character n-gram probabilities, were set
via 10-fold cross-validation, discussed below.
2.5 Cross Validation & Optimization
In total, the system uses 11 weights, each of which is
optimized for each language pair. In labeling named
entities, the output of the NER component is given one
weight and the named entity probabilities of the other
sources (emission and label transition components) is
given another weight, with these weights summing to
one. For the label transition component, the uni-, bi-
and trigram scores receive weights that sum to one.
Likewise, the emission probability component is com-
prised of the lexical probability and the character n-
gram probability, with weights that sum to one. The
character n-gram component is itself comprised of the
bi-, tri-, four- and five-gram scores, again with weights
that sum to one.
For each language pair, these weights were opti-
mized using a 10-fold cross validation script that splits
the original training data into a training file and a test
file, runs the split files through the system and averages
the output. As time did not allow an exhaustive search
for optimal weights in this multi-dimensional space, we
narrowed the space by first manually optimizing each
subset of weights independently, then exploring com-
binations of weights in the resulting neighborhood.
3 Results
3.1 Main Results
The results presented in this section are the official re-
sults provided by the organizers. The evaluation is split
into two parts: a tweet level evaluation and a token level
evaluation. On the tweet level, the evaluation concen-
trates on the capability of systems to distinguish mono-
lingual from multilingual tweets. The token level eval-
uation is concerned with the classification of individ-
ual words into the different classes: lang1, lang2,
ambiguous, mixed, ne, and other.
Our results for the tweet level evaluation, in com-
parison to the best or next-best performing system are
shown in table 1. They show that our system is ca-
pable of discriminating monolingual from multilingual
tweets with very high precision. This resulted in the
best results in the evaluation with regard to accuracy
for Mandarin-English and for both Arabic-dialects set-
tings. We note that for the latter setting, reaching good
results is exceedingly difficult without any Arabic re-
sources. This task is traditionally approached by us-
ing a morphological analyzer, but we decided to use
a knowledge poor approach. This resulted in a rather
high accuracy but in low precision and recall, espe-
cially for the first Arabic test set, which was extremely
skewed, with only 32 out of 2332 tweets displaying
codeswitching.
Our results for the token level evaluation, in com-
parison to the best performing system per language,
are shown in table 2. They show that our system sur-
passed the baseline for both language pairs for which
the organizers provided baselines. In terms of accu-
racy, our system is very close to the best performing
system for the pairs Spanish-English andMandarin En-
glish. For the other language pairs, we partially suffer
from a weak NER component. This is especially obvi-
ous for the Arabic dialect sets. However, this is also a
problem that can be easily fixed by using a more com-
104
lang. pair system Acc. Recall Precision F-score
Nep.-Eng. IUCL+ 91.2 95.6 94.9 95.2
dcu-uvt 95.8 99.4 96.1 97.7
Span.-Eng. IUCL+ 83.8 51.4 87.7 64.8
TAU 86.8 72.0 80.3 75.9
Man.-Eng. IUCL+ 82.4 94.3 85.0 89.4
MSR-India 81.8 95.5 83.7 89.2
Arab. dia. IUCL+ 97.4 12.5 11.1 11.8
MSR-India 94.7 34.4 9.7 15.2
Arab. dia. 2 IUCL+ 76.6 24.9 27.1 26.0
MSR-India 71.4 21.2 18.3 19.6
Table 1: Tweet level results in comparison to the system with (next-)highest accuracy.
lang1 lang2 mixed ne
lang. pair system Acc. R P F R P F R P F R P F
Nep.-Eng. IUCL+ 75.2 85.1 89.1 87.1 68.9 97.6 80.8 1.7 100 3.3 55.1 48.7 51.7
dcu-uvt 96.3 97.9 95.2 96.5 98.8 96.1 97.4 3.3 50.0 6.3 45.6 80.4 58.2
base 70.0 57.1 76.5 65.4 92.3 62.8 74.7 0.0 100 0.0 0.0 100 0.0
Span.-Eng. IUCL+ 84.4 88.9 82.3 85.5 85.1 89.9 87.4 0.0 100 0.0 30.4 48.5 37.4
TAU 85.8 90.0 83.0 86.4 86.9 91.4 89.1 0.0 100 0.0 31.3 54.1 39.6
base 70.3 85.1 67.6 75.4 78.1 72.8 75.4 0.0 100 0.0 0.0 100 0.0
Man.-Eng. IUCL+ 89.5 98.3 97.8 98.1 83.9 66.6 74.2 0.0 100 0.0 70.1 50.3 58.6
MSR-India 90.4 98.4 97.6 98.0 89.1 66.6 76.2 0.0 100 0.0 67.7 65.2 66.4
Arab. dia. IUCL+ 78.8 96.1 81.6 88.2 34.8 8.9 14.2 ? ? ? 3.3 23.4 5.8
CMU 91.0 92.2 97.0 94.6 57.4 4.9 9.0 ? ? ? 77.8 70.6 74.0
Arab. dia. 2 IUCL+ 51.9 90.7 43.8 59.0 47.7 78.3 59.3 0.0 0.0 0.0 8.5 28.6 13.1
CMU 79.8 85.4 69.0 76.3 76.1 87.3 81.3 0.0 100 0.0 68.7 78.8 73.4
Table 2: Token level results in comparison to the system with highest accuracy (results for ambiguous and
other are not reported).
lang1 lang2 ne
lang. pair system Acc. R P F R P F R P F
Nep.-Eng. IUCL+ 80.5 86.1 78.8 82.3 97.6 80.9 88.5 29.9 80.9 43.7
JustAnEagerStudent 86.5 91.3 80.2 85.4 93.6 91.1 92.3 39.4 83.3 53.5
Span.-Eng. IUCL+ 91.8 87.4 81.9 84.5 84.5 87.4 85.9 28.5 47.4 35.6
dcu-uvt 94.4 87.9 80.5 84.0 84.1 86.7 85.4 22.4 55.2 31.9
Arab. dia. IUCL+ 48.9 91.7 33.3 48.8 48.4 81.9 60.9 3.3 17.6 5.5
CMU 77.5 87.6 55.5 68.0 75.6 89.8 82.1 52.3 73.8 61.2
Table 3: Token level results for the out-of-domain data.
petitive, language dependent system. Another problem
constitutes the mixed cases, which cannot be reliably
annotated.
3.2 Out-Of-Domain Results
The shared task organizers provided ?surprise? data,
from domains different from the training data. Our re-
sults on those data sets are shown in table 3. For space
reasons, we concentrate on the token level results only.
The results show that our system is very robust with
regard to out-of-domain settings. For Nepali-English
and Spanish-English, we reach higher results than on
the original test sets, and for the Arabic dialects, the re-
sults are only slightly lower. These results need further
analysis for us to understand how our system performs
in such situations.
4 Conclusions
We have presented the IUCL+ system for word level
language identification. Our system is based on a
Markov model, which integrates different types of in-
formation, including the named entity analyses, lexical
and character n-gram probabilities as well as transition
probabilities. One strength of the system is that it is
completely language independent. The results of the
shared task have shown that the system generally pro-
vides reliable results, and it is fairly robust in an out-
of-domain setting.
105
References
Kenneth R. Beesley. 1988. Language identifier: A
computer program for automatic natural-language
identification of on-line text. In Proceedings of the
29th Annual Conference of the American Translators
Association, volume 47, page 54.
Yassine Benajiba and Paolo Rosso. 2008. Arabic
named entity recognition using conditional random
fields. In Proceedings of Workshop on HLT & NLP
within the Arabic World, LREC 2008, Marakech,
Morroco.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
pages 310?318.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 363?370.
Gregory Grefenstette. 1995. Comparing two language
identification schemes. In Proceedings of the Third
International Conference on Statistical Analysis of
Textual Data (JADT), volume 2.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-languagedocuments using
weakly supervised methods. In Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
Levi King, Sandra Ku?bler, and Wallace Hooper. 2014.
Word-level language identification in The Chymistry
of Isaac Newton. Literary and Linguistic Comput-
ing.
Marco Lui and Timothy Baldwin. 2014. Accurate lan-
guage identification of Twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden.
Dong Nguyen and A. Seza Dogruz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural LanguageProcess-
ing, pages 857?862.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1524?1534, Edinburgh, Scotland,
UK., July. Association for Computational Linguis-
tics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Julia Hirshberg, Alison Chang, and Pas-
cale Fung. 2014. Overview for the first shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar.
106
