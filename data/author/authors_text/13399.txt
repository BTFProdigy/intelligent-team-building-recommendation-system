Heterogeneous Automatic MT Evaluation
Through Non-Parametric Metric Combinations
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
Combining different metrics into a single
measure of quality seems the most direct
and natural way to improve over the quality
of individual metrics. Recently, several ap-
proaches have been suggested (Kulesza and
Shieber, 2004; Liu and Gildea, 2007; Al-
brecht and Hwa, 2007a). Although based
on different assumptions, these approaches
share the common characteristic of being
parametric. Their models involve a num-
ber of parameters whose weight must be
adjusted. As an alternative, in this work,
we study the behaviour of non-parametric
schemes, in which metrics are combined
without having to adjust their relative im-
portance. Besides, rather than limiting to
the lexical dimension, we work on a wide
set of metrics operating at different linguis-
tic levels (e.g., lexical, syntactic and se-
mantic). Experimental results show that
non-parametric methods are a valid means
of putting different quality dimensions to-
gether, thus tracing a possible path towards
heterogeneous automatic MT evaluation.
1 Introduction
Automatic evaluation metrics have notably acceler-
ated the development cycle of MT systems in the
last decade. There exist a large number of metrics
based on different similarity criteria. By far, the
most widely used metric in recent literature is BLEU
(Papineni et al, 2001). Other well-known metrics
are WER (Nie?en et al, 2000), NIST (Doddington,
2002), GTM (Melamed et al, 2003), ROUGE (Lin
and Och, 2004a), METEOR (Banerjee and Lavie,
2005), and TER (Snover et al, 2006), just to name
a few. All these metrics take into account informa-
tion at the lexical level1, and, therefore, their re-
liability depends very strongly on the heterogene-
ity/representativity of the set of reference transla-
tions available (Culy and Riehemann, 2003). In
order to overcome this limitation several authors
have suggested taking advantage of paraphrasing
support (Zhou et al, 2006; Kauchak and Barzilay,
2006; Owczarzak et al, 2006). Other authors have
tried to exploit information at deeper linguistic lev-
els. For instance, we may find metrics based on full
constituent parsing (Liu and Gildea, 2005), and on
dependency parsing (Liu and Gildea, 2005; Amigo?
et al, 2006; Mehay and Brew, 2007; Owczarzak et
al., 2007). We may find also metrics at the level
of shallow-semantics, e.g., over semantic roles and
named entities (Gime?nez and Ma`rquez, 2007), and
at the properly semantic level, e.g., over discourse
representations (Gime?nez, 2007).
However, none of current metrics provides, in iso-
lation, a global measure of quality. Indeed, all met-
rics focus on partial aspects of quality. The main
problem of relying on partial metrics is that we may
obtain biased evaluations, which may lead us to de-
rive inaccurate conclusions. For instance, Callison-
Burch et al (2006) and Koehn and Monz (2006)
have recently reported several problematic cases re-
lated to the automatic evaluation of systems ori-
ented towards maximizing different quality aspects.
Corroborating the findings by Culy and Riehemann
(2003), they showed that BLEU overrates SMT sys-
tems with respect to other types of systems, such
1ROUGE and METEOR may consider morphological vari-
ations. METEOR may also look up for synonyms in WordNet.
319
as rule-based, or human-aided. The reason is that
SMT systems are likelier to match the sublanguage
(e.g., lexical choice and order) represented by the
set of reference translations. We argue that, in order
to perform more robust, i.e., less biased, automatic
MT evaluations, different quality dimensions should
be jointly taken into account.
A natural solution to this challenge consists in
combining the scores conferred by different metrics,
ideally covering a heterogeneous set of quality as-
pects. In the last few years, several approaches to
metric combination have been suggested (Kulesza
and Shieber, 2004; Liu and Gildea, 2007; Albrecht
and Hwa, 2007a). In spite of working on a lim-
ited set of quality aspects, mostly lexical features,
these approaches have provided effective means of
combining different metrics into a single measure of
quality. All these methods implement a parametric
combination scheme. Their models involve a num-
ber of parameters whose weight must be adjusted
(see further details in Section 2).
As an alternative path towards heterogeneous MT
evaluation, in this work, we explore the possibility
of relying on non-parametric combination schemes,
in which metrics are combined without having to ad-
just their relative importance (see Section 3). We
have studied their ability to integrate a wide set of
metrics operating at different linguistic levels (e.g.,
lexical, syntactic and semantic) over several evalu-
ation scenarios (see Section 4). We show that non-
parametric schemes offer a valid means of putting
different quality dimensions together, effectively
yielding a significantly improved evaluation quality,
both in terms of human likeness and human accept-
ability. We have also verified that these methods port
well across test beds.
2 Related Work
Approaches to metric combination require two im-
portant ingredients:
Combination Scheme, i.e., how to combine sev-
eral metric scores into a single score. As
pointed out in Section 1, we distinguish be-
tween parametric and non-parametric schemes.
Meta-Evaluation Criterion, i.e., how to evaluate
the quality of a metric combination. The two
most prominent meta-evaluation criteria are:
? Human Acceptability: Metrics are evalu-
ated in terms of their ability to capture the
degree of acceptability to humans of auto-
matic translations, i.e., their ability to em-
ulate human assessors. The underlying as-
sumption is that ?good? translations should
be acceptable to human evaluators. Hu-
man acceptability is usually measured on
the basis of correlation between automatic
metric scores and human assessments of
translation quality2.
? Human Likeness: Metrics are evaluated in
terms of their ability to capture the fea-
tures which distinguish human from au-
tomatic translations. The underlying as-
sumption is that ?good? translations should
resemble human translations. Human
likeness is usually measured on the basis
of discriminative power (Lin and Och,
2004b; Amigo? et al, 2005).
In the following, we describe the most relevant
approaches to metric combination suggested in re-
cent literature. All are parametric, and most of them
are based on machine learning techniques. We dis-
tinguish between approaches relying on human like-
ness and approaches relying on human acceptability.
2.1 Approaches based on Human Likeness
The first approach to metric combination based
on human likeness was that by Corston-Oliver et
al. (2001) who used decision trees to distinguish
between human-generated (?good?) and machine-
generated (?bad?) translations. They focused on
evaluating only the well-formedness of automatic
translations (i.e., subaspects of fluency), obtaining
high levels of classification accuracy.
Kulesza and Shieber (2004) extended the ap-
proach by Corston-Oliver et al (2001) to take into
account other aspects of quality further than fluency
alone. Instead of decision trees, they trained Support
Vector Machine (SVM) classifiers. They used fea-
tures inspired by well-known metrics such as BLEU,
NIST, WER, and PER. Metric quality was evaluated
both in terms of classification accuracy and correla-
tion with human assessments at the sentence level.
2Usually adequacy, fluency, or a combination of the two.
320
A significant improvement with respect to standard
individual metrics was reported.
Gamon et al (2005) presented a similar approach
which, in addition, had the interesting property that
the set of human and automatic translations could
be independent, i.e., human translations were not re-
quired to correspond, as references, to the set of au-
tomatic translations.
2.2 Approaches based on Human Acceptability
Quirk (2004) applied supervised machine learning
algorithms (e.g., perceptrons, SVMs, decision trees,
and linear regression) to approximate human quality
judgements instead of distinguishing between hu-
man and automatic translations. Similarly to the
work by Gamon et al (2005) their approach does
not require human references.
More recently, Albrecht and Hwa (2007a; 2007b)
re-examined the SVM classification approach by
Kulesza and Shieber (2004) and, inspired by the
work of Quirk (2004), suggested a regression-based
learning approach to metric combination, with and
without human references. The regression model
learns a continuous function that approximates hu-
man assessments in training examples.
As an alternative to methods based on machine
learning techniques, Liu and Gildea (2007) sug-
gested a simpler approach based on linear combina-
tions of metrics. They followed a Maximum Corre-
lation Training, i.e., the weight for the contribution
of each metric to the overall score was adjusted so
as to maximize the level of correlation with human
assessments at the sentence level.
As expected, all approaches based on human ac-
ceptability have been shown to outperform that of
Kulesza and Shieber (2004) in terms of human ac-
ceptability. However, no results in terms of human
likeness have been provided, thus leaving these com-
parative studies incomplete.
3 Non-Parametric Combination Schemes
In this section, we provide a brief description of the
QARLA framework (Amigo? et al, 2005), which is,
to our knowledge, the only existing non-parametric
approach to metric combination. QARLA is non-
parametric because, rather than assigning a weight
to the contribution of each metric, the evaluation of
a given automatic output a is addressed through a
set of independent probabilistic tests (one per met-
ric) in which the goal is to falsify the hypothesis that
a is a human reference. The input for QARLA is a
set of test cases A (i.e., automatic translations), a set
of similarity metrics X, and a set of models R (i.e.,
human references) for each test case. With such a
testbed, QARLA provides the two essential ingredi-
ents required for metric combination:
Combination Scheme Metrics are combined inside
the QUEEN measure. QUEEN operates under
the unanimity principle, i.e., the assumption
that a ?good? translation must be similar to
all human references according to all metrics.
QUEENX(a) is defined as the probability, over
R ? R ? R, that, for every metric in X, the
automatic translation a is more similar to a hu-
man reference r than two other references, r?
and r??, to each other. Formally:
QUEENX,R(a) = Prob(?x ? X : x(a, r) ? x(r?, r??))
where x(a, r) stands for the similarity between
a and r according to the metric x. Thus,
QUEEN allows us to combine different similar-
ity metrics into a single measure, without hav-
ing to adjust their relative importance. Besides,
QUEEN offers two other important advantages
which make it really suitable for metric com-
bination: (i) it is robust against metric redun-
dancy, i.e., metrics covering similar aspects of
quality, and (ii) it is not affected by the scale
properties of metrics. The main drawback of
the QUEEN measure is that it requires at least
three human references, when in most cases
only a single reference translation is available.
Meta-evaluation Criterion Metric quality is eval-
uated using the KING measure of human like-
ness. All human references are assumed to be
equally optimal and, while they are likely to
be different, the best similarity metric is the
one that identifies and uses the features that
are common to all human references, group-
ing them and separating them from automatic
translations. Based on QUEEN, KING repre-
sents the probability that a human reference
321
does not receive a lower score than the score at-
tained by any automatic translation. Formally:
KINGA,R(X) = Prob(?a ? A : QUEENX,R?{r}(r) ?
QUEENX,R?{r}(a))
KING operates, therefore, on the basis of dis-
criminative power. The closest measure to
KING is ORANGE (Lin and Och, 2004b), which
is, however, not intended for the purpose of
metric combination.
Apart from being non-parametric, QARLA ex-
hibits another important feature which differentiates
it form other approaches; besides considering the
similarity between automatic translations and hu-
man references, QARLA also takes into account the
distribution of similarities among human references.
However, QARLA is not well suited to port from
human likeness to human acceptability. The reason
is that QUEEN is, by definition, a very restrictive
measure ?a ?good? translation must be similar to
all human references according to all metrics. Thus,
as the number of metrics increases, it becomes eas-
ier to find a metric which does not satisfy the QUEEN
assumption. This causes QUEEN values to get close
to zero, which turns correlation with human assess-
ments into an impractical meta-evaluation measure.
We have simulated a non-parametric scheme
based on human acceptability by working on uni-
formly averaged linear combinations (ULC) of met-
rics. Our approach is similar to that of Liu and
Gildea (2007) except that in our case all the metrics
in the combination are equally important3. In other
words, ULC is indeed a particular case of a paramet-
ric scheme, in which the contribution of each metric
is not adjusted. Formally:
ULCX(a,R) =
1
|X|
?
x?X
x(a,R)
where X is the metric set, and x(a,R) is the simi-
larity between the automatic translation a and the set
of references R, for the given test case, according to
the metric x. Since correlation with human assess-
ments at the system level is vaguely informative (it
is often estimated on very few system samples), we
3That would be assuming that all metrics operate in the same
range of values, which is not always the case.
AE04 CE04 AE05 CE05
#human references 5 5 5 4
#system outputs 5 10 7 10
#outputsassessed 5 10 6 5
#sentences 1,353 1,788 1,056 1,082
#sentencesassessed 347 447 266 272
Table 1: Description of the test beds
evaluate metric quality in terms of correlation with
human assessments at the sentence level (Rsnt). We
use the sum of adequacy and fluency to simulate a
global assessment of quality.
4 Experimental Work
In this section, we study the behavior of the two
combination schemes presented in Section 3 in the
context of four different evaluation scenarios.
4.1 Experimental Settings
We use the test beds from the 2004 and 2005
NIST MT Evaluation Campaigns (Le and Przy-
bocki, 2005)4. Both campaigns include two differ-
ent translations exercises: Arabic-to-English (?AE?)
and Chinese-to-English (?CE?). Human assessments
of adequacy and fluency are available for a subset
of sentences, each evaluated by two different human
judges. See, in Table 1, a brief numerical descrip-
tion including the number of human references and
system outputs available, as well as the number of
sentences per output, and the number of system out-
puts and sentences per system assessed.
For metric computation, we have used the IQMT
v2.1, which includes metrics at different linguistic
levels (lexical, shallow-syntactic, syntactic, shallow-
semantic, and semantic). A detailed description may
be found in (Gime?nez, 2007)5.
4.2 Evaluating Individual Metrics
Prior to studying the effects of metric combination,
we study the isolated behaviour of individual met-
rics. We have selected a set of metric representa-
tives from each linguistic level. Table 2 shows meta-
evaluation results for the test beds described in Sec-
tion 4.1, according both to human likeness (KING)
4http://www.nist.gov/speech/tests/
summaries/2005/mt05.htm
5The IQMT Framework may be freely downloaded from
http://www.lsi.upc.edu/?nlp/IQMT.
322
KING Rsnt
Level Metric AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05
1-WER 0.70 0.51 0.48 0.61 0.53 0.47 0.38 0.47
1-PER 0.64 0.43 0.45 0.58 0.50 0.51 0.29 0.40
1-TER 0.73 0.54 0.53 0.66 0.54 0.50 0.38 0.49
BLEU 0.70 0.49 0.52 0.59 0.50 0.46 0.36 0.39
NIST 0.74 0.53 0.55 0.68 0.53 0.55 0.37 0.46
Lexical GTM.e1 0.67 0.49 0.48 0.61 0.41 0.50 0.26 0.29
GTM.e2 0.69 0.52 0.51 0.64 0.49 0.54 0.43 0.48
ROUGEL 0.73 0.59 0.49 0.65 0.58 0.60 0.41 0.52
ROUGEW 0.75 0.62 0.54 0.68 0.59 0.57 0.48 0.54
METEORwnsyn 0.75 0.56 0.57 0.69 0.56 0.56 0.35 0.41
SP-Op-* 0.66 0.48 0.49 0.59 0.51 0.57 0.38 0.41
SP-Oc-* 0.65 0.44 0.46 0.59 0.55 0.58 0.42 0.41
Shallow SP-NISTl 0.73 0.51 0.55 0.66 0.53 0.54 0.38 0.44
Syntactic SP-NISTp 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39
SP-NISTiob 0.69 0.48 0.49 0.59 0.32 0.36 0.27 0.26
SP-NISTc 0.60 0.42 0.39 0.52 0.26 0.27 0.16 0.16
DP-HWCw 0.58 0.40 0.42 0.53 0.41 0.08 0.35 0.40
DP-HWCc 0.50 0.32 0.33 0.41 0.41 0.17 0.38 0.32
DP-HWCr 0.56 0.40 0.37 0.46 0.42 0.16 0.39 0.43
DP-Ol-* 0.58 0.48 0.41 0.52 0.52 0.48 0.36 0.37
Syntactic DP-Oc-* 0.65 0.45 0.44 0.55 0.49 0.51 0.43 0.41
DP-Or-* 0.71 0.57 0.54 0.64 0.55 0.55 0.50 0.50
CP-Op-* 0.67 0.47 0.47 0.60 0.53 0.57 0.38 0.46
CP-Oc-* 0.66 0.51 0.49 0.62 0.57 0.59 0.45 0.50
CP-STM 0.64 0.42 0.43 0.58 0.39 0.13 0.34 0.30
NE-Oe-** 0.65 0.45 0.46 0.57 0.47 0.56 0.32 0.39
Shallow SR-Or-* 0.48 0.22 0.34 0.41 0.28 0.10 0.32 0.21
Semantic SR-Orv 0.36 0.13 0.24 0.27 0.27 0.12 0.25 0.24
DR-Or-* 0.62 0.47 0.50 0.55 0.47 0.46 0.43 0.37
Semantic DR-Orp-* 0.58 0.42 0.43 0.50 0.37 0.35 0.36 0.26
Optimal Combination 0.79 0.64 0.61 0.70 0.64 0.63 0.54 0.61
Table 2: Metric Meta-evaluation
and human acceptability (Rsnt), computed over the
subsets of sentences for which human assessments
are available.
The first observation is that the two meta-
evaluation criteria provide very similar metric qual-
ity rankings for a same test bed. This seems to in-
dicate that there is a relationship between the two
meta-evaluation criteria employed. We have con-
firmed this intuition by computing the Pearson cor-
relation coefficient between values in columns 1 to
4 and their counterparts in columns 5 to 8. There
exists a high correlation (R = 0.79).
A second observation is that metric quality varies
significantly from task to task. This is due to the sig-
nificant differences among the test beds employed.
These are related to three main aspects: language
pair, translation domain, and system typology. For
instance, notice that most metrics exhibit a lower
quality in the case of the ?AE05? test bed. The reason
is that, while in the rest of test beds all systems are
statistical, the ?AE05? test bed presents the particu-
larity of providing automatic translations produced
by heterogeneous MT systems (i.e., systems belong-
ing to different paradigms)6. The fact that most sys-
tems are statistical also explains why, in general,
lexical metrics exhibit a higher quality. However,
highest levels of quality are not in all cases attained
by metrics at the lexical level (see highlighted val-
ues). In fact, there is only one metric, ?ROUGEW ?
(based on lexical matching), which is consistently
among the top-scoring in all test beds according to
both meta-evaluation criteria. The underlying cause
is simple: current metrics do not provide a global
measure of quality, but account only for partial as-
pects of it. Apart from evincing the importance of
the meta-evaluation process, these results strongly
suggest the need for conducting heterogeneous MT
evaluations.
6Specifically, all systems are statistical except one which is
human-aided.
323
Opt.K(AE.04) = {SP-NISTp}
Opt.K(CE.04) = {ROUGEW , SP-NISTp, ROUGEL}
Opt.K(AE.05) = {METEORwnsyn, SP-NISTp, DP-Or-*}
Opt.K(CE.05) = {SP-NISTp}
Opt.R(AE.04) = {ROUGEW ,ROUGEL,CP-Oc-*,METEORwnsyn,DP-Or-*,DP-Ol-*,GTM.e2,DR-Or-*,CP-STM}
Opt.R(CE.04) = {ROUGEL,CP-Oc-*,ROUGEW , SP-Op-*,METEORwnsyn,DP-Or-*,GTM.e2, 1-WER,DR-Or-*}
Opt.R(AE.05) = {DP-Or-*,ROUGEW }
Opt.R(CE.05) = {ROUGEW ,ROUGEL,DP-Or-*,CP-Oc-*, 1-TER,GTM.e2,DP-HWCr,CP-STM}
Table 3: Optimal metric sets
4.3 Finding Optimal Metric Combinations
In that respect, we study the applicability of the two
combination strategies presented. Optimal metric
sets are determined by maximizing over the corre-
sponding meta-evaluation measure (KING or Rsnt).
However, because exploring all possible combina-
tions was not viable, we have used a simple algo-
rithm which performs an approximate search. First,
individual metrics are ranked according to their
quality. Then, following that order, metrics are
added to the optimal set only if in doing so the global
quality increases. Since no training is required it has
not been necessary to keep a held-out portion of the
data for test (see Section 4.4 for further discussion).
Optimal metric sets are displayed in Table 3. In-
side each set, metrics are sorted in decreasing quality
order. The ?Optimal Combination? line in Table 2
shows the quality attained by these sets, combined
under QUEEN in the case of KING optimization, and
under ULC in the case of optimizing over Rsnt. In
most cases optimal sets consist of metrics operat-
ing at different linguistic levels, mostly at the lexical
and syntactic levels. This is coherent with the find-
ings in Section 4.2. Metrics at the semantic level
are selected only in two cases, corresponding to the
Rsnt optimization in ?AE04? and ?CE04? test beds.
Also in two cases, corresponding to the KING opti-
mization in ?AE04? and ?CE05? test beds, it has not
been possible to find any metric combination which
outperforms the best individual metric. This is not
a discouraging result. After all, in these cases, the
best metric alone achieves already a very high qual-
ity (0.79 and 0.70, respectively). The fact that a sin-
gle feature suffices to discern between manual and
automatic translations indicates that MT systems are
easily distinguishable, possibly because of their low
quality and/or because they are all based on the same
translation paradigm.
4.4 Portability
It can be argued that metric set optimization is itself
a training process; each metric would have an asso-
ciated binary parameter controlling whether it is se-
lected or not. For that reason, in Table 4, we have
analyzed the portability of optimal metric sets (i)
across test beds and (ii) across combination strate-
gies. As to portability across test beds (i.e., across
language pairs and years), the reader must focus
on the cells for which the meta-evaluation criterion
guiding the metric set optimization matches the cri-
terion used in the evaluation, i.e., the top-left and
bottom-right 16-cell quadrangles. The fact that the
4 values in each subcolumn are in a very similar
range confirms that optimal metric sets port well
across test beds. We have also studied the portabil-
ity of optimal metric sets across combination strate-
gies. In other words, although QUEEN and ULC
are thought to operate on metric combinations re-
spectively optimized on the basis of human likeness
and human acceptability, we have studied the effects
of applying either measure over metric combina-
tions optimized on the basis of the alternative meta-
evaluation criterion. In this case, the reader must
compare top-left vs. bottom-left (KING) and top-
right vs. bottom-right (Rsnt) 16-cell quadrangles. It
can be clearly seen that optimal metric sets, in gen-
eral, do not port well across meta-evaluation criteria,
particularly from human likeness to human accept-
ability. However, interestingly, in the case of ?AE05?
(i.e., heterogeneous systems), the optimal metric set
ports well from human acceptability to human like-
ness. We speculate that system heterogeneity has
contributed positively for the sake of robustness.
5 Conclusions
As an alternative to current parametric combination
techniques, we have presented two different meth-
324
Metric KING Rsnt
Set AE04 CE04 AE05 CE05 AE04 CE04 AE05 CE05
Opt.K(AE.04) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39
Opt.K(CE.04) 0.78 0.64 0.57 0.67 0.49 0.51 0.39 0.43
Opt.K(AE.05) 0.74 0.63 0.61 0.66 0.48 0.51 0.39 0.42
Opt.K(CE.05) 0.79 0.60 0.56 0.70 0.46 0.49 0.37 0.39
Opt.R(AE.04) 0.62 0.56 0.52 0.49 0.64 0.61 0.53 0.58
Opt.R(CE.04) 0.68 0.59 0.55 0.56 0.63 0.63 0.51 0.57
Opt.R(AE.05) 0.75 0.64 0.59 0.69 0.62 0.60 0.54 0.57
Opt.R(CE.05) 0.64 0.56 0.51 0.52 0.63 0.57 0.53 0.61
Table 4: Portability of combination strategies
ods: a genuine non-parametric method based on hu-
man likeness, and a parametric method based human
acceptability in which the parameter weights are set
equiprobable. We have shown that both strategies
may yield a significantly improved quality by com-
bining metrics at different linguistic levels. Besides,
we have shown that these methods generalize well
across test beds. Thus, a valid path towards hetero-
geneous automatic MT evaluation has been traced.
We strongly believe that future MT evaluation cam-
paigns should benefit from these results specially for
the purpose of comparing systems based on different
paradigms. These techniques could also be used to
build better MT systems by allowing system devel-
opers to perform more accurate error analyses and
less biased adjustments of system parameters.
As an additional result, we have found that there
is a tight relationship between human acceptability
and human likeness. This result, coherent with the
findings by Amigo? et al (2006), suggests that the
two criteria are interchangeable. This would be a
point in favour of combination schemes based on hu-
man likeness, since human assessments ?which are
expensive to acquire, subjective and not reusable?
are not required. We also interpret this result as an
indication that human assessors probably behave in
many cases in a discriminative manner. For each test
case, assessors would inspect the source sentence
and the set of human references trying to identify
the features which ?good? translations should com-
ply with, for instance regarding adequacy and flu-
ency. Then, they would evaluate automatic transla-
tions roughly according to the number and relevance
of the features they share and the ones they do not.
For future work, we plan to study the inte-
gration of finer features as well as to conduct a
rigorous comparison between parametric and non-
parametric combination schemes. This may involve
reproducing the works by Kulesza and Shieber
(2004) and Albrecht and Hwa (2007a). This would
also allow us to evaluate their approaches in terms of
both human likeness and human acceptability, and
not only on the latter criterion as they have been
evaluated so far.
Acknowledgements
This research has been funded by the Spanish Min-
istry of Education and Science, project OpenMT
(TIN2006-15307-C03-02). Our NLP group has
been recognized as a Quality Research Group (2005
SGR-00130) by DURSI, the Research Department
of the Catalan Government. We are thankful to En-
rique Amigo?, for his generous help and valuable
comments. We are also grateful to the NIST MT
Evaluation Campaign organizers, and participants
who agreed to share their system outputs and human
assessments for the purpose of this research.
References
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of
ACL, pages 880?887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of ACL, pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
325
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research. In Proceedings of EACL.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to
the Automatic Evaluation of Machine Translation. In
Proceedings of ACL, pages 140?147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd IHLT.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation.
Jesu?s Gime?nez. 2007. IQMT v 2.1. Technical
Manual. Technical report, TALP Research Center.
LSI Department. http://www.lsi.upc.edu/?nlp/IQMT/-
IQMT.v2.1.pdf.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of NLH-
NAACL.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102?121.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. Technical
report, NIST, August.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of COLING.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization.
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American chapter of the As-
sociation for Computational Linguistics (NAACL-07).
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. Evaluation Tool for Machine Trans-
lation: Fast Evaluation for MT Research. In Proceed-
ings of the 2nd LREC.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation. In
Proceedings of the 7th Conference of the Associa-
tion for Machine Translation in the Americas (AMTA),
pages 148?155.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalu-
ation of machine translation, RC22176, IBM. Techni-
cal report, IBM T.J. Watson Research Center.
Chris Quirk. 2004. Training a Sentence-Level Ma-
chine Translation Confidence Metric. In Proceedings
of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, , and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of AMTA, pages 223?231.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with Para-
phrase Support. In Proceedings of EMNLP.
326
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
MT Evaluation: Human-like vs. Human Acceptable
Enrique Amigo?
 
, Jesu?s Gime?nez  , Julio Gonzalo   , and Llu??s Ma`rquez 
 
Departamento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
Juan del Rosal, 16, E-28040, Madrid

enrique,julio  @lsi.uned.es
 TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado, 1?3, E-08034, Barcelona
 jgimenez,lluism  @lsi.upc.edu
Abstract
We present a comparative study on Ma-
chine Translation Evaluation according to
two different criteria: Human Likeness
and Human Acceptability. We provide
empirical evidence that there is a relation-
ship between these two kinds of evalu-
ation: Human Likeness implies Human
Acceptability but the reverse is not true.
From the point of view of automatic eval-
uation this implies that metrics based on
Human Likeness are more reliable for sys-
tem tuning.
Our results also show that current evalua-
tion metrics are not always able to distin-
guish between automatic and human trans-
lations. In order to improve the descrip-
tive power of current metrics we propose
the use of additional syntax-based met-
rics, and metric combinations inside the
QARLA Framework.
1 Introduction
Current approaches to Automatic Machine Trans-
lation (MT) Evaluation are mostly based on met-
rics which determine the quality of a given transla-
tion according to its similarity to a given set of ref-
erence translations. The commonly accepted crite-
rion that defines the quality of an evaluation metric
is its level of correlation with human evaluators.
High levels of correlation (Pearson over 0.9) have
been attained at the system level (Eck and Hori,
2005). But this is an average effect: the degree of
correlation achieved at the sentence level, crucial
for an accurate error analysis, is much lower.
We argue that there is two main reasons that ex-
plain this fact:
Firstly, current MT evaluation metrics are based
on shallow features. Most metrics work only at the
lexical level. However, natural languages are rich
and ambiguous, allowing for many possible differ-
ent ways of expressing the same idea. In order to
capture this flexibility, these metrics would require
a combinatorial number of reference translations,
when indeed in most cases only a single reference
is available. Therefore, metrics with higher de-
scriptive power are required.
Secondly, there exists, indeed, two different
evaluation criteria: (i) Human Acceptability, i.e.,
to what extent an automatic translation could be
considered acceptable by humans; and (ii) Human
Likeness, i.e., to what extent an automatic transla-
tion could have been generated by a human trans-
lator. Most approaches to automatic MT evalu-
ation implicitly assume that both criteria should
lead to the same results; but this assumption has
not been proved empirically or even discussed.
In this work, we analyze this issue through em-
pirical evidence. First, in Section 2, we inves-
tigate to what extent current evaluation metrics
are able to distinguish between human and auto-
matic translations (Human Likeness). As individ-
ual metrics do not capture such distinction well, in
Section 3 we study how to improve the descrip-
tive power of current metrics by means of met-
ric combinations inside the QARLA Framework
(Amigo? et al, 2005), including a new family of
metrics based on syntactic criteria. Second, we
claim that the two evaluation criteria (Human Ac-
ceptability and Human Likeness) are indeed of a
different nature, and may lead to different results
(Section 4). However, translations exhibiting a
high level of Human Likeness obtain good results
in human judges. Therefore, automatic evaluation
metrics based on similarity to references should be
17
optimized over their capacity to represent Human
Likeness. See conclusions in Section 5.
2 Descriptive Power of Standard Metrics
In this section we perform a simple experiment in
order to measure the descriptive power of current
state-of-the-art metrics, i.e., their ability to capture
the features which characterize human translations
with respect to automatic ones.
2.1 Experimental Setting
We use the data from the Openlab 2006 Initiative1
promoted by the TC-STAR Consortium2. This
test suite is entirely based on European Parlia-
ment Proceedings3, covering April 1996 to May
2005. We focus on the Spanish-to-English transla-
tion task. For the purpose of evaluation we use the
development set which consists of 1008 sentences.
However, due to lack of available MT outputs for
the whole set we used only a subset of 504 sen-
tences corresponding to the first half of the devel-
opment set. Three human references per sentence
are available.
We employ ten system outputs; nine are based
on Statistical Machine Translation (SMT) sys-
tems (Gime?nez and Ma`rquez, 2005; Crego et al,
2005), and one is obtained from the free Sys-
tran4 on-line rule-based MT engine. Evalua-
tion results have been computed by means of the
IQMT5 Framework for Automatic MT Evaluation
(Gime?nez and Amigo?, 2006).
We have selected a representative set of 22 met-
ric variants corresponding to six different fami-
lies: BLEU (Papineni et al, 2001), NIST (Dodding-
ton, 2002), GTM (Melamed et al, 2003), mPER
(Leusch et al, 2003), mWER (Nie?en et al, 2000)
and ROUGE (Lin and Och, 2004a).
2.2 Measuring Descriptive Power of
Evaluation Metrics
Our main assumption is that if an evaluation met-
ric is able to characterize human translations, then,
human references should be closer to each other
than automatic translations to other human refer-
ences. Based on this assumption we introduce two
measures (ORANGE and KING) which analyze
1http://tc-star.itc.it/openlab2006/
2http://www.tc-star.org/
3http://www.europarl.eu.int/
4http://www.systransoft.com.
5The IQMT Framework may be freely downloaded at
http://www.lsi.upc.edu/?nlp/IQMT.
the descriptive power of evaluation metrics from
diferent points of view.
ORANGE Measure
ORANGE compares automatic and manual
translations one-on-one. Let  and  be the sets
of automatic and reference translations, respec-
tively, and 	
 an evaluation metric which out-
puts the quality of an automatic translation 

by comparison to  . ORANGE measures the de-
scriptive power as the probability that a human ref-
erence  is more similar than an automatic transla-
tion 
 to the rest of human references:

Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 287?294,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Low-cost Enrichment of Spanish WordNet with Automatically Translated
Glosses: Combining General and Specialized Models
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
This paper studies the enrichment of Span-
ish WordNet with synset glosses automat-
ically obtained from the English Word-
Net glosses using a phrase-based Statisti-
cal Machine Translation system. We con-
struct the English-Spanish translation sys-
tem from a parallel corpus of proceed-
ings of the European Parliament, and study
how to adapt statistical models to the do-
main of dictionary definitions. We build
specialized language and translation mod-
els from a small set of parallel definitions
and experiment with robust manners to
combine them. A statistically significant
increase in performance is obtained. The
best system is finally used to generate a
definition for all Spanish synsets, which
are currently ready for a manual revision.
As a complementary issue, we analyze the
impact of the amount of in-domain data
needed to improve a system trained en-
tirely on out-of-domain data.
1 Introduction
Statistical Machine Translation (SMT) is today a
very promising approach. It allows to build very
quickly and fully automatically Machine Trans-
lation (MT) systems, exhibiting very competitive
results, only from a parallel corpus aligning sen-
tences from the two languages involved.
In this work we approach the task of enriching
Spanish WordNet with automatically translated
glosses1. The source glosses for these translations
are taken from the English WordNet (Fellbaum,
1Glosses are short dictionary definitions that accompany
WordNet synsets. See examples in Tables 5 and 6.
1998), which is linked, at the synset level, to Span-
ish WordNet. This resource is available, among
other sources, through the Multilingual Central
Repository (MCR) developed by the MEANING
project (Atserias et al, 2004).
We start by empirically testing the performance
of a previously developed English?Spanish SMT
system, built from the large Europarl corpus2
(Koehn, 2003). The first observation is that this
system completely fails to translate the specific
WordNet glosses, due to the large language varia-
tions in both domains (vocabulary, style, grammar,
etc.). Actually, this is confirming one of the main
criticisms against SMT, which is its strong domain
dependence. Since parameters are estimated from
a corpus in a concrete domain, the performance
of the system on a different domain is often much
worse. This flaw of statistical and machine learn-
ing approaches is well known and has been largely
described in the NLP literature, for a variety of
tasks (e.g., parsing, word sense disambiguation,
and semantic role labeling).
Fortunately, we count on a small set of Spanish
hand-developed glosses in MCR3. Thus, we move
to a working scenario in which we introduce a
small corpus of aligned translations from the con-
crete domain of WordNet glosses. This in-domain
corpus could be itself used as a source for con-
structing a specialized SMT system. Again, ex-
periments show that this small corpus alone does
not suffice, since it does not allow to estimate
good translation parameters. However, it is well
suited for combination with the Europarl corpus,
to generate combined Language and Translation
2The Europarl Corpus is available at: http://-
people.csail.mit.edu/people/koehn/publications/europarl
3About 10% of the 68,000 Spanish synsets contain a defi-
nition, generated without considering its English counterpart.
287
Models. A substantial increase in performance is
achieved, according to several standard MT eval-
uation metrics. Although moderate, this boost
in performance is statistically significant accord-
ing to the bootstrap resampling test described by
Koehn (2004b) and applied to the BLEU metric.
The main reason behind this improvement is
that the large out-of-domain corpus contributes
mainly with coverage and recall and the in-domain
corpus provides more precise translations. We
present a qualitative error analysis to support these
claims. Finally, we also address the important
question of how much in-domain data is needed
to be able to improve the baseline results.
Apart from the experimental findings, our study
has generated a very valuable resource. Currently,
we have the complete Spanish WordNet enriched
with one gloss per synset, which, far from being
perfect, constitutes an axcellent starting point for
a posterior manual revision.
Finally, we note that the construction of a
SMT system when few domain-specific data are
available has been also investigated by other au-
thors. For instance, Vogel and Tribble (2002) stud-
ied whether an SMT system for speech-to-speech
translation built on top of a small parallel corpus
can be improved by adding knowledge sources
which are not domain specific. In this work, we
look at the same problem the other way around.
We study how to adapt an out-of-domain SMT
system using in-domain data.
The rest of the paper is organized as follows.
In Section 2 the fundamentals of SMT and the
components of our MT architecture are described.
The experimental setting is described in Section 3.
Evaluation is carried out in Section 4. Finally, Sec-
tion 5 contains error analysis and Section 6 con-
cludes and outlines future work.
2 Background
Current state-of-the-art SMT systems are based on
ideas borrowed from the Communication Theory
field. Brown et al (1988) suggested that MT can
be statistically approximated to the transmission
of information through a noisy channel. Given a
sentence f = f1..fn (distorted signal), it is possi-
ble to approximate the sentence e = e1..em (origi-
nal signal) which produced f . We need to estimate
P (e|f), the probability that a translator produces
f as a translation of e. By applying Bayes? rule it
is decomposed into: P (e|f) = P (f |e)?P (e)P (f) .
To obtain the string e which maximizes the
translation probability for f , a search in the prob-
ability space must be performed. Because the de-
nominator is independent of e, we can ignore it for
the purpose of the search: e = argmaxeP (f |e) ?
P (e). This last equation devises three compo-
nents in a SMT system. First, a language model
that estimates P (e). Second, a translation model
representing P (f |e). Last, a decoder responsi-
ble for performing the arg-max search. Language
models are typically estimated from large mono-
lingual corpora, translation models are built out
from parallel corpora, and decoders usually per-
form approximate search, e.g., by using dynamic
programming and beam search.
However, in word-based models the modeling
of the context in which the words occur is very
weak. This problem is significantly alleviated by
phrase-based models (Och, 2002), which repre-
sent nowadays the state-of-the-art in SMT.
2.1 System Construction
Fortunately, there is a number of freely available
tools to build a phrase-based SMT system. We
used only standard components and techniques for
our basic system, which are all described below.
The SRI Language Modeling Toolkit (SRILM)
(Stolcke, 2002) supports creation and evaluation
of a variety of language models. We build trigram
language models applying linear interpolation and
Kneser-Ney discounting for smoothing.
In order to build phrase-based translation mod-
els, a phrase extraction must be performed on
a word-aligned parallel corpus. We used the
GIZA++ SMT Toolkit4 (Och and Ney, 2003) to
generate word alignments We applied the phrase-
extract algorithm, as described by Och (2002), on
the Viterbi alignments output by GIZA++. We
work with the union of source-to-target and target-
to-source alignments, with no heuristic refine-
ment. Phrases up to length five are considered.
Also, phrase pairs appearing only once are dis-
carded, and phrase pairs in which the source/target
phrase was more than three times longer than the
target/source phrase are ignored. Finally, phrase
pairs are scored by relative frequency. Note that
no smoothing is performed.
Regarding the arg-max search, we used the
Pharaoh beam search decoder (Koehn, 2004a),
which naturally fits with the previous tools.
4http://www.fjoch.com/GIZA++.html
288
3 Data Sets and Evaluation Metrics
As a general source of English?Spanish parallel
text, we used a collection of 730,740 parallel sen-
tences extracted from the Europarl corpus. These
correspond exactly to the training data from the
Shared Task 2: Exploiting Parallel Texts for Sta-
tistical Machine Translation from the ACL-2005
Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond5.
To be used as specialized source, we extracted,
from the MCR , the set of 6,519 English?Spanish
parallel glosses corresponding to the already de-
fined synsets in Spanish WordNet. These defini-
tions corresponded to 5,698 nouns, 87 verbs, and
734 adjectives. Examples and parenthesized texts
were removed. Parallel glosses were tokenized
and case lowered. We discarded some of these
parallel glosses based on the difference in length
between the source and the target. The gloss av-
erage length for the resulting 5,843 glosses was
8.25 words for English and 8.13 for Spanish. Fi-
nally, gloss pairs were randomly split into training
(4,843), development (500) and test (500) sets.
Additionally, we counted on two large mono-
lingual Spanish electronic dictionaries, consisting
of 142,892 definitions (2,112,592 tokens) (?D1?)
(Mart??, 1996) and 168,779 definitons (1,553,674
tokens) (?D2?) (Vox, 1990), respectively.
Regarding evaluation, we used up to four dif-
ferent metrics with the aim of showing whether
the improvements attained are consistent or not.
We have computed the BLEU score (accumu-
lated up to 4-grams) (Papineni et al, 2001), the
NIST score (accumulated up to 5-grams) (Dod-
dington, 2002), the General Text Matching (GTM)
F-measure (e = 1, 2) (Melamed et al, 2003),
and the METEOR measure (Banerjee and Lavie,
2005). These metrics work at the lexical level by
rewarding n-gram matches between the candidate
translation and a set of human references. Addi-
tionally, METEOR considers stemming, and al-
lows for WordNet synonymy lookup.
The discussion of the significance of the results
will be based on the BLEU score, for which we
computed a bootstrap resampling test of signifi-
cance (Koehn, 2004b).
5http://www.statmt.org/wpt05/.
4 Experimental Evaluation
4.1 Baseline Systems
As explained in the introduction we built two indi-
vidual baseline systems. The first baseline (?EU?)
system is entirely based on the training data from
the Europarl corpus. The second baseline system
(?WNG?) is entirely based on the training set from
of the in-domain corpus of parallel glosses. In the
second case phrase pairs occurring only once in
the training corpus are not discarded due to the ex-
tremely small size of the corpus.
Table 1 shows results of the two baseline sys-
tems, both for the development and test sets. We
compare the performance of the ?EU? baseline on
these data sets with respect to the (in-domain) Eu-
roparl test set provided by the organizers of the
ACL-2005 MT workshop. As expected, there is
a very significant decrease in performance (e.g.,
from 0.24 to 0.08 according to BLEU) when the
?EU? baseline system is applied to the new do-
main. Some of this decrement is also due to a cer-
tain degree of free translation exhibited by the set
of available ?quasi-parallel? glosses. We further
discuss this issue in Section 5.
The results obtained by ?WNG? are also very
low, though slightly better than those of ?EU?. This
is a very interesting fact. Although the amount of
data utilized to construct the ?WNG? baseline is
150 times smaller than the amount utilized to con-
struct the ?EU? baseline, its performance is higher
consistently according to all metrics. We interpret
this result as an indicator that models estimated
from in-domain data provide higher precision.
We also compare the results to those of a com-
mercial system such as the on-line version 5.0 of
SYSTRAN6, a general-purpose MT system based
on manually-defined lexical and syntactic trans-
fer rules. The performance of the baseline sys-
tems is significantly worse than SYSTRAN?s on
both development and test sets. This means that
a rule-based system like SYSTRAN is more ro-
bust than the SMT-based systems. The difference
against the specialized ?WNG? also suggests that
the amount of data used to train the ?WNG? base-
line is clearly insufficient.
4.2 Combining Sources: Language Models
In order to improve results, in first place we turned
our eyes to language modeling. In addition to
6http://www.systransoft.com/.
289
system BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EU-baseline 0.0737 2.8832 0.3131 0.2216 0.2881
WNG-baseline 0.1149 3.3492 0.3604 0.2605 0.3288
SYSTRAN 0.1625 3.9467 0.4257 0.2971 0.4394
test
EU-baseline 0.0790 2.8896 0.3131 0.2262 0.2920
WNG-baseline 0.0951 3.1307 0.3471 0.2510 0.3219
SYSTRAN 0.1463 3.7873 0.4085 0.2921 0.4295
acl05-test
EU-baseline 0.2381 6.5848 0.5699 0.2429 0.5153
Table 1: MT Results on development and test sets, for the two baseline systems compared to SYSTRAN and to the ?EU?
baseline system on the ACL-2005 SMT workshop test set extracted from the Europarl Corpus. BLEU.n4 shows the accumulated
BLEU score for 4-grams. NIST.n5 shows the accumulated NIST score for 5-grams. GTM.e1 and GTM.e2 show the GTM F1-
measure for different values of the e parameter (e = 1, e = 2, respectively). METEOR reflects the METEOR score.
the language model built from the Europarl cor-
pus (?EU?) and the specialized language model
based on the small training set of parallel glosses
(?WNG?), two specialized language models, based
on the two large monolingual Spanish electronic
dictionaries (?D1? and ?D2?) were used. We tried
several configurations. In all cases, language mod-
els are combined with equal probability. See re-
sults, for the development set, in Table 2.
As expected, the closer the language model is
to the target domain, the better results. Observe
how results using language models ?D1? and ?D2?
outperform results using ?EU?. Note also that best
results are in all cases consistently attained by us-
ing the ?WNG? language model. This means that
language models estimated from small sets of in-
domain data are helpful. A second conclusion is
that a significant gain is obtained by incrementally
adding (in-domain) specialized language models
to the baselines, according to all metrics but BLEU
for which no combination seems to significantly
outperform the ?WNG? baseline alone. Observe
that best results are obtained, except in the case
of BLEU, by the system using ?EU? as translation
model and ?WNG? as language model. We inter-
pret this result as an indicator that translation mod-
els estimated from out-of-domain data are help-
ful because they provide recall. A third interest-
ing point is that adding an out-of-domain language
model (?EU?) does not seem to help, at least com-
bined with equal probability than in-domain mod-
els. Same conclusions hold for the test set, too.
4.3 Tuning the System
Adjusting the Pharaoh parameters that control
the importance of the different probabilities that
govern the search may yield significant improve-
ments. In our case, it is specially important to
properly adjust the contribution of the language
models. We adjusted parameters by means of a
software based on the Downhill Simplex Method
in Multidimensions (William H. Press and Flan-
nery, 2002). The tuning was based on the improve-
ment attained in BLEU score over the develop-
ment set. We tuned 6 parameters: 4 language mod-
els (?lmEU , ?lmD1, ?lmD2, ?lmWNG), the transla-
tion model (??), and the word penalty (?w)7.
Results improve substantially. See Table 3. Best
results are still attained using the ?EU? translation
model. Interestingly, as suggested by Table 2, the
weight of language models is concentrated on the
?WNG? language model (?lmWNG = 0.95).
4.4 Combining Sources: Translation Models
In this section we study the possibility of combin-
ing out-of-domain and in-domain translation mod-
els aiming at achieving a good balance between
precision and recall that yields better MT results.
Two different strategies have been tried. In
a first stragegy we simply concatenate the out-
of-domain corpus (?EU?) and the in-domain cor-
pus (?WNG?). Then, we construct the translatation
model (?EUWNG?) as detailed in Section 2.1. A
second manner to proceed is to linearly combine
the two different translation models into a single
translation model (?EU+WNG?). In this case, we
can assign different weights (?) to the contribution
of the different models to the search. We can also
determine a certain threshold ? which allows us
7Final values when using the ?EU? translation model are
?lmEU = 0.22, ?lmD1 = 0, ?lmD2 = 0.01, ?lmWNG =
0.95, ?? = 1, and ?w = ?2.97, while when using the
?WNG? translation model final values are ?lmEU = 0.17,
?lmD1 = 0.07, ?lmD2 = 0.13, ?lmWNG = 1, ?? = 0.95,
and ?w = ?2.64.
290
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
EU EU 0.0737 2.8832 0.3131 0.2216 0.2881
EU WNG 0.1062 3.4831 0.3714 0.2631 0.3377
EU D1 0.0959 3.2570 0.3461 0.2503 0.3158
EU D2 0.0896 3.2518 0.3497 0.2482 0.3163
EU D1 + D2 0.0993 3.3773 0.3585 0.2579 0.3244
EU EU + D1 + D2 0.0960 3.2851 0.3472 0.2499 0.3160
EU D1 + D2 + WNG 0.1094 3.4954 0.3690 0.2662 0.3372
EU EU + D1 + D2 + WNG 0.1080 3.4248 0.3638 0.2614 0.3321
WNG EU 0.0743 2.8864 0.3128 0.2202 0.2689
WNG WNG 0.1149 3.3492 0.3604 0.2605 0.3288
WNG D1 0.0926 3.1544 0.3404 0.2418 0.3050
WNG D2 0.0845 3.0295 0.3256 0.2326 0.2883
WNG D1 + D2 0.0917 3.1185 0.3331 0.2394 0.2995
WNG EU + D1 + D2 0.0856 3.0361 0.3221 0.2312 0.2847
WNG D1 + D2 + WNG 0.0980 3.2238 0.3462 0.2479 0.3117
WNG EU + D1 + D2 + WNG 0.0890 3.0974 0.3309 0.2373 0.2941
Table 2: MT Results on development set, for several translation/language model configurations. ?EU? and ?WNG? refer to
the models estimated from the Europarl corpus and the training set of parallel WordNet glosses, respectively. ?D1?, and ?D2?
denote the specialized language models estimated from the two dictionaries.
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EU EU + D1 + D2 + WNG 0.1272 3.6094 0.3856 0.2727 0.3695
WNG EU + D1 + D2 + WNG 0.1269 3.3740 0.3688 0.2676 0.3452
test
EU EU + D1 + D2 + WNG 0.1133 3.4180 0.3720 0.2650 0.3644
WNG EU + D1 + D2 + WNG 0.1015 3.1084 0.3525 0.2552 0.3343
Table 3: MT Results on development and test sets after tuning for the ?EU + D1 + D2 + WNG? language model configuration
for the two translation models, ?EU? and ?WNG?.
to discard phrase pairs under a certain probability.
These weights and thresholds were adjusted8 as
detailed in Subsection 4.3. Interestingly, at combi-
nation time the importance of the ?WNG? transla-
tion model (?tmWNG = 0.9) is much higher than
that of the ?EU? translation model (?tmEU = 0.1).
Table 4 shows results for the two strategies.
As expected, the ?EU+WNG? strategy consistently
obtains the best results according to all metrics
both on the development and test sets, since it
allows to better adjust the relative importance of
each translation model. However, both techniques
achieve a very competitive performance. Results
improve, according to BLEU, from 0.13 to 0.16,
and from 0.11 to 0.14, for the development and
test sets, respectively.
We measured the statistical signficance of
the overall improvement in BLEU.n4 attained
with respect to the baseline results by ap-
plying the bootstrap resampling technique de-
scribed by Koehn (2004b). The 95% confi-
dence intervals extracted from the test set after
8We used values ?tmEU = 0.1, ?tmWNG = 0.9,
?tmEU = 0.1, and ?tmWNG = 0.01
10,000 samples are the following: IEU?base =
[0.0642, 0.0939], IWNG?base = [0.0788, 0.1112],
IEU+WNG?best = [0.1221, 0.1572]. Since the in-
tervals are not ovelapping, we can conclude that
the performance of the best combined method is
statistically higher than the ones of the two base-
line systems.
4.5 How much in-domain data is needed?
In principle, the more in-domain data we have the
better, but these may be difficult or expensive to
collect. Thus, a very interesting issue in the con-
text of our work is how much in-domain data is
needed in order to improve results attained using
out-of-domain data alone. To answer this question
we focus on the ?EU+WNG? strategy and analyze
the impact on performance (BLEU.n4) of special-
ized models extracted from an incrementally big-
ger number of example glosses. The results are
presented in the plot of Figure 1. We compute
three variants separately, by considering the use of
the in-domain data: only for the translation model
(TM), only for the language model (LM), and si-
multaneously in both models (TM+LM). In order
291
Translation Model Language Model BLEU.n4 NIST.n5 GTM.e1 GTM.e2 METEOR
development
EUWNG WNG 0.1288 3.7677 0.3949 0.2832 0.3711
EUWNG EU + D1 + D2 + WNG 0.1182 3.6034 0.3835 0.2759 0.3552
EUWNG EU + D1 + D2 + WNG (TUNED) 0.1554 3.8925 0.4081 0.2944 0.3998
EU+WNG WNG 0.1384 3.9743 0.4096 0.2936 0.3804
EU+WNG EU + D1 + D2 + WNG 0.1235 3.7652 0.3911 0.2801 0.3606
EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1618 4.1415 0.4234 0.3029 0.4130
test
EUWNG WNG 0.1123 3.6777 0.3829 0.2771 0.3595
EUWNG EU + D1 + D2 + WNG 0.1183 3.5819 0.3737 0.2772 0.3518
EUWNG EU + D1 + D2 + WNG (TUNED) 0.1290 3.6478 0.3920 0.2810 0.3885
EU+WNG WNG 0.1227 3.8970 0.3997 0.2872 0.3723
EU+WNG EU + D1 + D2 + WNG 0.1199 3.7353 0.3846 0.2812 0.3583
EU+WNG EU + D1 + D2 + WNG (TUNED) 0.1400 3.8930 0.4084 0.2907 0.3963
Table 4: MT Results on development and test sets for the two strategies for combining translations models.
 0.06
 0.07
 0.08
 0.09
 0.1
 0.11
 0.12
 0.13
 0.14
 0  500  1000  1500  2000  2500  3000  3500  4000  4500
BL
EU
.n
4
# glosses
baseline
TM + LM impact
TM impact
LM impact
Figure 1: Impact of the size of in-domain data on
MT system performance for the test set.
to avoid the possible effect of over-fitting we focus
on the behavior on the test set. Note that the opti-
mization of parameters is performed at each point
in the x-axis using only the development set.
A significant initial gain of around 0.3 BLEU
points is observed when adding as few as 100
glosses. In all cases, it is not until around 1,000
glosses are added that the ?EU+WNG? system sta-
bilizes. After that, results continue improving as
more in-domain data are added. We observe a
very significant increase by just adding around
3,000 glosses. Another interesting observation is
the boosting effect of the combination of TM and
LM specialized models. While individual curves
for TM and LM tend to be more stable with more
than 4,000 added examples, the TM+LM curve
still shows a steep increase in this last part.
5 Error Analysis
We inspected results at the sentence level based on
the GTM F-measure (e = 1) for the best config-
uration of the ?EU+WNG? system. 196 sentences
out from the 500 obtain an F-measure equal to or
higher than 0.5 on the development set (181 sen-
tences in the case of test set), whereas only 54
sentences obtain a score lower than 0.1. These
numbers give a first idea of the relative useful-
ness of our system. Table 5 shows some trans-
lation cases selected for discussion. For instance,
Case 1 is a clear example of unfair low score. The
problem is that source and reference are not par-
allel but ?quasi-parallel?. Both glosses define the
same concept but in a different way. Thus, metrics
based on rewarding lexical similarities are not well
suited for these cases. Cases 2, 3, 4 are examples
of proper cooperation between ?EU? and ?WNG?
models. ?EU? models provides recall, for instance
by suggesting translation candidates for ?bombs?
or ?price below?. ?WNG? models provide preci-
sion, for instance by choosing the right translation
for ?an attack? or ?the act of?.
We also compared the ?EU+WNG? system to
SYSTRAN. In the case of SYSTRAN 167 sen-
tences obtain a score equal to or higher than 0.5
whereas 79 sentences obtain a score lower than
0.1. These numbers are slightly under the per-
formance of the ?EU+WNG? system. Table 6
shows some translation cases selected for discus-
sion. Case 1 is again an example of both sys-
tems obtaining very low scores because of ?quasi-
parallelism?. Cases 2 and 3 are examples of SYS-
TRAN outperforming our system. In case 2 SYS-
TRAN exhibits higher precision in the translation
of ?accompanying? and ?illustration?, whereas in
case 3 it shows higher recall by suggesting ap-
propriate translation candidates for ?fibers?, ?silk-
worm?, ?cocoon?, ?threads?, and ?knitting?. Cases
292
FE FW FEW Source OutE OutW OutEW Reference
0.0000 0.1333 0.1111 of the younger de acuerdo con de la younger de acuerdo con que tiene
of two boys el ma?s joven de dos boys el ma?s joven de menos edad
with the same de dos boys tiene el mismo dos muchachos
family name con la misma nombre familia tiene el mismo
familia fama nombre familia
0.2857 0.2500 0.5000 an attack atacar por ataque ataque ataque con
by dropping cayendo realizado por realizado por bombas
bombs bombas dropping bombs cayendo bombas
0.1250 0.7059 0.5882 the act of acto de la accio?n y efecto accio?n y efecto accio?n y efecto
informing by informacio?n de informing de informaba de informar
verbal report por verbales por verbal por verbales con una expli-
ponencia explicacio?n explicacio?n cacio?n verbal
0.5000 0.0000 0.5000 a price below un precio por una price un precio por precio que esta?
the standard debajo de la below nu?mbero debajo de la por debajo de
price norma precio esta?ndar price esta?ndar precio lo normal
Table 5: MT output analysis of the ?EU?, ?WNG? and ?EU+WNG? systems. FE , FW and FEW refer to the GTM (e = 1)
F-measure attained by the ?EU?, ?WNG? and ?EU+WNG? systems, respectively. ?Source?, OutE , OutW and OutEW refer to
the input and the output of the systems. ?Reference? corresponds to the expected output.
4 and 5 are examples where our system outper-
forms SYSTRAN. In case 4, our system provides
higher recall by suggesting an adequate transla-
tion for ?top of something?. In case 5, our system
shows higher precision by selecting a better trans-
lation for ?rate?. However, we observed that SYS-
TRAN tends in most cases to construct sentences
exhibiting a higher degree of grammaticality.
6 Conclusions
In this work, we have enriched every synset in
Spanish WordNet with a preliminary gloss, which
can be later updated in a lighter process of manual
revision. Though imperfect, this material consti-
tutes a very valuable resource. For instance, Word-
Net glosses have been used in the past to generate
sense tagged corpora (Mihalcea and Moldovan,
1999), or as external knowledge for Question An-
swering systems (Hovy et al, 2001).
We have also shown the importance of using a
small set of in-domain parallel sentences in or-
der to adapt a phrase-based general SMT sys-
tem to a new domain. In particular, we have
worked on specialized language and translation
models and on their combination with general
models in order to achieve a proper balance be-
tween precision (specialized in-domain models)
and recall (general out-of-domain models). A sub-
stantial increase is consistently obtained according
to standard MT evaluation metrics, which has been
shown to be statistically significant in the case
of BLEU. Broadly speaking, we have shown that
around 3,000 glosses (very short sentence frag-
ments) suffice in this domain to obtain a signifi-
cant improvement. Besides, all the methods used
are language independent, assumed the availabil-
ity of the required in-domain additional resources.
In the future we plan to work on domain inde-
pendent translation models built from WordNet it-
self. We may use the WordNet topology to pro-
vide translation candidates weighted according to
the given domain. Moreover, we are experiment-
ing the applicability of current Word Sense Dis-
ambiguation (WSD) technology to MT. We could
favor those translation candidates showing a closer
semantic relation to the source. We believe that
coarse-grained is sufficient for the purpose of MT.
Acknowledgements
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02) and the Spanish Ministry of
Education and Science (TRANGRAM, TIN2004-
07925-C03-02). Our research group, TALP Re-
search Center, is recognized as a Quality Research
Group (2001 SGR 00254) by DURSI, the Re-
search Department of the Catalan Government.
Authors are grateful to Patrik Lambert for pro-
viding us with the implementation of the Simplex
Method, and specially to German Rigau for moti-
vating in its origin all this work.
References
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
293
FEW FS Source OutEW OutS Reference
0.0000 0.0000 a newspaper that perio?dico que un perio?dico publicacio?n
is published se publica diario que se publica perio?dica
every day cada d??a monotema?tica
0.1818 0.8333 brief description breve descripcio?n breve descripcio?n pequen?a descripcio?n
accompanying an adjuntas un aclaracio?n que acompan?a que acompan?a
illustration una ilustracio?n una ilustracio?n
0.1905 0.7333 fibers from silkworm fibers desde silkworm las fibras de los fibras de los capullos
cocoons provide cocoons proporcionan capullos del gusano de gusano de seda
threads for knitting threads para knitting de seda proporcionan que proporcionan
los hilos de rosca hilos para tejer
para hacer punto
1.0000 0.0000 the top of something parte superior de la tapa algo parte superior de
una cosa una cosa
0.6667 0.3077 a rate at which un ritmo al que una tarifa en la ritmo al que
something happens sucede algo cual algo sucede sucede una cosa
Table 6: MT output analysis of the ?EU+WNG? and SYSTRAN systems. FEW and FS refer to the GTM (e = 1) F-measure
attained by the ?EU+WNG? and SYSTRAN systems, respectively. ?Source?, OutEW and OutS refer to the input and the output
of the systems. ?Reference? corresponds to the expected output.
Vossen. 2004. The MEANING Multilingual Cen-
tral Repository. In Proceedings of 2nd GWC.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for Machine Translation
and/or Summarization.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, Robert L.
Mercer, , and Paul S. Roossin. 1988. A statistical
approach to language translation. In Proceedings of
COLING?88.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternation Conference on Human Language Technol-
ogy, pages 138?145.
C. Fellbaum, editor. 1998. WordNet. An Electronic
Lexical Database. The MIT Press.
Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin.
2001. The Use of External Knowledge of Factoid
QA. In Proceedings of TREC.
Philipp Koehn. 2003. Europarl: A Multilin-
gual Corpus for Evaluation of Machine Transla-
tion. Technical report, http://people.csail.mit.edu/-
people/koehn/publications/europarl/.
Philipp Koehn. 2004a. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA?04.
Philipp Koehn. 2004b. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP?04.
Mar??a Antonia Mart??, editor. 1996. Gran dic-
cionario de la Lengua Espan?ola. Larousse Planeta,
Barcelona.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL?03.
Rada Mihalcea and Dan Moldovan. 1999. An Au-
tomatic Method for Generating Sense Tagged Cor-
pora. In Proceedings of AAAI.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, IBM Research Re-
port, RC22176. Technical report, IBM T.J. Watson
Research Center.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of IC-
SLP?02.
Stephan Vogel and Alicia Tribble. 2002. Improv-
ing Statistical Machine Translation for a Speech-to-
Speech Translation Task. In Proceedings of ICSLP-
2002 Workshop on Speech-to-Speech Translation.
Vox, editor. 1990. Diccionario Actual de la Lengua
Espan?ola. Bibliograf, Barcelona.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing. Cambridge
University Press.
294
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 306?314,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
The Contribution of Linguistic Features to Automatic Machine
Translation Evaluation
Enrique Amigo?1 Jesu?s Gime?nez2 Julio Gonzalo 1 Felisa Verdejo1
1UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
2UPC, Barcelona
jgimenez@lsi.upc.edu
Abstract
A number of approaches to Automatic
MT Evaluation based on deep linguistic
knowledge have been suggested. How-
ever, n-gram based metrics are still to-
day the dominant approach. The main
reason is that the advantages of employ-
ing deeper linguistic information have not
been clarified yet. In this work, we pro-
pose a novel approach for meta-evaluation
of MT evaluation metrics, since correla-
tion cofficient against human judges do
not reveal details about the advantages and
disadvantages of particular metrics. We
then use this approach to investigate the
benefits of introducing linguistic features
into evaluation metrics. Overall, our ex-
periments show that (i) both lexical and
linguistic metrics present complementary
advantages and (ii) combining both kinds
of metrics yields the most robust meta-
evaluation performance.
1 Introduction
Automatic evaluation methods based on similarity
to human references have substantially accelerated
the development cycle of many NLP tasks, such
as Machine Translation, Automatic Summariza-
tion, Sentence Compression and Language Gen-
eration. These automatic evaluation metrics allow
developers to optimize their systems without the
need for expensive human assessments for each
of their possible system configurations. However,
estimating the system output quality according to
its similarity to human references is not a trivial
task. The main problem is that many NLP tasks
are open/subjective; therefore, different humans
may generate different outputs, all of them equally
valid. Thus, language variability is an issue.
In order to tackle language variability in the
context of Machine Translation, a considerable ef-
fort has also been made to include deeper linguis-
tic information in automatic evaluation metrics,
both syntactic and semantic (see Section 2 for de-
tails). However, the most commonly used metrics
are still based on n-gram matching. The reason is
that the advantages of employing higher linguistic
processing levels have not been clarified yet.
The main goal of our work is to analyze to what
extent deep linguistic features can contribute to the
automatic evaluation of translation quality. For
that purpose, we compare ? using four different
test beds ? the performance of 16 n-gram based
metrics, 48 linguistic metrics and one combined
metric from the state of the art.
Analyzing the reliability of evaluation met-
rics requires meta-evaluation criteria. In this re-
spect, we identify important drawbacks of the
standard meta-evaluation methods based on cor-
relation with human judgements. In order to
overcome these drawbacks, we then introduce six
novel meta-evaluation criteria which represent dif-
ferent metric reliability dimensions. Our analysis
indicates that: (i) both lexical and linguistic met-
rics have complementary advantages and different
drawbacks; (ii) combining both kinds of metrics
is a more effective and robust evaluation method
across all meta-evaluation criteria.
In addition, we also perform a qualitative analy-
sis of one hundred sentences that were incorrectly
evaluated by state-of-the-art metrics. The analysis
confirms that deep linguistic techniques are neces-
sary to avoid the most common types of error.
Section 2 examines the state of the art Section 3
describes the test beds and metrics considered in
our experiments. In Section 4 the correlation be-
tween human assessors and metrics is computed,
with a discussion of its drawbacks. In Section 5
different quality aspects of metrics are analysed.
Conclusions are drawn in the last section.
306
2 Previous Work on Machine
Translation Meta-Evaluation
Insofar as automatic evaluation metrics for ma-
chine translation have been proposed, different
meta-evaluation frameworks have been gradually
introduced. For instance, Papineni et al (2001)
introduced the BLEU metric and evaluated its re-
liability in terms of Pearson correlation with hu-
man assessments for adequacy and fluency judge-
ments. With the aim of overcoming some of the
deficiencies of BLEU, Doddington (2002) intro-
duced the NIST metric. Metric reliability was
also estimated in terms of correlation with human
assessments, but over different document sources
and for a varying number of references and seg-
ment sizes. Melamed et al (2003) argued, at the
time of introducing the GTM metric, that Pearson
correlation coefficients can be affected by scale
properties, and suggested, in order to avoid this
effect, to use the non-parametric Spearman corre-
lation coefficients instead.
Lin and Och (2004) experimented, unlike pre-
vious works, with a wide set of metrics, including
NIST, WER (Nie?en et al, 2000), PER (Tillmann
et al, 1997), and variants of ROUGE, BLEU and
GTM. They computed both Pearson and Spearman
correlation, obtaining similar results in both cases.
In a different work, Banerjee and Lavie (2005) ar-
gued that the measured reliability of metrics can
be due to averaging effects but might not be robust
across translations. In order to address this issue,
they computed the translation-by-translation cor-
relation with human judgements (i.e., correlation
at the segment level).
All that metrics were based on n-gram over-
lap. But there is also extensive research fo-
cused on including linguistic knowledge in met-
rics (Owczarzak et al, 2006; Reeder et al, 2001;
Liu and Gildea, 2005; Amigo? et al, 2006; Mehay
and Brew, 2007; Gime?nez and Ma`rquez, 2007;
Owczarzak et al, 2007; Popovic and Ney, 2007;
Gime?nez and Ma`rquez, 2008b) among others. In
all these cases, metrics were also evaluated by
means of correlation with human judgements.
In a different research line, several authors
have suggested approaching automatic evalua-
tion through the combination of individual metric
scores. Among the most relevant let us cite re-
search by Kulesza and Shieber (2004), Albrecht
and Hwa (2007). But finding optimal metric
combinations requires a meta-evaluation criterion.
Most approaches again rely on correlation with
human judgements. However, some of them mea-
sured the reliability of metric combinations in
terms of their ability to discriminate between hu-
man translations and automatic ones (human like-
ness) (Amigo? et al, 2005). .
In this work, we present a novel approach to
meta-evaluation which is distinguished by the use
of additional easily interpretable meta-evaluation
criteria oriented to measure different aspects of
metric reliability. We then apply this approach to
find out about the advantages and challenges of in-
cluding linguistic features in meta-evaluation cri-
teria.
3 Metrics and Test Beds
3.1 Metric Set
For our study, we have compiled a rich set of met-
ric variants at three linguistic levels: lexical, syn-
tactic, and semantic. In all cases, translation qual-
ity is measured by comparing automatic transla-
tions against a set of human references.
At the lexical level, we have included several
standard metrics, based on different similarity as-
sumptions: edit distance (WER, PER and TER),
lexical precision (BLEU and NIST), lexical recall
(ROUGE), and F-measure (GTM and METEOR). At
the syntactic level, we have used several families
of metrics based on dependency parsing (DP) and
constituency trees (CP). At the semantic level, we
have included three different families which op-
erate using named entities (NE), semantic roles
(SR), and discourse representations (DR). A de-
tailed description of these metrics can be found in
(Gime?nez and Ma`rquez, 2007).
Finally, we have also considered ULC, which
is a very simple approach to metric combina-
tion based on the unnormalized arithmetic mean
of metric scores, as described by Gime?nez and
Ma`rquez (2008a). ULC considers a subset of met-
rics which operate at several linguistic levels. This
approach has proven very effective in recent eval-
uation campaigns. Metric computation has been
carried out using the IQMT Framework for Auto-
matic MT Evaluation (Gime?nez, 2007)1. The sim-
plicity of this approach (with no training of the
metric weighting scheme) ensures that the poten-
tial advantages detected in our experiments are not
due to overfitting effects.
1http://www.lsi.upc.edu/?nlp/IQMT
307
2004 2005
AE CE AE CE
#references 5 5 5 4
#systemsassessed 5 10 5+1 5
#casesassessed 347 447 266 272
Table 1: NIST 2004/2005 MT Evaluation Cam-
paigns. Test bed description
3.2 Test Beds
We use the test beds from the 2004 and 2005
NIST MT Evaluation Campaigns (Le and Przy-
bocki, 2005)2. Both campaigns include two dif-
ferent translations exercises: Arabic-to-English
(?AE?) and Chinese-to-English (?CE?). Human as-
sessments of adequacy and fluency, on a 1-5 scale,
are available for a subset of sentences, each eval-
uated by two different human judges. A brief nu-
merical description of these test beds is available
in Table 1. The corpus AE05 includes, apart from
five automatic systems, one human-aided system
that is only used in our last experiment.
4 Correlation with Human Judgements
4.1 Correlation at the Segment vs. System
Levels
Let us first analyze the correlation with human
judgements for linguistic vs. n-gram based met-
rics. Figure 1 shows the correlation obtained by
each automatic evaluation metric at system level
(horizontal axis) versus segment level (vertical
axis) in our test beds. Linguistic metrics are rep-
resented by grey plots, and black plots represent
metrics based on n-gram overlap.
The most remarkable aspect is that there exists
a certain trade-off between correlation at segment
versus system level. In fact, this graph produces
a negative Pearson correlation coefficient between
system and segment levels of 0.44. In other words,
depending on how the correlation is computed,
the relative predictive power of metrics can swap.
Therefore, we need additional meta-evaluation cri-
teria in order to clarify the behavior of linguistic
metrics as compared to n-gram based metrics.
However, there are some exceptions. Some
metrics achieve high correlation at both levels.
The first one is ULC (the circle in the plot), which
combines both kind of metrics in a heuristic way
(see Section 3.1). The metric nearest to ULC is
2http://www.nist.gov/speech/tests/mt
Figure 1: Averaged Pearson correlation at system
vs. segment level over all test beds.
DP-Or-?, which computes lexical overlapping but
on dependency relationships. These results are a
first evidence of the advantages of combining met-
rics at several linguistic processing levels.
4.2 Drawbacks of Correlation-based
Meta-evaluation
Although correlation with human judgements is
considered the standard meta-evaluation criterion,
it presents serious drawbacks. With respect to
correlation at system level, the main problem is
that the relative performance of different metrics
changes almost randomly between testbeds. One
of the reasons is that the number of assessed sys-
tems per testbed is usually low, and then correla-
tion has a small number of samples to be estimated
with. Usually, the correlation at system level is
computed over no more than a few systems.
For instance, Table 2 shows the best 10 met-
rics in CE05 according to their correlation with
human judges at the system level, and then the
ranking they obtain in the AE05 testbed. There
are substantial swaps between both rankings. In-
deed, the Pearson correlation of both ranks is only
0.26. This result supports the intuition in (Baner-
jee and Lavie, 2005) that correlation at segment
level is necessary to ensure the reliability of met-
rics in different situations.
However, the correlation values of metrics at
segment level have also drawbacks related to their
interpretability. Most metrics achieve a Pearson
coefficient lower than 0.5. Figure 2 shows two
possible relationships between human and metric
308
Table 2: Metrics rankings according to correlation
with human judgements using CE05 vs. AE05
Figure 2: Human judgements and scores of two
hypothetical metrics with Pearson correlation 0.5
produced scores. Both hypothetical metrics A and
B would achieve a 0.5 correlation. In the case
of Metric A, a high score implies a high human
assessed quality, but not the reverse. This is the
tendency hypothesized by Culy and Riehemann
(2003). In the case of Metric B, the high scored
translations can achieve both low or high quality
according to human judges but low scores ensure
low quality. Therefore, the same Pearson coeffi-
cient may hide very different behaviours. In this
work, we tackle these drawbacks by defining more
specific meta-evaluation criteria.
5 Alternatives to Correlation-based
Meta-evaluation
We have seen that correlation with human judge-
ments has serious limitations for metric evalua-
tion. Therefore, we have focused on other aspects
of metric reliability that have revealed differences
between n-gram and linguistic based metrics:
1. Is the metric able to accurately reveal im-
provements between two systems?
2. Can we trust the metric when it says that a
translation is very good or very bad?
Figure 3: SIP versus SIR
3. Are metrics able to identify good translations
which are dissimilar from the models?
We now discuss each of these aspects sepa-
rately.
5.1 Ability of metrics to Reveal System
Improvements
We now investigate to what extent a significant
system improvement according to the metric im-
plies a significant improvement according to hu-
man assessors, and viceversa. In other words: are
the metrics able to detect any quality improve-
ment? Is a metric score improvement a strong ev-
idence of quality increase? Knowing that a metric
has a 0.8 Pearson correlation at the system level or
0.5 at the segment level does not provide a direct
answer to this question.
In order to tackle this issue, we compare met-
rics versus human assessments in terms of pre-
cision and recall over statistically significant im-
provements within all system pairs in the test
beds. First, Table 3 shows the amount of signif-
icant improvements over human judgements ac-
cording to the Wilcoxon statistical significant test
(? ? 0.025). For instance, the testbed CE2004
consists of 10 systems, i.e. 45 system pairs; from
these, in 40 cases (rightmost column) one of the
systems significantly improves the other.
Now we would like to know, for every metric, if
the pairs which are significantly different accord-
ing to human judges are also the pairs which are
significantly different according to the metric.
Based on these data, we define two meta-
metrics: Significant Improvement Precision (SIP)
and Significant Improvement Recall (SIR). SIP
309
Systems System pairs Sig. imp.
CE2004 10 45 40
AE2004 5 10 8
CE2005 5 10 4
AE2005 5 10 6
Total 25 75 58
Table 3: System pairs with a significant difference
according to human judgements (Wilcoxon test)
(precision) represents the reliability of improve-
ments detected by metrics. SIR (recall) represents
to what extent the metric is able to cover the sig-
nificant improvements detected by humans. Let
Ih be the set of significant improvements detected
by human assessors and Im the set detected by the
metric m. Then:
SIP =
|Ih ? Im|
|Im|
SIR =
|Ih ? Im|
|Ih|
Figure 3 shows the SIR and SIP values obtained
for each metric. Linguistic metrics achieve higher
precision values but at the cost of an important re-
call decrease. Given that linguistic metrics require
matching translation with references at additional
linguistic levels, the significant improvements de-
tected are more reliable (higher precision or SIP),
but at the cost of recall over real significant im-
provements (lower SIR).
This result supports the behaviour predicted in
(Gime?nez and Ma`rquez, 2009). Although linguis-
tic metrics were motivated by the idea of model-
ing linguistic variability, the practical effect is that
current linguistic metrics introduce additional re-
strictions (such as dependency tree overlap, for in-
stance) for accepting automatic translations. Then
they reward precision at the cost of recall in the
evaluation process, and this explains the high cor-
relation with human judgements at system level
with respect to segment level.
All n-gram based metrics achieve SIP and SIR
values between 0.8 and 0.9. This result suggests
that n-gram based metrics are reasonably reliable
for this purpose. Note that the combined met-
ric, ULC (the circle in the figure), achieves re-
sults comparable to n-gram based metrics with
this test3. That is, combining linguistic and n-
gram based metrics preserves the good behavior
of n-gram based metrics in this test.
3Notice that we just have 75 significant improvement
samples, so small differences in SIP or SIR have no relevance
5.2 Reliability of High and Low Metric
Scores
The issue tackled in this section is to what extent
a very low or high score according to the metric
is reliable for detecting extreme cases (very good
or very bad translations). In particular, note that
detecting wrong translations is crucial in order to
analyze the system drawbacks.
In order to define an accuracy measure for the
reliability of very low/high metric scores, it is nec-
essary to define quality thresholds for both the
human assessments and metric scales. Defining
thresholds for manual scores is immediate (e.g.,
lower than 4/10). However, each automatic evalu-
ation metric has its own scale properties. In order
to solve scaling problems we will focus on equiva-
lent rank positions: we associate the ith translation
according to the metric ranking with the quality
value manually assigned to the ith translation in
the manual ranking.
Being Qh(t) and Qm(t) the human and met-
ric assessed quality for the translation t, and being
rankh(t) and rankm(t) the rank of the translation
t according to humans and the metric, the normal-
ized metric assessed quality is:
QNm(t) = Qh(t
?)| (rankh(t
?) = rankm(t))
In order to analyze the reliability of metrics
when identifying wrong or high quality transla-
tions, we look for contradictory results between
the metric and the assessments. In other words,
we look for metric errors in which the quality es-
timated by the metric is low (QNm(t) ? 3) but the
quality assigned by assessors is high (Qh(t) ? 5)
or viceversa (QNm(t) ? 7 and Qh(t) ? 4).
The vertical axis in Figure 4 represents the ra-
tio of errors in the set of low scored translations
according to a given metric. The horizontal axis
represents the ratio of errors over the set of high
scored translations. The first observation is that
all metrics are less reliable when they assign low
scores (which corresponds with the situation A de-
scribed in Section 4.2). For instance, the best met-
ric erroneously assigns a low score in more than
20% of the cases. In general, the linguistic met-
rics do not improve the ability to capture wrong
translations (horizontal axis in the figure). How-
ever, again, the combining metric ULC achieves
the same reliability as the best n-gram based met-
ric.
310
In order to check the robustness of these results,
we computed the correlation of individual metric
failures between test beds, obtaining 0.67 Pearson
for the lowest correlated test bed pair (AE2004 and
CE2005) and 0.88 for the highest correlated pair
(AE2004 and CE2004).
Figure 4: Counter sample ratio for high vs low
metric scored translations
5.2.1 Analysis of Evaluation Samples
In order to shed some light on the reasons for the
automatic evaluation failures when assigning low
scores, we have manually analyzed cases in which
a metric score is low but the quality according to
humans is high (QNm ? 3 and Qh ? 7). We
have studied 100 sentence evaluation cases from
representatives of each metric family including: 1-
PER, BLEU, DP-Or-?, GTM (e = 2), METEOR
and ROUGEL. The evaluation cases have been ex-
tracted from the four test beds. We have identified
four main (non exclusive) failure causes:
Format issues, e.g. ?US ? vs ?United States?).
Elements such as abbreviations, acronyms or num-
bers which do not match the manual translation.
Pseudo-synonym terms, e.g. ?US Scheduled the
Release? vs. ?US set to Release?). ) In most of
these cases, synonymy can only be identified from
the discourse context. Therefore, terminological
resources (e.g., WordNet) are not enough to tackle
this problem.
Non relevant information omissions, e.g.
?Thank you? vs. ?Thank you very much? or
?dollar? vs. ?US dollar?)). The translation
system obviates some information which, in
context, is not considered crucial by the human
assessors. This effect is specially important in
short sentences.
Incorrect structures that change the meaning
while maintaining the same idea (e.g., ?Bush
Praises NASA ?s Mars Mission? vs ? Bush praises
nasa of Mars mission? ).
Note that all of these kinds of failure - except
formatting issues - require deep linguistic process-
ing while n-gram overlap or even synonyms ex-
tracted from a standard ontology are not enough to
deal with them. This conclusion motivates the in-
corporation of linguistic processing into automatic
evaluation metrics.
5.3 Ability to Deal with Translations that are
Dissimilar to References.
The results presented in Section 5.2 indicate that a
high score in metrics tends to be highly related to
truly good translations. This is due to the fact that
a high word overlapping with human references is
a reliable evidence of quality. However, in some
cases the translations to be evaluated are not so
similar to human references.
An example of this appears in the test bed
NIST05AE which includes a human-aided sys-
tem, LinearB (Callison-Burch, 2005). This system
produces correct translations whose words do not
necessarily overlap with references. On the other
hand, a statistics based system tends to produce
incorrect translations with a high level of lexical
overlapping with the set of human references. This
case was reported by Callison-Burch et al (2006)
and later studied by Gime?nez and Ma`rquez (2007).
They found out that lexical metrics fail to pro-
duce reliable evaluation scores. They favor sys-
tems which share the expected reference sublan-
guage (e.g., statistical) and penalize those which
do not (e.g., LinearB).
We can find in our test bed many instances in
which the statistical systems obtain a metric score
similar to the assisted system while achieving a
lower mark according to human assessors. For in-
stance, for the following translations, ROUGEL
assigns a slightly higher score to the output of a
statistical system which contains a lot of grammat-
ical and syntactical failures.
Human assisted system: The Chinese President made un-
precedented criticism of the leaders of Hong Kong after
political failings in the former British colony on Mon-
day . Human assessment=8.5.
Statistical system: Chinese President Hu Jintao today un-
precedented criticism to the leaders of Hong Kong
wake political and financial failure in the former
British colony. Human assessment=3.
311
Figure 5: Maximum translation quality decreasing
over similarly scored translation pairs.
In order to check the metric resistance to be
cheated by translations with high lexical over-
lapping, we estimate the quality decrease that
we could cause if we optimized the human-aided
translations according to the automatic metric. For
this, we consider in each translation case c, the
worse automatic translation t that equals or im-
proves the human-aided translation th according
to the automatic metric m. Formally the averaged
quality decrease is:
Quality decrease(m) =
Avgc(maxt(Qh(th)?Qh(t)|Qm(th) ? Qm(t)))
Figure 5 illustrates the results obtained. All
metrics are suitable to be cheated, assigning sim-
ilar or higher scores to worse translations. How-
ever, linguistic metrics are more resistant. In addi-
tion, the combined metric ULC obtains the best re-
sults, better than both linguistic and n-gram based
metrics. Our conclusion is that including higher
linguistic levels in metrics is relevant to prevent
ungrammatical n-gram matching to achieve simi-
lar scores than grammatical constructions.
5.4 The Oracle System Test
In order to obtain additional evidence about the
usefulness of combining evaluation metrics at dif-
ferent processing levels, let us consider the follow-
ing situation: given a set of reference translations
we want to train a combined system that takes
the most appropriate translation approach for each
text segment. We consider the set of translations
system presented in each competition as the trans-
lation approaches pool. Then, the upper bound on
the quality of the combined system is given by the
Metric OST
maxOST 6.72
ULC 5.79
ROUGEW 5.71
DP-Or-? 5.70
CP-Oc-? 5.70
NIST 5.70
randOST 5.20
minOST 3.67
Table 4: Metrics ranked according to the Oracle
System Test
predictive power of the employed automatic eval-
uation metric. This upper bound is obtained by se-
lecting the highest scored translation t according
to a specific metric m for each translation case c.
The Oracle System Test (OST) consists of com-
puting the averaged human assessed quality Qh
of the selected translations according to human as-
sessors across all cases. Formally:
OST(m) = Avgc(Qh(Argmaxt(Qm(t))|t ? c))
We use the sum of adequacy and fluency, both
in a 1-5 scale, as a global quality measure. Thus,
OST scores are in a 2-10 range. In summary,
the OST represents the best combined system that
could be trained according to a specific automatic
evaluation metric.
Table 4 shows OST values obtained for the best
metrics. In the table we have also included a ran-
dom, a maximum (always pick the best transla-
tion according to humans) and a minimum (al-
ways pick the worse translation according to hu-
man) OST for all 4. The most remarkable result
in Table 4 is that metrics are closer to the random
baseline than to the upperbound (maximum OST).
This result confirms the idea that an improvement
on metric reliability could contribute considerably
to the systems optimization process. However, the
key point is that the combined metric, ULC, im-
proves all the others (5.79 vs. 5.71), indicating
the importance of combining n-gram and linguis-
tic features.
6 Conclusions
Our experiments show that, on one hand, tradi-
tional n-gram based metrics are more or equally
4In all our experiments, the meta-metric values are com-
puted over each test bed independently before averaging in
order to assign equal relevance to the four possible contexts
(test beds)
312
reliable for estimating the translation quality at the
segment level, for predicting significant improve-
ment between systems and for detecting poor and
excellent translations.
On the other hand, linguistically motivated met-
rics improve n-gram metrics in two ways: (i) they
achieve higher correlation with human judgements
at system level and (ii) they are more resistant to
reward poor translations with high word overlap-
ping with references.
The underlying phenomenon is that, rather
than managing the linguistics variability, linguis-
tic based metrics introduce additional restrictions
for assigning high scores. This effect decreases
the recall over significant system improvements
achieved by n-gram based metrics and does not
solve the problem of detecting wrong translations.
Linguistic metrics, however, are more difficult to
cheat.
In general, the greatest pitfall of metrics is the
low reliability of low metric values. Our qualita-
tive analysis of evaluated sentences has shown that
deeper linguistic techniques are necessary to over-
come the important surface differences between
acceptable automatic translations and human ref-
erences.
But our key finding is that combining both kinds
of metrics gives top performance according to ev-
ery meta-evaluation criteria. In addition, our Com-
bined System Test shows that, when training a
combined translation system, using metrics at sev-
eral linguistic processing levels improves substan-
tially the use of individual metrics.
In summary, our results motivate: (i) work-
ing on new linguistic metrics for overcoming the
barrier of linguistic variability and (ii) perform-
ing new metric combining schemes based on lin-
ear regression over human judgements (Kulesza
and Shieber, 2004), training models over hu-
man/machine discrimination (Albrecht and Hwa,
2007) or non parametric methods based on refer-
ence to reference distances (Amigo? et al, 2005).
Acknowledgments
This work has been partially supported by the
Spanish Government, project INES/Text-Mess.
We are indebted to the three ACL anonymous re-
viewers which provided detailed suggestions to
improve our work.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression
for Sentence-Level MT Evaluation with Pseudo Ref-
erences. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Summarization. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
280?289.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
Like vs. Human Acceptable. In Proceedings of
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL), pages 17?24.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in
Machine Translation Research. In Proceedings of
11th Conference of the European Chapter of the As-
sociation for Computational Linguistics (EACL).
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Christopher Culy and Susanne Z. Riehemann. 2003.
The Limits of N-gram Translation Evaluation Met-
rics. In Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternational Conference on Human Language Tech-
nology, pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008b. On the Ro-
bustness of Linguistic Features for Automatic MT
Evaluation. (Under submission).
313
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Jesu?s Gime?nez. 2007. IQMT v 2.0. Technical Manual
(LSI-07-29-R). Technical report, TALP Research
Center. LSI Department. http://www.lsi.
upc.edu/?nlp/IQMT/IQMT.v2.1.pdf.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), pages 75?84.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Offi-
cial release of automatic evaluation scores for all
submissions, August.
Chin-Yew Lin and Franz Josef Och. 2004. Automatic
Evaluation of Machine Translation Quality Using
Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evalu-
ation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation (LREC).
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual Bitext-
Derived Paraphrases in Automatic MT Evaluation.
In Proceedings of the 7th Conference of the As-
sociation for Machine Translation in the Americas
(AMTA), pages 148?155.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Labelled Dependencies in Machine
Translation Evaluation. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
104?111.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, RC22176. Technical
report, IBM T.J. Watson Research Center.
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Appli-
cations for Error Analysis. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 48?55, Prague, Czech Republic, June.
Association for Computational Linguistics.
Florence Reeder, Keith Miller, Jennifer Doyon, and
John White. 2001. The Naming of Things and
the Confusion of Tongues: an MT Metric. In Pro-
ceedings of the Workshop on MT Evaluation ?Who
did what to whom?? at Machine Translation Summit
VIII, pages 55?59.
Christoph Tillmann, Stefan Vogel, Hermann Ney,
A. Zubiaga, and H. Sawaf. 1997. Accelerated DP
based Search for Statistical Translation. In Proceed-
ings of European Conference on Speech Communi-
cation and Technology.
314
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 193?196, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling as Sequential Tagging
Llu??s Ma`rquez, Pere Comas, Jesu?s Gime?nez and Neus Catala`
TALP Research Centre
Technical University of Catalonia (UPC)
{lluism,pcomas,jgimenez,ncatala}@lsi.upc.edu
Abstract
In this paper we present a semantic role
labeling system submitted to the CoNLL-
2005 shared task. The system makes
use of partial and full syntactic informa-
tion and converts the task into a sequen-
tial BIO-tagging. As a result, the label-
ing architecture is very simple . Build-
ing on a state-of-the-art set of features, a
binary classifier for each label is trained
using AdaBoost with fixed depth decision
trees. The final system, which combines
the outputs of two base systems performed
F1=76.59 on the official test set. Addi-
tionally, we provide results comparing the
system when using partial vs. full parsing
input information.
1 Goals and System Architecture
The goal of our work is twofold. On the one hand,
we want to test whether it is possible to implement
a competitive SRL system by reducing the task to a
sequential tagging. On the other hand, we want to
investigate the effect of replacing partial parsing in-
formation by full parsing. For that, we built two dif-
ferent individual systems with a shared sequential
strategy but using UPC chunks-clauses, and Char-
niak?s parses, respectively. We will refer to those
systems as PPUPC and FPCHA, hereinafter.
Both partial and full parsing annotations provided
as input information are of hierarchical nature. Our
system navigates through these syntactic structures
in order to select a subset of constituents organized
sequentially (i.e., non embedding). Propositions are
treated independently, that is, each target verb gen-
erates a sequence of tokens to be annotated. We call
this pre-processing step sequentialization.
The sequential tokens are selected by exploring
the sentence spans or regions defined by the clause
boundaries1. The top-most syntactic constituents
falling inside these regions are selected as tokens.
Note that this strategy is independent of the input
syntactic annotation explored, provided it contains
clause boundaries. It happens that, in the case of
full parses, this node selection strategy is equivalent
to the pruning process defined by Xue and Palmer
(2004), which selects sibling nodes along the path of
ancestors from the verb predicate to the root of the
tree2. Due to this pruning stage, the upper-bound re-
call figures are 95.67% for PPUPC and 90.32% for
FPCHA. These values give F1 performance upper
bounds of 97.79 and 94.91, respectively, assuming
perfect predictors (100% precision).
The nodes selected are labeled with B-I-O tags
depending if they are at the beginning, inside, or out-
side of a verb argument. There is a total of 37 argu-
ment types, which amount to 37*2+1=75 labels.
Regarding the learning algorithm, we used gen-
eralized AdaBoost with real-valued weak classifiers,
which constructs an ensemble of decision trees of
fixed depth (Schapire and Singer, 1999). We con-
sidered a one-vs-all decomposition into binary prob-
1Regions to the right of the target verb corresponding to an-
cestor clauses are omitted in the case of partial parsing.
2With the unique exception of the exploration inside sibling
PP constituents proposed by (Xue and Palmer, 2004).
193
lems to address multi-class classification.
AdaBoost binary classifiers are used for labeling
test sequences in a left-to-right tagging scheme us-
ing a recurrent sliding window approach with infor-
mation about the tag assigned to the preceding to-
ken. This tagging module ensures some basic con-
straints, e.g., BIO correct structure, arguments do
not cross clause boundaries nor base chunk bound-
aries, A0-A5 arguments not present in PropBank
frames for a certain verb are not allowed, etc. We
also tried beam search on top of the classifiers? pre-
dictions to find the sequence of labels with highest
sentence-level probability (as a summation of indi-
vidual predictions). But the results did not improve
the basic greedy tagging.
Regarding feature representation, we used all
input information sources, with the exception of
verb senses and Collins? parser. We did not con-
tribute with significantly original features. Instead,
we borrowed most of them from the existing liter-
ature (Gildea and Jurafsky, 2002; Carreras et al,
2004; Xue and Palmer, 2004). Broadly speaking, we
considered features belonging to four categories3:
(1) On the verb predicate:
? Form; Lemma; POS tag; Chunk type and Type of
verb phrase in which verb is included: single-word or
multi-word; Verb voice: active, passive, copulative, in-
finitive, or progressive; Binary flag indicating if the verb
is a start/end of a clause.
? Subcategorization, i.e., the phrase structure rule expand-
ing the verb parent node.
(2) On the focus constituent:
? Type; Head: extracted using common head-word rules;
if the first element is a PP chunk, then the head of the first
NP is extracted;
? First and last words and POS tags of the constituent.
? POS sequence: if it is less than 5 tags long; 2/3/4-grams
of the POS sequence.
? Bag-of-words of nouns, adjectives, and adverbs in the
constituent.
? TOP sequence: sequence of types of the top-most syn-
tactic elements in the constituent (if it is less than 5 ele-
ments long); in the case of full parsing this corresponds to
the right-hand side of the rule expanding the constituent
node; 2/3/4-grams of the TOP sequence.
? Governing category as described in (Gildea and Juraf-
sky, 2002).
3Features extracted from partial parsing and Named Enti-
ties are common to PPUPC and FPCHA models, while features
coming from Charniak parse trees are implemented exclusively
in the FPCHA model.
? NamedEnt, indicating if the constituent embeds or
strictly-matches a named entity along with its type.
? TMP, indicating if the constituent embeds or strictly
matches a temporal keyword (extracted from AM-TMP ar-
guments of the training set).
(3) Context of the focus constituent:
? Previous and following words and POS tags of the con-
stituent.
? The same features characterizing focus constituents are
extracted for the two previous and following tokens,
provided they are inside the clause boundaries of the cod-
ified region.
(4) Relation between predicate and constituent:
? Relative position; Distance in words and chunks; Level
of embedding with respect to the constituent: in number
of clauses.
? Constituent path as described in (Gildea and Jurafsky,
2002); All 3/4/5-grams of path constituents beginning at
the verb predicate or ending at the constituent.
? Partial parsing path as described in (Carreras et al,
2004); All 3/4/5-grams of path elements beginning at the
verb predicate or ending at the constituent.
? Syntactic frame as described by Xue and Palmer (2004)
2 Experimental Setting and Results
We trained the classification models using the com-
plete training set (sections from 02 to 21). Once con-
verted into one sequence per target predicate, the re-
sulting set amounts 1,049,049 training examples in
the PPUPC model and 828,811 training examples in
the FPCHA model. The average number of labels per
argument is 2.071 and 1.068, respectively. This fact
makes ?I? labels very rare in the FPCHA model.
When running AdaBoost, we selected as weak
rules decision trees of fixed depth 4 (i.e., each branch
may represent a conjunction of at most 4 basic fea-
tures) and trained a classification model per label for
up to 2,000 rounds.
We applied some simplifications to keep training
times and memory requirements inside admissible
bounds. First, we discarded all the argument la-
bels that occur very infrequently and trained only
the 41 most frequent labels in the case of PPUPC
and the 35 most frequent in the case of FPCHA.
The remaining labels where joined in a new label
?other? in training and converted into ?O? when-
ever the SRL system assigns a ?other? label dur-
ing testing. Second, we performed a simple fre-
quency filtering by discarding those features occur-
ring less than 15 times in the training set. As an
194
exception, the frequency threshold for the features
referring to the verb predicate was set to 3. The final
number of features we worked with is 105,175 in the
case of PPUPC and 80,742 in the case of FPCHA.
Training with these very large data and feature
sets becomes an issue. Fortunately, we could split
the computation among six machines in a Linux
cluster. Using our current implementation combin-
ing Perl and C++ we could train the complete mod-
els in about 2 days using memory requirements be-
tween 1.5GB and 2GB. Testing with the ensembles
of 2,000 decision trees per label is also not very effi-
cient, though the resulting speed is admissible, e.g.,
the development set is tagged in about 30 minutes
using a standard PC.
The overall results obtained by our individual
PPUPC and FPCHA SRL systems are presented in ta-
ble 1, with the best results in boldface. As expected,
the FPCHA system significantly outperformed the
PPUPC system, though the results of the later can
be considered competitive. This fact is against the
belief, expressed as one of the conclusions of the
CoNLL-2004 shared task, that full-parsing systems
are about 10 F1 points over partial-parsing systems.
In this case, we obtain a performance difference of
2.18 points in favor of FPCHA.
Apart from resulting performance, there are addi-
tional advantages when using the FPCHA approach.
Due to the coarser granularity of sequence tokens,
FPCHA sequences are shorter. There are 21% less
training examples and a much lower quantity of ?I?
tags to predict (the mapping between syntactic con-
stituents and arguments is mostly one-to-one). As
a consequence, FPCHA classifiers train faster with
less memory requirements, and achieve competitive
results (near the optimal) with much less rounds of
boosting. See figure 1. Also related to the token
granularity, the number of completely correct out-
puts is 4.13 points higher in FPCHA, showing that
the resulting labelings are structurally better than
those of PPUPC.
Interestingly, the PPUPC and FPCHA systems
make quite different argument predictions. For in-
stance, FPCHA is better at recognizing A0 and A1
arguments since parse constituents corresponding to
these arguments tend to be mostly correct. Compar-
atively, PPUPC is better at recognizing A2-A4 argu-
ments since they are further from the verb predicate
 64
 66
 68
 70
 72
 74
 76
 78
 200  400  600  800  1000  1200  1400  1600  1800  2000
O
ve
ra
ll F
1
Number of rounds
PP-upc
FP-cha
PP best
FP-cha best
Figure 1: Overall F1 performance of individual sys-
tems on the development set with respect to the num-
ber of learning rounds
Perfect props Precision Recall F?=1
PPUPC 47.38% 76.86% 70.55% 73.57
FPCHA 51.51% 78.08% 73.54% 75.75
Combined 51.39% 78.39% 75.53% 76.93
Table 1: Overall results of the individual systems on
the development set.
and tend to accumulate more parsing errors, while
the fine granularity of the PPUPC sequences still al-
low to capture them4. Another interesting observa-
tion is that the precision of both systems is much
higher than the recall.
The previous two facts suggest that combining the
outputs of the two systems may lead to a significant
improvement. We experimented with a greedy com-
bination scheme for joining the maximum number of
arguments from both solutions in order to increase
coverage and, hopefully, recall. It proceeds depart-
ing from an empty solution by: First, adding all the
arguments from FPCHA in which this method per-
forms best; Second, adding all the arguments from
PPUPC in which this method performs best; and
Third, making another loop through the two meth-
ods adding the arguments not considered in the first
loop. At each step, we require that the added argu-
ments do not overlap/embed with arguments in the
current solution and also that they do not introduce
repetitions of A0-A5 arguments. The results on the
4As an example, the F1 performance of PPUPC on A0 and
A2 arguments is 79.79 and 65.10, respectively. The perfor-
mance of FPCHA on the same arguments is 84.03 and 62.36.
195
Precision Recall F?=1
Development 78.39% 75.53% 76.93
Test WSJ 79.55% 76.45% 77.97
Test Brown 70.79% 64.35% 67.42
Test WSJ+Brown 78.44% 74.83% 76.59
Test WSJ Precision Recall F?=1
Overall 79.55% 76.45% 77.97
A0 87.11% 86.28% 86.69
A1 79.60% 76.72% 78.13
A2 69.18% 67.75% 68.46
A3 76.38% 56.07% 64.67
A4 79.78% 69.61% 74.35
A5 0.00% 0.00% 0.00
AM-ADV 59.15% 52.37% 55.56
AM-CAU 73.68% 57.53% 64.62
AM-DIR 71.43% 35.29% 47.24
AM-DIS 77.14% 75.94% 76.54
AM-EXT 63.64% 43.75% 51.85
AM-LOC 62.74% 54.27% 58.20
AM-MNR 54.33% 52.91% 53.61
AM-MOD 96.16% 95.46% 95.81
AM-NEG 99.13% 98.70% 98.91
AM-PNC 53.49% 40.00% 45.77
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 77.68% 78.75% 78.21
R-A0 86.84% 88.39% 87.61
R-A1 75.32% 76.28% 75.80
R-A2 54.55% 37.50% 44.44
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-TMP 69.81% 71.15% 70.48
V 99.16% 99.16% 99.16
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
development set (presented in table 1) confirm our
expectations, since a performance increase of 1.18
points over the best individual system was observed,
mainly caused by recall improvement. The final sys-
tem we presented at the shared task performs exactly
this solution merging procedure. When applied on
the WSJ test set, the combination scheme seems to
generalize well, since an improvement is observed
with respect to the development set. See the offi-
cial results of our system, which are presented in ta-
ble 2. Also from that table, it is worth noting that the
F1 performance drops by more than 9 points when
tested on the Brown test set, indicating that the re-
sults obtained on the WSJ corpora do not generalize
well to corpora with other genres. The study of the
sources of this lower performance deserves further
investigation, though we do not believe that it is at-
tributable to the greedy combination scheme.
3 Conclusions
We have presented a simple SRL system submit-
ted to the CoNLL-2005 shared task, which treats
the SRL problem as a sequence tagging task (us-
ing a BIO tagging scheme). Given the simplic-
ity of the approach, we believe that the results are
very good and competitive compared to the state-
of-the-art. We also provided a comparison between
two SRL systems sharing the same architecture, but
build on partial vs. full parsing, respectively. Al-
though the full parsing approach obtains better re-
sults and has some implementation advantages, the
partial parsing system shows also a quite competi-
tive performance. The results on the development
set differ in 2.18 points, but the outputs generated
by the two systems are significantly different. The
final system, which scored F1=76.59 in the official
test set, is a combination of both individual systems
aiming at increasing coverage and recall.
Acknowledgements
This research has been partially supported by the
European Commission (CHIL project, IP-506909).
Jesu?s Gime?nez is a research fellow from the Span-
ish Ministry of Science and Technology (ALIADO
project, TIC2002-04447-C02). We would like to
thank also Xavier Carreras for providing us with
many software components and Mihai Surdeanu for
fruitful discussions on the problem and feature engi-
neering.
References
X. Carreras, L. Ma`rquez, and G. Chrupa?a. 2004. Hierarchical
recognition of propositional arguments with perceptrons. In
Proceedings of CoNLL-2004.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
R. E. Schapire and Y. Singer. 1999. Improved Boosting Algo-
rithms Using Confidence-rated Predictions. Machine Learn-
ing, 37(3).
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing (EMNLP).
196
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 145?148,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Combining Linguistic Data Views for Phrase-based SMT
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
We describe the Spanish-to-English LDV-
COMBO system for the Shared Task 2:
?Exploiting Parallel Texts for Statistical
Machine Translation? of the ACL-2005
Workshop on ?Building and Using Par-
allel Texts: Data-Driven Machine Trans-
lation and Beyond?. Our approach ex-
plores the possibility of working with
alignments at different levels of abstrac-
tion, using different degrees of linguistic
annotation. Several phrase-based trans-
lation models are built out from these
alignments. Their combination significa-
tively outperforms any of them in isola-
tion. Moreover, we have built a word-
based translation model based on Word-
Net which is used for unknown words.
1 Introduction
The main motivation behind our work is to intro-
duce linguistic information, other than lexical units,
to the process of building word and phrase align-
ments. Many other authors have tried to do so. See
(Och and Ney, 2000), (Yamada and Knight, 2001),
(Koehn and Knight, 2002), (Koehn et al, 2003),
(Schafer and Yarowsky, 2003) and (Gildea, 2003).
Far from full syntactic complexity, we suggest to
go back to the simpler alignment methods first de-
scribed by (Brown et al, 1993). Our approach ex-
ploits the possibility of working with alignments at
two different levels of granularity, lexical (words)
and shallow parsing (chunks). In order to avoid con-
fusion so forth we will talk about tokens instead of
words as the minimal alignment unit.
Apart from redefining the scope of the alignment
unit, we may use different degrees of linguistic an-
notation. We introduce the general concept of data
view, which is defined as any possible representation
of the information contained in a bitext. We enrich
data view tokens with features further than lexical
such as PoS, lemma, and chunk label.
As an example of the applicability of data views,
suppose the case of the word ?plays? being seen in
the training data acting as a verb. Representing this
information as ?playsVBZ? would allow us to distin-
guish it from its homograph ?playsNNS? for ?plays? as
a noun. Ideally, one would wish to have still deeper
information, moving through syntax onto semantics,
such as word senses. Therefore, it would be possible
to distinguish for instance between two realizations
of ?plays? with different meanings: ?hePRP playsVBG
guitarNN? and ?hePRP playsVBG basketballNN?.
Of course, there is a natural trade-off between the
use of data views and data sparsity. Fortunately, we
hava data enough so that statistical parameter esti-
mation remains reliable.
2 System Description
The LDV-COMBO system follows the SMT architec-
ture suggested by the workshop organizers.
First, training data are linguistically annotated for
the two languages involved (See subsection 2.1).
10 different data views have been built. Notice
that it is not necessary that the two parallel coun-
terparts of a bitext share the same data view, as
145
long as they share the same granularity. How-
ever, in all our experiments we have annotated both
sides with the same linguistic information. See
token descriptions: (W) word, (WL) word and
lemma, (WP) word and PoS, (WC) word and chunk
label, (WPC) word, PoS and chunk label, (Cw)
chunk of words (Cwl), chunk of words and lem-
mas, (Cwp) chunk of words and PoS (Cwc) chunk
of words and chunk labels (Cwpc) chunk of words,
PoS and chunk labels. By chunk label we re-
fer to the IOB label associated to every word in-
side a chunk, e.g. ?IB?NP declareB?V P resumedI?V P
theB?NP sessionI?NP ofB?PP theB?NP EuropeanI?NP
ParliamentI?NP .O?). We build chunk tokens by ex-
plicitly connecting words in the same chunk, e.g.
?(I)NP (declare resumed)V P (the session)NP (of)PP
(the European Parliament)NP ?. See examples of
some of these data views in Table 1.
Then, running GIZA++, we obtain token align-
ments for each of the data views. Combined phrase-
based translation models are built on top of the
Viterbi alignments output by GIZA++. See details
in subsection 2.2. Combo-models must be then post-
processed in order to remove the additional linguis-
tic annotation and split chunks back into words, so
they fit the format required by Pharaoh.
Moreover, we have used the Multilingual Central
Repository (MCR), a multilingual lexical-semantic
database (Atserias et al, 2004), to build a word-
based translation model. We back-off to this model
in the case of unknown words, with the goal of im-
proving system recall. See subsection 2.3.
2.1 Data Representation
In order to achieve robustness the same tools have
been used to linguistically annotate both languages.
The SVMTool1 has been used for PoS-tagging
(Gime?nez and Ma`rquez, 2004). The Freeling2 pack-
age (Carreras et al, 2004) has been used for lemma-
tizing. Finally, the Phreco software by (Carreras et
al., 2005) has been used for shallow parsing.
No additional tokenization or pre-processing
steps other than case lowering have been performed.
Special treatment of named entities, dates, numbers,
1The SVMTool may be freely downloaded at
http://www.lsi.upc.es/?nlp/SVMTool/ .
2Freeling Suite of Language Analyzers may be downloaded
at http://www.lsi.upc.es/?nlp/freeling/
currency, etc., should be considered so as to further
enhance the system.
2.2 Building Combined Translation Models
Because data views capture different, possibly com-
plementary, aspects of the translation process it
seems reasonable to combine them. We consider
two different ways of building such combo-models:
LPHEX Local phrase extraction. To build a separate
phrase-based translation model for each data
view alignment, and then combine them. There
are two ways of combining translation models:
MRG Merging translation models. We work on
a weighted linear interpolation of models.
These weights may be tuned, although a
uniform weight selection yields good re-
sults. Additionally, phrase-pairs may be
filtered out by setting a score threshold.
noMRG Passing translation models directly to
the Pharaoh decoder. However, we en-
countered many problems with phrase-
pairs that were not seen in all single mod-
els. This obliged us to apply arbitrary
smoothing values to score these pairs.
GPHEX Global phrase extraction. To build a sin-
gle phrased-based translation model from the
union of alignments from several data views.
In its turn, any MRG operation performed on a
combo-model results again in a valid combo-model.
In any case, phrase extraction3 is performed as de-
picted by (Och, 2002).
2.3 Using the MCR
Outer knowledge may be supplied to the Pharaoh
decoder by annotating the input with alternative
translation options via XML-markup. We enrich
every unknown word by looking up every possi-
ble translation for all of its senses in the MCR.
These are scored by relative frequency according to
the number of senses that lexicalized in the same
manner. Let wf , pf be the source word and PoS,
and we be the target word, we define a function
3We always work with the union of alignments, no heuristic
refinement, and phrases up to 5 tokens. Phrase pairs appearing
only once have been discarded. Scoring is performed by relative
frequency. No smoothing is applied.
146
It[PRP :B?NP ] would[MD:B?V P ] appear[VB:I?V P ] that[IN:B?SBAR] a[DT :B?NP ] speech[NN:I?NP ] made[VBN:B?V P ]
at[IN:B?PP ] the[DT :B?NP ] weekend[NN:I?NP ] by[IN:B?PP ] Mr[NNP :B?NP ] Fischler[NNP :I?NP ]
indicates[VBZ:B?V P ] a[DT :B?NP ] change[NN:I?NP ] of[IN:B?PP ] his[PRP$:B?NP ] position[NN:I?NP ] .[.:O]
WPC
Fischler[VMN:B?V P ] pronuncio?[VMI:B?V P ] un[DI:B?NP ] discurso[NC:I?NP ] este[DD:B?NP ] fin[NC:I?NP ]
de[SP :B?PP ] semana[NC:B?NP ] en[SP :B?PP ] el[DA:B?SBAR] que[PR0:I?SBAR] parec??a[VMI:B?V P ]
haber[VAN:I?V P ] cambiado[VMP :I?V P ] de[SP :B?PP ] actitud[NC:B?NP ] .[Fp:O]
(It[PRP :B?NP ]]) (would[MD:B?V P ]] appear[VB:I?V P ]) (that[IN:B?SBAR]) (a[DT :B?NP ] speech[NN:I?NP ])
(made[VBN:B?V P ]) (at[IN:B?PP ]) (the[DT :B?NP ] weekend[NN:I?NP ]) (by[IN:B?PP ])
(Mr[NNP :B?NP ] Fischler[NNP :I?NP ]) (indicates[VBZ:B?V P ]) (a[DT :B?NP ] change[NN:I?NP ])
(of[IN:B?PP ]) (his[PRP$:B?NP ] position[NN:I?NP ]) (.[.:O])
Cwpc
(Fischler[VMN:B?V P ]) (pronuncio?[VMI:B?V P ]) (un[DI:B?NP ] discurso[NC:I?NP ]) (este[DD:B?NP ] fin[NC:I?NP ])
(de[SP :B?PP ]) (semana[NC:B?NP ]) (en[SP :B?PP ]) (el[DA:B?SBAR] que[PR0:I?SBAR])
(parec??a[VMI:B?V P ] haber[VAN:I?V P ] cambiado[VMP :I?V P ]) (de[SP :B?PP ]) (actitud[NC:B?NP ]) (.[Fp:O])
Table 1: An example of 2 rich data views: (WPC) word, PoS and IOB chunk label (Cwpc) chunk of word, PoS and chunk label.
Scount(wf , pf , we) which counts the number of
senses for (wf , pf ) which can lexicalize as we. A
translation pair is scored as:
score(wf , pf |we) =
Scount(wf , pf , we)
?
(wf ,pf ) Scount(wf , pf , we)
(1)
Better results would be expected working with
word sense disambiguated text. We are not at this
point yet. A first approach could be to work with the
most frequent sense heuristic.
3 Experimental Results
3.1 Data and Evaluation Metrics
We have used the data sets and language model pro-
vided by the organization. No extra training or de-
velopment data were used in our experiments.
We evaluate results with 3 different metrics: GTM
F1-measure (e = 1, 2), BLEU score (n = 4) as pro-
vided by organizers, and NIST score (n = 5).
3.2 Experimenting with Data Views
Table 2 presents MT results for the 10 elementary
data views devised in Section 2. Default parameters
are used for ?tm, ?lm, and ?w. No tuning has been
performed. As expected, word-based views obtain
significatively higher results than chunk-based. All
data views at the same level of granularity obtain
comparable results.
In Table 3 MT results for different data view com-
binations are showed. Merged model weights are
set equiprobable, and no phrase-pair score filtering
data view GTM-1 GTM-2 BLEU NIST
W 0.6108 0.2609 25.92 7.1576
WL 0.6110 0.2601 25.77 7.1496
WP 0.6096 0.2600 25.74 7.1415
WC 0.6124 0.2600 25.98 7.1852
WPC 0.6107 0.2587 25.79 7.1595
Cw 0.5749 0.2384 22.73 6.6149
Cwl 0.5756 0.2385 22.73 6.6204
Cwp 0.5771 0.2395 23.06 6.6403
Cwc 0.5759 0.2390 22.86 6.6207
Cwpc 0.5744 0.2379 22.77 6.5949
Table 2: MT Results for the 10 elementary data views on the
development set.
is performed. We refer to the W model as our base-
line. In this view, only words are used. The 5W-MRG
and 5W-GPHEX models use a combination of the 5
word-based data views, as in MRG and GPHEX, re-
spectively. The 5C-MRG and 5C-GPHEX system use
a combination of the 5 chunk based data views, as
in MRG and GPHEX, respectively. The 10-MRG sys-
tem uses all 10 data views combined as in MRG. The
10-GPHEX/MRG system uses the 5 word based views
combined as in GPHEX, the 5 chunk based views
combined as in GPHEX, and then a combination of
these two combo-models as in MRG.
data view GTM-1 GTM-2 BLEU NIST
W 0.6108 0.2609 25.92 7.1576
5W-MRG 0.6134 0.2631 26.25 7.2122
5W-GPHEX 0.6172 0.2615 26.95 7.2823
5C-MRG 0.5786 0.2407 23.18 6.6754
5C-GPHEX 0.5739 0.2368 22.80 6.5714
10-MRG 0.6130 0.2624 26.24 7.2196
10-GPHEX/MRG 0.6142 0.2600 26.58 7.2542
Table 3: MT Results without tuning, for some data view com-
binations on the development set.
147
It can be seen that results improve by combining
several data views. Furthermore, global phrase ex-
traction (GPHEX) seems to work much finer than lo-
cal phrase extraction (LPHEX).
Table 4 shows MT results after optimizing ?tm,
?lm, ?w, and the weights for the MRG operation,
by means of the Downhill Simplex Method in Multi-
dimensions (William H. Press and Flannery, 2002).
Observe that tuning the system improves the perfor-
mance considerably. The ?w parameter is particu-
larly sensitive to tuning.
Even though the performance of chunk-based
models is poor, the best results are obtained by com-
binining the two levels of abstraction, thus proving
that syntactically motivated phrases may help. 10-
MRG and 10-GPHEX models achieve a similar per-
formance. The 10-MRG-bestWN system corresponds
to the 10-MRG model using WordNet. The 10-MRG-
subWN system is this same system at the time of sub-
mission. Results using WordNet, taking into account
that the number of unknown4 words in the develop-
ment set was very small, are very promising.
data view GTM-1 GTM-2 BLEU NIST
W 0.6174 0.2583 28.13 7.1540
5W-MRG 0.6206 0.2605 28.50 7.2076
5W-GPHEX 0.6207 0.2603 28.38 7.1992
5C-MRG 0.5882 0.2426 25.06 6.6773
5C-GPHEX 0.5816 0.2387 24.40 6.5595
10-MRG 0.6218 0.2623 28.88 7.2491
10-GPHEX/MRG 0.6229 0.2622 28.82 7.2414
10-MRGWN 0.6228 0.2625 28.90 7.2583
10-MRG-subWN 0.6228 0.2622 28.79 7.2528
Table 4: MT Results for some data view combinations after
tuning on the development set.
4 Conclusions
We have showed that it is possible to obtain better
phrase-based translation models by utilizing align-
ments built on top of different linguistic data views.
These models can be robustly combined, signifi-
cantly outperforming all of their components in iso-
lation. We leave for further work the experimen-
tation of new data views such as word senses and
semantic roles, as well as their natural porting and
evolution from the alignment step to phrase extrac-
tion and decoding.
4Translation for 349 unknown words was found in the MCR.
Acknowledgements
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02). Authors are thankful to Pa-
trik Lambert for providing us with the implementa-
tion of the Simplex Method used for tuning.
References
Jordi Atserias, Luis Villarejo, German Rigau, Eneko
Agirre, John Carroll, Bernardo Magnini, and Piek
Vossen. 2004. The meaning multilingual central
repository. In Proceedings of GWC, Brno, Czech Re-
public, January. ISBN 80-210-3302-9.
Peter E Brown, Stephen A. Della Pietra, Robert L. Mer-
cer, and Vincent J. Della Pietra. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. Freeling: An open-source suite of lan-
guage analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of ACL.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. Svmtool: A
general pos tagger generator based on support vector
machines. In Proceedings of 4th LREC.
Philipp Koehn and Kevin Knight. 2002. Chunkmt:
Statistical machine translation with richer linguistic
knowledge. Draft.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Charles Schafer and David Yarowsky. 2003. Statistical
machine translation using coercive two-level syntactic
transduction. In Proceedings of EMNLP.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing. Cambridge
University Press.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
148
Proceedings of the Workshop on Statistical Machine Translation, pages 166?169,
New York City, June 2006. c?2006 Association for Computational Linguistics
The LDV-COMBO system for SMT
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
We describe the LDV-COMBO system pre-
sented at the Shared Task. Our approach
explores the possibility of working with
alignments at different levels of abstrac-
tion using different degrees of linguis-
tic analysis from the lexical to the shal-
low syntactic level. Translation mod-
els are built on top of combinations of
these alignments. We present results
for the Spanish-to-English and English-to-
Spanish tasks. We show that liniguistic in-
formation may be helpful, specially when
the target language has a rich morphology.
1 Introduction
The main motivation behind our work is to introduce
linguistic information, other than lexical units, to the
process of building word and phrase alignments. In
the last years, many efforts have been devoted to this
matter (Yamada and Knight, 2001; Gildea, 2003).
Following our previous work (Gime?nez and
Ma`rquez, 2005), we use shallow syntactic informa-
tion to generate more precise alignments. Far from
full syntactic complexity, we suggest going back to
the simpler alignment methods first described by
IBM (1993). Our approach exploits the possibil-
ity of working with alignments at two different lev-
els of granularity, lexical (words) and shallow pars-
ing (chunks). Apart from redefining the scope of
the alignment unit, we may use different linguistic
data views. We enrich tokens with features further
than lexical such as part-of-speech (PoS), lemma,
and chunk IOB label.
For instance, suppose the case illustrated in Fig-
ure 1 where the lexical item ?plays? is seen acting as
a verb and as a noun. Considering these two words,
with the same lexical realization, as a single token
adds noise to the word alignment process. Repre-
senting this information, by means of linguistic data
views, as ?playsV BZ? and ?playsNNS? would allow us
to distinguish between the two cases. Ideally, one
would wish to have still deeper information, moving
through syntax onto semantics, such as word senses.
Therefore, it would be possible to distinguish for
instance between two realizations of ?plays? with
different meanings: ?hePRP playsV BG guitarNN ? and
?hePRP playsV BG footballNN ?. Of course, there is a
natural trade-off between the use of linguistic data
views and data sparsity. Fortunately, we hava data
enough so that statistical parameter estimation re-
mains reliable.
The approach which is closest to ours is that by
Schafer and Yarowsky (2003) who suggested a com-
bination of models based on shallow syntactic anal-
ysis (part-of-speech tagging and phrase chunking).
They followed a backoff strategy in the application
of their models. Decoding was based on Finite State
Automata. Although no significant improvement in
MT quality was reported, results were promising
taking into account the short time spent in the de-
velopment of the linguistic tools utilized.
Our system is further described in Section 2. Re-
sults are reported in Section 3. Conclusions and fur-
ther work are briefly outlined in Section 4.
166
Figure 1: A case of word alignment possibilities on top of lexical units (a) and linguistic data views (b).
2 System Description
The LDV-COMBO system follows the SMT architec-
ture suggested by the workshop organizers. We use
the Pharaoh beam-search decoder (Koehn, 2004).
First, training data are linguistically annotated. In
order to achieve robustness the same tools have been
used to linguistically annotate both languages. The
SVMTool1 has been used for PoS-tagging (Gime?nez
and Ma`rquez, 2004). The Freeling2 package (Car-
reras et al, 2004) has been used for lemmatizing.
Finally, the Phreco software (Carreras et al, 2005)
has been used for shallow parsing. In this paper we
focus on data views at the word level. 6 different
data views have been built: (W) word, (L) lemma,
(WP) word and PoS, (WC) word and chunk IOB la-
bel, (WPC) word, PoS and chunk IOB label, (LC)
lemma and chunk IOB label.
Then, running GIZA++ (Och and Ney, 2003), we
obtain token alignments for each of the data views.
Combined phrase-based translation models are built
on top of the Viterbi alignments output by GIZA++.
Phrase extraction is performed following the phrase-
extract algorithm depicted by Och (2002). We do
not apply any heuristic refinement. We work with
phrases up to 5 tokens. Phrase pairs appearing only
once have been discarded. Scoring is performed by
relative frequency. No smoothing is applied.
In this paper we focus on the global phrase ex-
traction (GPHEX) method described by Gime?nez
1The SVMTool may be freely downloaded at
http://www.lsi.upc.es/?nlp/SVMTool/ .
2Freeling Suite of Language Analyzers may be downloaded
at http://www.lsi.upc.es/?nlp/freeling/
and Ma`rquez (2005). We build a single translation
model from the union of alignments from the 6 data
views described above. This model must match the
input format. For instance, if the input is annotated
with word and PoS (WP), so must be the translation
model. Therefore either the input must be enriched
with linguistic annotation or translation models must
be post-processed in order to remove the additional
linguistic annotation. We did not observe significant
differences in either alternative. Therefore, we sim-
ply adapted translations models to work under the
assumption of unannotated inputs (W).
3 Experimental Work
3.1 Setting
We have used only the data sets and language model
provided by the organization. For evaluation we
have selected a set of 8 metric variants correspond-
ing to seven different families: BLEU (n = 4) (Pa-
pineni et al, 2001), NIST (n = 5) (Lin and Hovy,
2002), GTM F1-measure (e = 1, 2) (Melamed et al,
2003), 1-WER (Nie?en et al, 2000), 1-PER (Leusch
et al, 2003), ROUGE (ROUGE-S*) (Lin and Och,
2004) and METEOR3 (Banerjee and Lavie, 2005).
Optimization of the decoding parameters (?tm, ?lm,
?w) is performed by means of the Downhill Simplex
Method in Multidimensions (William H. Press and
Flannery, 2002) over the BLEU metric.
3For Spanish-to-English we applied all available modules:
exact + stemming + WordNet stemming + WordNet synonymy
lookup. However, for English-to-Spanish we were forced to use
the exact module alone.
167
Spanish-to-English
System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S*
Baseline 0.5514 0.3741 0.2709 0.6159 0.2579 0.5836 7.2958 0.3643
LDV-COMBO 0.5478 0.3657 0.2708 0.6202 0.2585 0.5928 7.2433 0.3671
English-to-Spanish
System 1-PER 1-WER BLEU-4 GTM-1 GTM-2 METEOR NIST-5 ROUGE-S*
Baseline 0.5158 0.3776 0.2272 0.5673 0.2418 0.4954 6.6835 0.3028
LDV-COMBO 0.5382 0.3560 0.2611 0.5910 0.2462 0.5400 7.1054 0.3240
Table 1: MT results comparing the LDV-COMBO system to a baseline system, for the test set both on the
Spanish-to-English and English-to-Spanish tasks.
English Reference: consider germany , where some leaders [...]
Spanish Reference: pensemos en alemania , donde algunos dirigentes [...]
English-to-Spanish Baseline estiman que alemania , donde algunos dirigentes [...]
LDV-COMBO pensemos en alemania , donde algunos dirigentes [...]
Table 2: A case of error analysis.
3.2 Results
Table 1 presents MT results for the test set both
for the Spanish-to-English and English-to-Spanish
tasks. The variant of the LDV-COMBO system de-
scribed in Section 2 is compared to a baseline vari-
ant based only on lexical items. In the case of
Spanish-to-English performance varies from metric
to metric. Therefore, an open issue is which metric
should be trusted. In any case, the differences are
minor. However, in the case of English-to-Spanish
all metrics but ?1-WER? agree to indicate that the
LDV-COMBO system significantly outperforms the
baseline. We suspect this may be due to the richer
morphology of Spanish. In order to test this hy-
pothesis we performed an error analysys at the sen-
tence level based on the GTM F-measure. We found
many cases where the LDV-COMBO system outper-
forms the baseline system by choosing a more ac-
curate translation. For instance, in Table 2 we may
see a fragment of the case of sentence 2176 in the
test set. A better translation for ?consider? is pro-
vided, ?pensemos?, which corresponds to the right
verb and verbal form (instead of ?estiman?). By in-
specting translation models we confirmed the better
adjustment of probabilities.
Interestingly, LDV-COMBO translation models are
between 30% and 40% smaller than the models
based on lexical items alone. The reason is that we
are working with the union of alignments from dif-
ferent data views, thus adding more constraints into
the phrase extraction step. Fewer phrase pairs are
extracted, and as a consequence we are also effec-
tively eliminating noise from translation models.
4 Conclusions and Further Work
Many researchers remain sceptical about the use-
fulness of linguistic information in SMT, because,
except in a couple of cases (Charniak et al, 2003;
Collins et al, 2005), little success has been reported.
In this work we have shown that liniguistic informa-
tion may be helpful, specially when the target lan-
guage has a rich morphology (e.g. Spanish).
Moreover, it has often been argued that linguistic
information does not yield significant improvements
in MT quality, because (i) linguistic processors in-
troduce many errors and (ii) the BLEU score is not
specially sensitive to the grammaticality of MT out-
put. We have minimized the impact of the first ar-
gument by using highly accurate tools for both lan-
guages. In order to solve the second problem more
sophisticated metrics are required. Current MT eval-
uation metrics fail to capture many aspects of MT
168
quality that characterize human translations with re-
spect to those produced by MT systems. We are de-
voting most of our efforts to the deployment of a new
MT evaluation framework which allows to combine
several similarity metrics into a single measure of
quality (Gime?nez and Amigo?, 2006).
We also leave for further work the experimenta-
tion of new data views such as word senses and se-
mantic roles, as well as their natural porting from the
alignment step to phrase extraction and decoding.
Acknowledgements
This research has been funded by the Spanish
Ministry of Science and Technology (ALIADO
TIC2002-04447-C02). Authors are thankful to Pa-
trik Lambert for providing us with the implementa-
tion of the Simplex Method used for tuning.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Peter E Brown, Stephen A. Della Pietra, Robert L. Mer-
cer, and Vincent J. Della Pietra. 1993. The Mathemat-
ics of Statistical Machine Translation: Parameter Esti-
mation. Computational Linguistics, 19(2):263?311.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro.
2005. Filtering-Ranking Perceptron Learning for Par-
tial Parsing. Machine Learning, 59:1?31.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based Language Models for Machine
Translation. In Proceedings of MT SUMMIT IX.
Michael Collins, Philipp Koehn, and Ivona Kucerova?.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL.
Daniel Gildea. 2003. Loosely Tree-Based Alignment for
Machine Translation. In Proceedings of ACL.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In Pro-
ceedings of the Workshop on Building and Using Par-
allel Texts, ACL.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA.
G. Leusch, N. Ueffing, and H. Ney. 2003. A Novel
String-to-String Distance Measure with Applications
to Machine Translation Evaluation. In Proceedings of
MT Summit IX.
Chin-Yew Lin and E.H. Hovy. 2002. Automatic Eval-
uation of Machine Translation Quality Using N-gram
Co-Occurrence Statistics. Technical report, National
Institute of Standards and Technology.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of ACL.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL.
S. Nie?en, F.J. Och, G. Leusch, and H. Ney. 2000. Eval-
uation Tool for Machine Translation: Fast Evaluation
for MT Research. In Proceedings of the 2nd Interna-
tional Conference on Language Resources and Evalu-
ation.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation, rc22176. Technical report,
IBM T.J. Watson Research Center.
Charles Schafer and David Yarowsky. 2003. Statistical
Machine Translation Using Coercive Two-Level Syn-
tactic Transduction. In Proceedings of EMNLP.
William T. Vetterling William H. Press, Saul A. Teukol-
sky and Brian P. Flannery. 2002. Numerical Recipes
in C++: the Art of Scientific Computing. Cambridge
University Press.
Kenji Yamada and Kevin Knight. 2001. A Syntax-based
Statistical Translation Model. In Proceedings of ACL.
169
Proceedings of the Second Workshop on Statistical Machine Translation, pages 159?166,
Prague, June 2007. c?2007 Association for Computational Linguistics
Context-aware Discriminative Phrase Selection
for Statistical Machine Translation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
In this work we revise the application
of discriminative learning to the problem
of phrase selection in Statistical Machine
Translation. Inspired by common tech-
niques used in Word Sense Disambiguation,
we train classifiers based on local context
to predict possible phrase translations. Our
work extends that of Vickrey et al (2005) in
two main aspects. First, we move from word
translation to phrase translation. Second, we
move from the ?blank-filling? task to the ?full
translation? task. We report results on a set
of highly frequent source phrases, obtaining
a significant improvement, specially with re-
spect to adequacy, according to a rigorous
process of manual evaluation.
1 Introduction
Translations tables in Phrase-based Statistical Ma-
chine Translation (SMT) are often built on the ba-
sis of Maximum-likelihood Estimation (MLE), be-
ing one of the major limitations of this approach that
the source sentence context in which phrases occur
is completely ignored (Koehn et al, 2003).
In this work, inspired by state-of-the-art Word
Sense Disambiguation (WSD) techniques, we sug-
gest using Discriminative Phrase Translation (DPT)
models which take into account a wider feature
context. Following the approach by Vickrey et al
(2005), we deal with the ?phrase translation? prob-
lem as a classification problem. We use Support
Vector Machines (SVMs) to predict phrase transla-
tions in the context of the whole source sentence.
We extend the work by Vickrey et al (2005) in two
main aspects. First, we move from ?word transla-
tion? to ?phrase translation?. Second, we move from
the ?blank-filling? task to the ?full translation? task.
Our approach is fully described in Section 2. We
apply it to the Spanish-to-English translation of Eu-
ropean Parliament Proceedings. In Section 3, prior
to considering the ?full translation? task, we ana-
lyze the impact of using DPT models for the iso-
lated ?phrase translation? task. In spite of working
on a very specific domain, a large room for improve-
ment, coherent with WSD performance, and results
by Vickrey et al (2005), is predicted. Then, in Sec-
tion 4, we tackle the full translation task. DPT mod-
els are integrated in a ?soft? manner, by making them
available to the decoder so they can fully interact
with other models. Results using a reduced set of
highly frequent source phrases show a significant
improvement, according to several automatic eval-
uation metrics. Interestingly, the BLEU metric (Pap-
ineni et al, 2001) is not able to reflect this improve-
ment. Through a rigorous process of manual eval-
uation we have verified the gain. We have also ob-
served that it is mainly related to adequacy. These
results confirm that better phrase translation proba-
bilities may be helpful for the full translation task.
However, the fact that no gain in fluency is reported
indicates that the integration of these probabilities
into the statistical framework requires further study.
2 Discriminative Phrase Translation
In this section we describe the phrase-based SMT
baseline system and how DPT models are built and
integrated into this system in a ?soft? manner.
159
2.1 Baseline System
The baseline system is a phrase-based SMT sys-
tem (Koehn et al, 2003), built almost entirely us-
ing freely available components. We use the SRI
Language Modeling Toolkit (Stolcke, 2002) for lan-
guage modeling. We build trigram language models
applying linear interpolation and Kneser-Ney dis-
counting for smoothing. Translation models are
built on top of word-aligned parallel corpora linguis-
tically annotated at the level of shallow syntax (i.e.,
lemma, part-of-speech, and base phrase chunks)
as described by Gime?nez and Ma`rquez (2005).
Text is automatically annotated, using the SVM-
Tool (Gime?nez and Ma`rquez, 2004), Freeling (Car-
reras et al, 2004), and Phreco (Carreras et al, 2005)
packages. We used the GIZA++ SMT Toolkit1 (Och
and Ney, 2003) to generate word alignments. We
apply the phrase-extract algorithm, as described by
Och (2002), on the Viterbi alignments output by
GIZA++ following the ?global phrase extraction?
strategy described by Gime?nez and Ma`rquez (2005)
(i.e., a single phrase translation table is built on top
of the union of alignments corresponding to dif-
ferent linguistic data views). We work with the
union of source-to-target and target-to-source align-
ments, with no heuristic refinement. Phrases up to
length five are considered. Also, phrase pairs ap-
pearing only once are discarded, and phrase pairs
in which the source/target phrase is more than three
times longer than the target/source phrase are ig-
nored. Phrase pairs are scored on the basis of un-
smoothed relative frequency (i.e., MLE). Regard-
ing the argmax search, we used the Pharaoh beam
search decoder (Koehn, 2004), which naturally fits
with the previous tools.
2.2 DPT for SMT
Instead of relying on MLE estimation to score the
phrase pairs (fi, ej) in the translation table, we
suggest considering the translation of every source
phrase fi as a multi-class classification problem,
where every possible translation of fi is a class.
We use local linear SVMs 2. Since SVMs are bi-
nary classifiers, the problem must be binarized. We
1http://www.fjoch.com/GIZA++.html
2We use the SVMlight package, which is freely available at
http://svmlight.joachims.org (Joachims, 1999).
have applied a simple one-vs-all binarization, i.e., a
SVM is trained for every possible translation candi-
date ej . Training examples are extracted from the
same training data as in the case of MLE models,
i.e., an aligned parallel corpus, obtained as described
in Section 2.1. We use each sentence pair in which
the source phrase fi occurs to generate a positive ex-
ample for the classifier corresponding to the actual
translation of fi in that sentence, according to the
automatic alignment. This will be as well a negative
example for the classifiers corresponding to the rest
of possible translations of fi.
2.2.1 Feature Set
We consider different kinds of information, al-
ways from the source sentence, based on standard
WSD methods (Yarowsky et al, 2001). As to the
local context, inside the source phrase to disam-
biguate, and 5 tokens to the left and to the right,
we use n-grams (n ? {1, 2, 3}) of: words, parts-
of-speech, lemmas and base phrase chunking IOB
labels. As to the global context, we collect topical
information by considering the source sentence as a
bag of lemmas.
2.2.2 Decoding. A Trick.
At translation time, we consider every instance of
fi as a separate case. In each case, for all possi-
ble translations of fi, we collect the SVM score, ac-
cording to the SVM classification rule. We are in
fact modeling P (ej |fi). However, these scores are
not probabilities. We transform them into proba-
bilities by applying the softmax function described
by Bishop (1995). We do not constrain the decoder
to use the translation ej with highest probability. In-
stead, we make all predictions available and let the
decoder choose. We have avoided implementing a
new decoder by pre-computing all the SVM pre-
dictions for all possible translations for all source
phrases appearing in the test set. We input this in-
formation onto the decoder by replicating the entries
in the translation table. In other words, each distinct
occurrence of every single source phrase has a dis-
tinct list of phrase translation candidates with their
corresponding scores. Accordingly, the source sen-
tence is transformed into a sequence of identifiers,
160
in our case a sequence of (w, i) pairs3, which allow
us to uniquely identify every distinct instance of ev-
ery word in the test set during decoding, and to re-
trieve DPT predictions in the translation table. For
that purpose, source phrases in the translation table
must comply with the same format.
This imaginative trick4 saved us in the short run
a gigantic amount of work. However, it imposes a
severe limitation on the kind of features which the
DPT system may use. In particular, features from
the target sentence under construction and from
the correspondence between source and target (i.e.,
alignments) can not be used.
3 Phrase Translation
Analogously to the ?word translation? definition by
Vickrey et al (2005), rather than predicting the sense
of a word according to a given sense inventory, in
?phrase translation?, the goal is to predict the correct
translation of a phrase, for a given target language,
in the context of a sentence. This task is simpler than
the ?full translation? task, but provides an insight to
the gain prospectives.
We used the data from the Openlab 2006 Initia-
tive5 promoted by the TC-STAR Consortium6. This
test suite is entirely based on European Parliament
Proceedings. We have focused on the Spanish-to-
English task. The training set consists of 1,281,427
parallel sentences. Performing phrase extraction
over the training data, as described in Section 2.1,
we obtained translation candidates for 1,729,191
source phrases. We built classifiers for all the source
phrases with more than one possible translation and
more than 10 occurrences. 241,234 source phrases
fulfilled this requirement. For each source phrase,
we used 80% of the instances for training, 10% for
development, and 10% for test.
Table 1 shows ?phrase translation? results over
the test set. We compare the performance, in terms
of accuracy, of DPT models and the ?most fre-
quent translation? baseline (?MFT?). The MFT base-
3w is a word and i corresponds to the number of instances
of word w seen in the test set before the current instance.
4We have checked that results following this type of decod-
ing when translation tables are estimated on the basis of MLE
are identical to regular decoding results.
5http://tc-star.itc.it/openlab2006/
6http://www.tc-star.org/
phrase set model macro micro
all MFT 0.66 0.70
DPT 0.68 0.76
frequent MFT 0.76 0.75
DPT 0.86 0.86
Table 1: ?Phrase Translation? Accuracy (test set).
line is equivalent to selecting the translation candi-
date with highest probability according to MLE. The
?macro? column shows macro-averaged results over
all phrases, i.e., the accuracy for each phrase counts
equally towards the average. The ?micro? column
shows micro-averaged accuracy, where each test ex-
ample counts equally. The ?all? set includes results
for the 241,234 phrases, whereas the ?frequent? set
includes results for a selection of 41 very frequent
phrases ocurring more than 50,000 times.
A priori, DPT models seem to offer a significant
room for potential improvement. Although phrase
translation differs from WSD in a number of as-
pects, the increase with respect to the MFT baseline
is comparable. Results are also coherent with those
attained by Vickrey et al (2005).
-1
-0.5
 0
 0.5
 1
 0  50000  100000  150000  200000  250000  300000
a
cc
u
ra
cy
(D
PT
) -
 ac
cu
rac
y(M
LE
)
#examples
Figure 1: Analysis of ?Phrase Translation? Results
on the development set (Spanish-to-English).
Figure 1 shows the relationship between the accu-
racy7 gain and the number of training examples. In
general, with a sufficient number of examples (over
10,000), DPT outperforms the MFT baseline.
7We focus on micro-averaged accuracy.
161
4 Full Translation
In the ?phrase translation? task the predicted phrase
does not interact with the rest of the target sentence.
In this section we analyze the impact of DPT models
when the goal is to translate the whole sentence.
For evaluation purposes we count on a set of 1,008
sentences. Three human references per sentence are
available. We randomly split this set in two halves,
and use them for development and test, respectively.
4.1 Evaluation
Evaluating the effects of using DPT predictions, di-
rected towards a better word selection, in the full
translation task presents two serious difficulties.
In first place, the actual room for improvement
caused by a better translation modeling is smaller
than estimated in Section 3. This is mainly due to
the SMT architecture itself which relies on a search
over a probability space in which several models co-
operate. For instance, in many cases errors caused
by a poor translation modeling may be corrected by
the language model. In a recent study, Vilar et al
(2006) found that only around 25% of the errors are
related to word selection. In half of these cases er-
rors are caused by a wrong word sense disambigua-
tion, and in the other half the word sense is correct
but the lexical choice is wrong.
In second place, most conventional automatic
evaluation metrics have not been designed for this
purpose. For instance, metrics such as BLEU (Pa-
pineni et al, 2001) tend to favour longer n-gram
matchings, and are, thus, biased towards word or-
dering. We might find better suited metrics, such
as METEOR (Banerjee and Lavie, 2005), which is
oriented towards word selection8. However, a new
problem arises. Because different metrics are biased
towards different aspects of quality, scores conferred
by different metrics are often controversial.
In order to cope with evaluation difficulties we
have applied several complementary actions:
1. Based on the results from Section 3, we focus
on a reduced set of 41 very promising phrases
trained on more than 50,000 examples. This
set covers 25.8% of the words in the test set,
8METEOR works at the unigram level, may consider word
stemming and, for the case of English is also able to perform a
lookup for synonymy in WordNet (Fellbaum, 1998).
and exhibits a potential absolute accuracy gain
around 11% (See Table 1).
2. With the purpose of evaluating the changes re-
lated only to this small set of very promis-
ing phrases, we introduce a new measure, Apt,
which computes ?phrase translation? accuracy
for a given list of source phrases. For every
test case, Apt counts the proportion of phrases
from the list appearing in the source sentence
which have a valid9 translation both in the tar-
get sentence and in any of the reference trans-
lations. In fact, because in general source-to-
target algnments are not known, Apt calculates
an approximate10 solution.
3. We evaluate overall MT quality on the basis
of ?Human Likeness?. In particular, we use
the QUEEN11 meta-measure from the QARLA
Framework (Amigo? et al, 2005). QUEEN op-
erates under the assumption that a good trans-
lation must be similar to all human references
according to all metrics. Given a set of auto-
matic translations A, a set of similarity metrics
X, and a set of human references R, QUEEN is
defined as the probability, over R?R?R, that
for every metric in X the automatic translation
a is more similar to a reference r than two other
references r? and r?? to each other. Formally:
QUEENX,R(a) = Prob(?x ? X : x(a, r) ? x(r?, r??))
QUEEN captures the features that are common
to all human references, rewarding those auto-
matic translations which share them, and pe-
nalizing those which do not. Thus, QUEEN pro-
vides a robust means of combining several met-
rics into a single measure of quality. Following
the methodology described by Gime?nez and
Amigo? (2006), we compute the QUEEN mea-
sure over the metric combination with high-
est KING, i.e., discriminative power. We have
considered all the lexical metrics12 provided by
9Valid translations are provided by the translation table.
10Current Apt implementation searches phrases from left to
right in decreasing length order.
11QUEEN is available inside the IQMT package for MT
Evaluation based on ?Human Likeness? (Gime?nez and Amigo?,
2006). http://www.lsi.upc.edu/?nlp/IQMT
12Consult the IQMT Technical Manual v1.3 for a detailed de-
scription of the metric set. http://www.lsi.upc.edu/
?nlp/IQMT/IQMT.v1.3.pdf
162
QUEEN Apt BLEU METEOR ROUGE
P (e) + PMLE(f |e) 0.43 0.86 0.59 0.77 0.42
P (e) + PMLE(e|f) 0.45 0.87 0.62 0.77 0.43
P (e) + PDPT (e|f) 0.47 0.89 0.62 0.78 0.44
Table 2: Automatic evaluation of the ?full translation? results on the test set.
IQMT. The optimal set is:
{ METEORwnsyn, ROUGEw 1.2 }
which includes variants of METEOR, and
ROUGE (Lin and Och, 2004).
4.2 Adjustment of Parameters
Models are combined in a log-linear fashion:
logP (e|f) ? ?lmlogP (e) + ?glogPMLE(f |e)
+ ?dlogPMLE(e|f) + ?DPT logPDPT (e|f)
P (e) is the language model probability.
PMLE(f |e) corresponds to the MLE-based
generative translation model, whereas PMLE(e|f)
corresponds to the analogous discriminative model.
PDPT (e|f) corresponds to the DPT model which
uses SVM-based predictions in a wider feature
context. In order to perform fair comparisons,
model weights must be adjusted.
Because we have focused on a reduced set of fre-
quent phrases, in order to translate the whole test set
we must provide alternative translation probabilities
for all the source phrases in the vocabulary which
do not have a DPT prediction. We have used MLE
predictions to complete the model. However, inter-
action between DPT and MLE models is problem-
atic. Problems arise when, for a given source phrase,
fi, DPT predictions must compete with MLE pre-
dictions for larger phrases fj overlapping with or
containing fi (See Section 4.3). We have alleviated
these problems by splitting DPT tables in 3 subta-
bles: (1) phrases with DPT prediction, (2) phrases
with DPT prediction only for subphrases of it, and
(3) phrases with no DPT prediction for any sub-
phrase; and separately adjusting their weights.
Counting on a reliable automatic measure of qual-
ity is a crucial issue for system development. Opti-
mal configurations may vary very significantly de-
pending on the metric governing the optimization
process. We optimize the system parameters over
the QUEEN measure, which has proved to lead to
more robust system configurations than BLEU (Lam-
bert et al, 2006). We exhaustively try all possible
parameter configurations, at a resolution of 0.1, over
the development set and select the best one. In order
to keep the optimization process feasible, in terms of
time, the search space is pruned13 during decoding.
4.3 Results
We compare the systems using the generative and
discriminative MLE-based translation models to the
discriminative translation model which uses DPT
predictions for the set of 41 very ?frequent? source
phrases. Table 2 shows automatic evaluation re-
sults on the test set, according to several metrics.
Phrase translation accuracy (over the ?frequent? set
of phrases) and MT quality are evaluated by means
of the Apt and QUEEN measures, respectively. For
the sake of informativeness, BLEU, METEORwnsyn
and ROUGEw 1.2 scores are provided as well.
Interestingly, discriminative models outperform
the (noisy-channel) default generative model. Im-
provement in Apt measure also reveals that DPT pre-
dictions provide a better translation for the set of
?frequent? phrases than the MLE models. This im-
provement remains when measuring overall transla-
tion quality via QUEEN. If we take into account that
DPT predictions are available for only 25% of the
words in the test set, we can say that the gain re-
ported by the QUEEN and Apt measures is consistent
with the accuracy prospectives predicted in Table 1.
METEORwnsyn and ROUGEw 1.2 reflect a slight im-
provement as well. However, according to BLEU
there is no difference between both systems. We
suspect that BLEU is unable to accurately reflect the
possible gains attained by a better ?phrase selection?
over a small set of phrases because of its tendency
13For each phrase only the 30 top-scoring translations are
used. At all times, only the 100 top-scoring solutions are kept.
We also disabled distortion and word penalty models. There-
fore, translations are monotonic, and source and target tend to
have the same number of words (that is not mandatory).
163
to reward long n-gram matchings. In order to clar-
ify this scenario a rigorous process of manual evalu-
ation has been conducted. We have selected a subset
of sentences based on the following criteria:
? sentence length between 10 and 30 words.
? at least 5 words have a DPT prediction.
? DPT and MLE outputs differ.
A total of 114 sentences fulfill these require-
ments. In each translation case, assessors must judge
whether the output by the discriminative ?MLE? sys-
tem is better, equal to or worse than the output by
the ?DPT? system, with respect to adequacy, fluency,
and overall quality. In order to avoid any bias in the
evaluation, we have randomized the respective posi-
tion in the display of the sentences corresponding to
each system. Four judges participated in the evalua-
tion. Each judge evaluated only half of the cases.
Each case was evaluated by two different judges.
Therefore, we count on 228 human assessments.
Table 3 shows the results of the manual system
comparison. Statistical significance has been deter-
mined using the sign-test (Siegel, 1956). According
to human assessors, the ?DPT? system outperforms
the ?MLE? system very significantly with respect to
adequacy, whereas for fluency there is a slight ad-
vantage in favor of the ?MLE? system. Overall, there
is a slight but significant advantage in favor of the
?DPT? system. Manual evaluation confirms our sus-
picion that the BLEU metric is less sensitive than
QUEEN to improvements related to adequacy.
Error Analysis
Guided by the QUEEN measure, we carefully inspect
particular cases. We start, in Table 4, by show-
ing a positive case. The three phrases highlighted
in the source sentence (?tiene?, ?sen?ora? and ?una
cuestio?n?) find a better translation with the help of
the DPT models: ?tiene? translates into ?has? instead
of ?i give?, ?sen?ora? into ?mrs? instead of ?lady?, and
?una cuestio?n? into ?a point? instead of ?a ... motion?.
In contrast, Table 5 shows a negative case. The
translation of the Spanish word ?sen?ora? as ?mrs? is
acceptable. However, it influences very negatively
the translation of the following word ?diputada?,
whereas the ?MLE? system translates the phrase
?sen?ora diputada?, which does not have a DPT pre-
diction, as a whole. Similarly, the translation of
Adequacy Fluency Overall
MLE > DPT 39 84 83
MLE = DPT 100 76 46
MLE < DPT 89 68 99
Table 3: Manual evaluation of the ?full translation?
results on the test set. Counts on the number of
translation cases for which the ?MLE? system is bet-
ter than (>), equal to (=), or worse than (<) the
?DPT? system, with respect to adequacy, fluency,
and overall MT quality, are presented.
?cuestio?n? as ?matter?, although acceptable, is break-
ing the phrase ?cuestio?n de orden? of high cohe-
sion, which is commonly translated as ?point of or-
der?. The cause underlying these problems is that
DPT predictions are available only for a subset of
phrases. Thus, during decoding, for these cases our
DPT models may be in disadvantage.
5 Related Work
Recently, there is a growing interest in the appli-
cation of WSD technology to MT. For instance,
Carpuat and Wu (2005b) suggested integrating
WSD predictions into a SMT system in a ?hard?
manner, either for decoding, by constraining the set
of acceptable translation candidates for each given
source word, or for post-processing the SMT sys-
tem output, by directly replacing the translation of
each selected word with the WSD system predic-
tion. They did not manage to improve MT quality.
They encountered several problems inherent to the
SMT architecture. In particular, they described what
they called the ?language model effect? in SMT:
?The lexical choices are made in a way that heav-
ily prefers phrasal cohesion in the output target sen-
tence, as scored by the language model.?. This prob-
lem is a direct consequence of the ?hard? interaction
between their WSD and SMT systems. WSD pre-
dictions cannot adapt to the surrounding target con-
text. In a later work, Carpuat and Wu (2005a) ana-
lyzed the converse question, i.e. they measured the
WSD performance of SMT models. They showed
that dedicated WSD models significantly outper-
form current state-of-the-art SMT models. Conse-
quently, SMT should benefit from WSD predictions.
Simultaneously, Vickrey et al (2005) studied the
164
Source tiene la palabra la sen?ora mussolini para una cuestio?n de orden .
Ref 1 mrs mussolini has the floor for a point of order .
Ref 2 you have the floor , missus mussolini , for a question of order .
Ref 3 ms mussolini has now the floor for a point of order .
P (e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion .
P (e) + PDPT (e|f) has the floor the mrs mussolini on a point of order .
Table 4: Case of Analysis of sentence #422. DPT models help.
Source sen?ora diputada , e?sta no es una cuestio?n de orden .
Ref 1 mrs mussolini , that is not a point of order .
Ref 2 honourable member , this is not a question of order .
Ref 3 my honourable friend , this is not a point of order .
P (e) + PMLE(e|f) honourable member , this is not a point of order .
P (e) + PDPT (e|f) mrs karamanou , this is not a matter of order .
Table 5: Case of Analysis of sentence #434. DPT models fail.
application of discriminative models based on WSD
technology to the ?blank-filling? task, a simplified
version of the translation task, in which the target
context surrounding the word translation is avail-
able. They did not encounter the ?language model
effect? because they approached the task in a ?soft?
way, i.e., allowing their WSD models to interact
with other models during decoding. Similarly, our
DPT models are, as described in Section 2.2, softly
integrated in the decoding step, and thus do not suf-
fer from the detrimental ?language model effect? ei-
ther, in the context of the ?full translation? task. Be-
sides, DPT models enforce phrasal cohesion by con-
sidering disambiguation at the level of phrases.
6 Conclusions and Further Work
Despite the fact that measuring improvements in
word selection is a very delicate issue, we have
showed that dedicated discriminative translation
models considering a wider feature context provide
a useful mechanism in order to improve the qual-
ity of current phrase-based SMT systems, specially
with regard to adequacy. However, the fact that no
gain in fluency is reported indicates that the integra-
tion of these probabilities into the statistical frame-
work requires further study.
Moreover, there are several open issues. First, for
practical reasons, we have limited to a reduced set of
?frequent? phrases, and we have disabled reordering
and word penalty models. We are currently studying
the impact of a larger set of phrases, covering over
99% of the words in the test set. Experiments with
enabled reordering and word penalty models should
be conducted as well. Second, automatic evalua-
tion of the results revealed a low agreement between
BLEU and other metrics. For system comparison, we
solved this through a process of manual evaluation.
However, this is impractical for the adjustment of
parameters, where hundreds of different configura-
tions are tried. In this work we have relied on auto-
matic evaluation based on ?Human Likeness? which
allows for metric combinations and provides a sta-
ble and robust criterion for the metric set selection.
Other alternatives could be tried. The crucial issue,
in our opinion, is that the metric guiding the opti-
mization is able to capture the changes.
Finally, we argue that, if DPT models considered
features from the target side, and from the corre-
spondence between source and target, results could
further improve. However, at the short term, the in-
corporation of these type of features will force us to
either build a new decoder or extend an existing one,
or to move to a new MT architecture, for instance,
in the fashion of the architectures suggested by Till-
mann and Zhang (2006) or Liang et al (2006).
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
165
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
TC-STAR Consortium for providing such very valu-
able data sets.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Christopher M. Bishop. 1995. 6.4: Modeling conditional
distributions. In Neural Networks for Pattern Recog-
nition, page 215. Oxford University Press.
Marine Carpuat and Dekai Wu. 2005a. Evaluating the
Word Sense Disambiguation Performance of Statisti-
cal Machine Translation. In Proceedings of IJCNLP.
Marine Carpuat and Dekai Wu. 2005b. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings of ACL.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In Pro-
ceedings of the Workshop on Building and Using Par-
allel Texts, ACL.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. The MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings of AMTA.
Patrik Lambert, Jesu?s Gime?nez, Marta R. Costa-jussa?,
Enrique Amigo?, Rafael E. Banchs, Llu??s Ma?rquez, and
J.A. R. Fonollosa. 2006. Machine Translation Sys-
tem Development based on Human Likeness. In Pro-
ceedings of IEEE/ACL 2006 Workshop on Spoken Lan-
guage Technology.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, , and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
COLING-ACL06.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation, rc22176. Technical report,
IBM T.J. Watson Research Center.
Sidney Siegel. 1956. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of ICSLP.
Christoph Tillmann and Tong Zhang. 2006. A Discrim-
inative Global Training Algorithm for Statistical MT.
In Proceedings of COLING-ACL06.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-Sense Disambiguation for Machine Translation.
In Proceedings of HLT/EMNLP.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proceedings of the 5th LREC.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles
Schafer, and Richard Wicentowski. 2001. The Johns
Hopkins Senseval2 System Descriptions. In Proceed-
ings of Senseval-2: Second International Workshop on
Evaluating Word Sense Disambiguation Systems.
166
Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264,
Prague, June 2007. c?2007 Association for Computational Linguistics
Linguistic Features for Automatic Evaluation of Heterogenous MT Systems
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
Evaluation results recently reported by
Callison-Burch et al (2006) and Koehn and
Monz (2006), revealed that, in certain cases,
the BLEU metric may not be a reliable MT
quality indicator. This happens, for in-
stance, when the systems under evaluation
are based on different paradigms, and there-
fore, do not share the same lexicon. The
reason is that, while MT quality aspects are
diverse, BLEU limits its scope to the lex-
ical dimension. In this work, we suggest
using metrics which take into account lin-
guistic features at more abstract levels. We
provide experimental results showing that
metrics based on deeper linguistic informa-
tion (syntactic/shallow-semantic) are able to
produce more reliable system rankings than
metrics based on lexical matching alone,
specially when the systems under evaluation
are of a different nature.
1 Introduction
Most metrics used in the context of Automatic Ma-
chine Translation (MT) Evaluation are based on
the assumption that ?acceptable? translations tend to
share the lexicon (i.e., word forms) in a predefined
set of manual reference translations. This assump-
tion works well in many cases. However, several
results in recent MT evaluation campaigns have cast
some doubts on its general validity. For instance,
Callison-Burch et al (2006) and Koehn and Monz
(2006) reported and analyzed several cases of strong
disagreement between system rankings provided by
human assessors and those produced by the BLEU
metric (Papineni et al, 2001). In particular, they
noted that when the systems under evaluation are
of a different nature (e.g., rule-based vs. statistical,
human-aided vs. fully automatical, etc.) BLEU may
not be a reliable MT quality indicator. The reason is
that BLEU favours MT systems which share the ex-
pected reference lexicon (e.g., statistical systems),
and penalizes those which use a different one.
Indeed, the underlying cause is much simpler. In
general, lexical similarity is nor a sufficient neither
a necessary condition so that two sentences convey
the same meaning. On the contrary, natural lan-
guages are expressive and ambiguous at different
levels. Consequently, the similarity between two
sentences may involve different dimensions. In this
work, we hypothesize that, in order to ?fairly? evalu-
ate MT systems based on different paradigms, simi-
larities at more abstract linguistic levels must be an-
alyzed. For that purpose, we have compiled a rich
set of metrics operating at the lexical, syntactic and
shallow-semantic levels (see Section 2). We present
a comparative study on the behavior of several met-
ric representatives from each linguistic level in the
context of some of the cases reported by Koehn and
Monz (2006) and Callison-Burch et al (2006) (see
Section 3). We show that metrics based on deeper
linguistic information (syntactic/shallow-semantic)
are able to produce more reliable system rankings
than those produced by metrics which limit their
scope to the lexical dimension, specially when the
systems under evaluation are of a different nature.
256
2 A Heterogeneous Metric Set
For our experiments, we have compiled a represen-
tative set of metrics1 at different linguistic levels.
We have resorted to several existing metrics, and
we have also developed new ones. Below, we group
them according to the level at which they operate.
2.1 Lexical Similarity
Most of the current metrics operate at the lexical
level. We have selected 7 representatives from dif-
ferent families which have been shown to obtain
high levels of correlation with human assessments:
BLEU We use the default accumulated score up to
the level of 4-grams (Papineni et al, 2001).
NIST We use the default accumulated score up to
the level of 5-grams (Doddington, 2002).
GTM We set to 1 the value of the e parame-
ter (Melamed et al, 2003).
METEOR We run all modules: ?exact?, ?porter-
stem?, ?wn stem? and ?wn synonymy?, in that
order (Banerjee and Lavie, 2005).
ROUGE We used the ROUGE-S* variant (skip bi-
grams with no max-gap-length). Stemming is
enabled (Lin and Och, 2004a).
mWER We use 1 ? mWER (Nie?en et al, 2000).
mPER We use 1 ? mPER (Tillmann et al, 1997).
Let us note that ROUGE and METEOR may con-
sider stemming (i.e., morphological variations). Ad-
ditionally, METEOR may perform a lookup for syn-
onyms in WordNet (Fellbaum, 1998).
2.2 Beyond Lexical Similarity
Modeling linguistic features at levels further than
the lexical level requires the usage of more complex
linguistic structures. We have defined what we call
?linguistic elements? (LEs).
2.2.1 Linguistic Elements
LEs are linguistic units, structures, or relation-
ships, such that a sentence may be partially seen as a
?bag? of LEs. Possible kinds of LEs are: word forms,
parts-of-speech, dependency relationships, syntactic
phrases, named entities, semantic roles, etc. Each
1All metrics used in this work are publicly available inside
the IQMT Framework (Gime?nez and Amigo?, 2006). http://
www.lsi.upc.edu/?nlp/IQMT
LE may consist, in its turn, of one or more LEs,
which we call ?items? inside the LE. For instance, a
?phrase? LE may consist of ?phrase? items, ?part-of-
speech? (PoS) items, ?word form? items, etc. Items
may be also combinations of LEs. For instance, a
?phrase? LE may be seen as a sequence of ?word-
form:PoS? items.
2.2.2 Similarity Measures
We are interested in comparing linguistic struc-
tures, and linguistic units. LEs allow for compar-
isons at different granularity levels, and from dif-
ferent viewpoints. For instance, we might compare
the semantic structure of two sentences (i.e., which
actions, semantic arguments and adjuncts exist) or
we might compare lexical units according to the se-
mantic role they play inside the sentence. For that
purpose, we use two very simple kinds of similarity
measures over LEs: ?Overlapping? and ?Matching?.
We provide a general definition:
Overlapping between items inside LEs, according
to their type. Formally:
Overlapping(t) =
X
i?itemst(hyp)
count?hyp(i, t)
X
i?itemst(ref)
countref (i, t)
where t is the LE type2, itemst(s) refers to the
set of items occurring inside LEs of type t in
sentence s, countref(i, t) denotes the number
of times item i appears in the reference trans-
lation inside a LE of type t, and count?hyp(i, t)
denotes the number of times i appears in the
candidate translation inside a LE of type t, lim-
ited by the number of times i appears in the ref-
erence translation inside a LE of type t. Thus,
?Overlapping? provides a rough measure of the
proportion of items inside elements of a cer-
tain type which have been ?successfully? trans-
lated. We also introduce a coarser metric, ?Over-
lapping(*)?, which considers the uniformly aver-
aged ?overlapping? over all types:
Overlapping(?) = 1|T |
X
t?T
Overlapping(t)
where T is the set of types.
2LE types vary according to the specific LE class. For in-
stance, in the case of Named Entities types may be ?PER? (i.e.,
person), ?LOC? (i.e., location), ?ORG? (i.e., organization), etc.
257
Matching between items inside LEs, according to
their type. Its definition is analogous to the
?Overlapping? definition, but in this case the
relative order of the items is important. All
items inside the same element are considered as
a single unit (i.e., a sequence in left-to-right or-
der). In other words, we are computing the pro-
portion of ?fully? translated elements, accord-
ing to their type. We also introduce a coarser
metric, ?Matching(*)?, which considers the uni-
formly averaged ?Matching? over all types.
notes:
? ?Overlapping? and ?Matching? operate on the
assumption of a single reference translation.
The extension to the multi-reference setting is
computed by assigning the maximum value at-
tained over all human references individually.
? ?Overlapping? and ?Matching? are general met-
rics. We may apply them to specific scenarios
by defining the class of linguistic elements and
items to be used. Below, we instantiate these
measures over several particular cases.
2.3 Shallow Syntactic Similarity
Metrics based on shallow parsing (?SP?) analyze
similarities at the level of PoS-tagging, lemmati-
zation, and base phrase chunking. Outputs and
references are automatically annotated using state-
of-the-art tools. PoS-tagging and lemmatization
are provided by the SVMTool package (Gime?nez and
Ma`rquez, 2004), and base phrase chunking is pro-
vided by the Phreco software (Carreras et al, 2005).
Tag sets for English are derived from the Penn Tree-
bank (Marcus et al, 1993).
We instantiate ?Overlapping? over parts-of-speech
and chunk types. The goal is to capture the propor-
tion of lexical items correctly translated, according
to their shallow syntactic realization:
SP-Op-t Lexical overlapping according to the part-
of-speech ?t?. For instance, ?SP-Op-NN? roughly
reflects the proportion of correctly translated
singular nouns. We also introduce a coarser
metric, ?SP-Op-*? which computes average
overlapping over all parts-of-speech.
SP-Oc-t Lexical overlapping according to the
chunk type ?t?. For instance, ?SP-Oc-NP? roughly
reflects the successfully translated proportion
of noun phrases. We also introduce a coarser
metric, ?SP-Oc-*? which considers the average
overlapping over all chunk types.
At a more abstract level, we use the NIST
metric (Doddington, 2002) to compute accumu-
lated/individual scores over sequences of:
Lemmas ? SP-NIST(i)l-n
Parts-of-speech ? SP-NIST(i)p-n
Base phrase chunks ? SP-NIST(i)c-n
For instance, ?SP-NISTl-5? corresponds to the accu-
mulated NIST score for lemma n-grams up to length
5, whereas ?SP-NISTip-5? corresponds to the individ-
ual NIST score for PoS 5-grams.
2.4 Syntactic Similarity
We have incorporated, with minor modifications,
some of the syntactic metrics described by Liu and
Gildea (2005) and Amigo? et al (2006) based on de-
pendency and constituency parsing.
2.4.1 On Dependency Parsing (DP)
?DP? metrics capture similarities between depen-
dency trees associated to automatic and reference
translations. Dependency trees are provided by the
MINIPAR dependency parser (Lin, 1998). Similari-
ties are captured from different viewpoints:
DP-HWC(i)-l This metric corresponds to the HWC
metric presented by Liu and Gildea (2005). All
head-word chains are retrieved. The fraction of
matching head-word chains of a given length,
?l?, is computed. We have slightly modified
this metric in order to distinguish three differ-
ent variants according to the type of items head-
word chains may consist of:
Lexical forms ? DP-HWC(i)w -l
Grammatical categories ? DP-HWC(i)c-l
Grammatical relationships ? DP-HWC(i)r-l
Average accumulated scores up to a given chain
length may be used as well. For instance,
?DP-HWCiw-4? retrieves the proportion of match-
ing length-4 word-chains, whereas ?DP-HWCw -
4? retrieves average accumulated proportion of
matching word-chains up to length-4. Anal-
ogously, ?DP-HWCc-4?, and ?DP-HWCr -4? com-
258
pute average accumulated proportion of cate-
gory/relationship chains up to length-4.
DP-Ol|Oc|Or These metrics correspond exactly to
the LEVEL, GRAM and TREE metrics intro-
duced by Amigo? et al (2006).
DP-Ol-l Overlapping between words hanging
at level ?l?, or deeper.
DP-Oc-t Overlapping between words directly
hanging from terminal nodes (i.e. gram-
matical categories) of type ?t?.
DP-Or-t Overlapping between words ruled
by non-terminal nodes (i.e. grammatical
relationships) of type ?t?.
Node types are determined by grammatical cat-
egories and relationships defined by MINIPAR.
For instance, ?DP-Or-s? reflects lexical overlap-
ping between subtrees of type ?s? (subject). ?DP-
Oc-A? reflects lexical overlapping between ter-
minal nodes of type ?A? (Adjective/Adverbs).
?DP-Ol-4? reflects lexical overlapping between
nodes hanging at level 4 or deeper. Addition-
ally, we consider three coarser metrics (?DP-Ol-
*?, ?DP-Oc-*? and ?DP-Or -*?) which correspond
to the uniformly averaged values over all lev-
els, categories, and relationships, respectively.
2.4.2 On Constituency Parsing (CP)
?CP? metrics capture similarities between con-
stituency parse trees associated to automatic and
reference translations. Constituency trees are pro-
vided by the Charniak-Johnson?s Max-Ent reranking
parser (Charniak and Johnson, 2005).
CP-STM(i)-l This metric corresponds to the STM
metric presented by Liu and Gildea (2005).
All syntactic subpaths in the candidate and the
reference trees are retrieved. The fraction of
matching subpaths of a given length, ?l?, is
computed. For instance, ?CP-STMi-5? retrieves
the proportion of length-5 matching subpaths.
Average accumulated scores may be computed
as well. For instance, ?CP-STM-9? retrieves av-
erage accumulated proportion of matching sub-
paths up to length-9.
2.5 Shallow-Semantic Similarity
We have designed two new families of metrics, ?NE?
and ?SR?, which are intended to capture similari-
ties over Named Entities (NEs) and Semantic Roles
(SRs), respectively.
2.5.1 On Named Entities (NE)
?NE? metrics analyze similarities between auto-
matic and reference translations by comparing the
NEs which occur in them. Sentences are automati-
cally annotated using the BIOS package (Surdeanu
et al, 2005). BIOS requires at the input shallow
parsed text, which is obtained as described in Sec-
tion 2.3. See the list of NE types in Table 1.
Type Description
ORG Organization
PER Person
LOC Location
MISC Miscellaneous
O Not-a-NE
DATE Temporal expressions
NUM Numerical expressions
ANGLE QUANTITY
DISTANCE QUANTITY
SIZE QUANTITY Quantities
SPEED QUANTITY
TEMPERATURE QUANTITY
WEIGHT QUANTITY
METHOD
MONEY
LANGUAGE Other
PERCENT
PROJECT
SYSTEM
Table 1: Named Entity types.
We define two types of metrics:
NE-Oe-t Lexical overlapping between NEs accord-
ing to their type t. For instance, ?NE-Oe-PER? re-
flects lexical overlapping between NEs of type
?PER? (i.e., person), which provides a rough es-
timate of the successfully translated proportion
of person names. The ?NE-Oe-*? metric consid-
ers the average lexical overlapping over all NE
types. This metric includes the NE type ?O?
(i.e., Not-a-NE). We introduce another variant,
?NE-Oe-**?, which considers only actual NEs.
NE-Me-t Lexical matching between NEs accord-
ing to their type t. For instance, ?NE-Me-LOC?
reflects the proportion of fully translated NEs
of type ?LOC? (i.e., location). The ?NE-Me-*?
259
metric considers the average lexical matching
over all NE types, this time excluding type ?O?.
Other authors have measured MT quality over
NEs in the recent literature. In particular, the ?NE-
Me-*? metric is similar to the ?NEE? metric defined
by Reeder et al (2001).
2.5.2 On Semantic Roles (SR)
?SR? metrics analyze similarities between auto-
matic and reference translations by comparing the
SRs (i.e., arguments and adjuncts) which occur in
them. Sentences are automatically annotated using
the SwiRL package (Ma`rquez et al, 2005). This
package requires at the input shallow parsed text en-
riched with NEs, which is obtained as described in
Section 2.5.1. See the list of SR types in Table 2.
Type Description
A0
A1
A2 arguments associated with a verb predicate,
A3 defined in the PropBank Frames scheme.
A4
A5
AA Causative agent
AM-ADV Adverbial (general-purpose) adjunct
AM-CAU Causal adjunct
AM-DIR Directional adjunct
AM-DIS Discourse marker
AM-EXT Extent adjunct
AM-LOC Locative adjunct
AM-MNR Manner adjunct
AM-MOD Modal adjunct
AM-NEG Negation marker
AM-PNC Purpose and reason adjunct
AM-PRD Predication adjunct
AM-REC Reciprocal adjunct
AM-TMP Temporal adjunct
Table 2: Semantic Roles.
We define three types of metrics:
SR-Or-t Lexical overlapping between SRs accord-
ing to their type t. For instance, ?SR-Or-A0? re-
flects lexical overlapping between ?A0? argu-
ments. ?SR-Or -*? considers the average lexical
overlapping over all SR types.
SR-Mr-t Lexical matching between SRs accord-
ing to their type t. For instance, the met-
ric ?SR-Mr-AM-MOD? reflects the proportion of
fully translated modal adjuncts. The ?SR-Mr -*?
metric considers the average lexical matching
over all SR types.
SR-Or This metric reflects ?role overlapping?, i.e..
overlapping between semantic roles indepen-
dently from their lexical realization.
Note that in the same sentence several verbs, with
their respective SRs, may co-occur. However, the
metrics described above do not distinguish between
SRs associated to different verbs. In order to account
for such a distinction we introduce a more restric-
tive version of these metrics (?SR-Mrv-t?, ?SR-Orv-t?,
?SR-Mrv -*?, ?SR-Orv -*?, and ?SR-Orv ?), which require
SRs to be associated to the same verb.
3 Experimental Work
In this section, we study the behavior of some
of the metrics described in Section 2, according
to the linguistic level at which they operate. We
have selected a set of coarse-grained metric vari-
ants (i.e., accumulated/average scores over linguis-
tic units and structures of different kinds)3. We ana-
lyze some of the cases reported by Koehn and Monz
(2006) and Callison-Burch et al (2006). We distin-
guish different evaluation contexts. In Section 3.1,
we study the case of a single reference translation
being available. In principle, this scenario should
diminish the reliability of metrics based on lexical
matching alone, and favour metrics based on deeper
linguistic features. In Section 3.2, we study the case
of several reference translations available. This sce-
nario should alleviate the deficiencies caused by the
shallowness of metrics based on lexical matching.
We also analyze separately the case of ?homoge-
neous? systems (i.e., all systems being of the same
nature), and the case of ?heterogenous? systems (i.e.,
there exist systems based on different paradigms).
As to the metric meta-evaluation criterion, the two
most prominent criteria are:
Human Acceptability Metrics are evaluated on the
basis of correlation with human evaluators.
Human Likeness Metrics are evaluated in terms of
descriptive power, i.e., their ability to distin-
guish between human and automatic transla-
tions (Lin and Och, 2004b; Amigo? et al, 2005).
In our case, metrics are evaluated on the basis of
?Human Acceptability?. Specifically, we use Pear-
son correlation coefficients between metric scores
3When computing ?lexical? overlapping/matching, we use
lemmas instead of word forms.
260
and the average sum of adequacy and fluency as-
sessments at the document level. The reason is
that meta-evaluation based on ?Human Likeness? re-
quires the availability of heterogenous test beds (i.e.,
representative sets of automatic outputs and human
references), which, unfortunately, is not the case of
all the tasks under study. First, because most transla-
tion systems are statistical. Second, because in most
cases only one reference translation is available.
3.1 Single-reference Scenario
We use some of the test beds corresponding to
the ?NAACL 2006 Workshop on Statistical Machine
Translation? (WMT 2006) (Koehn and Monz, 2006).
Since linguistic features described in Section 2 are
so far implemented only for the case of English be-
ing the target language, among the 12 translation
tasks available, we studied only the 6 tasks corre-
sponding to the Foreign-to-English direction. A sin-
gle reference translation is available. System out-
puts consist of 2000 and 1064 sentences for the ?in-
domain? and ?out-of-domain? test beds, respectively.
In each case, human assessments on adequacy and
fluency are available for a subset of systems and sen-
tences. Table 3 shows the number of sentences as-
sessed in each case. Each sentence was evaluated
by two different human judges. System scores have
been obtained by averaging over all sentence scores.
in out sys
French-to-English 2,247 1,274 11/14
German-to-English 2,401 1,535 10/12
Spanish-to-English 1,944 1,070 11/15
Table 3: WMT 2006. ?in? and ?out? columns
show the number of sentences assessed for the ?in-
domain? and ?out-of-domain? subtasks. The ?sys?
column shows the number of systems counting on
human assessments with respect to the total number
of systems which presented to each task.
Evaluation of Heterogeneous Systems
In four of the six translation tasks under study, all
the systems are statistical except ?Systran?, which is
rule-based. This is the case of the German/French-
to-English in-domain/out-of-domain tasks. Table 4
shows correlation with human assessments for some
metric representatives at different linguistic levels.
fr2en de2en
Level Metric in out in out
1-PER 0.73 0.64 0.57 0.46
1-WER 0.73 0.73 0.32 0.38
BLEU 0.71 0.87 0.60 0.67
Lexical NIST 0.74 0.82 0.56 0.63
GTM 0.84 0.86 0.12 0.70
METEOR 0.92 0.95 0.76 0.81
ROUGE 0.85 0.89 0.65 0.79
SP-Op-* 0.81 0.88 0.64 0.71
SP-Oc-* 0.81 0.89 0.65 0.75
Shallow SP-NISTl-5 0.75 0.81 0.56 0.64
Syntactic SP-NISTp-5 0.75 0.91 0.77 0.77
SP-NISTc-5 0.73 0.88 0.71 0.54
DP-HWCw-4 0.76 0.88 0.64 0.74
DP-HWCc-4 0.93 0.97 0.88 0.72
DP-HWCr-4 0.92 0.96 0.91 0.76
Syntactic DP-Ol-* 0.87 0.94 0.84 0.84
DP-Oc-* 0.91 0.95 0.88 0.87
DP-Or-* 0.87 0.97 0.91 0.88
CP-STM-9 0.93 0.95 0.93 0.87
NE-Me-* 0.80 0.79 0.93 0.63
NE-Oe-* 0.79 0.76 0.91 0.59
NE-Oe-** 0.81 0.87 0.63 0.70
SR-Mr-* 0.83 0.95 0.92 0.84
Shallow SR-Or-* 0.89 0.95 0.88 0.90
Semantic SR-Or 0.95 0.85 0.80 0.75
SR-Mrv-* 0.77 0.92 0.72 0.85
SR-Orv-* 0.81 0.93 0.76 0.94
SR-Orv 0.84 0.93 0.81 0.92
Table 4: WMT 2006. Evaluation of Heterogeneous
Systems. French-to-English (fr2en) / German-to-
English (de2en), in-domain and out-of-domain.
Although the four cases are different, we have
identified several regularities. For instance, BLEU
and, in general, all metrics based on lexical match-
ing alone, except METEOR, obtain significantly
lower levels of correlation than metrics based on
deeper linguistic similarities. The problem with lex-
ical metrics is that they are unable to capture the ac-
tual quality of the ?Systran? system. Interestingly,
METEOR obtains a higher correlation, which, in
the case of French-to-English, rivals the top-scoring
metrics based on deeper linguistic features. The rea-
son, however, does not seem to be related to its ad-
ditional linguistic operations (i.e., stemming or syn-
onymy lookup), but rather to the METEOR matching
strategy itself (unigram precision/recall).
Metrics at the shallow syntactic level are in the
same range of lexical metrics. At the properly
syntactic level, metrics obtain in most cases high
correlation coefficients. However, the ?DP-HWCw-4?
metric, which, although from the viewpoint of de-
261
pendency relationships, still considers only lexical
matching, obtains a lower level of correlation. This
reinforces the idea that metrics based on rewarding
long n-grams matchings may not be a reliable qual-
ity indicator in these cases.
At the level of shallow semantics, while ?NE?
metrics are not equally useful in all cases, ?SR? met-
rics prove very effective. For instance, correlation
attained by ?SR-Or-*? reveals that it is important to
translate lexical items according to the semantic role
they play inside the sentence. Moreover, correlation
attained by the ?SR-Mr-*? metric is a clear indication
that in order to achieve a high quality, it is impor-
tant to ?fully? translate ?whole? semantic structures
(i.e., arguments/adjuncts). The existence of all the
semantic structures (?SR-Or?), specially associated to
the same verb (?SR-Orv?), is also important.
Evaluation of Homogeneous Systems
In the two remaining tasks, Spanish-to-English
in-domain/out-of-domain, all the systems are sta-
tistical. Table 5 shows correlation with human as-
sessments for some metric representatives. In this
case, BLEU proves very effective, both in-domain
and out-of-domain. Indeed, all metrics based on lex-
ical matching obtain high levels of correlation with
human assessments. However, still metrics based on
deeper linguistic analysis attain in most cases higher
correlation coefficients, although not as significantly
higher as in the case of heterogeneous systems.
3.2 Multiple-reference Scenario
We study the case reported by Callison-Burch et
al. (2006) in the context of the Arabic-to-English
exercise of the ?2005 NIST MT Evaluation Cam-
paign?4 (Le and Przybocki, 2005). In this case all
systems are statistical but ?LinearB?, a human-aided
MT system (Callison-Burch, 2005). Five reference
translations are available. System outputs consist of
1056 sentences. We obtained permission5 to use 7
system outputs. For six of these systems we counted
4http://www.nist.gov/speech/tests/
summaries/2005/mt05.htm
5Due to data confidentiality, we contacted each participant
individually and asked for permission to use their data. A num-
ber of groups and companies responded positively: Univer-
sity of Southern California Information Sciences Institute (ISI),
University of Maryland (UMD), Johns Hopkins University &
University of Cambridge (JHU-CU), IBM, University of Edin-
burgh, MITRE and LinearB.
es2en
Level Metric in out
1-PER 0.82 0.78
1-WER 0.88 0.83
BLEU 0.89 0.87
Lexical NIST 0.88 0.84
GTM 0.86 0.80
METEOR 0.84 0.81
ROUGE 0.89 0.83
SP-Op-* 0.88 0.80
SP-Oc-* 0.89 0.84
Shallow SP-NISTl-5 0.88 0.85
Syntactic SP-NISTp-5 0.85 0.86
SP-NISTc-5 0.84 0.83
DP-HWCw-4 0.94 0.83
DP-HWCc-4 0.91 0.87
DP-HWCr-4 0.91 0.88
Syntactic DP-Ol-* 0.91 0.84
DP-Oc-* 0.88 0.83
DP-Or-* 0.88 0.84
CP-STM-9 0.89 0.86
NE-Me-* 0.75 0.76
NE-Oe-* 0.71 0.71
NE-Oe-** 0.88 0.80
SR-Mr-* 0.86 0.82
Shallow SR-Or-* 0.92 0.92
Semantic SR-Or 0.91 0.92
SR-Mrv-* 0.89 0.88
SR-Orv-* 0.91 0.92
SR-Orv 0.91 0.91
Table 5: WMT 2006. Evaluation of Homogeneous
Systems. Spanish-to-English (es2en), in-domain
and out-of-domain.
on a subjective manual evaluation based on ade-
quacy and fluency for a subset of 266 sentences (i.e.,
1596 sentences were assessed). Each sentence was
evaluated by two different human judges. System
scores have been obtained by averaging over all sen-
tence scores.
Table 6 shows the level of correlation with hu-
man assessments for some metric representatives
(see ?ALL? column). In this case, lexical metrics
obtain extremely low levels of correlation. Again,
the problem is that lexical metrics are unable to cap-
ture the actual quality of ?LinearB?. At the shallow
syntactic level, only metrics which do not consider
any lexical information (?SP-NISTp-5? and ?SP-NISTc-
5?) attain a significantly higher quality. At the prop-
erly syntactic level, all metrics attain a higher corre-
lation. At the shallow semantic level, again, while
?NE? metrics are not specially useful, ?SR? metrics
prove very effective.
On the other hand, if we remove ?LinearB? (see
262
ar2en
Level Metric ALL SMT
1-PER -0.35 0.75
1-WER -0.50 0.69
BLEU 0.06 0.83
Lexical NIST 0.04 0.81
GTM 0.03 0.92
ROUGE -0.17 0.81
METEOR 0.05 0.86
SP-Op-* 0.05 0.84
SP-Oc-* 0.12 0.89
Shallow SP-NISTl-5 0.04 0.82
Syntactic SP-NISTp-5 0.42 0.89
SP-NISTc-5 0.44 0.68
DP-HWCw-4 0.52 0.86
DP-HWCc-4 0.80 0.75
DP-HWCr-4 0.88 0.86
Syntactic DP-Ol-* 0.51 0.94
DP-Oc-* 0.53 0.91
DP-Or-* 0.72 0.93
CP-STM-9 0.74 0.95
NE-Me-* 0.33 0.78
NE-Oe-* 0.24 0.82
NE-Oe-** 0.04 0.81
SR-Mr-* 0.72 0.96
Shallow SR-Or-* 0.61 0.87
Semantic SR-Or 0.66 0.75
SR-Mrv-* 0.68 0.97
SR-Orv-* 0.47 0.84
SR-Orv 0.46 0.81
Table 6: NIST 2005. Arabic-to-English (ar2en) ex-
ercise. ?ALL? refers to the evaluation of all systems.
?SMT? refers to the evaluation of statistical systems
alone (i.e., removing ?LinearB?).
?SMT? column), lexical metrics attain a much higher
correlation, in the same range of metrics based on
deeper linguistic information. However, still met-
rics based on syntactic parsing, and semantic roles,
exhibit a slightly higher quality.
4 Conclusions
We have presented a comparative study on the
behavior of a wide set of metrics for automatic
MT evaluation at different linguistic levels (lexical,
shallow-syntactic, syntactic, and shallow-semantic)
under different scenarios. We have shown, through
empirical evidence, that linguistic features at more
abstract levels may provide more reliable system
rankings, specially when the systems under evalu-
ation do not share the same lexicon.
We strongly believe that future MT evaluation
campaigns should benefit from these results, by in-
cluding metrics at different linguistic levels. For in-
stance, the following set could be used:
{ ?DP-HWCr-4?, ?DP-Oc-*?, ?DP-Ol-*?, ?DP-Or-*?, ?CP-
STM-9?, ?SR-Or-*?, ?SR-Orv? }
All these metrics are among the top-scoring in all
the translation tasks studied. However, none of these
metrics provides, in isolation, a ?global? measure of
quality. Indeed, all these metrics focus on ?partial?
aspects of quality. We believe that, in order to per-
form ?global? evaluations, different quality dimen-
sions should be integrated into a single measure of
quality. With that purpose, we are currently explor-
ing several metric combination strategies. Prelim-
inary results, based on the QUEEN measure inside
the QARLA Framework (Amigo? et al, 2005), indi-
cate that metrics at different linguistic levels may be
robustly combined.
Experimental results also show that metrics re-
quiring linguistic analysis seem very robust against
parsing errors committed by automatic linguistic
processors, at least at the document level. That
is very interesting, taking into account that, while
reference translations are supposedly well formed,
that is not always the case of automatic translations.
However, it remains pending to test the behaviour at
the sentence level, which could be very useful for er-
ror analysis. Moreover, relying on automatic proces-
sors implies two other important limitations. First,
these tools are not available for all languages. Sec-
ond, usually they are too slow to allow for massive
evaluations, as required, for instance, in the case of
system development. In the future, we plan to incor-
porate more accurate, and possibly faster, linguistic
processors, also for languages other than English, as
they become publicly available.
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
WMT organizers for providing such valuable test
beds. Authors are also thankful to Audrey Le (from
NIST), and to the 2005 NIST MT Evaluation Cam-
paign participants who agreed to share their system
263
outputs and human assessments for the purpose of
this research.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pen?as, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of COLING-ACL06.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the Role of BLEU in Ma-
chine Translation Research. In Proceedings of EACL.
Chris Callison-Burch. 2005. Linear B system descrip-
tion for the 2005 NIST MT evaluation exercise. In
Proceedings of the NIST 2005 Machine Translation
Evaluation Workshop.
Xavier Carreras, Llu??s Ma?rquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1?31.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of ACL.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the 2nd IHLT.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jesu?s Gime?nez and Enrique Amigo?. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102?121.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. Technical
report, NIST, August.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of ACL.
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of COLING.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Proceedings of the Workshop on the
Evaluation of Parsing Systems.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or Sum-
marization.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation.
In Proceedings of HLT/NAACL.
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. Robust Combination Strat-
egy for Semantic Role Labeling. In Proceedings of
HLT/EMNLP.
S. Nie?en, F.J. Och, G. Leusch, and H. Ney. 2000. Eval-
uation Tool for Machine Translation: Fast Evaluation
for MT Research. In Proceedings of the 2nd LREC.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalu-
ation of machine translation, rc22176, ibm. Technical
report, IBM T.J. Watson Research Center.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Con-
fusion of Tongues: an MT Metric. In Proceedings
of the Workshop on MT Evaluation ?Who did what to
whom?? at MT Summit VIII, pages 55?59.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
Domain Speech. In Proceedings of the 9th Inter-
national Conference on Speech Communication and
Technology (Interspeech).
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based Search for Statistical
Translation. In Proceedings of European Conference
on Speech Communication and Technology.
264
Proceedings of the Third Workshop on Statistical Machine Translation, pages 195?198,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Smorgasbord of Features for Automatic MT Evaluation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
This document describes the approach by the
NLP Group at the Technical University of Cat-
alonia (UPC-LSI), for the shared task on Au-
tomatic Evaluation of Machine Translation at
the ACL 2008 Third SMT Workshop.
1 Introduction
Our proposal is based on a rich set of individual
metrics operating at different linguistic levels: lex-
ical (i.e., on word forms), shallow-syntactic (e.g., on
word lemmas, part-of-speech tags, and base phrase
chunks), syntactic (e.g., on dependency and con-
stituency trees), shallow-semantic (e.g., on named
entities and semantic roles), and semantic (e.g., on
discourse representations). Although from differ-
ent viewpoints, and based on different similarity as-
sumptions, in all cases, translation quality is mea-
sured by comparing automatic translations against
human references. Extensive details on the met-
ric set may be found in the IQMT technical manual
(Gime?nez, 2007).
Apart from individual metrics, we have also
applied a simple integration scheme based on
uniformly-averaged linear metric combinations
(Gime?nez and Ma`rquez, 2008a).
2 What is new?
The main novelty, with respect to the set of metrics
presented last year (Gime?nez and Ma`rquez, 2007),
is the incorporation of a novel family of metrics
at the properly semantic level. DR metrics ana-
lyze similarities between automatic and reference
translations by comparing their respective discourse
representation structures (DRS), as provided by the
the C&C Tools (Clark and Curran, 2004). DRS are
essentially a variation of first-order predicate calcu-
lus which can be seen as semantic trees. We use
three different kinds of metrics:
DR-STM Semantic Tree Matching, a la Liu and
Gildea (2005), but over DRS instead of over
constituency trees.
DR-Or-? Lexical overlapping over DRS.
DR-Orp-? Morphosyntactic overlapping on DRS.
Further details on DR metrics can be found in
(Gime?nez and Ma`rquez, 2008b).
2.1 Improved Sentence Level Behavior
Metrics based on deep linguistic analysis rely on
automatic processors trained on out-domain data,
which may be, thus, prone to error. Indeed, we found
out that in many cases, metrics are unable to pro-
duce a result due to the lack of linguistic analysis.
For instance, in our experiments, for SR metrics, we
found that the semantic role labeler was unable to
parse 14% of the sentences. In order to improve the
recall of these metrics, we have designed two simple
variants. Given a linguistic metric x, we define:
? xb ? by backing off to lexical overlapping,
Ol, only when the linguistic processor is not
able to produce a linguistic analysis. Other-
wise, x score is returned. Lexical scores are
conveniently scaled so that they are in a similar
range to scores of x. Specifically, we multiply
195
them by the average x score attained over all
other test cases for which the parser succeeded.
? xi ? by linearly interpolating x and Ol scores
for all test cases, via the arithmetic mean.
In both cases, system scores are calculated by av-
eraging over all sentence scores. Currently, these
variants are applied only to SR and DR metrics.
2.2 Uniform Linear Metric Combinations
We have simulated a non-parametric combination
scheme based on human acceptability by working
on uniformly averaged linear combinations (ULC)
of metrics (Gime?nez and Ma`rquez, 2008a). Our ap-
proach is similar to that of Liu and Gildea (2007)
except that in our case the contribution of each met-
ric to the overall score is not adjusted.
Optimal metric sets are determined by maximiz-
ing the correlation with human assessments, either
at the document or sentence level. However, because
exploring all possible combinations was not viable,
we have used a simple algorithm which performs an
approximate search. First, metrics are ranked ac-
cording to their individual quality. Then, following
that order, metrics are added to the optimal set only
if in doing so the global quality increases.
3 Experimental Work
We use all into-English test beds from the 2006
and 2007 editions of the SMT workshop (Koehn
and Monz, 2006; Callison-Burch et al, 2007).
These include the translation of three differ-
ent language-pairs: German-to-English (de-en),
Spanish-to-English (es-en), and French-to-English
(fr-en), over two different scenarios: in-domain (Eu-
ropean Parliament Proceedings) and out-of-domain
(News Commentary Corpus)1. In all cases, a single
reference translation is available. In addition, hu-
man assessments on adequacy and fluency are avail-
able for a subset of systems and sentences. Each
sentence has been evaluated at least by two different
judges. A brief numerical description of these test
beds is available in Table 1.
1We have not used the out-of-domain Czech-to-English test
bed from the 2007 shared task because it includes only 4 sys-
tems, and only 3 of them count on human assessments.
WMT 2006
in-domain out-of-domain
2,000 cases 1,064 cases
#snt #sys #snt #sys
de-en 2,281 10/12 1,444 10/12
es-en 1,852 11/15 1,008 11/15
fr-en 2,268 11/14 1,281 11/14
WMT 2007
in-domain out-of-domain
2,000 cases 2,007 cases
#snt #sys #snt #sys
de-en 956 7/8 947 5/6
es-en 812 8/10 675 7/9
fr-en 624 7/8 741 7/7
Table 1: Test bed description. ?#snt? columns show the
number of sentences assessed (considering all systems).
?#sys? columns shows the number of systems counting
on human assessments with respect to the total number
of systems which participated in each task.
Metrics are evaluated in terms of human accept-
ability, i.e., according to their ability to capture
the degree of acceptability to humans of automatic
translations. We measure human acceptability by
computing Pearson correlation coefficients between
automatic metric scores and human assessments of
translation quality both at document and sentence
level. We use the sum of adequacy and fluency to
simulate a global assessment of quality. Assess-
ments from different judges over the same test case
are averaged into a single score.
3.1 Individual Performance
In first place, we study the behavior of individual
metrics. Table 2 shows meta-evaluation results, over
into-English WMT 2007 test beds, in-domain and
out-of-domain, both at the system and sentence lev-
els, for a set of selected representatives from several
linguistic levels.
At the system level (columns 1-6), corroborating
previous findings by Gime?nez and Ma`rquez (2007),
highest levels of correlation are attained by met-
rics based on deep linguistic analysis (either syn-
tactic or semantic). In particular, two kinds of met-
rics, respectively based on head-word chain match-
ing over grammatical categories and relations (?DP-
196
System Level Sentence Level
de-en es-en fr-en de-en es-en fr-en
Level Metric in out in out in out in out in out in out
1-TER 0.64 0.41 0.83 0.58 0.72 0.47 0.43 0.29 0.23 0.23 0.29 0.20
BLEU 0.87 0.76 0.88 0.70 0.74 0.54 0.46 0.27 0.33 0.20 0.20 0.12
Lexical GTM (e = 2) 0.82 0.69 0.93 0.71 0.76 0.60 0.56 0.36 0.43 0.33 0.27 0.18
ROUGEW 0.87 0.91 0.96 0.78 0.85 0.83 0.58 0.40 0.43 0.35 0.30 0.31
METEORwn 0.83 0.92 0.96 0.74 0.91 0.86 0.53 0.41 0.35 0.28 0.33 0.32
Ol 0.79 0.75 0.91 0.55 0.81 0.66 0.48 0.33 0.35 0.30 0.30 0.21
CP-Oc-? 0.84 0.88 0.95 0.62 0.84 0.76 0.49 0.37 0.38 0.33 0.32 0.25
DP-HWCw-4 0.85 0.93 0.96 0.68 0.84 0.80 0.31 0.26 0.33 0.07 0.10 0.14
Syntactic DP-HWCc-4 0.91 0.98 0.96 0.90 0.98 0.95 0.30 0.25 0.23 0.06 0.13 0.12
DP-HWCr-4 0.89 0.97 0.97 0.92 0.97 0.95 0.33 0.28 0.29 0.08 0.16 0.16
DP-Or-? 0.88 0.96 0.97 0.84 0.89 0.89 0.57 0.41 0.44 0.36 0.33 0.30
CP-STM-4 0.88 0.97 0.97 0.79 0.89 0.89 0.49 0.39 0.40 0.37 0.32 0.26
NE-Me-? -0.13 0.79 0.95 0.68 0.87 0.92 -0.03 0.07 0.07 -0.05 0.05 0.06
NE-Oe-?? -0.18 0.78 0.95 0.58 0.81 0.71 0.32 0.26 0.37 0.26 0.31 0.20
SR-Or-? 0.55 0.96 0.94 0.69 0.89 0.85 0.26 0.14 0.30 0.11 0.08 0.19
SR-Or-?b 0.24 0.98 0.94 0.68 0.92 0.87 0.33 0.21 0.35 0.15 0.18 0.24
Shallow SR-Or-?i 0.51 0.95 0.93 0.67 0.88 0.83 0.37 0.26 0.38 0.19 0.24 0.27
Semantic SR-Mr-? 0.38 0.95 0.96 0.83 0.79 0.75 0.32 0.18 0.28 0.18 0.08 0.14
SR-Mr-?b 0.14 0.98 0.97 0.82 0.84 0.79 0.37 0.23 0.32 0.21 0.15 0.17
SR-Mr-?i 0.38 0.94 0.96 0.80 0.79 0.74 0.40 0.27 0.36 0.24 0.20 0.20
SR-Or 0.73 0.99 0.94 0.66 0.97 0.93 0.12 0.09 0.16 0.07 -0.04 0.17
SR-Ori 0.66 0.99 0.94 0.64 0.95 0.89 0.29 0.25 0.29 0.19 0.15 0.28
DR-Or-? 0.87 0.89 0.96 0.71 0.78 0.75 0.50 0.40 0.37 0.35 0.27 0.28
DR-Or-?b 0.91 0.93 0.97 0.72 0.83 0.80 0.52 0.41 0.38 0.34 0.28 0.27
DR-Or-?i 0.87 0.87 0.96 0.68 0.79 0.74 0.53 0.42 0.39 0.35 0.30 0.28
DR-Orp-? 0.92 0.98 0.99 0.81 0.91 0.89 0.42 0.32 0.29 0.25 0.21 0.30
Semantic DR-Orp-?b 0.93 0.98 0.99 0.81 0.94 0.91 0.45 0.34 0.32 0.22 0.22 0.30
DR-Orp-?i 0.91 0.95 0.98 0.75 0.89 0.85 0.50 0.38 0.36 0.28 0.27 0.33
DR-STM-4 0.89 0.95 0.98 0.79 0.85 0.87 0.28 0.29 0.25 0.21 0.15 0.22
DR-STM-4b 0.92 0.97 0.98 0.80 0.90 0.91 0.36 0.31 0.29 0.21 0.19 0.23
DR-STM-4i 0.91 0.94 0.97 0.74 0.87 0.86 0.43 0.35 0.34 0.26 0.24 0.27
Optimal07 0.93 1.00 0.99 0.92 0.98 0.95 0.60 0.46 0.47 0.42 0.36 0.39
Optimal06 0.01 0.95 0.96 0.75 0.97 0.87 0.50 0.41 0.40 0.20 0.27 0.30
ULC Optimal?07 0.93 0.98 0.99 0.81 0.94 0.91 0.58 0.45 0.46 0.39 0.35 0.34
Optimal?06 0.34 0.96 0.98 0.82 0.92 0.93 0.54 0.41 0.42 0.32 0.32 0.34
Optimalh 0.87 0.98 0.97 0.79 0.91 0.89 0.56 0.44 0.43 0.32 0.31 0.35
Table 2: Meta-evaluation results based on human acceptability for the WMT 2007 into-English translation tasks
HWCc-4?, ?DP-HWCr-4?), and morphosyntactic over-
lapping over discourse representations (?DR-Orp-??),
are consistently among the top-scoring in all test
beds. At the lexical level, variants of ROUGE and
METEOR attain the best results, close to the perfor-
mance of syntactic and semantic features. It can also
be observed that metrics based on semantic roles
and named entities have serious troubles with the
German-to-English in-domain test bed (column 1).
At the sentence level, the highest levels of corre-
lation are attained by metrics based on lexical simi-
larity alone, only rivaled by lexical overlapping over
dependency relations (?DP-Or-??) and discourse rep-
resentations (?DR-Or-??). We speculate the underly-
ing cause might be on the side of parsing errors. In
that respect, lexical back-off strategies report in all
cases a significant improvement.
It can also be observed that, over these test beds,
metrics based on named entities are completely use-
less at the sentence level, at least in isolation. The
reason is that they capture a very partial aspect of
quality which may be not relevant in many cases.
This has been verified by computing the ?NE-Oe-
??? variant which considers also lexical overlapping
over regular items. Observe how this metric attains
a much higher correlation with human assessments.
197
3.2 Metric Combinations
We also study the behavior of metric combinations
under the ULC scheme. Last 5 rows in Table 2
shows meta-evaluation results following 3 different
optimization strategies:
Optimal: the metric set is optimized for each test
bed (language-pair and domain) individually.
Optimal?: the metric set is optimized over the
union of all test beds.
Optimalh: the metric set is heuristically defined
so as to include several of the top-scoring
representatives from each level: Optimalh =
{ ROUGEW , METEORwnsyn, DP-HWCc-4, DP-
HWCr-4, DP-Or-?, CP-STM-4, SR-Mr-?i, SR-
Or-?i, SR-Ori, DR-Or-?i, DR-Orp-?b }.
We present results optimizing over the 2006 and
2007 data sets. Let us provide, as an illustration,
Optimal?07 sets. For instance, at the system level,
no combination improved the isolated global perfor-
mance of the ?DR-Orp-?b? metric (R=0.94). In con-
trast, at the sentence level, the optimal metric set
contains several metrics from each linguistic level:
Optimal?07 = { ROUGEW , DP-Or-?, CP-STM-4, SR-
Or-?i, SR-Mr-?i, DR-Or-?i }. A similar pattern is
observed for all test beds, both at the system and
sentence levels, although with different metrics.
The behavior of optimal metric sets is in general
quite stable, except for the German-to-English in-
domain test bed which presents an anomalous be-
havior when meta-evaluating WMT 2006 optimal
metric sets at the system level. The reason for this
anomaly is in the ?NE-Me-?? metric, which is in-
cluded in the 2006 optimal set: { ?NE-Me-??, ?SR-
Ori? }. ?NE-Me-?? is based on lexical matching over
named entities, and attains in the 2006 German-to-
English in-domain test bed a very high correlation
of 0.95 with human assessments. This partial aspect
of quality seems to be of marginal importance in the
2007 test bed. We have verified this hypothesis by
computing optimal metrics sets without considering
NE variants. Correlation increases to more reason-
able values (e.g., from 0.01 to 0.66 and from 0.34
to 0.91. This result suggests that more robust metric
combination schemes should be pursued.
For future work, we plan to apply parametric
combination schemes based on human likeness clas-
sifiers, as suggested by Kulesza and Shieber (2004).
We must also further investigate the impact of pars-
ing errors on the performance of linguistic metrics.
Acknowledgments
This research has been funded by the Spanish Min-
istry of Education and Science (OpenMT, TIN2006-
15307-C03-02). Our group is recognized by DURSI
as a Quality Research Group (2005 SGR-00130).
References
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
Evaluation of Machine Translation. In Proceedings of
the ACL Second SMT Workshop, pages 136?158.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and Log-Linear Models. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 104?111.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Second SMT
Workshop, pages 256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008a. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
IJCNLP, pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008b. On the
Robustness of Linguistic Features for Automatic MT
Evaluation. To be published.
Jesu?s Gime?nez. 2007. IQMT v 2.1. Tech-
nical Manual (LSI-07-29-R). Technical re-
port, TALP Research Center. LSI Department.
http://www.lsi.upc.edu/ nlp/IQMT/IQMT.v2.1.pdf.
Philipp Koehn and Christof Monz. 2006. Manual and
Automatic Evaluation of Machine Translation between
European Languages. In Proceedings of the Workshop
on Statistical Machine Translation, pages 102?121.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th TMI, pages 75?84.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization.
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of
NAACL, pages 41?48.
198
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 250?258,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
On the Robustness of Syntactic and Semantic
Features for Automatic MT Evaluation
Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center, LSI Department
Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, E-08034, Barcelona
{jgimenez,lluism}@lsi.upc.edu
Abstract
Linguistic metrics based on syntactic and
semantic information have proven very
effective for Automatic MT Evaluation.
However, no results have been presented
so far on their performance when applied
to heavily ill-formed low quality transla-
tions. In order to glean some light into this
issue, in this work we present an empirical
study on the behavior of a heterogeneous
set of metrics based on linguistic analysis
in the paradigmatic case of speech transla-
tion between non-related languages. Cor-
roborating previous findings, we have ver-
ified that metrics based on deep linguis-
tic analysis exhibit a very robust and sta-
ble behavior at the system level. How-
ever, these metrics suffer a significant de-
crease at the sentence level. This is in
many cases attributable to a loss of recall,
due to parsing errors or to a lack of parsing
at all, which may be partially ameliorated
by backing off to lexical similarity.
1 Introduction
Recently, there is a growing interest in the devel-
opment of automatic evaluation metrics which ex-
ploit linguistic knowledge at the syntactic and se-
mantic levels. For instance, we may find metrics
which compute similarities over shallow syntac-
tic structures/sequences (Gime?nez and Ma`rquez,
2007; Popovic and Ney, 2007), constituency
trees (Liu and Gildea, 2005) and dependency
trees (Liu and Gildea, 2005; Amigo? et al, 2006;
Mehay and Brew, 2007; Owczarzak et al, 2007).
We may also find metrics operating over shallow
semantic structures, such as named entities and se-
mantic roles (Gime?nez and Ma`rquez, 2007).
Linguistic metrics have been proven to produce
more reliable system rankings than metrics limit-
ing their scope to the lexical dimension, in partic-
ular when applied to test beds with a rich system
typology, i.e., test beds in which there are auto-
matic outputs produced by systems based on dif-
ferent paradigms, e.g., statistical, rule-based and
human-aided (Gime?nez and Ma`rquez, 2007). The
reason is that they are able to capture deep MT
quality distinctions which occur beyond the shal-
low level of lexical similarities.
However, these metrics have the limitation of
relying on automatic linguistic processors, tools
which are not equally available for all languages
and whose performance may vary depending on
the type of analysis conducted and the applica-
tion domain. Thus, it could be argued that lin-
guistic metrics should suffer a significant quality
drop when applied to a different translation do-
main, or to ill-formed sentences. Clearly, met-
ric scores computed on partial or wrong syntac-
tic/semantic structures will be less informed. But,
should this necessarily lead to less reliable eval-
uations? In this work, we have analyzed this is-
sue by conducting a contrastive empirical study on
the behavior of a heterogeneous set of metrics over
several evaluation scenarios of decreasing transla-
tion quality. In particular, we have studied the case
of Chinese-to-English speech translation, which is
a paradigmatic example of low quality and heavily
ill-formed output.
The rest of the paper is organized as follows. In
Section 2, prior to presenting experimental work,
we describe the set of metrics employed in our
experiments. We also introduce a novel family
of metrics which operate at the properly semantic
level by analyzing similarities over discourse rep-
resentations. Experimental work is then presented
in Section 3. Metrics are evaluated both in terms of
human likeness and human acceptability (Amigo?
et al, 2006). Finally, in Section 4, main conclu-
sions are summarized and future work is outlined.
250
2 A Heterogeneous Metric Set
We have used a heterogeneous set of metrics se-
lected out from the metric repository provided
with the IQMT evaluation package (Gime?nez and
Ma`rquez, 2007)1. We have considered several
metric representatives from different linguistic
levels (lexical, syntactic and semantic). A brief
description of the metric set is available in Ap-
pendix A.
In addition, taking advantage of newly available
semantic processors, we have designed a novel
family of metrics based on the Discourse Repre-
sentation Theory, a theoretical framework offer-
ing a representation language for the examination
of contextually dependent meaning in discourse
(Kamp, 1981). A discourse is represented in a
discourse representation structure (DRS), which is
essentially a variation of first-order predicate cal-
culus ?its forms are pairs of first-order formulae
and the free variables that occur in them.
2.1 Exploiting Semantic Similarity for
Automatic MT Evaluation
?DR? metrics analyze similarities between auto-
matic and reference translations by comparing
their respective DRSs. These are automatically
obtained using the C&C Tools (Clark and Cur-
ran, 2004)2. Sentences are first parsed on the basis
of a combinatory categorial grammar (Bos et al,
2004). Then, the BOXER component (Bos, 2005)
extracts DRSs. As an illustration, Figure 1 shows
the DRS representation for the sentence ?Every
man loves Mary.?. The reader may find the out-
put of the BOXER component (top) together with
the equivalent first-order formula (bottom).
DRS may be viewed as semantic trees, which
are built through the application of two types of
DRS conditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
Three kinds of metrics have been defined:
1http://www.lsi.upc.edu/?nlp/IQMT
2http://svn.ask.it.usyd.edu.au/trac/
candc
DR-STM-l (Semantic Tree Matching) These
metrics are similar to the Syntactic Tree
Matching metric defined by Liu and Gildea
(2005), in this case applied to DRSs instead
of constituency trees. All semantic subpaths
in the candidate and the reference trees are
retrieved. The fraction of matching subpaths
of a given length, l ? [1..9], is computed.
Then, average accumulated scores up to a
given length are retrieved. For instance, ?DR-
STM-4? corresponds to the average accumu-
lated proportion of matching subpaths up to
length-4.
DR-Or-t These metrics compute lexical overlap-
ping3 between discourse representation struc-
tures (i.e., discourse referents and discourse
conditions) according to their type ?t?. For
instance, ?DR-Or -pred? roughly reflects lexi-
cal overlapping between the referents associ-
ated to predicates (i.e., one-place properties),
whereas ?DR-Or -imp? reflects lexical overlap-
ping between referents associated to implica-
tion conditions. We also introduce the ?DR-
Or -?? metric, which computes average lexical
overlapping over all DRS types.
DR-Orp-t These metrics compute morphosyn-
tactic overlapping (i.e., between parts of
speech associated to lexical items) between
discourse representation structures of the
same type t. We also define the ?DR-Orp-??
metric, which computes average morphosyn-
tactic overlapping over all DRS types.
Note that in the case of some complex condi-
tions, such as implication or question, the respec-
tive order of the associated referents in the tree
is important. We take this aspect into account
by making order information explicit in the con-
struction of the semantic tree. We also make ex-
plicit the type, symbol, value and date of condi-
tions when these are applicable (e.g., predicates,
relations, named entities, time expressions, cardi-
nal expressions, or anaphoric conditions).
Finally, the extension to the evaluation setting
based on multiple references is computed by as-
signing the maximum score attained against each
individual reference.
3Overlapping is measured following the formulae and
definitions by Gime?nez and Ma`rquez (2007). A short defi-
nition may be found in Appendix A.
251
Formally:
?y named(y,mary, per) ? (?x man(x) ? ?z love(z) ? event(z) ? agent(z, x) ? patient(z, y))
Figure 1: DRS representation for ?Every man loves Mary.?
3 Experimental Work
In this section, we present an empirical study on
the behavior of a heterogeneous set of metrics
based on linguistic analysis in the case of speech
translation between non-related languages.
3.1 Evaluation Scenarios
We have used the test bed from the Chinese-
to-English translation task at the ?2006 Evalua-
tion Campaign on Spoken Language Translation?
(Paul, 2006)4. The test set comprises 500 transla-
tion test cases corresponding to simple conversa-
tions (question/answer scenario) in the travel do-
main. In addition, there are 3 different evalua-
tion subscenarios of increasing translation diffi-
culty, according to the translation source:
CRR: Translation of correct recognition results
(as produced by human transcribers).
ASR read: Translation of automatic read speech
recognition results.
ASR spont: Translation of automatic sponta-
neous speech recognition results.
For the purpose of automatic evaluation, 7 hu-
man reference translations and automatic outputs
by 14 different MT systems for each evaluation
subscenario are available. In addition, we count
on the results of a process of manual evaluation.
4http://www.slc.atr.jp/IWSLT2006/
For each subscenario, 400 test cases from 6 differ-
ent system outputs were evaluated, by three human
assessors each, in terms of adequacy and fluency
on a 1-5 scale (LDC, 2005). A brief numerical de-
scription of these test beds is available in Table 1.
It includes the number of human references and
system outputs available, as well as the number
of sentences per output, and the number of system
outputs and sentences per system assessed. For the
sake of completeness, we report the performance
of the Automatic Speech Recognition (ASR) sys-
tem, in terms of accuracy, over the source Chinese
utterances, both at the word and sentence levels.
Also, in order to give an idea of the translation
quality exhibited by automatic systems, average
adequacy and fluency scores are also provided.
3.2 Meta-Evaluation
Our experiment requires a mechanism for evaluat-
ing the quality of evaluation metrics, i.e., a meta-
evaluation criterion. The two most prominent are:
? Human Acceptability: Metrics are evaluated
in terms of their ability to capture the de-
gree of acceptability to humans of automatic
translations, i.e., their ability to emulate hu-
man assessors. The underlying assumption
is that good translations should be acceptable
to human evaluators. Human acceptability is
usually measured on the basis of correlation
between automatic metric scores and human
assessments of translation quality.
252
CRR ASR read ASR spont
#human-references 7 7 7
#system-outputs 14 14 13
#sentences 500 500 500
#outputsassessed 6 6 6
#sentencesassessed 400 400 400
Word Recognition Accuracy ? 0.74 0.68
Sentence Recognition Accuracy ? 0.23 0.17
Average Adequacy 1.40 1.02 0.93
Average Fluency 1.16 0.98 0.98
Table 1: IWSLT 2006 MT Evaluation Campaign. Chinese-to-English test bed description
? Human Likeness: Metrics are evaluated in
terms of their ability to capture the fea-
tures which distinguish human from auto-
matic translations. The underlying assump-
tion is that good translations should resemble
human translations. Human likeness is usu-
ally measured on the basis of discriminative
power (Lin and Och, 2004b; Amigo? et al,
2005).
In this work, metrics are evaluated both in terms
of human acceptability and human likeness. In the
case of human acceptability, metric quality is mea-
sured on the basis of correlation with human as-
sessments both at the sentence and document (i.e.,
system) levels. We compute Pearson correlation
coefficients. The sum of adequacy and fluency is
used as a global measure of quality. Assessments
from different judges have been averaged.
In the case of human likeness, we use the proba-
bilistic KING measure defined inside the QARLA
Framework (Amigo? et al, 2005). KING repre-
sents the probability, estimated over the set of test
cases, that the score attained by a human reference
is equal or greater than the score attained by any
automatic translation. Although KING computa-
tions do not require human assessments, for the
sake of comparison, we have limited to the set of
test cases counting on human assessments.
3.3 Results
Table 2 presents meta-evaluation results for a set
of metric representatives from different linguistic
levels over the three subscenarios defined (?CRR?,
?ASR read? and ?ASR spont?). Highest scores
in each column have been highlighted. Lowest
scores appear in italics.
System-level Behavior
At the system level (Rsys, columns 7-9), the high-
est quality is in general attained by metrics based
on deep linguistic analysis, either syntactic or se-
mantic. Among lexical metrics, the highest cor-
relation is attained by BLEU and the variant of
GTM rewarding longer matchings (e = 2).
As to the impact of sentence ill-formedness,
while most metrics at the lexical level suffer a sig-
nificant variation across the three subscenarios, the
performance of metrics at deeper linguistic levels
is in general quite stable. However, in the case of
the translation of automatically recognized spon-
taneous speech (ASR spont) we have found that
the ?SR-Or-?? and ?SR-Mr-?? metrics, respectively
based on lexical overlapping and matching over
semantic roles, suffer a very significant decrease
far below the performance of most lexical metrics.
Although ?SR-Or-?? has performed well on other
test beds (Gime?nez and Ma`rquez, 2007), its low
performance over the BTEC data suggests that it
is not fully portable across all kind of evaluation
scenarios.
Finally, it is highly remarkable the degree of ro-
bustness exhibited by semantic metrics introduced
in Section 2.1. In particular, the metric variants
based on lexical and morphosyntactic overlapping
over discourse representations (?DR-Or-?? and ?DR-
Orp-??, respectively), obtain a high system-level
correlation with human assessments across the
three subscenarios.
Sentence-level Behavior
At the sentence level (KING and Rsnt, columns
1-6), highest quality is attained in most cases by
metrics based on lexical matching. This result was
expected since all MT systems are statistical and
the test set is in-domain, that is it belongs to the
253
Human Likeness Human Acceptability
KING Rsnt Rsys
ASR ASR ASR ASR ASR ASR
Level Metric CRR read spont CRR read spont CRR read spont
1-WER 0.63 0.69 0.71 0.47 0.50 0.48 0.50 0.32 0.52
1-PER 0.71 0.79 0.79 0.44 0.48 0.45 0.67 0.39 0.60
1-TER 0.69 0.75 0.77 0.49 0.52 0.50 0.66 0.36 0.62
BLEU 0.69 0.72 0.73 0.54 0.53 0.52 0.79 0.74 0.62
Lexical NIST 0.79 0.84 0.85 0.53 0.54 0.53 0.12 0.26 -0.02
GTM (e = 1) 0.75 0.81 0.83 0.50 0.52 0.52 0.35 0.10 -0.09
GTM (e = 2) 0.72 0.78 0.79 0.62 0.64 0.61 0.78 0.65 0.62
METEORwnsyn 0.81 0.86 0.86 0.44 0.50 0.48 0.55 0.39 0.08
ROUGEW 1.2 0.74 0.79 0.81 0.58 0.60 0.58 0.53 0.69 0.43
Ol 0.74 0.81 0.82 0.57 0.62 0.58 0.77 0.51 0.34
SP-Op-? 0.75 0.80 0.82 0.54 0.59 0.56 0.77 0.54 0.48
SP-Oc-? 0.74 0.81 0.82 0.54 0.59 0.55 0.82 0.52 0.49
Shallow SP-NISTl 0.79 0.84 0.85 0.52 0.53 0.52 0.10 0.25 -0.03
Syntactic SP-NISTp 0.74 0.78 0.80 0.44 0.42 0.43 -0.02 0.24 0.04
SP-NISTiob 0.65 0.69 0.70 0.33 0.32 0.35 -0.09 0.17 -0.09
SP-NISTc 0.55 0.59 0.59 0.24 0.22 0.25 -0.07 0.19 0.08
CP-Op-? 0.75 0.81 0.82 0.57 0.63 0.59 0.84 0.67 0.52
CP-Oc-? 0.74 0.80 0.82 0.60 0.64 0.61 0.71 0.53 0.43
DP-Ol-? 0.68 0.75 0.76 0.48 0.50 0.50 0.84 0.77 0.67
DP-Oc-? 0.71 0.76 0.77 0.41 0.46 0.43 0.76 0.65 0.71
Syntactic DP-Or -? 0.75 0.80 0.81 0.51 0.53 0.51 0.81 0.75 0.62
DP-HWCw 0.54 0.57 0.57 0.29 0.32 0.28 0.73 0.74 0.37
DP-HWCc 0.48 0.51 0.52 0.17 0.18 0.22 0.73 0.64 0.67
DP-HWCr 0.44 0.49 0.48 0.20 0.21 0.25 0.71 0.58 0.56
CP-STM 0.71 0.77 0.80 0.53 0.56 0.54 0.65 0.58 0.47
SR-Mr -? 0.40 0.43 0.45 0.29 0.28 0.29 0.52 0.60 0.20
SR-Or -? 0.45 0.49 0.51 0.35 0.35 0.36 0.56 0.58 0.14
Shallow SR-Or 0.31 0.33 0.35 0.16 0.15 0.18 0.68 0.73 0.53
Semantic SR-Mrv -? 0.38 0.41 0.42 0.33 0.34 0.34 0.79 0.81 0.42
SR-Orv -? 0.40 0.44 0.45 0.36 0.38 0.38 0.64 0.72 0.72
SR-Orv 0.36 0.40 0.40 0.27 0.31 0.29 0.34 0.78 0.38
DR-Or -? 0.67 0.73 0.75 0.48 0.53 0.50 0.86 0.74 0.77
Semantic DR-Orp-? 0.59 0.64 0.65 0.34 0.35 0.33 0.84 0.78 0.95
DR-STM 0.58 0.63 0.65 0.23 0.26 0.26 0.75 0.62 0.67
Table 2: Meta-evaluation results for a set of metric representatives from different linguistic levels
same domain in which systems have been trained.
Therefore, translation outputs have a strong ten-
dency to share the sublanguage (i.e., word selec-
tion and word ordering) represented by the prede-
fined set of human reference translations.
Metrics based on lexical overlapping and
matching over shallow syntactic categories and
syntactic structures (?SP-Op-??, ?SP-Oc-??, ?CP-Op-??,
?CP-Oc-??, ?DP-Ol-??, ?DP-Oc-??, and ?DP-Or-??) per-
form similarly to lexical metrics. However, com-
puting NIST scores over base phrase chunk se-
quences (?SP-NISTiob?, ?SP-NISTc?) is not as effec-
tive. Metrics based on head-word chain match-
ing (?DP-HWCw?, ?DP-HWCc?, ?DP-HWCr?) suffer also
a significant decrease. Interestingly, the metric
based on syntactic tree matching (?CP-STM?) per-
formed well in all scenarios.
Metrics at the shallow semantic level suffer also
a severe drop in performance. Particularly signif-
icant is the case case of the ?SR-Or? metric, which
does not consider any lexical information. Inter-
estingly, the ?SR-Orv? variant, which only differs
in that it distinguishes between SRs associated to
different verbs, performs slightly better.
At the semantic level, metrics based on lex-
ical and morphosyntactic overlapping over dis-
course representations (?DR-Or-?? and ?DR-Orp-??)
suffer only a minor decrease, whereas semantic
tree matching (?DR-STM?) reports as a specially bad
predictor of human acceptability (Rsnt).
However, the most remarkable result, in rela-
tion to the goal of this work, is that the behavior
of syntactic and semantic metrics across the three
evaluation subscenarios is, in general, quite stable
?the three values in each subrow are in a very
similar range. Therefore, answering the question
posed in the introduction, sentence ill-formedness
is not a limiting factor in the performance of lin-
guistic metrics.
254
Human Likeness Human Acceptability
KING Rsnt Rsys
ASR ASR ASR ASR ASR ASR
Level Metric CRR read spont CRR read spont CRR read spont
Lexical NIST 0.79 0.84 0.85 0.53 0.54 0.53 0.12 0.26 -0.02
GTM (e = 2) 0.72 0.78 0.79 0.62 0.64 0.61 0.78 0.65 0.62
METEORwnsyn 0.81 0.86 0.86 0.44 0.50 0.48 0.55 0.39 0.08
Ol 0.74 0.81 0.82 0.57 0.62 0.58 0.77 0.51 0.34
CP-Op-? 0.75 0.81 0.82 0.57 0.63 0.59 0.84 0.67 0.52
Syntactic CP-Oc-? 0.74 0.80 0.82 0.60 0.64 0.61 0.71 0.53 0.43
DP-Ol-? 0.68 0.75 0.76 0.48 0.50 0.50 0.84 0.77 0.67
SR-Mr -? 0.40 0.43 0.45 0.29 0.28 0.29 0.52 0.60 0.20
SR-Mr -?b 0.68 0.72 0.73 0.31 0.30 0.31 0.52 0.60 0.20
SR-Mr -?i 0.84 0.86 0.88 0.34 0.34 0.34 0.56 0.63 0.25
SR-Or -? 0.45 0.49 0.51 0.35 0.35 0.36 0.56 0.58 0.14
SR-Or -?b 0.71 0.75 0.78 0.38 0.38 0.38 0.56 0.58 0.14
SR-Or -?i 0.84 0.88 0.89 0.41 0.41 0.41 0.62 0.60 0.22
SR-Or 0.31 0.33 0.35 0.16 0.15 0.18 0.68 0.73 0.53
SR-Or b 0.54 0.58 0.60 0.19 0.18 0.20 0.68 0.73 0.53
Shallow SR-Or i 0.72 0.77 0.79 0.26 0.26 0.27 0.80 0.73 0.67
Semantic SR-Mrv -? 0.38 0.41 0.42 0.33 0.34 0.34 0.79 0.81 0.42
SR-Mrv -?b 0.70 0.73 0.74 0.34 0.35 0.34 0.79 0.81 0.42
SR-Mrv -?i 0.88 0.90 0.92 0.36 0.38 0.37 0.81 0.82 0.45
SR-Orv -? 0.40 0.44 0.45 0.36 0.38 0.38 0.64 0.72 0.72
SR-Orv -?b 0.72 0.76 0.77 0.38 0.40 0.39 0.64 0.72 0.72
SR-Orv -?i 0.88 0.90 0.91 0.40 0.42 0.41 0.69 0.74 0.74
SR-Orv 0.36 0.40 0.40 0.27 0.31 0.29 0.34 0.78 0.38
SR-Orv b 0.66 0.70 0.71 0.29 0.32 0.30 0.34 0.78 0.38
SR-Orv i 0.83 0.86 0.88 0.33 0.36 0.33 0.49 0.82 0.56
DR-Or -? 0.67 0.73 0.75 0.48 0.53 0.50 0.86 0.74 0.77
DR-Or -?b 0.69 0.75 0.77 0.50 0.53 0.50 0.90 0.69 0.56
Semantic DR-Or -?i 0.83 0.87 0.89 0.53 0.57 0.53 0.88 0.70 0.61
DR-Orp-? 0.59 0.64 0.65 0.34 0.35 0.33 0.84 0.78 0.95
DR-Orp-?b 0.61 0.65 0.67 0.35 0.36 0.34 0.86 0.71 0.57
DR-Orp-?i 0.80 0.84 0.85 0.43 0.46 0.43 0.90 0.75 0.70
DR-STM 0.58 0.63 0.65 0.23 0.26 0.26 0.75 0.62 0.67
DR-STM-b 0.64 0.68 0.71 0.23 0.26 0.27 0.75 0.62 0.67
DR-STM-i 0.83 0.87 0.87 0.33 0.36 0.36 0.84 0.63 0.66
Table 3: Meta-evaluation results. Improved sentence-level evaluation of SR and DR metrics
Improved Sentence-level Behavior
By inspecting particular instances, we have found
that linguistic metrics are, in many cases, unable to
produce any evaluation result. The number of un-
scored sentences is particularly significant in the
case of SR metrics. For instance, the ?SR-Or-??
metric is unable to confer an evaluation score in
57% of the cases. Several reasons explain this fact.
The first and most important is that linguistic met-
rics rely on automatic processors trained on out-
of-domain data, which are, thus, prone to error.
Second, we argue that the test bed itself does not
allow for fully exploiting the capabilities of these
metrics. Apart from being based on a reduced vo-
cabulary (2,346 distinct words), test cases consist
mostly of very short segments (14.64 words on av-
erage), which in their turn consist of even shorter
sentences (8.55 words on average)5.
5Vocabulary size and segment/sentence average lengths
have been computed over the set of reference translations.
A possible solution could be to back off to
a measure of lexical similarity in those cases in
which linguistic processors are unable to produce
any linguistic analysis. This should significantly
increase their recall. With that purpose, we have
designed two new variants for each of these met-
rics. Given a linguistic metric x, we define:
? xb ? by backing off to lexical overlapping,
Ol, only when the linguistic processor was
not able to produce a parsing. Lexical scores
are conveniently scaled so that they are in a
similar range to x scores. Specifically, we
multiply them by the average x score attained
over all other test cases for which the parser
succeeded. Formally, given a test case t be-
longing to a set of test cases T :
xb(t) =
{
x(t) if t ? ok(T )
Ol(t)
P
j?ok(T ) x(j)
|ok(T )| otherwise
255
where ok(T ) is the subset of test cases in T
which were successfully parsed.
? xi ? by linearly interpolating x and Ol
scores for all test cases, via arithmetic mean:
xi(t) =
x(t) + Ol(t)
2
In both cases, system-level scores are calculated
by averaging over all sentence-level scores.
Table 3 shows meta-evaluation results on the
performance of these variants for several repre-
sentatives from the SR and DR families. For the
sake of comparison, we also show the scores at-
tained by the base versions, and by some of the
top-scoring metrics from other linguistic levels.
The first observation is that in all cases the new
variants outperform their respective base metric,
being linear interpolation the best alternative. The
increase is particularly significant in terms of hu-
man likeness. New variants even outperform lex-
ical metrics, including the Ol metric, which sug-
gests that, in spite of its simplicity, this is a valid
combination scheme. However, in terms of human
acceptability, the gain is only moderate, and still
their performance is far from top-scoring metrics.
Sentence-level improvements are also reflected
at the system level, although to a lesser extent.
Interestingly, in the case of the translation of au-
tomatically recognized spontaneous speech (ASR
spont, column 9), mixing with lexical overlap-
ping improves the low-performance ?SR-Or? and
?SR-Orv? metrics, at the same time that it causes
a significant drop in the high-performance ?DR-Or?
and ?DR-Orp? metrics. Still, the performance of lin-
guistic metrics at the sentence level is under the
performance of lexical metrics. This is not sur-
prising. After all, apart from relying on automatic
processors, linguistic metrics focus on very par-
tial aspects of quality. However, since they operate
at complementary quality dimensions, their scores
are suitable for being combined.
4 Conclusions and Future Work
We have presented an empirical study on the ro-
bustness of a heterogeneous set of metrics operat-
ing at different linguistic levels for the particular
case of Chinese-to-English speech translation of
basic travel expressions. As an additional contri-
bution, we have presented a novel family of met-
rics which operate at the semantic level by analyz-
ing discourse representations.
Corroborating previous findings by Gime?nez
and Ma`rquez (2007), results at the system level,
show that metrics guided by deeper linguistic
knowledge, either syntactic or semantic, are, in
general, more effective and stable than metrics
which limit their scope to the lexical dimension.
However, at the sentence level, results indicate
that metrics based on deep linguistic analysis are
not as reliable overall quality estimators as lexical
metrics, at least when applied to low quality trans-
lations, as it is the case. This behavior is mainly at-
tributable a drop in recall due to parsing errors. By
inspecting particular sentences we have observed
that in many cases these metrics are unable to pro-
duce any result. In that respect, we have showed
that backing off to lexical similarity is a valid and
effective strategy so as to improve the performance
of these metrics.
But the most remarkable result, in relation to the
goal of this work, is that syntactic and semantic
metrics exhibit a very robust behavior across the
three evaluation subscenarios of decreasing trans-
lation quality analyzed. Therefore, sentence ill-
formedness is not a limiting factor in the perfor-
mance of linguistic metrics. The quality drop,
when moving from the system to the sentence
level, seems, thus, more related to a shift in the
application domain.
For future work, we are currently studying the
possibility of further improving the sentence-level
behavior of present evaluation methods by com-
bining the outcomes of metrics at different linguis-
tic levels into a single measure of quality (citation
omitted for the sake of anonymity).
Acknowledgements
This research has been funded by the Span-
ish Ministry of Education and Science, project
OpenMT (TIN2006-15307-C03-02). Our NLP
group has been recognized as a Quality Research
Group (2005 SGR-00130) by DURSI, the Re-
search Department of the Catalan Government.
We are grateful to the SLT Evaluation Campaign
organizers and participants for providing such
valuable test beds.
References
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and
Felisa Verdejo. 2005. QARLA: a Framework for
the Evaluation of Automatic Sumarization. In Pro-
256
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and
Llu??s Ma`rquez. 2006. MT Evaluation: Human-
Like vs. Human Acceptable. In Proceedings of
the Joint 21st International Conference on Com-
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguistics
(COLING-ACL).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
Coverage Semantic Representations from a CCG
Parser. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING), pages 1240?1246.
Johan Bos. 2005. Towards Wide-Coverage Seman-
tic Interpretation. In Proceedings of the Sixth In-
ternational Workshop on Computational Semantics,
pages 42?53.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and Log-Linear Models. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
104?111.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd In-
ternation Conference on Human Language Technol-
ogy, pages 138?145.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Hans Kamp. 1981. A Theory of Truth and Seman-
tic Representation. In J.A.G. Groenendijk, T.M.V.
Janssen, , and M.B.J. Stokhof, editors, Formal Meth-
ods in the Study of Language, pages 277?322, Ams-
terdam. Mathematisch Centrum.
LDC. 2005. Linguistic Data Annotation Specification:
Assessment of Adequacy and Fluency in Trans-
lations. Revision 1.5. Technical report, Linguis-
tic Data Consortium. http://www.ldc.upenn.edu/-
Projects/TIDES/Translation/TransAssess04.pdf.
Chin-Yew Lin and Franz Josef Och. 2004a. Au-
tomatic Evaluation of Machine Translation Qual-
ity Using Longest Common Subsequence and Skip-
Bigram Statics. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. OR-
ANGE: a Method for Evaluating Automatic Evalu-
ation Metrics for Machine Translation. In Proceed-
ings of the 20th International Conference on Com-
putational Linguistics (COLING).
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evalu-
ation. In Proceedings of the 11th Conference on
Theoretical and Methodological Issues in Machine
Translation (TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Joint Conference on Hu-
man Language Technology and the North American
Chapter of the Association for Computational Lin-
guistics (HLT-NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-Based Automatic Eval-
uation for Machine Translation. In Proceedings of
SSST, NAACL-HLT/AMTA Workshop on Syntax and
Structure in Statistical Translation, pages 80?87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic eval-
uation of machine translation, RC22176. Technical
report, IBM T.J. Watson Research Center.
Michael Paul. 2006. Overview of the IWSLT 2006
Evaluation Campaign. In Proceedings of the In-
ternational Workshop on Spoken Language Trans-
lation, pages 1?15.
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Appli-
cations for Error Analysis. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 48?55, Prague, Czech Republic, June.
Association for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of AMTA, pages 223?231.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and
H. Sawaf. 1997. Accelerated DP based Search for
Statistical Translation. In Proceedings of European
Conference on Speech Communication and Technol-
ogy.
257
A Metric Set
Metrics are grouped according to the linguistic di-
mension at which they operate:
? Lexical Similarity
WER (Nie?en et al, 2000).
PER (Tillmann et al, 1997).
BLEU (Papineni et al, 2001).
NIST (Doddington, 2002).
GTM (Melamed et al, 2003).
ROUGE (Lin and Och, 2004a).
METEOR. (Banerjee and Lavie, 2005).
TER (Snover et al, 2006).
Ol (Gime?nez and Ma`rquez, 2007). Ol is a
short name for lexical overlapping. Au-
tomatic and reference translations are
considered as unordered sets of lexical
items. Ol is computed as the cardinal-
ity of the intersection of the two sets di-
vided into the cardinality of their union.
? Shallow Syntactic Similarity (SP)
SP-Op-?. Average lexical overlapping over
parts-of-speech.
SP-Oc-?. Average lexical overlapping over
base phrase chunk types.
SP-NIST. NIST score over sequences of:
SP-NISTl Lemmas.
SP-NISTp Parts-of-speech.
SP-NISTc Base phrase chunks.
SP-NISTiob Chunk IOB labels.
? Syntactic Similarity
On Dependency Parsing (DP)
DP-HWC Head-word chain matching
(HWCM), as presented by Liu and
Gildea (2005), but slightly modi-
fied so as to consider different head-
word chain types:
DP-HWCw words.
DP-HWCc categories.
DP-HWCr relations.
In all cases only chains up to length
4 are considered.
DP-Ol|Oc|Or These metrics cor-
respond exactly to the LEVEL,
GRAM and TREE metrics intro-
duced by Amigo? et al (2006):
DP-Ol-? Average overlapping be-
tween words hanging at the same
level of the tree.
DP-Oc-? Average overlapping be-
tween words assigned the same
grammatical category.
DP-Or-? Average overlapping be-
tween words ruled by the same
type of grammatical relations.
On Constituency Parsing (CP)
CP-STM Syntactic tree matching
(STM), as presented by Liu and
Gildea (2005), i.e., limited up to
length-4 subtrees.
CP-Op-? Average lexical overlap-
ping over parts-of-speech, similarly
to ?SP-Op-??, except that parts-of-
speech are now consistent with the
full parsing.
CP-Oc-? Average lexical overlapping
over phrase constituents. The differ-
ence between this metric and ?SP-Oc-
?? is in the phrase scope. In con-
trast to base phrase chunks, con-
stituents allow for phrase embed-
ding and overlapping.
? Shallow-Semantic Similarity
On Semantic Roles (SR)
SR-Or-? Average lexical overlapping
between SRs of the same type.
SR-Mr-? Average lexical matching
between SRs of the same type.
SR-Or Overlapping between semantic
roles independently from their lexi-
cal realization.
We also consider a more restrictive ver-
sion of these metrics (?SR-Mrv -??, ?SR-
Orv -??, and ?SR-Orv ?), which require
SRs to be associated to the same verb.
? Semantic Similarity
On Discourse Representations (DR)
DR-STM Average semantic tree
matching considering semantic
subtrees up to length 4.
DR-Or-? Average lexical overlapping
between DRSs of the same type.
DR-Orp-? Average morphosyntactic
overlapping between DRSs of the
same type.
258
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455?466,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Corroborating Text Evaluation Results with Heterogeneous Measures
Enrique Amigo? ? Julio Gonzalo ? Jesu?s Gime?nez ? Felisa Verdejo?
? UNED, Madrid
{enrique,julio,felisa}@lsi.uned.es
? UPC, Barcelona
{jgimenez}@lsi.upc.edu
Abstract
Automatically produced texts (e.g. transla-
tions or summaries) are usually evaluated with
n-gram based measures such as BLEU or
ROUGE, while the wide set of more sophisti-
cated measures that have been proposed in the
last years remains largely ignored for practical
purposes. In this paper we first present an in-
depth analysis of the state of the art in order
to clarify this issue. After this, we formalize
and verify empirically a set of properties that
every text evaluation measure based on simi-
larity to human-produced references satisfies.
These properties imply that corroborating sys-
tem improvements with additional measures
always increases the overall reliability of the
evaluation process. In addition, the greater the
heterogeneity of the measures (which is mea-
surable) the higher their combined reliability.
These results support the use of heterogeneous
measures in order to consolidate text evalua-
tion results.
1 Introduction
The automatic evaluation of textual outputs is a
core issue in many Natural Language Processing
(NLP) tasks such as Natural Language Generation,
Machine Translation (MT) and Automatic Sum-
marization (AS). State-of-the-art automatic evalu-
ation methods all operate by rewarding similari-
ties between automatically-produced candidate out-
puts and manually-produced reference solutions, so-
called human references or models.
Over the last decade, a wide variety of measures,
based on different quality assumptions, have been
proposed. Recent work suggests exploiting exter-
nal knowledge sources and/or deep linguistic an-
notation, and measure combination (see Section 2).
However, original measures based on lexical match-
ing, such as BLEU (Papineni et al, 2001a) and
ROUGE (Lin, 2004) are still preferred as de facto
standards in MT and AS, respectively. There are,
in our opinion, two main reasons behind this fact.
First, the use of a common measure certainly allows
researchers to carry out objective comparisons be-
tween their work and other published results. Sec-
ond, the advantages of novel measures are not easy
to demonstrate in terms of correlation with human
judgements.
Our goal is not to answer which is the most re-
liable metric or to propose yet another novel mea-
sure. Rather than this, we first analyze in depth the
state of the art, concluding that it is not easy to de-
termine the reliability of a measure. In absence of a
clear proof of the advantages of novel measures, sys-
tem developers naturally tend to prefer well-known
standard measures. Second, we formalize and check
empirically two intrinsic properties that any evalua-
tion measure based on similarity to human-produced
references satisfies. Assuming that a measure satis-
fies a set of basic formal constraints, these properties
imply that corroborating a system comparison with
additional measures always increases the overall re-
liability of the evaluation process, even when the
added measures have a low correlation with human
judgements. In most papers, evaluation results are
corroborated with similar n-gram based measures
(eg. BLEU and ROUGE). However, according to
our second property, the greater the heterogeneity of
455
the measures (which is measurable) the higher their
reliability. The practical implication is that, corrob-
orating evaluation results with measures based on
higher linguistic levels increases the heterogeneity,
and therefore, the reliability of evaluation results.
2 State of the Art
2.1 Individual measures
Among NLP disciplines, MT probably has the
widest set of automatic evaluation measures. The
dominant approach to automatic MT evaluation is,
today, based on lexical metrics (also called n-gram
based metrics). These metrics work by rewarding
lexical similarity between candidate translations and
a set of manually-produced reference translations.
Lexical metrics can be classified according to how
they compute similarity. Some are based on edit dis-
tance, e.g., WER (Nie?en et al, 2000), PER (Till-
mann et al, 1997), and TER (Snover et al, 2006).
Other metrics are based on computing lexical preci-
sion, e.g., BLEU (Papineni et al, 2001b) and NIST
(Doddington, 2002), lexical recall, e.g., ROUGE
(Lin and Och, 2004a) and CDER (Leusch et al,
2006), or a balance between the two, e.g., GTM
(Melamed et al, 2003; Turian et al, 2003b), ME-
TEOR (Banerjee and Lavie, 2005), BLANC (Lita et
al., 2005), SIA (Liu and Gildea, 2006), MAXSIM
(Chan and Ng, 2008), and Ol (Gime?nez, 2008).
The lexical measure BLEU has been criticized in
many ways. Some drawbacks of BLEU are the lack
of interpretability (Turian et al, 2003a), the fact that
it is not necessary to increase BLEU to improve sys-
tems (Callison-burch and Osborne, 2006), the over-
scoring of statistical MT systems (Le and Przybocki,
2005), the low reliability over rich morphology lan-
guages (Homola et al, 2009), or even the fact that a
poor system translation of a book can obtain higher
BLEU results than a manually produced translation
(Culy and Riehemann, 2003).
The reaction to these criticisms has been focused
on the development of more sophisticated measures
in which candidate and reference translations are
automatically annotated and compared at different
linguistic levels. Some of the features employed
include parts of speech (Popovic and Ney, 2007;
Gime?nez and Ma`rquez, 2007), syntactic dependen-
cies (Liu and Gildea, 2005; Gime?nez and Ma`rquez,
2007; Owczarzak et al, 2007a; Owczarzak et al,
2007b; Owczarzak et al, 2008; Chan and Ng,
2008; Kahn et al, 2009), CCG parsing (Mehay and
Brew, 2007), syntactic constituents (Liu and Gildea,
2005; Gime?nez and Ma`rquez, 2007), named entities
(Reeder et al, 2001; Gime?nez and Ma`rquez, 2007),
semantic roles (Gime?nez and Ma`rquez, 2007), dis-
course representations (Gime?nez, 2008), and textual
entailment features (Pado? et al, 2009). In general,
when a higher linguistic level is incorporated, lin-
guistic features at lower levels are preserved.
The proposals for summarization evaluation are
less numerous. Some proposals for AS tasks are
based on syntactic units (Tratz and Hovy, 2008), de-
pendency triples (Owczarzak, 2009) or convolution
kernels (Hirao et al, 2005) which reported some re-
liability improvement over ROUGE in terms of cor-
relation with human judgements.
In general, however, it is not easy to determine
clearly the contribution of deeper linguistic knowl-
edge in those proposals. In the case of MT, im-
provements versus BLEU have been reported (Liu
and Gildea, 2005; Kahn et al, 2009), but not over
a more elaborated metric such as METEOR (Mehay
and Brew, 2007; Chan and Ng, 2008). Besides, con-
troversial results on their performance at sentence vs
system level have been reported in shared evaluation
tasks (Callison-Burch et al, 2008; Callison-Burch et
al., 2009; Callison-Burch et al, 2010).
2.2 Combined measures
Several researchers have suggested integrating het-
erogeneous measures. Some of them optimize the
measure combination function according to the met-
ric?s ability to emulate the behavior of human as-
sessors (i.e., correlation with human assessments).
For instance, using linear combinations (Pado? et al,
2009; Liu and Gildea, 2007; Gime?nez and Ma`rquez,
2008), Decision Trees (Akiba et al, 2001; Quirk,
2004), regression based algorithms (Paul et al,
2007; Albrecht and Hwa, 2007a; Albrecht and Hwa,
2007b) or a variety of supervised machine learn-
ing algorithms(Quirk et al, 2005; Corston-Oliver et
al., 2001; Kulesza and Shieber, 2004; Gamon et al,
2005; Amigo? et al, 2005).
Some of these works report evidence on the con-
tribution of combining heterogeneous measures. For
instance, Albrecht and Hwa included syntax-based
456
measures together with lexical measures, outper-
forming other combination schemes (Albrecht and
Hwa, 2007a; Albrecht and Hwa, 2007b). Liu and
Gildea, after examining the contribution of each
component metric, found that ?metrics showing dif-
ferent properties of a sentence are more likely to
make a good combined metric?(Liu and Gildea,
2007). Akiba et al, which combined multiple edit-
distance features based on lexical, morphosyntac-
tic and lexical semantic information, observed that
their approach improved single editing distance for
several data sets (Akiba et al, 2001). More evi-
dence was provided by Corston and Oliver. They
showed that results on the task of discriminating be-
tween manual and automatic translations improve
when combining linguistic and n-gram based fea-
tures. In addition, they showed that this mixed com-
bination improved over the combination of linguistic
or n-gram based measures alone (Corston-Oliver et
al., 2001). (Pado? et al, 2009) reported a reliability
improvement by including measures based on tex-
tual entailment in the set. In (Gime?nez and Ma`rquez,
2008), a simple arithmetic mean of scores for com-
bining measures at different linguistic levels was ap-
plied with remarkable results in recent shared evalu-
ation tasks (Callison-Burch et al, 2010).
2.3 Meta-evaluation criteria
Meta-evaluation methods have been gradually intro-
duced together with evaluation measures. For in-
stance, Papineni et al (2001b) evaluated the reliabil-
ity of the BLEU metric according to its ability to em-
ulate human assessors, as measured in terms of Pear-
son correlation with human assessments of adequacy
and fluency at the document level. The measure
NIST (Doddington, 2002) was meta-evaluated also
in terms of correlation with human assessments, but
over different document sources and for a varying
number of references and segment sizes. Melamed
et al (2003) argued, at the time of introducing the
GTM metric, that Pearson correlation coefficients
can be affected by scale properties. They suggested
using the non-parametric Spearman correlation co-
efficients instead. Lin and Och meta-evaluated
ROUGE over both Pearson and Spearman correla-
tion over a wide set of metrics, including NIST,
WER, PER, and variants of ROUGE, BLEU and
GTM. They obtained similar results in both cases
(Lin and Och, 2004a). Banerjee and Lavie (2005)
argued that the reliability of metrics at the document
level can be due to averaging effects but might not
be robust across sentence translations. In order to
address this issue, they computed the translation-by-
translation correlation with human assessments (i.e.,
correlation at the sentence level).
However, correlation with human judgements is
not enough to determine the reliability of measures.
First, correlation at sentence level (unlike correla-
tion at system level) tends to be low and difficult to
interpret. Second, correlation at system and segment
levels can produce contradictory results. In (Amigo?
et al, 2009) it is observed that higher linguistic lev-
els in measures increases the correlation with human
judgements at the system level at the cost of corre-
lation at the segment level. As far as we know, a
clear explanation for these phenomena has not been
provided yet.
Third, a high correlation at system level does
not ensure a high reliability. Culy and Rieheman
observed that, although BLEU can achieve a high
correlation at system level in some test suites, it
over-scores a poor automatic translation of ?Tom
Sawyer? against a human produced translation (Culy
and Riehemann, 2003). This meta-evaluation crite-
rion based on the ability to discern between man-
ual and automatic translations have been referred to
as human likeness (Amigo? et al, 2006), in contrast
to correlation with human judgements which is re-
ferred to as human acceptability. Examples of meta-
measures based on this criterion are ORANGE (Lin
and Och, 2004b) and KING (Amigo? et al, 2005).
In addition, many of the approaches to metric com-
bination described in Section 2.2 take human like-
ness as the optimization criterion (Corston-Oliver
et al, 2001; Kulesza and Shieber, 2004; Gamon et
al., 2005). The main advantage of meta-evaluation
based on human likeness is that, since human as-
sessments are not required, metrics can be evaluated
over larger test beds. However, the meta-evaluation
in terms of human likeness is difficult to interpret.
2.4 The use of evaluation measures
In general, the state of the art includes a wide set
of results that show the drawbacks of n-gram based
measures as BLEU, and a wide set of proposals for
new single and combined measures which are meta-
457
evaluated in terms of human acceptability (i.e., their
ability to emulate human judges, typically measured
in terms of correlation with human judgements) or
human-likeness (i.e., their ability to discern between
automatic and human translations) (Amigo? et al,
2006). However, the original measures BLEU and
ROUGE are still preferred.
We believe that one of the reasons is the lack of
an in-depth study on to what extent providing ad-
ditional evaluation results with other metrics con-
tributes to the reliability of such results. The state of
the art suggests that the use of heterogeneous mea-
sures can improve the evaluation reliability. How-
ever, as far as we know, there is no comprehen-
sive analysis on the contribution of novel measures
when corroborating evaluation results with addi-
tional measures.
3 Similarity Based Evaluation Measures
In general, automatic evaluation measures applied
in tasks like MT or AS are similarity measures be-
tween system outputs and human references. These
measures are related with precision, recall or overlap
over specific types of linguistic units. For instance,
ROUGE measures n-gram recall. Other measures
that work at higher linguistic levels apply precision,
recall or overlap of linguistic components such as
dependency relations, grammatical categories, se-
mantic roles, etc.
In order to delimit our hypothesis, let us first de-
fine what is a similarity measure in this context. Un-
fortunately, as far as we know, there is no formal
concept covering the properties of current evaluation
similarity measures. A close concept is that of ?met-
ric? or ?distance function?. But, actually, measures
such as ROUGE or BLEU are not proper ?metrics?,
because they do not satisfy the symmetry and the tri-
angle inequality properties. Therefore, we need a
new definition.
Being ? the universe of system outputs s and
gold-standards g, we assume that a similarity mea-
sure, in our context, is a function x : ?2 ?? < such
that there exists a decomposition function f : ? ??
{e1..en} (e.g., words or other linguistic units or
relationships) satisfying the following constraints:
(i) maximum similarity is achieved only when then
the decomposition of the system output resembles
exactly the gold-standard decomposition; and (ii)
growing overlap or removing non overlapped ele-
ments implies growing x. Formally, if x ranges from
0 to 1:
f(s) = f(g)? x(s, g) = 1
(f(s) = f(s?) ? {e ? f(g) \ f(s?)})? x(s, g) > x(s?, g)
(f(s) = f(s?)? {e ? f(s?) \ f(g)})? x(s, g) > x(s?, g)
For instance, a random function and the reversal
of a similarity funtion (f ?(s) = 1f(s) ) do not satisfythese constraints. While the F measure over Pre-
cision and Recall satisfies these constraints1, pre-
cision and recall in isolation do not satisfy all of
them: maximum recall can be achieved without re-
sembling the goldstandard text decomposition; and
maximum precision can be achieved with only a few
overlapped elements.
BLEU (Papineni et al, 2001a) computes the n-
gram precision while the metric ROUGE (Lin and
Och, 2004a) computes the n-gram recall. How-
ever, in general, both metrics satisfy all the con-
straints, given that BLEU includes a brevity penalty
and ROUGE penalizes or limits the system output
length. The measure METEOR creates an align-
ment between the two strings (Banerjee and Lavie,
2005). This overlap-based measure satisfies also the
previous constraints. Measures based on edit dis-
tance over n-grams (Tillmann et al, 1997; Nie?en
et al, 2000) or other linguistic units (Akiba et al,
2001; Popovic and Ney, 2007) match also our def-
inition of similarity measure. The editing distance
is minimum when the two compared text are equal.
The more the evaluated text contains elements from
the gold-standard the more the editing distance is re-
duced (higher similarity). The word ordering can be
also expressed in terms of a decomposition function.
A similar reasoning applies to every relevant mea-
sure in the state-of-the art.
4 Data Sets and Measures
4.1 Data sets
In this paper, we provide empirical results for
MT and AS. For MT, we use the data sets from
the Arabic-to-English (AE) and Chinese-to-English
(CE) NIST MT Evaluation campaigns in 2004 and
1There is an exception. In an extreme case, when recall is
zero, removing non overlapped elements does not modify the F
measure.
458
AE2004 CE2004 AE2005 CE2005
#human-references 5 5 5 4
#systems 5 10 7 10
#system-outputs-assessed 5 10 6 5
#system-outputs 1,353 1,788 1,056 1,082
#outputs-assessed per-system 347 447 266 272
Table 1: Description of the test beds from 2004 and 2005 NIST MT evaluation campaigns used in the experiments
throughout the paper.
DUC 2005 DUC 2006
#human-references 3-4 3-4
#systems 32 35
#system-outputs-assessed 32 35
#system-outputs 50 50
#outputs-assessed per-system 50 50
Table 2: Description of the test beds from 2005 and 2006 DUC evaluation campaigns used in the experiments through-
out the paper.
20052. Both include two translations exercises: for
the 2005 campaign we contacted each participant
individually and asked for permission to use their
data3. In our experiments, we take the sum of ad-
equacy and fluency, both in a 1-5 scale, as a global
measure of quality (LDC, 2005). Thus, human as-
sessments are in a 2-10 scale. For AS, we have used
the AS test suites developed in the DUC 2005 and
DUC 2006 evaluation campaigns4. This AS task
was to generate a question focused summary of 250
words from a set of 25-50 documents to a complex
question. Summaries were evaluated according to
several criteria. Here, we will consider the respon-
siveness judgements, in which the quality score was
an integer between 1 and 5. See Tables 1 and 2 for a
brief quantitative description of these test beds.
2http://www.nist.gov/speech/tests/mt
3We are grateful to a number of groups and companies who
responded positively: University of Southern California Infor-
mation Sciences Institute (ISI), University of Maryland (UMD),
Johns Hopkins University & University of Cambridge (JHU-
CU), IBM, University of Edinburgh, University of Aachen
(RWTH), National Research Council of Canada (NRC), Chi-
nese Academy of Sciences Institute of Computing Technology
(ICT), Instituto Trentino di Cultura - Centro per la Ricerca Sci-
entifica e Tecnologica(ITC-IRST), MITRE.
4http://duc.nist.gov/
4.2 Measures
As for evaluation measures, for MT we have used a
rich set of 64 measures provided within the ASIYA
Toolkit (Gime?nez and Ma`rquez, 2010)5. This in-
cludes measures operating at different linguistic lev-
els: lexical, syntactic, and semantic. At the lexical
level this set includes variants of 8 measures em-
ployed in the state of the art: BLEU, NIST, GTM,
METEOR, ROUGE, WER, PER and TER. In addi-
tion, we have included a basic measure Ol that com-
putes the lexical overlap without considering word
ordering. All these measures have similar granular-
ity. They use n-grams of a varying length as the ba-
sic unit with additional information provided by lin-
guistic tools. The underlying similarity criteria in-
clude precision, recall, overlap, or edit rate, and the
decomposition functions include words, dependency
tree nodes (DP HWC, DP-Or, etc.), constituency
parsing (CP-STM), discourse roles (DR-Or), seman-
tic roles (SR-Or), named entities, etc. Further details
on the measure set may be found in the ASIYA tech-
nical manual (Gime?nez and Ma`rquez, 2010).
According to our computations, our measures
cover high and low correlations at both levels. Cor-
relation at system level spans between 0.63 and 0.95.
Correlations at sentence level ranges from 0.18 up to
0.54. We will discriminate between two subsets of
5http://www.lsi.upc.edu/?nlp/Asiya
459
measures. The first one includes those that decom-
pose the text into words, n-grams, stems or lexical
semantic tags. This set includes BLEU, ROUGE,
NIST, GTM, PER and WER families. We will re-
fer to them as ?lexical? measures. The second set
are those that consider deeper linguistic levels such
as parts of speech, syntactic dependencies, syntactic
constituents, etc. We will refer to them as ?linguis-
tic? measures.
In the case of automatic summarization (AS), we
have employed the standard variants of ROUGE
(Lin, 2004). These 7 measures are ROUGE-{1..4},
ROUGE-SU, ROUGE-L and ROUGE-W. In addi-
tion we have included the reversed precision version
for each variant and the F measure of both. Notice
that the original ROUGE measures are oriented to
recall. In total, we have 21 measures for the sum-
marization task. All of them are based on n-gram
overlap.
5 Additive reliability
As discussed in Section 2, a number of recent pub-
lications address the problem of measure combi-
nation with successful results, specially when het-
erogeneous measures are combined. The following
property clarifies this issue and justifies the use of
heterogeneous measures when corroborating evalu-
ation results. It asserts that the reliability of system
improvements always increases when the evaluation
result is corroborated by an additional similarity
measure, regardless of the correlation achieved by
the additional measure in isolation.
For the sake of clarity, in the rest of the paper,
we will denote the similarity x(s, g) between sys-
tem output s and human reference g by x(s). The
quality of a system output s will be referred to as
Q(s). Let us define the reliability R(X) of a mea-
sure set as the probability of a real improvement (as
measured by human judges) when a score improve-
ment is observed simultaneously for all measures in
the set X. :
R(X) ? P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X)
According to this definition, we may not be able
to predict the quality of any system output (i.e. a
translation) with a highly reliable measure set, but
we can ensure a system improvement when all mea-
sures corroborate the result. Then the additive relia-
bility property can be stated as:
R(X ? {x}) ? R(X)
We could think of violating this property by
adding, for instance, a measure consisting of a ran-
dom function (x?(s) = rand(0..1)) or a reversal of
the original measure (x?(s) = 1/x(s)). These kind
of measures, however, would not satisfy the con-
straints defined in Section 3.
This property is based on the idea that similar-
ity with human references according to any aspect
should not imply statistically a quality decrease. Al-
though our test suites includes measures with low
correlation at segment and system level, we can con-
firm empirically that all of them satisfy this property.
We have developed the following experiment:
taking all possible measure pairs in the test suites,
we have compared their reliability as a set versus the
maximal reliability of any of them (by computing
the difference R(X)?max(R(x1), R(x2)). Figure
1 shows the obtained distribution of this difference
for our MT and AS test suites. Remarkably, in al-
most every case this difference is positive.
This result has a key implication: Corroborating
evaluation results with a new measure, even when
it has lower correlation with human judgements, in-
creases the reliability of results. Therefore, if the
correlation with judgements is not determinant, the
question is now what factor determines the contri-
bution of the new measures. According to the fol-
lowing property, this factor is the heterogeneity of
measures.
6 Heterogeneity
This property states that the reliability of any mea-
sure combination is lower bounded by the hetero-
geneity of the measure set. In other words, a single
measure can be more or less reliable, but a system
improvement according to all measures in an het-
erogeneous set is reliable.
Let us define the heterogeneity H(X) of a set of
measures X as, given two system outputs s and s?
such that g 6= s 6= s? 6= g (g is the reference
text), the probability that there exist two measures
that contradict each other. That is:
H(X) ? P (?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
460
Figure 1: Additive reliability for metric pairs.
Thus, given a set X of measures, the property
states that there exists a strict growing function F
such that:
R(X) ? F (H(X)) and H(X) = 1? R(X) = 1
In other words, the more the similarity measures
tend to contradict each other, the more a unanimous
improvement over all similarity measures is reliable.
Clearly, the harder it is that measures agree, the more
meaningful it is when they do.
The first part is derived from the Additive Re-
liability property. Intuitively, any individual mea-
sure has zero heterogeneity. Increasing the hetero-
geneity implies joining measures or measure sets
progressively. According to the Additive Reliabil-
ity property, this joining implies a reliability in-
crease. Therefore, the higher the heterogeneity, the
higher the minimum Reliability achieved by the cor-
responding measure sets.
The second part is derived from the Heterogeneity
definition. If H(X) = 1 then, for any distinct pair
of outputs that differ from the reference, there exist
at least two measures in the set contradicting each
other. That is, H(X) = 1 implies that:
?s 6= s? 6= g(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))
Therefore, if one output improves the other ac-
cording to all measures, then the output must be
equal than the reference.
?(?x, x? ? X.x(s) > x(s?) ? x?(s) < x?(s?))?
Figure 2: Heterogeneity vs. reliability in MT test suites.
?(g 6= s 6= s? 6= g)? g = s ? g = s?
According to the first constraint of similarity mea-
sures, a text that is equal to the reference achieves
the maximum score:
g = s? f(g) = g(s)? ?x.x(s) ? x(s?)
Finally, if we assume that the reference (human pro-
duced texts) has a maximum quality, then it will
have equal or higher quality than the other output.
g = s? Q(s) ? Q(s?)
Therefore, the reliability of the measure set is maxi-
mal. In summary, if H(X) = 1 then:
R(X) = P (Q(s) ? Q(s?)|x(s) ? x(s?) ?x ? X) =
= P (Q(s) ? Q(s?)|s = g) = 1
Figures 2 and 3 show the relationship between the
heterogeneity of randomly selected measure sets and
their reliability for the MT and summarization test
suites. As the figures show, the higher the hetero-
geneity, the higher the reliability of the measure set.
The results in AS are less pronounced due to the re-
dundancy in ROUGE measure.
Notice that the heterogeneity property does not
necessarily imply a high correlation between reli-
ability and heterogeneity. For instance, an ideal
single measure would have zero heterogeneity and
461
Figure 3: Heterogeneity vs. reliability in summarization
test suites.
achieve maximum reliability, appearing in the top
left area. The property rather brings us to the fol-
lowing situation: let us suppose that we have a set
of single measures available which achieve a certain
range of reliability. We can improve our system ac-
cording to any of these measures. Without human
assessments, we do not know what is the most re-
liable measure. But if we combine them, increas-
ing the heterogeneity, the minimal reliability of the
selected measures will be higher. This implies that
combining heterogeneous measures (e.g. at high lin-
guistic levels) that do not achieve high correlation
in isolation, is better than corroborating results with
any individual measure alone, such as ROUGE and
BLEU, which is the common practice in the state of
the art.
The main drawback of this property is that in-
creasing the heterogeneity implies a sensitivity re-
duction. For instance, if H(X) = 0.9, then only
for 10% of output pairs in the corpus there exists
an improvement according to all measures. In other
words, unanimous evaluation results from heteroge-
neous measures are reliable but harder to achieve for
the system developer. The next section investigates
on this issue.
Finally, Figure 4 shows that linguistic measures
increase the heterogeneity of measure sets. We have
generated sets of metrics of size 1 to 10 made up
by lexical or lexical and linguistic metrics. As the
figure shows, in the second case, the measure sets
achieve a higher heterogeneity.
Figure 4: Heterogeneity of lexical measures vs. lexical
and linguistic measures.
7 Score thresholds vs. Additive Reliability
According to the previous properties, corroborating
evaluation results with several measures increases
the reliability of evaluation results at the cost of sen-
sitivity. On the other hand, increasing the score
threshold of a single measure should have a similar
effect. Which is then the best methodology to im-
prove reliability? In this section we provide exper-
imental evidence on the relationship between both
ways of increasing reliability: we have found that,
corroborating evaluation results over single texts
with additional measures is more reliable than re-
quiring higher score differences according to any in-
dividual measure in the set. More specifically, we
have found that the reliability of a measure set is
higher than the reliability of each of the individual
measures at a similar level of sensitivity.
Formally, we define the sensitivity S(X) of a met-
ric set X as the probability of finding a score im-
provement within text pairs with a real (i.e. human
assessed) quality improvement:
S(X) = P (x(s) ? x(s?)?x ? X|Q(s) ? Q(s?))
Being Rth(x) and Sth(x) the reliability and sen-
sitivity of a single measure x for a certain increase
score threshold th:
462
Figure 5: Heterogeneity vs. reliability Gain for MT test
suites.
Rth(x) = P (Q(s) ? Q(s?)|x(s)? x(s?) ? th)
Sth(x) = P (x(s)? x(s?) ? th|Q(s) ? Q(s?))
The property that we want to check is that, at the
same sensitivity level, combining measures is more
reliable than increasing the score threshold of single
measures:
S(X) = Sth(x).x ? X ?? R(X) ? Rth(x)
Note that if we had a perfect measure xp such that
R(xp) = S(xp) = 1, then combining this measure
with a low reliability measure xl would produce a
lower sensitivity, but the maximal reliability would
be preserved.
In order to confirm empirically this property, we
have developed the following experiment: (i) We
compute the reliability and sensitivity of randomly
chosen measure sets over single text pairs. We have
generated sets of 2,3,5,10,20 and 40 measures. In
the case of summarization corpora we have com-
bined up to 20 measures. In addition, we com-
pute also the heterogeneity H(X) of each measure
set; (ii) Experimenting with different values for the
threshold th, we compute the reliability of single
measures for all potential sensitivity levels; (iii) For
each measure set, we compare the reliability of the
measure set versus the reliability of single measures
at the same sensitivity level. We will refer to this as
the Reliability Gain:
Figure 6: Heterogeneity vs. reliability Gain for MT test
suites.
Reliability Gain =
R(X)?max{Rth(x)/x ? X ? Sth(x) = S(X)}
If there are several reliability values with the same
sensitivity for a given single measures, we choose
the highest reliability value for the single measure.
Figures 5 and 6 illustrate the results for the MT
and AS corpora. The horizontal axis represents the
Heterogeneity of measure sets, while the vertical
axis represents the reliability gain. Remarkably, the
reliability gain is positive for all cases in our test
suites. The maximum reliability gain is 0.34 in the
case of MT and 0.08 for AS (note that summariza-
tion measures are more redundant in our corpora).
In both test suites, the largest information gains are
obtained with highly heterogeneous measure sets.
In summary, given comparable measures in terms
of reliability, corroborating evaluation results with
several measures is more effective than optimizing
systems according to the best measure in the set.
This empirical property provides an additional ev-
idence in favour of the use of heterogeneous mea-
sures and, in particular, of the use of linguistic mea-
sures in combination with standard lexical measures.
8 Conclusions
In this paper, we have analyzed the state of the art in
order to clarify why novel text evaluation measures
463
are not exploited by the community. Our first con-
clusion is that it is not easy to determine the reliabil-
ity of measures, which is highly corpus-dependent
and often contradictory when comparing correlation
with human judgements at segment vs. system lev-
els.
In order to tackle this issue, we have studied a
number of properties that suggest the convenience of
using heterogeneous measures to corroborate eval-
uation results. According to these properties, we
can ensure that, even when if we can not determine
the reliability of individual measures, corroborating
a system improvement with additional measures al-
ways increases the reliability of the results. In ad-
dition, the more heterogeneous the measures em-
ployed (which is measurable), the higher the relia-
bility of the results. But perhaps the most impor-
tant practical finding is that the reliability at similar
sensitivity levels by corroborating evaluation results
with several measures is always higher than improv-
ing systems according to any of the combined mea-
sures in isolation.
These properties point to the practical advantages
of considering linguistic knowledge (beyond lexi-
cal information) in measures, even if they do not
achieve a high correlation with human judgements.
Our experiments show that linguistic knowledge in-
creases the heterogeneity of measure sets, which
in turn increases the reliability of evaluation results
when corroborating system comparisons with sev-
eral measures.
Acknowledgements
This work has been partially funded by the Spanish
Government (Holopedia, TIN2010-21128-C02 and
OpenMT-2, TIN2009-14675-C03) and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement number
247762 (FAUST project, FP7-ICT-2009-4-247762).
References
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001. Using Multiple Edit Distances to Automatically
Rank Machine Translation Output. In Proceedings of
Machine Translation Summit VIII, pages 15?20.
Joshua Albrecht and Rebecca Hwa. 2007a. A Re-
examination of Machine Learning Approaches for
Sentence-Level MT Evaluation. In Proceedings of the
45th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 880?887.
Joshua Albrecht and Rebecca Hwa. 2007b. Regression
for Sentence-Level MT Evaluation with Pseudo Refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 296?303.
Enrique Amigo?, Julio Gonzalo, Anselmo Pe nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Summarization. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 280?289.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT Evaluation: Human-Like vs. Hu-
man Acceptable. In Proceedings of the Joint 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 17?
24.
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Fe-
lisa Verdejo. 2009. The contribution of linguis-
tic features to automatic machine translation evalua-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 306?314, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Chris Callison-burch and Miles Osborne. 2006. Re-
evaluating the role of bleu in machine translation re-
search. In In EACL, pages 249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53. Revised August 2010.
464
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM:
A maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55?62.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A Machine Learning Approach to the
Automatic Evaluation of Machine Translation. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 140?147.
Christopher Culy and Susanne Z. Riehemann. 2003. The
Limits of N-gram Translation Evaluation Metrics. In
Proceedings of MT-SUMMIT IX, pages 1?8.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
national Conference on Human Language Technology,
pages 138?145.
Michael Gamon, Anthony Aue, and Martine Smets.
2005. Sentence-Level MT evaluation without refer-
ence translations: beyond language modeling. In Pro-
ceedings of EAMT, pages 103?111.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguistic
Features for Automatic Evaluation of Heterogeneous
MT Systems. In Proceedings of the ACL Workshop on
Statistical Machine Translation, pages 256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Hetero-
geneous Automatic MT Evaluation Through Non-
Parametric Metric Combinations. In Proceedings of
the Third International Joint Conference on Natural
Language Processing (IJCNLP), pages 319?326.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, 1(94):77?86.
Jesu?s Gime?nez. 2008. Empirical Machine Transla-
tion and its Evaluation. Ph.D. thesis, Universitat
Polite`cnica de Catalunya.
Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.
2005. Kernel-based approach for automatic evaluation
of natural language generation technologies: Applica-
tion to automatic summarization. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 145?152, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Petr Homola, Vladislav Kubon?, and Pavel Pecina. 2009.
A simple automatic mt evaluation metric. In Proceed-
ings of the Fourth Workshop on Statistical Machine
Translation, StatMT ?09, pages 33?36, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jeremy G. Kahn, Matthew Snover, and Mari Ostendorf.
2009. Expected Dependency Pair Match: Predicting
translation quality with expected syntactic structure.
Machine Translation.
Alex Kulesza and Stuart M. Shieber. 2004. A learning
approach to improving sentence-level MT evaluation.
In Proceedings of the 10th International Conference
on Theoretical and Methodological Issues in Machine
Translation (TMI), pages 75?84.
LDC. 2005. Linguistic Data Annotation Spec-
ification: Assessment of Adequacy and Flu-
ency in Translations. Revision 1.5. Tech-
nical report, Linguistic Data Consortium.
http://www.ldc.upenn.edu/Projects/
TIDES/Translation/TransAssess04.pdf.
Audrey Le and Mark Przybocki. 2005. NIST 2005 ma-
chine translation evaluation official results. In Official
release of automatic evaluation scores for all submis-
sions, August.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT Evaluation Using Block Move-
ments. In Proceedings of 11th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics (EACL), pages 241?248.
Chin-Yew Lin and Franz Josef Och. 2004a. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguistics (ACL).
Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics
for Machine Translation. In Proceedings of the 20th
International Conference on Computational Linguis-
tics (COLING).
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Lucian Vlad Lita, Monica Rogati, and Alon Lavie. 2005.
BLANC: Learning Evaluation Metrics for MT. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing (HLT-EMNLP), pages 740?747.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for MT and/or Summarization, pages
25?32.
Ding Liu and Daniel Gildea. 2006. Stochastic Iter-
ative Alignment for Machine Translation Evaluation.
In Proceedings of the Joint 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association for Computational
Linguistics (COLING-ACL), pages 539?546.
465
Ding Liu and Daniel Gildea. 2007. Source-Language
Features and Maximum Correlation Training for Ma-
chine Translation Evaluation. In Proceedings of the
2007 Meeting of the North American Chapter of the
Association for Computational Linguistics (NAACL),
pages 41?48.
Dennis Mehay and Chris Brew. 2007. BLEUATRE:
Flattening Syntactic Dependencies for MT Evaluation.
In Proceedings of the 11th Conference on Theoreti-
cal and Methodological Issues in Machine Translation
(TMI).
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An Evaluation Tool for Machine
Translation: Fast Evaluation for MT Research. In Pro-
ceedings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC).
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007a. Dependency-Based Automatic Evalua-
tion for Machine Translation. In Proceedings of SSST,
NAACL-HLT/AMTA Workshop on Syntax and Struc-
ture in Statistical Translation, pages 80?87.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2007b. Labelled Dependencies in Machine Transla-
tion Evaluation. In Proceedings of the ACL Workshop
on Statistical Machine Translation, pages 104?111.
Karolina Owczarzak, Josef van Genabith, and Andy Way.
2008. Evaluating machine translation with lfg depen-
dencies. Machine Translation, 21(2):95?119.
Karolina Owczarzak. 2009. Depeval(summ):
dependency-based evaluation for automatic sum-
maries. In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natural
Language Processing of the AFNLP: Volume 1, pages
190?198, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Sebastian Pado?, Michael Galley, Dan Jurafsky, and
Christopher D. Manning. 2009. Robust machine
translation evaluation with entailment features. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 297?305.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001a.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, jul.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001b. Bleu: a method for automatic evalu-
ation of machine translation, RC22176. Technical re-
port, IBM T.J. Watson Research Center.
Michael Paul, Andrew Finch, and Eiichiro Sumita. 2007.
Reducing Human Assessments of Machine Transla-
tion Quality to Binary Classifiers. In Proceedings of
the 11th Conference on Theoretical and Methodologi-
cal Issues in Machine Translation (TMI).
Maja Popovic and Hermann Ney. 2007. Word Error
Rates: Decomposition over POS classes and Applica-
tions for Error Analysis. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
48?55, Prague, Czech Republic, June. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 271?279.
Chris Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Metric. In Proceedings of the
4th International Conference on Language Resources
and Evaluation (LREC), pages 825?828.
Florence Reeder, Keith Miller, Jennifer Doyon, and John
White. 2001. The Naming of Things and the Confu-
sion of Tongues: an MT Metric. In Proceedings of
the Workshop on MT Evaluation ?Who did what to
whom?? at Machine Translation Summit VIII, pages
55?59.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human Anno-
tation. In Proceedings of the 7th Conference of the
Association for Machine Translation in the Americas
(AMTA), pages 223?231.
Christoph Tillmann, Stefan Vogel, Hermann Ney, A. Zu-
biaga, and H. Sawaf. 1997. Accelerated DP based
Search for Statistical Translation. In Proceedings of
European Conference on Speech Communication and
Technology.
Stephen Tratz and Eduard Hovy. 2008. Summarization
evaluation using transformed basic elements. In In
Proceedings of TAC-08. Gaithersburg, Maryland.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003a.
Evaluation of machine translation and its evaluation.
In In Proceedings of MT Summit IX, pages 386?393.
Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003b. Evaluation of Machine Translation and its
Evaluation. In Proceedings of MT SUMMIT IX.
466
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 139?144,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Graphical Interface for MT Evaluation and Error Analysis
Meritxell Gonza`lez and Jesu?s Gime?nez and Llu??s Ma`rquez
TALP Research Center
Universitat Polite`cnica de Catalunya
{mgonzalez,jgimenez,lluism}@lsi.upc.edu
Abstract
Error analysis in machine translation is a nec-
essary step in order to investigate the strengths
and weaknesses of the MT systems under de-
velopment and allow fair comparisons among
them. This work presents an application that
shows how a set of heterogeneous automatic
metrics can be used to evaluate a test bed of
automatic translations. To do so, we have
set up an online graphical interface for the
ASIYA toolkit, a rich repository of evaluation
measures working at different linguistic lev-
els. The current implementation of the inter-
face shows constituency and dependency trees
as well as shallow syntactic and semantic an-
notations, and word alignments. The intelli-
gent visualization of the linguistic structures
used by the metrics, as well as a set of navi-
gational functionalities, may lead towards ad-
vanced methods for automatic error analysis.
1 Introduction
Evaluation methods are a key ingredient in the de-
velopment cycle of machine translation (MT) sys-
tems. As illustrated in Figure 1, they are used to
identify and analyze the system weak points (error
analysis), to introduce new improvements and adjust
the internal system parameters (system refinement),
and to measure the system performance in compari-
son to other systems or previous versions of the same
system (evaluation).
We focus here on the processes involved in the
error analysis stage in which MT developers need to
understand the output of their systems and to assess
the improvements introduced.
Automatic detection and classification of the er-
rors produced by MT systems is a challenging prob-
lem. The cause of such errors may depend not only
on the translation paradigm adopted, but also on the
language pairs, the availability of enough linguistic
resources and the performance of the linguistic pro-
cessors, among others. Several past research works
studied and defined fine-grained typologies of trans-
lation errors according to various criteria (Vilar et
al., 2006; Popovic? et al, 2006; Kirchhoff et al,
2007), which helped manual annotation and human
analysis of the systems during the MT development
cycle. Recently, the task has received increasing at-
tention towards the automatic detection, classifica-
tion and analysis of these errors, and new tools have
been made available to the community. Examples
of such tools are AMEANA (Kholy and Habash,
2011), which focuses on morphologically rich lan-
guages, and Hjerson (Popovic?, 2011), which ad-
dresses automatic error classification at lexical level.
In this work we present an online graphical inter-
face to access ASIYA, an existing software designed
to evaluate automatic translations using an heteroge-
neous set of metrics and meta-metrics. The primary
goal of the online interface is to allow MT develop-
ers to upload their test beds, obtain a large set of met-
ric scores and then, detect and analyze the errors of
their systems using just their Internet browsers. Ad-
ditionally, the graphical interface of the toolkit may
help developers to better understand the strengths
and weaknesses of the existing evaluation measures
and to support the development of further improve-
ments or even totally new evaluation metrics. This
information can be gathered both from the experi-
139
Figure 1: MT systems development cycle
ence of ASIYA?s developers and also from the statis-
tics given through the interface to the ASIYA?s users.
In the following, Section 2 gives a general
overview of the ASIYA toolkit. Section 3 describes
the variety of information gathered during the eval-
uation process, and Section 4 provides details on the
graphical interface developed to display this infor-
mation. Finally, Section 5 overviews recent work re-
lated to MT error analysis, and Section 6 concludes
and reports some ongoing and future work.
2 The ASIYA Toolkit
ASIYA is an open toolkit designed to assist devel-
opers of both MT systems and evaluation measures
by offering a rich set of metrics and meta-metrics
for assessing MT quality (Gime?nez and Ma`rquez,
2010a). Although automatic MT evaluation is still
far from manual evaluation, it is indeed necessary
to avoid the bottleneck introduced by a fully man-
ual evaluation in the system development cycle. Re-
cently, there has been empirical and theoretical justi-
fication that a combination of several metrics scoring
different aspects of translation quality should corre-
late better with humans than just a single automatic
metric (Amigo? et al, 2011; Gime?nez and Ma`rquez,
2010b).
ASIYA offers more than 500 metric variants for
MT evaluation, including the latest versions of the
most popular measures. These metrics rely on dif-
ferent similarity principles (such as precision, recall
and overlap) and operate at different linguistic layers
(from lexical to syntactic and semantic). A general
classification based on the similarity type is given
below along with a brief summary of the informa-
tion they use and the names of a few examples1.
Lexical similarity: n-gram similarity and edit dis-
tance based on word forms (e.g., PER, TER,
WER, BLEU, NIST, GTM, METEOR).
Syntactic similarity: based on part-of-speech tags,
base phrase chunks, and dependency and con-
stituency trees (e.g., SP-Overlap-POS, SP-
Overlap-Chunk, DP-HWCM, CP-STM).
Semantic similarity: based on named entities, se-
mantic roles and discourse representation (e.g.,
NE-Overlap, SR-Overlap, DRS-Overlap).
Such heterogeneous set of metrics allow the user
to analyze diverse aspects of translation quality at
system, document and sentence levels. As discussed
in (Gime?nez and Ma`rquez, 2008), the widely used
lexical-based measures should be considered care-
fully at sentence level, as they tend to penalize trans-
lations using different lexical selection. The combi-
nation with complex metrics, more focused on ad-
equacy aspects of the translation (e.g., taking into
account also semantic information), should help re-
ducing this problem.
3 The Metric-dependent Information
ASIYA operates over a fixed set of translation test
cases, i.e., a source text, a set of candidate trans-
lations and a set of manually produced reference
translations. To run ASIYA the user must provide
a test case and select the preferred set of metrics
(it may depend on the evaluation purpose). Then,
ASIYA outputs complete tables of score values for
all the possible combination of metrics, systems,
documents and segments. This kind of results is
valuable for rapid evaluation and ranking of trans-
lations and systems. However, it is unfriendly for
MT developers that need to manually analyze and
compare specific aspects of their systems.
During the evaluation process, ASIYA generates
a number of intermediate analysis containing par-
tial work outs of the evaluation measures. These
data constitute a priceless source for analysis pur-
poses since a close examination of their content al-
lows for analyzing the particular characteristics that
1A more detailed description of the metric set and its imple-
mentation can be found in (Gime?nez and Ma`rquez, 2010b).
140
Reference The remote control of the Wii
helps to diagnose an infantile
ocular disease .
Ol score
Candidate 1 The Wii Remote to help diag-
nose childhood eye disease .
7
17 = 0.41
Candidate 2 The control of the Wii helps
to diagnose an ocular infantile
disease .
13
14 = 0.93
Table 1: The reference sentence, two candidate
translation examples and the Ol scores calculation
differentiate the score values obtained by each can-
didate translation.
Next, we review the type of information used by
each family of measures according to their classifi-
cation, and how this information can be used for MT
error analysis purposes.
Lexical information. There are several variants un-
der this family. For instance, lexical overlap (Ol)
is an F-measure based metric, which computes sim-
ilarity roughly using the Jaccard coefficient. First,
the sets of all lexical items that are found in the ref-
erence and the candidate sentences are considered.
Then, Ol is computed as the cardinality of their in-
tersection divided by the cardinality of their union.
The example in Table 1 shows the counts used to cal-
culate Ol between the reference and two candidate
translations (boldface and underline indicate non-
matched items in candidate 1 and 2, respectively).
Similarly, metrics in another category measure the
edit distance of a translation, i.e., the number of
word insertions, deletions and substitutions that are
needed to convert a candidate translation into a ref-
erence. From the algorithms used to calculate these
metrics, these words can be identified in the set of
sentences and marked for further processing. On
another front, metrics as BLEU or NIST compute
a weighted average of matching n-grams. An inter-
esting information that can be obtained from these
metrics are the weights assigned to each individual
matching n-gram. Variations of all of these mea-
sures include looking at stems, synonyms and para-
phrases, instead of the actual words in the sentences.
This information can be obtained from the imple-
mentation of the metrics and presented to the user
through the graphical interface.
Syntactic information. ASIYA considers three lev-
els of syntactic information: shallow, constituent
and dependency parsing. The shallow parsing an-
notations, that are obtained from the linguistic pro-
cessors, consist of word level part-of-speech, lem-
mas and chunk Begin-Inside-Outside labels. Use-
ful figures such as the matching rate of a given
(sub)category of items are the base of a group of
metrics (i.e., the ratio of prepositions between a
reference and a candidate). In addition, depen-
dency and constituency parse trees allow for captur-
ing other aspects of the translations. For instance,
DP-HCWM is a specific subset of the dependency
measures that consists of retrieving and matching all
the head-word chains (or the ones of a given length)
from the dependency trees. Similarly, CP-STM, a
subset of the constituency parsing family of mea-
sures, consists of computing the lexical overlap ac-
cording to the phrase constituent of a given type.
Then, for error analysis purposes, parse trees com-
bine the grammatical relations and the grammati-
cal categories of the words in the sentence and dis-
play the information they contain. Figure 2 and 3
show, respectively, several annotation levels of the
sentences in the example and the constituency trees.
Semantic information. ASIYA distinguishes also
three levels of semantic information: named enti-
ties, semantic roles and discourse representations.
The former are post-processed similarly to the lex-
ical annotations discussed above; and the semantic
predicate-argument trees are post-processed and dis-
played in a similar manner to the syntactic trees.
Instead, the purpose of the discourse representation
analysis is to evaluate candidate translations at doc-
ument level. In the nested discourse structures we
could identify the lexical choices for each discourse
sub-type. Presenting this information to the user re-
mains as an important part of the future work.
4 The Graphical Interface
This section presents the web application that makes
possible a graphical visualization and interactive ac-
cess to ASIYA. The purpose of the interface is
twofold. First, it has been designed to facilitate the
use of the ASIYA toolkit for rapid evaluation of test
beds. And second, we aim at aiding the analysis of
the errors produced by the MT systems by creating
141
Figure 2: PoS, chunk and named entity annota-
tions on the source, reference and two translation
hypotheses
Figure 3: Constituency trees for the reference and
second translation candidate
a significant visualization of the information related
to the evaluation metrics.
The online interface consists of a simple web form
to supply the data required to run ASIYA, and then,
it offers several views that display the results in
friendly and flexible ways such as interactive score
tables, graphical parsing trees in SVG format and
interactive sentences holding the linguistic annota-
tions captured during the computation of the met-
rics, as described in Section 3.
4.1 Online MT evaluation
ASIYA allows to compute scores at three granular-
ity levels: system (entire test corpus), document and
sentence (or segment). The online application ob-
tains the measures for all the metrics and levels and
generates an interactive table of scores displaying
the values for all the measures. Table organiza-
Figure 4: The bar charts plot to compare the metric
scores for several systems
tion can swap among the three levels of granularity,
and it can also be transposed with respect to sys-
tem and metric information (transposing rows and
columns). When the metric basis table is shown, the
user can select one or more metric columns in or-
der to re-rank the rows accordingly. Moreover, the
source, reference and candidate translation are dis-
played along with metric scores. The combination of
all these functionalities makes it easy to know which
are the highest/lowest-scored sentences in a test set.
We have also integrated a graphical library2 to
generate real-time interactive plots to show the met-
ric scores graphically. The current version of the in-
terface shows interactive bar charts, where different
metrics and systems can be combined in the same
plot. An example is shown in Figure 4.
4.2 Graphically-aided Error Analysis and
Diagnosis
Human analysis is crucial in the development cy-
cle because humans have the capability to spot er-
rors and analyze them subjectively, in relation to the
underlying system that is being examined and the
scores obtained. Our purpose, as mentioned previ-
ously, is to generate a graphical representation of
the information related to the source and the trans-
lations, enabling a visual analysis of the errors. We
have focused on the linguistic measures at the syn-
tactic and semantic level, since they are more robust
than lexical metrics when comparing systems based
on different paradigms. On the one hand, one of
the views of the interface allows a user to navigate
and inspect the segments of the test set. This view
highlights the elements in the sentences that match a
2http://www.highcharts.com/
142
given criteria based on the various linguistic annota-
tions aforementioned (e.g., PoS prepositions). The
interface integrates also the mechanisms to upload
word-by-word alignments between the source and
any of the candidates. The alignments are also vi-
sualized along with the rest of the annotations, and
they can be also used to calculate artificial annota-
tions projected from the source in such test beds for
which there is no linguistic processors available. On
the other hand, the web application includes a library
for SVG graph generation in order to create the de-
pendency and the constituent trees dynamically (as
shown in Figure 3).
4.3 Accessing the Demo
The online interface is fully functional and accessi-
ble at http://nlp.lsi.upc.edu/asiya/. Al-
though the ASIYA toolkit is not difficult to install,
some specific technical skills are still needed in or-
der to set up all its capabilities (i.e., external com-
ponents and resources such as linguistic processors
and dictionaries). Instead, the online application re-
quires only an up to date browser. The website in-
cludes a tarball with sample input data and a video
recording, which demonstrates the main functional-
ities of the interface and how to use it.
The current web-based interface allows the user
to upload up to five candidate translation files, five
reference files and one source file (maximum size of
200K each, which is enough for test bed of about
1K sentences). Alternatively, the command based
version of ASIYA can be used to intensively evaluate
a large set of data.
5 Related Work
In the literature, we can find detailed typologies of
the errors produced by MT systems (Vilar et al,
2006; Farru?s et al, 2011; Kirchhoff et al, 2007) and
graphical interfaces for human classification and an-
notation of these errors, such as BLAST (Stymne,
2011). They represent a framework to study the
performance of MT systems and develop further re-
finements. However, they are defined for a specific
pair of languages or domain and they are difficult
to generalize. For instance, the study described in
(Kirchhoff et al, 2007) focus on measures relying on
the characterization of the input documents (source,
genre, style, dialect). In contrast, Farru?s et al (2011)
classify the errors that arise during Spanish-Catalan
translation at several levels: orthographic, morpho-
logical, lexical, semantic and syntactic errors.
Works towards the automatic identification and
classification of errors have been conducted very re-
cently. Examples of these are (Fishel et al, 2011),
which focus on the detection and classification of
common lexical errors and misplaced words using
a specialized alignment algorithm; and (Popovic?
and Ney, 2011), which addresses the classifica-
tion of inflectional errors, word reordering, missing
words, extra words and incorrect lexical choices us-
ing a combination of WER, PER, RPER and HPER
scores. The AMEANA tool (Kholy and Habash,
2011) uses alignments to produce detailed morpho-
logical error diagnosis and generates statistics at dif-
ferent linguistic levels. To the best of our knowl-
edge, the existing approaches to automatic error
classification are centered on the lexical, morpho-
logical and shallow syntactic aspects of the transla-
tion, i.e., word deletion, insertion and substitution,
wrong inflections, wrong lexical choice and part-
of-speech. In contrast, we introduce additional lin-
guistic information, such as dependency and con-
stituent parsing trees, discourse structures and se-
mantic roles. Also, there exist very few tools de-
voted to visualize the errors produced by the MT
systems. Here, instead of dealing with the automatic
classification of errors, we deal with the automatic
selection and visualization of the information used
by the evaluation measures.
6 Conclusions and Future Work
The main goal of the ASIYA toolkit is to cover the
evaluation needs of researchers during the develop-
ment cycle of their systems. ASIYA generates a
number of linguistic analyses over both the candi-
date and the reference translations. However, the
current command-line interface returns the results
only in text mode and does not allow for fully ex-
ploiting this linguistic information. We present a
graphical interface showing a visual representation
of such data for monitoring the MT development cy-
cle. We believe that it would be very helpful for car-
rying out tasks such as error analysis, system com-
parison and graphical representations.
143
The application described here is the first release
of a web interface to access ASIYA online. So
far, it includes the mechanisms to analyze 4 out of
10 categories of metrics: shallow parsing, depen-
dency parsing, constituent parsing and named en-
tities. Nonetheless, we aim at developing the sys-
tem until we cover all the metric categories currently
available in ASIYA.
Regarding the analysis of the sentences, we have
conducted a small experiment to show the ability of
the interface to use word level alignments between
the source and the target sentences. In the near fu-
ture, we will include the mechanisms to upload also
phrase level alignments. This functionality will also
give the chance to develop a new family of evalua-
tion metrics based on these alignments.
Regarding the interactive aspects of the interface,
the grammatical graphs are dynamically generated
in SVG format, which proffers a wide range of inter-
active functionalities. However their interactivity is
still limited. Further development towards improved
interaction would provide a more advanced manip-
ulation of the content, e.g., selection, expansion and
collapse of branches.
Concerning the usability of the interface, we will
add an alternative form for text input, which will re-
quire users to input the source, reference and candi-
date translation directly without formatting them in
files, saving a lot of effort when users need to ana-
lyze the translation results of one single sentence.
Finally, in order to improve error analysis capa-
bilities, we will endow the application with a search
engine able to filter the results according to varied
user defined criteria. The main goal is to provide
the mechanisms to select a case set where, for in-
stance, all the sentences are scored above (or below)
a threshold for a given metric (or a subset of them).
Acknowledgments
This research has been partially funded by the Span-
ish Ministry of Education and Science (OpenMT-
2, TIN2009-14675-C03) and the European Commu-
nity?s Seventh Framework Programme under grant
agreement numbers 247762 (FAUST project, FP7-
ICT-2009- 4-247762) and 247914 (MOLTO project,
FP7-ICT-2009-4- 247914).
References
Enrique Amigo?, Julio Gonzalo, Jesu?s Gime?nez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proc. of the
EMNLP, Edinburgh, UK, pages 455?466.
Mireia Farru?s, Marta R. Costa-Jussa`, Jose? B. Marin?o,
Marc Poch, Adolfo Herna?ndez, Carlos Henr??quez, and
Jose? A. Fonollosa. 2011. Overcoming Statistical Ma-
chine Translation Limitations: Error Analysis and Pro-
posed Solutions for the Catalan?Spanish Language
Pair. LREC, 45(2):181?208.
Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and Jan Berka.
2011. Automatic Translation Error Analysis. In Proc.
of the 14th TSD, volume LNAI 3658. Springer Verlag.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. Towards Het-
erogeneous Automatic MT Error Analysis. In Proc. of
LREC, Marrakech, Morocco.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77?86.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):77?86.
Ahmed El Kholy and Nizar Habash. 2011. Automatic
Error Analysis for Morphologically Rich Languages.
In Proc. of the MT Summit XIII, Xiamen, China, pages
225?232.
Katrin Kirchhoff, Owen Rambow, Nizar Habash, and
Mona Diab. 2007. Semi-Automatic Error Analysis for
Large-Scale Statistical Machine Translation Systems.
In Proc. of the MT Summit XI, Copenhagen, Denmark.
Maja Popovic? and Hermann Ney. 2011. Towards Auto-
matic Error Analysis of Machine Translation Output.
Computational Linguistics, 37(4):657?688.
Maja Popovic?, Hermann Ney, Adria` de Gispert, Jose? B.
Marin?o, Deepa Gupta, Marcello Federico, Patrik Lam-
bert, and Rafael Banchs. 2006. Morpho-Syntactic
Information for Automatic Error Analysis of Statisti-
cal Machine Translation Output. In Proc. of the SMT
Workshop, pages 1?6, New York City, USA. ACL.
Maja Popovic?. 2011. Hjerson: An Open Source Tool
for Automatic Error Classification of Machine Trans-
lation Output. The Prague Bulletin of Mathematical
Linguistics, 96:59?68.
Sara Stymne. 2011. Blast: a Tool for Error Analysis of
Machine Translation Output. In Proc. of the 49th ACL,
HLT, Systems Demonstrations, pages 56?61.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proc. of the LREC, pages 697?702,
Genoa, Italy.
144
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 333?338,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Document-level Automatic MT Evaluation
based on Discourse Representations
Jesu?s Gime?nez and
Llu??s Ma`rquez
TALP UPC
Barcelona, Spain
{jgimenez, lluism}
@lsi.upc.edu
Elisabet Comelles and
Irene Castello?n
Universitat de Barcelona
Barcelona, Spain
{elicomelles,
icastellon} @ub.edu
Victoria Arranz
ELDA/ELRA
Paris, France
arranz@elda.org
Abstract
This paper describes the joint submission of
Universitat Polite`cnica de Catalunya and Uni-
versitat de Barcelona to the Metrics MaTr
2010 evaluation challenge, in collaboration with
ELDA/ELRA. Our work is aimed at widening the
scope of current automatic evaluation measures
from sentence to document level. Preliminary ex-
periments, based on an extension of the metrics by
Gime?nez and Ma`rquez (2009) operating over dis-
course representations, are presented.
1 Introduction
Current automatic similarity measures for Ma-
chine Translation (MT) evaluation operate all,
without exception, at the segment level. Trans-
lations are analyzed on a segment-by-segment1
fashion, ignoring the text structure. Document
and system scores are obtained using aggregate
statistics over individual segments. This strategy
presents the main disadvantage of ignoring cross-
sentential/discursive phenomena.
In this work we suggest widening the scope
of evaluation methods. We have defined genuine
document-level measures which are able to ex-
ploit the structure of text to provide more informed
evaluation scores. For that purpose we take advan-
tage of two coincidental facts. First, test beds em-
ployed in recent MT evaluation campaigns include
a document structure grouping sentences related
to the same event, story or topic (Przybocki et al,
2008; Przybocki et al, 2009; Callison-Burch et al,
2009). Second, we count on automatic linguistic
processors which provide very detailed discourse-
level representations of text (Curran et al, 2007).
Discourse representations allow us to focus on
relevant pieces of information, such as the agent
1A segment typically consists of one or two sentences.
(who), location (where), time (when), and theme
(what), which may be spread all over the text.
Counting on a means of discerning the events, the
individuals taking part in each of them, and their
role, is crucial to determine the semantic equiva-
lence between a reference document and a candi-
date translation.
Moreover, the discourse analysis of a document
is not a mere concatenation of the analyses of its
individual sentences. There are some phenom-
ena which may go beyond the scope of a sen-
tence and can only be explained within the con-
text of the whole document. For instance, in a
newspaper article, facts and entities are progres-
sively added to the discourse and then referred
to anaphorically later on. The following extract
from the development set illustrates the impor-
tance of such a phenomenon in the discourse anal-
ysis: ?Among the current or underlying crises in
the Middle East, Rod Larsen mentioned the Arab-
Israeli conflict and the Iranian nuclear portfolio,
as well as the crisis between Lebanon and Syria.
He stated: ?All this leads us back to crucial val-
ues and opinions, which render the situation prone
at any moment to getting out of control, more so
than it was in past days.??. The subject pronoun
?he? works as an anaphoric pronoun whose an-
tecedent is the proper noun ?Rod Larson?. The
anaphoric relation established between these two
elements can only be identified by analyzing the
text as a whole, thus considering the gender agree-
ment between the third person singular masculine
subject pronoun ?he? and the masculine proper
noun ?Rod Larson?. However, if the two sen-
tences were analyzed separately, the identification
of this anaphoric relation would not be feasible
due to the lack of connection between the two ele-
ments. Discourse representations allow us to trace
links across sentences between the different facts
and entities appearing in them. Therefore, provid-
ing an approach to the text more similar to that of
333
a human, which implies taking into account the
whole text structure instead of considering each
sentence separately.
The rest of the paper is organized as follows.
Section 2 describes our evaluation methods and
the linguistic theory upon which they are based.
Experimental results are reported and discussed in
Section 3. Section 4 presents the metric submitted
to the evaluation challenge. Future work is out-
lined in Section 5.
As an additional result, document-level metrics
generated in this study have been incorporated to
the IQMT package for automatic MT evaluation2.
2 Metric Description
This section provides a brief description of our ap-
proach. First, in Section 2.1, we describe the un-
derlying theory and give examples on its capabili-
ties. Then, in Section 2.2, we describe the associ-
ated similarity measures.
2.1 Discourse Representations
As previously mentioned in Section 1, a document
has some features which need to be analyzed con-
sidering it as a whole instead of dividing it up
into sentences. The anaphoric relation between
a subject pronoun and a proper noun has already
been exemplified. However, this is not the only
anaphoric relation which can be found inside a
text, there are some others which are worth men-
tioning:
? the connection between a possessive adjec-
tive and a proper noun or a subject pro-
noun, as exemplified in the sentences ?Maria
bought a new sweater. Her new sweater is
blue.?, where the possessive feminine adjec-
tive ?her? refers to the proper noun ?Maria?.
? the link between a demonstrative pronoun
and its referent, which is exemplified in the
sentences ?He developed a new theory on
grammar. However, this is not the only the-
ory he developed?. In the second sentence,
the demonstrative pronoun ?this? refers back
to the noun phrase ?new theory on grammar?
which occurs in the previous sentence.
? the relation between a main verb and an aux-
iliary verb in certain contexts, as illustrated in
the following pair of sentences ?Would you
2http://www.lsi.upc.edu/
?
nlp/IQMT
like more sugar? Yes, I would?. In this ex-
ample, the auxiliary verb ?would? used in
the short answer substitutes the verb phrase
?would like?.
In addition to anaphoric relations, other features
need to be highlighted, such as the use of discourse
markers which help to give cohesion to the text,
link parts of a discourse and show the relations es-
tablished between them. Below, some examples
are given:
? ?Moreover?, ?Furthermore?, ?In addition?
indicate that the upcoming sentence adds
more information.
? ?However?, ?Nonetheless?, ?Nevertheless?
show contrast with previous ideas.
? ?Therefore?, ?As a result?, ?Consequently?
show a cause and effect relation.
? ?For instance?, ?For example? clarify or il-
lustrate the previous idea.
It is worth noticing that anaphora, as well as dis-
course markers, are key features in the interface
between syntax, semantics and pragmatics. Thus,
when dealing with these phenomena at a text level
we are not just looking separately at the different
language levels, but we are trying to give a com-
plete representation of both the surface and the
deep structures of a text.
2.2 Definition of Similarity Measures
In this work, as a first proposal, instead of elabo-
rating on novel similarity measures, we have bor-
rowed and extended the Discourse Representation
(DR) metrics defined by Gime?nez and Ma`rquez
(2009). These metrics analyze similarities be-
tween automatic and reference translations by
comparing their respective discourse representa-
tions over individual sentences.
For the discursive analysis of texts, DR met-
rics rely on the C&C Tools (Curran et al, 2007),
specifically on the Boxer component (Bos, 2008).
This software is based on the Discourse Represen-
tation Theory (DRT) by Kamp and Reyle (1993).
DRT is a theoretical framework offering a rep-
resentation language for the examination of con-
textually dependent meaning in discourse. A dis-
course is represented in a discourse representation
structure (DRS), which is essentially a variation of
first-order predicate calculus ?its forms are pairs
334
of first-order formulae and the free variables that
occur in them.
DRSs are viewed as semantic trees, built
through the application of two types of DRS con-
ditions:
basic conditions: one-place properties (pred-
icates), two-place properties (relations),
named entities, time-expressions, cardinal
expressions and equalities.
complex conditions: disjunction, implication,
negation, question, and propositional attitude
operations.
For instance, the DRS representation for the
sentence ?Every man loves Mary.? is as follows:
?y named(y,mary, per) ? (?x man(x) ?
?z love(z) ? event(z) ? agent(z, x) ?
patient(z, y)). DR integrates three different
kinds of metrics:
DR-STM These metrics are similar to the Syntac-
tic Tree Matching metric defined by Liu and
Gildea (2005), in this case applied to DRSs
instead of constituent trees. All semantic sub-
paths in the candidate and reference trees are
retrieved. The fraction of matching subpaths
of a given length (l=4 in our experiments) is
computed.
DR-Or(?) Average lexical overlap between dis-
course representation structures of the same
type. Overlap is measured according to the
formulae and definitions by Gime?nez and
Ma`rquez (2007).
DR-Orp(?) Average morphosyntactic overlap,
i.e., between grammatical categories ?parts-
of-speech? associated to lexical items, be-
tween discourse representation structures of
the same type.
We have extended these metrics to operate at
document level. For that purpose, instead of run-
ning the C&C Tools in a sentence-by-sentence
fashion, we run them document by document.
This is as simple as introducing a ?<META>? tag
at the beginning of each document to denote doc-
ument boundaries3 .
3Details on the advanced use of Boxer are avail-
able at http://svn.ask.it.usyd.edu.au/trac/
candc/wiki/BoxerComplex.
3 Experimental Work
In this section, we analyze the behavior of the new
DR metrics operating at document level with re-
spect to their sentence-level counterparts.
3.1 Settings
We have used the ?mt06? part of the development
set provided by the Metrics MaTr 2010 organiza-
tion, which corresponds to a subset of 25 docu-
ments from the NIST 2006 Open MT Evaluation
Campaign Arabic-to-English translation. The to-
tal number of segments is 249. The average num-
ber of segments per document is, thus, 9.96. The
number of segments per document varies between
2 and 30. For the purpose of automatic evaluation,
4 human reference translations and automatic out-
puts by 8 different MT systems are available. In
addition, we count on the results of a process of
manual evaluation. Each translation segment was
assessed by two judges. After independently and
completely assessing the entire set, the judges re-
viewed their individual assessments together and
settled on a single final score. Average system ad-
equacy is 5.38.
In our experiments, metrics are evaluated in
terms of their correlation with human assess-
ments. We have computed Pearson, Spearman
and Kendall correlation coefficients between met-
ric scores and adequacy assessments. Document-
level and system-level assessments have been ob-
tained by averaging over segment-level assess-
ments. We have computed correlation coefficients
and confidence intervals applying bootstrap re-
sampling at a 99% statistical significance (Efron
and Tibshirani, 1986; Koehn, 2004). Since the
cost of exhaustive resampling was prohibitive, we
have limited to 1,000 resamplings. Confidence in-
tervals, not shown in the tables, are in all cases
lower than 10?3.
3.2 Metric Performance
Table 1 shows correlation coefficients at the docu-
ment level for several DR metric representatives,
and their document-level counterparts (DRdoc).
For the sake of comparison, the performance of
the METEOR metric is also reported4.
Contrary to our expectations, DRdoc variants
obtain lower levels of correlation than their DR
4We have used METEOR version 1.0 with default param-
eters optimized by its developers over adequacy and fluency
assessments. The METEOR metric is publicly available at
http://www.cs.cmu.edu/
?
alavie/METEOR/
335
Metric Pearson? Spearman? Kendall?
METEOR 0.9182 0.8478 0.6728
DR-Or(?) 0.8567 0.8061 0.6193
DR-Orp(?) 0.8286 0.7790 0.5875
DR-STM 0.7880 0.7468 0.5554
DRdoc-Or(?) 0.7936 0.7784 0.5875
DRdoc-Orp(?) 0.7219 0.6737 0.4929
DRdoc-STM 0.7553 0.7421 0.5458
Table 1: Meta-evaluation results at document level
Metric Pearson? Spearman? Kendall?
METEOR 0.9669 0.9151 0.8533
DR-Or(?) 0.9100 0.6549 0.5764
DR-Orp(?) 0.9471 0.7918 0.7261
DR-STM 0.9295 0.7676 0.7165
DRdoc-Or(?) 0.9534 0.8434 0.7828
DRdoc-Orp(?) 0.9595 0.9101 0.8518
DRdoc-STM 0.9676 0.9655 0.9272
DR-Or(?)? 0.9836 0.9594 0.9296
DR-Orp(?)? 0.9959 1.0000 1.0000
DR-STM? 0.9933 0.9634 0.9307
Table 2: Meta-evaluation results at system level
counterparts. There are three different factors
which could provide a possible explanation for
this negative result. First, the C&C Tools, like any
other automatic linguistic processor are not per-
fect. Parsing errors could be causing the metric
to confer less informed scores. This is especially
relevant taking into account that candidate transla-
tions are not always well-formed. Secondly, we
argue that the way in which we have obtained
document-level quality assessments, as an average
of segment-level assessments, may be biasing the
correlation. Thirdly, perhaps the similarity mea-
sures employed are not able to take advantage of
the document-level features provided by the dis-
course analysis. In the following subsection we
show some error analysis we have conducted by
inspecting particular cases.
Table 2 shows correlation coefficients at system
level. In the case of DR and DRdoc metrics, sys-
tem scores are computed by simple average over
individual documents. Interestingly, in this case
DRdoc variants seem to obtain higher correlation
than their DR counterparts. The improvement is
especially substantial in terms of Spearman and
Kendall coefficients, which do not consider ab-
solute values but ranking positions. However, it
could be the case that it was just an average ef-
fect. While DR metrics compute system scores as
an average of segment scores, DRdoc metrics av-
erage directly document scores. In order to clarify
this result, we have modified DR metrics so as to
compute system scores as an average of document
scores (DR? variants, the last three rows in the ta-
ble). It can be observed that DR? variants out-
perform their DRdoc counterparts, thus confirming
our suspicion about the averaging effect.
3.3 Analysis
It is worth noting that DRdoc metrics are able to
detect and deal with several linguistic phenomena
related to both syntax and semantics at sentence
and document level. Below, several examples il-
lustrating the potential of this metric are presented.
Control structures. Control structures (either
subject or object control) are always a
difficult issue as they mix both syntactic and
semantic knowledge. In Example 1 a couple
of control structures must be identified
and DRdoc metrics deal correctly with the
argument structure of all the verbs involved.
Thus, in the first part of the sentence, a
subject control verb can be identified being
?the minister? the agent of both verb forms
?go? and ?say?. On the other hand, in the
336
quoted question, the verb ?invite? works as
an object control verb because its patient
?Chechen representatives? is also the agent
of the verb visit.
Example 1: The minister went on to say,
?What would Moscow say if we were to invite
Chechen representatives to visit Jerusalem??
Anaphora and pronoun resolution. Whenever
there is a pronoun whose antecedent is a
named entity (NE), the metric identifies
correctly its antecedent. This feature is
highly valuable because a relationship be-
tween syntax and semantics is established.
Moreover, when dealing with Semantic
Roles the roles of Agent or Patient are given
to the antecedents instead of the pronouns.
Thus, in Example 2 the antecedent of the
relative pronoun ?who? is the NE ?Putin?
and the patient of the verb ?classified? is
also the NE ?Putin? instead of the relative
pronoun ?who?.
Example 2: Putin, who was not classified
as his country Hamas as ?terrorist organiza-
tions?, recently said that the European Union
is ?a big mistake? if it decided to suspend fi-
nancial aid to the Palestinians.
Nevertheless, although Boxer was expected
to deal with long-distance anaphoric relations
beyond the sentence, after analyzing several
cases, results show that it did not succeed in
capturing this type of relations as shown in
Example 3. In this example, the antecedent
of the pronoun ?he? in the second sentence
is the NE ?Roberto Calderoli? which ap-
pears in the first sentence. DRdoc metrics
should be capable of showing this connec-
tion. However, although the proper noun
?Roberto Calderoli? is identified as a NE, it
does not share the same reference as the third
person singular pronoun ?he?.
Example 3: Roberto Calderoli does not in-
tend to apologize. The newspaper Corriere
Della Sera reported today, Saturday, that
he said ?I don?t feel responsible for those
deaths.?
4 Our Submission
Instead of participating with individual metrics,
we have combined them by averaging their scores
as described in (Gime?nez and Ma`rquez, 2008).
This strategy has proven as an effective means of
combining the scores conferred by different met-
rics (Callison-Burch et al, 2008; Callison-Burch
et al, 2009). Metrics submitted are:
DRdoc an arithmetic mean over a heuristically-
defined set of DRdoc metric variants, respec-
tively computing lexical overlap, morphosyn-
tactic overlap, and semantic tree match-
ing (M = {?DRdoc-Or(?)?, ?DRdoc-Orp(?)?, ?DRdoc-
STM4?}). Since DRdoc metrics do not operate
over individual segments, we have assigned
each segment the score of the document in
which it is contained.
DR a measure analog to DRdoc but using the de-
fault version of DR metrics operating at the
segment level (M = {?DR-Or(?)?, ?DR-Orp(?)?,
?DR-STM4?}).
ULCh an arithmetic mean over a heuristically-
defined set of metrics operating at differ-
ent linguistic levels, including lexical met-
rics, and measures of overlap between con-
stituent parses, dependency parses, seman-
tic roles, and discourse representations (M =
{?ROUGEW ?, ?METEOR?, ?DP-HWCr?, ?DP-Oc(?)?,
?DP-Ol(?)?, ?DP-Or(?)?, ?CP-STM4?, ?SR-Or(?)?,
?SR-Orv?, ?DR-Orp(?)?}). This metric corre-
sponds exactly to the metric submitted in our
previous participation.
The performance of these metrics at the docu-
ment and system levels is shown in Table 3.
5 Conclusions and Future Work
We have presented a modified version of the DR
metrics by Gime?nez and Ma`rquez (2009) which,
instead of limiting their scope to the segment level,
are able to capture and exploit document-level fea-
tures. However, results in terms of correlation
with human assessments have not reported any im-
provement of these metrics over their sentence-
level counterparts as document and system quality
predictors. It must be clarified whether the prob-
lem is on the side of the linguistic tools, in the
similarity measure, or in the way in which we have
built document-level human assessments.
For future work, we plan to continue the er-
ror analysis to clarify why DRdoc metrics do not
outperform their DR counterparts at the document
level, and how to improve their behavior. This
337
Document level System level
Metric Pearson? Spearman? Kendall? Pearson? Spearman? Kendall?
ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145
ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435
ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638
Table 3: Meta-evaluation results at document and system level for submitted metrics
may imply defining new metrics possibly using
alternative linguistic processors. In addition, we
plan to work on the identification and analysis
of discourse markers. Finally, we plan to repeat
this experiment over other test beds with docu-
ment structure, such as those from the 2009 Work-
shop on Statistical Machine Translation shared
task (Callison-Burch et al, 2009) and the 2009
NIST MT Evaluation Campaign (Przybocki et al,
2009). In the case that document-level assess-
ments are not provided, we will also explore the
possibility of producing them ourselves.
Acknowledgments
This work has been partially funded by the
Spanish Government (projects OpenMT-2,
TIN2009-14675-C03, and KNOW, TIN-2009-
14715-C0403) and the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under grant agreement numbers 247762 (FAUST
project, FP7-ICT-2009-4-247762) and 247914
(MOLTO project, FP7-ICT-2009-4-247914). We
are also thankful to anonymous reviewers for their
comments and suggestions.
References
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277?286. College Publi-
cations.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 70?106.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically motivated large-scale nlp with c&c
and boxer. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 33?36.
Bradley Efron and Robert Tibshirani. 1986. Bootstrap
Methods for Standard Errors, Confidence Intervals,
and Other Measures of Statistical Accuracy. Statis-
tical Science, 1(1):54?77.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2007. Linguis-
tic Features for Automatic Evaluation of Heteroge-
neous MT Systems. In Proceedings of the ACL
Workshop on Statistical Machine Translation, pages
256?264.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2008. A Smor-
gasbord of Features for Automatic MT Evaluation.
In Proceedings of the Third Workshop on Statistical
Machine Translation, pages 195?198.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2009. On the Ro-
bustness of Syntactic and Semantic Features for Au-
tomatic MT Evaluation. In Proceedings of the 4th
Workshop on Statistical Machine Translation (EACL
2009).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic: An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Dordrecht: Kluwer.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 388?395.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Proceed-
ings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization,
pages 25?32.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2008. NIST Metrics for Machine Translation
2008 Evaluation (MetricsMATR08). Technical re-
port, National Institute of Standards and Technol-
ogy.
Mark Przybocki, Kay Peterson, and Se?bastien Bron-
sart. 2009. NIST Open Machine Translation 2009
Evaluation (MT09). Technical report, National In-
stitute of Standards and Technology.
338
