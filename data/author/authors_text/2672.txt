Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 398?401,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNIBA: JIGSAW algorithm for Word Sense Disambiguation
P. Basile and M. de Gemmis and A.L. Gentile and P. Lops and G. Semeraro
Department of Computer Science - University of Bari - Via E. Orabona, 4 70125 Bari ITALY
{basilepp, degemmis, al.gentile, lops, semeraro}@di.uniba.it
Abstract
Word Sense Disambiguation (WSD) is tra-
ditionally considered an AI-hard problem.
A breakthrough in this field would have a
significant impact on many relevant web-
based applications, such as information re-
trieval and information extraction. This pa-
per describes JIGSAW, a knowledge-based
WSD system that attemps to disambiguate
all words in a text by exploiting WordNet1
senses. The main assumption is that a spe-
cific strategy for each Part-Of-Speech (POS)
is better than a single strategy. We evalu-
ated the accuracy of JIGSAW on SemEval-
2007 task 1 competition2. This task is an
application-driven one, where the applica-
tion is a fixed cross-lingual information re-
trieval system. Participants disambiguate
text by assigning WordNet synsets, then the
system has to do the expansion to other lan-
guages, index the expanded documents and
run the retrieval for all the languages in
batch. The retrieval results are taken as a
measure for the effectiveness of the disam-
biguation.
1 The JIGSAW algorithm
The goal of a WSD algorithm consists in assigning
a word w
i
occurring in a document d with its appro-
priate meaning or sense s, by exploiting the context
C in where w
i
is found. The context C for w
i
is de-
fined as a set of words that precede and follow w
i
.
The sense s is selected from a predefined set of pos-
sibilities, usually known as sense inventory. In the
proposed algorithm, the sense inventory is obtained
from WordNet 1.6, according to SemEval-2007 task
1 instructions. JIGSAW is a WSD algorithm based
on the idea of combining three different strategies to
disambiguate nouns, verbs, adjectives and adverbs.
The main motivation behind our approach is that
1http://wordnet.princeton.edu/
2http://www.senseval.org/
the effectiveness of a WSD algorithm is strongly
influenced by the POS tag of the target word. An
adaptation of Lesk dictionary-based WSD algorithm
has been used to disambiguate adjectives and ad-
verbs (Banerjee and Pedersen, 2002), an adaptation
of the Resnik algorithm has been used to disam-
biguate nouns (Resnik, 1995), while the algorithm
we developed for disambiguating verbs exploits the
nouns in the context of the verb as well as the nouns
both in the glosses and in the phrases that WordNet
utilizes to describe the usage of a verb. JIGSAW
takes as input a document d = {w
1
, w
2
, . . . , w
h
} and
returns a list of WordNet synsets X = {s
1
, s
2
, . . . ,
s
k
} in which each element s
i
is obtained by disam-
biguating the target word w
i
based on the informa-
tion obtained from WordNet about a few immedi-
ately surrounding words. We define the context C of
the target word to be a window of n words to the left
and another n words to the right, for a total of 2n
surrounding words. The algorithm is based on three
different procedures for nouns, verbs, adverbs and
adjectives, called JIGSAW
nouns
, JIGSAW
verbs
,
JIGSAW
others
, respectively. More details for each
one of the above mentioned procedures follow.
1.1 JIGSAW
nouns
The procedure is obtained by making some varia-
tions to the algorithm designed by Resnik (1995) for
disambiguating noun groups. Given a set of nouns
W = {w
1
, w
2
, . . . , w
n
}, obtained from document
d, with each w
i
having an associated sense inven-
tory S
i
= {s
i1
, s
i2
, . . . , s
ik
} of possible senses, the
goal is assigning each w
i
with the most appropri-
ate sense s
ih
? S
i
, according to the similarity of
w
i
with the other words in W (the context for w
i
).
The idea is to define a function ?(w
i
, s
ij
), w
i
? W ,
s
ij
? S
i
, that computes a value in [0, 1] representing
the confidence with which word w
i
can be assigned
with sense s
ij
. The intuition behind this algorithm
is essentially the same exploited by Lesk (1986) and
other authors: The most plausible assignment of
senses to multiple co-occurring words is the one that
maximizes relatedness of meanings among the cho-
398
sen senses. JIGSAW
nouns
differs from the original
algorithm by Resnik (1995) in the similarity mea-
sure used to compute relatedness of two senses. We
adopted the Leacock-Chodorow measure (Leacock
and Chodorow, 1998), which is based on the length
of the path between concepts in an IS-A hierarchy.
The idea behind this measure is that similarity be-
tween two synsets, s
1
and s
2
, is inversely propor-
tional to their distance in the WordNet IS-A hierar-
chy. The distance is computed by finding the most
specific subsumer (MSS) between s
1
and s
2
(each
ancestor of both s
1
and s
2
in the WordNet hierar-
chy is a subsumer, the MSS is the one at the lowest
level) and counting the number of nodes in the path
between s
1
and s
2
that traverse their MSS. We ex-
tended this measure by introducing a parameter k
that limits the search for the MSS to k ancestors (i.e.
that climbs the WordNet IS-A hierarchy until either
it finds the MSS or k + 1 ancestors of both s
1
and
s
2
have been explored). This guarantees that ?too
abstract? (i.e. ?less informative?) MSSs will be ig-
nored. In addition to the semantic similarity func-
tion, JIGSAW
nouns
differs from the Resnik algo-
rithm in the use of:
1. a Gaussian factor G, which takes into account the dis-
tance between the words in the text to be disambiguated;
2. a factor R, which gives more importance to the synsets
that are more common than others, according to the fre-
quency score in WordNet;
3. a parametrized search for the MSS between two concepts
(the search is limited to a certain number of ancestors).
Algorithm 1 describes the complete procedure for
the disambiguation of nouns. This algorithm consid-
ers the words in W pairwise. For each pair (w
i
,w
j
),
the most specific subsumer MSS
ij
is identified, by
reducing the search to depth1 ancestors at most.
Then, the similarity sim(w
i
, w
j
, depth2) between
the two words is computed, by reducing the search
for the MSS to depth2 ancestors at most. MSS
ij
is
considered as supporting evidence for those synsets
s
ik
in S
i
and s
jh
in S
j
that are descendants of
MSS
ij
. The MSS search is computed choosing the
nearest MSS in all pairs of synsets s
ik
,s
jh
. Like-
wise, the similarity for (w
i
,w
j
) is the max similarity
computed in all pairs of s
ik
,s
jh
and is weighted by
a gaussian factor that takes into account the posi-
tion of w
i
and w
j
in W (the shorter is the distance
Algorithm 1 The procedure for disambiguating
nouns derived from the algorithm by Resnik
1: procedure JIGSAW
nouns
(W, depth1, depth2) 
finds the proper synset for each polysemous noun in the set
W = {w
1
, w
2
, . . . , w
n
}, depth1 and depth2 are used in
the computation of MSS
2: for all w
i
, w
j
? W do
3: if i < j then
4: sim ? sim(w
i
, w
j
, depth1) ?
G(pos(w
i
), pos(w
j
))  G(x, y) is a Gaussian
function which takes into account the difference between
the positions of w
i
and w
j
5: MSS
ij
? MSS(w
i
, w
j
, depth2) 
MSS
ij
is the most specific subsumer between w
i
and w
j
,
search for MSS restricted to depth2 ancestors
6: for all s
ik
? S
i
do
7: if is-ancestor(MSS
ij
,s
ik
) then  if
MSS
ij
is an ancestor of s
ik
8: sup
ik
? sup
ik
+ sim
9: end if
10: end for
11: for all s
jh
? S
j
do
12: if is-ancestor(MSS
ij
,s
jh
) then
13: sup
jh
? sup
jh
+ sim
14: end if
15: end for
16: norm
i
? norm
i
+ sim
17: norm
j
? norm
j
+ sim
18: end if
19: end for
20: for all w
i
? W do
21: for all s
ik
? S
i
do
22: if norm
i
> 0 then
23: ?(i, k) ? ? ? sup
ik
/norm
i
+ ? ? R(k)
24: else
25: ?(i, k) ? ?/|S
i
| + ? ? R(k)
26: end if
27: end for
28: end for
29: end procedure
between the words, the higher is the weight). The
value ?(i, k) assigned to each candidate synset s
ik
for the word w
i
is the sum of two elements. The
first one is the proportion of support it received, out
of the support possible, computed as sup
ik
/norm
i
in Algorithm 1. The other element that contributes
to ?(i, k) is a factor R(k) that takes into account
the rank of s
ik
in WordNet, i.e. how common is the
sense s
ik
for the word w
i
. R(k) is computed as:
R(k) = 1 ? 0.8 ?
k
n ? 1
(1)
where n is the cardinality of the sense inventory S
i
for w
i
, and k is the rank of s
ik
in S
i
, starting from 0.
Finally, both elements are weighted by two pa-
rameters: ?, which controls the contribution given
399
to ?(i, k) by the normalized support, and ?, which
controls the contribution given by the rank of s
ik
.
We set ? = 0.7 and ? = 0.3. The synset assigned
to each word in W is the one with the highest ?
value. Notice that we used two different parameters,
depth1 and depth2 for setting the maximum depth
for the search of the MSS: depth1 limits the search
for the MSS computed in the similarity function,
while depth2 limits the computation of the MSS
used for assigning support to candidate synsets. We
set depth1 = 6 and depth2 = 3.
1.2 JIGSAW
verbs
Before describing the JIGSAW
verbs
procedure, the
description of a synset must be defined. It is the
string obtained by concatenating the gloss and the
sentences that WordNet uses to explain the usage
of a synset. First, JIGSAW
verbs
includes, in the
context C for the target verb w
i
, all the nouns in
the window of 2n words surrounding w
i
. For each
candidate synset s
ik
of w
i
, the algorithm computes
nouns(i, k), that is the set of nouns in the descrip-
tion for s
ik
.
max
jk
= max
w
l
?nouns(i,k)
{sim(w
j
,w
l
,depth)} (2)
where sim(w
j
,w
l
,depth) is defined as in
JIGSAWnouns. In other words, max
jk
is the
highest similarity value for w
j
wrt the nouns related
to the k-th sense for w
i
. Finally, an overall simi-
larity score among s
ik
and the whole context C is
computed:
?(i, k) = R(k) ?
P
w
j
?C
G(pos(w
i
), pos(w
j
)) ? max
jk
P
h
G(pos(w
i
), pos(w
h
))
(3)
where R(k) is defined as in Equation 1 with a differ-
ent constant factor (0.9) and G(pos(w
i
), pos(w
j
)) is
the same Gaussian factor used in JIGSAWnouns,
that gives a higher weight to words closer to the tar-
get word. The synset assigned to w
i
is the one with
the highest ? value. Algorithm 2 provides a detailed
description of the procedure.
1.3 JIGSAW
others
This procedure is based on the WSD algorithm pro-
posed by Banerjee and Pedersen (2002). The idea is
to compare the glosses of each candidate sense for
Algorithm 2 The procedure for the disambiguation
of verbs
1: procedure JIGSAW
verbs
(w
i
, d, depth)  finds the
proper synset of a polysemous verb w
i
in document d
2: C ? {w
1
, ..., w
n
}  C is
the context for w
i
. For example, C = {w
1
, w
2
, w
4
, w
5
},
if the sequence of words {w
1
, w
2
, w
3
, w
4
, w
5
} occurs in d,
w
3
being the target verb, w
j
being nouns, j 6= 3
3: S
i
? {s
i1
, ...s
im
}  S
i
is the sense inventory for w
i
, that is the set of all candidate
synsets for w
i
returned by WordNet
4: s ? null  s is the synset to be returned
5: score ? ?MAXDOUBLE  score is the
similarity score assigned to s
6: p ? 1  p is the position of the synsets for w
i
7: for all s
ik
? S
i
do
8: max ? {max
1k
, ..., max
nk
}
9: nouns(i, k) ? {noun
1
, ..., noun
z
} 
nouns(i, k) is the set of all nouns in the description of s
ik
10: sumGauss ? 0
11: sumTot ? 0
12: for all w
j
? C do  computation of the similarity
between C and s
ik
13: max
jk
? 0  max
jk
is the highest similarity
value for w
j
, wrt the nouns related to the k-th sense for w
i
.
14: sumGauss ? G(pos(w
i
), pos(w
j
)) 
Gaussian function which takes into account the difference
between the positions of the nouns in d
15: for all noun
l
? nouns(i, k) do
16: sim ? sim(w
j
, noun
l
, depth)  sim is
the similarity between the j-th noun in C and l-th noun in
nouns(i, k)
17: if sim > max
jk
then
18: max
jk
? sim
19: end if
20: end for
21: end for
22: for all w
j
? C do
23: sumTot ? sumTot+G(pos(w
i
), pos(w
j
))?
max
jk
24: end for
25: sumTot ? sumTot/sumGauss
26: ?(i, k) ? R(k) ? sumTot  R(k) is defined as in
JIGSAW
nouns
27: if ?(i, k) > score then
28: score ? ?(i, k)
29: p ? k
30: end if
31: end for
32: s ? s
ip
33: return s
34: end procedure
the target word to the glosses of all the words in its
context. Let W
i
be the sense inventory for the tar-
get word w
i
. For each s
ik
? W
i
, JIGSAW
others
computes the string targetGloss
ik
that contains the
words in the gloss of s
ik
. Then, the procedure
computes the string contextGloss
i
, which contains
the words in the glosses of all the synsets corre-
400
sponding to each word in the context for w
i
. Fi-
nally, the procedure computes the overlap between
contextGloss
i
and targetGloss
ik
, and assigns the
synset with the highest overlap score to w
i
. This
score is computed by counting the words that occur
both in targetGloss
ik
and in contextGloss
i
. If ties
occur, the most common synset in WordNet is cho-
sen.
2 Experiment
We performed the experiment following the instruc-
tions for SemEval-2007 task 1 (Agirre et al, 2007).
JIGSAW is implemented in JAVA, by using JWNL
library3 in order to access WordNet 1.6 dictionary.
We ran the experiment on a Linux-based PC with
Intel Pentium D processor having a speed of 3 GHz
and 2 GB of RAM. The dataset consists of 29,681
documents, including 300 topics. Results are re-
ported in Table 1. Only two systems (PART-A and
PART-B) partecipated to the competition, thus the
organizers decided to add a third system (ORGA-
NIZERS) developed by themselves. The systems
were scored according to standard IR/CLIR mea-
sures as implemented in the TREC evaluation pack-
age4. Our system is labelled as PART-A.
system IR documents IR topics CLIR
no expansion 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
PART-A 0.3030 0.1521 0.1373
PART-B 0.3036 0.1482 0.1734
Table 1: SemEval-2007 task 1 Results
All systems show similar results in IR tasks, while
their behaviour is extremely different on CLIR task.
WSD results are reported in Table 2. These re-
sults are encouraging as regard precision, consid-
ering that our system exploits only WordNet as
kwnoledge-base, while ORGANIZERS uses a su-
pervised method that exploits SemCor to train a
kNN classifier.
3 Conclusions
In this paper we have presented a WSD algorithm
that exploits WordNet as knowledge-base and uses
3http://sourceforge.net/projects/jwordnet
4http://trec.nist.gov/
system precision recall attempted
SENSEVAL-2
ORGANIZERS 0.584 0.577 93.61%
PART-A 0.498 0.375 75.39%
PART-B 0.388 0.240 61.92%
SENSEVAL-3
ORGANIZERS 0.591 0.566 95.76%
PART-A 0.484 0.338 69.98%
PART-B 0.334 0.186 55.68%
Table 2: WSD results on all-words task
three different methods for each part-of-speech. The
algorithm has been evaluated by SemEval-2007 task
1. The system shows a good performance in all
tasks, but low precision in CLIR evaluation. Prob-
ably, the negative result in CLIR task depends on
complex interaction of WSD, expansion and index-
ing. Contrarily to other tasks, organizers do not plan
to provide a ranking of systems on SemEval-2007
task 1. As a consequence, the goal of this task - what
is the best WSD system in the context of a CLIR
system? - is still open. This is why the organizers
stressed in the call that this was ?a first try?.
References
E. Agirre, B. Magnini, o. Lopez de Lacalle, A. Otegi,
G. Rigau, and Vossen. 2007. Semeval-2007 task
1: Evaluating wsd on cross-language information re-
trieval. In Proceedings of SemEval-2007. Association
for Computational Linguistics.
S. Banerjee and T. Pedersen. 2002. An adapted lesk
algorithm for word sense disambiguation using word-
net. In CICLing?02: Proc. 3rd Int?l Conf. on Com-
putational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
C. Leacock and M. Chodorow. 1998. Combining local
context and wordnet similarity for word sense identifi-
cation. In C. Fellbaum (Ed.), WordNet: An Electronic
Lexical Database, pages 305?332. MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine cone
from an ice cream cone. In Proceedings of the 1986
SIGDOC Conference, pages 20?29. ACM Press.
P. Resnik. 1995. Disambiguating noun groupings with
respect to WordNet senses. In Proceedings of the
Third Workshop on Very Large Corpora, pages 54?68.
Association for Computational Linguistics.
401
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 991?1002,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Harnessing different knowledge sources to measure semantic relatedness 
under a uniform model 
 
Ziqi Zhang Anna Lisa Gentile Fabio Ciravegna 
Department of Computer Science, University of Sheffield 
211 Portobello, Regent Court 
Sheffield, S1 4DP 
z.zhang@dcs.shef.ac.
uk 
a.l.gentile@dcs.shef
.ac.uk 
f.ciravegna@dcs.shef
.ac.uk 
 
Abstract 
Measuring semantic relatedness between 
words or concepts is a crucial process to 
many Natural Language Processing tasks. 
Exiting methods exploit semantic evidence 
from a single knowledge source, and are 
predominantly evaluated only in the 
general domain. This paper introduces a 
method of harnessing different knowledge 
sources under a uniform model for 
measuring semantic relatedness between 
words or concepts. Using Wikipedia and 
WordNet as examples, and evaluated in 
both the general and biomedical domains, it 
successfully combines strengths from both 
knowledge sources and outperforms state-
of-the-art on many datasets. 
1    Introduction 
Semantic relatedness (SR) measures how much 
two (strings of) words or concepts are related by 
encompassing all kinds of relations between them 
(Strube and Ponzetto, 2006). It is more general 
than semantic similarity. SR is often an important 
pre-processing step to many complex Natural 
Language Processing (NLP) tasks, such as Word 
Sense Disambiguation (Leacock and Chodorow, 
1998; Han and Zhao, 2010), and information 
retrieval (Finkelstein et al, 2002). In the 
biomedical domain, SR is an important technique 
for discovering gene functions and interactions 
(Wu et al, 2005; Ye et al, 2005).  
There is an abundant literature on measuring 
SR between words or concepts. Typically, these 
methods extract semantic evidence of words and 
concepts from a background knowledge source, 
with which their relatedness is assessed. The 
knowledge sources can be unstructured documents 
or (semi-)structured resources such as Wikipedia, 
WordNet, and domain specific ontologies (e.g., the 
Gene Ontology1).  
In this paper, we identify two issues that have 
not been addressed in the previous works. First, 
existing works typically employ a single 
knowledge source of semantic evidence. Research 
(Strube and Ponzetto, 2006; Zesch and Gurevych, 
2010; Zhang et al, 2010) has shown that the 
accuracy of an SR method differs depending on the 
choice of the knowledge sources, and there is no 
conclusion which knowledge source is superior to 
others. Zhang et al (2010) argue that this indicates 
different knowledge sources may complement each 
other. Second, the majority of SR methods have 
been evaluated in general domains only, except a 
few earlier WordNet-based methods that have been 
adapted to biomedical ontologies and evaluated in 
that domain (Lord et al, 2003; Pedersen et al, 
2006; Pozo et al, 2008). Given the significant 
attention that SR has received in specific domains 
(Pesquita et al, 2007), evaluation of SR methods 
in specific domains is increasingly important.  
This paper addresses these issues by proposing 
a generic and uniform model for computing SR 
between words or concepts using multiple 
knowledge sources, and evaluating the proposed 
method in both general and specific domains. The 
method combines and integrates semantic evidence 
of words or concepts extracted from any 
knowledge source in a generic graph 
representation, with which the SR between 
concepts or words is computed. Using two of the 
most popular general-domain knowledge sources, 
                                                         
1 http://www.geneontology.org/, last retrieved in Mar. 2011 
991
Wikipedia and WordNet as examples, the method 
is evaluated on 7 benchmarking datasets, including 
three datasets from the biomedical domain and 
four from the general domain. It has achieved 
excellent results: compared to the baselines that 
use each single knowledge sources, combining 
both knowledge sources has improved the accuracy 
on all datasets by 2~11%; compared to state-of-
the-art on the general domain datasets, the method 
achieves the best results on three datasets; and on 
the other three biomedical datasets, it obtains the 
best result in one case; and second and third best 
results on the other two among eight participating 
methods, where all other competitors exploit some 
domain-specific knowledge sources.  
The remainder of this paper is organized as 
follows. Section 2 discusses related work; Section 
3 presents the proposed method; Section 4 
describes the experiments and evaluation; Section 
5 discusses results and findings; Section 6 
concludes this paper. 
2    Related work 
2.1    SR methods 
Methods for computing SR can be classified into 
path based, Information Content (IC) based, 
statistical and hybrid methods. Path based 
methods (Hirst and St-Onge, 1998; Leacock and 
Chodorow, 1998; Pekar and Staab, 2002; Rada et 
al., 1989; Wu and Palmer, 1994) measure SR 
between words or concepts as a function of their 
distance in a semantic network, usually calculated 
based on the path connecting the words or concepts 
by certain semantic (typically is-a) links. IC based 
methods (Jiang and Conrath, 1997; Lin, 1998; 
Pirro et al, 2009; Resnik, 1995; Seco et al, 2004) 
assess relatedness between words or concepts by 
the amount of information they share, usually 
determined by a higher level concept that 
subsumes both concepts in a taxonomic structure. 
Statistical methods measure relatedness between 
words or concepts based on their distribution of 
contextual evidence. This can be formalized as co-
occurrence statistics collected from unstructured 
documents (Chen et al, 2006; Cilibrasi and 
Vitanyi, 2007; Matsuo et al, 2006), or 
distributional concept or word vectors with 
features extracted from either unstructured 
documents (Harrington, 2010; Wojtinnek and 
Pulman, 2011) or (semi-)structured knowledge 
resources (Agirre et al, 2009; Gabrilovich and 
Markovitch, 2007; Gouws et al, 2010; Zesch and 
Gurevych, 2007; Zhang et al, 2010). Hybrid 
methods combine different purebred methods in 
certain ways. For example Riensche et al (2007) 
employ both an IC based method (Resnik, 1995) 
and a statistical method (cosine vector similarity) 
in their study. Pozo et al (2008) derive a taxonomy 
of terms from unstructured documents by applying 
hierarchical clustering based on corpus statistics, 
then apply path based method on this taxonomy to 
compute SR. Han and Zhao (2010) use one IC 
based method and two statistical methods to 
compute SR, then derive an aggregated score.  
2.2    SR knowledge sources and domains 
Computing SR requires background knowledge 
about concepts or words, which can be extracted 
from unstructured corpora, semi-structured and 
structured knowledge resources. Unstructured 
corpora are easier to create and cheaper to 
maintain, however, semantic relations between 
words or concepts are implicit. Methods (Chen et 
al., 2006; Cilibrasi and Vitanyi 2007; Matsuo et al, 
2006) that exploit unstructured corpora typically 
depend on distributional statistics, and thus may 
ignore important semantic evidences present in 
(semi-)structured knowledge sources (Pan and 
Farrell, 2007). Recent studies (Harrington, 2010; 
Pozo et al, 2008; Wojtinnek and Pulman, 2011) 
propose to pre-process a corpus to learn a semantic 
network, with which SR is computed. This creates 
high pre-processing cost; also, the choice of corpus 
and its size often have a direct correlation with the 
accuracy of SR methods (Batet et al, 2010). 
(Semi-)Structured knowledge sources on the 
other hand, organize semantic knowledge about 
concepts and words explicitly and interlink them 
with semantic relations. They have been popular 
choices in the studies of SR, and they include 
lexical resources such as WordNet, Wiktionary, 
and (semi-)structured encyclopedic resources such 
as Wikipedia. WordNet has been used in earlier 
studies (Hirst and St-Onge, 1998; Jiang and 
Conrath, 1997; Lin, 1998; Leacock and Chodorow 
1998; Resnik, 1995; Seco et al, 2004; Wu and 
Palmer, 1994) and is still a preferred knowledge 
source in recent works (Agirre et al, 2009). 
However, its effectiveness may be hindered by its 
lack of coverage of specialized lexicons and 
domain specific concepts (Strube and Ponzetto, 
992
2006; Zhang et al, 2010). Wikipedia and 
Wiktionary are collaboratively maintained know-
ledge sources and therefore may overcome this 
limitation. Wikipedia in particular, is found to have 
reasonable coverage of many domains (Holloway 
et al, 2007; Halavais, 2008). It has become 
increasingly popular in SR studies recently. 
However, research (Zesch and Gurevych, 2010) 
have shown that methods based on Wikipedia have 
no clear advantage over WordNet-based methods 
on some general domain datasets in terms of 
accuracy, while Zhang et al (2010) argue that 
different knowledge sources may complement each 
other, and SR methods may benefit from 
harnessing different knowledge sources.  
Several studies (Lord et al, 2003; Pedersen et 
al., 2006; Petrakis et al, 2006; Pozo et al, 2008) 
have adapted state-of-the-art to domain specific 
knowledge sources (e.g., the Gene Ontology, the 
MeSH2) and evaluated them therein. Despite these 
efforts, a large proportion of state-of-the-art is still 
only evaluated in the general domain.  
2.3    SR methods similar to this work 
Few works have attempted at combining different 
knowledge sources in SR studies, especially (semi-
)structured knowledge sources. The closest studies 
are Han and Zhao (2010) and Tsang and Stevenson 
(2010). Han and Zhao firstly compute SR between 
words using three state-of-the-art SR methods 
separately. Next, one score is chosen subject to an 
arbitrary preference order, and used to create a 
connected graph of weighted edges between 
words. A recursive function is then applied to the 
graph to compute final SR scores between words. 
Essentially, each SR method is applied in isolation 
and features from different sources are used 
separately with each distinctive method. Although 
this retains advantages of each method, the 
limitations of them are also combined.  
Tsang and Stevenson (2010) combine WordNet 
and unstructured documents by weighing each 
word found in WordNet using its frequency 
observed in a large corpus. The frequencies 
however, are sensitive to the choice of corpus, thus 
different corpora may result in different accuracies. 
Furthermore, their method is only applicable to 
computing SR between pairs of sets of words or 
concepts.  
                                                         
2 http://www.nlm.nih.gov/mesh/ last retrieved in March 2011 
3    Methodology  
We define a set of requirements for SR methods 
that harness different knowledge sources: 
? It should improve over the same method 
based on a single knowledge source 
? It should be generic and applicable to any 
knowledge source 
? It should be robust in dealing with 
knowledge source specific features but 
also tolerate the quality and coverage 
issues of individual knowledge source 
Our method of harnessing different knowledge 
sources contains four steps. Firstly (Section 3.1), 
each word or word segment is searched in each 
knowledge source to identify their contexts that is 
specific to that knowledge source. We define a 
context as the representation of meaning or a 
concept for a word. In the following, we say that 
each context is associated with a distinct concept. 
Secondly (Section 3.2), for each concept of an 
input word, features are extracted from its context 
and a graph representation of each concept and 
their features is created. Thirdly (Section 3.3), 
cross-source contexts are mapped where they refer 
to the same concept, thus their features from 
different sources can be combined to derive an 
enriched representation. This creates a final, 
uniform graph representation where input words 
are connected by shared features of their 
underlying candidate concepts. Then (Section 3.4) 
the graph is submitted to a generic algorithm to 
compute SR between words. 
In the following, we discuss details with respect 
to different types of knowledge sources, while 
focusing on Wikipedia and WordNet in our 
experiments for two reasons. First, they are used 
by the majority of SR methods and are therefore 
most representative knowledge sources. Second, 
they have strongly distinctive and complementary 
characteristics, which make ideal testbeds for the 
requirements. On one hand, WordNet is a lexical 
resource containing rich and strict semantic 
relations between words, but lacks coverage of 
specialized vocabularies. On the other hand, 
Wikipedia is a semi-structured resource with good 
coverage of domains and named entities, but the 
semantic knowledge is organized in a looser way. 
993
3.1    Context retrieval 
Given a pair of words or word segments, we firstly 
identify contexts representing the underlying 
meanings or concepts from each knowledge 
source. For lexical resources, this could be 
distinctive word senses. In WordNet (WN), a 
context corresponds to a single synset, which 
corresponds to a concept. We search each word in 
WordNet and extract all possible synsets. Let w be 
a word or word segment (e.g., ?cat?), and   
   
    
      
      
    be the set of k concepts of w 
extracted from WordNet.  
Using Wikipedia (WK) as an example semi-
structured resource, the context can be an article 
that describes a unique concept. Thus we search 
for underlying articles that describe different 
concepts. Firstly, we search w in Wikipedia, where 
three situations may be anticipated. If a single non-
disambiguation page describing a concept is 
returned, the concept is selected and the retrieval is 
complete. In the second case, a disambiguation 
page linking to all possible concept pages may be 
returned. This page lists all underlying concepts 
and entities referenced by w as links and a short 
description with each link. In this case, we always 
keep the first concept page, which is found often to 
be the most common sense of the word; 
additionally, we select other concept pages whose 
short descriptions contain the word w. We do not 
select all linked pages because many of these in 
fact link to a concept relevant to w, but not 
necessarily a candidate sense of w. Thirdly, if no 
pages are returned for w, we search for the most 
relevant page using w as keyword(s) in an inverted 
index of all Wikipedia pages (e.g., via search 
engines). We denote concepts retrieved from 
Wikipedia as   
       
      
      
   .  
For unstructured sources such as documents, a 
simple approach could be defining a word context 
as a text passage around each occurrence of w, and 
grouping similar contexts of w as representation of 
its underlying meanings, or concepts. Alternatively, 
more complex approaches such as Pozo et al 
(2008) and Harrington (2010) may be applied to 
extract a lexical network of words, whereby similar 
methods to WordNet can be applied. 
3.2 Feature extraction and representation 
Next, for each concept identified from a 
knowledge source, features are extracted from their 
corresponding contexts. In our case, for each 
    
  , we follow the work by Zhang et al 
(2010) to extract four types of features from their 
corresponding Wikipedia pages. Figure 1 shows an 
example representation of a concept and its 
Wikipedia features: 
? Words from page titles and redirection 
links (can be considered as synonyms) 
? Words from categories, used as higher 
level hypernyms in some studies (Zesch et 
al., 2010; Strube and Ponzetto, 2006) 
? Words from outgoing links 
? Top n most frequent words from a page 
 
Figure 1. Representation of the concept ?cat, the 
mammal? using different types of features 
extracted from Wikipedia. The shaded circle 
represents the concept; ovals represent feature 
values; edges connecting feature values to the 
concept and <labels> represent feature types 
 
For each     
  , we extract ten features from 
WordNet: hypernyms, hyponyms, meronyms, 
holonyms, synonyms, antonyms, attributes, ?see 
also? words, ?related? words, and gloss. These are 
also represented in the same way as in Figure 1.  
With unstructured sources, contextual words 
can be used as features. Alternatively, if a lexical 
network is extracted, features may be extracted in a 
similar way to those of WordNet. 
 
Additionally, with WordNet and Wikipedia, we 
also propose several intra-resource feature merging 
strategies to study the effect of feature 
diversification. This is because, while some 
approaches (such as Agirre et al, 2009; 
Harrington, 2010; Yeh et al, 2009) do not 
distinguish different feature types in graph 
construction, or adopt a bag-of-words feature 
representation (such as Zesch and Gurevych, 
2010), others (such as Yazdani and Popescu-Belis, 
2010; Zhang et al, 2010) have used differentiated 
994
feature types and weights in their model. We 
therefore carry out studies to investigate this issue. 
Specifically, for the original four Wikipedia 
features, we create a bag-of-words feature that 
simply merges all feature types (i.e., all edges in 
Figure 1 will have the same label). For the original 
ten WordNet features, we propose two merged 
representations corresponding to that of Wikipedia, 
so as to support the studies of feature enrichment 
in the following section. We introduce a bag-of-
words feature that collapses all different feature 
types, and a four-feature representation as follow: 
? wn-synant merges WordNet synonyms and 
antonyms.  
? wn-hypoer merges WordNet hypernyms 
and hyponyms, collectively representing 
features by ?is-a? semantic relation 
? wn-assc merges WordNet meronyms, 
holonyms, related and ?see also?, which 
are features corresponding to associative 
relations  
? wn-dist merges WordNet gloss and 
attributes that generally describe a concept.  
3.3 Concept mapping and feature enrichment 
Our method essentially harnesses different 
knowledge sources by combining features 
extracted from different sources in a uniform 
model. This requires two sub-processes: cross-
source concept mapping and cross-source 
feature enrichment.  
In cross-source concept mapping, concepts 
extracted from different knowledge sources are 
mapped according to similar meanings such that 
cross-source features can be combined. To do so, 
we select the concepts from one knowledge source 
as the reference concept set; then concepts from 
other knowledge sources are mapped to reference 
concepts of similar meanings. There can be 
different criteria of choosing reference knowledge 
source concepts. Empirically, we found it 
necessary to choose the knowledge source with 
broader coverage and richer features. This will be 
discussed later in Section 5. Following this 
strategy, in our example,   
   is chosen as 
reference concepts, and for each   
     
  we 
select a   
     
   such that   
   and   
   refer to 
the same meaning. To do so, we apply a simple 
maximum set overlap metric to their feature 
values. Let F(c) be a function that returns all 
feature values of c as bag-of-words, then for each 
  
     
  , it is mapped to a   
   such that 
     
          
     is maximized among all 
  
     
  . The resulting concept candidates are 
denoted as   
    
, where   
    
=    
     
    is a 
mapped set of concepts potentially referring to the 
same meaning. If   
     then   
    
 
  
   
        
  . 
Next, cross-source feature enrichment creates 
a uniform feature representation for each mapped 
sets of concepts. The process can be considered as 
enriching the features from one knowledge source 
with others. The most straightforward approach is 
to simply collect features extracted from each 
knowledge source on to a single graph, retaining 
the diversity in feature types. For example, Figure 
2 shows a graph representation based on the 
collection of the four Wikipedia features and the 
four derived WordNet features. We refer to this 
approach as ?feature combination?.  
 
Figure 2. Representation of ?cat, the mammal? 
after concept mapping and feature combination 
 
On the other hand, cross-source features may be 
merged according to their semantics.  For example, 
WordNet and Wikipedia contain features based on 
synonyms of concepts; while Wikipedia and 
unstructured documents contain word distribution-
al features. Thus we define ?feature integration? 
as merging feature types from different knowledge 
sources into single types of features based on their 
similarity in semantics.  With WordNet and Wiki-
pedia, we integrate features as below (Figure 3): 
? merged-synant merges Wikipedia page 
titles and redirection links with wn-synant 
? merged-hypoer merges merges Wikipedia 
categories with wn-hypoer 
995
? merged-assc merges Wikipedia links with 
wn-assc. We consider Wikipedia links bear 
other associative relations and are 
therefore merged with features extracted 
by other WordNet relations 
? merged-dist merges Wikipedia frequent n 
words with wn-dist.  
 
Figure 3. Representation of ?cat, the mammal? 
after concept mapping and feature integration 
 
Note that the difference between cross-source 
feature combination and integration is that the 
former introduces more types of features, whereas 
the latter retains same number of feature types but 
increases feature values for each type. Both have 
the effect of establishing additional path (via 
features) between concepts, but in different ways. 
 
With intra-resource feature diversification, cross-
source feature combination and feature 
integration, we create a total of nine intra- and 
cross-source feature representations to be tested 
with the uniform random walk model: 
? four types of Wikipedia features (wk-4F) 
? one type of Wikipedia features (wk-1F) 
? ten types of WordNet features (wn-10F) 
? four types of WordNet features (wn-4F) 
? one type of WordNet features (wn-1F) 
? wk-4F combines wn-4F: wk-4F+wn4F,C 
? wk-4F integrates wn-4F: wk-4F+wn4F,I 
? wk-1F combines wn-1F: wk-1F+wn1F,C 
? wk-1F integrates wn-1F: wk-1F+wn1F,I 
3.4 Computing SR using the graph 
The algorithm for computing SR using the graph is 
based on the idea of random walk. It formalizes the 
idea that taking successive steps along the paths in 
a graph, the ?easier? it is to arrive at a target node 
starting from a source node, the more related the 
two nodes are. Following the previous steps, the 
feature representations of all candidate concepts 
relevant to the input word pairs are joined, which 
creates a single undirected, weighted, bi-partite 
graph. Let G = (V, E) be the graph, where V is the 
set of nodes (concepts and feature values); E is the 
set of edges (feature types) that connect concepts 
and features. As shown in Figure 4, different 
concepts are connected if they share same values 
of same types of features, namely, there exists a 
path that connects one concept to another.  
 
Figure 4. Paths are established between different 
concepts if they share values of same feature types 
<bold underlined> 
Using Figure 4 it is easier to comprehend the 
difference between feature combination and 
integration. Since concept nodes can only be 
connected by same types of edges (feature types), 
feature combination increases the chances of 
connectivity by adding in more types of edges, 
while integration merges similar types of edges 
across knowledge sources and increases the 
number of feature nodes connected by each type.  
From the graph, we start by building an 
adjacency matrix W of initial probability 
distribution: 
??
??
?
??
??
? ?????? ? ?
otherwise
EjililEi
lw
W Ll k
k
ij
k
,0
),(,|),(:),(|
)( [1] 
Where Wij is the i
th-line and jth-column entry of W, 
indexed by V; l(i, j) is a function that returns the 
type of edge (i.e., type of feature) connecting 
nodes i and j; L is the set of all possible types; w(l) 
returns the weight for that type. Essentially, L is 
the collection of all feature types, and w(l) assigns 
996
a weight to a particular feature type. Next, we 
compute the transition probability matrix P(t)(j|i) = 
[(D?1W)t]ij (Dii = ?kWik), which returns the 
probability of reaching other nodes from a starting 
node on the graph after t steps. In this method, we 
follow the work by Rowe and Ciravegna (2010) to 
set t=2 in order to preserve locally connected 
nodes. Next, we extract the probability vectors 
corresponding to concept nodes from P, and 
compute pair-wise relatedness using the cosine 
function. Effectively, this formalizes the notion 
that two concepts related to a third concept is also 
semantically related, which is similar to the 
hypothesis proposed by Patwardhan and Pedersen 
(2006) in their method based on second-order 
context vectors. The final SR between the input 
word pair is the maximum pair-wise concept SR. 
4    Experiment and evaluation 
We evaluate the method based on correlation 
against human judgment (gold standard) on seven 
benchmarking datasets covering both general and 
technical domains. These include four general 
domain datasets: the Rubenstein and Goodenough 
(1965) dataset containing 65 pairs of nouns 
(RG65); the Miller and Charles (1991) dataset that 
is a subset of the RG-65 dataset and contains 30 
pairs (MC30); the Finkelstein et al (2002) dataset 
with 353 pairs of words, including nouns, verbs, 
adjectives, as well as named entities. This contains 
two subsets, a set of 153 pairs (Fin153) and a set of 
200 (Fin200) pairs each annotated by a different 
groups of annotators. Zesch and Gurevych (2010) 
show largely varying Inter-Annotator-Agreement 
(IAA) between the two sets (Table 1), and argue 
that they should be treated as separate datasets. 
Three biomedical datasets are selected to evaluate 
domain-specific performance of the proposed 
method. These include a set of 36 MeSH term pairs 
in Petrakis et al (2006) (MeSH36), 30 pairs of 
medical terms annotated by a group of physicians 
as in Pedersen et al (2006) (Ped30-p) and the same 
set annotated by a different group of medical 
coders (Ped30-c). Table 1 shows statistics of the 
seven datasets.  
The correlation is computed using the 
Spearman rank order coefficient for two reasons. 
First, it is a better metric than other alternatives 
(Zesch and Gurevych, 2010). Second, it is 
consistent with the majority of studies such that 
results can be compared.  
 
Dataset Size Domain IAA 
MC30 30 General 0.9 
RG65 65 General 0.8 
Fin153 153 General 0.73 
Fin200 200 General 0.55 
Ped30-p 30 Biomedical 0.68 
Ped30-c 30 Biomedical 0.78 
MeSH36 36 Biomedical - 
Table 1: Information of benchmarking datasets 
 
We distribute feature weights w(l) across 
different feature types L evenly in each feature 
representation. Although Zhang et al (2010) show 
that discriminated feature weights leads to 
improved accuracy; this is not the focus of this 
study. Since we aim to investigate the effects of 
harnessing different knowledge sources, we 
obtained baseline performances by applying the 
method to those feature representations based on 
single knowledge sources (i.e., wk-4F, wk-1F, wn-
10F, wn-4F, wn-1F). Tables 2 and 3 show the best 
results obtained with baselines and corresponding 
knowledge sources and feature representation.  
 
Dataset Corr. Feature Coverage (% pairs) 
MC30 0.77 wn-1F 77% 
RG65 0.71 wn-1F 65% 
Fin153 0.45 wn-4F 82% 
Fin200 0.35 wn-4F 76% 
Ped30-p 0.66 wn-4F 33% 
Ped30-c 0.8 wn-4F 33% 
MeSH36 0.49 wn-1F 50% 
Table 2: Correlation obtained using WordNet.  
Many word pairs are not covered due to sparse 
feature space and lack of coverage. Only covered 
pairs are accounted. 
 
Dataset Corr. Feature 
MC30 0.74 wk-1F 
RG65 0.67 wk-1F 
Fin153 0.7 wk-1F 
Fin200 0.51 wk-4F 
Ped30-p 0.53 wk-4F 
Ped30-c 0.58 wk-4F 
MeSH36 0.73 wk-4F 
Table 3: Correlation obtained using only 
Wikipedia. All word pairs are 100% covered. 
 
997
Tables 4 ? 6 show results obtained with 
enriched feature representation. 
 
 Combination (C) Integration (I) 
Dataset wn-4F + 
wk-4F 
wn-1F + 
wk-1F 
wn-4F + 
wk-4F 
wn-1F 
+ wk-1F 
MC30 0.77 0.8 0.8 0.79 
RG65 0.74 0.73 0.73 0.729 
Fin153 0.73 0.75 0.74 0.73 
Fin200 0.52 0.54 0.53 0.54 
Ped30-p 0.63 0.52 0.64 0.47 
Ped30-c 0.64 0.52 0.67 0.49 
MeSH36 0.7 0.694 0.75 0.7 
Table 4: Correlation obtained using both 
knowledge sources. Word pairs are 100% covered. 
 
 KS and # of feature types 
 WN WK WK+WN,C WK+WN, I  
MC30 1 1 1 4 
RG65 1 1 4 4 
Fin153 4 1 1 4 
Fin200 4 4 1 1 
Ped30-p 4 4 4 4 
Ped30-c 4 4 4 4 
MeSH36 1 4 4 4 
Table 5: Number of feature types with which best 
results are obtained on each dataset. KS: 
Knowledge Source 
 
 Single KS Multiple KS Impr. 
Dataset Best corr. Best corr. Strategy  
MC30 0.74 0.8 C/I 0.06 
RG65 0.67 0.74 C 0.07 
Fin153 0.7 0.75 C 0.05 
Fin200 0.51 0.54 C/I 0.03 
Ped30-p 0.53 0.64 I 0.11 
Ped30-c 0.58 0.67 I 0.09 
MeSH36 0.73 0.75 I 0.02 
Table 6: Improvement achieved by harnessing 
multiple KSs. Best correlation with single KS is 
based on Wikipedia, which provides 100% 
coverage of word pairs. 
 
 
Tables 7 and 8 compare our method against state-
of-the-art. For Table 8, figures for other state-of-
the-art systems can be found in corresponding 
publications; while we only list the best 
performing systems for comparison. 
 
 
 
 
 
 MC30 RG65 Fin153 Fin200 KS 
best of 
WN+WK  
0.8 0.74 0.75 0.54 Both 
Rad89* 0.75 0.79 0.33 0.24 WN 
LC98* 0.75 0.79 0.33 0.24 WN 
WP94* 0.77 0.78 0.38 0.24 WN 
HS98* 0.76 0.79 0.33 0.32 WN 
Res95* 0.72 0.74 0.35 0.26 WN 
JC97* 0.68 0.58 0.28 0.10 WN 
Lin98* 0.67 0.60 0.27 0.17 WN 
Zes07* 0.77 0.82 0.6 0.51 WK 
GM07* 0.67 0.75 0.69 0.51 WK 
Zha10 0.71 0.76 0.71 0.46 WK 
Table 73: Comparison against state-of-the-art in the 
general domain. (* figures from Zesch and 
Gurevych, 2010) 
 
 Ped30-p Ped30-c MeSH36 KS 
best of 
WN+WK 
0.64 0.67 0.75 WN+
WK 
Pet06 best - - 0.74 MeSH 
Ped06 best 0.84 0.75 - GO, D 
Ped06 second 0.62 0.68 - GO, D 
Table 84: Comparison against state-of-the-art in the 
biomedical domain. GO ? Gene Ontology; D ? 
document sets.  
 
Given the fact that some datasets (i.e., MC30, 
Ped30-p, Ped30-c, MeSH36) have a relatively low 
sample size, we cannot always be sure that 
correlation values are accurate or occurred by 
chance. Therefore, we measure the statistical 
significance of correlation by computing the p-
value for the correlation values reported for our 
system in Tables 7 and 8. For all cases, a p-value 
of less than 0.001 is obtained, which indicates that 
correlation values are statistically significant. 
                                                         
3 Rada (1989) (Rad89); Leacock and Chodorow (1998) 
(LC98); Wu and Palmer (1994) (WP04); Hirst and St-Onge 
(1998) (HS98); Resnik (1995) (Res95); Jiang and Conrath 
(1997) (JC97); Lin (1998) (Lin98); Zesch and Gurevych 
(2007) (ZG07); Gabrilovich and Markovitch (2007) (GM07); 
Zhang et al (2010) (Zha10) 
4 Petrakis et al (2006) (Pet06); Pedersen et al (2006) (Ped06). 
Original participating systems can be found in these works. 
998
5    Discussion  
Single v.s. multiple knowledge sources As shown 
in Table 6, considering the best performances 
across all feature enrichment strategies and feature 
sets, the proposed method successfully harnessed 
different knowledge sources and improved over the 
baselines using single knowledge sources by 0.02 
~ 0.11. The biggest improvement (0.11) is on a 
domain-specific dataset, on which the method 
based on single knowledge source performed 
poorly in terms of coverage and accuracy. The best 
enrichment strategy that has consistently improved 
the baselines is wk-4F+wn-4F, Integration (Table 
4 v.s. Table 3).  With features enriched from 
multiple knowledge sources, the method also 
consistently improved over their corresponding 
single-source features on all datasets, except 
MeSH36, on which wk-4F+wn-4F, Combination 
(Table 4) slightly reduced the accuracy obtained 
with wk-4F (Table 3) only.  
The large proportion of uncovered word pairs 
using WordNet is due to its lack of coverage of 
specialized lexicons, and sparser semantic content. 
For example, of all 115 distinctive terms in the 
Ped30 and MeSH36 datasets, 30% are not included 
in WordNet. And of all 447 distinctive words in all 
general domain datasets, only 69% have multiple 
synonyms. Features such as attributes and ?see 
also? are present for less than 20 words. This is the 
reason that some approaches using WordNet (e.g., 
Agirre et al, 2009) require a graph of all WordNet 
lexicons to be built, thus intermediate words may 
?bridge? input words even if they do not connect 
directly by their features. Nevertheless, the 
improvement in accuracy and 100% coverage after 
harnessing both knowledge sources suggests that 
they complement each other well. On one hand, 
Wikipedia brings its strength in domain and 
content coverage; on the other hand, WordNet 
brings useful semantic evidences for words that are 
covered. 
Concept mapping and feature enrichment 
methods While the set overlap based method for 
cross-source concept mapping using the reference 
knowledge source concepts is simple and proved 
successful, the accuracy of mapping and its 
correlation with the accuracy of the SR method 
was not studied. This will be explored in the future. 
Also, alternative mapping methods will be 
investigated. For example, Toral and Mu?oz (2006) 
describe a different method of mapping Wikipedia 
articles to WordNet synsets; one could also adopt a 
simple disambiguation process to select the best 
candidate concept from each knowledge source 
suited for the input word pairs, whereby cross-
source concept mapping becomes straightforward. 
In terms of feature enrichment strategies, there is 
no strong indication (Table 6) of which (feature 
combination v.s. integration) is more effective, 
although the system consistently outperforms the 
baselines (Table 4 v.s. Table 3) with the wk-
4F+wn-4F, Integration strategy. 
Feature diversification v.s. unification Table 
5 suggests that in most cases, differentiating 
feature types leads to better results than merging 
them uniformly, despite the knowledge sources 
used. This is consistent with the findings by Zhang 
et al (2010). This can be understandable since 
although unifying feature types effectively 
increases possibility of sharing features, equally, 
this may also increase the proportion of noisy 
features. For example, consider the Wikipedia 
article of ?Horse? (animal), which has a category 
label ?livestock?; and the article ?Famine?, which 
has an outgoing link ?livestock? (in a sentence 
describing diseases that caused decline of livestock 
production). By differentiating the feature types 
?has_category? and ?has_outlink?, the two 
concepts will not be connected even if they both 
have the same word ?livestock? in their feature 
representation. However, using a bag-of-words 
representation where feature types are 
undistinguished, the strength of their relatedness is 
boosted by sharing this word, which may be 
uninteresting in this occasion. 
Compared against state-of-the-art, the 
proposed method has achieved promising results. 
Overall, by harnessing different knowledge sources, 
the method achieves, and in many cases, 
outperforms state-of-the-art. In the general domain, 
it outperforms state-of-the-art on three out of four 
datasets. It is worth noting that all methods based 
on WordNet generally have poor performance on 
the Fin153 and Fin200 datasets (Table 7). Despite 
the heterogeneity in these datasets, this may also 
relate to the quality of the feature space generated 
with WordNet. In fact methods using Wikipedia 
perform better on these datasets. With enriched 
features from both knowledge sources, the 
accuracies are further improved.   
999
In the biomedical domain, the proposed method 
outperforms state-of-the-art on one dataset and 
produces competitive results on others. Note that 
all other methods exploit domain-specific 
ontologies and corpora. The Ped06 best and Ped06 
second methods also depend on a corpus of one 
million documents. These results further confirmed 
the benefits of our method: harnessing knowledge 
from general-purpose knowledge sources of 
limited domain coverage, it is possible to achieve 
results that rival methods based on well-curated 
and specially tailored domain-specific knowledge 
sources. This is an encouraging finding. Although 
there are abundant resources in the biomedical 
domain for this type of tasks, such resources may 
be scarce in other domains and are expensive to 
build. However, the results suggest that the 
proposed method offers a more affordable 
approach that provides reasonable coverage and 
quality, even if individual general knowledge 
sources may be limited in themselves. 
Generality of the method. The proposed 
method represents features extracted from different 
knowledge sources in a generic manner, which 
facilitates cross-source feature enrichment and 
requires generic algorithm computation. As 
discussed in Section 3, semantic evidence of words 
and concepts may be extracted from different 
knowledge sources in different ways, while 
harnessed in the generic model. In contrast, other 
methods using multiple knowledge sources (e.g., 
Han and Zhao, 2010; Tsang and Stevenson, 2010) 
introduce algorithms that are bound to the 
knowledge sources, which may limit their 
adaptability and portability. 
6    Conclusion  
This paper introduced a generic method of 
harnessing different knowledge sources to compute 
semantic relatedness. We have shown empirically 
that different knowledge sources contain 
complementary semantic evidence, which, when 
combined together under a uniform model, can 
improve the accuracy of SR methods. Moreover, 
we have demonstrated its robustness in dealing 
with knowledge sources of different quality and 
coverage. Several remaining issues will be studied 
in the future. First, additional knowledge sources 
will be studied, particularly unstructured corpora 
and domain-specific resources. The experiments 
have shown that although harnessing different 
knowledge sources achieved encouraging results 
on biomedical datasets, they are still far from being 
perfect. While it should be appreciated that the 
results are obtained using only general purpose 
knowledge sources, it would be interesting to 
investigate whether harnessing domain specific 
knowledge sources (where available) further 
improves the performance. Second, different 
methods of concept mapping will be studied. We 
will also design methods for assessing the quality 
of mapping, and analyze their correlations with the 
SR methods. Third, analyses will be carried out to 
uncover the differences between feature 
combination and integration that have led to 
different accuracies. 
Acknowledgments 
Part of this research has been funded under the EC 
7th Framework Program, in the context of the 
SmartProducts project (231204). 
References  
Agirre, E., Alfonseca, E., Hall, K., Kravalova, J., Pasca, 
M., Soroa, A. 2009. A Study on Similarity and 
Relatedness Using Distributional and WordNet-
based Approaches. In Proceedings of NAACL?09 
Batet, M., S?nchez, D., Valls, A. 2010. An ontology-
based measure to compute semantic similarity in 
biomedicine. In Journal of Biomedical Informatics, 
44(1), 118-125 
Chen, H., Lin, M., Wei, Y. 2006. Novel association 
measures using web search with double checking. 
Proceedings of COLING?06-ACL?06, pp. 1009-
1016 
Cilibrasi, R., Vitanyi, P. 2007. The Google Similarity 
Distance. In IEEE Transactions on Knowledge and 
Data Engineering. 19(3), 370-383 
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., 
Solan, Z., Wolfman, G., and Ruppin, E. (2002). 
Placing search in context: the concept revisited. In 
ACM Transactions on Information Systems, 20 (1), 
pp. 116 ? 131 
Gabrilovich, E., Markovitch, S. 2007. Computing 
semantic relatedness using Wikipedia-based explicit 
semantic analysis. In proceeding of IJCAI'07 
Gouws, S., Rooyen, G., Engelbrecht, H. 2010. 
Measuring conceptual similarity by spreading 
activation over Wikipedia?s hyperlink structure. 
Proceedings of the 2nd Workshop on The People?s 
Web Meets NLP: Collaboratively Constructed 
Semantic Resources 
1000
Halavais , A. 2008. An Analysis of Topical Coverage of 
Wikipedia. Journal of Computer-Mediated 
Communication, 13(2) 
Han, X., Zhao, J. 2010. Structural semantic relatedness: 
a knowledge-based method to named entity 
disambiguation. In the 48th Annual Meeting of the 
Association for Computational Linguistics. 
Harrington, B. 2010. A semantic network approach to 
measuring relatedness. In Proceedings of COLING? 
10 
Hirst, G., and St-Onge, D. 1998. Lexical chains as 
representation of context for the detection and 
correction malapropisms. In Christiane Fellbaum 
(ed.), WordNet: An Electronic Lexical Database and 
Some of Its Applications, pp. 305?332. Cambridge, 
MA: The MIT Press. 
Holloway, T., Bozicevic, M., B?rner, K. 2007. 
Analyzing and visualizing the semantic coverage of 
Wikipedia and its authors. In Journal of Complexity, 
Special issue on Understanding Complex Systems, 
12(3), 30-40 
Jiang, J. and D. Conrath. 1997. Semantic similarity 
based on corpus statistics and lexical taxonomy. 
Proceedings of the International Conference on 
Research in Computational Linguistics, pp. 19-33 
Leacock, C., Chodorow, M. 1998. Combining local 
context and WordNet similarity for word sense 
identification. In C. Fellbaum (Ed.), WordNet. An 
Electronic Lexical Database, Chp. 11, pp. 265-283. 
Lin, D. 1998. An information-theoretic definition of 
similarity. Proceedings of the Fifteenth International 
Conference on Machine Learning, pp. 296-304 
Lord, P., Stevens, R., Brass, A., Goble, C. 2003. 
Investigating semantic similarity measures across 
the Gene Ontology: the relationship between 
sequence and annotation. In Bioinformatics, 19(10), 
pp. 1275?1283 
Matsuo, Y., T. Sakaki., K., Uchiyama, M., Ishizuka. 
2006. Graph-based word clustering using a web 
search engine. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP), pp.542-550 
Miller, G., Charles, W. 1991. Contextual correlates of 
semantic similarity. In Language and Cognitive 
Processes, 6(1): 1-28 
Pan, F., Farrell, R. 2007. Computing semantic similarity 
between skill statements for approximate matching. 
In Proceedings of NAACL-HLT?07, pp. 572-579 
Patwardhan, S., Pedersen, T. 2006. Using WordNet-
based context vectors to estimate the semantic 
relatedness of concepts. Proceedings of the EACL 
2006 Workshop on Making Sense of Sense: 
Bringing Computational Linguistics and 
Psycholinguistics Together 
Pedersen, T., Pakhomov, S., Patwardhan, S., Chute, C. 
2006. Measures of semantic similarity and 
relatedness in the biomedical domain. Journal of 
Biomedical Informatics 40(3), 288-299 
Pekar, V., Staab, S. 2002. Taxonomy learning: factoring 
the structure of a taxonomy into a semantic 
classification decision. Proceedings of COLING?02. 
pp. 786-792 
Pesquita, C., Faria, D., Bastos, H., Falc?o, A., Couto, F. 
(2007). Evaluating GO-based Semantic Similarity 
Measures. ISMB/ECCB 2007 SIG Meeting Program 
Materials, International Society for Computational 
Biology 2007 
Petrakis, E., Varelas, G., Hliaoutakis, A., Raftopoulou, 
P. 2006. Design and evaluation of semantic 
similarity measures for concepts stemming from the 
same or different ontologies. In 4th Workshop on 
Multimedia Semantics (WMS'06), pp. 44-52. 
Pirro, G. 2009. A semantic similarity metric combining 
features and intrinsic information content. In Data 
and Knowledge Engineering, 68(11), pp. 1289-1308 
Pozo A., Pazos F., Valencia, A. 2008. Defining 
functional distances over gene ontology. In BMC 
Bioinformatics 9, pp.50 
Rada, R., Mili, H., Bicknell, E., Blettner, M. 1989. 
Development and application of a metric on 
semantic nets. In IEEE Transactions on Systems, 
Man and Cybernetics 19(1), pp.17-30 
Resnik, P. (1995). Using information content to evaluate 
semantic similarity in a taxonomy. In Proceedings of 
IJCAI-95, pp. 448-453 
Riensche, R., Baddeley, B., Sanfilippo, A., Posse, C., 
Gopalan, B. 2007. XOA: Web-Enabled Cross-
Ontological Analytics. IEEE Congress on Services, 
pp. 99-105 
Rowe, M., Ciravegna, F. 2010. Disambiguating identity 
web references using Web 2.0 data and semantics. 
M Rowe and F Ciravegna. The Journal of Web 
Semantics. 
Rubenstein, H., Goodenough, J. 1965. Contextual 
correlates of synonymy. In Communications of the 
ACM, 8(10):627-633 
Seco, N., and Hayes, T. 2004. An intrinsic information 
content metric for semantic similarity in WordNet. 
In Proceedings of the 16th European conference on 
Artificial Intelligence 
Strube, M., Ponzetto, S. 2006. WikiRelate! Computing 
semantic relatedness using Wikipedia. In 
Proceedings of the 21st national conference on 
Artificial intelligence (AAAI) 
Toral, A., Mu?oz, R. 2006. A Proposal to Automatically 
Build and Maintain Gazetteers for Named Entity 
Recognition by using Wikipedia. In Proceedings of 
Workshop on New Text, ACL?06. 
Tsang, V., Stevenson, S. 2010. A graph-theoretic 
framework for semantic distance. In Journal of 
Computational Linguistics, 36(1). 
1001
Wojtinnek, P., Pulman, S. 2011. Semantic relatedness 
from automatically generated semantic networks. In 
Proceedings of the Ninth International Conference 
on Computational Semantics (IWCS?11) 
Wu, Z. Palmer, M. 1994. Verbs semantics and lexical 
selection. Proceedings of the 32nd annual meeting 
on Association for Computational Linguistics, pp. 
133-138 
Wu, H., Su, Z., Mao, F., Olman, V., Xu, Y. 2005. 
Prediction of functional modules based on 
comparative genome analysis and gene ontology 
application. Nucleic Acids Research, 33, pp. 2822?
2837.  
Yazdani, M., Popescu-Belis, A. 2010. A random walk 
framework to compute textual semantic similarity: a 
unified model for three benchmark tasks. IEEE 
Fourth International Conference on Semantic 
Computing (ICSC), pp. 424-429 
Ye, P., Peyser, B., Pan, X., Boek, J., Spencer, F., Bader, 
J. 2005. Gene function prediction from congruent 
synthetic lethal interactions in yeast. In Molecular 
system biology 
Yeh, E., Ramage, D., Manning, C., Agirre, E., Soroa, A. 
2009. WikiWalk: random walks on Wikipedia for 
semantic relatedness. In Proceedings of the 
TextGraphs-4, Workshop on Graph-based Methods 
for Natural Language Processing, ACL2009 
Zesch, T., and Gurevych, I. 2007. Analysis of the 
Wikipedia category graph for NLP applications. In 
Proceedings of the TextGraphs-2 Workshop 
(NAACL-HLT 2007), pp. 1?8 
Zesch, T., Gurevych, I. 2010. Wisdom of crowds versus 
wisdom of linguists: measuring the semantic 
relatedness of words. In Journal of Natural 
Language Engineering, 16, pp. 25-59 
Zhang, Z., Gentile, A., Xia, L., Iria, J., Chapman, S. 
2010. A random graph walk based approach to 
compute semantic relatedness using knowledge from 
Wikipedia. In Proceedings of LREC?10. 
 
1002
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 289?293,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Mining Equivalent Relations from Linked Data 
 
 
Ziqi Zhang1 Anna Lisa Gentile1 Isabelle Augenstein1 
Eva Blomqvist2 Fabio Ciravegna1 
1 Department of Computer Science, 
University of Sheffield, UK 
2 Department of Computer and Information 
Science, Link?ping University, Sweden 
{z.zhang, a.l.gentile, i.augenstein, 
f.ciravegna}@dcs.shef.ac.uk, eva.blomqvist@liu.se
 
 
Abstract 
Linking heterogeneous resources is a major re-
search challenge in the Semantic Web. This 
paper studies the task of mining equivalent re-
lations from Linked Data, which was insuffi-
ciently addressed before. We introduce an un-
supervised method to measure equivalency of 
relation pairs and cluster equivalent relations. 
Early experiments have shown encouraging 
results with an average of 0.75~0.87 precision 
in predicting relation pair equivalency and 
0.78~0.98 precision in relation clustering. 
1 Introduction 
Linked Data defines best practices for exposing, 
sharing, and connecting data on the Semantic 
Web using uniform means such as URIs and 
RDF. It constitutes the conjunction between the 
Web and the Semantic Web, balancing the rich-
ness of semantics offered by Semantic Web with 
the easiness of data publishing. For the last few 
years Linked Open Data has grown to a gigantic 
knowledge base, which, as of 2013, comprised 
31 billion triples in 295 datasets1.  
A major research question concerning Linked 
Data is linking heterogeneous resources, the fact 
that publishers may describe analogous infor-
mation using different vocabulary, or may assign 
different identifiers to the same referents. Among 
such work, many study mappings between ontol-
ogy concepts and data instances (e.g., Isaac et al 
2007; Mi et al, 2009; Le et al, 2010; Duan et al, 
2012). An insufficiently addressed problem is 
linking heterogeneous relations, which is also 
widely found in data and can cause problems in 
information retrieval (Fu et al, 2012). Existing 
work in linking relations typically employ string 
similarity metrics or semantic similarity mea-
                                                 
1 http://lod-cloud.net/state/ 
sures that require a-priori domain knowledge and 
are limited in different ways (Zhong et al, 2002; 
Volz et al, 2009; Han et al, 2011; Zhao and 
Ichise, 2011; Zhao and Ichise, 2012).  
This paper introduces a novel method to dis-
cover equivalent groups of relations for Linked 
Data concepts. It consists of two components: 1) 
a measure of equivalency between pairs of rela-
tions of a concept and 2) a clustering process to 
group equivalent relations. The method is unsu-
pervised; completely data-driven requiring no a-
priori domain knowledge; and also language in-
dependent. Two types of experiments have been 
carried out using two major Linked Data sets: 1) 
evaluating the precision of predicting equivalen-
cy of relation pairs and 2) evaluating the preci-
sion of clustering equivalent relations. Prelimi-
nary results have shown encouraging results as 
the method achieves between 0.75~0.85 preci-
sion in the first set of experiments while 
0.78~0.98 in the latter. 
2 Related Work  
Research on linking heterogeneous ontological 
resources mostly addresses mapping classes (or 
concepts) and instances (Isaac et al 2007; Mi et 
al., 2009; Le et al, 2010; Duan et al, 2012; 
Schopman et al, 2012), typically based on the 
notions of similarity. This is often evaluated by 
string similarity (e.g. string edit distance), se-
mantic similarity (Budanitsky and Hirst, 2006), 
and distributional similarity based on the overlap 
in data usage (Duan et al, 2012; Schopman et 
al., 2012). There have been insufficient studies 
on mapping relations (or properties) across on-
tologies. Typical methods make use of a combi-
nation of string similarity and semantic similarity 
metrics (Zhong et al, 2002; Volz et al, 2009; 
Han et al, 2011; Zhao and Ichise, 2012). While 
string similarity fails to identify equivalent rela-
tions if their lexicalizations are distinct, semantic 
similarity often depends on taxonomic structures 
289
in existing ontologies (Budanitsky and Hirst, 
2006). Unfortunately many Linked Data instanc-
es use relations that are invented arbitrarily or 
originate in rudimentary ontologies (Parundekar 
et al, 2012). Distributional similarity has also 
been used to discover equivalent or similar rela-
tions. Mauge et al (2012) extract product proper-
ties from an e-commerce website and align 
equivalent properties using a supervised maxi-
mum entropy classification method. We study 
linking relations on Linked Data and propose an 
unsupervised method. Fu et al (2012) identify 
similar relations using the overlap of the subjects 
of two relations and the overlap of their objects. 
On the contrary, we aim at identifying strictly 
equivalent relations rather than similarity in gen-
eral. Additionally, the techniques introduced our 
work is also related to work on aligning multilin-
gual Wikipedia resources (Adar et al, 2009; 
Bouma et al, 2009) and semantic relatedness 
(Budanitsky and Hirst, 2006). 
3 Method 
Let t denote a 3-tuple (triple) consisting of a sub-
ject (ts), predicate (tp) and object (to). Linked Da-
ta resources are typed and its type is called class. 
We write type (ts) = c meaning that ts is of class c. 
p denotes a relation and rp is a set of triples 
whose tp=p, i.e., rp={t | tp = p}. 
Given a specific class c, and its pairs of rela-
tions (p, p?) such that rp={t|tp=p, type(ts)=c} and 
rp?={t|tp=p?, type (ts)=c}, we measure the equiv-
alency of p and p? and then cluster equivalent 
relations. The equivalency is calculated locally 
(within same class c) rather than globally (across 
all classes) because two relations can have iden-
tical meaning in specific class context but not 
necessarily so in general. For example, for the 
class Book, the relations dbpp:title and foaf:name 
are used with the same meaning, however for 
Actor, dbpp:title is used interchangeably with 
awards dbpp:awards (e.g., Oscar best actor). 
In practice, given a class c, our method starts 
with retrieving all t from a Linked Data set 
where type(ts)=c, using the universal query lan-
guage SPARQL with any SPARQL data end-
point. This data is then used to measure equiva-
lency for each pair of relations (Section 3.1). The 
equivalence scores are then used to group rela-
tions in equivalent clusters (Section 3.2). 
3.1 Measure of equivalence 
The equivalence for each distinct pair of rela-
tions depends on three components. 
Triple overlap evaluates the degree of over-
lap2 in terms of the usage of relations in triples. 
Let SO(p) be the collection of subject-object 
pairs from rp and SOint the intersection 
)r(SO)r(SO)'p,p(SO 'ppint ??
           [1] 
then the triple overlap TO(p, p?) is calculated as 
}|r|
|)r,r(SO|,|r|
|)r,r(SO|{MAX
'p
'ppint
p
'ppint
        [2] 
Intuitively, if two relations p and p? have a 
large overlap of subject-object pairs in their data 
instances, they are likely to have identical mean-
ing. The MAX function allows addressing infre-
quently used, but still equivalent relations (i.e., 
where the overlap covers most triples of an in-
frequently used relation but only a very small 
proportion of a much more frequently used).  
Subject agreement While triple overlap looks 
at the data in general, subject agreement looks at 
the overlap of subjects of two relations, and the 
degree to which these subjects have overlapping 
objects. Let S(p) return the set of subjects of rela-
tion p, and O(p|s) returns the set of objects of 
relation p whose subjects are s, i.e.: 
}st,pt|t{)s|r(O)s|p(O spop ????
        [3] 
we define: 
)r(S)r(S)'p,p(S 'ppint ??
         [4] 
|)'p,p(S|
otherwise,
|)s|'p(O)s|p(O|if,
int
)'p,p(Ss int
?
?
??
??
0
01
         [5] 
|)'p(S)p(S|/|)'p,p(S| int ???
        [6] 
then the agreement AG(p, p?) is  
????)'p,p(AG            [7] 
Equation [5] counts the number of overlapping 
subjects whose objects have at least one overlap. 
The higher the value of ?, the more the two rela-
tions ?agree? in terms of their shared subjects. 
For each shared subject of p and p? we count 1 if 
they have at least 1 overlapping object and 0 oth-
erwise. This is because both p and p? can be 
1:many relations and a low overlap value could 
mean that one is densely populated while the 
other is not, which does not necessarily mean 
they do not ?agree?. Equation [6] evaluates the 
degree to which two relations share the same set 
of subjects. The agreement AG(p, p?) balances 
the two factors by taking the product. As a result, 
                                                 
2 In this paper overlap is based on ?exact? match. 
290
relations that have high level of agreement will 
have more subjects in common, and higher pro-
portion of shared subjects with shared objects. 
Cardinality ratio is a ratio between cardinali-
ty of the two relations. Cardinality of a relation 
CD(p) is calculated based on data: 
|)r(S|
|r|)p(CD
p
p?
         [8] 
and the cardinality ratio is calculated as 
)}'p(CD),p(CD{MAX
)}'p(CD),p(CD{IN)'p,p(CDR ?
       [9] 
The final equivalency measure integrates all 
the three components to return a value in [0, 2]: 
)'p,p(CDR
)'p,p(AG)'p,p(TO)'p,p(E ??
              [10] 
The measure will favor two relations that have 
similar cardinality.  
3.2 Clustering 
We apply the measure to every pair of relations 
of a concept, and keep those with a non-zero 
equivalence score. The goal of clustering is to 
create groups of equivalent relations based on the 
pair-wise equivalence scores. We use a simple 
rule-based agglomerative clustering algorithm 
for this purpose. First, we rank all relation pairs 
by their equivalence score, then we keep a pair if 
(i) its score and (ii) the number of triples covered 
by each relation are above a certain threshold, 
TminEqvl and TminTP respectively. Each pair forms 
an initial cluster. To merge clusters, given an 
existing cluster c and a new pair (p, p?) where 
either p?c or p??c, the pair is added to c if E(p, 
p?) is close (as a fractional number above the 
threshold TminEqvlRel) to the average scores of all 
connected pairs in c. This preserves the strong 
connectivity in a cluster. This is repeated until no 
merge action is taken. Adjusting these thresholds 
allows balancing between precision and recall. 
4 Experiment Design 
To our knowledge, there is no publically availa-
ble gold standard for relation equivalency using 
Linked Data. We randomly selected 21 concepts 
(Figure 1) from the DBpedia ontology (v3.8): 
Actor, Aircraft, Airline, Airport, Automobile, 
Band, BasketballPlayer, Book, Bridge, Comedian, 
Film, Hospital, Magazine, Museum, Restaurant, 
Scientist, TelevisionShow, TennisPlayer, Theatre, 
University, Writer 
Figure 1. Concepts selected for evaluation. 
We apply our method to each concept to dis-
cover clusters of equivalent relations, using as 
SPARQL endpoint both DBpedia3 and Sindice4 
and report results separately. This is to study 
how the method performs in different conditions: 
on one hand on a smaller and cleaner dataset 
(DBpedia); on the other hand on a larger and 
multi-lingual dataset (Sindice) to also test cross-
lingual capability of our method. We chose rela-
tively low thresholds, i.e. TminEqvl=0.1, TminTP= 
0.01% and TminEqvlRel=0.6, in order to ensure high 
recall without sacrificing much precision.  
Four human annotators manually annotated 
the output for each concept. For this preliminary 
evaluation, we have limited the amount of anno-
tations to a maximum of 100 top scoring pairs of 
relations per concept, resulting in 16~100 pairs 
per concept (avg. 40) for DBpedia experiment 
and 29~100 pairs for Sindice (avg. 91). The an-
notators were asked to rate each edge in each 
cluster with -1 (wrong), 1 (correct) or 0 (cannot 
decide). Pairs with 0 are ignored in the evalua-
tion (about 12% for DBpedia; and 17% for Sin-
dice mainly due to unreadable encoded URLs for 
certain languages). To evaluate cross-lingual 
pairs, we asked annotators to use translation 
tools. Inter-Annotator-Agreement (observed 
IAA) is shown in Table 1. Also using this data, 
we derived a gold standard for clustering based 
on edge connectivity and we evaluate (i) the pre-
cision of top n% (p@n%) ranked equivalent rela-
tion pairs and (ii) the precision of clustering for 
each concept.  
 Mean High Low 
DBpedia 0.79 0.89 0.72 
Sindice 0.75 0.82 0.63 
Table 1. IAA on annotating pair equivalency 
So far the output of 13 concepts has been an-
notated. This dataset 5  contains ?1800 relation 
pairs and is larger than the one by Fu et al 
(2012). Annotation process shows that over 75% 
of relation pairs in the Sindice experiment con-
tain non-English relations and mostly are cross-
lingual. We used this data to report performance, 
although the method has been applied to all the 
21 concepts, and the complete results can be vis-
ualized at our demo website link. Some examples 
are shown in Figure 2.  
                                                 
3 http://dbpedia.org/sparql 
4 http://sparql.sindice.com/ 
5 http://staffwww.dcs.shef.ac.uk/people/Z.Zhang/ re-
sources/paper/acl2013short/web/ 
291
 Figure 2. Examples of visualized clusters 
5 Result and Discussion 
Figure 3 shows p@n% for pair equivalency6 and 
Figure 4 shows clustering precision.  
 
Figure 3. p@n%. The box plots show the ranges of 
precision at each n%; the lines show the average. 
 
Figure 4. Clustering precision  
As it is shown in Figure 2, Linked Data rela-
tions are often heterogeneous. Therefore, finding 
equivalent relations to improve coverage is im-
portant. Results in Figure 3 show that in most 
cases the method identifies equivalent relations 
with high precision. It is effective for both sin-
gle- and cross-language relation pairs. The worst 
performing case for DBpedia is Aircraft (for all 
n%), mostly due to duplicating numeric valued 
objects of different relations (e.g., weight, length, 
capacity). The decreasing precision with respect 
to n% suggests the measure effectively ranks 
correct pairs to the top. This is a useful feature 
from IR point of view. Figure 4 shows that the 
method effectively clusters equivalent relations 
with very high precision: 0.8~0.98 in most cases. 
                                                 
6 Per-concept results are available on our website. 
Overall we believe the results of this early proof-
of-concept are encouraging. As a concrete exam-
ple to compare against Fu et al (2012), for Bas-
ketballPlayer, our method creates separate clus-
ters for relations meaning ?draft team? and ?for-
mer team? because although they are ?similar? 
they are not ?equivalent?. 
We noticed that annotating equivalent rela-
tions is a non-trivial task. Sometimes relations 
and their corresponding schemata (if any) are 
poorly documented and it is impossible to under-
stand the meaning of relations (e.g., due to acro-
nyms) and even very difficult to reason based on 
data. Analyses of the evaluation output show that 
errors are typically found between highly similar 
relations, or whose object values are numeric 
types. In both cases, there is a very high proba-
bility of having a high overlap of subject-object 
pairs between relations. For example, for Air-
craft, the relations dbpp:heightIn and dbpp: 
weight are predicted to be equivalent because 
many instances have the same numeric value for 
the properties. Another example are the Airport 
properties dbpp:runwaySurface, dbpp:r1Surface, 
dbpp:r2Surface etc., which according to the data 
seem to describe the construction material (e.g., 
concrete, asphalt) of airport runways. The rela-
tions are semantically highly similar and the ob-
ject values have a high overlap. A potential solu-
tion to such issues is incorporating ontological 
knowledge if available. For example, if an ontol-
ogy defines the two distinct properties of Airport 
without explicitly defining an ?equivalence? re-
lation between them, they are unlikely to be 
equivalent even if the data suggests the opposite.  
6 Conclusion 
This paper introduced a data-driven, unsuper-
vised and domain and language independent 
method to learn equivalent relations for Linked 
Data concepts. Preliminary experiments show 
encouraging results as it effectively discovers 
equivalent relations in both single- and multi-
lingual settings. In future, we will revise the 
equivalence measure and also experiment with 
clustering algorithms such as (Beeferman et al, 
2000). We will also study the contribution of 
individual components of the measure in such 
task. Large scale comparative evaluations (incl. 
recall) are planned and this work will be extend-
ed to address other tasks such as ontology map-
ping and ontology pattern mining (Nuzzolese et 
al., 2011).  
 
292
Acknowledgement 
Part of this research has been sponsored by the 
EPSRC funded project LODIE: Linked Open 
Data for Information Extraction, EP/J019488/1. 
Additionally, we also thank the reviewers for 
their valuable comments given for this work. 
 
References  
Eytan Adar, Michael Skinner, Daniel Weld. 
2009. Information Arbitrage across Multi-
lingual Wikipedia. Proceedings of the Second 
ACM International Conference on Web 
Search and Data Mining, pp. 94 ? 103. 
Gosse Bouma, Sergio Duarte, Zahurul Islam. 
2009. Cross-lingual Alignment and Comple-
tion of Wikipedia Templates. Proceedings of 
the Third International Workshop on Cross 
Lingual Information Access: Addressing the 
Information Need of Multilingual Societies, 
pp. 61 ? 69   
Doug Beeferman, Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. 
Proceedings of the sixth ACM SIGKDD inter-
national conference on Knowledge discovery 
and data mining, pp. 407-416. 
Alexander Budanitsky and Graeme Hirst. 2006. 
Evaluating WordNet-based Measures of Se-
mantic Distance. Computational Linguistics, 
32(1), pp.13-47. 
Songyun Duan, Achille Fokoue, Oktie Has-
sanzadeh, Anastasios Kementsietsidis, Kavitha 
Srinivas, and Michael J. Ward. 2012. In-
stance-Based Matching of Large Ontologies 
Using Locality-Sensitive Hashing. ISWC 
2012, pp. 46 ? 64 
Linyun Fu, Haofen Wang, Wei Jin, Yong Yu. 
2012. Towards better understanding and uti-
lizing relations in DBpedia. Web Intelligence 
and Agent Systems , Volume 10 (3) 
Andrea Nuzzolese, Aldo Gangemi, Valentina 
Presutti, Paolo Ciancarini. 2011. Encyclopedic 
Knowledge Patterns from Wikipedia Links. 
Proceedings of the 10th International Semantic 
Web Conference, pp. 520-536 
Lushan Han, Tim Finin and Anupam Joshi. 2011. 
GoRelations: An Intuitive Query System for 
DBpedia. Proceedings of the Joint Internation-
al Semantic Technology Conference 
Antoine Isaac, Lourens van der Meij, Stefan 
Schlobach, Shenghui Wang. 2007. An empiri-
cal study of instance-based ontology match-
ing. Proceedings of the 6th International Se-
mantic Web Conference and the 2nd Asian 
conference on Asian Semantic Web Confer-
ence, pp. 253-266 
Ngoc-Thanh Le, Ryutaro Ichise, Hoai-Bac Le. 
2010. Detecting hidden relations in geograph-
ic data. Proceedings of the 4th International 
Conference on Advances in Semantic Pro-
cessing, pp. 61 ? 68 
Karin Mauge, Khash Rohanimanesh, Jean-David 
Ruvini. 2012. Structuring E-Commerce Inven-
tory. Proceedings of ACL2012, pp. 805-814 
Jinhua Mi, Huajun Chen, Bin Lu, Tong Yu, 
Gang Pan. 2009. Deriving similarity graphs 
from open linked data on semantic web. Pro-
ceedings of the 10th IEEE International Con-
ference on Information Reuse and Integration, 
pp. 157?162. 
Rahul Parundekar, Craig Knoblock,  Jos? Luis. 
Ambite. 2012. Discovering Concept Cover-
ings in Ontologies of Linked Data Sources. 
Proceedings of ISWC2012, pp. 427?443. 
Balthasar Schopman, Shenghui Wang, Antoine 
Isaac, Stefan Schlobach. 2012. Instance-Based 
Ontology Matching by Instance Enrichment. 
Journal on Data Semantics, 1(4), pp 219-236 
Julius Volz, Christian Bizer, Martin Gaedke, 
Georgi Kobilarov. 2009. Silk ? A Link Discov-
ery Framework for the Web of Data. Proceed-
ings of the 2nd Workshop on Linked Data on 
the Web 
Lihua Zhao, Ryutaro Ichise. 2011. Mid-ontology 
learning from linked data. Proceedings of the 
Joint International Semantic Technology Con-
ference, pp. 112 ? 127. 
Lihua Zhao, Ryutaro Ichise. 2012. Graph-based 
ontology analysis in the linked open data. Pro-
ceedings of the 8th International Conference 
on Semantic Systems, pp. 56 ? 63 
Jiwei Zhong, Haiping Zhu, Jianming Li and 
Yong Yu. 2002. Conceptual Graph Matching 
for Semantic Search. The 2002 International 
Conference on Computational Science. 
293
