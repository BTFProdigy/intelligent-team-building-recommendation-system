Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1475?1483,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Chinese Semantic Role Labeling with Shallow Parsing
Weiwei Sun and Zhifang Sui and Meng Wang and Xin Wang
Institute of Computational Linguistics
Peking University
Key Laboratory of Computational Linguistics
Ministry of Education, China
weiwsun@gmail.com;{szf,wm}@pku.edu.cn;xinwang.cpku@gmail.com;
Abstract
Most existing systems for Chinese Seman-
tic Role Labeling (SRL) make use of full
syntactic parses. In this paper, we evalu-
ate SRL methods that take partial parses as
inputs. We first extend the study on Chi-
nese shallow parsing presented in (Chen
et al, 2006) by raising a set of addi-
tional features. On the basis of our shal-
low parser, we implement SRL systems
which cast SRL as the classification of
syntactic chunks with IOB2 representation
for semantic roles (i.e. semantic chunks).
Two labeling strategies are presented: 1)
directly tagging semantic chunks in one-
stage, and 2) identifying argument bound-
aries as a chunking task and labeling their
semantic types as a classification task. For
both methods, we present encouraging re-
sults, achieving significant improvements
over the best reported SRL performance
in the literature. Additionally, we put
forward a rule-based algorithm to auto-
matically acquire Chinese verb formation,
which is empirically shown to enhance
SRL.
1 Introduction
In the last few years, there has been an increas-
ing interest in Semantic Role Labeling (SRL) on
several languages, which consists of recognizing
arguments involved by predicates of a given sen-
tence and labeling their semantic types. Nearly
all previous Chinese SRL research took full syn-
tactic parsing as a necessary pre-processing step,
such as (Sun and Jurafsky, 2004; Xue, 2008; Ding
and Chang, 2008). Many features are extracted to
encode the complex syntactic information. In En-
glish SRL research, there have been some attempts
at relaxing the necessity of using full syntactic
parses; better understanding of SRL with shallow
parsing is achieved by CoNLL-2004 shared task
(Carreras and M`arquez, 2004). However, it is still
unknown how these methods perform on other lan-
guages, such as Chinese.
To date, the best SRL performance reported on
the Chinese Proposition Bank (CPB) corresponds
to a F-measure is 92.0, when using the hand-
crafted parse trees from Chinese Penn Treebank
(CTB). This performance drops to 71.9 when a
real parser is used instead
1
(Xue, 2008). Com-
paratively, the best English SRL results reported
drops from 91.2 (Pradhan et al, 2008) to 80.56
(Surdeanu et al, 2007). These results suggest that
as still in its infancy stage, Chinese full parsing
acts as a central bottleneck that severely limits our
ability to solve Chinese SRL. On the contrary, Chi-
nese shallow parsing has gained a promising re-
sult (Chen et al, 2006); hence it is an alternative
choice for Chinese SRL.
This paper addresses the Chinese SRL problem
on the basis of shallow syntactic information at
the level of phrase chunks. We first extend the
study on Chinese chunking presented in (Chen et
al., 2006) by raising a set of additional features.
The new set of features yield improvement over
the strong chunking system described in (Chen et
al., 2006). On the basis of our shallow parser, we
implement lightweight systems which solve SRL
as a sequence labeling problem. This is accom-
plished by casting SRL as the classification of syn-
tactic chunks (e.g. NP-chunk) into one of semantic
labels with IOB2 representation (?). With respect
to the labeling strategy, we distinguish two differ-
ent approaches. The first one directly recognizes
semantic roles by an IOB-type sequence tagging.
The second approach divides the problem into two
independent subtasks: 1) Argument Identification
(AI) and 2) Semantic Role Classification (SRC).
1
This F-measure is evaluated on the basis of hand-crafted
word segmentation and POS tagging.
1475
A Chinese word consists of one or more char-
acters, and each character, in most cases, is a mor-
pheme. The problem of how the words are con-
structed from morphemes, known as word for-
mation, is very important for a majority of Chi-
nese language processing tasks. To capture Chi-
nese verb formation information, we introduce a
rule-based algorithm with a number of heuristics.
Experimental results indicate that word formation
features can help both shallow parsing and SRL.
We present encouraging SRL results on CPB
2
.
The best F-measure performance (74.12) with
gold segmentation and POS tagging can be
achieved by the first method. This result yield
significant improvement over the best reported
SRL performance (71.9) in the literature (Xue,
2008). The best recall performance (71.50) can be
achieved by the second method. This result is also
much higher than the best reported recall (65.6) in
(Xue, 2008).
2 Related Work
Previous work on Chinese SRL mainly focused on
how to implement SRL methods which are suc-
cessful on English, such as (Sun and Jurafsky,
2004; Xue and Palmer, 2005; Xue, 2008; Ding
and Chang, 2008). Sun and Jurafsky (2004) did
the preliminary work on Chinese SRL without
any large semantically annotated corpus of Chi-
nese. Their experiments were evaluated only on
ten specified verbs with a small collection of Chi-
nese sentences. This work made the first attempt
on Chinese SRL and produced promising results.
After the CPB was built, (Xue and Palmer, 2005)
and (Xue, 2008) have produced more complete
and systematic research on Chinese SRL. Ding
and Chang (2008) divided SRC into two sub-tasks
in sequence. Under the hierarchical architecture,
each argument should first be determined whether
it is a core argument or an adjunct, and then be
classified into fine-grained categories. Chen et
al. (2008) introduced an application of transduc-
tive SVM in Chinese SRL. Because their experi-
ments took hand-crafted syntactic trees as input,
how transductive SVMs perform in Chinese SRL
in realistic situations is still unknown.
Most existing systems for automatic Chinese
SRL make use of a full syntactic parse of the sen-
tence in order to define argument boundaries and
2
Our system is available at
http://code.google.com/p/csrler/
to extract relevant information for training clas-
sifiers to disambiguate between role labels. On
the contrary, in English SRL research, there have
been some attempts at relaxing the necessity of us-
ing syntactic information derived from full parse
trees. For example, Hacioglu and Ward (2003)
considered SRL as a chunking task; Pradhan et
al. (2005) introduced a new procedure to incor-
porate SRL results predicted respectively on full
and shallow syntactic parses. Previous work on
English suggests that even good labeling perfor-
mance has been achieved by full parse based SRL
systems, partial parse based SRL systems can still
enhance their performance. Though better under-
standing of SRL with shallow parsing on English
is achieved by CoNLL-2004 shared task (Carreras
and M`arquez, 2004), little is known about how
these SRL methods perform on Chinese.
3 Chinese Shallow Parsing
There have been some research on Chinese shal-
low parsing, and a variety of chunk defini-
tions have been proposed. However, most of
these studies did not provide sufficient detail.
In our system, we use chunk definition pre-
sented in (Chen et al, 2006), which provided
a chunk extraction tool. The tool to extract
chunks from CTB was developed by modify-
ing the English tool used in CoNLL-2000 shared
task, Chunklink
3
, and is publicly available at
http://www.nlplab.cn/chenwl/chunking.html. The
definition of syntactic chunks is illustrated in Line
CH in Figure 1. For example, ?????/the in-
surance company?, consisting of two nouns, is a
noun phrase.
With IOB2 representation (Ramshaw and Mar-
cus, 1995), the problem of Chinese chunking can
be regarded as a sequence labeling task. In this
paper, we first implement the chunking method
described in (Chen et al, 2006) as a strong base-
line. To conveniently illustrate, we denote a word
in focus with a fixed window w
?2
w
?1
ww
+1
w
+2
,
where w is current token. The baseline features
includes:
? Uni-gram word/POS tag feature: w
?2
, w
?1
,
w, w
+1
, w
+2
;
? Bi-gram word/POS tag feature: w
?2
w
?1
,
w
?1
w, w w
+1
, w
+1
w
+2
;
3
http://ilk.uvt.nl/team/sabine/chunklink/chunklink 2-2-
2000 for conll.pl
1476
WORD: ?? ?? ?? ?? ? ? ?? ?? ?? ?? ??
POS: [P] [NT] [NN NN] [AD] [P] [NR] [NN] [VP] [NN NN]
CH: [PP NP] [NP] [ADVP] [PP NP NP ] [VP] [NP]
M1: B-A* I-A*
4
B-A0 B-AM-ADV B-A2 I-A2 I-A2 B-V B-A1
M2-AI: B-A I-A B-A B-A B-A I-A I-A B-V B-A
M2-SRC: AM-TMP A0 AM-ADV A2 Rel A1
Until now, the insurance company has provided insurance services for the Sanxia Project.
Figure 1: An example from Chinese PropBank.
That means 18 features are used to represent a
given token. For instance, the bi-gram Word fea-
tures at 5th word position (???/company?) in
Figure 1 are ?? ???, ??? ???, ??? ??,
?? ??.
To improve shallow parsing, we raised an addi-
tional set of features. We will discuss these fea-
tures in section 5.
4 SRL with Shallow Parsing
The CPB is a project to add predicate-argument
relations to the syntactic trees of the CTB. Similar
to English PropBank, the semantic arguments of a
predicate are labeled with a contiguous sequence
of integers, in the form of AN (i.e. ArgN ); the ad-
juncts are annotated as such with the label AM (i.e.
ArgM) followed by a secondary tag that represents
the semantic classification of the adjunct. The as-
signment of argument labels is illustrated in Figure
1, where the predicate is the verb ???/provide?.
For example, the noun phrase ?????/the in-
surance company? is labeled as A0, meaning that it
is the proto-Agent of ??; the preposition phrase
?????/until now? is labeled as AM-TMP, in-
dicating a temporal component.
4.1 System Architecture
SRL is a complex task which has to be decom-
posed into a number of simpler decisions and tag-
ging schemes in order to be addressed by learn-
ing techniques. Regarding the labeling strategy,
we can distinguish at least two different strategies.
The first one consists of performing role identifi-
cation directly as IOB-type sequence tagging. The
second approach consists of dividing the problem
into two independent subtasks.
4
The semantic chunk labels here are B-AM-TMP and I-
AM-TMP. Limited to the document length, we cannot put all
detailed chunk labels in one line in Figure 1.
4.1.1 One-stage Strategy
In the one-stage strategy, on the basis of syntac-
tic chunks, we define semantic chunks which do
not overlap nor embed using IOB2 representation.
Syntactic chunks outside a chunk receive the tag
O. For syntactic chunks forming a chunk of type
A*, the first chunk receives the B-A* tag (Begin),
and the remaining ones receive the tag I-A* (In-
side). Then a SRL system can work directly by
using sequence tagging techinique. Since the se-
mantic annotation in the PropBank corpus does
not have any embedded structure, there is no loss
of information in this representation. The line M1
in Figure 1 illustrates this semantic chunk defini-
tion.
4.1.2 Two-stage Strategy
In the two-stage architecture, we divide Chinese
SRL into two subtasks: 1) semantic chunking for
AI, in which the argument boundaries are pre-
dicted, and 2) classification for SRC, in which the
already recognized arguments are assigned role la-
bels. In the first stage, we define semantic chunks
B-A which means begin of an argument and I-A
which means inside of an argument. In the second
stage, we solve SRC problem as a multi-class clas-
sification. The lines M2-AI and M2-SRC in Fig-
ure 1 illustrate this two-stage architecture. For ex-
ample, the noun phrase ?????/the insurance
company? is proto-Agent, and thus should be la-
beled as B-A in the AI chunking phase, and then
be tagged as A0. The phrase ??????/for the
Sanxia Project? consists of three chunks, which
should be labeled as B-A, I-A, and I-A respectively
in the AI chunking phase, then these three chunks
as a whole argument should be recognized as A2.
4.1.3 Chunk-by-Chunk
There is also another semantic chunk definition,
where the basic components of a semantic chunk
are words rather than syntactic chunks. A good
election for this problem is chunk-by-chunk pro-
1477
cessing instead of word-by-word. The motivation
is twofold: 1) phrase boundaries are almost always
consistent with argument boundaries; 2) chunk-
by-chunk processing is computationally less ex-
pensive and allows systems to explore a relatively
larger context. This paper performs a chunk-by-
chunk processing, but admitting a processing by
words within the target verb chunks.
4.2 Features
Most of the feature templates are ?standard?,
which have been used in previous SRL research.
We give a brief description of ?standard? features,
but explain our new features in detail.
5
4.2.1 Features for Semantic Chunking
In the semantic chunking tasks, i.e. the one-stage
method and the first step in the two-stage method,
we use the same set of features. The features
are extracted from three types of elements: syn-
tactic chunks, target verbs, links between chunks
and target verbs. They are formed making use
of words, POS tags and chunks of the sentence.
Xue (2008) put forward a rough verb classifica-
tion where verb classes are automatically derived
from the frame files, which are verb lexicon for
the CPB annotation. This kind of verb class in-
formation has been shown very useful for Chinese
SRL. Our system also includes this feature. In our
experiments, we represent a verb in two dimen-
sions: 1) number of arguments, and 2) number of
framesets. For example, a verb may belong to the
class ?C1C2,? which means that this verb has two
framesets, with the first frameset having one argu-
ment and the second having two arguments.
To conveniently illustrate, we de-
note a token chunk with a fixed context
w
i?1
[
c
k
w
i
...w
h
...w
j
]w
j+1
, where w
h
is the
head word of this chunk c
k
. The complete list of
features is listed here.
Extraction on Syntactic Chunks
Chunk type: c
k
.
Length: the number of words in a chunk.
Head word/POS tag. The rules described in
(Sun and Jurafsky, 2004) are used to extract head
word.
IOB chunk tag of head word: chunk tag of head
word with IOB2 representation (e.g. B-NP, I-NP).
5
The source code of our system also provides lots of com-
ments for implementation of all features.
Chunk words/POS tags context. Chunk con-
text includes one word before and one word after:
w
i?1
and w
j+1
.
POS tag chain: sequential containers of each
word?s POS tag: w
i
... w
j
. For example, this fea-
ture for ?????? is ?NN NN?.
Position: the position of the phrase with respect
to the predicate. It has three values as before, after
and here.
Extraction on Target Verbs Given a target verb
w
v
and its context, we extract the following fea-
tures.
Predicate, its POS tag, and its verb class.
Predicate IOB chunk tag context: the chain of
IOB2 chunk tags centered at the predicate within
a window of size -2/+2.
Predicate POS tag context: the POS tags of
the words that immediately precede and follow the
predicate.
Number of predicates: the number of predicates
in the sentence.
Extraction on Links To capture syntactic prop-
erties of links between the chunks and the verbs,
we use the following features.
Path: a flat path is defined as a chain of base
phrases between the token and the predicate. At
both ends, the chain is terminated with the POS
tags of the predicate and the headword of the to-
ken.
Distance: we have two notions of distance. The
first is the distance of the token from the predicate
as a number of base phrases, and the second is the
same distance as the number of VP chunks.
Combining Features We also combine above
features as some new features.
Conjunctions of position and head word, tar-
get verb, and verb class, including: position w
h
,
position w
v
, position w
h
w
v
, position class,
and position w
h
class.
Conjunctions of position and POS tag of
head word, target verb, and verb class, in-
cluding: position w
h
w
v
, position w
h
, and
position w
h
class.
4.2.2 Features for SRC
In the SRC stage of the two-stage method, dif-
ferent from previous work, our system only uses
word-based features, i.e. features extracted from
words and POS tags, to represent a given argu-
ment. Experiments show that a good semantic
1478
role classifier can be trained by using only word-
based features. To gather all argument position
information predicted in AI stage, we design a
coarse frame feature, which is a sequential collec-
tion of arguments. So far, we do not know the
detailed semantic type of each argument, and we
use XP as each item in the frame. To distinguish
the argument in focus, we use a special symbol
to indicate the corresponding frame item. For in-
stance, the Frame feature for argument ???
? is XP+XP+XP+XP+V+!XP, where !XP means
that it is the argument in focus.
Denote 1) a given argument
w
i?2
w
i?1
[w
i
w
i+1
...w
j?1
w
j
]w
j+1
w
j+2
, and
2) a given predicate w
v
. The features for SRC are
listed as follows.
Words/POS tags context of arguments: the con-
tents and POS tags of the following words: w
i
,
w
i?1
, w
i?2
, w
i+1
, w
i+2
, w
j
, w
j+1
, w
j?1
, w
j?2
,
w
j+1
, w
j+2
; the POS tags of the following words:
w
i+1
, w
i+2
, w
j+1
, w
j+2
.
Token Position.
Predicate, its POS, and its verb class.
Coarse Frame.
Combining features: conjunctions of bound-
ary words, including w
i?1
w
j+1
and w
i?2
w
j+2
;
conjunction of POS tags of boundary words, in-
cluding w
i?1
w
j+1
and w
i?2
w
j+2
; conjunction
of token position, boundary words, and predi-
cate word, including position w
i
w
j
, w
i
w
j
w
v
;
position w
i
w
j
w
v
; conjunction of token posi-
tion, boundary words? POS tags, and predicate
word, also including position w
i
w
j
, w
i
w
j
w
v
;
position w
i
w
j
w
v
; conjunction of predicate and
frame; conjunction of target verb class and frame;
conjunction of boundary words? POS tags, and
predicate word.
5 Automatic Chinese Verb Formation
Analyzing
5.1 Introduction to Chinese Word Formation
Chinese words consist of one or more charac-
ters, and each character, in most cases, is a mor-
pheme which is the smallest meaningful unit of
the language. According to the number of mor-
phemes, the words can be grouped into two sets,
simple words (consisting of one morpheme) and
compound words (consisting of two morphemes
or more). There are 9 kinds of word formation in
Chinese compound words, and table 1 shows the
detail with examples. Note that, attributive-head
and complementarity are not for Chinese verbs.
Types Examples
reduplication ??(look)??(think)
affixation ??(intensify)??(feel)
subject-verb ??(hear)??(dictate)
verb-object ??(quit smoking)
??(haircut)
verb-complement ??(inform)??(plant)
verb-result ??(exceed)??(boil)
adverbial-head ??(retreat)??(misuse)
coordinate ??(cherish)??(chase)
attributive-head* ??(rumor)??(hospital)
complementarity* ??(paper)??(horse)
Table 1: Example Words with Formation
The internal structure of a word constraints its
external grammatical behavior, and the formation
of a verb can provide very important information
for Chinese SRL. Take ???/exceed? as an ex-
ample, the two characters are both verbal mor-
phemes, and the character ??? means ?pass? and
the character ??? with the meaning of ?over?
shows the complement of the action of ???. In
this word, ??? is usually collocated with an ob-
ject, and hence a Patient role should comes af-
ter the verb ????. Note that, the verb ???,
however, is unlikely to have an object. Take ??
?/haircut? as another example, the first charac-
ter ??? is a verbal morpheme with the meaning
of ?cut? and the second character ??? is a nomi-
nal morpheme with the meaning of ?hair?. In this
word, ??? acts as the object of ???, and the word
???? is unlikely to have an Patient any more in
the sentential context.
5.2 Verb Formation Analyzing Method
To automatically analyze verb formation, we in-
troduce a rule-based algorithm. Pseudo code in
Algorithm 1 illustrates our algorithm. This algo-
rithm takes three string (one or more Chinese char-
acters) sets as lexicon knowledge:
? adverbial suffix set A: strings in A are usu-
ally realized as the modifier in a adverbial-
head type word, e.g. ?/not, ?/not,
?/always,?/both,?/all.
? object head setO: strings inO are usually re-
alized as the head in a verb-object type word,
e.g. ?/change,?/get,?/talk,?/send.
1479
Algorithm 1: Verb Formation Analyzing.
Data: adverbial suffix set A, object head set
O, complement suffix set C
input : word W = c
1
...c
n
and its POS P
output: head character h, adverbial character
a, complement character c, object
character o
begin
h = c = a = o = null;
if n = 4 and c
1
= c
3
and c
2
= c
4
then
return Verb formation of W
?
= c
1
c
3
;
else if n = 3 and c
2
= c
3
then
h = c
1
, c = c
2
;
else if n = 2 and c
1
= c
2
then
h = c
1
;
else if n = 1 then
h = c
1
;
else if c
n
? C and c
n?1
c
n
? C and
P=?VV? then
h = c
1
, c = c
n
/c
n?1
c
n
;
else if c
1
? A then
a = c
1
, h = c
2
...c
n
;
else if c
1
? O and P=?VV? then
h = c
1
, o = c
2
...c
n
;
end
? complement suffix set C: strings in C are
usually realized as complement in a verb-
complement type word: e.g. ?/out, ?/in,
?/finish,?/come,??/not.
Note that, to date there is no word formation
annotation corpus, so direct evaluation of our rule-
based algorithm is impossible. This paper makes
task-oriented evaluation which measures improve-
ments in SRL.
5.3 Using Word Formation Information to
improve Shallow Parsing
The majority of Chinese nouns are of type
attributive-head. This means that for most nouns
the last character provides very important infor-
mation indicating the head of the noun. For ex-
ample, the word formations of ???/peach?, ??
?/willow? and ????/boxtree? (three different
kinds of trees), are attributive-head and they have
the same head word ??/tree?. While for verbs, the
majority are of three types: verb-object, coordi-
nate and adverbial-head. For example, words ??
?/enlarge?, ???/make more drastic? and ??
?/accelerate? have the same head ??/add?. The
head morpheme is very useful in alleviating the
data sparseness in word level. However, for any
given word, it is very hard to accurately find the
head. In the shallow paring experiments, we use
a very simple rule to get a pseudo head character:
1) extracting the last word for a noun, and 2) ex-
tracting the first word for a verb. The new features
include:
Pattern 1: conjunction of pseudo head of w
i?1
and POS tags of w
i?1
and w
i
.
Pattern 2: conjunction of pseudo head of w
i
and
POS tags of w
i?1
and w
i
.
Pattern 3: conjunction of length/POS tags of
w
i?1
, w
i
, w
i+1
.
5.4 Using Verb Formation Information to
improve SRL
We use some new verb formation features to im-
prove our SRL system. The new features are listed
as follows. The first four are used in semantic
chunking task, and all are used in SRC task.
First/last characters.
Word length.
Conjunction of word length and first/last char-
acter.
Conjunction of token position and first/last
character.
The head string of a verb (e.g. ??? in ????).
The adverbial string of a verb (e.g. ??? in ??
??).
The complement string of a verb (e.g. ??? in
????).
The object string of a verb (e.g. ??? in ??
??).
6 Results and Discussion
6.1 Experimental Setting
6.1.1 Data
Experiments in previous work are mainly based on
CPB and CTB, but the experimental data prepar-
ing procedure does not seem consistent. For ex-
ample, the sum of each semantic role reported in
(Ding and Chang, 2008) is extremely smaller than
the corresponding occurrence statistics in origi-
nal data files in CPB. In this paper, we mod-
ify CoNLL-2005 shared task software
6
to pro-
cess CPB and CTB. In our experiments, we use
the CPB 1.0 and CTB 5.0. The data is divided
into three parts: files from chtb 081 to chtb 899
are used as training set; files from chtb 041 to
6
http://www.lsi.upc.edu/?srlconll/soft.html
1480
chtb 080 as development set; files from chtb 001
to chtb 040, and chtb 900 to chtb 931 as test set.
The data setting is the same as (Xue, 2008). The
results were evaluated for precision, recall and F-
measure numbers using the srl-eval.pl script pro-
vided by CoNLL-2005 shared task.
6.1.2 Classifier
For both syntactic and semantic chunking, we
used TinySVM along with YamCha
7
(Kudo and
Matsumoto, 2000; Kudo and Matsumoto, 2001).
In the chunking experiments, all SVM classifiers
were realized with a polynomial kernel of de-
gree 2. Pair-wise strategy is used to solve multi-
class classification problem. For the SRC ex-
periments, we use a linear SVM classifier, along
with One-Vs-All approach for multi-class classifi-
cation. SVM
lin
8
, a fast linear SVM solvers, is used
for supervised learning. l
2
-SVM-MFN (modified
finite newton) method is used to solve the opti-
mization problem (Keerthi and DeCoste, 2005).
6.2 Shallow Parsing Performance
P(%) R(%) F
?=1
Baseline 93.54 93.00 93.27
Ours 93.83 93.39 93.61
Table 2: Shallow parsing performance
Table 2 summarizes the overall shallow pars-
ing performance on test set. The first line shows
the performance of baseline. Comparing the best
system performance 94.13 F-measure of CoNLL
2000 shared task (Syntactic Chunking on English),
we can see Chinese shallow parsing has reached
a comparable result, tough the comparison of nu-
meric performance is not very fair, because of dif-
ferent languages, different chunk definition, dif-
ferent training data sizes, etc.. The second line
Ours shows the performance when new features
are added, from which we can see the word for-
mation based features can help shallow parsing.
Table 3 shows the detailed performance of noun
phrase (NP) and verb phrase (VP), which make up
most of phrase chunks in Chinese. Our new fea-
tures help NP more, whereas the effect of new fea-
tures for VP is not significant. That is in part be-
cause most VP chunk recognition error is caused
by long dependency, where word formation fea-
7
http://chasen.org/?taku/index.html.en
8
http://people.cs.uchicago.edu/?vikass/svmlin.html
P(%) R(%) F
?=1
NP(Baseline) 90.84 90.05 90.44
NP(Ours) 91.42 90.78 91.10
VP(Baseline) 94.44 94.55 94.50
VP(Ours) 94.65 94.74 94.69
Table 3: Performance of NP-chunk and VP-chunk
tures do not work. Take the sentences below for
example:
1. [
V P
??????]? (Therefore (we)
achieve victory.)
2. [
ADV P
??] [
V P
????] ?????
????? (Therefore the major changes
have not been met before.)
The contexts of the word ???/therefore? in the
two sentences are similar, where ???? is fol-
lowed by verbal components. In the second sen-
tence, the word ???/therefore? will be correctly
recognized as an adverbial phrase unless classifier
knows the following component is a clause. Un-
fortunately, word formation features cannot sup-
ply this kind of information.
6.3 SRL Performance
P(%) R(%) A(%) F
?=1
(Xue, 2008) 79.5 65.6 ? 71.9
M1? 79.02 69.12 ? 73.74
M1+ 79.25 69.61 ? 74.12
M2?/AI 80.34 75.11 ? 77.63
M2+/AI 80.01 75.15 ? 77.51
M2?/SRC ? ? 92.57 ?
M2+wf/SRC ? ? 93.25 ?
M2+/SRC ? ? 93.42 ?
M2?AI+SRC 76.48 71.50 ? 73.90
Table 4: Overall SRL performance of different
methods
Table 4 lists the overall SRL performance num-
bers on test set using different methods mentioned
earlier; these results are based on features com-
puted from gold standard segmentation and POS
tagging, but automatic recognized chunks, which
is parsed by our improved shallow parsing sys-
tem. For the AI and the whole SRL tasks, we
report the precision (P), recall (R) and the F
?=1
-
measure scores, and for the SRC task we report
the classification accuracy (A). The first line (Xue,
1481
2008) shows the SRL performance reported in
(Xue, 2008). To the authors? knowledge, this re-
sult is best SRL performance in the literature. Line
2 and 3 shows the performance of the one-stage
systems: 1) Line M1? is the performance without
word formation features; 2) Line M1+ is the per-
formance when verb formation features are added.
Line 4 to 8 shows the performance of the two-stage
systems: 1) Line M2?/AI and M2+/AI shows the
performance of AI phase without and within word
formation features respectively; 2) Line M2?/SRC
shows the SRC performance with trivial word-
based features (i.e. frame features and verb forma-
tion features are not used); 3) Line M2+wf/SRC is
the improved SRC performance when coarse verb
formation features are added; 4) Line M2+/SRC
is the SRC performance with all features; 5) Line
M2?AI+SRC shows the performance of SRL sys-
tem, which uses baseline features to identify argu-
ments, and use all features to classify arguments.
6.4 Discussion
The results summarized in Table 4 indicate that
according to the-state-of-the-art in Chinese pars-
ing, SRL systems based on shallow parsing out-
performs the ones based on full parsing. Com-
parison between one-stage strategy and two-stage
strategy indicates 1) that there is no significant dif-
ference in the F-measure; and 2) that two-stage
strategy method can achieve higher recall while
one-stage strategy method can achieve higher pre-
cision. Both the one-stage strategy and two-stage
strategy methods yield significant improvements
over the best reported SRL performance in the lit-
erature, especially in terms of recall performance.
Comparison SRL performance with full parses
and partial parses indicates that both models have
strong and weak points. The full parse based
method can implement high precision SRL sys-
tems, while the partial parse based methods can
implement high recall SRL systems. This is fur-
ther justification for combination strategies that
combine these independent SRL models.
Generally, Table 4 shows that verb formation
features can enhance Chinese SRL, especially for
fine-grained role classification. The effect of word
formation in formation in both shallow parsing
and SRL suggests that automatic word formation
analyzing is very important for Chinese language
processing. The rule-based algorithm is just a pre-
liminary study on this new topic, which requires
Num of words P (%) R (%) F
?=1
Length = 1 84.69% 75.48% 79.82
Length = 2 82.14% 74.21% 77.97
Length = 3 75.43% 63.98% 69.24
Length = 4 75.71% 65.63% 70.32
Length = 5 72.46% 64.38% 68.18
Length = 6 72.97% 66.21% 69.43
Length = 7 77.03% 67.65% 72.04
Length = 8 74.39% 57.28% 64.72
Length = 9 66.67% 51.16% 57.89
Length = 10 68.08% 58.28% 62.80
Length = 11+ 67.40% 57.71% 62.18
Table 5: SRL performance with arguments of dif-
ferent length
more research effort.
Though our SRC module does not use any pars-
ing information, our system can achieve 93.42%
accuracy, comparing the best gold parse based re-
sult 94.68% in the literature. This result suggests
that Chinese SRC system, even without parsing,
can reach a considerable good performance. The
main reason is that in Chinese, arguments with dif-
ferent semantic types have discriminative bound-
ary words, which can be extracted without pars-
ing. It is very clear that the main bottleneck for
Chinese SRL is to accurately identify arguments
rather than to disambiguate their detailed seman-
tic types.
Table 5 summarizes the labeling performance
for argument of different length. It is not surpris-
ing that arguments are more and more difficult to
rightly recognize as the increase of their length.
But the performance decline slows up when the
length of arguments is larger than 10. In other
words, some of the arguments that are composed
of many words can still be rightly identified. The
main reason for this point is that these arguments
usually have clear collocation words locating at ar-
gument boundaries. Take the sentences below for
example,
3. ??[A1 . . . . . .?] (including ... etc.)
the object of the verb ???/include? has a defi-
nite collocation word ??/etc.?, and therefore this
object is easy to be recognized as a A1.
7 Conclusion
In this paper, we discuss Chinese SRL on the ba-
sis of partial syntactic structure. Our systems ad-
vance the state-of-the-art in Chinese SRL. We first
1482
extend the study on Chinese shallow parsing and
implement a good shallow parser. On the ba-
sis of partial parses, SRL are formulated as a se-
quence labeling problem, performing IOB2 deci-
sions on the syntactic chunks of the sentence. We
exploit a wide variety of features based on words,
POS tags, and partial syntax. Additionally, we
discuss a language special problem, i.e. Chinese
word formation. Experimental results show that
coarse word formation information can help shal-
low parsing, especially for NP-chunk recognition.
A rule-based algorithm is put forward to automat-
ically acquire Chinese verb formation, which is
empirically shown to enhance SRL.
Acknowledgments
This work is supported by NSFC Project
60873156, 863 High Technology Project of
China 2006AA01Z144 and the Project of Toshiba
(China) R&D Center.
We would like to thank Weiwei Ding for his
good advice on this research.
We would also like to thank the anonymous re-
viewers for their helpful comments.
References
Xavier Carreras and Llu??s M`arquez. 2004. Introduc-
tion to the conll-2004 shared task: Semantic role
labeling. In Hwee Tou Ng and Ellen Riloff, edi-
tors, HLT-NAACL 2004 Workshop: Eighth Confer-
ence on Computational Natural Language Learn-
ing (CoNLL-2004), pages 89?97, Boston, Mas-
sachusetts, USA, May 6 - May 7. Association for
Computational Linguistics.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 97?104, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Yaodong Chen, Ting Wang, Huowang Chen, and Xis-
han Xu. 2008. Semantic role labeling of Chinese
using transductive svm and semantic heuristics. In
Proceedings of the Third International Joint Confer-
ence on Natural Language Processing: Volume-II.
Weiwei Ding and Baobao Chang. 2008. Improv-
ing Chinese semantic role classification with hier-
archical feature selection strategy. In Proceedings
of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 324?333, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support
vector machines. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 25?27, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
S. Sathiya Keerthi and Dennis DeCoste. 2005. A mod-
ified finite newton method for fast solution of large
scale linear svms. J. Mach. Learn. Res., 6:341?361.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of the 2nd workshop on Learning language in
logic and the 4th conference on Computational natu-
ral language learning, pages 142?144, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL ?01: Sec-
ond meeting of the North American Chapter of the
Association for Computational Linguistics on Lan-
guage technologies 2001, pages 1?8, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic role chunking combining complementary
syntactic views. In Proceedings of the Ninth Confer-
ence on Computational Natural Language Learning
(CoNLL-2005), pages 217?220, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Sameer S. Pradhan, Wayne Ward, and James H. Mar-
tin. 2008. Towards robust semantic role labeling.
Comput. Linguist., 34(2):289?310.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In Pro-
ceedings of the 3rd ACL/SIGDAT Workshop on Very
Large Corpora, Cambridge, Massachusetts, USA,
pages 82?94.
Honglin Sun and Daniel Jurafsky. 2004. Shallow se-
mantc parsing of Chinese. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings.
Mihai Surdeanu, Llu??s M`arquez, Xavier Carreras, and
Pere Comas. 2007. Combination strategies for se-
mantic role labeling. J. Artif. Intell. Res. (JAIR),
29:105?151.
Nianwen Xue and Martha Palmer. 2005. Automatic
semantic role labeling for Chinese verbs. In in Pro-
ceedings of the 19th International Joint Conference
on Artificial Intelligence, page 2005.
Nianwen Xue. 2008. Labeling chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
1483
Domain Knowledge Engineering  
Based on Encyclopedias and the Web Text*
SUI Zhifang 
Institute of 
Computational 
Linguistics,
Peking University 
szf@pku.edu.cn
CUI Gaoying
Institute of 
Computational 
Linguistics,
Peking University 
cuigy@pku.edu.cn
DING Wansong 
Institute of 
Computational 
Linguistics,
Peking University 
dws@pku.edu.cn
ZHANG Qinlong
Institute of 
Computational 
Linguistics,
Peking University 
zql@pku.edu.cn
                                                          
*  This  research was funded by 973 Natural Basic Research Program of China 2004CB318102 and Natural Sciences 
Foundation of Beijing 4052019 
Abstract
Domain knowledge is the fundamental 
resources required by all intelligent 
information processing systems. With 
the upsurge of new technology and new 
products in various domains, the 
manual construction and updating of 
domain knowledge base can hardly 
meet the real needs of application 
systems, in terms of coverage or 
effectiveness. 
Based on natural language text analysis, 
this paper intends to draw a basic 
framework for the construction of 
domain knowledge base. Using 
encyclopedia resources and text 
information resources on the Web, we 
focus on the method of constructing 
domain knowledge base through 
technologies in natural language text 
analysis and machine learning. 
Moreover, an open network platform 
will be developed, through which 
common users can work with domain 
experts to contribute domain 
knowledge.
The technology can be applied to the 
construction and updating of domain 
knowledge base for intelligent 
information processing, and it can also 
provide help for the knowledge 
updating of encyclopedias. 
Keywords: Domain knowledge base, 
Natural language text analysis, machine 
learning, encyclopedia, open platform 
1 Introduction 
Domain knowledge is the indispensable resource 
for an intelligent information processing system. 
With the XSsurge of technology, more and more 
new technology, new product and new 
techniques come into being. The manual 
construction and updating domain knowledge 
base can hardly meet the real needs of 
application systems, in terms of coverage or 
effectiveness. In order to improve the robust of 
an information system, we need to study a 
computer-aided method to solve the bottleneck 
of domain knowledge acquisition.  
The Institute of Computational Linguistics, 
Peking University, now cooperates with 
Encyclopedia of China Publishing Hall on the 
project of human-machine interactive 
encyclopedia knowledge engineering. We want 
to study how to exploit, use and update 
encyclopedia resource properly. We will use the 
technique of natural language processing, 
machine learning and text mining to acquire 
domain knowledge semi-automatically from 
1
both encyclopedia and the Web text. 
Furthermore, we set up an open platform for 
domain knowledge acquisition, so that common 
network users and domain experts can work 
together to contribute new domain knowledge. 
Based on this technology, we can build domain 
knowledge base on each domain. This 
technology can be used as an important method 
of constructing and updating the domain 
knowledge base for the intelligent information 
retrieval and extraction systems. In the same 
time, it can also provide help for the knowledge 
updating of encyclopedias. 
2 Related works 
The researches on knowledge acquisition can be 
divided into three parts: artificial construction, 
semi-automatic construction and automatic 
construction. 
Artificial methods are usually used in 
constructing the common sense knowledge base, 
such as CYC[1], WordNet[2], EuroWordNet[3], 
HowNet[4], and CCD[5] etc. That?s because 
common sense is steady comparatively and it 
can not be affected by the task, also it can be 
reused by various kinds of system when 
constructed. For instance, since the WordNet 
was established in 1985, it had been widely used 
in IR, Text categorization, QA system etc. 
Similarly, the HowNet is being used in many 
Chinese information procession systems. It?s 
worthy of large-scale devotion for long-time 
using.
On the contrary, domain knowledge is tied 
with some concrete domain. Once the 
application domain changed, we need to re-
construct the domain knowledge base. 
Furthermore, domain knowledge updates 
continually, so that the domain knowledge base 
should be updates frequently. So it?s unrealistic 
to construct the domain knowledge base 
manually.  
In the way of constructing domain knowledge 
base, the semi-automatic method is mainly used. 
[6][7][8][9][10] established the platform of 
human-computer interactively working for the 
construction of domain knowledge base. They 
use various kinds of text processing and 
language analysis tools, which have the 
functions of morphological analysis, partial 
syntactic analysis, partial semantic analysis, 
with the mode of online cooperation, helping 
knowledge engineers or domain experts to find 
the domain concepts and the relations among 
them. The acquired knowledge can be added 
into the domain knowledge base. All these 
methods try to use pattern matching or various 
layers of NLP technology to acquire domain 
knowledge from large-scale free text. Free text 
is easy to get, however, it comes from different 
kinds of domain, including complicated 
language phenomenon hence is hard to 
understand. It?s difficult to extract knowledge 
reliably using current technology of NLP and 
machine learning from such free text. If there 
not exists a pre-defined basic domain knowledge 
architecture, it is difficult to acquire the 
concepts and the relations relative to the domain. 
Also, among the above-mentioned methods, the 
construction of domain knowledge base depends 
on the expert?s point of view and opinions. 
However, it?s very difficult to let experts to 
construct the ?real-time? domain architecture 
objectively and roundly and hence express it 
clearly in the given time. 
3 Domain Knowledge Engineering 
Based on Encyclopedias and the Web 
text
In this paper, we propose a technology of 
domain knowledge engineering based on 
encyclopedias and the web text. (ncyclopedia is 
the embodiment of the systematization and 
centralization of existed domain knowledge. The 
knowledge has been compiled and modified by 
many experts. Compared with free text, there are 
more canonical and NLP technologies can be 
used comparatively easily to extract knowledge 
from it. Since the knowledge in encyclopedia is 
more systematic, we can easily construct the 
basic frame of domain knowledge. So we will 
use NLP technology and machine learning 
method to construct the kernel of domain 
knowledge based on the analysis of the 
encyclopedia. Then based on the kernel of 
domain knowledge base, we can extract domain 
knowledge from other text resources. 
There exist some researches on extracting 
knowledge from the encyclopedia [11] [12] [13] 
[14]. These researches use the encyclopedias as 
the only source to acquire knowledge. However, 
with the high-speed improvement in each 
2
domain, there is severe knowledge lag in 
encyclopedias. So it is inadequate to use 
encyclopedias as the only source for knowledge 
acquisition. We need to learn more domain 
knowledge from other text resource besides 
encyclopedias. 
With the surge of Internet, information in it is 
increasing exponentially. Abundant knowledge 
lies in this huge Web resource. If we can extract 
knowledge from the Web, we could update and 
expand domain knowledge base most efficiently. 
Standing in the computational linguistics? point, 
we focus on retrieving information from the 
content rather than from the structure of the 
Web.
This paper studies the technology of domain 
knowledge engineering. Using encyclopedia 
resource and text information resource on the 
Web, we focus on the method of constructing 
domain knowledge base through technologies in 
natural language text analysis and machine 
learning. Moreover, an open network platform 
will be developed, through which common users 
can work together with domain experts to 
contribute domain knowledge. 
4 Strategy and Research Plan 
4.1 Learning the style of knowledge-
dense text from encyclopedias 
The compilation of encyclopedias always 
follows some specified compilatory model. 
Encyclopedias have the relatively formal diction 
and different compilatory model for different 
kinds of entries. Because of that, the 
paraphrasable text in encyclopedias has clearer 
model to express the relation among concepts in 
most cases. For instance, ?X is a kind of ?Y?, 
?X is composed of A, B, C and D?, ?A, B and C 
make up D?, ?X can be divided into A, B and C?. 
Through recognizing the terms and partial 
parsing for the paraphrasable text in the 
encyclopedia, we could learn the patterns, which 
express the relations among concepts. 
Furthermore, we could learn the styles of 
knowledge dense text based on those patterns. 
Next step, we will follow the styles and combine 
the HTML target set to acquire more knowledge 
dense text fragments from the web. Based on 
such knowledge-dense text, some deeper natural 
language processing technologies could be used 
to extract domain knowledge reliably. 
4.2 Automatic extraction of terms 
Through analyzing the characters and expression 
forms of Chinese terms, we learn term 
knowledge from large-scale domain corpus and 
term bank. Using natural language processing 
method combing rule and statistics, we can 
automatically extract Chinese terms from corpus. 
A term is a kind of phrases, whose 
components are close related. Further more, it 
has strong domain feature. The close relation of 
the components in a term can be captured 
through calculating the static association rate 
between the words that compose a term 
candidate. The linguistic feature can be captured 
through analysis the grammatical structural 
information of the terms. While the domain 
feature of a term can be captured through the 
domain component that has the possibility of 
composing a term. For example, ?movable 
terminal? and ?social economy? are both 
composed by the close related components. 
While, the former is a term in the domain of 
information science and technology, and the 
latter is just a common phrase instead of a term. 
The reason lies in that the former has the domain 
feature comes from one of its components? 
terminal?, while the latter has not the domain 
feature.
We will use the above characteristic and 
representation forms of a term to perform 
automatic term extraction. The system of 
automatic term extraction includes two phases: 
learning stage and application stage. 
In the stage of learning, we use a series of 
machine learning methods to get various kinds 
of integrated knowledge for automatic term 
extraction from a large-scale corpus and a term 
bank. These knowledge includes the inner 
structural knowledge of terms, the statistical 
domain features of term component, the 
statistical mutual information between the 
components of terms, the outer environment 
features of terms and the distinct text-level 
features of term recognition etc... In the stage of 
application, through an efficient model, we use 
all these various types of knowledge into 
automatic term extraction. 
3
4.3 Design and implementation of 
partially analysis technology 
oriented for knowledge-dense text 
The knowledge-dense text fragments (including 
encyclopedia and the Web text segments whose 
style is similar to encyclopedias) is relatively 
simple. Therefore, it?s possible to implement 
deeper analysis on it using the natural language 
processing technology. 
We use comprehensive language knowledge 
and statistical technique together to design 
language analysis method oriented for 
knowledge-dense text. We will use the 
comprehensive language knowledge resource, 
such as The Grammatical Dictionary-base of 
Contemporary Chinese, Chinese Semantic 
Dictionary, Chinese Concept Dictionary (CCD), 
and Termbank of Information Technology, 
which was developed by Institute of 
Computational Linguistics (ICL) of Peking 
University. Moreover, we will design a natural 
language partial parsing and understanding 
technology combining statistic technology base 
on the 80,000,000 words IT corpus. Concretely, 
on the base of the developed software such as 
word segmentation, POS tagging, term 
extraction and identification, skeletal 
dependency analysis of sentence, we will 
combine semantic restrict information with 
syntax rules, so that during syntax analysis we 
can get the semantic restrict information 
between syntax components at the same time. 
We will label semantic roles for predicate-head 
and its central valency components using 
Chinese Semantic Dictionary. So we can get 
shallow case frames of sentences after natural 
language partial parsing and understanding for 
text sentences. And domain knowledge will be 
extracted in the later stage from this analysis 
result.
4.4 Establishing the basic knowledge 
description frame based on the 
encyclopedia
The knowledge of encyclopedia is relatively 
systematic, mature and intensive. On this 
foundation, it is easier to set up a basic domain 
knowledge base which includes the kernel of 
domain knowledge. In the encyclopedia, every 
subject is described by attributes, and different 
subjects are organized hierarchically. 
Figure1: An example of the fragments of 
domain knowledge framework 
For example as showed in Figure1, aiming at 
the subject ?Input equipment? in the domain of 
computer hardware, the encyclopedia describes 
the basic knowledge around the subject from 
many of point of views such as components, 
function, classification etc, which we call 
attributes here. On the other hand, ?Input 
equipment?, ?Output equipment?, ?Terminal 
unit? constitute the subject ?computer I/O 
equipment?; furthermore, ?Input equipment?, 
?Computer storage equipment?, ?Network 
equipment? are also components of the 
?Computer hardware?. This paper will make the 
?classification + Attribute? as the basic 
knowledge description method for constructing 
the basic domain knowledge base. When the 
basic knowledge description method is set up, 
we take every entry in the encyclopedia as a 
subject, through analyzing the correlative 
sentence and recognizing the key terms in the 
paraphrase text and the relations among the 
terms, we can describe the basic knowledge on 
this subject. In the next step, we may couple 
several subjects in the same domain gradually in 
order to construct the basic domain knowledge 
base in this domain. 
4.5 Using bootstrapping method to 
expand domain knowledge base 
The structure of the web text is incompact, and 
the diction is not canonical enough. However, 
the web text is easy to get and contains a great 
amount of new knowledge. So based on the 
4
basic domain knowledge base, we can select the 
knowledge dense text fragments from the web 
resource as the source to acquire more new 
knowledge.
We collect language patterns, which are 
known showing some kind of domain 
knowledge from encyclopedia. Using the 
language patterns as the seed set, we could learn 
more language patterns from the web text using 
boot strapping machine learning method.  Using 
the expanded seed set, we could learn more 
language patterns from the larger text. This 
technique can expand domain knowledge base 
iteratively. 
The system structure is as Figure2. 
Figure2: The system structure for domain 
knowledge acquisition. 
4.6 Developing open platform for 
domain knowledge collecting 
With the rapid development of Internet, people 
could communicate and collaborate without face 
to face. They can share work and collaborate 
through the web. So we constructed an open 
human-computer interactive platform to call on 
domain knowledge experts and spacious 
common network users to collaborate together 
and contribute new domain knowledge. This 
platform could also assist the experts of 
encyclopedia in editing and managing new 
domain knowledge. 
5 Current result 
5.1 Automatic extraction of terms 
We have exploited a term extraction system, 
including term extraction, human-computer 
interactive updating etc. The system is made up 
of basic source layer, learning layer, application 
layer and service layer. 
We select the texts from 16 representative 
Chinese journals in the field of science and 
technology to construct the testing set.  
The Principles for Testing 
First of all, we manually tagged the terms in the 
testing texts. The principles we used in term 
tagging is very strict, that is, we only tag the 
longest terms in the texts, while ignore any of 
the term fragments in a longest term. For 
example, for the word sequence ??? ?? ?
?  (Interface Technology Specification)?, we 
only select ??? ?? ??  (Interface 
Technology Specification)? as a term, while 
ignore ??? ??  (Interface Technology?, 
although it may be also a term in other context.  
Similarly, for word sequence ??? ?? ?
? (Digital Television Signal)?, we only select 
??????? (Digital Television Signal)? as 
a term, while ignore ??? ??  (Digital 
Television)?.
The above testing principles may result in a 
great decrease of the precision and recall of term 
recognition. However, through these principles, 
we can find more problems existed in the term 
recognition algorithm.   
The Testing Results 
Based on the above testing principles, we get the 
precision and recall of term recognition as Table 
1.
RECALL PRECISION THE JOURNALS 
OF THE 
TESTING
TEXTS
% %
Semi-Conductor 
Technology (1999-
01)
65.6 55.1
Telecom Science 
(1998-01) 
52.9 60.4
Computer and the 
Peripheral
Equipments (1999-
01)
52.9 71.2
5
The Research and 
Progress of Solid 
Electronics (2000-
01)
57.6 62.4
Compute-Aided 
Design and (1999-
01)
60.0 54.6
Computer 
Engineering (1999-
04)
65.1 73.4
Computer 
Application (1999-
01)
57.2 68.9
Automatic
Measure and 
Control (1998-02) 
51 59.5
Control Theory 
and Application 
(1999-01) 
49.4 64.1
Software (1998-01) 52.6 54.1
Micro-Electronics 
(1999-01) 
65.6 55.0
Wireless
Communication 
Technology (2000-
01)
57.7 69.6
Remote Sensing 
(1999-01) 
67.1 62.1
System Emulation 
(1999-01) 
64.9 75.4
Motional
Communication 
(1999-01) 
61 51.3
Chinese Cable 
Television (2000-
01)
60.0 57.0
AVERAGE 57.8 62.2
Table 1: Testing Result 
There is no unique standard for term?s 
determination. What is a term? What is a 
common word? What is a term fragment? It is 
difficult to give an objective and unique 
standard that is operable for computers. 
Therefore, what the automatic term recognition 
system find can only be taken as the term 
candidates attached with the confidences. We 
still need the human terminology experts to give 
a final confirmation of the terms.  
Our software includes human-computer 
interactive updating interface besides automatic 
term extraction. The interactive updating 
interface is as Figure3: 
Figure3: The interactive updating interface after 
term extraction 
5.2 Set up the basic database of 
encyclopedia
A lot of key concepts in the encyclopedia are 
well-marked with hyperlinks, titles, bookmarks 
and other Html tags according to different kinds 
of information respectively in the paraphrasable 
text. Using the information supplied by ?China 
encyclopedia? e-press, we put encyclopedia 
subject information, relationship between 
subjects and term hierarchy into database to 
form an encyclopedia database for a primary 
domain knowledge base. 
The core structure of encyclopedia database is 
presented as (main entry, relation term, 
relationship). Main entry is the entry that is 
listed in the encyclopedia. Relation terms are the 
hyperlink, bookmark, subtitle and so on. The 
relationship between main term and these 
elements now is null that need to be added with 
human assistance. 
For example, the paraphrasable text of term 
?frequency divider? is showed in the database as 
Table2.
Main entry Relation term Relationship
Frequency 
divider
Crystal 
oscillator
Unknown
Frequency 
divider
Impulse 
frequency 
divider
Unknown
Frequency 
divider
Trigger Unknown
Frequency Regenerate Unknown
6
divider frequency 
divider
Frequency 
divider
Trigger Unknown
Frequency 
divider
Regenerate
frequency 
divider
Unknown
Table2: the database fragment for the term 
?frequency divider? 
5.3 Attribute relation template 
extraction
attribute relation type template example 
definition xxx?/??/??/?
??/???/???/
??
substitutable name 
mark
xxx?/??/? xxx
country xxx????xxx?
nationality xxx?/??
native place or home 
place
???/?? xxx/xxx
?
experience ??? xxx?? xxx
??
literature ??/??/??/??
xxx/??? xxx/??
? xxx/???? xxx
working experience xxx?? xxx?xxx?
? xxx?? xxx??
? xxx??? xxx?
??? xxx???
xxx???? xxx
achievement and 
influence
??? xxx/? xxx?
?/xxx???
Table3: The examples of the attribute 
relation template of human entry 
We have semi-automatically extracted several 
attribute relation templates for human entries 
from encyclopedia text. The attribute relation 
template of human entry examples are as Table3. 
5.4 Open platform for domain 
knowledge collection
We design the open platform for domain 
knowledge collection using ASP.NET network 
programming technology. We establish 
interactive working relation among domain 
knowledge engineers, domain experts and 
common users through the platform. The 
functions including: 
z Domain knowledge requirement collection: 
on-line collection of new term entries of 
current domain, which are needed by the 
users.
z Domain knowledge supply collection: on-
line collection of more detailed attribute 
information of the new terms. 
z On-line management: system 
administrators manage new term 
information, which were submitted on line 
by the users. 
The interface of the platform is as Figure4. 

Figure4: The interface of the open platform for 
domain knowledge collection 
6 Conclusion
The construction of domain knowledge base is a 
kind of high intelligent knowledge engineering. 
Since there is still have big gap between current 
level of technological development and real 
need, it is unrealistic to build domain knowledge 
base using automatic method or manual method 
only. However, in the human-computer 
interaction process, how to sufficiently absorb 
the knowledge resource which human being has 
already mastered and use it to supervise 
7
automatic acquisition of new knowledge? How 
to call together knowledge engineers, domain 
experts and common network users and realize 
multi-member collaboration during the updating 
and extending process of domain knowledge 
base? These are the key problems to be settled in 
the knowledge engineering domain. This paper 
tries to do some exploration on these aspects. 
References 
[1] http://www.opencyc.org/
[2] http://www.cogsci.princeton.edu/~wn
[3] http://www.illc.uva.nl/EuroWordNet/
[4] http://www.keenage.com
[5] Yu Jiangshen, Liu Yang, Yu Shiwen, The 
specification of the Chinese Concept Dictionary, 
Journal of Chinese Language and Computing, 
Vol.13, 2003.  
[6]A.Maedche, S.Staab, Semi-Automatic 
Engineering of Ontologies from text, Proceedings 
of International Conference on Software 
Engineering and Knowledge Engineering (SEKE' 
2000), Chicago, IL, USA, 2000 
[7] Szpakowicz,S., Semi-automatic acquisition of 
conceptual structure from technical texts, 
International journal of Man-machine Studies, 
33(4),385-397,1990 
[8] Biebow, B., Szulman, S., TERMINAE: a 
linguistic-based tool for the building of domain 
ontology. In Dieter fensel, Rudi Studer (eds.), 
Knowledge Acquisition, Modeling and 
Management, pp.49-66, 1999 
[9] Lapalut, S., How to handle multiple expertise 
from several experts: a general text clustering 
approach. In F. Maurer (Ed.), Proc. 2nd Knowledge 
Engineering Forum (KEF?96), Karlsruhe, Jan., 
1996. 
[10] Mikheev, A., Finch, S., A workbench for 
acquisition of ontological knowledge from natural 
text. In proc. Of the 7th conference of the 
European Chapter for Computational Linguistics 
(EACL?95), Dublin, Ireland, pp. 194-201, 1995 
[11] Richard Hull, Fernando Gomez, Automatic 
acquisition of biographic knowledge from 
encyclopedic texts, ExpertSystems with 
Applications, 16(1999), pp.261-270, 1999 
[12] Fernando Gomez, Richard Hull, Carlos Segami, 
1994, Acquiring Knowledge from Encyclopedic 
Texts, Proceedings of the 4th ACL Conference on 
Applied Natural Language Processing, Stuttgart, 
Germany, 1994 
[13] Song Rou, Xu Yong, An Experiment on 
Knowledge Extraction from an Encyclopedia 
Based on Lexicon Semantics, pp.101-112, 2002 
[14] Gu Fang, Cao Cungen, Biological Knowledge 
Acquisition from the Electronic Encyclopedia of 
China, Proceedings of ICYCS?2001, pp.1199-
1203, 2001 
8
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 253?256,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Prediction of Thematic Rank for Structured Semantic Role Labeling
Weiwei Sun and Zhifang Sui and Meng Wang
Institute of Computational Linguistics
Peking University
Key Laboratory of Computational Linguistics
Ministry of Education, China
weiwsun@gmail.com;{wm,szf}@pku.edu.cn
Abstract
In Semantic Role Labeling (SRL), it is rea-
sonable to globally assign semantic roles
due to strong dependencies among argu-
ments. Some relations between arguments
significantly characterize the structural in-
formation of argument structure. In this
paper, we concentrate on thematic hierar-
chy that is a rank relation restricting syn-
tactic realization of arguments. A log-
linear model is proposed to accurately
identify thematic rank between two argu-
ments. To import structural information,
we employ re-ranking technique to incor-
porate thematic rank relations into local
semantic role classification results. Exper-
imental results show that automatic pre-
diction of thematic hierarchy can help se-
mantic role classification.
1 Introduction
In Semantic Role Labeling (SRL), it is evident that
the arguments in one sentence are highly corre-
lated. For example, a predicate will have no more
than one Agent in most cases. It is reasonable to
label one argument while taking into account other
arguments. More structural information of all ar-
guments should be encoded in SRL approaches.
This paper explores structural information of
predicate-argument structure from the perspec-
tive of rank relations between arguments. The-
matic hierarchy theory argues that there exists a
language independent rank of possible semantic
roles, which establishes priority among arguments
with respect to their syntactic realization (Levin
and Hovav, 2005). This construct has been widely
implicated in linguistic phenomena, such as in the
subject selection rule of Fillmore?s Case Grammar
(1968): ?If there is an A [=Agent], it becomes the
subject; otherwise, if there is an I [=Instrument],
it becomes the subject; otherwise, the subject is
the O [=Object, i.e., Patient/Theme]?. This rule
implicitly establishes precedence relations among
semantic roles mentioned and can be simplified to:
Agent  Instrument  Patient/Theme
Emerging from a range of more basic semantic
properties of the ranked semantic roles, thematic
hierarchies can help to construct mapping from se-
mantics to syntax. It is therefore an appealing op-
tion for argument structure analysis. For example,
if the the rank of argument a
i
is shown higher than
a
j
, then the assignment [a
i
=Patient, a
j
=Agent] is
illegal, since the role Agent is the highest role.
We test the hypothesis that thematic rank be-
tween arguments can be accurately detected by
using syntax clues. In this paper, the concept
?thematic rank? between two arguments a
i
and a
j
means the relationship that a
i
is prior to a
j
or a
j
is
prior to a
i
. Assigning different labels to different
relations between a
i
and a
j
, we formulate predic-
tion of thematic rank between two arguments as a
multi-class classification task. A log-linear model
is put forward for classification. Experiments on
CoNLL-2005 data show that this approach can
get an good performance, achieving 96.42% ac-
curacy on gold parsing data and 95.14% accuracy
on Charniak automatic parsing data.
Most existing SRL systems divide this task into
two subtasks: Argument Identification (AI) and
Semantic Role Classification (SRC). To add struc-
tural information to a local SRL approach, we in-
corporate thematic hierarchy relations into local
classification results using re-ranking technique
in the SRC stage. Two re-ranking approaches,
1) hard constraint re-ranking and 2) soft con-
straint re-ranking, are proposed to filter out un-
like global semantic role assignment. Experiments
on CoNLL-2005 data indicate that our method
can yield significant improvement over a state-of-
the-art SRC baseline, achieving 0.93% and 1.32%
253
absolute accuracy improvements on hand-crafted
and automatic parsing data.
2 Prediction of Thematic Rank
2.1 Ranking Arguments in PropBank
There are two main problems in modeling the-
matic hierarchy for SRL on PropBank. On the one
hand, there is no consistent meaning of the core
roles (i.e. Arg0-5/ArgA). On the other hand, there
is no consensus over hierarchies of the roles in the
thematic hierarchy. For example, the Patient occu-
pies the second highest hierarchy in some linguis-
tic theories but the lowest in some other theories
(Levin and Hovav, 2005).
In this paper, the proto-role theory (Dowty,
1991) is taken into account to rank PropBank argu-
ments, partially resolving the two problems above.
There are three key points in our solution. First,
the rank of Arg0 is the highest. The Agent is al-
most without exception the highest role in pro-
posed hierarchies. Though PropBank defines se-
mantic roles on a verb by verb basis, for a particu-
lar verb, Arg0 is generally the argument exhibit-
ing features of a prototypical Agent while Arg1
is a prototypical Patient or Theme (Palmer et al,
2005). As being the proto-Agent, the rank of Arg0
is higher than other numbered arguments. Second,
the rank of the Arg1 is second highest or lowest.
Both hierarchy of Arg1 are tested and discussed in
section 4. Third, we do not rank other arguments.
Two sets of roles closely correspond to num-
bered arguments: 1) referenced arguments and 2)
continuation arguments. To adapt the relation to
help these two kinds of arguments, the equivalence
relation is divided into several sub-categories. In
summary, relations of two arguments a
i
and a
j
in
this paper include: 1) a
i
 a
j
: a
i
is higher than
a
j
, 2) a
i
? a
j
: a
i
is lower than a
j
, 3) a
i
ARa
j
: a
j
is the referenced argument of a
i
, 4) a
i
RAa
j
: a
i
is
the referenced argument of a
j
, 5) a
i
ACa
j
: a
j
is
the continuation argument of a
i
, 6) a
i
CAa
j
: a
i
is
the continuation argument of a
j
, 7) a
i
= a
j
: a
i
and a
j
are labeled as the same role label, and 8)
a
i
? a
j
: a
i
and a
j
are labeled as the Arg2-5, but
not in the same type.
2.2 Prediction Method
Assigning different labels to possible rank be-
tween two arguments a
i
and a
j
, such as labeling
a
i
 a
j
as ??, identification of thematic rank
can be formulated as a classification problem. De-
lemma, POS Tag, voice, and SCF of predicate
categories, position of two arguments; rewrite
rules expanding subroots of two arguments
content and POS tags of the boundary words
and head words
category path from the predicate to candidate
arguments
single character category path from the
predicate to candidate arguments
conjunction of categories, position, head
words, POS of head words
category and single character category path
from the first argument to the second argument
Table 1: Features for thematic rank identification.
note the set of relationsR. Formally, given a score
function S
TH
: A?A?R 7? R, the relation r is
recognized in argmax flavor:
r? = r
?
(a
i
, a
j
) = argmax
r?R
S
TH
(a
i
, a
j
, r)
A probability function is chosen as the score func-
tion and the log-linear model is used to estimate
the probability:
S
TH
(a
i
, a
j
, r) =
exp{?(a
i
, a
j
, r) ?w}
?
r?R
exp{?(a
i
, a
j
, r) ?w}
where ? is the feature map and w is the param-
eter vector to learn. Note that the model pre-
dicts the rank of a
i
and a
j
through calculating
S
TH
(a
i
, a
j
, r) rather than S
TH
(a
j
, a
i
, r), where
a
i
precedes a
j
. In other words, the position infor-
mation is implicitly encoded in the model rather
than explicitly as a feature.
The system extracts a number of features to rep-
resent various aspects of the syntactic structure of
a pair of arguments. All features are listed in Table
1. The Path features are designed as a sequential
collection of phrase tags by (Gildea and Jurafsky,
2002). We also use Single Character Category
Path, in which each phrase tag is clustered to a cat-
egory defined by its first character (Pradhan et al,
2005). To characterize the relation between two
constituents, we combine features of the two indi-
vidual arguments as new features (i.e. conjunction
features). For example, if the category of the first
argument is NP and the category of the second is S,
then the conjunction of category feature is NP-S.
3 Re-ranking Models for SRC
Toutanova et al (2008) empirically showed that
global information is important for SRL and that
254
structured solutions outperform local semantic
role classifiers. Punyakanok et al (2008) raised an
inference procedure with integer linear program-
ming model, which also showed promising results.
Identifying relations among arguments can pro-
vide structural information for SRL. Take the sen-
tence ?[
Arg0
She] [
V
addressed] [
Arg1
her hus-
band] [
ArgM?MNR
with her favorite nickname].?
for example, if the thematic rank of she and her
husband is predicted as that she is higher than her
husband, then her husband should not be assigned
the highest role.
To incorporate the relation information to lo-
cal classification results, we employ re-ranking ap-
proach. Assuming that the local semantic classi-
fier can produce a list of labeling results, our sys-
tem then attempts to pick one from this list accord-
ing to the predicted ranks. Two different polices
are implemented: 1) hard constraint re-ranking,
and 2) soft constraint re-ranking.
Hard Constraint Re-ranking The one picked
up must be strictly in accordance with the ranks.
If the rank prediction result shows the rank of ar-
gument a
i
is higher than a
j
, then role assignments
such as [a
i
=Patient and a
j
=Agent] will be elim-
inated. Formally, the score function of a global
semantic role assignment is:
S(a, s) =
?
i
S
l
(a
i
, s
i
)
?
i,j,i<j
I(r
?
(a
i
, a
j
), r(s
i
, s
j
))
where the function S
l
locally scores an argument;
r
?
: A ? A 7? R is to predict hierarchy of two
arguments; r : S ? S 7? R is to point out the the-
matic hierarchy of two semantic roles. For exam-
ple, r(Agent, Patient) = ?  ?. I : R ?R 7?
{0, 1} is identity function.
In some cases, there is no role assignment sat-
isfies all predicted relations because of prediction
mistakes. For example, if the hierarchy detec-
tion result of a = (a
1
, a
2
, a
3
) is (r
?
(a
1
, a
2
) =
, r
?
(a
2
, a
3
) =, r
?
(a
1
, a
3
) =?), there will be no
legal role assignment. In these cases, our system
returns local SRL results.
Soft Constraint Re-ranking In this approach,
the predicted confidence score of relations is
added as factor items to the score function of the
semantic role assignment. Formally, the score
function in soft constraint re-ranking is:
S(a, s) =
?
i
S
l
(a
i
, s
i
)
?
i,j,i<j
S
TH
(a
i
, a
j
, r(s
i
, s
j
))
4 Experiments
4.1 Experimental Settings
We evaluated our system using the CoNLL-2005
shared task data. Hierarchy labels for experimen-
tal corpora are automatically set according to the
definition of relation labels described in section
2.1. Charniak parser (Charniak, 2000) is used for
POS tagging and full parsing. UIUC Semantic
Role Labeler
1
is a state-of-the-art SRL system. Its
argument classification module is used as a strong
local semantic role classifier. This module is re-
trained in our SRC experiments, using parameters
described in (Koomen et al, 2005). Experiments
of SRC in this paper are all based on good ar-
gument boundaries which can filter out the noise
raised by argument identification stage.
4.2 Which Hierarchy Is Better?
Detection SRL (S) SRL (G)
Baseline ? 94.77% ?
A 94.65% 95.44% 96.89%
A & P? 95.62% 95.07% 96.39%
A & P? 94.09% 95.13% 97.22%
Table 2: Accuracy on different hierarchies
Table 2 summarizes the performance of the-
matic rank prediction and SRC on different the-
matic hierarchies. All experiments are tested on
development corpus. The first row shows the per-
formance of the local sematic role classifier. The
second to the forth rows show the performance
based on three ranking approach. A means that
the rank of Agent is the highest; P? means that the
rank of Patient is the second highest; P? means
that the rank of the Patient is the lowest. Col-
umn SRL(S) shows SRC performance based on
soft constraint re-ranking approach, and column
SRL(G) shows SRC performance based on gold
hierarchies. The data shows that the third the-
matic hierarchy fits SRL best, but is harder to
learn. Compared with P?, P? is more suitable for
SRL. In the following SRC experiments, we use
the first hierarchy because it is most helpful when
predicted relations are used.
4.3 Results And Improvement Analysis
Table 3 summarizes the precision, recall, and F-
measure of this task. The second column is fre-
quency of relations in the test data, which can be
1
http://l2r.cs.uiuc.edu/?cogcomp/srl-demo.php
255
seen as a simple baseline. Moreover, another natu-
ral baseline system can predict hierarchies accord-
ing to the roles classified by local classifier. For
example, if the a
i
is labeled as Arg0 and a
j
is la-
beled as Arg2, then the relation is predicted as .
The third column BL shows the F-measure of this
baseline. It is clear that our approach significantly
outperforms the two baselines.
Rel Freq. BL P(%) R(%) F
 57.40 94.79 97.13 98.33 97.73
? 9.70 51.23 98.52 97.24 97.88
? 23.05 13.41 94.49 93.59 94.04
= 0.33 19.57 93.75 71.43 81.08
AR 5.55 95.43 99.15 99.72 99.44
AC 3.85 78.40 87.77 82.04 84.81
CA 0.16 30.77 83.33 50.00 62.50
All ? 75.75 96.42
Table 3: Thematic rank prediction performance
Table 4 summarizes overall accuracy of SRC.
Baseline performance is the overall accuracy of
the local classifier. We can see that our re-ranking
methods can yield significant improvemnts over
the baseline.
Gold Charniak
Baseline 95.14% 94.12%
Hard 95.71% 94.74%
Soft 96.07% 95.44%
Table 4: Overall SRC accuracy.
Hierarchy prediction and re-ranking can be
viewed as modification for local classification re-
sults with structural information. Take the sen-
tence ?[Some ?circuit breakers? installed after the
October 1987] crash failed [their first test].? for
example, where phrases ?Some ... 1987? and
?their ... test? are two arguments. The table be-
low shows the local classification result (column
Score(L)) and the rank prediction result (column
Score(H)). The baseline system falsely assigns
roles as Arg0+Arg1, the rank relation of which is
. Taking into account rank prediction result that
relation ? gets a extremely high probability, our
system returns Arg1+Arg2 as SRL result.
Assignment Score(L) Score(H)
Arg0+Arg1 78.97%? 82.30% :0.02%
Arg1+Arg2 14.25%? 11.93% ?:99.98%
5 Conclusion and Future Work
Inspired by thematic hierarchy theory, this paper
concentrates on thematic hierarchy relation which
characterize the structural information for SRL.
The prediction of thematic rank is formulated as
a classification problem and a log-linear model
is proposed to solve this problem. To improve
SRC, we employ re-ranking technique to incorpo-
rate thematic rank information into the local se-
mantic role classifier. Experimental results show
that our methods can construct high-performance
thematic rank detector and that identification of ar-
guments? relations can significantly improve SRC.
Acknowledgments
This work is supported by NSFC Project
60873156, 863 High Technology Project of
China 2006AA01Z144 and the project of Toshiba
(China) Co., Ltd. R&D Center.
References
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of NAACL-00.
David R. Dowty. 1991. Thematic proto-roles and ar-
gument selection. Language, 67:547?619.
Charles Fillmore. 1968. The case for case. In Em-
mon Bach and Richard Harms, editors, Universals
in Linguistic Theory, pages 1?90. Holt, Rinehart and
Winston, New York, New York.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28:245?288.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized inference with
multiple semantic role labeling systems. In Pro-
ceedings of the CoNLL-2005, pages 181?184, June.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Research Surveys in Linguistics.
Cambridge University Press, New York.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus
of semantic roles. Computational Linguistics, 31.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argu-
ment classification. In Machine Learning.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Comput. Linguist.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist.
256
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 833?840
Manchester, August 2008
Prediction of Maximal Projection for Semantic Role Labeling
Weiwei Sun
?
, Zhifang Sui
Institute of Computational Linguistics
Peking University
Beijing, 100871, China
{ws, szf}@pku.edu.cn
Haifeng Wang
Toshiba (China) R&D Center
501, Tower W2, Oriental Plaza
Beijing, 100738, China
wanghaifeng@rdc.toshiba.com.cn
Abstract
In Semantic Role Labeling (SRL), argu-
ments are usually limited in a syntax sub-
tree. It is reasonable to label arguments lo-
cally in such a sub-tree rather than a whole
tree. To identify active region of argu-
ments, this paper models Maximal Pro-
jection (MP), which is a concept in D-
structure from the projection principle of
the Principle and Parameters theory. This
paper makes a new definition of MP in S-
structure and proposes two methods to pre-
dict it: the anchor group approach and the
single anchor approach. The anchor group
approach achieves an accuracy of 87.75%
and the single anchor approach achieves
83.63%. Experimental results also indicate
that the prediction of MP improves seman-
tic role labeling.
1 Introduction
Semantic Role Labeling (SRL) has gained the in-
terest of many researchers in the last few years.
SRL consists of recognizing arguments involved
by predicates of a given sentence and labeling their
semantic types. As a well defined task of shallow
semantic parsing, SRL has a variety of applications
in many kinds of NLP tasks.
A variety of approaches has been proposed
for the different characteristics of SRL. More re-
cent approaches have involved calibrating features
(Gildea and Jurafsky, 2002; Xue and Palmer, 2004;
?
This work was partial completed while this author was at
Toshiba (China) R&D Center.
?
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Pradhan et al, 2005), analyzing the complex input
? syntax trees (Moschitti, 2004; Liu and Sarkar,
2007), exploiting the complicated output ? the
predicate-structure (Toutanova et al, 2005), as
well as capturing paradigmatic relations between
predicates (Gordon and Swanson, 2007).
In prior SRL methods, role candidates are ex-
tracted from a whole syntax tree. Though sev-
eral pruning algorithms have been raised (Xue and
Palmer, 2004), the policies are all in global style.
In this paper, a statistical analysis of Penn Prop-
Bank indicates that arguments are limited in a local
syntax sub-tree rather than a whole one. Prior SRL
methods do not take such locality into account and
seek roles in a wider area. The neglect of local-
ity of arguments may cause labeling errors such
as constituents outside active region of arguments
may be falsely recognized as roles.
This paper uses insights from generative lin-
guistics to guide the solution of locality of argu-
ments. In particular, Maximal Projection (MP)
which dominates
1
active region of arguments ac-
cording to the projection principle of principle and
parameters. Two methods, the anchor group ap-
proach and the single anchor approach, are pro-
posed to find the active sub-tree which is rooted by
MP and covers all roles. The solutions put forward
in this paper borrow ideas from NP-movement
principle in generative linguistics and are in statis-
tical flavor. The anchor group approach achieves
an accuracy of 87.75%, and the single anchor ap-
proach achieves 83.63%. Though the accuracy is
lower, the single anchor approach fits SRL better.
1
Dominate is an concept in X-bar theory are modeled. As-
suming ? and ? are two nodes in a syntax tree: ? dominates
? means ? is ancestor of ?.
833
Figure 1: A sentence from WSJ test corpus of CoNLL-2005 shared task
2 Maximal Projection and Its
Government of Arguments
2.1 Maximal Projection
Principle and parameters theory is a framework of
generative grammar. X-bar theory, as a module
of principle and parameters, restricts context-free
phrase structure rules as follows:
1. a phrase always contains a head of the same
type, i.e. NPs Ns, VPs Vs, PPs Ps, etc.
2. XP(X?) ? specifier X?
3. X??X complement(s)
These structural properties are conventionally rep-
resented as shown in figure 2.
Figure 2: X-bar structure
X is the head of the phrase XP. X? and XP(X?)
are called projections of X. The head is also called
the zero projection. X-bar structure is integrated
with the properties of lexical items via the Projec-
tion Principle of principle and parameters. This
principle is summed up as the properties of lexi-
cal information project onto the syntax of the sen-
tence. For instance:
? Sue likes Picasso
? *Sue likes
The subcategorization frame of the lexical item
like [ ,NP] ensures that the verb is followed by an
NP and the second sentence is of ungrammatical
form.
Maximal Projection (MP) is the constituent
which is projected to the highest level of an X-bar
structure from lexical entities and is therefore the
top node XP of the X-bar structure.
Take figure 1 for instance, S is the MP of the
predicate come. Though the syntax tree is not in D-
structure (deep structure), the S-structure (surface
structure) headed by come is similar to its genuine
D-structure. In a latter part of this section, a spe-
cific definition of MP in S-structure will be given
for application.
2.2 MP Limits Active Region of Arguments
MP holds all lexical properties of heads. In partic-
ular, the MP of a predicate holds predicate struc-
ture information and the constituents out of its do-
main cannot occupy argument positions. ?-theory
and government are two modules of principle and
parameters. They both suggest that the possi-
ble positions of semantic roles are in the sub-tree
rooted by MP.
834
Concerning assignment of semantic roles to
constituents, ?-theory suggests that semantic roles
are assigned by predicates to their sisters (Chom-
sky, 1986). Furthermore, in a X-bar theory, com-
plements are assigned semantic roles by the pred-
icate and specifiers get roles from the V?. In both
situations the process of roles assignment is in sis-
terhood condition and limited in the sub-structure
which is dominated by the MP. Only constituents
under MP can get semantic roles. The Case As-
signment Principle also points out: Case is as-
signed under government (Chomsky, 1981). Take
figure 1 for instance, only NP-1 and PP-2 can get
semantic roles of the head come.
From generative linguists? point, MP limits sub-
tree of arguments. Therefore, finding the MP is
equivalent to finding the active region of predicate
structure.
2.3 Definition of MP in S-structure
Though a clear enough definition of MP in D-
structure has been previously illustrated, it is still
necessary to define a specific one in S-structure
for application, especially for automatic parsing
which are not exactly correct. This paper de-
fines MP in S-structure (hereinafter denote MP
for short) as following: for every predicate p in the
syntax tree T , there exists one and only one MP
mp s.t.
1. mp dominates all arguments of p;
2. all descendent nodes of mp don?t satisfy the
former condition.
Due to its different characteristics from argu-
ments, adjunct-like arguments are excluded from
the set of arguments in generative grammar and
many other linguistic theories. For this reason, this
paper does not take them into account.
For gold syntax tree, there exists a one-to-one
mapping between arguments and nodes of syn-
tax trees, whereas automatic syntactic parsing con-
tains no such mapping. This paper do not take
arguments which cannot get corresponding con-
stituents into account to reduce the influence of au-
tomatic parsing error.
Take the sentence of figure 1 to illustrate our
definition of MP: S is MP of come since NP-1 and
PP-2 are arguments of it. There is no node map-
ping to the argument Wall Street professionals in
the parsing tree. Instead of covering argument?s
fragments, we simply take it PP-4 as MP.
2.4 Using MP Information in SRL
The boundaries of a predicate structure are two
word positions of the sentence. It is difficult to
model these two words. On the contrary, MP, as
one ancestor of predicate, has a clear-cut meaning
and is ideal for modeling. In this paper, the pol-
icy to predict MP rather than two word positions is
carried out to deal with locality of arguments.
Automatic prediction of MP can be viewed as a
preprocessing especially a pruning preprocessing
for SRL. Given a sentence and its parsing, SRL
systems can take seeking the active sub-tree rooted
by MP as the first step. Then SRL systems can
work on the shrunk syntax tree, and follow-up la-
beling processes can be in a various form. Most
of previous SRL methods still work without spe-
cial processing. Take figure 1 for example: when
labeling include, as the MP is PP-4, just NP-7 will
be extracted as argument candidate.
3 Analysis of Locality of Arguments
Principle and parameters suggests that MP bounds
arguments. Additionally, a statistical analysis
shows that possible positions of arguments are lim-
ited in a narrow region of syntax tree. An opposite
experiment also shows that MP information is use-
ful for SRL.
3.1 Data and Baseline System
In this paper, CoNLL-2005 SRL shared task
data (Carreras and M`arquez, 2005) is used as cor-
pus. The data consists of the Wall Street Jour-
nal (WSJ) part of the Penn TreeBank with infor-
mation on predicate argument structures extracted
from the PropBank corpus. In addition, the test
set of the shared task includes three sections of the
Brown corpus. Statistical analysis is based on sec-
tion 02-21 of WSJ. Experiments are conducted on
WSJ and Brown corpus. As defined by the shared
task, section 02-21 of PropBank are used for train-
ing models while section 23 and Brown corpus are
used for test. In terms of syntax information, we
use Charniak parser for POS tagging and full pars-
ing.
A majority of prior SRL approaches formulate
the SRL propblem as a multi-class classification
propblem. Generally speaking, these SRL ap-
proaches use a two-stage architecture: i) argument
identification; ii) argument classification, to solve
the task as a derivation of Gildea and Jurafsky?s
pioneer work (Gildea and Jurafsky, 2002). UIUC
835
Precision Recall F
?=1
Arg0 86.28% 87.01% 86.64
Arg1 79.37% 75.06% 77.15
Arg2 69.48% 62.97% 66.07
Arg3 69.01% 56.65% 62.22
Arg4 72.64% 75.49% 74.04
Table 1: SRL performance of UIUC SRLer
Precision Recall F
?=1
Arg0 91.84% 89.98% 90.90
Arg1 81.73% 75.93% 78.72
Arg2 69.86% 63.06% 66.29
Arg3 71.13% 58.38% 64.13
Arg4 73.08% 74.51% 73.79
Table 2: SRL performance of UIUC SRLer using
information of gold MP
Semantic Role Labeler
2
(UIUC SRLer) is a state-
of-the-art SRL system that based on the champion
system of CoNLL-2005 shared task (Carreras and
M`arquez, 2005). It is utilized as a baseline system
in this paper. The system participated in CoNLL-
2005 is based on several syntactic parsing results.
However, experiments of this paper just use the
best parsing result from Charniak parser. Param-
eters for training SRL models are the same as de-
scribed in (Koomen, 2005).
3.2 Active Region of Arguments
According to a statistical analysis, the average
depth from a target predicate to the root of a syntax
tree is 5.03, and the average depth from a predicate
to MP is just 3.12. This means about 40% of an-
cestors of a predicate do not dominate arguments
directly. In addition, the quantity of leaves in syn-
tax tree is another measure to analyze the domain.
On average, a syntax tree covers 28.51 leaves, and
MP dominates only 18.19. Roughly speaking, only
about 60% of words are valid for semantic roles.
Statistics of corpora leads to the following conclu-
sion: arguments which are assigned semantic roles
are in a local region of a whole syntax tree.
3.3 Typical Errors Caused by Neglect of
Locality of Arguments
The neglect of the locality of arguments in prior
SRL methods shows that it may cause errors.
Some constituents outside active region of argu-
ments may be falsely labeled as roles especially for
those being arguments of other predicates. A sta-
tistical analysis shows 20.62% of falsely labeled
arguments are constituents out of MP domain in
labeling results of UIUC SRLer. Take figure 1 for
instance, UIUC SRLer makes a mistake when la-
beling NP-1 which is Arg1 of the predicate come
for the target include; it labels Arg0 to NP. In fact,
the active region of include is the sub-tree rooted
2
http://l2r.cs.uiuc.edu/ cogcomp/srl-demo.php
by PP-4. Since NP-1 is an argument of another
predicate, some static properties of NP-1 make it
confusing as an argument.
3.4 SRL under Gold MP
If MP has been found before labeling semantic
roles, the set of role candidates will be shrunk,
and the capability to identify semantic roles may
be improved. An opposite experiment verifies this
idea. In the first experiment, UIUC SRLer is re-
trained as a baseline. For comparison, during the
second experiment, syntax sub-trees dominated by
gold MP are used as syntactic information. Both
training and test data are preprocessed with gold
MP information. That is to say we use pruned data
for training, and test is conducted on pruned syntax
sub-trees.
Table 1 and 2 show that except for Arg4, all ar-
guments get improved labeling performance, espe-
cially Arg0. Since arguments except for Arg0 are
realized as objects on the heel of predicate in most
case, the information of MP is not so useful for
them as Arg0. The experiment suggests that high
performance prediction of MP can improve SRL.
4 Prediction of MP
Conforming to government and ?-theory, MP is
not too difficult to predict in D-structure. Unfor-
tunately, sentences being looked at are in their sur-
face form and region of arguments has been ex-
panded. Simple rules alone are not adequate for
finding MP owing to a variety of movement be-
tween D-structure and S-structure. This paper de-
signs two data driven algorithms based on move-
ment principles for prediction of MP.
4.1 NP-movement and Prediction of MP
4.1.1 NP-movement in Principle and
Parameters
The relationship between D-structure and S-
structure is movement: S-structure equals D-
836
structure plus movement. NP-movement prin-
ciple in principle and parameters indicates that
noun phrases only move from A-positions (argu-
ment position) which have been assigned roles
to A-positions which have not, leaving an NP-
trace. On account of ?-theory and government, A-
positions are nodes m-commanded
3
by predicates
in D-structure. In NP-movement, arguments move
to positions which are C-commanded
4
by target
predicate and m-commanded by other predicates.
Broadly speaking, A-positions are C-commanded
by predicates after NP-movement. The key of the
well-known pruning algorithm raised in (Xue and
Palmer, 2004) is extracting sisters of ancestors as
role candidates. Those candidate nodes are all C-
commanders of a predicate. NP-movement can
give an explanation why the algorithm works.
4.1.2 Definition of Argument Anchor
To capture the characteristics of A-positions, we
make definition of A-anchor as following. For ev-
ery predicate p in the syntax tree T , denote A the
set of C-commanders of p:
? a left-A-anchor satisfies:
1. left-A-anchor belongs to A;
2. left-A-anchor is a noun phrase (includ-
ing NNS, NNP, etc.) or simple declara-
tive clause (S);
3. left-A-anchor is on the left hand of p.
? a right-A-anchor satisfies:
1. right-A-anchor belongs to A;
2. right-A-anchor is a noun phrase (includ-
ing NNS, NNP, etc.);
3. right-A-anchor is on the right hand of p.
Take figure 1 for example, NP-1, NP-4 and NP-
6 are left-A-anchors of include, and no right-A-
anchor. There is a close link between A-position
and the A-anchor that we defined, since A-anchors
occupy A-positions.
4.1.3 Anchor Model for Prediction of MP
Parents of A-anchors and first branching ances-
tor of the predicate can cover 96.25% of MP and
the number of those ancestors is 2.78 times of the
3
M-command is an concept in X-bar syntax. Assuming
? and ? are two nodes in a syntax tree: ? m-commands ?
means ? C-commands ? and the MP of ? dominates ?
4
C-command is an concept in X-bar theory. Assuming ?
and ? are two nodes in a syntax tree: ? C-commands ? means
every parent of ? is ancestor of ?.
number of MP. The number of all ancestors is 6.65
times. The data suggests that taking only these
kinds of ancestors as MP candidates can shrink the
candidate set with a relatively small loss.
4.2 Anchor Group Approach
MP is one ancestor of a predicate. An natural ap-
proach to predict MP is searching the set of all
ancestors. This idea encounters the difficulty that
there are too many ancestors. In order to reduce
the noise brought by non-anchors? parents, the an-
chor group approach prunes away useless ances-
tors which are neither parents of A-anchors nor
first branching node upon predicate from MP can-
didate set. Then the algorithm scores all candidates
and chooses the MP in argmax flavor. Formally,
we denote the set of MP candidates C and the score
function S(.).
m?p = argmax
c?C
S(mp|c)
Probability function is chosen as score func-
tion in this paper. In estimating of the probability
P (MP |C), log-linear model is used. This model is
often called maximum entropy model in research
of NLP. Let the set {1,-1} denotes whether a con-
stituent is MP and ?(c, {?1, 1}) ? R
s
denotes
a feature map from a constituent and the possible
class to the vector space R
s
. Formally, the model
of our system is defined as:
m?p = argmax
c?C
e
<?(c,1),?>
e
<?(c,1),?>
+e
<?(c,0),?>
The algorithm is also described in pseudo code
as following.
Ancestor Algorithm:
1: collect parents of anchors and the first
branching ancestor, denote them set C
2: for every c ? C
3: calculate P (mp|c)
4: return c? that gets the maximal P (mp|c)
4.2.1 Features
We use some features to represent various as-
pects of the syntactic structure as well as lexical
information. The features are listed as follows:
Path The path features are similar to the path
feature which is designed by (Gildea and Jurafsky,
2002).A path is a sequential collection of phrase
tags. There are two kinds of path features here: one
is from target predicate through to the candidate;
the other is from the candidate to the root of the
syntax tree. For include in the sentence of figure 1,
the first kind of path of PP-2 is VBG+PP+NP+PP
and the second is PP+VP+S.
837
C-commander Thread As well as path features,
C-commander threads are other features which
reflect aspects of the syntactic structures. C-
commander thread features are sequential contain-
ers of constituents which C-command the target
predicate. We design three kinds of C-commander
threads: 1) down thread collects C-commanders
from the anchor to the target predicate; 2) up
thread collects C-commanders from the anchor to
the left/right most C-commander; 3) full thread
collects all C-commanders in the left/right direc-
tion from the target predicate. Direction is depen-
dent on the type of the anchor - left or right anchor.
Considering the grammatical characteristics of
phrase, we make an equivalence between such
phrase types:
? JJ, JJR, JJS, ADJP
? NN, NNP, NNS, NNPS, NAC, NX, NP
Besides the equivalent constituents, we discard
these types of phrases:
? MD, RB, RBS, RBR, ADVP
For include in figure 1, the up thread of
NP-4 is VBG+,+NP+NP; the down thread
is NP+IN+VBD+NP; the full thread is
VBG+,+NP+NP+IN+VBD+NP.
The phrase type of candidate is an important fea-
ture for prediction
Candidate of MP. We also select the rank num-
ber of the current candidate and the number of all
candidates as features. For the former example,
the two features for PP-2 are 2 and 3, since NP-
4 is the second left-A-anchor and there are three
A-anchors of include.
Anchor Features of anchor include the head
word of the anchor, the boundary words and their
POS, and the number of the words in the anchor.
Those features are clues of judgment of whether
the anchor?s position is an A-position.
Forward predicate For the former example, the
forward predicate of NP-4 is come. The features
include the predicate itself, the Levin class and the
SCF of the predicate.
predicate Features of predicate include lemma,
Levin class, POS and SCF of the predicate.
Figure 3: Flow diagram of the single anchor ap-
proach
Formal Subject An anchor may be formal sub-
ject. Take It is easy to say the specialist is not do-
ing his job for example, the formal subject will be
recognized as anchor of do. We use a heuristic rule
to extract this feature: if the first NP C-commander
of the anchor is ?it? and the left word of predicate
is ?to?, the value of this feature is 1; otherwise 0.
The Maximal Length of C-commanders Con-
stituent which consists of many words may be a
barrier between the predicate and an A-position.
For the former example, if the target predicate is
include, this feature of NP-1 is 2, since the largest
constituent NP-4 is made up of two words.
4.3 Single Anchor Approach
Among all A-anchors, the right most left-A-anchor
such as NP-6 of include in figure 1 is the most im-
portant one for MP prediction. The parent of this
kind of left-A-anchor is the MP of the predicate,
obtaining a high probability of 84.59%. The single
anchor approach is designed based on right most
left-A-anchor. The key of this approach is an ac-
tion prediction that when right most left-A-anchor
is found, the algorithm predicts next action to re-
turn which node of syntax tree as MP. There is
a label set of three types for learning ? here, up,
down. After action is predicted, several simple
rules are executed as post process of this predic-
tion: i) if there is no left-A-anchor, return the root
of the whole syntax tree as MP; ii)if the predicted
label is here, return the parent of right most left-
A-anchor; iii) if the predicted label is down, return
838
Prediction Accuracy
Corpus Action MP
WSJ ? 87.75%
Brown ? 88.84%
Table 3: Accuracy of the anchor group ap-
proach
Prediction Accuracy
Corpus Action MP
WSJ 88.45% 83.63%
Brown 90.10% 85.70%
Table 4: Accuracy of the single anchor ap-
proach
Precision Recall F
?=1
Arg0 86.23% 87.90% 87.06
Arg1 80.21% 74.79% 77.41
Arg2 70.09% 62.70% 66.19
Arg3 71.74% 57.23% 63.67
Arg4 74.76% 75.49% 75.12
Table 5: SRL performance of UIUC SRLer us-
ing information of predicted MP; the anchor
group approach; WSJ test corpus
Precision Recall F
?=1
Arg0 87.03% 87.59% 87.31
Arg1 80.24% 74.77% 77.41
Arg2 70.35% 63.06% 66.51
Arg3 71.43% 57.80% 63.90
Arg4 73.33% 75.49% 74.40
Table 6: SRL performance of UIUC SRLer us-
ing information of predicted MP; the single an-
chor approach; WSJ test corpus
the first branching node upon the predicate; iv) if
the predicted label is up, return the root. The ac-
tion prediction also uses maximum entropy model.
Figure 3 is the flow diagram of the single anchor
approach. Features for this approach are similar
to the former method. Features of the verb which
is between the anchor and the predicate are added,
including the verb itself and the Levin class of that
verb.
5 Experiments and Results
Experiment data and toolkit have been illustrated
in section 3. Maxent
5
, a maximum entropy model-
ing toolkit, is used as a classifier in the experiments
of MP prediction.
5.1 Experiments of Prediction of MP
The results are reported for both the anchor group
approach and the single anchor approach. Table 3
summaries the accuracy results of MP prediction
for the anchor group approach; table 4 summaries
results of both action prediction and MP prediction
for the single anchor approach. Both the anchor
group approach and the single anchor approach
have better prediction performance in Brown test
set, though the models are trained on WSJ cor-
pus. These results illustrate that anchor approaches
which are based on suitable linguistic theories have
robust performance and overcome limitations of
training corpus.
5
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
5.2 Experiments of SRL Using MP Prediction
Like the experiments in the end of section 3, we
perform similar experiments under predicted MP.
Both training and test corpus make use of predicted
MP information. It is an empirical tactic that pre-
dicted information of maximal projection, instead
of gold information, is chosen for a training set.
Experiments suggest predicted information is bet-
ter. Table 5 is SRL performance using the anchor
group approach to predict MP; Table 6 is SRL per-
formance using the single anchor approach.
Compared with table 1 on page 4, table 5 and
table 6 both indicate the predicted MP can help to
label semantic roles. However, there is an interest-
ing phenomenon. Even though the anchor group
approach achieves a higher performance of MP,
the single anchor approach is more helpful to SRL.
18.56% of falsely labeled arguments are out of MP
domain using the single anchor approach to predict
MP, compared to 20.62% of the baseline system.
In order to test robustness of the contribution
of MP prediction to SRL, another opposite exper-
iment is performed using the test set from Brown
corpus. Table 7 is the SRL performance of UIUC
SRLer on Brown test set. Table 8 is the corre-
sponding performance using MP information pre-
dicted by the single anchor approach. Comparison
between table 7 and table 8 indicates the approach
of MP prediction proposed in this paper adapts to
other genres of corpora.
Capability of labeling Arg0 gets significant im-
provement. Subject selection rule, a part of the-
839
Precision Recall F
?=1
Arg0 82.88% 85.51% 84.17
Arg1 66.30% 63.17% 64.70
Arg2 50.00% 45.58% 47.69
Arg3 0.00% 0.00% 0.00
Arg4 60.00% 20.00% 30.00
Table 7: SRL performance of UIUC SRLer;
Brown test corpus
Precision Recall F
?=1
Arg0 83.85% 86.22% 85.02
Arg1 66.67% 63.02% 64.79
Arg2 50.38% 44.90% 47.48
Arg3 0.00% 0.00% 0.00
Arg4 60.00% 20.00% 30.00
Table 8: SRL performance of UIUC SRLer us-
ing information of predicted MP; the single an-
chor approach; Brown test corpus
matic hierarchy theory, states that the argument
that the highest role (i.e. proto-agent, Arg0 in
PropBank) is the subject. This means that Arg0 is
usually realized as a constituent preceding a predi-
cate and has a long distance from the predicate. As
a solution of finding active region of arguments,
MP prediction is helpful to shrink the searching
range of arguments preceding the predicate. From
this point, we give a rough explanation why exper-
iment results for Arg0 are better.
6 Conclusion
Inspired by the locality phenomenon that argu-
ments are usually limited in a syntax sub-tree, this
paper proposed to label semantic roles locally in
the active region arguments dominated by maximal
projection, which is a concept in D-structure from
the projection principle of the principle and param-
eters theory. Statistical analysis showed that MP
information was helpful to avoid errors in SRL,
such as falsely recognizing constituents outside ac-
tive region as arguments. To adapt the projection
concept to label semantic roles, this paper defined
MP in S-structure and proposed two methods to
predict MP, namely the anchor group approach and
the single anchor approach. Both approaches were
based on NP-movement principle of principle and
parameters. Experimental results indicated that
our MP prediction methods improved SRL.
Acknowlegements
The work is supported by the National Natu-
ral Science Foundation of China under Grants
No. 60503071, 863 the National High Technol-
ogy Research and Development Program of China
under Grants No.2006AA01Z144, 973 Natural
Basic Research Program of China under Grants
No.2004CB318102.
References
Carreras, Xavier and Llu??s M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: semantic role
labeling. In Proceedings of Conference on Natural
Language Learning.
Chomsky, Noam. 1981. Lectures on Government and
Binding. Foris Publications, Dordrecht.
Chomsky, Noam. 1986. Barriers. MIT Press, Barriers.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computional Linguis-
tics, 28(3):245?288.
Gordon, Andrew and Reid Swanson. 2007. Generaliz-
ing Semantic Role Annotations Across Syntactically
Similar Verbs. In Proceedings of Conference on As-
sociation for Computational Linguistics.
Koomen, Peter, Vasina Punyakanok, Dan Roth and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of Conference on Natural Language Learn-
ing.
Liu, Yudong and Anoop Sarkar. 2004. Experimen-
tal Evaluation of LTAG-Based Features for Semantic
Role Labeling. In Proceedings of Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning.
Mocshitti, Alessandro. 2004. A Study on Convolu-
tion Kernels for Shallow Semantic Parsing. In Pro-
ceedings of Conference on Association for Compu-
tational Linguistics.
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. In Proceedings of Conference
on Association for Computational Linguistics.
Toutanova, Kristina, Aria Haghighi and Christopher
Manning. 2005. Joint Learning Improves Seman-
tic Role Labeling. In Proceedings of Conference on
Association for Computational Linguistics.
Xue, Nianwen and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing.
840
An Information-Theory-Based Feature Type Analysis for the
Modelling of Statistical Parsing
SUI Zhifang ??, ZHAO Jun ?, Dekai WU ?
? Hong Kong University of Science & Technology
Department of Computer Science
Human Language Technology Center
Clear Water Bay, Hong Kong
? Peking University
Department of Computer Science & Technology
Institute of Computational Linguistics
Beijing, China
suizf@icl.pku.edu.cn, zhaojun@cs.ust.hk, dekai@cs.ust.hk
Abstract
The paper proposes an information-theory-
based method for feature types analysis in
probabilistic evaluation modelling for
statistical parsing. The basic idea is that we
use entropy and conditional entropy to
measure whether a feature type grasps some
of the information for syntactic structure
prediction. Our experiment quantitatively
analyzes several feature types? power for
syntactic structure prediction and draws a
series of interesting conclusions.
1  Introduction
In the field of statistical parsing, various
probabilistic evaluation models have been
proposed where different models use different
feature types [Black, 1992] [Briscoe, 1993]
[Brown, 1991] [Charniak, 1997] [Collins, 1996]
[Collins, 1997] [Magerman, 1991] [Magerman,
1992] [Magerman, 1995] [Eisner, 1996]. How to
evaluate the different feature types? effects for
syntactic parsing? The paper proposes an
information-theory-based feature types analysis
model, which uses the measures of predictive
information quantity, predictive information
gain, predictive information redundancy and
predictive information summation to
quantitatively analyse the different contextual
feature types? or feature types combination?s
predictive power for syntactic structure.
  In the following, Section 2 describes the
probabilistic evaluation model for syntactic trees;
Section 3 proposes an information-theory-based
feature type analysis model; Section 4
introduces several experimental issues; Section 5
quantitatively analyses the different contextual
feature types or feature types combination in the
view of information theory and draws a series of
conclusion on their predictive powers for
syntactic structures.
2  The probabilistic evaluation model
for statistical syntactic parsing
Given a sentence, the task of statistical syntactic
parsing is to assign a probability to each
candidate parsing tree that conforms to the
grammar and select the one with highest
probability as the final analysis result. That is:
)|(maxarg STPT
T
best =  (1)
where S denotes the given sentence, T denotes
the set of all the candidate parsing trees that
conform to the grammar, P(T|S) denotes the
probability of parsing tree T for the given
sentence S.
  The task of probabilistic evaluation model in
syntactic parsing is the estimation of P(T|S). In
the syntactic parsing model which uses rule-
based grammar, the probability of a parsing tree
can be defined as the probability of the
derivation which generates the current parsing
tree for the given sentence. That is,
?
?
=
=
?
=
=
=
n
i
ii
n
i
ii
n
ShrP
SrrrrP
SrrrPSTP
1
1
121
21
),|(
),,,,|(
)|,,,()|(


(2)
Where, 121 ,,, ?irrr   denotes a derivation rule
sequence, hi denotes the partial parsing tree
derived from 121 ,,, ?irrr  .
  In order to accurately estimate the parameters,
we need to select some feature types
mFFF ,,, 21  , depending on which we can
divide the contextual condition Shi ,  for
predicting rule ri into some equivalence classes,
that is, ],[, ,,, 21 ShSh iFFFi m???? ??  , so that
??
==
?
n
i
ii
n
i
ii ShrPShrP
11
]),[|(),|(  (3)
According to the equation of (2) and (3), we
have the following equation:
?
=
?
n
i
ii ShrPSTP
1
]),[|()|(  (4)
  In this way, we can get a unite expression of
probabilistic evaluation model for statistical
syntactic parsing. The difference among the
different parsing models lies mainly in that they
use different feature types or feature type
combination to divide the contextual condition
into equivalent classes. Our ultimate aim is to
determine which combination of feature types is
optimal for the probabilistic evaluation model of
statistical syntactic parsing. Unfortunately, the
state of knowledge in this regard is very limited.
Many probabilistic evaluation models have been
published inspired by one or more of these
feature types [Black, 1992] [Briscoe, 1993]
[Charniak, 1997] [Collins, 1996] [Collins, 1997]
[Magerman, 1995] [Eisner, 1996], but
discrepancies between training sets, algorithms,
and hardware environments make it difficult, if
not impossible, to compare the models
objectively. In the paper, we propose an
information-theory-based feature type analysis
model by which we can quantitatively analyse
the predictive power of different feature types or
feature type combinations for syntactic structure
in a systematic way. The conclusion is expected
to provide reliable reference for feature type
selection in the probabilistic evaluation
modelling for statistical syntactic parsing.
3 The information-theory-based
feature type analysis model for statistical
syntactic parsing
In the prediction of stochastic events, entropy
and conditional entropy can be used to evaluate
the predictive power of different feature types.
To predict a stochastic event, if the entropy of
the event is much larger than its conditional
entropy on condition that a feature type is
known, it indicates that the feature type grasps
some of the important information for the
predicted event.
  According to the above idea, we build the
information-theory-based feature type analysis
model, which is composed of four concepts:
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation.
z Predictive Information Quantity (PIQ)
);( RFPIQ , the predictive information quantity
of feature type F to predict derivation rule R, is
defined as the difference between the entropy of
R and the conditional entropy of R on condition
that the feature type F is known.
?
?? ?
=
?=
RrFf rPfP
rfP
rfP
FRHRHRFPIQ
,
)()(
),(log),(
)|()();(
   (5)
  Predictive information quantity can be used to
measure the predictive power of a feature type in
feature type analysis.
z Predictive Information Gain (PIG)
For the prediction of rule R,
PIG(Fx;R|F1,F2,...,Fi), the predictive information
gain of taking Fx as a variant model on top of a
baseline model employing F1,F2,...,Fi as feature
type combination, is defined as the difference
between the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi
and the conditional entropy of predicting R
based on feature type combination F1,F2,...,Fi,Fx.
)6(),,,(
),,(
),,,(
),,,,(log),,,,(
),,,|(),,|(),,|;(
1
1
1
1
1
111
11
rffP
ffP
fffP
rfffP
rfffP
FFFRHFFRHFFRFPIG
i
i
xi
xi
Rr
Ff
Ff
Ff
xi
xiiix
xx
ii







?=
?=
?
?
?
?
?
 If ),,,|;(),,,|;( 2121 iyix FFFRFPIGFFFRFPIG  > ,
then Fx is deemed to be more informative than
Fy for predicting R on top of F1,F2,...,Fi, no
matter whether PIQ(Fx;R) is larger than
PIQ(Fy;R) or not.
z Predictive Information Redundancy(PIR)
Based on the above two definitions, we can
further draw the definition of predictive
information redundancy as follows.
PIR(Fx,{F1,F2,...,Fi};R) denotes the redundant
information between feature type Fx and feature
type combination {F1,F2,...,Fi} in predicting R,
which is defined as the difference between
PIQ(Fx;R) and PIG(Fx;R|F1,F2,...,Fi). That is,
),,,|;();(
)};,,,{,(
21
21
ixx
ix
FFFRFPIGRFPIQ
RFFFFPIR


?=
 (7)
  Predictive information redundancy can be
used as a measure of the redundancy between
the predictive information of a feature type and
that of a feature type combination.
z Predictive Information Summation (PIS)
PIS(F1,F2,...,Fm;R), the predictive information
summation of feature type combination
F1,F2,...,Fm, is defined as the total information
that F1,F2,...,Fm can provide for the prediction of
a derivation rule. Exactly,
?
=
?
+=
m
i
ii
m
FFRFPIGRFPIQ
RFFFPIS
2
111
21
),,|;();(
);,,,(


 (8)
4 Experimental Issues
4.1 The classification of the feature
types
The predicted event of our experiment is the
derivation rule to extend the current non-
terminal node. The feature types for prediction
can be classified into two classes, history feature
types and objective feature types. In the
following, we will take the parsing tree shown in
Figure-1 as the example to explain the
classification of the feature types.
In Figure-1, the current predicted event is the
derivation rule to extend the framed non-
terminal node VP, the part connected by the
solid line belongs to history feature types, which
is the already derived partial parsing tree,
representing the structural environment of the
current non-terminal node. The part framed by
the larger rectangle belongs to the objective
feature types, which is the word sequence
containing the leaf nodes of the partial parsing
tree rooted by the current node, representing the
final objectives to be derived from the current
node.
4.2 The corpus used in the experiment
The experimental corpus is derived from Penn
TreeBank[Marcus,1993]. We semi-
automatically assign a headword and a POS tag
to each non-terminal node. 80% of the corpus
(979,767 words) is taken as the training set, used
for estimating the various co-occurrence
probabilities, 10% of the corpus (133,814 words)
is taken as the testing set, used to calculate
predictive information quantity, predictive
information gain, predictive information
redundancy and predictive information
summation. The other 10% of the corpus
(133,814 words) is taken as the held-out set. The
grammar rule set is composed of 8,126 CFG
rules extracted from Penn TreeBank.
S
V P
V P
N N P
Pierre
N N P
Vinken
M D
will
V B
join
D T
the
N N
board
IN
as
D T
a
JJ
nonexecut ive
N N
director
N N P
Nov.
C D
29
.
.
N P N P N P
P P
N P
Figure-1: The classification of feature types
4.3 The smoothing method used in the
experiment
In the information-theory-based feature type
analysis model, we need to estimate joint
probability ),,,,( 21 rfffP i . Let F1,F2,...,Fi be
the feature type series selected till now,
RrFfFfFf ii ???? ,,,, 2211  , we use a
blended probability ),,,,(~ 21 rfffP i  to
approximate probability ),,,,( 21 rfffP i  in
order to solve the sparse data problem[Bell,
1992].
  
?
=
??
++=
i
j
jj
i
rfffPwrPwrPw
rfffP
1
210011
21
),,,,()()(
),,,,(~


   (9)
In the above formula,
         
?
?
?
=
Rr
rc
rP
?
1 )?(
1)(
              (10)
  
?
?
=
Rr
rc
rc
rP
?
0 )?(
)()(                (11)
where )(rc is the total number of time that r has
been seen in the corpus.
  According to the escape mechanism in [Bell,
1992], we define the weights wk )1( ik ?<?  in
the formula (9) as follows.
   
ii
i
ks
skk
ew
ikeew
?=
????= ?
+=
1
1,)1(
1       (12)
where ek denotes the escape probability of
context ),,,( 21 kfff   , that is, the probability
in which (f1 , f2 , ... , fk , r) is unseen in the corpus.
In such case, the blending model has to escape
to the lower contexts to approximate
),,,,( 21 rfffP k . Exactly, escape probability is
defined as
  
???
???
?
?=
??
= ?
?
?
?
1,0
0,)?,,...,,(
)?,,...,,(
?
21
?
21
k
ik
rfffc
rfffd
e
Rr
k
Rr
k
k    (13)
where
   
??
?
=
>
=
0)?,,...,,(,0
0)?,,...,,(,1)?,,...,,(
21
21
21
rfffcif
rfffcif
rfffd
k
k
k  (14)
In the above blending model, a special
probability ?
?
?
=
Rr
rc
rP
?
1 )?(
1)(  is used, where all
derivation rules are given an equal probability.
As a result, 0),,,,(~ 21 >rfffP i  as long as
0)?(
?
>?
?Rr
rc .
5 The information-theory-based
feature type analysis
The experiments led to a number of interesting
conclusions on the predictive power of various
feature types and feature type combinations,
which is expected to provide reliable reference
for the modelling of probabilistic parsing.
5.1 The analysis to the predictive
information quantities of lexical feature
types, part-of-speech feature types and
constituent label feature types
z Goal
One of the most important variation in statistical
parsing over the last few years is that statistical
lexical information is incorporated into the
probabilistic evaluation model. Some statistical
parsing systems show that the performance is
improved after the lexical information is added.
Our research aims at a quantitative analysis of
the differences among the predictive information
quantities provided by the lexical feature types,
part-of-speech feature types and constituent
label feature types from the view of information
theory.
z Data
The experiment is conducted on the history
feature types of the nodes whose structural
distance to the current node is within 2.
  In Table-1, ?Y? in PIQ(X of Y; R) represents
the node, ?X? represents the constitute label, the
headword or POS of the headword of the node.
In the following, the units of PIQ are bits.
z Conclusion
Among the feature types in the same structural
position of the parsing tree, the predictive
information quantity of lexical feature type is
larger than that of part-of-speech feature type,
and the predictive information quantity of part-
of-speech feature type is larger than that of the
constituent label feature type.
Table-1: The predictive information quantity of the history feature type candidates
PIQ(X of Y; R) X= constituent label X= headword X= POS of
the headword
Y= the current node 2.3609 3.7333 2.7708
Y= the parent 1.1598 2.3253 1.1784
Y= the grandpa 0.6483 1.6808 0.6612
Y= the first right brother of the current node 0.4730 1.1525 0.7502
Y= the first left brother of the current node 0.5832 2.1511 1.2186
Y= the second right brother of the current node 0.1066 0.5044 0.2525
Y= the second left brother of the current node 0.0949 0.6171 0.2697
Y= the first right brother of the parent 0.1068 0.3717 0.2133
Y= the first left brother of the parent 0.2505 1.5603 0.6145
5.2 The analysis to the influence of the
structural relation and the structural
distance to the predictive information
quantities of the history feature types
z Goal:
In this experiment, we wish to find out the
influence of the structural relation and structural
distance between the current node and the node
that the given feature type related to has to the
predictive information quantities of these feature
types.
z Data:
In Table-2, SR represents the structural relation
between the current node and the node that the
given feature type related to. SD represents the
structural distance between the current node and
the node that the given feature type related to.
Table-2: The predictive information quantity of the selected history feature types
PIQ(constituent label
of Y; R)
SR= parent relation SR= brother relation SR= mixed parent and
brother relation
0.5832
(Y= the first left brother)
SD=1 1.1598
(Y= the parent)
0.4730
(Y= the first right brother)
0.2505
(Y= the first left brother
of the parent)
0.0949
(Y= the second left brother)
SD=2 0.6483
(Y= the grandpa)
0.1066
(Y= the second right brother)
0.1068
(Y= the first right
brother of the parent)
z Conclusion
Among the history feature types which have the
same structural relation with the current node
(the relations are both parent-child relation, or
both brother relation, etc), the one which has
closer structural distance to the current node will
provide larger predictive information quantity;
Among the history feature types which have the
same structural distance to the current node, the
one which has parent relation with the current
node will provide larger predictive information
quantity than the one that has brother relation or
mixed parent and brother relation to the current
node (such as the parent's brother node).
5.3 The analysis to the predictive
information quantities of the history
feature types and the objective feature
types
z Goal
Many of the existing probabilistic evaluation
models prefer to use history feature types other
than objective feature types. We select some of
history feature types and objective feature types,
and quantitatively compare their predictive
information quantities.
z Data
The history feature type we use here is the
headword of the parent, which has the largest
predictive information quantity among all the
history feature types. The objective feature types
are selected stochastically, which are the first
word and the second word in the objective word
sequence of the current node (Please see 4.1 and
Figure-1 for detailed descriptions on the selected
feature types).
Table-3: The predictive information quantity of the selected history and objective feature types
Class Feature type PIQ(Y;R)
History feature type Y= headword of the parent 2.3253
Y= the first word in the objective word sequence 3.2398Objective feature type
Y= the second word in the objective word sequence 3.0071
z Conclusion
Either of the predictive information quantity of
the first word and the second word in the
objective word sequence is larger than that of
the headword of the parent node which has the
largest predictive information quantity among all
of the history feature type candidates. That is to
say, objective feature types may have larger
predictive power than that of the history feature
type.
5.4 The analysis to the predictive
information quantities of the objective
features types selected respectively on the
physical position information, the
heuristic information of headword and
modifier, and the exact headword
information
z Goal
Not alike the structural history feature types, the
objective feature types are sequential. Generally,
the candidates of the objective feature types are
selected according to the physical position.
However, from the linguistic viewpoint, the
physical position information can hardly grasp
the relations between the linguistic structures.
Therefore, besides the physical position
information, our research try to select the
objective feature types respectively according to
the exact headword information and the heuristic
information of headword and modifier. Through
the experiment, we hope to find out what
influence the exact headword information, the
heuristic information of headword and modifier,
and the physical position information have
respectively to the predictive information
quantities of the feature types.
z Data:
  Table-4 gives the evidence for the claim.
Table-4: the predictive information quantity of the selected objective feature types
the information used to select the objective
feature types
PIQ(Y;R)
the physical position information 3.2398
(Y= the first word in the objective word sequence)
Heuristic information 1: determine whether a
word has the possibility to act as the headword of
the current constitute according to its POS
3.1401
(Y= the first word in the objective word sequence
which has the possibility to act as the headword of
the current constitute)
Heuristic information 2: determine whether a
word has the possibility to act as the modifier of
the current constitute according to its POS
3.1374
(Y= the first word in the objective word sequence
which has the possibility to act as the modifier of the
current constitute)
Heuristic information 3: given the current
headword, determine whether a word has the
possibility to modify the headword
2.8757
(Y= the first word in the objective word sequence
which has the possibility to modify the headword)
the exact headword information 3.7333
(Y= the headword of the current constitute)
z Conclusion
The predictive information quantity of the
headword of the current node is larger than that
of a feature type selected according to the
selected heuristic information of headword or
modifier, and larger than that of a feature type
selected according to the physical positions; The
predictive information quantity of a feature type
selected according to the physical positions is
larger than that of a feature types selected
according to the selected heuristic information
of headword or modifier.
5.5 The selection of the feature type
combination which has the optimal
predictive information summation
z Goal:
We aim at proposing a method to select the
feature types combination that has the optimal
predictive information summation for prediction.
z Approach
We use the following greedy algorithm to select
the optimal feature type combination.
  In building a model, the first feature type to
be selected is the feature type which has the
largest predictive information quantity for the
prediction of the derivation rule among all of the
feature type candidates, that is,
);(maxarg1 RFPIQF i
Fi ??
=    (15)
Where ?  is the set of candidate feature types.
  Given that the model has selected feature type
combination jFFF ,,, 21  , the next feature
type to be added into the model is the feature
type which has the largest predictive information
gain in all of the feature type candidate except
jFFF ,,, 21  , on condition that jFFF ,,, 21 
is known. That is,
)16(),,,|;( 21
},,2,1{
1 maxarg ji
jFFFiF
iF
j FFFRFPIGF 
?
??
+ =
z Data:
Among the feature types mentioned above, the
optimal feature type combination (i.e. the feature
type combination with the largest predictive
information summation) which is composed of 6
feature types is, the headword of the current
node (type1), the headword of the parent node
(type2), the headword of the grandpa node
(type3), the first word in the objective word
sequence(type4), the first word in the objective
word sequence which have the possibility to act
as the headword of the current constitute(type5),
the headword of the right brother node(type6).
The cumulative predictive information
summation is showed in Figure-2
0
1
2
3
4
5
6
7
type1 type2 type3 type4 type5 type6
feature type
cu
m
m
u
la
tiv
e 
pr
ed
ic
tin
g 
in
fo
rm
at
io
n
su
m
m
at
io
n
Figure-2: The cumulative predictive information summation of the feature type combinations
6 Conclusion
The paper proposes an information-theory-based
feature type analysis method, which not only
presents a series of heuristic conclusion on the
predictive power of the different feature types
and feature type combination for syntactic
parsing, but also provides a guide for the
modeling of syntactic parsing in the view of
methodology, that is, we can quantitatively
analyse the different contextual feature types or
feature types combination's effect for syntactic
structure prediction in advance. Based on these
analysis, we can select the feature type or feature
types combination that has the optimal
predictive information summation to build the
probabilistic parsing model.
  However, there are still some questions to be
answered in this paper. For example, what is the
beneficial improvement in the performance after
using this method in a real parser? Whether the
improvements in PIQ will lead to the
improvement of parsing accuracy or not? In the
following research, we will incorporate these
conclusions into a real parser to see whether the
parsing accuracy can be improved or not.
Another work we will do is to do some
experimental analysis to find the impact of data
sparseness on feature type analysis, which is
critical to the performance of real systems.
  The proposed feature type analysis method
can be used in not only the probabilistic
modelling for statistical syntactic parsing, but
also language modelling in more general fields
[WU, 1999a] [WU, 1999b].
References
Bell, T.C., Cleary, J.G., Witten,I.H. 1992. Text
Compression, PRENTICE HALL, Englewood
Cliffs, New Jersey 07632, 1992
Black, E., Jelinek, F.,Lafferty, J.,Magerman, D.M.,
Mercer, R. and Roukos, S. 1992.  Towards
history-based grammars: using richer models of
context in probabilistic parsing. In Proceedings of
the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
Brown, P., Jelinek, F., & Mercer, R. 1991. Basic
method of probabilistic context-free grammars.
IBM internal Report, Yorktown Heights, NY.
T.Briscoe and J. Carroll. 1993. Generalized LR
parsing of natural language (corpora) with
unification-based grammars. Computational
Linguistics, 19(1): 25-60
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statics. In
Proceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park.
Stanley F. Chen and Joshua Goodman. 1999.  An
Empirical Study of Smoothing Techniques for
Language Modeling. Computer Speech and
Language, Vol.13, 1999
Michael John Collins. 1996.  A new statistical
parser based on bigram lexical dependencies. In
Proceedings of the 34th Annual Meeting of the
ACL.
Michael John Collins. 1997. Three generative
lexicalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of the
ACL.
J.Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In
Proceedings of COLING-96, pages 340-345
Joshua Goodman. 1998. Parsing Inside-Out. PhD.
Thesis, Harvard University, 1998
Magerman, D.M. and Marcus, M.P. 1991. Pearl: a
probabilistic chart parser. In Proceedings of the
European ACL Conference, Berlin, Germany.
Magerman, D.M. and Weir, C. 1992. Probabilistic
prediction and Picky chart parsing. In Proceedings
of the February 1992 DARPA Speech and Natural
Language Workshop, Arden House, NY.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33th
Annual Meeting of the ACL.
Mitchell P. Marcus, Beatrice Santorini & Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn treebank.
Computational Linguistics 19, pages 313-330
C. E. Shannon. 1951. Prediction and Entropy of
Printed English. Bell System Technical Journal,
1951
Dekai,Wu, Sui Zhifang, Zhao Jun. 1999a. An
Information-Based Method for Selecting Feature
Types for Word Prediction. Proceedings of
Eurospeech'99, Budapest Hungary
Dekai, Wu, Zhao Jun, Sui Zhifang. 1999b. An
Information-Theoretic Empirical Analysis of
Dependency-Based Feature Types for Word
Prediction Models. Proceedings of EMNLP'99,
University of Maryland, USA
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1?11,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Event-based Time Label Propagation for Automatic Dating of News Articles
Tao Ge Baobao Chang? Sujian Li Zhifang Sui
Key Laboratory of Computational Linguistics, Ministry of Education
School of Electronics Engineering and Computer Science, Peking University
No.5 Yiheyuan Road, Haidian District, Beijing, P.R.China, 100871
{getao,chbb,lisujian,szf}@pku.edu.cn
Abstract
Since many applications such as timeline sum-
maries and temporal IR involving temporal
analysis rely on document timestamps, the
task of automatic dating of documents has
been increasingly important. Instead of using
feature-based methods as conventional mod-
els, our method attempts to date documents
in a year level by exploiting relative tempo-
ral relations between documents and events,
which are very effective for dating documents.
Based on this intuition, we proposed an event-
based time label propagation model called
confidence boosting in which time label in-
formation can be propagated between docu-
ments and events on a bipartite graph. The ex-
periments show that our event-based propaga-
tion model can predict document timestamps
in high accuracy and the model combined with
a MaxEnt classifier outperforms the state-of-
the-art method for this task especially when
the size of the training set is small.
1 Introduction
Time is an important dimension of any informa-
tion space and can be useful in information re-
trieval, question-answering systems and timeline
summaries. In the applications involving tempo-
ral analysis, document timestamps are very useful.
For instance, temporal information retrieval mod-
els take into consideration the document?s creation
time for document retrieval and ranking (Kalczyn-
ski and Chou, 2005; Berberich et al, 2007) for bet-
ter dealing with time-sensitive queries; some infor-
?Corresponding author
mation retrieval applications such as Google Scholar
can list articles published during the time a user
specifies for better satisfying users? needs. In addi-
tion, timeline summarization techniques (Hu et al,
2011; Binh Tran et al, 2013) and some event-event
ordering models (Chambers and Jurafsky, 2008;
Yoshikawa et al, 2009) also rely on the timestamps.
Unfortunately, many documents on the web do not
have a credible timestamp, as Chambers (2012) re-
ported. Therefore, it is significant to date docu-
ments, that is to predict document creation time.
One typical method for dating document is based
on temporal language models, which were first used
for dating by de Jong et al (2005). They learned
language models (unigram) for specific time periods
and scored articles with normalized log-likelihood
ratio scores. The other typical approach for the task
was proposed by Nathanael Chambers (2012). In
Chambers?s work, discriminative classifiers ? max-
imum entropy (MaxEnt) classifiers were used by
incorporating linguistic features and temporal con-
straints for training, which outperforms the previous
temporal language models on a subset of Gigaword
Corpus (Graff et al, 2003).
However, the conventional methods have some
limitations because they predict creation time of
documents mainly based on feature-based models
without understanding content of documents, which
may lead to wrong predictions in some cases. For
instance, assume that D1 and D2 are documents
whose content is given as follows:
(D1) Sudan last year accused Eritrea of
backing an offensive by rebels in the east-
ern border region.
1
(D2) Two years ago, Sudan accused Er-
itrea of backing an offensive by rebels in
the eastern border region.
SinceD1 andD2 share many important features, the
previous dating methods are very likely to predict
the same timestamp for the two documents. How-
ever, it will be easy to infer that the creation time of
D1 should be one year earlier than that of D2 if we
analyze the content of the two documents.
Unlike the previous methods, this paper exploits
relative temporal relations between events and doc-
uments for dating documents on the basis of an un-
derstanding of document content.
It is known that each event in a news article has
a relative temporal relation with the document. By
analyzing the relative temporal relation, time of the
event can be known if we know the document times-
tamp; on the other hand, if the time of an event is
known, it can also be used to predict the creation
time of documents mentioning the event, which can
be best demonstrated with the above-mentioned ex-
ample of D1 and D2. In the example, ?last year?
is an important cue to infer that the event mentioned
by the documents occurred in 2002 if we know the
timestamp of D1 is 2003. With the information that
the event occurred in 2002, it can also be inferred
from the temporal expression ?Two years ago? that
D2 was written in 2004. In this way, the timestamp
of the labeled document (D1) is propagated to the
unlabeled document (D2) through the event both of
them mention, which is the main intuition of this pa-
per.
In fact, this intuition seems practical to date doc-
uments on the web because web data is very re-
dundant. Many documents on the web can be con-
nected via events because an event is usually men-
tioned by different documents. According to our
analysis of a collection of news articles spanning 5
years, it is found that an event is mentioned by 3.44
news articles on average; on the other hand, a doc-
ument usually refers to multiple events. Therefore,
if one knows a document timestamp, time of events
the document mentions can be obtained by analyz-
ing the relative temporal relations between the doc-
ument and the events. Likewise, if the time of an
event is known, then it can be used to predict cre-
ation time of the documents which mention it.
Based on the intuition, we proposed an event-
based time label propagation model called confi-
dence boosting in which timestamps are propagated
according to relative temporal relations between
documents and events. In this way, documents can
be dated with an understanding of content so that
this model can date document more credibly. To our
knowledge, it is the first time that the relative tempo-
ral relations between documents and events are ex-
ploited for dating documents, which is proved to be
effective by the experimental results.
2 Event-based Time Label Propogation
As mentioned above, the relative temporal relations
between documents and events are useful for dat-
ing documents. By analyzing the temporal relations,
even if there are only a small number of documents
labeled with timestamps, this information can be
propagated to documents connected with them on a
bipartite graph using breadth first traversal (BFS).
Figure 1: An example of BFS-based propagation
As shown in figure 1, there are two kinds of nodes
in the bipartite graph. A document node is a single
document while an event node represents an event.
The edge between a document node and an event
node means that the document mentions the event.
Also, the edge carries the information of the rela-
tive temporal relation between the document and the
event. The label propagation from node i to node j
will occur if BFS condition which is defined as fol-
lows is satisfied:{
eij ? E
i ? L and j /? L
(BFS condition)
When the timestamp of i is propagated to j:
Y (j) = Y (i) + ?(i, j)
L = L ? {j}
where E is the set of edges of the bipartite graph,
eij denotes the edge between node i and j, L is the
set of nodes which have been already labeled with
timestamps, Y (i) is the year of node i and ?(i, j) is
the relative temporal relation between node i and j.
2
In figure 1, the timestamp of document D1 is 2003,
which is known. This information can be propagated
to its adjacent nodes i.e. the event nodes it men-
tions according to the relative temporal relations.
Then, these event nodes propagate their timestamps
to other documents which mention them. By re-
peating this process, the timestamp of the document
can be propagated to documents which are reachable
from the initially labeled document on the bipartite
graph.
Although the BFS-based propagation process can
propagate timestamps from few labeled documents
to a large number of unlabeled ones, it has two short-
comings for this task. First, once one timestamp is
propagated incorrectly, this error will lead to more
mistakes in the following propagations. If such an
error occurred at the beginning of the propagation
process, it would lead to propagation of errors. Sec-
ond, BFS-based method cannot address conflict of
predictions during propagation, which is shown in
figure 2.
Figure 2: Conflict of predictions during propagation
To address the problems of the BFS-based
method, we proposed a novel propagation model
called confidence boosting model which improves
the BFS-based model by optimizing the global con-
fidence of the bipartite graph. In the confidence
boosting model, every node in the bipartite graph
has a confidence which measures the credibility of
the predicted timestamp of the node. When the
timestamp of a node is propagated to other nodes,
its confidence will be also propagated to the tar-
get nodes with some loss. The loss of confi-
dence is called confidence decay. Formally, the
confidence decay process is described as follows:
c(j) = c(i)? ?(i, j)
where c(i) denotes confidence of node i and
?(i, j) is the decay factor from node i to
node j. For guaranteeing that timestamps
can be propagated on the bipartite graph cred-
ibly, we define the following condition which
is called CB (Confidence Boosting) condition:{
eij ? E
c(i)? ?(i, j) > c(j)
(CB condition)
In the confidence boosting model, propagation from
node i to node j will occur only if CB condition is
satisfied. When timestamps are propagated on the
bipartite graph, timestamps and confidence of nodes
will be updated dynamically. A node with high con-
fidence is more active than nodes with low confi-
dence to propagate its timestamp because a node
with high confidence is more likely to satisfy the CB
condition for propagating its timestamp. Moreover,
a prediction with low confidence can be corrected by
the prediction with high confidence. Therefore, the
confidence boosting model can address both prop-
agation of errors and conflict of predictions which
cannot be tackled by the BFS-based model.
However, there are challenges for running such
propagation models in practice. First, the relative
temporal relations between documents and events
are usually unavailable. Second, events extracted
from different documents do not have any connec-
tion even if they refer to the same event. There-
fore, each event is connected with only one docu-
ment in the bipartite graph and thus cannot prop-
agate its timestamp to other documents unless we
perform event coreference resolution. Third, propa-
gations from generic events are very likely to lead to
propagation errors because generic events can hap-
pen in any year. Also, how to set the confidence and
decay factors reasonably in practice for a confidence
boosting model is worthy of investigation. All these
challenges for the propagation models and their cor-
responding solutions will be discussed in Section 3.
3 Details of Event-based Propagation
Models
In this section, details of the event-based time la-
bel propagation models including challenges and
their corresponding solutions are presented. We first
discuss the event extraction and processing involv-
ing relative temporal relation mining, event coref-
erence resolution and distinguishing specific extrac-
tions from generic ones in Section 3.1. Then, we
show the confidence boosting algorithm in detail in
Section 3.2.
3
3.1 Event extraction and processing
As mentioned in previous sections, events play a key
role in the propagation models. We define an event
as a Subject-Predicate-Object (SPO) triple. To ex-
tract events from raw text, an open information ex-
traction software - ReVerb (Fader et al, 2011) is
used. ReVerb is a program that automatically iden-
tifies and extracts relationships from English sen-
tences. It takes raw text as input and outputs SPO
triples which are called extractions.
However, extractions extracted by ReVerb cannot
be used directly for our propagation models for three
main reasons. First, the relative temporal relations
between documents and the extractions are unavail-
able. Second, the extractions extracted from differ-
ent documents do not have any connection even if
they refer to the same event. Third, propagations
from generic events are very likely to lead to propa-
gation errors.
For addressing the three challenges for the prop-
agation models, we first presented a rule-based
method for mining the relative temporal relations be-
tween extractions and documents in Section 3.1.1.
Then, an efficient event coreference resolution
method is introduced in Section 3.1.2. Finally, the
method for distinguishing specific extractions from
generic ones is shown in Section 3.1.3.
3.1.1 Relative temporal relation mining
We used a rule-based method to extract temporal
expressions and used Stanford parser (De Marneffe
et al, 2006) to analyze association between the tem-
poral expressions and the extractions. Specifically,
we define that an extraction is associated with a tem-
poral expression if there is an arc from the predicate
of the extraction to the temporal expression in the
dependency tree. For a certain extraction, there are
the following four cases whose instances are shown
in table 1 for handling.
Case 1: The extraction is associated with an abso-
lute temporal expressions with year mentions in the
sentence.
In this case, the time of the extraction is equal to
the year mention:
Y (ex) = Y earMention
For the example in table 1, Y (ex) = 1999.
Case 2: The extraction is associated with a relative
temporal expression (not involving year) in the sen-
Case Instance
1 In 1999, South Korea exported 89,000
tons of pork to Japan.
2
In April, however, the BOI investments
showed marked improvement.
Last month, Kazini vowed to resign his
top army job.
3 Julius Erving moved with his family to
Florida three years ago.
4 The meeting focused on ways to revive
the stalled Mideast peace process.
Table 1: Instances of various temporal expressions
tence.
In this case, the time of the extraction is equal to
the creation time of the document:
Y (ex) = Y (d)
Case 3: The extraction is associated with a relative
temporal expression (involving specific year gap) in
the sentence.
In this case, the time of the extraction is computed
as follows:
Y (ex) = Y (d)? Y earGap
For the example in table 1, Y (ex) = Y (d)? 3.
Case 4: The extraction is not associated with any
temporal expression in the sentence or the other
cases.
In this case, it is difficult to recognize the rela-
tive temporal relations. However, timeliness can be
leveraged to determine the relations as a heuristic
method. It is known that timeliness is an important
feature of news so that events reported by a news ar-
ticle usually took place a couple of days or weeks
before the article was written. Therefore, we heuris-
tically consider the year of the extraction is the same
with that of its source document in this case:
Y (ex) = Y (d)
In the cases except case 1, the relative tempo-
ral relation between an extraction and the docu-
ment it comes from can be determined. To evalu-
ate the performance of the rule-based method, we
sampled 3,000 extractions from documents written
in the year of 1995-1999 of Gigaword corpus and
manually labeled these extractions with a timestamp
based on their context and their corresponding docu-
ment timestamps as golden standard. Table 2 shows
4
the accuracy of each case which will be used as a
part of the decay factor in the confidence boosting
model.
Case Accuracy
1 0.774(168/217)
2 0.994(844/849)
3 0.836(281/336)
4 0.861(1376/1598)
Total 0.890(2669/3000)
Table 2: Accuracy of the four cases
We define the set of these determined relative tem-
poral relations R as follows:
R = {rd,ex|d = doc(ex), ex ? C2 ? C3 ? C4}
rd,ex =< d, ex, ?(d, ex) >
?(d, ex) = ??(ex, d) = {0,?1,?2,?3, ...}
where Ck is the set of extractions in case k and
doc(ex) is the document which extraction ex comes
from. rd,ex is a triple describing the relative tempo-
ral relation between d and ex. For example, triple
rd,ex =< d, ex,?1 > means that the time of ex-
traction ex is one year before the time of document
d.
3.1.2 Event coreference resolution
Extractions from different documents have no
connections. However, there are a great number of
extractions referring to the same event. For find-
ing such coreferential event extractions efficiently,
hierarchical agglomerative clustering (HAC) is used
to cluster highly similar extractions into one cluster.
We use cosine to measure the similarity between ex-
tractions and select bag of words as features. Note
that it is less meaningful to cluster the extractions
from the same document because coreferential ex-
tractions from the same document are not helpful for
timestamp propagations. For this reason, similarity
between extractions from the same documents is set
to 0.
For HAC, selection of threshold is important. If
the threshold is set too high, only a few extractions
can be clustered despite high purity; on the contrary,
if the threshold is set too low, purity of clusters will
descend. In fact, selection of threshold is a trade-off
between the precision and recall of event corefer-
ence resolution. For selecting a suitable threshold,
extractions from documents written in 1995-1999
are used as a development set.
In practice, it is difficult for us to directly evalu-
ate the performance of the coreference resolution of
event extractions without golden standard which re-
quires much labors for manual annotations. Alterna-
tively, entropy which measures the purity of clusters
is used for evaluation because it can indirectly re-
flect the precision of coreference resolution to some
extent:
Entropy = ?
?
j
nj
n
?
i
P (i, j)? log2 P (i, j)
where P (i, j) is the probability of finding an extrac-
tion whose timestamp is i in the cluster j, nj is the
number of items in cluster j and n is the total num-
ber of extractions. Note that timestamp of an extrac-
tion is assigned based on its document timestamp
using the method proposed in Section 3.1.1.
Figure 3 shows the effect of selection of the
threshold on cluster performance. It can be found
that when the threshold reaches 0.8, the entropy
starts descending gently and is low enough. Since
we want to find as many coreferential extractions as
possible on the premise that the precision is good,
the threshold is set to 0.8. Note that extractions
which are single in one cluster will be filtered out
because they do not have any connections with any
other documents.
0.6 0.65 0.7 0.75 0.8 0.85 0.90.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
Threshold
Ent
ropy
Figure 3: Entropy of clusters under different thresholds
3.1.3 Distinguishing specific events from
generic ones
Not all extractions extracted by ReVerb refer to
a specific event. For instance, the extraction ?Ger-
many?s DAX index was down 0.2 percent? is un-
desirable for our task because it refers to a generic
5
event and this event may occur in any year. In other
words, it is not able to indicate a certain timestamp
and thus propagations from a generic event node are
very likely to result in propagation errors. In con-
trast, the extraction ?some of the provinces in China
were hit by SARS? refers to a specific event which
took place in 2003. For our task, such specific event
extractions which are associated with one certain
timestamp are desirable. For the sake of distinguish-
ing such extractions from the generic ones, a Max-
Ent classifier is used to classify extractions as either
specific ones or generic ones.
Training Set Generation A training set is indis-
pensable for training a MaxEnt classifier. In order
to generate training examples, we performed HAC
discussed in Section 3.1.2 for event coreference res-
olution on extractions from all documents written
in May and June of 1995-1999 and then analyzed
each cluster. If extractions in a cluster have different
timestamps, then the extractions in this cluster will
be labeled as generic extractions (negative); other-
wise, extractions in the cluster are labeled as spe-
cific ones (positive). In this way, the training set can
be generated without manually labeling. To avoid
bias of positive and negative examples, we sampled
3,500 positive examples and 3,500 negative exam-
ples to train the model.
Feature Selection The following features were se-
lected for training:
Named Entities: People and places are often dis-
cussed during specific time periods, particularly in
news genre. Intuitively, if an extraction contains
specific named entities then this extraction is less
likely to be a generic event. If an extraction con-
tains named entities, types and uninterrupted tokens
of the named entities will be included as features.
Numeral: According to our analysis of the train-
ing set generated by the above-mentioned method,
generic extractions usually contain numerals. For
example, the extraction ?15 people died in this ac-
cident? and the extraction ?225 people died in this
accident? have the same tokens except numerals and
they are labeled as a generic event because they are
clustered into one group due to high similarity but
they in fact refer to different events happening in
different years. Therefore, if an extraction contains
numerals, the feature ?NUM? will be included.
Bag of words: Bag of words can also be an indicator
of specific extractions and generic ones. For exam-
ple, an extraction containing ?stock?, ?index?, ?fell?
and ?exchange? is probably a generic one.
The model obtained after training can be used to
predict whether an extraction is a specific one. We
define P (S = 1|ex) as the probability that an ex-
traction is a specific one, which can be provided by
the classifier. Extractions whose probability to be a
specific one is less than 0.05 are filtered out. For the
other extractions, this probability is used as a part of
the decay factor in the confidence boosting model,
which will be discussed in detail in Section 3.2.
3.2 Confidence boosting
After extracting and processing the event extrac-
tions, relative temporal relations between documents
and events can be constructed. This can be for-
mally represented by a bipartite graph G=?V,E?.
There are two kinds of nodes on the bipartite graph:
document nodes and event nodes. Slightly dif-
ferent with the event node mentioned in Section
2, an event node in practice is a cluster of coref-
erential extractions and it can be connected with
multiple document nodes. Note that the bipar-
tite graph does not contain any isolate node. For
briefness, we define DNode as the set of docu-
ment nodes and ENode as the set of event nodes.
The set of edges E is formally defined as follows:
E = {eij , eji|i ? DNode, j ? ENode, ri,j ? R}
where R is the set of relative temporal relations de-
fined as Section 3.1.1.
3.2.1 Confidence and decay factor
As mentioned in Section 2, the confidence of a
node measures the credibility of the predicted times-
tamp. According to the definition, we set the confi-
dence of initially labeled nodes to 1 and set confi-
dence of nodes without any timestamp to 0 in prac-
tice. When the timestamp of a node is propagated
to another node, its confidence will be propagated to
the target node with some loss, as discussed in Sec-
tion 2. The confidence loss is caused by two factors
in practice. The first one is the credibility of the rel-
ative temporal relation between two nodes and the
other one depends on whether an extraction refers to
a specific event.
Relative temporal relations between documents
6
and extractions we mined using the rule-based
method in Section 3.1.1 are not absolutely correct.
The credibility of the relations has an effect on the
confidence decay. Formally, we used pi(i, j) to de-
note the credibility of the relative temporal relation
between node i and node j. The credibility of a rel-
ative temporal relation in each case can be estimated
through table 2. If the credibility of the relative tem-
poral relation between i and j is low, propagation
from node i to j probably leads to error. Therefore,
the confidence loss should be much in this case. On
contrary, if the relation is highly credible, it will be
less likely that propagation errors occur. Therefore,
the confidence loss should be little.
In addition, whether an extraction refers to a
generic event or a specific one exerts an impact on
the confidence loss. If an extraction refers to a
generic event, then the extractions in the same clus-
ter with it probably have different timestamps. Since
our propagation model assumes that extractions in a
cluster are coreferent and thus they should have the
same timestamp, propagations from a generic event
node are very likely to result in propagation errors.
Therefore, the timestamp of a generic event node
in fact is less credible for propagations and confi-
dence of such event nodes should be low for limiting
propagations from the nodes. For this reason, prop-
agation from a document node to a generic event
node leads to much loss of confidence. We define
the probability that an event node refers to a specific
event as follows:
P (S = 1|enode) =
1
|C|
?
ex?C
P (S = 1|ex)
where C is the set of extractions in the event node
and P (S = 1|ex) is the probability that an extrac-
tion refers to a specific event, which can be provided
by the MaxEnt classifier discussed in Section 3.1.3.
Considering the two factors for confidence loss,
we formally define the decay factor by (1).
?(s, t) = (1)
{
pi(s, t) if t ? DNode
pi(s, t)? P (S = 1|t) otherwise
3.2.2 Confidence boosting algorithm
In confidence boosting model, the propagation
from i to j will occur only if the CB condition is
Figure 4: Algorithm of confidence boosting
satisfied. The confidence boosting propagation pro-
cess can be described as figure 4.
Whenever timestamps are propagated to other
nodes, the global confidence of the bipartite graph
will increase. For this reason, this propagation pro-
cess is called confidence boosting. In this model,
a node with high confidence is more active than
nodes with low confidence to propagate its times-
tamp. Moreover, a prediction with low confidence
can be corrected by the prediction with high con-
fidence. Therefore, the confidence boosting model
can alleviate the problem of propagation of errors
to some extent and handle conflict of predictions.
Thus, it can propagate timestamps more credibly
than the BFS-based model. It can also be proved
that each node on the bipartite graph must reach the
highest confidence it can reach so that the global
confidence of the bipartite graph must be optimal
when the confidence boosting propagation process
ends regardless of propagation orders, which will be
discussed in Section 3.2.3.
3.2.3 Proof of the optimality of confidence
boosting
Proof by contradiction can be used to prove that
propagation orders do not affect the optimality of the
confidence boosting model.
Proof Assume by contradiction that there is some
node that does not reach its highest confidence it can
reach when a confidence boosting process in propa-
gation order A ends:
?vt s.t. cA(vt) < c?(vt)
where cA(vt) is the confidence of vt when the
propagation process in order A ends and c?(vt) is
the highest confidence that vt can reach. Assume
that (v1, v2, ? ? ? , vt?1, vt) is the optimal propagation
7
path from the propagation source node v1 to the
node vt that leads to the highest confidence of vt,
which means that c?(vt) = c?(vt?1) ? ?(vt?1, vt),
c?(vt?1) = c?(vt?2) ? ?(vt?2, vt?1), ..., c?(v2) =
c?(v1) ? ?(v1, v2). Then according to CB condi-
tion, since cA(vt?1) ? ?(vt?1, vt) ? cA(vt) <
c?(vt) = c?(vt?1) ? ?(vt?1, vt), the inequality
cA(vt?1) < c?(vt?1) must hold. Similarly, it can be
easily inferred that cA(vt?2) < c?(vt?2) and finally
cA(v1) < c?(v1). Since v1 is the source node whose
timestamp is initially labeled and its confidence is 1,
the inequality cA(v1) < c?(v1) cannot hold. Thus,
the assumption that cA(vt) < c?(vt) cannot be sat-
isfied. Therefore, it can be proved that each node
on the bipartite graph must reach the highest con-
fidence it can reach so that the global confidence of
the bipartite graph must be optimal when confidence
boosting propagation process ends no matter what
order time labels are propagated in.
4 Experiments
In this section, we evaluate the performance of our
time label propagation models and different auto-
matic document dating models on the Gigaword
dataset. We first present the experimental setting.
Then we show experimental results and perform an
analysis.
4.1 Experimental Setting
Dataset To simulate the environment of the web
where data is very redundant, we use all documents
written in April, June, July and September of 2000-
2004 of Gigaword Corpus as dataset instead of sam-
pling a subset of documents from each period. The
dataset contains 900,199 news articles.
Pre-processing Many extractions extracted by Re-
Verb are short and uninformative and do not carry
any valuable information for propagating temporal
information. Also, some extractions do not refer
to events which already happened. These extrac-
tions may affect the performance of event corefer-
ence resolution and the rule-based method proposed
in Section 3.1.1 for mining relative temporal rela-
tions. Therefore, we filter out these undesirable ex-
tractions in advance with a rule-based method. The
rules are shown in table 3. This preprocessing re-
moves large numbers of ?bad? extractions which are
undesirable for our task. As a result, not only com-
putation efficiency but also precision of event coref-
erence resolution will be improved.
Rule1 If the number of tokens of the extrac-
tion is less than 5 then this extraction
will be filtered out.
Rule2 If the maximum idf of terms of the ex-
traction is less than 3.0 then this ex-
traction will be filtered out.
Rule3 If the tense of the extraction is not past
tense then this extraction will be fil-
tered out.
Rule4 If the extraction is the content of di-
rect quotation then this extraction will
be filtered out.
Table 3: Pre-processing Rules
|DNode| 550,124
|ENode| 968,064
|E| 3,104,666
Table 4: Basic information of the bi-partite graph
Basic information of the document-event bipartite
graph constructed is shown in table 4.
Evaluation To evaluate the performance of the
propagation models for the task of dating on differ-
ent sizes of the training set, we used different sizes
of the labeled documents for training and consid-
ered the remaining documents as the test set. Note
that the training set is randomly sampled from the
dataset. To be more persuasive, we repeated above
experiments for five times.
However, in the time label propagation process,
not all documents can be labeled. For those doc-
uments which cannot be labeled in the process of
propagation, a MaxEnt classifier serves as a comple-
mentary approach to predict their timestamps. For
the MaxEnt classifier, unigrams and named entities
are simply selected as features and the initially la-
beled documents as well as documents labeled dur-
ing propagation process are used for training.
Baseline methods are temporal language models
proposed by de Jong et al (2005) and the state-of-
the-art discriminative classifier with linguistic fea-
tures and temporal constraints which was proposed
8
Initially Labeled 1k 5k 10k 50k 100k 200k 500k
Reached Min 443980 448653 453022 484562 518603 599724 732701
Reached Max 444266 448998 454028 484996 519333 579878 732799
Reached Avg 444107 448742 453786 484622 519110 579835 732758
Prop Ratio 444.1 89.7 45.4 9.7 5.2 2.9 1.5
Prop acc(BFS) 0.438 0.515 0.551 0.646 0.691 0.725 0.775
Prop acc(CB) 0.494 0.569 0.603 0.701 0.746 0.776 0.807
Table 5: Performance of Propagation
Initially Labeled 1k 5k 10k 50k 100k 200k 500k
Temporal LMs 0.277 0.323 0.353 0.412 0.422 0.425 0.420
Maxent(Unigrams) 0.326 0.378 0.407 0.486 0.517 0.553 0.590
Maxent(Unigrams+NER) 0.331 0.383 0.418 0.506 0.549 0.590 0.665
Chambers?s 0.331 0.386 0.423 0.524 0.571 0.615 0.690
BFS+Maxent 0.459 0.508 0.533 0.595 0.626 0.658 0.707
CB+Maxent 0.486 0.535 0.559 0.624 0.655 0.685 0.726
Table 6: Overall accuracy of dating models
by Nathanael Chambers (2012). In Chambers?s joint
model, the interpolation parameter ? is set to 0.35
which is considered optimal in his work.
4.2 Experimental Results
Table 5 shows the performance of propagation mod-
els where Reached denotes the number of docu-
ments labeled when the propagation process ends,
prop ratio and prop accuracy are defined as follows:
Prop Ratio =
#ReachedDocNodes
#LabeledDocNodes
Prop Accuracy =
#CorrectDocNodes?#LabeledDocNodes
#ReachedDocNodes?#LabeledDocNodes
where #LabeledDocNodes is the number of ini-
tially labeled document nodes which are documents
in the training set and #ReachedDocNodes is the
number of document nodes labeled when the propa-
gation process ends.
Note that prop ratio and accuracy in table 5 are
the mean of the prop ratio and accuracy of the five
groups of experiments. It is clear that confidence
boosting model improves the prop accuracy over
BFS-based model. When only 1,000 documents
are initially labeled with timestamps, the confidence
boosting model can propagate their timestamps to
more than 400,000 documents with an accuracy of
0.494, approximately 12.8% relative improvement
over the BFS counterpart, which proves effective-
ness of the confidence boosting model.
However, as shown in table 5, hardly can the prop-
agation process propagate timestamps to all doc-
uments. One reason is that the number of docu-
ment nodes on the bipartite graph is only 550,124,
approximately 61.1% of all documents. The other
documents may not mention events which are also
mentioned by other documents, which means they
are isolate and thus are excluded from the bipartite
graph. Also, the event coreference resolution phase
does not guarantee finding all coreferential extrac-
tions; in other words, recall of event coreference res-
olution is not 100%. The other reason is that some
documents are unreachable from the initially labeled
nodes even if they are in the bipartite graph.
The overall accuracy of different dating models
is shown in table 6. As with table 5, overall accu-
racy in table 6 is the average performance of mod-
els in the five groups of experiments. As reported
by Nathanael Chambers (2012), the discriminative
classifier performs much better than the temporal
language models on the Gigaword dataset. In the
case of 500,000 training examples, the Maxent clas-
sifier using unigram features outperforms the tem-
poral language models by 40.5% relative accuracy.
If the size of the training set is large enough, named
9
entities and linguistic features as well as temporal
constraints will improve the overall accuracy sig-
nificantly. However, if the size of the training set
is small, these features will not result in much im-
provement.
Compared with the previous models, the propaga-
tion models predict the document timestamps much
more accurately especially in the case where the size
of the training set is small. When the size of the
training set is 1,000, our BFS-based model and con-
fidence boosting model combined with the MaxEnt
classifier outperform Chambers?s joint model which
is considered the state-of-the-art model for the task
of automatic dating of documents by 38.7% and
46.8% relative accuracy respectively. This is be-
cause the feature-based methods are not very reli-
able especially when the size of the training set is
small. In contrast, our propagation models can pre-
dict timestamps of documents with an understand-
ing of document content, which allows our method
to date documents more credibly than the baseline
methods. Also, by comparing table 5 with table 6,
it can be found that prop accuracy is almost always
higher than overall accuracy, which also verifies that
the propagation models are more credible for dat-
ing document than the feature-based models. More-
over, data is so redundant that a great number of
documents can be connected with events they share.
Therefore, even if a small number of documents are
labeled, the labeled information can be propagated
to large numbers of articles through the connections
between documents and events according to relative
time relations. Even if the size of the training set
is large, e.g. 500,000, our propagation models still
outperform the state-of-the-art dating method. Ad-
ditionally, some event nodes on the bipartite graph
may be labeled with a timestamp during the process
of propagation as a byproduct. The temporal infor-
mation of the events would be useful for other tem-
poral analysis tasks.
5 Related Work
In addition to work of de Jong et al (2005) and
Chambers (2012) introduced in previous sections,
there is also other research focusing on the task of
document dating. Kanhabua and Norvag (2009) im-
proved temporal language models by incorporating
temporal entropy and search statistics and apply-
ing two filtering techniques to the unigrams in the
model. Kumar et al (2011) is also based on the
temporal language models, but more historically-
oriented, which models the timeline from the present
day back to the 18th century. In addition, they used
KL-divergence instead of normalized log likelihood
ratio to measure differences between a document
and a time period?s language model.
However, these methods are based on tempo-
ral language models so they also suffer from the
problem of the method of de Jong et al (2005).
Therefore, they inevitably make wrong predictions
in some cases, just as mentioned in Section 1. Com-
pared with these methods, our event-based propaga-
tion models exploit relative temporal relations be-
tween documents and events for dating document
on a basis of an understanding of document content,
which is more reasonable and also proved to be more
effective by the experimental results.
6 Conclusion
The main contribution of this paper is exploiting
relative temporal relations between events and doc-
uments for the document dating task. Different
with the conventional work which dates documents
with feature-based methods, we proposed an event-
based time label propagation model called confi-
dence boosting in which timestamps are propagated
on a document-event bipartite graph according to
relative temporal relations between documents and
events for dating documents on a basis of an under-
standing of document content. We discussed chal-
lenges for the propagation models and gave the cor-
responding solutions in detail. The experimental re-
sults show that our event-based propagation model
can predict document timestamps in high accuracy
and the model combined with a MaxEnt classifier
outperforms the state-of-the-art method on a data-
redundant dataset.
Acknowledgements
We thank the anonymous reviewers for their valu-
able suggestions. This paper is supported by
NSFC Project 61075067, NSFC Project 61273318
and National Key Technology R&D Program (No:
2011BAH10B04-03).
10
References
Klaus Berberich, Srikanta Bedathur, Thomas Neumann,
and Gerhard Weikum. 2007. A time machine for
text search. In Proceedings of the 30th annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 519?526.
ACM.
Giang Binh Tran, Mohammad Alrifai, and Dat
Quoc Nguyen. 2013. Predicting relevant news events
for timeline summaries. In Proceedings of the 22nd
international conference on World Wide Web compan-
ion, pages 91?92. International World Wide Web Con-
ferences Steering Committee.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
698?706. Association for Computational Linguistics.
Nathanael Chambers. 2012. Labeling documents with
timestamps: Learning from their time expressions. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers-
Volume 1, pages 98?106. Association for Computa-
tional Linguistics.
FMG de Jong, Henning Rode, and Djoerd Hiemstra.
2005. Temporal language models for the disclosure
of historical text. Royal Netherlands Academy of Arts
and Sciences.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449?454.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1535?1545. Association for Computational Linguis-
tics.
David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.
2003. English gigaword. Linguistic Data Consortium,
Philadelphia.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic ret-
rospection. In Data Mining (ICDM), 2011 IEEE 11th
International Conference on, pages 260?269. IEEE.
Pawel Jan Kalczynski and Amy Chou. 2005. Temporal
document retrieval model for business news archives.
Information processing management, 41(3):635?650.
Nattiya Kanhabua and Kjetil N?rva?g. 2009. Us-
ing temporal language models for document dating.
In Machine Learning and Knowledge Discovery in
Databases, pages 738?741. Springer.
Abhimanu Kumar, Matthew Lease, and Jason Baldridge.
2011. Supervised language modeling for temporal res-
olution of texts. In Proceedings of the 20th ACM in-
ternational conference on Information and knowledge
management, pages 2069?2072. ACM.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with markov logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language: Volume 1-Volume 1, pages 405?
413. Association for Computational Linguistics.
11
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 810?815,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Towards Accurate Distant Supervision for Relational Facts Extraction
Xingxing Zhang1 Jianwen Zhang2? Junyu Zeng3 Jun Yan2 Zheng Chen2 Zhifang Sui1
1Key Laboratory of Computational Linguistics (Peking University), Ministry of Education,China
2Microsoft Research Asia
3Beijing University of Posts and Telecommunications
1{zhangxingxing,szf}@pku.edu.cn
2{jiazhan,junyan,zhengc}@microsoft.com
3junyu.zeng@gmail.com
Abstract
Distant supervision (DS) is an appealing
learning method which learns from exist-
ing relational facts to extract more from
a text corpus. However, the accuracy is
still not satisfying. In this paper, we point
out and analyze some critical factors in
DS which have great impact on accuracy,
including valid entity type detection,
negative training examples construction
and ensembles. We propose an approach
to handle these factors. By experimenting
on Wikipedia articles to extract the facts in
Freebase (the top 92 relations), we show
the impact of these three factors on the
accuracy of DS and the remarkable im-
provement led by the proposed approach.
1 Introduction
Recently there are great efforts on building large
structural knowledge bases (KB) such as Free-
base, Yago, etc. They are composed of relational
facts often represented in the form of a triplet,
(SrcEntity, Relation, DstEntity),
such as ?(Bill Gates, BornIn, Seattle)?. An impor-
tant task is to enrich such KBs by extracting more
facts from text. Specifically, this paper focuses on
extracting facts for existing relations. This is dif-
ferent from OpenIE (Banko et al, 2007; Carlson et
al., 2010) which needs to discover new relations.
Given large amounts of labeled sentences,
supervised methods are able to achieve good
performance (Zhao and Grishman, 2005; Bunescu
and Mooney, 2005). However, it is difficult to
handle large scale corpus due to the high cost
of labeling. Recently an approach called distant
supervision (DS) (Mintz et al, 2009) was pro-
posed, which does not require any labels on the
text. It treats the extraction problem as classifying
? The contact author.
a candidate entity pair to a relation. Then an
existing fact in a KB can be used as a labeled
example whose label is the relation name. Then
the features of all the sentences (from a given text
corpus) containing the entity pair are merged as
the feature of the example. Finally a multi-class
classifier is trained.
However, the accuracy of DS is not satisfying.
Some variants have been proposed to improve
the performance (Riedel et al, 2010; Hoffmann
et al, 2011; Takamatsu et al, 2012). They ar-
gue that DS introduces a lot of noise into the
training data by merging the features of all the
sentences containing the same entity pair, because
a sentence containing the entity pair of a relation
may not talk about the relation. Riedel et al
(2010) and Hoffmann et al (2011) introduce
hidden variables to indicate whether a sentence
is noise and try to infer them from the data.
Takamatsu et al (2012) design a generative model
to identify noise patterns. However, as shown in
the experiments (Section 4), the above variants do
not lead to much improvement in accuracy.
In this paper, we point out and analyze some
critical factors in DS which have great impact on
the accuracy but has not been touched or well han-
dled before. First, each relation has its own schema
definition, i.e., the source entity and the destina-
tion entity should be of valid types, which is over-
looked in DS. Therefore, we propose a component
of entity type detection to check it. Second, DS
introduces many false negative examples into the
training set and we propose a new method to con-
struct negative training examples. Third, we find it
is difficult for a single classifier to achieve high ac-
curacy and hence we train multiple classifiers and
ensemble them.
We also notice that Nguyen and Moschitti
(2011a) and Nguyen and Moschitti (2011b) utilize
external information such as more facts from Yago
and labeled sentences from ACE to improve the
810
performance. These methods can also be equipped
with the approach proposed in this paper.
2 Critical Factors Affecting the Accuracy
DS has four steps: (1) Detect candidate entity
pairs in the corpus. (2) Label the candidate pairs
using the KB. (3) Extract features for the pair
from sentences containing the pair. (4) Train a
multi-class classifier. Among these steps, we find
the following three critical factors have great
impact on the accuracy (see Section 4 for the
experimental results).
Valid entity type detection. In DS, a sentence
with a candidate entity pair a sentence with two
candidate entities is noisy. First, the schema of
each relation in the KB requires that the source
and destination entities should be of valid types,
e.g., the source and destination entity of the
relation ?DirectorOfFilm? should be of the types
?Director? and ?Film? respectively. If the two
entities in a sentence are not of the valid types, the
sentence is noisy. Second, the sentence may not
talk about the relation even when the two entities
are of the valid types. The previous works (Riedel
et al, 2010; Hoffmann et al, 2011; Takamatsu et
al., 2012) do not distinguish the two types of noise
but directly infer the overall noise from the data.
We argue that the first type of noise is very difficult
to be inferred just from the noisy relational labels.
Instead, we decouple the two types of noise, and
utilize external labeled data, i.e., the Wikipedia
anchor links, to train an entity type detection mod-
ule to handle the first type of noise. We notice that
when Ling and Weld (2012) studied a fine-grained
NER method, they applied the method to relation
extraction by adding the recognized entity tags to
the features. We worry that the contribution of the
entity type features may be drowned when many
other features are used. Their method works well
on relatively small relations, but not that well on
big ones (Section 4.2).
Negative examples construction. DS treats the
relation extraction as a multi-class classification
task. For a relation, it implies that the facts of all
the other relations together with the ?Other? class
are negative examples. This introduces many false
negative examples into the training data. First,
many relations are not exclusive with each other,
e.g., ?PlaceOfBorn? and ?PlaceOfDeath?, the
born place of a person can be also the death place.
Second, in DS, the ?Other? class is composed
of all the candidate entity pairs not existed in
the KB, which actually contains many positive
facts of non-Other relations because the KB is
not complete. Therefore we use a different way to
construct negative training examples.
Feature space partition and ensemble. The
features used in DS are very sparse and many
examples do not contain any features. Thus we
employ more features. However we find it is
difficult for a single classifier on all the features
to achieve high accuracy and hence we divide
the features into different categories and train
a separate classifier for each category and then
ensemble them finally.
3 Accurate Distant Supervision (ADS)
Different from DS, we treat the extraction
problem as N binary classification problems,
one for each relation. We modify the four steps
of DS (Section 2). In step (1), when detecting
candidate entity pairs in sentences, we use our
entity type detection module (Section 3.1) to filter
out the sentences where the entity pair is of invalid
entity types. In step (2), we use our new method
to construct negative examples (Section 3.2). In
step (3), we employ more features and design an
ensemble classifier (Section 3.3). In step (4), we
train N binary classifiers separately.
3.1 Entity Type Detection
We divide the entity type detection into two steps.
The first step, called boundary detection, is to
detect phrases as candidate entities. The second
step, called named entity disambiguation, maps
a detected candidate entity to some entity types,
e.g., ?FilmDirector?. Note that an entity might be
mapped to multiple types. For instance, ?Ventura
Pons? is a ?FilmDirector? and a ?Person?.
Boundary Detection Two ways are used for
boundary detection. First, for each relation, from
the training set of facts, we get two dictionaries
(one for source entities and one for destination en-
tities). The two dictionaries are used to detect the
source and destination entities. Second, an exist-
ing NER tool (StanfordNER here) is used with the
following postprocessing to filter some unwanted
entities, because a NER tool sometimes produces
too many entities. We first find the compatible N-
ER tags for an entity type in the KB. For example,
811
for the type ?FilmDirector?, the compatible NER
tag of Standford NER is ?Person?. To do this,
for each entity type in the KB, we match all the
entities of that type (in the training set) back to the
training corpus and get the probability Ptag(ti) of
each NER tag (including the ?NULL? tag meaning
not recognized as a named entity) recognized
by the NER tool. Then we retain the top k tags
Stags = {t1, ? ? ? , tk} with the highest probabil-
ities to account for an accumulated mass z:
k = argmin
k
(( k?
i=1
Ptag(ti)
)
? z
)
(1)
In the experiments we set z = 0.9. The compati-
ble ner tags are Stags\{?NULL?}. If the retained
tags contain only ?NULL?, the candidate entities
recognized by NER tool will be discarded.
Named Entity Disambiguation (NED) With
a candidate entity obtained by the boundary
detection, we need a NED component to assign
some entity types to it. To obtain such a NED, we
leverage the anchor text in Wikipedia to generate
training data and train a NED component. The
referred Freebase entity and the types of an anchor
link in Wikipedia can be obtained from Freebase.
The following features are used to train the
NED component. Mention Features: Uni-grams,
Bi-grams, POS tags, word shapes in the mention,
and the length of the mention. Context Features:
Uni-grams and Bi-grams in the windows of the
mention (window size = 5).
3.2 Negative Examples Construction
Treating the problem as a multi-class classification
implies introducing many false negative examples
for a relation; therefore, we handle each relation
with a separate binary classifier. However, a KB
only tells us which entity pairs belong to a relation,
i.e., it only provides positive examples for each re-
lation. But we also need negative examples to train
a binary classifier. To reduce the number of false
negative examples, we propose a new method
to construct negative examples by utilizing the
1-to-1/1-to-n/n-to-1/n-to-n property of a relation.
1-to-1/n-to-1/1-to-n Relation A 1-to-1 or n-to-
1 relation is a functional relation: for a relation r,
for each valid source entity e1, there is only one
unique destination entity e2 such that (e1, e2) ? r.
However, in a real KB like Freebase, very few
relations meet the exact criterion. Thus we use the
following approximate criterion instead: relation
r is approximately a 1-to-1/n-to-1 relation if the
Inequalities (2,3) hold, where M is the number of
unique source entities in relation r, and ?(?) is an
indicator function which returns 1 if the condition
is met and returns 0 otherwise. Inequality (2)
says the proportion of source entities which have
exactly one counterpart destination entity should
be greater than a given threshold. Inequality (3)
says the average number of destination entities of
a source entity should be less than the threshold.
To check whether r is a 1-to-n relation, we simply
swap the source and destination entities of the
relation and check whether the reversed relation
is a n-to-1 relation by the above two inequalities.
In experiments we set ? = 0.7 and ? = 1.1.
1
M
M?
i=1
?
(??{e?|(ei, e?) ? r}
?? = 1
)
? ? (2)
1
M
M?
i=1
??{e?|(ei, e?) ? r}
?? ? ? (3)
n-to-n Relation Relations other than 1-to-1/n-
to-1/1-to-n are n-to-n relations. We approximately
categorize a n-to-n relation to n-to-1 or 1-to-n by
checking which one it is closer to. This is done
by computing the following two values ?src and
?dst. r is treated as a 1-to-n relation if ?src > ?dst
and as a 1-to-n relation otherwise.
?src =
1
Msrc
Msrc?
i=1
??{e?|(ei, e?) ? r}
??
?dst =
1
Mdst
Mdst?
i=1
??{e?|(e?, ei) ? r}
??
(4)
Negative examples For a candidate entity pair
(e1, e2) not in the relation r of the KB, we first
determine whether it is 1-to-n or n-to-1 using the
above method. If r is 1-to-1/n-to-1 and e1 exists in
some fact of r as the source entity, then (e1, e2) is
a negative example as it violates the 1-to-1/n-to-1
constraint. If r is 1-to-n, the judgement is similar
and just simply swap the source and destination
entities of the relation.
3.3 Feature Space Partition and Ensemble
The features of DS (Mintz et al, 2009) are very
sparse in the corpus. We add some features in (Yao
et al, 2011): Trigger Words (the words on the
dependency path except stop words) and Entity
String (source entity and destination entity).
812
Relation Taka Ensemble
works written 0.76 0.98
river/basin countries 0.48 1
/film/director/film 0.82 1
Average 0.79 0.89
Table 1: Manual evaluation of top-ranked 50 rela-
tion instances for the most frequent 15 relations.
We find that without considering the reversed
order of entity pairs in a sentence, the precision
can be higher, but the recall decreases. For exam-
ple, for the entity pair ?Ventura Pons, Actrius?, we
only consider sentences with the right order (e.g.
Ventura Pons is directed by Actrius.). For each re-
lation, we train four classifiers: C1 (without con-
sidering reversed order), C2 (considering reversed
order), C1more (without considering reversed or-
der and employ more feature) and C2more (con-
sidering reversed order and employ more feature).
We then ensemble the four classifiers by averaging
the probabilities of predictions:
P (y|x) = P1 + P2 + P1more + P2more4 (5)
4 Experiments
4.1 Dataset and Configurations
We aimed to extract facts of the 92 most frequent
relations in Freebase 2009. The facts of each
relation were equally split to two parts for training
and testing. Wikipedia 2009 was used as the target
corpus, where 800,000 articles were used for
training and 400,000 for testing. During the NED
phrase, there are 94 unique entity types (they are
also relations in Freebase) for the source and desti-
nation entities. Note that some entity types contain
too few entities and they are discarded. We used
500,000 Wikipedia articles (2,000,000 sentences)
for generating training data for the NED compo-
nent. We used Open NLP POS tagger, Standford
NER (Finkel et al, 2005) and MaltParser (Nivre
et al, 2006) to label/tag sentences. We employed
liblinear (Fan et al, 2008) as classifiers for NED
and relation extraction and the solver is L2LR.
4.2 Performance of Relation Extraction
Held-out Evaluation. We evaluate the perfor-
mance on the half hold-on facts for testing. We
compared performance of the n = 50, 000 best ex-
tracted relation instances of each method and the
Precision-Recall (PR) curves are in Figure 1 and
0 0.1 0.2 0.3 0.4 0.5 0.6 0.70
0.2
0.4
0.6
0.8
1
Recall
Pre
cis
ion
 
 
OrigDS
MultiR
Taka
ADS
Figure 1: Performance of different methods.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.25
0.4
0.6
0.8
1
Recall
Pre
cisi
on
 
 OrigDSDS_FigerETDETD+NegMoreEnsemble(ADS)
Figure 2: Contributions of different components.
Figure 2. For a candidate fact without any enti-
ty existing in Freebase, we are not able to judge
whether it is correct. Thus we only evaluate the
candidate facts that at least one entity occurs as
the source or destination entity in the test fact set.
In Figure 1, we compared our method with
two previous methods: MultiR (Hoffmann et al,
2011) and Takamatsu et al (2012) (Taka). For
MultiR, we used the author?s implementation1.
We re-implemented Takamatsu?s algorithm. As
Takamatsu?s dataset (903,000 Wikipedia articles
for training and 400,000 for testing) is very similar
to ours, we used their best reported parameters.
Our method leads to much better performance.
Manual Evaluation. Following (Takamatsu et
al., 2012), we selected the top 50 ranked (accord-
ing to their classification probabilities) relation
facts of the 15 largest relations. We compared our
results with those of Takamatsu et al (2012) and
we achieved greater average precision (Table 1).
1available at http://www.cs.washington.edu/ai/raphaelh/mr
We set T = 120, which leads to the best performance.
813
Pmicro Rmicro Pmacro Rmacro
0.950 0.845 0.947 0.626
Table 2: Performance of the NED component
4.3 Contribution of Each Component
In Figure 2, with the entity type detection (ETD),
the performance is better than the original DS
method (OrigDS). As for the performance of NED
in the Entity Type Detection, the Micro/Macro
Precision-Recall of our NED component are in
Table 2. ETD is also better than adding the entity
types of the pair to the feature vector (DS Figer)2
as in (Ling and Weld, 2012). If we also employ the
negative example construction strategy in Section
3.2 (ETD+Neg), the precision of the top ranked
instances is improved. By adding more features
(More) and employing the ensemble learning
(Ensemble(ADS)) to ETD+Neg, the performance
is further improved.
5 Conclusion
This paper dealt with the problem of improving the
accuracy of DS. We find some factors are crucial-
ly important, including valid entity type detection,
negative training examples construction and en-
sembles. We have proposed an approach to handle
these issues. Experiments show that the approach
is very effective.
References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Pro-
ceedings of the 20th international joint conference
on Artifical intelligence, IJCAI?07, pages 2670?
2676, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Razvan Bunescu and Raymond Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 724?
731, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R Hruschka Jr, and Tom M
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010), volume 2, pages 3?3.
2We use Figer (Ling and Weld, 2012) to detect entity types
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05). Association for Computational Linguis-
tics.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
X. Ling and D.S. Weld. 2012. Fine-grained entity
recognition. In Proceedings of the 26th Conference
on Artificial Intelligence (AAAI).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Truc-Vien T. Nguyen and Alessandro Moschitti.
2011a. End-to-end relation extraction using distant
supervision from external semantic repositories. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 277?282, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Truc-Vien T Nguyen and AlessandroMoschitti. 2011b.
Joint distant and direct supervision for relation ex-
traction. In Proceeding of the International Joint
Conference on Natural Language Processing, pages
732?740.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In In Proc. of LREC-2006, pages
2216?2219.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of the Sixteenth Eu-
ropean Conference on Machine Learning (ECML-
2010), pages 148?163.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
814
Annual Meeting of the Association for Computation-
al Linguistics (Volume 1: Long Papers), pages 721?
729, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456?1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistic-
s (ACL?05), pages 419?426, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
815
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 243?247
Manchester, August 2008
The Integration of Dependency Relation Classification and Semantic Role
Labeling Using Bilayer Maximum Entropy Markov Models
Weiwei Sun and Hongzhan Li and Zhifang Sui
Institute of Computational Linguistics
Peking University
{weiwsun, lihongzhan.pku}@gmail.com, szf@pku.edu.cn
Abstract
This paper describes a system to solve
the joint learning of syntactic and seman-
tic dependencies. An directed graphical
model is put forward to integrate depen-
dency relation classification and semantic
role labeling. We present a bilayer di-
rected graph to express probabilistic re-
lationships between syntactic and seman-
tic relations. Maximum Entropy Markov
Models are implemented to estimate con-
ditional probability distribution and to do
inference. The submitted model yields
76.28% macro-average F1 performance,
for the joint task, 85.75% syntactic depen-
dencies LAS and 66.61% semantic depen-
dencies F1.
1 Introduction
Dependency parsing and semantic role labeling are
becoming important components in many kinds of
NLP applications. Given a sentence, the task of de-
pendency parsing is to identify the syntactic head
of each word in the sentence and classify the rela-
tion between the dependent and its head; the task
of semantic role labeling consists of analyzing the
propositions expressed by some target predicates.
The integration of syntactic and semantic parsing
interests many researchers and some approaches
has been proposed (Yi and Palmer, 2005; Ge and
Mooney, 2005). CoNLL 2008 shared task pro-
poses the merging of both syntactic dependencies
and semantic dependencies under a unique unified
representation (Surdeanu et al, 2008). We explore
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the integration problem and evaluate our approach
using data provided on CoNLL 2008.
This paper explores the integration of depen-
dency relation classification and semantic role la-
beling, using a directed graphical model that is also
known as Bayesian Networks. The directed graph
of our system can be seen as one chain of obser-
vations with two label layers: the observations are
argument candidates; one layer?s label set is syn-
tactic dependency relations; the other?s is semantic
dependency relations. To estimate the probability
distribution of each arc and do inference, we im-
plement a Maximum Entropy Markov Model (Mc-
Callum et al, 2000). Specially, a logistic regres-
sion model is used to get the conditional probabil-
ity of each arc; dynamic programming algorithm
is applied to solve the ?argmax? problem.
2 System Description
Our DP-SRL system consists of 5 stages:
1. dependency parsing;
2. predicate prediction;
3. syntactic dependency relation classification
and semantic dependency relation identifica-
tion;
4. semantic dependency relation classification;
5. semantic dependency relation inference.
2.1 Dependency Parsing
In dependency parsing stage, MSTParser
1
(Mc-
Donald et al, 2005), a dependency parser that
searches for maximum spanning trees over di-
rected graphs, is used. we use MSTParser?s default
1
http://www.seas.upenn.edu/ strctlrn/MSTParser/MSTParser.html
243
Lemma and its POS tag
Number of children
Sequential POS tags of children
Lemma and POS of Neighboring words
Lemma and POS of parent
Is the word in word list of NomBank
Is the word in word list of PropBank
Is POS of the word is VB* or NN*
Table 1: Features used to predict target predicates
parameters to train a parsing model. In the third
stage of our system, dependency relations between
argument candidates and target predicates are up-
dated, if there are dependency between the candi-
dates and the predicates.
2.2 Predicate Prediction
Different from CoNLL-2005 shared task, the tar-
get predicates are not given as input. Our system
formulates the predicate predication problem as a
two-class classification problem using maximum
entropy classifier MaxEnt
2
(Berger et al, 1996).
Table 1 lists features used. We use a empirical
threshold to filter words: if the ?being target? prob-
ability of a word is greater than 0.075, it is seen as
a target predicate. This strategy achieves a 79.96%
precision and a 98.62% recall.
2.3 Syntactic Dependency Relation
Classification and Semantic Dependency
Relation Identification
We integrate dependency parsing and semantic
role labeling to some extent in this stage. Some de-
pendency parsing systems prefer two-stage archi-
tecture: unlabeled parsing and dependency clas-
sification (Nivre et al, 2007). Previous semantic
role labeling approaches also prefer two-stage ar-
chitecture: argument identification and argument
classification. Our system does syntactic relations
classification and semantic relations identification
at the same time. Specially, using a pruning al-
gorithm, we collect a set of argument candidates;
then we classify dependency relations between ar-
gument candidates and the predicates and predict
whether a candidate is an argument. A directed
graphical model is used to represent the relations
between syntactic and semantic relations.
2
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.h
tml
Lemma, POS tag voice of predicates
POS pattern of predicate?s children
Is the predicate from NomBank or PropBank
Predicate class. This information is extracted
form frame file of each predicate.
Position: whether the candidate is before or
after the predicate
Lemma and POS tag of the candidate
Lemma and POS of Neighboring words of the
candidate
Lemma and POS of sibling words of the
candidate
Length of the constituent headed by the
candidate
Lemma and POS of the left and right most
words of the constituent of the candidate
Punctuation before and after the candidate
POS path: the chain of POS from candidate to
predicate
Single Character POS path: each POS in a path
is clustered to a category defined by its
first character
POS Pattern (string of POS tags) of all
candidates
Single Character POS Pattern of all candidates
Table 2: Features used for semantic role labeling
2.4 Semantic Dependency Relation
Classification
This stage assigns the final argument labels to the
argument candidates supplied from the previous
stage. A multi-class classifier is trained to classify
the types of the arguments supplied by the previous
stage. Table 2 lists the features used. It is clear that
the general type of features used here is strongly
based on previous work on the SRL task (Gildea
and Jurafsky, 2002; Pradhan et al, 2005; Xue and
Palmer, 2004). Different from CoNLL-2005, the
sense of predicates should be labeled as a part of
the task. Our system assigns 01 to all predicates.
This is a harsh tactic since it do not take the lin-
guistic meaning of the argument-structure into ac-
count.
2.5 Semantic Dependency Relation Inference
The purpose of inference stage is to incorporate
some prior linguistic and structural knowledge,
such as ?each predicate takes at most one argument
of each type.? We use the inference process intro-
244
duced by (Punyakanok et al, 2004; Koomen et al,
2005). The process is modeled as an integer Lin-
ear Programming Problem (ILP). It takes the pre-
dicted probability over each type of the arguments
as inputs, and takes the optimal solution that max-
imizes the linear sum of the probability subject to
linguistic constraints as outputs. The constraints
are a subset of constraints raised by Koomen et al
(2005) and encoded as following: 1) No overlap-
ping or embedding arguments; 2) No duplicate ar-
gument classes for A0-A5; 3) If there is an R-arg
argument, then there has to be an arg argument;
4) If there is a C-arg argument, there must be an
arg argument; moreover, the C-arg argument must
occur after arg; 5) Given the predicate, some argu-
ment types are illegal. The list of illegal argument
types is extracted from framefile.
The ILP process can improve SRL performance
on constituent-based parsing (Punyakanok et al,
2004). In our experiment, it also works on
dependency-based parsing.
3 Bilayer Maximum Entropy Markov
Models
3.1 Sequentialization
The sequentialization of a argument-structure is si-
miliar to the pruning algorithm raised by (Xue and
Palmer, 2004). Given a constituent-based parsing
tree, the recursive pruning process starts from a tar-
get predicate. It first collects the siblings of the
predicate; then it moves to the parent of the pred-
icate, and collects the siblings of the parent. In
addition, if a constituent is a prepositional phrase,
its children are also collected.
Our system uses a similar pruning algorithm to
filter out very unlikely argument candidates in a
dependency-based parsing tree. Given a depen-
dency parsing tree, the pruning process also starts
from a target predicate. It first collects the depen-
dents of the predicate; then it moves to the parent
of the predicate, and collects all the dependents
again. Note that, the predicate is also taken into
account. If the target predicate is a verb, the pro-
cess goes on recursively until it reaches the root.
The process of a noun target ends when it sees a
PMOD, NMOD, SBJ or OBJ dependency relation.
If a preposition is returned as a candidate, its child
is also collected. When the predicate is a verb, the
set of constituents headed by survivors of our prun-
ing algorithm is a superset of the set of survivors of
the previous pruning algorithm on the correspond-
Figure 1: Directed graphical Model of The system
ing constituent-based parsing tree. This pruning
algorithm will recall 99.08% arguments of verbs,
and the candidates are 3.75 times of the real argu-
ments. If the stop relation such as PMOD of a noun
is not taken into account, the recall is 97.67% and
the candidates is 6.28 times of arguments. If the
harsh stop condition is implemented, the recall is
just 80.29%. Since the SRL performance of nouns
is very low, the harsh pruning algorithm works bet-
ter than the original one.
After pruning, our system sequentializes all ar-
gument candidates of the target predicate accord-
ing to their linear order in the given sentence.
3.2 Graphical Model
Figure 1 is the directed graph of our system.
There is a chain of candidates x = (x
0
=
BOS, x
1
, ..., x
n
) in the graph which are observa-
tions. There are two tag layers in the graph: the up
layer is information of semantic dependency rela-
tions; the down layer is information of syntactic
dependency relations.
Given x, denote the corresponding syntactic de-
pendency relations d = (d
0
= BOS, d
1
, ..., d
n
)
and the corresponding semantic dependency rela-
tions s = (s
0
= BOS, s
1
, ..., s
n
). Our system
labels the syntactic and semantic relations accord-
ing to the conditional probability in argmax fla-
vor. Formally, labels the system assigned make
the score p(d, s|x) reaches its maximum. We de-
compose the probability p(d, s|x) according to the
directed graph modeled as following:
p(d, s|x) = p(s
1
|s
0
, d
1
;x)p(d
1
|s
0
, d
0
;x) ? ? ?
p(s
i+1
|s
i
, d
i+1
;x)p(d
i+1
|s
i
, d
i
;x) ? ? ?
p(s
n
|s
n?1
, d
n
;x)p(d
n
|s
n?1
, d
n?1
;x)
=
n
?
i=1
p(s
i
|s
i?1
, d
i
;x)p(d
i
|s
i?1
, d
i?1
;x)
245
Lemma, POS tag voice of predicates
POS pattern of predicate?s children
Lemma and POS tag of the candidate
Lemma and POS of Neighboring words of the
candidate
Lemma and POS of sibling words of the
candidate
Length of the constituent headed by the
candidate
Lemma and POS of the left and right most
words of the constituent of the candidate
Conjunction of lemma of candidates and
predicates; Conjunction of POS of candidates
and predicates
POS Pattern of all candidates
Table 3: Features used to predict syntactic depen-
dency parsing
3.3 Probability Estimation
The system defines the conditional probability
p(s
i
|s
i?1
, d
i
;x) and p(d
i
|s
i?1
, d
i?1
;x) by using
the maximum entropy (Berger et al, 1996) frame-
work Denote the tag set of syntactic dependency
relations D and the tag set of semantic dependency
relations S. Formally, given a feature map ?
s
and
a weight vector w
s
,
p
w
s
(s
i
|s
i?1
, d
i
;x) =
exp{w
s
? ?
s
(x, s
i
, s
i?1
, d
i
)}
Z
x,s
i?1
,d
i
;w
s
where,
Z
x,s
i?1
,d
i
;w
s
=
?
s?S
exp{w
s
? ?
s
(x, s, s
i?1
, d
i
)}
Similarly, given a feature map ?
d
and
a weight vector w
d
, (p
w
d
(d
i
) is short for
p
w
d
(d
i
|s
i?1
, d
i?1
;x)
p
w
d
(d
i
) =
exp{w
d
? ?
d
(x, d
i
, s
i?1
, d
i?1
)}
Z
x,s
i?1
,d
i?1
;w
d
where,
Z
x,s
i?1
,d
i?1
;w
d
=
?
d?D
exp{w
d
? ?
d
(x, d, s
i?1
, d
i?1
)}
For different characteristic properties between
syntactic parsing and semantic parsing, different
feature maps are taken into account. Table 2
lists the features used to predict semantic depen-
dency relations, whereas table 3 lists the features
used to predict the syntactic dependency relations.
The features used for syntactic dependency rela-
tion classification are strongly based on previous
works (McDonald et al, 2006; Nakagawa, 2007).
We just integrate syntactic dependency Rela-
tion classification and semantic dependency rela-
tion here. If one combines identification and clas-
sification of semantic roles as one multi-class clas-
sification, the tag set of the second layer can be
substituted by the tag set of semantic roles plus a
NULL (?not an argument?) label.
3.4 Inference
The ?argmax problem? in structured prediction is
not tractable in the general case. However, the bi-
layer graphical model presented in form sections
admits efficient search using dynamic program-
ming solution. Searching for the highest probabil-
ity of a graph depends on the factorization chosen.
According to the form of the global score
p(d, s|x) =
n
?
i=1
p(s
i
|s
i?1
, d
i
;x)p(d
i
|s
i?1
, d
i?1
;x)
, we define forward probabilities ?
t
(s, d) to be the
probability of semantic relation being s and syn-
tactic relation being d at time t given observation
sequence up to time t. The recursive dynamic pro-
gramming step is
?
t+1
(d, s) = arg max
d?D,s?S
?
d
?
?D,s
?
?S
?
t
(d
?
, s
?
) ?
p(s
i
|s
i?1
, d
i
;x)p(d
i
|s
i?1
, d
i?1
;x)
Finally, to compute the globally most proba-
ble assignment (
?
d,
?
s) = argmax
d,s
p(d, s|x), a
Viterbi recursion works well.
4 Results
We trained our system using positive examples
extracted from all training data of CoNLL 2008
shared task. Table 4 shows the overall syntactic
parsing results obtained on the WSJ test set (Sec-
tion 23) and the Brown test set (Section ck/01-03).
Table 5 shows the overall semantic parsing results
obtained on the WSJ test set (Section 23) and the
Brown test set (Section ck/01-03).
246
Test Set UAS LAS Label Accuracy
WSJ 89.25% 86.37% 91.25%
Brown 86.12% 80.75% 87.14%
Table 4: Overall syntactic parsing results
Task Precision Recall F
?=1
WSJ ID 73.76% 85.24% 79.08
ID&CL 63.07% 72.88% 67.62
Brown ID 70.77% 80.50% 75.32
ID&CL 54.74% 62.26% 58.26
Table 5: Overall semantic parsing results
Test WSJ Precision(%) Recall(%) F
?=1
SRL of Verbs
All 73.53 73.28 73.41
Core-Arg 78.83 76.93 77.87
AM-* 62.51 64.83 63.65
SRL of Nouns
All 62.06 45.49 52.50
Core-Arg 61.47 46.56 52.98
AM-* 66.19 39.93 49.81
Table 6: Semantic role labeling results on verbs
and nouns. Core-Arg means numbered argument.
Table 6 shows the detailed semantic parsing re-
sults obtained on the WSJ test set (Section 23)
of verbs and nouns respectively. The comparison
suggests that SRL on NomBank is much harder
than PropBank.
Acknowlegements
The work is supported by the National Natural
Science Foundation of China under Grants No.
60503071, 863 the National High Technology Re-
search and Development Program of China un-
der Grants No.2006AA01Z144, and the Project of
Toshiba (China) Co., Ltd. R&D Center.
References
Berger, Adam, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A Maximum Entropy Approach to
Natural Language Processing. Computional Lin-
guistics, 22(1):39?71.
Ge, Ruifang and Raymond J. Mooney. 2005. A Statis-
tical Semantic Parser that Integrates Syntax and Se-
mantics. In Proceedings of the Conference of Com-
putational Natural Language Learning.
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computional Linguis-
tics, 28(3):245?288.
Koomen, Peter, Vasina Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of Conference on Natural Language Learn-
ing.
McCallum, Andrew, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for Information Extraction and Segmentation.
In Proceedings of International Conference on Ma-
chine Learning.
McDonald, Ryan, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing.
McDonald, Ryan, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual Dependency Analysis
with a Two-Stage Discriminative Parser. In Proceed-
ings of Conference on Natural Language Learning.
Nakawa, Tetsuji. 2007. Multilingual Dependency
Parsing using Global Features. In Proceedings of
Conference on Natural Language Learning.
Nivre, Joakim, Johan Hall, Sandra K?ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. The CoNLL 2007 Shared Task on Depen-
dency Parsing. 2007. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, 915?
932,
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James Martin, and Daniel Jurafsky.
2005. Support Vector Learning for Semantic Argu-
ment Classification. In Proceedings of Conference
on Association for Computational Linguistics.
Punyakanok, Vasin , Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
the 20th International Conference on Computational
Linguistics.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Llu??s M`arquez, and Nivre, Joakim. 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings
of the 12th Conference on Computational Natural
Language Learning (CoNLL-2008).
Xue, Nianwen and Martha Palmer. 2004. Calibrat-
ing Features for Semantic Role Labeling. In Pro-
ceedings of Empirical Methods in Natural Language
Processing.
Yi, Szu-ting and Martha Palmer. 2005. The Integra-
tion of Syntactic Parsing and Semantic Role Label-
ing. In Proceedings of the Conference of Computa-
tional Natural Language Learning.
247
