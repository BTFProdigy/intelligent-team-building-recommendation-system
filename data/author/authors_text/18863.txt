Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 269?278, Dublin, Ireland, August 23-29 2014.
Rapid Development of a Corpus with Discourse Annotations
using Two-stage Crowdsourcing
Daisuke Kawahara
??
Yuichiro Machida
?
Tomohide Shibata
??
Sadao Kurohashi
??
Hayato Kobayashi
?
Manabu Sassano
?
?
Graduate School of Informatics, Kyoto University
?
CREST, Japan Science and Technology Agency
?
Yahoo Japan Corporation
{dk, shibata, kuro}@i.kyoto-u.ac.jp, machida@nlp.ist.i.kyoto-u.ac.jp,
{hakobaya, msassano}@yahoo-corp.jp
Abstract
We present a novel approach for rapidly developing a corpus with discourse annotations using
crowdsourcing. Although discourse annotations typically require much time and cost owing to
their complex nature, we realize discourse annotations in an extremely short time while retaining
good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experi-
ment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run.
Based on this corpus, we also develop a supervised discourse parser and evaluate its performance
to verify the usefulness of the acquired corpus.
1 Introduction
Humans understand text not by individually interpreting clauses or sentences, but by linking such a text
fragment with another in a particular context. To allow computers to understand text, it is essential to
capture the precise relations between these text fragments. This kind of analysis is called discourse
parsing or discourse structure analysis, and is an important and fundamental task in natural language
processing (NLP). Systems for discourse parsing are, however, available only for major languages, such
as English, owing to the lack of corpora with discourse annotations.
For English, several corpora with discourse annotations have been developed manually, consuming a
great deal of time and cost in the process. These include the Penn Discourse Treebank (Prasad et al.,
2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse Graphbank (Wolf and Gibson,
2005). Discourse parsers trained on these corpora have also been developed and practically used. To
create the same resource-rich environment for another language, a quicker method than the conventional
time-consuming framework should be sought. One possible approach is to use crowdsourcing, which
has actively been used to produce various language resources in recent years (e.g., (Snow et al., 2008;
Negri et al., 2011; Hong and Baker, 2011; Fossati et al., 2013)). It is, however, difficult to crowdsource
the difficult judgments for discourse annotations, which typically consists of two steps: finding a pair of
spans with a certain relation and identifying the relation between the pair.
In this paper, we propose a method for crowdsourcing discourse annotations that simplifies the proce-
dure by dividing it into two steps. The point is that by simplifying the annotation task it is suitable for
crowdsourcing, but does not skew the annotations for use in practical discourse parsing. First, finding a
discourse unit for the span is a costly process, and thus we adopt a clause as the discourse unit, since this
is reliable enough to be automatically detected. We also limit the length of each target document to three
sentences and at most five clauses to facilitate the annotation task. Secondly, we detect and annotate
clause pairs in a document that hold logical discourse relations. However, since this is too complicated
to assign as one task using crowdsourcing, we divide the task into two steps: determining the existence
of logical discourse relations and annotating the type of relation. Our two-stage approach is a robust
method in that it confirms the existence of the discourse relations twice. We also designed the tagset
of discourse relations for crowdsourcing, which consists of two layers, where the upper layer contains
the following three classes: ?CONTINGENCY,? ?COMPARISON? and ?OTHER.? Although the task
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
269
settings are simplified for crowdsourcing, the obtained corpus and knowledge of discourse parsing could
be still useful in general discourse parsing.
In our experiments, we crowdsourced discourse annotations for Japanese, for which there are no pub-
licly available corpora with discourse annotations. The resulting corpus consists of 10,000 documents,
each of which comprises three sentences extracted from the web. Carrying out this two-stage crowd-
sourcing task took less than eight hours. The time elapsed was significantly shorter than the conventional
corpus building method.
We also developed a discourse parser by exploiting the acquired corpus with discourse annotations.
We learned a machine learning-based model for discourse parsing based on this corpus and evaluated its
performance. An F1 value of 37.9% was achieved for contingency relations, which would be roughly
comparable with state-of-the-art discourse parsers on English. This result indicates the usefulness of the
acquired corpus. The resulting discourse parser would be effectively exploited in NLP applications, such
as sentiment analysis (Zirn et al., 2011) and contradiction detection (Murakami et al., 2009; Ennals et
al., 2010).
The novel contributions of this study are summarized below:
? We propose a framework for developing a corpus with discourse annotations using two-stage crowd-
sourcing, which is both cheap and quick to execute, but still retains good quality of the annotations.
? We construct a Japanese discourse corpus in an extremely short time.
? We develop a discourse parser based on the acquired corpus.
The remainder of this paper is organized as follows. Section 2 introduces related work, while Section
3 describes our proposed framework and reports the experimental results for the creation of a corpus with
discourse annotations. Section 4 presents a method for discourse parsing based on the corpus as well as
some experimental results. Section 5 concludes the paper.
2 Related Work
Snow et al. (2008) applied crowdsourcing to five NLP annotation tasks, but the settings of these tasks
are very simple. There have also been several attempts to construct language resources with complex
annotations using crowdsourcing. Negri et al. (2011) proposed a method for developing a cross-lingual
textual entailment (CLTE) corpus using crowdsourcing. They tackled this complex data creation task by
dividing it into several simple subtasks: sentence modification, type annotation and sentence translation.
The creative CLTE task and subtasks are quite different from our non-creative task and subtasks of
discourse annotations. Fossati et al. (2013) proposed FrameNet annotations using crowdsourcing. Their
method is a single-step approach to only detect frame elements. They verified the usefulness of their
approach through an experiment on a small set of verbs with only two frame ambiguities per verb.
Although they seem to be running a larger-scale experiment, its result has not been revealed yet. Hong
and Baker (2011) presented a crowdsourcing method for selecting FrameNet frames, which is a part of
the FrameNet annotation process. Since their task is equivalent to word sense disambiguation, it is not
very complex compared to the whole FrameNet annotation process. These FrameNet annotations are
still different from discourse annotations, which are our target. To the best of our knowledge, there have
been no attempts to crowdsource discourse annotations.
There are several manually-crafted corpora with discourse annotation for English, such as the Penn
Discourse Treebank (Prasad et al., 2008), RST Discourse Treebank (Carlson et al., 2001), and Discourse
Graphbank (Wolf and Gibson, 2005). These corpora were developed from English newspaper articles.
Several attempts have been made to manually create corpora with discourse annotations for languages
other than English. These include the Potsdam Commentary Corpus (Stede, 2004) for German (news-
paper; 2,900 sentences), Rhetalho (Pardo et al., 2004) for Portuguese (scientific papers; 100 documents;
1,350 sentences), and the RST Spanish Treebank for Spanish (da Cunha et al., 2011) (several genres;
267 documents; 2,256 sentences). All of these consist of relatively small numbers of sentences com-
pared with the English corpora containing several tens of thousands sentences.
270
In recent years, there have been many studies on discourse parsing on the basis of the above hand-
annotated corpora (e.g., (Pitler et al., 2009; Pitler and Nenkova, 2009; Subba and Di Eugenio, 2009;
Hernault et al., 2010; Ghosh et al., 2011; Lin et al., 2012; Feng and Hirst, 2012; Joty et al., 2012; Joty
et al., 2013; Biran and McKeown, 2013; Lan et al., 2013)). This surge of research on discourse parsing
can be attributed to the existence of corpora with discourse annotations. However, the target language is
mostly English since English is the only language that has large-scale discourse corpora. To develop and
improve discourse parsers for languages other than English, it is necessary to build large-scale annotated
corpora, especially in a short period if possible.
3 Development of Corpus with Discourse Annotations using Crowdsourcing
3.1 Corpus Specifications
We develop a tagged corpus in which pairs of discourse units are annotated with discourse relations.
To achieve this, it is necessary to determine target documents, discourse units, and a discourse relation
tagset. The following subsections explain the details of these three aspects.
3.1.1 Target Text and Discourse Unit
In previous studies on constructing discourse corpora, the target documents were mainly newspaper
texts, such as the Wall Street Journal for English. However, discourse parsers trained on such newspaper
corpora usually have a problem of domain adaptation. That is to say, while discourse parsers trained on
newspaper corpora are good at analyzing newspaper texts, they generally cannot perform well on texts
of other domains.
To address this problem, we set out to create an annotated corpus covering a variety of domains.
Since the web contains many documents across a variety of domains, we use the Diverse Document
Leads Corpus (Hangyo et al., 2012), which was extracted from the web. Each document in this corpus
consists of the first three sentences of a Japanese web page, making these short documents suitable for
our discourse annotation method based on crowdsourcing.
We adopt the clause as a discourse unit, since spans are too fine-grained to annotate using crowdsourc-
ing and sentences are too coarse-grained to capture discourse relations. Clauses, which are automatically
identified, do not need to be manually modified since they are thought to be reliable enough. Clause
identification is performed using the rules of Shibata and Kurohashi (2005). For example, the following
rules are used to identify clauses as our discourse units:
? clauses that function as a relatively strong boundary in a sentence are adopted,
? relative clauses are excluded.
Since workers involved in our crowdsourcing task need to judge whether clause pairs have discourse
relations, the load of these workers increases combinatorially as the number of clauses in a sentence
increases. To alleviate this problem, we limit the number of clauses in a document to five. This limitation
excludes only about 5% of the documents in the original corpus.
Our corpus consists of 10,000 documents corresponding to 30,000 sentences. The total number of
clauses in this corpus is 39,032, and thus the average number of clauses in a document is 3.9. The total
number of clause pairs is 59,426.
3.1.2 Discourse Relation Tagset
One of our supposed applications of discourse parsing is to automatically generate a bird?s eye view of a
controversial topic as in Statement Map (Murakami et al., 2009) and Dispute Finder (Ennals et al., 2010),
which identify various relations between statements, including contradictory relations. We assume that
expansion relations, such as elaboration and restatement, and temporal relations are not important for this
purpose. This setting is similar to the work of Bethard et al. (2008), which annotated temporal relations
independently of causal relations. We also suppose that temporal relations can be annotated separately
for NLP applications that require temporal information. We determined the tagset of discourse relations
271
Upper type Lower type Example
CONTINGENCY
Cause/Reason ???????????????????
[since (I) pushed the button] [hot water was turned on]
Purpose ?????????????????????
[to pass the exam] [(I) studied a lot]
Condition ?????????????????
[if (you) push the button] [hot water will be turned on]
Ground ??????????????????????????
[here is his/her bag] [he/she would be still in the company]
COMPARISON
Contrast ?????????????????????????????
[at that restaurant, sushi is good] [ramen is so-so]
Concession ??????????????????????????
[that restaurant is surely good] [the price is high]
OTHER (Other) ???????????????????
[After being back home] [it began to rain]
Table 1: Discourse relation tagset with examples.
by referring to the Penn Discourse Treebank. This tagset consists of two layers, where the upper layer
contains three classes and the lower layer seven classes as follows:
? CONTINGENCY
? Cause/Reason (causal relations and not conditional relations)
? Purpose (purpose-action relations where the purpose is not necessarily accomplished)
? Condition (conditional relations)
? Ground (other contingency relations including pragmatic cause/condition)
? COMPARISON (same as the Penn Discourse Treebank)
? Contrast
? Concession
? OTHER (other weak relation or no relation)
Note that we do not consider the direction of relations to simplify the annotation task for crowdsourcing.
Table 1 shows examples of our tagset.
Therefore, our task is to annotate clause pairs in a document with one of the discourse relations given
above. Sample annotations of a document are shown below. Here, clause boundaries are shown by ?::?
and clause pairs that are not explicitly marked are allocated the ?OTHER? relation.
Cause/Reason ?????::??????????::????????????::???????
???????::??????????????
... [the surgery of my father ended safely] [(I) am relieved a little bit]
Contrast ???????????????????????::????????????
????????????????????????::???????????
??????::????????
... [There is tailwind to live,] [there is also headwind.]
3.2 Two-stage Crowdsourcing for Discourse Annotations
We create a corpus with discourse annotations using two-stage crowdsourcing. We divide the annotation
task into the following two subtasks: determining whether a clause pair has a discourse relation excluding
?OTHER,? and then, ascertaining the type of discourse relation for a clause pair that passes the first stage.
272
Probability Number
= 1.0 64
> 0.99 554
> 0.9 1,065
> 0.8 1,379
> 0.5 2,655
> 0.2 4,827
> 0.1 5,895
> 0.01 9,068
> 0.001 12,277
> 0.0001 15,554
Table 2: Number of clause pairs resulting from the judgments of discourse relation existence.
3.2.1 Stage 1: Judgment of Discourse Relation Existence
This subtask determines whether each clause pair in a document has one of the following discourse
relations: Cause/Reason, Purpose, Condition, Ground, Contrast, and Concession (that is, all the relations
except ?OTHER?). Workers are shown examples of these relations and asked to determine only the
existence thereof.
In this subtask, an item presented to a worker at a particular time consists of all the judgments of
clause pairs in a document. By adopting this approach, each worker considers the entire document when
making his/her judgments.
3.2.2 Stage 2: Judgment of Discourse Relation Type
This subtask involves ascertaining the discourse relation type for a clause pair that passes the first stage.
The result of this subtask is one of the seven lower types in our discourse relation tagset. Workers
are shown examples of these types and then asked to select one of the relations. If a worker chooses
?OTHER,? this corresponds to canceling the positive determination of the existence of the discourse
relation in stage one.
In this subtask, an item is the judgment of a clause pair. That is, if a document contains more than
one clause pair that must be judged, the judgments for this document are divided into multiple items,
although this is rare.
3.3 Experiment and Discussion
We conducted an experiment of the two-stage crowdsourcing approach using Yahoo! Crowdsourcing.
1
To increase the reliability of the produced corpus, we set the number of workers for each item for each
task to 10. The reason why we chose this value is as follows. While Snow et al. (2008) claimed that an
average of 4 non-expert labels per item in order to emulate expert-level label quality, the quality of some
tasks increased by increasing the number of workers to 10. We also tested hidden gold-standard items
once every 10 items to examine worker?s quality. If a worker failed these items in serial, he/she would
have to take a test to continue the task.
We obtained judgments for the 59,426 clause pairs in the 10,000 documents of our corpus in the
first stage of crowdsourcing, i.e., the subtask of determining the existence of discourse relations. We
calculated the probability of each label using GLAD
2
(Whitehill et al., 2009), which was proved to
be more reliable than the majority voting. This probability corresponds to the probability of discourse
relation existence of each clause pair. Table 2 lists the results. We set a probability threshold to select
those clause pairs whose types were to be judged in the second stage of crowdsourcing. With this
threshold set to 0.01, 9,068 clause pairs (15.3% of all the clause pairs) were selected. The threshold was
set fairly low to allow low-probability judgments to be re-examined in the second stage.
1
http://crowdsourcing.yahoo.co.jp/
2
http://mplab.ucsd.edu/?jake/OptimalLabelingRelease1.0.3.tar.gz
273
Lower type All prob > 0.8
Cause/Reason 2,104 1,839 (87.4%)
Purpose 755 584 (77.4%)
Condition 1,109 925 (83.4%)
Ground 442 273 (61.8%)
Contrast 437 354 (81.0%)
Concession 80 49 (61.3%)
Sum of the above discourse relations 4,927 4,024 (81.7%)
Other 4,141 3,753 (90.6%)
Total 9,068 7,777 (85.8%)
Table 3: Results of the judgments of lower discourse relation types.
Upper type All prob > 0.8
CONTINGENCY 4,439 3,993 (90.0%)
COMPARISON 516 417 (80.8%)
Sum of the above discourse relations 4,955 4,410 (89.0%)
OTHER 4,113 3,753 (91.2%)
Total 9,068 8,163 (90.0%)
Table 4: Results of the judgments of upper discourse relation types.
The discourse relation types of the 9,068 clause pairs were determined in the second stage of crowd-
sourcing. We extended GLAD (Whitehill et al., 2009) for application to multi-class tasks, and calculated
the probability of the labels of each clause pair. We assigned the label (discourse relation type) with the
highest probability to each clause pair. Table 3 gives some statistics of the results. The second column in
this table denotes the numbers of each discourse relation type, while the third column gives the numbers
of each type of clause pair with a probability higher than 0.80. Table 4 gives statistics of the results when
the lower discourse relation types are merged into the upper types. Table 5 shows some examples of the
resulting annotations.
Carrying out the two separate subtasks using crowdsourcing took approximately three hours and five
hours with 1,458 and 1,100 workers, respectively. If we conduct this task at a single stage, it would take
approximately 33 (5 hours / 0.153) hours. It would be four times longer than our two-stage approach.
Such single-stage approach is also not robust since it does not have a double check mechanism, with
which the two-stage approach is equipped. We spent 111 thousand yen and 113 thousand yen (approx-
imately 1,100 USD, respectively) for these subtasks, which would be extremely less expensive than the
projects of conventional discourse annotations.
For the examples in Table 5, we confirmed that the discourse relation types of the top four examples
were surely correct. However, we judged the type (Contrast) of the bottom example as incorrect. Since
the second clause is an instantiation of the first clause, the correct type should be ?Other.? We found such
errors especially in the clause pairs with a probability lower than 0.80.
4 Development of Discourse Parser based on Acquired Discourse Corpus
To verify the usefulness of the acquired corpus with discourse annotations, we developed a supervised
discourse parser based on the corpus, and evaluated its performance. We built two discourse parsers using
the annotations of the lower and upper discourse relation types, respectively. From the annotations in the
first stage of crowdsourcing (i.e., judging the existence of discourse relations), we assigned annotations
with a probability less than 0.01 as ?OTHER.? Of the annotations acquired in the second stage (i.e.,
judging discourse relation types), we adopted those with a probability greater than 0.80 and discarded
the rest. After this preprocessing, we obtained 58,135 (50,358 + 7,777) instances of clause pairs for
the lower-type discourse parser and 58,521 (50,358 + 8,163) instances of clause pairs for the upper-type
274
Prob # W Type Document
1.00 6/10 Cause/Reason ???????????????????????????????
??????????????????????????????
????
... [Since the flower blooms in the fifth lunar month] [it is called ?Sat-
suki.?] ...
0.99 4/10 Condition ??????????????????????????????
???????????????????????????????
?????????????????????????????
??????
[If you click the balloon on the map] [you can see the recommended
route] ...
0.81 3/10 Purpose ?????????????????????????????
?????????????????????????????
??????????????????????????????
?????????????????????????????
... [And seeking ?Great harvest?] [each country is engaged in a war]
0.61 2/10 Cause/Reason ??????????????????????????????
?????????????????????????????
??????????????????????????????
??????????????????????????????
?????
... [by transmitting power to the front and rear axle with the combina-
tion of gears and shafts] [(it) drives the four wheels.]
0.54 3/10 Contrast ?????????????????????????????
??????????????????????????????
???????????????????????????
... [a scramble for customers by department stores would be severe.]
[What comes out is the possibility of the closure of Fukuoka Mit-
sukoshi.]
Table 5: Examples of Annotations. The first column denotes the estimated label probability and the
second column denotes the number of workers that assigned the designated type. In the fourth column,
the clause pair annotated with the type is marked with?? ([ ] in English translations).
discourse parser. Of these, 4,024 (6.9%) and 4,410 (7.5%) instances, respectively, had one of the types
besides ?OTHER.? We conducted experiments using five-fold cross validation on these instances.
To extract features of machine learning, we applied the Japanese morphological analyzer, JUMAN,
3
and the Japanese dependency parser, KNP,
4
to the corpus. We used the features listed in Table 6, which
are usually used for discourse parsing.
We adopted Opal (Yoshinaga and Kitsuregawa, 2010)
5
for the machine learning implementation. This
tool enables online learning using a polynomial kernel. As parameters for Opal, we used the passive-
aggressive algorithm (PA-I) with a polynomial kernel of degree two as a learner and the extension to
multi-class classification (Matsushima et al., 2010). The numbers of classes were seven and three for the
lower- and upper-type discourse parsers, respectively. We set the aggressiveness parameter C to 0.001,
which generally achieves good performance for many classification tasks. Other parameters were set to
the default values of Opal.
To measure the performance of the discourse parsers, we adopted precision, recall and their harmonic
mean (F1). These metrics were calculated as the proportion of the number of correct clause pairs to the
3
http://nlp.ist.i.kyoto-u.ac.jp/EN/?JUMAN
4
http://nlp.ist.i.kyoto-u.ac.jp/EN/?KNP
5
http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
275
Name Description
clause distance clause distance between two clauses
sentence distance sentence distance between two clauses
bag of words bag of words (lemmas) for each clause
predicate a content word (lemma) of the predicate of each clause
conjugation form of predicate a conjugation form of the predicate of each clause
conjunction a conjunction if it is located at the beginning of a clause
word overlapping ratio an overlapping ratio of words between the two clauses
clause type a lexical type output by KNP for each clause (about 100 types)
topic marker existence existence of a topic marker in each clause
topic marker cooccurrence existence of a topic marker in both clauses
Table 6: Features for our discourse parsers.
Type Precision Recall F1
Cause/Reason 0.623 (441/708) 0.240 (441/1,839) 0.346
Purpose 0.489 (44/90) 0.075 (44/584) 0.131
Condition 0.581 (256/441) 0.277 (256/925) 0.375
Ground 0.000 (0/12) 0.000 (0/273) 0.000
Contrast 0.857 (6/7) 0.017 (6/354) 0.033
Concession 0.000 (0/0) 0.000 (0/49) 0.000
Other 0.944 (53,702/56,877) 0.992 (53,702/54,111) 0.968
Table 7: Performance of our lower-type discourse parser.
Type Precision Recall F1
CONTINGENCY 0.625 (1,084/1,735) 0.272 (1,084/3,993) 0.379
COMPARISON 0.412 (7/17) 0.017 (7/417) 0.032
OTHER 0.942 (53,454/56,769) 0.988 (53,454/54,111) 0.964
Table 8: Performance of our upper-type discourse parser.
number of all recognized or gold-standard ones for each discourse relation type. Tables 7 and 8 give the
accuracies for the lower- and upper-type discourse parsers, respectively.
From Table 8, we can see that our upper-type discourse parser achieved an F1 of 37.9% for contingency
relations. It is difficult to compare our results with those in previous work due to the use of different data
set and different languages. We, however, anticipate that our results would be comparable with those
of state-of-the-art English discourse parsers. For example, the end-to-end discourse parser of Lin et al.
(2012) achieved an F1 of 20.6% ? 46.8% on the Penn Discourse Treebank.
We also obtained a low F1 for comparison relations. This tendency is similar to the previous results
on the Penn Discourse Treebank. The biggest cause of this low F1 is the lack of unambiguous explicit
discourse connectives for these relations. Although there are explicit discourse connectives in Japanese,
many of them have multiple meanings and cannot be used as a direct clue for discourse relation detection
(e.g., as described in Kaneko and Bekki (2014)). As reported in Pitler et al. (2009) and other studies,
the identification of implicit discourse relations are notoriously difficult. To improve its performance, we
need to incorporate external knowledge sources other than the training data into the discourse parsers.
A promising way is to use large-scale knowledge resources that are automatically acquired from raw
corpora.
276
5 Conclusion
We presented a rapid approach for building a corpus with discourse annotations and a discourse parser
using two-stage crowdsourcing. The acquired corpus is made publicly available and can be used for
research purposes.
6
This corpus can be used not only to build a discourse parser but also to evaluate
its performance. The availability of the corpus with discourse annotations will accelerate the develop-
ment and improvement of discourse parsing. In the future, we intend integrating automatically acquired
knowledge from corpora into the discourse parsers to further enhance their performance. We also aim to
apply our framework to other languages without available corpora with discourse annotations.
References
Steven Bethard, William Corvey, Sara Klingenstein, and James H. Martin. 2008. Building a corpus of temporal-
causal structure. In Proceedings of the 6th International Conference on Language Resources and Evaluation,
pages 908?915.
Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disam-
biguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume
2: Short Papers), pages 69?73.
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and
Dialogue.
Iria da Cunha, Juan-Manuel Torres-Moreno, and Gerardo Sierra. 2011. On the development of the RST Spanish
treebank. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 1?10.
Rob Ennals, Beth Trushkowsky, and John Mark Agosta. 2010. Highlighting disputed claims on the web. In
Proceedings of the 19th international conference on World Wide Web, pages 341?350.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceed-
ings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 60?68. Association for Computational Linguistics.
Marco Fossati, Claudio Giuliano, and Sara Tonelli. 2013. Outsourcing FrameNet to the crowd. In Proceedings of
the 51st Annual Meeting of the Association for Computational Linguistics, pages 742?747.
Sucheta Ghosh, Sara Tonelli, Giuseppe Riccardi, and Richard Johansson. 2011. End-to-end discourse parser
evaluation. In Fifth IEEE International Conference on Semantic Computing (ICSC), pages 169?172.
Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. 2012. Building a diverse document leads corpus
annotated with semantic relations. In Proceedings of 26th Pacific Asia Conference on Language Information
and Computing, pages 535?544.
Hugo Hernault, Helmut Prendinger, David duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser
using support vector machine classification. Dialogue & Discourse, 1(3):1?33.
Jisup Hong and Collin F. Baker. 2011. How good is the crowd at ?real? WSD? In Proceedings of the 5th Linguistic
Annotation Workshop, pages 30?37.
Shafiq Joty, Giuseppe Carenini, and Raymond Ng. 2012. A novel discriminative framework for sentence-level
discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 904?915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra- and multi-sentential
rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the
Association for Computational Linguistics, pages 486?496.
Kimi Kaneko and Daisuke Bekki. 2014. Building a Japanese corpus of temporal-causal-discourse structures
based on SDRT for extracting causal relations. In Proceedings of the EACL 2014 Workshop on Computational
Approaches to Causality in Language (CAtoCL), pages 33?39.
6
http://nlp.ist.i.kyoto-u.ac.jp/EN/?DDLC
277
Man Lan, Yu Xu, and Zhengyu Niu. 2013. Leveraging synthetic discourse data via multi-task learning for implicit
discourse relation recognition. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 476?485.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A PDTB-styled end-to-end discourse parser. Natural
Language Engineering, pages 1?34.
Shin Matsushima, Nobuyuki Shimizu, Kazuhiro Yoshida, Takashi Ninomiya, and Hiroshi Nakagawa. 2010. Exact
passive-aggressive algorithm for multiclass classification using support class. In Proceedings of 2010 SIAM
International Conference on Data Mining (SDM2010), pages 303?314.
Koji Murakami, Eric Nichols, Suguru Matsuyoshi, Asuka Sumida, Shouko Masuda, Kentaro Inui, and Yuji Mat-
sumoto. 2009. Statement map: Assisting information credibility analysis by visualizing arguments. In Pro-
ceedings of the 3rd Workshop on Information Credibility on the Web, pages 43?50.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo Giampiccolo, and Alessandro Marchetti. 2011. Divide
and conquer: Crowdsourcing the creation of cross-lingual textual entailment corpora. In Proceedings of the
2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679.
Thiago Alexandre Salgueiro Pardo, Maria das Grac?as Volpe Nunes, and Lucia Helena Machado Rino. 2004.
Dizer: An automatic discourse analyzer for Brazilian Portuguese. In Advances in Artificial Intelligence?SBIA
2004, pages 224?234. Springer.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13?16.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in
text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 683?691.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn discourse treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation, pages 2961?2968.
Tomohide Shibata and Sadao Kurohashi. 2005. Automatic slide generation based on discourse structure analysis.
In Proceedings of Second International Joint Conference on Natural Language Processing, pages 754?766.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and AndrewNg. 2008. Cheap and fast ? but is it good? evaluating
non-expert annotations for natural language tasks. In Proceedings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 254?263.
Manfred Stede. 2004. The Potsdam commentary corpus. In Proceedings of the 2004 ACL Workshop on Discourse
Annotation, pages 96?102.
Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 566?574.
Jacob Whitehill, Paul Ruvolo, Ting fan Wu, Jacob Bergsma, and Javier Movellan. 2009. Whose vote should
count more: Optimal integration of labels from labelers of unknown expertise. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information Processing Systems 22,
pages 2035?2043.
FlorianWolf and Edward Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational
Linguistics, 31(2):249?287.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING2010), pages
1245?1253.
C?acilia Zirn, Mathias Niepert, Heiner Stuckenschmidt, and Michael Strube. 2011. Fine-grained sentiment analysis
with structural features. In Proceedings of 5th International Joint Conference on Natural Language Processing,
pages 336?344.
278
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 797?806,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Perplexity on Reduced Corpora
Hayato Kobayashi?
Yahoo Japan Corporation
9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
hakobaya@yahoo-corp.jp
Abstract
This paper studies the idea of remov-
ing low-frequency words from a corpus,
which is a common practice to reduce
computational costs, from a theoretical
standpoint. Based on the assumption that a
corpus follows Zipf?s law, we derive trade-
off formulae of the perplexity of k-gram
models and topic models with respect to
the size of the reduced vocabulary. In ad-
dition, we show an approximate behavior
of each formula under certain conditions.
We verify the correctness of our theory on
synthetic corpora and examine the gap be-
tween theory and practice on real corpora.
1 Introduction
Removing low-frequency words from a corpus
(often called cutoff) is a common practice to save
on the computational costs involved in learning
language models and topic models. In the case
of language models, we often have to remove
low-frequency words because of a lack of com-
putational resources, since the feature space of k-
grams tends to be so large that we sometimes need
cutoffs even in a distributed environment (Brants
et al, 2007). In the case of topic models, the in-
tuition is that low-frequency words do not make a
large contribution to the statistics of the models.
Actually, when we try to roughly analyze a corpus
with topic models, a reduced corpus is enough for
the purpose (Steyvers and Griffiths, 2007).
A natural question arises: How many low-
frequency words can we remove while maintain-
ing sufficient performance? Or more generally,
by how much can we reduce a corpus/model us-
ing a certain strategy and still keep a sufficient
level of performance? There have been many stud-
?This work was mainly carried out while the author was
with Toshiba Corporation.
ies addressing the question as it pertains to differ-
ent strategies (Stolcke, 1998; Buchsbaum et al,
1998; Goodman and Gao, 2000; Gao and Zhang,
2002; Ha et al, 2006; Hirsimaki, 2007; Church
et al, 2007). Each of these studies experimen-
tally discusses trade-off relationships between the
size of the reduced corpus/model and its perfor-
mance measured by perplexity, word error rate,
and other factors. To our knowledge, however,
there is no theoretical study on the question and
no evidence for such a trade-off relationship, es-
pecially for topic models.
In this paper, we first address the question from
a theoretical standpoint. We focus on the cutoff
strategy for reducing a corpus, since a cutoff is
simple but powerful method that is worth study-
ing; as reported in (Goodman and Gao, 2000;
Gao and Zhang, 2002), a cutoff is competitive
with sophisticated strategies such as entropy prun-
ing. As the basis of our theory, we assume Zipf?s
law (Zipf, 1935), which is an empirical rule repre-
senting a long-tail property of words in a corpus.
Our approach is essentially the same as those in
physics, in the sense of constructing a theory while
believing experimentally observed results. For ex-
ample, we can derive the distance to the landing
point of a ball thrown up in the air with initial
speed v
0
and angle ? as v
0
2
sin(2?)/g by believ-
ing in the experimentally observed gravity acceler-
ation g. In a similar fashion, we will try to clarify
the trade-off relationship by believing Zipf?s law.
The rest of the paper is organized as follows. In
Section 2, we define the notation and briefly ex-
plain Zipf?s law and perplexity. In Section 3, we
theoretically derive the trade-off formulae of the
cutoff for unigram models, k-gram models, and
topic models, each of which represents its per-
plexity with respect to a reduced vocabulary, un-
der the assumption that the corpus follows Zipf?s
law. In addition, we show an approximate behav-
ior of each formula under certain conditions. In
797
Section 4, we verify the correctness of our theory
on synthetic corpora and examine the gap between
theory and practice on several real corpora. Sec-
tion 5 concludes the paper.
2 Preliminaries
Let us consider a corpus w := w
1
? ? ?w
N
of cor-
pus size N and vocabulary size W . We use an
abridged notation {w} := {w ? w} to repre-
sent the vocabulary of w. Clearly, N = |w| and
W = |{w}| hold. When w has additional nota-
tions, N and W inherit them. For example, we
will use N ? as the size of w? without its definition.
2.1 Power law and Zipf?s law
A power law is a mathematical relationship be-
tween two quantities x and y, where y is propor-
tional to the c-th power of x, i.e., y ? xc, and
c is a real number. Zipf?s law (Zipf, 1935) is a
power law discovered on real corpora, wherein for
any word w ? w in a corpus w, its frequency (or
word count) f(w) is inversely proportional to its
frequency ranking r(w), i.e.,
f(w) =
C
r(w)
.
Here, f(w) := |{w? ? w | w? = w}|, and
r(w) := |{w
?
? w | f(w
?
) ? f(w)}|. From
the definition, the constant C is the maximum fre-
quency in the corpus. Taking the natural loga-
rithms ln(?) of both sides of the above equation,
we find that its plot becomes linear on a log-log
graph of r(w) and f(w). In fact, the result based
on a statistical test in (Clauset et al, 2009) reports
that the frequencies of words in a corpus com-
pletely follow a power law, whereas many datasets
with long-tail properties, such as networks, actu-
ally do not follow power laws.
2.2 Perplexity
Perplexity is a widely used evaluation measure of
k-gram models and topic models. Let p be a pre-
dictive distribution over words, which was learned
from a training corpus w based on a certain model.
Formally, perplexity PP is defined as the geomet-
ric mean of the inverse of the per-word likelihood
on the held-out test corpus w
?
, i.e.,
PP :=
(
?
w?w
?
1
p(w)
)
1
N
?
.
Intuitively, PP means how many possibilities one
has for estimating the next word in a test cor-
pus. According to the definition, a lower perplex-
ity means better generalization performance of p.
Another well-known evaluation measure is cross-
entropy. Since cross-entropy is easily calculated
as log
2
PP, we can apply many of the results of
this paper to cross-entropy.
3 Perplexity on Reduced Corpora
Now let us consider what a cutoff is. In our study,
we simply define a corpus that has been reduced
by removing low-frequency words from the origi-
nal corpus with a certain threshold. Formally, we
say w? is a corpus reduced from the original cor-
pus w, if w? is the longest subsequence of w such
that max
w
?
?w
?
r(w
?
) = W
?
. Note that a sub-
sequence can include gaps in contrast to a sub-
string. For example, supposing we have a corpus
w = abcaba with a vocabulary {w} = {a, b, c},
w
?
1
= ababa is a reduced corpus, while w?
2
=
aba and w?
3
= acaa are not.
After learning a distribution p? from a re-
duced corpus w?, we need to infer the distri-
bution p learned from the original corpus w.
Here, we use constant restoring (defined below),
which assumes the frequencies of the reduced low-
frequency words are a constant.
Definition 1 (Constant Restoring). Given a pos-
itive constant ?, a distribution p? over a reduced
corpus w?, and a corpus w, we say that p? is
a ?-restored distribution of p? from w? to w, if
?
w?{w}
p?(w) = 1, and for any w ? w,
p?(w) ?
{
p
?
(w) (w ? w
?
)
? (w /? w
?
).
Constant restoring is similar to the additive
smoothing defined by p?(w) ? p?(w)+?, which is
used to solve the zero-frequency problem of lan-
guage models (Chen and Goodman, 1996). The
only difference is the addition of a constant ?
only to zero-frequency words. We think con-
stant restoring is theoretically natural in our set-
ting, since we can derive the above equation by
letting each frequency of reduced words be ?N ?
and defining a restored frequency function as fol-
lows:
?
f(w) =
{
f(w) (w ? w
?
)
?N
?
(w /? w
?
).
798
Informally, constant restoring involves padding
the vocabulary, while additive smoothing involves
padding the corpus. Smoothing should be carried
out after restoring.
3.1 Perplexity of Unigram Models
Let us consider the perplexity of a unigram model
learned from a reduced corpus. In unigram mod-
els, a predictive distribution p? on a reduced cor-
pus w? can be simply calculated as p?(w?) =
f(w
?
)/N
?
. We shall start with an analysis of
training-set perplexity, since we can derive an ex-
act formula for it, which will give us a sufficient
idea for making an approximate analysis of test-
set perplexity.
Let ?PP
1
:=
(
?
w?w
1
p?(w)
)
1
N be the perplexity
of a ?-restored distribution p? on a unigram model.
The next lemma gives the optimal restoring con-
stant ?? minimizing ?PP
1
.
Lemma 2. For any ?-restored distribution p? of a
distribution p? from a reduced corpus w? to the
original corpus w, its perplexity is minimized by
?
?
=
N ?N
?
(W ?W
?
)N
?
.
Proof. Let w
R
be the longest subsequence such
that min
w
?
?w
?
r(w
?
) = W
?
+ 1. Since w
R
is the
remainder of w?, N
R
= N ?N
? and W
R
= W ?
W
? hold. After substituting the normalized form
of p? of Definition 1 into ?PP
1
, we have
?PP
1
=
(
?
w
?
?w
?
1
p?(w
?
)
?
w
R
?w
R
1
p?(w
R
)
)
1
N
=
(
?
w
?
?w
?
1 + W
R
?
p
?
(w
?
)
?
w
R
?w
R
1 + W
R
?
?
)
1
N
=
1 + W
R
?
?
N
R
N
(
?
w
?
?w
?
1
p
?
(w
?
)
)
1
N
.
We obtain the optimal smoothing factor ?? when
?
??
?PP
1
?
?
??
(1 + W
R
?)/?
N
R
N
= 0.
By using a similar argument to the one in the
above lemma, we can obtain the optimal constant
of additive smoothing as ?? ? N?N ?
WN
?
, when N is
sufficiently large.
The next theorem gives the exact formula of the
training-set perplexity of a unigram model learned
from a reduced corpus.
Theorem 3. For any distribution p? on a unigram
model learned from a corpus w? reduced from the
original corpus w following Zipf?s law, the per-
plexity ?PP
1
of the ??-restored distribution p? of p?
from w? to w is calculated by
?PP
1
(W
?
) =H(W ) exp
(
B(W
?
)
H(W )
)
(
W ?W
?
H(W )?H(W
?
)
)
1?
H(W
?
)
H(W )
,
where H(X) :=
?
X
x=1
1
x
and B(X) :=
?
X
x=1
lnx
x
.
Proof. We expand the first part of ?PP
1
in the proof
of Lemma 2 using ?? as follows:
1 + W
R
?
?
?
?
N
R
N
=
(
1 +
N
R
N
?
)(
W
R
N
?
N
R
)
N
R
N
=
(
N
N
?
)(
(W ?W
?
)N
?
N ?N
?
)
1?
N
?
N
.
The second part of ?PP
1
is as follows:
(
?
w
?
?w
?
1
p
?
(w
?
)
)
1
N
=
?
w
?
?{w
?
}
(
1
p
?
(w
?
)
)
f(w
?
)
N
=
W
?
?
r=1
(
rN
?
C
)
C
rN
=
W
?
?
r=1
(
N
?
C
)
C
rN
W
?
?
r=1
r
C
rN
=
(
N
?
C
)
N
?
N
exp
(
C
N
W
?
?
r=1
ln r
r
)
.
We obtain the objective formula by putting the
above two formulae together with N = CH(W )
and N ? = CH(W ?), which are derived from
Zipf?s law.
The functions H(X) and B(X) are the X-th
partial sum of the harmonic series and Bertrand
series (special form), respectively. An approxima-
tion by definite integrals yields H(X) ? lnX+?,
where ? is the Euler-Mascheroni constant, and
B(X) ?
1
2
ln
2
X . We may omit ? from the ap-
proximate analysis.
Now let us consider an approximate form of
?PP
1
(W
?
) in Theorem 3. For further discussion,
799
we define the last part of ?PP
1
(W
?
) as follows:
F (W,W
?
) :=
(
W ?W
?
H(W )?H(W
?
)
)
1?
H(W
?
)
H(W )
.
Since W ? = ?W holds for an appropriate ratio ?,
we have
F (W, ?W ) =
(
W ? ?W
H(W )?H(?W )
)
1?
H(?W )
H(W )
?
(
W ? ?W
lnW ? ln (?W )
)
1?
ln (?W )
lnW
=
(
W (1? ?)
? ln ?
)
? ln ?
lnW
?
1
?
(W ? ?).
Therefore, when W is sufficiently large, we can
use F (W,W ?) ? W
W
?
, since F (W, ?W ) ? 1
?
holds
for any ratio ? : 0 < ? < 1. Using this fact,
we obtain an approximate formula ?PP
1
of ?PP
1
as
follows:
?PP
1
(W
?
) = lnW exp
(
ln
2
W
?
2 lnW
)
W
W
?
=
?
W lnW exp
(lnW
?
? lnW )
2
2 lnW
.
The complexity of ?PP
1
is quasi-polynomial,
i.e., ?PP
1
(W
?
) = O(W
?
lnW
?
), which behaves as
a quadratic function on a log-log graph. Since
?PP
1
(W
?
) is convex, i.e., ?2
?W
?
2
?PP
1
(W
?
) > 0, and
its gradient ?
?W
?
?PP
1
(W
?
) is zero when W ? = W ,
we infer that low-frequency words may not largely
contribute to the statistics.
Considering the special case of W ? = W , we
obtain the perplexity PP
1
of the unigram model
learned from the original corpus w as
PP
1
= H(W ) exp
(
B(W )
H(W )
)
?
?
W lnW.
Interestingly, PP
1
is approximately expressed as
a simple elementary function of vocabulary size
W . This suggests that models learned from cor-
pora with the same vocabulary size theoretically
have the same perplexity.
For the test-set perplexity, we assume that both
the training corpus w and test corpus w
?
are gen-
erated from the same distribution based on Zipf?s
law. This assumption is natural, considering the
situation of an in-domain test or cross validation
test. Let w
?
? be the longest subsequence of w
?
such that for any w ? w
?
?
, w ? w
? holds. For-
mally, we assume p?(w) ? p
?
?
(w) for any w ? w?
?
when W
?
> W
?
, where p
?
? is the true distribu-
tion over w
?
?
. Using similar arguments to those
of Lemma 2 and Theorem 3 for w
?
, we obtain
an approximation formula for the test-set perplex-
ity, where we simply substitute W and W ? in the
exact formula for the training-set perplexity with
W
?
and W
?
?
, respectively. For simplicity, we will
only consider training-set perplexity from now on,
since we can make a similar argument for the test-
set perplexity in the later analysis.
3.2 Perplexity of k-gram Models
Here, we will consider the perplexity of a k-gram
model learned from a reduced corpus as a standard
extension of a unigram model. Our theory only
assumes that the corpus is generated on the basis
of Zipf?s law. Thus, we can use a simple model
where k-grams are calculated from a random word
sequence based on Zipf?s law. This model seems
to be stupid, since we can easily notice that the
bigram ?is is? is quite frequent, and the two bi-
grams ?is a? and ?a is? have the same frequency.
However, the experiments described later uncov-
ered the fact that the model can roughly capture
the behavior of real corpora.
The frequency f
k
of k-gram word w
k
? w
k in
the model is represented by the following formula:
f
k
(w
k
) =
C
k
g
k
(r
k
(w
k
))
,
where C
k
is the maximal frequency in k-grams, r
k
is the frequency ranking of w
k
over k-grams, and
g
k
expresses the frequency decay in k-grams. For
example, the decay function g
2
of bigrams is as
follows:
(g
2
(i))
i
:= (g
2
(1), g
2
(2), g
2
(3), ? ? ? )
= (1 ? 1, 1 ? 2, 2 ? 1, 1 ? 3, 3 ? 1, ? ? ? )
= (1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, ? ? ? ).
This is an inverse of the sum of Piltz?s divisor
functions d
2
(n) :=
?
i
1
?i
2
=n
1, which represents
the number of divisors of an integer n (cf. (OEIS,
2001)). In general, we formally define g
k
through
its inverse: g?1
k
(?) := S
k
(?), where S
k
(?) :=
?
?
n=1
d
k
(n) and d
k
(n) :=
?
i
1
?i
2
???i
k
=n
1. Since
(g
k
(i))
i
is a sorted sequence of the elements of the
k-th tensor power of vector (1, ? ? ? ,W ), we can
calculate the maximum frequency C
k
as follows.
800
Lemma 4. For any corpus w following Zipf?s law,
the maximum frequency of k-grams in our model
is calculated by
C
k
=
N ? (k ? 1)D
(H(W ))
k
,
where D denotes the number of documents in w.
Proof. We use?
w
k
f
k
(w
k
) = C
k
(
?
w
1/r(w))
k
.
The sum S
k
(?) of Piltz?s divisor functions can
be approximated by ?P
k
(ln ?), where P
k
(x) is a
polynomial of degree k ? 1 with respect to x,
and the main term of ?P
k
(ln ?) is given by the
following residue Res
s=1
?
k
(s)x
s
s
, where ?(s) is
the Riemann zeta function (Li, 2005). Using this
fact, we obtain an approximation ln (g?1
k
(?)) ?
ln ? + O(ln (ln ?)) ? ln ?, when ? is sufficiently
large. Thus, when the corpus is sufficiently large,
we can see that the behavior of f
k
is roughly linear
on a log-log graph, i.e., f
k
(w
k
) ? r
k
(w
k
)
?1
, since
if g?1
k
(?) ? ?
c holds, then f
k
(r) ? (g
k
(r))
?1
?
r
?
1
c holds.
Unfortunately, however, most corpora in the
real world are not so large that the above-
mentioned relation holds. Actually, Ha et al (Ha
et al, 2002; Ha et al, 2006) experimentally found
that although a k-gram corpus roughly follows a
power law even when k > 1, its exponent is
smaller than 1 (for Zipf?s law). They pointed out
that the exponent of bigrams is about 0.66, and
that of 5-grams is about 0.59 in the Wall Street
Journal corpus (WSJ87). Believing their claim
that there exists a constant ?
k
such that f
k
(w
k
) ?
r
k
(w
k
)
??
k , we estimated the exponent of k-grams
in an actual situation in the form of the following
lemma.
Lemma 5. Assuming that f
k
(w
k
) ? r
k
(w
k
)
??
k
holds for any k-gram word w
k
? w
k in a corpus
w following Zipf?s law, the optimal exponent in
our model based on the least squares criterion is
calculated by
?
k
=
lnW
(k ? 1) ln (lnW ) + lnW
.
Proof. We find the optimal exponent ?
k
by mini-
mizing the sum of squared errors between the gra-
dients of g?1
k
(r) and r
1
pi
k on a log-log graph:
?
{
?
?y
(y + lnP
k
(y))?
?
?y
(
1
?
k
y
)}
2
dy,
where y = ln r.
In the case of unigrams (k = 1), the formula
exactly represents Zipf?s law. In the case of k-
grams (k > 1), we found that the formula ap-
proaches Zipf?s law when W approaches infinity,
i.e., lim
W??
?
k
= 1.
Let us consider the perplexity of a k-gram
model learned from a reduced corpus. We im-
mediately obtain the following corollary using
Lemma 5.
Corollary 6. For any distribution p? on a k-gram
model learned from a corpus w? reduced from the
original corpus w following Zipf?s law, assuming
that f
k
(w
k
) ? r
k
(w
k
)
??
k holds for any k-gram
word w
k
? w
k and the optimal exponent ?
k
in
Lemma 5, the perplexity ?PP
k
of the ??-restored
distribution p? of p? from w? to w is calculated by
?PP
k
(W
?
) =H
?
k
(W ) exp
(
B
?
k
(W
?
)
H
?
k
(W )
)
(
W ?W
?
H
?
k
(W )?H
?
k
(W
?
)
)
1?
H
pi
k
(W
?
)
H
pi
k
(W )
,
where H
a
(X) :=
?
X
x=1
1
x
a
and B
a
(X) :=
?
X
x=1
a lnx
x
a
.
H
a
(X) is the X-th partial sum of the P-series
or hyper-harmonic series, which is a generaliza-
tion of the harmonic series H(X). B
a
(X) is the
X-th partial sum of the Bertrand series (another
special form of B(X)). When 0 < a < 1, we can
easily calculate ?PP
k
(W
?
) by using the following
approximations:
H
a
(X) ?
(X + 1)
1?a
? 1
1? a
B
a
(X) ?
a
1? a
(X + 1)
1?a
ln(X + 1)
?
a
(1? a)
2
(X + 1)
1?a
+
a
(1? a)
2
.
By putting the approximations of H
a
(X) and
B
a
(X) into the formula of Corollary 6, we ob-
tain an approximation ?PP
k
(W
?
) ? O(W
?
W
?
1?pi
k
).
This implies that ?PP
k
(W
?
) is approximately linear
on a log-log graph, when ?
k
is close to 1, i.e., k is
relatively small and W is sufficiently large. Note
that we must use the approximation of H(X), not
H
a
(X), when a = 1.
The fact that the frequency of k-grams follows
a power law leads us to an additional convenient
801
property, since the process of generating a cor-
pus in our theory can be treated as a variant of
the coupon collector?s problem. In this problem,
we consider how many trials are needed for col-
lecting all coupons whose occurrence probabilities
follow some stable distribution. According to a
well-known result about power law distributions
(Boneh and Papanicolaou, 1996), we need a cor-
pus of size kWk
1??
k
lnW when ?
k
< 1, and W ln2 W
when ?
k
= 1 for collecting all of the k-grams, the
number of which is W k. Using results in (Atso-
nios et al, 2011), we can easily obtain a lower and
upper bound of the actual vocabulary size ?W
k
of
k-grams from the corpus size N and vocabulary
size W as
?
W
k
? (?
k
+ 1)
(
1? e
?
(1?pi
k
)N
W
k
?1
?ln
W
k
?1
W
k
)
?
W
k
?
?
k
?
k
? 1
(
N
H
?
k
(W
k
)
)
1
pi
k
?
NW
1??
k
(?
k
? 1)H
?
k
(W
k
)
.
This means that we can determine the rough
sparseness of k-grams and adjust some of the pa-
rameters such as the gram size k in learning statis-
tical language models.
3.3 Perplexity of Topic Models
In this section, we consider the perplexity of the
widely used topic model, Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003), by using the nota-
tion given in (Griffiths and Steyvers, 2004). LDA
is a probabilistic language model that generates a
corpus as a mixture of hidden topics, and it allows
us to infer two parameters: the document-topic
distribution ? that represents the mixture rate of
topics in each document, and the topic-word dis-
tribution ? that represents the occurrence rate of
words in each topic. For a given corpus w, the
model is defined as
?
d
i
? Dirichlet(?)
z
i
|?
d
i
? Multi(?
d
i
)
?
z
i
? Dirichlet(?)
w
i
|z
i
, ?
z
i
? Multi(?
z
i
),
where d
i
and z
i
are respectively the document
that includes the i-th word w
i
and the hidden
topic that is assigned to w
i
. In the case of infer-
ence by Gibbs sampling presented in (Griffiths and
Steyvers, 2004), we can sample a ?good? topic as-
signment z
i
for each word w
i
with high probabil-
ity. Using the assignments z, we obtain the pos-
terior distributions of two parameters as ??
d
(z) ?
n
(d)
z
+ ? and ??
z
(w) ? n
(w)
z
+ ?, where n(d)
z
and
n
(w)
z
respectively represent the number of times
assigning topic z in document d and the number
of times topic z is assigned to word w.
Since an exact analysis is very hard, we will
place rough assumptions on ?? and ?? to reduce the
complexity. The assumption placed on ?? is that the
word distribution ??
z
of each topic z follows Zipf?s
law. We think this is acceptable since we can re-
gard each topic as a corpus that follows Zipf?s law.
Since ??
z
is normalized for each topic, we can as-
sume that for any two topics, z and z?, and any
two words, w and w?, ??
z
(w) ?
?
?
z
?
(w
?
) holds if
r
z
(w) = r
z
?
(w
?
), where r
z
(w) is the frequency
ranking of w with respect to n(w)
z
. Note that the
above assumption pertains to a posterior, and we
do not discuss the fact that a Pitman-Yor process
prior is better suited for a power law (Goldwater et
al., 2011).
The assumption placed on ?? may not be reason-
able in the case of ??, because we can easily think
of a document with only one topic, and we usu-
ally use a small number T of topics for LDA, e.g.,
T = 20. Thus, we consider two extreme cases.
One is where each document evenly has all topics,
and the other is where each document only has one
topic. Although these two cases might be unreal-
istic, the actual (theoretical) perplexity is expected
to be between their values. We believe that analyz-
ing such extreme cases is theoretically important,
since it would be useful for bounding the compu-
tational complexity and predictive performance.
We can regard the former case as a unigram
model, since the marginal predictive distribution
?
T
z=1
?
?
d
(z)
?
?
z
(w) ?
?
T
z=1
n
(w)
z
+?
T
?
?
f(w) is in-
dependent of d; here we have used ??
d
(z) = 1/T
from the assumption. In the latter case, we can
obtain an exact formula for the perplexity of LDA
when the topic assigned to each document follows
a discrete uniform distribution, as shown in the
next theorem. Note that a mixture of corpora fol-
lowing Zipf?s law can be approximately regarded
as following Zipf?s law, when W is sufficiently
large.
Theorem 7. For any distribution p? on the LDA
model with T topics learned from a corpus w? re-
duced from the original corpus w following Zipf?s
law, assuming that each document only has one
topic which is assigned based on a discrete uni-
form distribution, the perplexity ?PPMix of the ??-
restored distribution p? of p? from w? to w is calcu-
802
Table 1: Details of Reuters, 20news, Enwiki,
Zipf1, and ZipfMix.
vocab. size corpus size doc. size
Reuters 70,258 2,754,800 18,118
20news 192,667 4,471,151 19,997
Enwiki 409,902 16,711,226 51,231
Zipf1 69,786 2,754,800 18,118
ZipfMix 70,093 2,754,800 18,118
lated by
?PPMix(W ?) =H(W/T ) exp
(
B(W
?
/T )
H(W/T )
)
(
W ?W
?
H(W/T )?H(W
?
/T )
)
1?
H(W
?
/T )
H(W/T )
Proof. We can prove this by using a similar argu-
ment to that of Theorem 3 for each topic.
The formula of the theorem is nearly identical
to the one of Theorem 3 for a 1/T corpus. This
implies that the growth rate of the perplexity of
LDA models is larger than that of unigram mod-
els, whereas the perplexity of LDA models for
the original corpus is smaller than that of unigram
models. In fact, a similar argument to the one in
the approximate analysis in Section 3.1 leads to an
approximate formula ?PPMix of ?PPMix as
?PPMix(W ?) =
?
W
T
ln
W
T
exp
(lnW
?
? lnW )
2
2 ln (W/T )
,
when W is sufficiently large. That is, ?PPMix(W ?)
also has a quadratic behavior in a log-log graph,
i.e., ?PPMix(W ?) = O(W ?lnW
?
).
4 Experiments
We performed experiments on three real corpora
(Reuters, 20news, and Enwiki) and two syn-
thetic corpora (Zipf1 and ZipfMix) to verify
the correctness of our theory and to examine the
gap between theory and practice. Reuters and
20news here denote corpora extracted from the
Reuters-21578 and 20 Newsgroups data sets, re-
spectively. Enwiki is a 1/100 corpus of the En-
glish Wikipedia. Zipf1 is a synthetic corpus gen-
erated by Zipf?s law, whose corpus is the same size
as Reuters, and ZipfMix is a mixture of 20 syn-
thetic corpora, sizes are 1/20th of Reuters. We
used ZipfMix only for the experiments on topic
models. Table 1 lists the details of all five corpora.
Fig. 1(a) shows the word frequency of
Reuters, 20news, Enwiki, and Zipf1 versus
frequency ranking on a log-log graph. In all cor-
pora, we can regard each curve as linear with a
gradient close to 1. This means that all corpora
roughly follow Zipf?s law. Furthermore, since the
curve of Zipf1 is similar to that of Reuters,
Zipf1 can be regarded as acceptable.
Fig. 1(b) plots the perplexity of unigram mod-
els learned from Reuters, 20news, Enwiki,
and Zipf1 versus the size of reduced vocabu-
lary on a log-log graph. Each value is the aver-
age over different test sets of five-fold cross val-
idation. Theory1 is calculated using the for-
mula in Theorem 3. The graph shows that the
curve of Theory1 is nearly identical to that of
Zipf1. Since the vocabulary size W
?
of each test
set is small in this experiment, some errors appear
when W ? is large, i.e., W
?
< W
?
. This clearly
means that our theory is theoretically correct for
an ideal corpus Zipf1. Comparing Zipf1 with
Reuters, however, we find that their perplex-
ities are quite different. The reason is that the
gap between the frequencies of low-ranking (high-
frequency) words is considerably large. For ex-
ample, the frequency of the 1st-rank word of
Reuters is f(w) = 136, 371, while that of
Zipf1 is f(w) = 234, 705. Our theory seems to
be suited for inferring the growth rate of perplexity
rather than the perplexity value itself.
As for the approximate formula ?PP
1
of Theo-
rem 3, we can surely regard the curve of Zipf1
as being roughly quadratic. The curves of real
corpora also have a similar tendency, although
their gradients are slightly steeper. This difference
might have been caused by the above-mentioned
errors. However, at least, we can ascertain the
important fact that the results for the corpora re-
duced by 1/100 are not so different from those of
the original corpora from the perspective of their
perplexity measures.
Fig. 1(c) plots the frequency of k-grams (k ?
{1, 2, 3}) in Reuters versus frequency ranking
on a log-log graph. TheoryFreq (1-3) are calcu-
lated using C
k
in Lemma 4 and ?
k
in Lemma 5.
A comparison of TheoryFreq and Zipf verifies
the correctness of our theory. However, comparing
Zipf and Reuters, we see that C
k
is poorly es-
timated when the gram size is large, whereas ?
k
is
roughly correct. This may have happened because
we did not put any assumptions on the word se-
803
10
0
10
1
10
2
10
3
10
4
10
5
10
6
Frequency Ranking
10
0
10
1
10
2
10
3
10
4
10
5
10
6
10
7
F
r
e
q
u
e
n
c
y
Reuters
20news
Enwiki
Zipf1
(a) Frequency of unigrams
10
0
10
1
10
2
10
3
10
4
10
5
10
6
Reduced Vocabulary Size
10
3
10
4
10
5
T
e
s
t
-
s
e
t
 
P
e
r
p
l
e
x
i
t
y
Reuters
20news
Enwiki
Zipf1
Theory1
(b) Perplexity of unigram models
10
0
10
1
10
2
10
3
10
4
10
5
Frequency Ranking
10
1
10
2
10
3
10
4
10
5
F
r
e
q
u
e
n
c
y
Reuters
Zipf1
TheoryFreq1
Reuters2
Zipf2
TheoryFreq2
Reuters3
Zipf3
TheoryFreq3
(c) Frequency of k-grams
1 2 3 4 5 6 7 8 9 10
Gram Size
0.4
0.6
0.8
1.0
1.2
E
x
p
o
n
e
n
t
Reuters
TheoryExp
(d) Exponent of a power law over k-
grams
10
0
10
1
10
2
10
3
10
4
10
5
10
6
10
7
Reduced Vocabulary Size
10
2
10
3
10
4
10
5
10
6
T
e
s
t
-
s
e
t
 
P
e
r
p
l
e
x
i
t
y
Reuters
Zipf1
Theory1
Reuters2
Zipf2
Theory2
Reuters3
Zipf3
Theory3
(e) Perplexity of k-gram models
10
0
10
1
10
2
10
3
10
4
10
5
10
6
Reduced Vocabulary Size
10
3
10
4
10
5
T
e
s
t
-
s
e
t
 
P
e
r
p
l
e
x
i
t
y
Reuters
20news
Enwiki
Zipf1
Theory1
ZipfMix
TheoryMix
TheoryAve
(f) Perplexity of topic models
Figure 1: (a) Word frequency of Reuters, 20news, Enwiki, and Zipf1 versus frequency ranking.
(b) Perplexity of unigram models learned from Reuters, 20news, Enwiki, and Zipf1 versus size of
reduced vocabulary. Theory1 is calculated using the formula in Theorem 3. (c) Frequency of k-grams
(k ? {1, 2, 3}) in Reuters and Zipf1 versus frequency ranking. The suffix digit of each label means
its gram size. TheoryFreq (1-3) are calculated using Lemma 4 and Lemma 5. (d) Exponent of a power
law over k-grams in Reuters versus gram size. TheoryGrad is calculated using ?
k
in Lemma 5. (e)
Perplexity of k-gram models learned from Reuters versus size of reduced vocabulary. Theory2 and
Theory3 are calculated using the formula in Corollary 6. (f) Perplexity of topic models learned from
Reuters, 20news, Enwiki, Zipf1, and ZipfMix versus size of reduced vocabulary. TheoryMix is
calculated using the formula in Theorem 7.
quences in our simple model. The frequencies of
high-order k-grams tend to be lower than in real-
ity. We might need to place a hierarchical assump-
tion on the a power law, as in done in hierarchical
Pitman-Yor processes (Wood et al, 2011).
Fig. 1(d) plots the exponent of the power law
over k-grams in Reuters versus the gram size
on a normal graph. We estimated each exponent
of Reuters by using the least-squares method.
TheoryGrad is calculated using ?
k
in Lemma 5.
Surprisingly, the real exponents of Reuters are
almost the same as the theoretical estimate ?
k
based on our ?stupid? model that does not care
about the order of words. Note that we do not use
any information other than the vocabulary size W
and the gram size k for estimating ?
k
.
Fig. 1(e) plots the perplexity of k-gram mod-
els (k ? {1, 2, 3}) learned from Reuters versus
the size of reduced vocabulary on a log-log graph.
Theory2 and Theory3 are calculated using the
formula in Corollary 6. In the case of bigrams,
the perplexities of Theory2 are almost the same
as that of Zipf2 when the size of reduced vocab-
ulary is large. However, in the case of trigrams,
the perplexities of Theory3 are far from those of
Zipf3. This difference may be due to the sparse-
ness of trigrams in Zipf3. To verify the correct-
ness of our theory for higher order k-gram models,
we need to make assumptions that include backoff
and smoothing.
Fig. 1(f) plots the perplexity of LDA models
with 20 topics learned from Reuters, 20news,
Enwiki, Zipf1, and ZipfMix versus the size of
reduced vocabulary on a log-log graph. We used
a collapsed Gibbs sampler with 100 iterations to
infer the parameters and set the hyper parameters,
? = 0.1 and ? = 0.1. In evaluating the perplexity,
we estimated a posterior document-topic distribu-
804
Table 2: Computational time and memory size
for LDA learning on the original corpus, (1/10)-
reduced corpus, and (1/20)-reduced corpus of
Reuters.
corpus time memory perplexity
original 4m3.80s 71,548KB 500
(1/10) 3m55.70s 46,648KB 550
(1/20) 3m42.63s 34,024KB 611
tion ??
d
by using the first half of each test document
and calculated the perplexity on the second half,
as is done in (Asuncion et al, 2009). Each value
is the average over different test sets of five-fold
cross validation. Theory1 and TheoryMix
are calculated using the formulae in Theorem 3
and Theorem 7, respectively. Comparing Zipf1
with Theory1, and ZipfMix with TheoryMix,
we find that our theory of the extreme cases
discussed in Section 3.3 is theoretically cor-
rect. TheoryAve is the average of Theory1
and TheoryMix. Comparing Reuters and
TheoryAve, we see that their curves are almost
the same. If theoretical perplexity ?PP has a
similar tendency as real perplexity PP on a
log-log graph, i.e., ln PP(W ?) ? ln ?PP(W ?) + c
for some constant c, we can approximate
its deterioration rate as PP(W ?)/PP(W ) ?
exp (ln
?PP(W ?) + c)/ exp (ln ?PP(W ) + c) =
?PP(W ?)/ ?PP(W ). Therefore, we can use
TheoryAve as a heuristic function for estimat-
ing the perplexity of topic models. Since we
can calculate an inverse of TheoryAve from
the bisection or Newton-Raphson method, we
can maximize the reduction rate and ensure an
acceptable perplexity based on a user-specified
deterioration rate. According to the fact that the
three real corpora with different sizes have a
similar tendency, it is expected that we can use
our theory for a larger corpus.
Finally, let us examine the computational costs
for LDA learning. Table 2 shows computa-
tional time and memory size for LDA learning
on the original corpus, (1/10)-reduced corpus, and
(1/20)-reduced corpus of Reuters. Comparing
the memory used in the learning with the origi-
nal corpus and with the (1/10)-reduced corpus of
Reuters, we find that the learning on the (1/10)-
reduced corpus used 60% of the memory used by
the learning on the original corpus. While the
computational time decreased a little, we believe
that reducing the memory size helps to reduce
computational time for a larger corpus in the sense
that it can relax the constraint for in-memory com-
puting. Although we did not examine the accuracy
of real tasks in this paper, there is an interesting
report that the word error rate of language mod-
els follows a power law with respect to perplexity
(Klakow and Peters, 2002). Thus, we conjecture
that the word error rate also has a similar tendency
as perplexity with respect to the reduced vocabu-
lary size.
5 Conclusion
We studied the relationship between perplexity
and vocabulary size of reduced corpora. We de-
rived trade-off formulae for the perplexity of k-
gram models and topic models with respect to the
size of reduced vocabulary and showed that each
formula approximately has a simple behavior on a
log-log graph under certain conditions. We veri-
fied the correctness of our theory on synthetic cor-
pora and examined the gap between theory and
practice on real corpora. We found that the es-
timation of the perplexity growth rate is reason-
able. This means that we can maximize the reduc-
tion rate, thereby ensuring an acceptable perplex-
ity based on a user-specified deterioration rate.
Furthermore, this suggests the possibility that we
can theoretically derive empirical parameters, or
?rules of thumb?, for different NLP problems, as-
suming that a corpus follows Zipf?s law. We be-
lieve that our theoretical estimation has the advan-
tages of computational efficiency and scalability
especially for very large corpora, although exper-
imental estimations such as cross-validation may
be more accurate.
In the future, we want to find out the cause of
the gap between theory and practice and extend
our theory to bridge the gap, in the same way that
we can construct equations of motion with air re-
sistance in the example of the landing point of
a ball in Section 1. For example, promising re-
search directions include using a general law such
as the Zipf-Mandelbrot law (Mandelbrot, 1965), a
sophisticated model that cares the order of words
such as hierarchical Pitman-Yor processes (Wood
et al, 2011), and smoothing/backoff methods to
handle the sparseness problem.
Acknowledgments
The author would like to thank the reviewers for
their helpful comments.
805
References
Arthur Asuncion, Max Welling, Padhraic Smyth, and
Yee Whye Teh. 2009. On smoothing and infer-
ence for topic models. In Proceedings of the 25th
Conference on Uncertainty in Artificial Intelligence
(UAI2009), pages 27?34. AUAI Press.
Ioannis Atsonios, Olivier Beaumont, Nicolas Hanusse,
and Yusik Kim. 2011. On power-law distributed
balls in bins and its applications to view size esti-
mation. In Proceedings of the 22nd International
Symposium on Algorithms and Computation (ISAAC
2011), pages 504?513. Springer-Verlag.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Shahar Boneh and Vassilis G. Papanicolaou. 1996.
General asymptotic estimates for the coupon collec-
tor problem. Journal of Computational and Applied
Mathematics, 67(2):277?289.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large Language
Models in Machine Translation. In Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL2007), pages 858?867.
ACL.
Adam L. Buchsbaum, Raffaele Giancarlo, and Jef-
fery R. Westbrook. 1998. Shrinking Language
Models by Robust Approximation. In Proceed-
ings of the 1998 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP
1998), pages 685?688.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics
(ACL 1996), pages 310?318. ACL.
Ken Church, Ted Hart, and Jianfeng Gao. 2007. Com-
pressing Trigram Language Models with Golomb
Coding. In Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL
2007), pages 199?207. ACL.
Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J.
Newman. 2009. Power-Law Distributions in Em-
pirical Data. SIAM Review, 51(4):661?703.
Jianfeng Gao and Min Zhang. 2002. Improving Lan-
guage Model Size Reduction using Better Pruning
Criteria. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), pages 176?182. ACL.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2011. Producing Power-Law Distribu-
tions and Damping Word Frequencies with Two-
Stage Language Models. Journal of Machine Learn-
ing Research, 12:2335?2382.
Joshua Goodman and Jianfeng Gao. 2000. Lan-
guage Model Size Reduction by Pruning and Clus-
tering. In Proceedings of the 6th International
Conference on Spoken Language Processing (ICSLP
2000), pages 110?113. ISCA.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the National
Academy of Sciences of the United States of America
(PNAS 2004), volume 101, pages 5228?5235.
Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J.
Smith. 2002. Extension of Zipf?s Law to Words and
Phrases. In Proceedings of the 19th International
Conference on Computational Linguistics (COLING
2002), pages 1?6. ACL.
Le Quan Ha, P. Hanna, D. W. Stewart, and F. J. Smith.
2006. Reduced n-gram models for English and Chi-
nese corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, Proceedings of the Conference
(COLING-ACL 2006), pages 309?315. ACL.
Teemu Hirsimaki. 2007. On Compressing N-Gram
Language Models. In Proceedings of the 2007 IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP 2007), pages 949?952.
Dietrich Klakow and Jochen Peters. 2002. Testing the
correlation of word error rate and perplexity. Speech
Communication, 38(1):19?28.
Hailong Li. 2005. On Generalized Euler Constants
and an Integral Related to the Piltz Divisor Problem.
?Siauliai Mathematical Seminar, 8:81?93.
Benoit B. Mandelbrot. 1965. Information Theory
and Psycholinguistics: A Theory of Word Frequen-
cies. In Scientific Psychology: Principles and Ap-
proaches. Basic Books.
OEIS. 2001. The on-line encyclopedia of inte-
ger sequences (a061017). http://oeis.org/
A061017/.
Mark Steyvers and Tom Griffiths. 2007. Probabilis-
tic Topic Models. In Handbook of Latent Semantic
Analysis, pages 424?440. Lawrence Erlbaum Asso-
ciates.
Andreas Stolcke. 1998. Entropy-based Pruning of
Backoff Language Models. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274.
Frank Wood, Jan Gasthaus, Ce?dric Archambeau,
Lancelot James, and Yee Whye Teh. 2011. The Se-
quence Memoizer. Communications of the Associa-
tion for Computing Machines, 54(2):91?98.
George Kingsley Zipf. 1935. The Psychobiology of
Language. Houghton-Mifflin.
806
