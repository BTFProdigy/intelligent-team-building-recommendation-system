Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 59?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Visual Development Process for  
Automatic Generation of Digital Games Narrative Content  
 
Maria Fernanda Caropreso1 Diana Inkpen1 Shahzad Khan2 Fazel Keshtkar1 
 
1University of Ottawa 
{caropres,diana}@site.uottawa.ca 
akesh081@uottawa.ca 
2DISTIL Interactive 
s.khan2@distilinteractive.com 
 
Abstract 
Users of Natural Language Generation 
systems are required to have sophisti-
cated linguistic and sometimes even pro-
gramming knowledge, which has hin-
dered the adoption of this technology by 
individuals outside the computational 
linguistics research community. We have 
designed and implemented a visual envi-
ronment for creating and modifying NLG 
templates which requires no program-
ming ability and minimum linguistic 
knowledge. It allows specifying tem-
plates with any number of variables and 
dependencies between them. Internally, it 
uses SimpleNLG to provide the linguistic 
background knowledge. We tested the 
performance of our system in the context 
of an interactive simulation game. We 
describe the templates used for testing 
and show examples of sentences that our 
system generates from these templates. 
1 Introduction 
Natural Language Generation (NLG) is the proc-
ess of constructing outputs from non-linguistic 
inputs (Bateman, 2002) (Dalianis, 1996) (Reiter 
and Dale, 2000). 
NLG systems are useful in systems in which 
verbal or textual interaction with the users is re-
quired, as for example Gaming, Robotics, and 
Automatic Help Desks. Using NLG systems in-
stead of manually authored sentences would en-
able the software to adapt the expressed mes-
sages to the context of the conversation, and ex-
press past and future actions that may form this 
interaction. 
However, the use of the available NLG sys-
tems is far from simple. The most complete sys-
tems often require extensive linguistic knowl-
edge. Some systems also require programming 
knowledge. This knowledge cannot be assumed 
for the content and subject matter experts who 
are members of a development team. However, 
these individuals do need to interact with the 
NLG system in order to make use of the message 
generation capability to support their product 
development efforts. It is then necessary to pro-
vide them with an environment that will allow 
them to have access in a simpler way to the fea-
tures they need of a specific NLG system.  
There are two widely adopted approaches to 
NLG, the ?deep-linguistic? and the ?template-
based? (van Deemter et al, 2005). The deep-
linguistic approach attempts to build the sen-
tences up from a wholly logical representation. 
The template-based NLG systems provide scaf-
folding in the form of templates that contain a 
predefined structure and perhaps some of the 
final text.  
SimpleNLG is an NLG system that allows the 
user to specify a sentence by giving its content 
words and its grammatical roles (such as subject 
or verb). SimpleNLG also permits the user to 
specify several features for the main verb, such 
as: tense (present, past or future); whether or not 
it is subjective, progressive, passive or perfect; 
whether or not it is in interrogative form; wheth-
er or not it is negated; and which, if any, modal 
to use (i.e. could, must).While some of these fea-
tures affect only the verb, others affect the struc-
ture of the whole sentence, as for example when 
it has to be expressed in the passive voice.  
SimpleNLG is implemented as a java library 
and it requires java programming knowledge to 
be used. Because of the programming nature of 
SimpleNLG, it allows the user to define flexible 
templates by using programming variables in the 
sentence specification. The variable parts of the 
templates could be filled with different values. 
When templates are defined using SimpleNLG 
they keep all the functionality of the NLG system 
(for example, being able to modify the verb fea-
59
tures or the output format, and making use of the 
grammatical knowledge), while also allowing for 
the variable values to change. 
We have designed an environment that pro-
vides simple access to the use of the SimpleNLG 
system in order to generate sentences with vari-
able parts or templates. We developed this NLG 
Template Authoring Environment guided by the 
need of templates required for generating content 
for digital-based training games at DISTIL Inter-
active1. An early prototype of the tool, with a 
text-only interface, is presented in (Caropreso et 
al., 2009). 
In training games the player is typically pre-
sented with challenging situations and is encour-
aged to practice different strategies at dealing 
with them, in a safe, virtual environment. 
Through tips and feedback, the player develops 
an understanding of the problem and what are the 
successful ways of confronting it (French et al, 
1999). 
In training games there is usually an explosion 
of possible scenarios and situations. The narra-
tive should ideally reflect the past events and 
decisions taken. The considerable amount of tex-
tual information required in order to keep the 
feedback consistent with the updated narrative 
can be a burden on the game designers. It is then 
necessary to include templates that statically 
provide the basic information, combined with 
variable parts that adapt the narrative to the cir-
cumstances. 
The goal of the NLG Template Authoring En-
vironment was to provide the game content de-
signers with an accessible tool they could use to 
create and manipulate the NLG templates, and 
thus generate sentences that would support the 
narrative progression of the game.  
In the rest of this paper we describe our NLG 
Template Authoring Environment, its design, 
implementation and capabilities. We describe the 
templates that we used to test the system and we 
explain the user?s knowledge required in order to 
create them. We finish the paper presenting our 
conclusions and future work. 
2 Template Authoring Environment 
The NLG Template Authoring Environment 
asks for a model sentence and allows the user to 
mark the sections that are variable. For each va-
riable indicated, the user has to specify its type 
(i.e., personal pronoun, possessive pronoun, Em-
                                               
1
 http://www.distilinteractive.com/ 
ployee_type) and which values of that type are 
allowed (i.e., all personal pronouns, or only 
?she? and ?he?). Additionally, the user can also 
indicate dependencies between variable elements 
and information for the verb (i.e., tense, form, 
modals). The system then shows the user all the 
possible sentences that could be generated from 
the given template by calculating all the possible 
combinations of variable values that respect the 
specified dependencies and follow the verb se-
lections. The user can then refine the template by 
changing the given example or the specified va-
riables, dependencies or verb options, in order to 
adjust the generated sentences to the needs of the 
game.  
The NLG Template Authoring Environment 
has been implemented in Java. The SimpleNLG 
library was used to automatically generate cor-
rect sentences and provide the user with the pos-
sibility of exploring different attributes to the 
verb. It has a user-friendly intuitive graphical 
interface, part of which is shown in Figure 1. 
 
Figure 1: Graphical Interface 
 
 
After entering an example sentence and click-
ing on Analyze, the user indicates that a section 
is variable by giving a type or semantic class to 
the word in that section. The values of a semantic 
class are stored in a text file, which allows the 
user to create new semantic classes as needed. 
These files contain all the possible values and 
their respective syntactic information (person, 
number and gender) which will be used for 
agreement with the verb and for dependency be-
tween variables purposes. Restrictions to the val-
ues that a variable can take are also indicated 
60
through the graphical interface. Dependencies 
can be indicated only between already declared 
variables. The main verb and all its options are 
indicated in the section at the bottom of the 
graphical interface. 
In the template shown in Figure 1, the exam-
ple sentence is ?I walk my dog?, ?I? is a variable 
of type personal pronoun, ?walk? is the main 
verb, ?my? is a variable of type possessive pro-
noun, ?dog? is a variable of type animal and 
there is a dependency between ?I? and ?my? 
(which will allow to make their values agree in 
person, number and gender when generating all 
possible combinations). 
In Figure 1 we also see that the user has se-
lected the values ?present and past? for the verb 
tense and ?normal? and ?imperative? for the verb 
form. Therefore, four sentences will be generated 
for each combination of the variables? values 
(one sentence for each combination of the tense 
and form selections). All these sentences will 
have the verb negated and will use the perfect 
tenses (as indicated by the extra verb options). 
3 Testing the NLG Template Authoring 
Environment 
In order to verify the correct functioning of the 
NLG Template Authoring Environment, we se-
lected a set of sentence templates from the game 
?Business in Balance: Implementing an Envi-
ronmental Management System? from DISTIL 
Interactive. The templates were selected manu-
ally, while keeping in mind the need to cover 
different aspects, as for example the number and 
type of the variables and dependencies. The test-
ing of these examples covers for many more 
templates of the same type. The five selected 
sentence templates that form our testing set are 
displayed in Table 1 and are identified in the rest 
of this section by their reference number or order 
in the table.  
Table 1. Testing examples 
Ref.  
number 
Template 
1 The ACTORS (ME/US) could help 
DEPARTMENTS. 
2 The ACTORS IS/ARE now avail-
able to help. 
3 I/WE struggled because of 
MY/OUR lack of knowledge. 
4 I/WE AM/ARE pleased to report 
that I/WE completed the task 
TASKS. 
5 I/WE WAS/WERE not the great-
est choice for keeping things 
moving along quickly. 
 
In these template examples, we show in capi-
tals the variable parts of the templates. ACTORS, 
DEPARTMENTS and TASKS refer to one of several 
possible nouns previously defined for each of the 
classes with those names. The terms in capitals 
separated by a ?/? already display all the ac-
cepted values for that variable (for example 
I/WE represent a variable of type personal pro-
noun which could take only the selected values 
?I? or ?we? and the rest are filtered out). 
The first template example has two variables 
of predefined closed class nouns, ACTORS and 
DEPARTMENTS. The latter is independent, while 
the former has a dependency with a variable of 
type personal pronoun (in objective case form) 
that could only take the values ?me? or ?us?. 
This template is used in the game when the ac-
tor/character available to help is the same ac-
tor/character that is providing the information. 
This template can be successfully generated with 
our system by declaring the variables, restricting 
the values of the pronoun variable, and establish-
ing the dependency. When filtering non-valid 
sentences, the system will eliminate those cases 
where the value?s number of the variable ACTOR 
and the personal pronoun do not agree (i.e., it 
will only allow sentences that use ?me? if the 
actor is singular, and sentences that use ?us?, if 
the actor is plural). When creating this template, 
the user will have to be aware that the main verb 
is ?to help? and indicate ?could? as a modal to be 
used. This is important as otherwise SimpleNLG 
will modify the main verb in order to agree with 
the number of the subject. It is also necessary in 
case some of the options to change the main verb 
are specified. 
Two examples of the generated sentences us-
ing the first template are shown below. 
? The HR Training Manager (me) could 
help the Design Department. 
? The Implementation Team (us) could 
help the Deputy Management Represen-
tative. 
The second template is one that found a prob-
lem with our system and provided us with a rea-
son and an opportunity to improve it. This exam-
ple template also uses a variable of the closed 
class noun ACTOR together with the verb ?to be? 
in the present tense, agreeing in number with the 
actor. It might seem trivial to indicate this de-
pendency between the actor variable and the 
verb. But in our system the verbs are not treated 
as a regular variable (even when their values can 
be variable), but they are left for SimpleNLG to 
find the correct verb form. We needed then to 
61
inform SimpleNLG the number to which the 
verb should agree (by default it would assume 
singular). In this case we needed to inform Sim-
pleNLG that the number to agree with would be 
the number of the variable ACTOR. We also have 
to consider the case when the subject number 
does not depend on a variable and is plural, as 
for example in a template where the subject is 
?The members of DEPARTMENT?. To accom-
modate for these cases, we improved our system 
by asking the user to indicate in a pull down 
menu whether the template?s verb should agree 
with a variable value or it should be always used 
in plural or in singular. (This option is displayed 
in the bottom right corner of the interface and not 
shown in the partial screen shot on Figure 1.) 
The third template presents a dependency be-
tween a variable of type personal pronoun in the 
subjective case form, and a variable of type pos-
sessive pronoun in the complement. Both vari-
ables accept only a pair of their possible values, 
and the dependency between them establishes 
that they have to agree in person, number and 
gender. That is not a problem for our system. 
With respect to the verb, the user has to indicate 
the past tense as the only option.  
In the fourth and fifth template, there is a per-
sonal pronoun variable taking the place of the 
subject, which should agree in person and num-
ber with the verb. This is, as mentioned before, 
left to SimpleNLG to solve. As the subject in 
these cases consists of only a personal pronoun 
and SimpleNLG can detect this fact, no extra 
information is required. In the fourth template, 
there is also a dependency between the personal 
pronoun variable in the subject role and the per-
sonal pronoun variable in the complement. Once 
again the person and number of these two vari-
ables have to agree, and the sentences not satis-
fying this restriction are filtered out by our sys-
tem. Finally, for the fifth template the user is 
forced to specify that the verb ?to be? has to be 
used in its past tense. 
4 Conclusions and Future Work 
We have identified the need for an NLG Tem-
plate Authoring Environment that allows game 
content designers without linguistic and pro-
gramming background to experiment with and 
finally create language templates.  
We have designed and implemented a system 
that allows the user to specify an example sen-
tence together with variables, its dependencies, 
and verb options that complete the template. This 
system shows the user all the possible sentences 
that could be generated with the specified tem-
plate. It can be used to refine the template until it 
satisfies the user?s needs.  
The system makes use of the SimpleNLG java 
library which provides us with correct sentences 
and the possibility of including many verb varia-
tions, such as tense, form and modals. 
We have evaluated our NLG Template Au-
thoring Environment in a set of sentence tem-
plates from a digital-based interactive simulation 
game that covered different characteristics.  
We have implemented a user-friendly intuitive 
graphical interface for the system. The conven-
ience of use of this interface will be evaluated in 
the context of the development of a new game. 
Acknowledgements 
This work is supported by the Ontario Centres of 
Excellence (OCE) and Precarn Incorporated. 
References 
J. A. Bateman. 2002. Natural Language Generation: 
an introduction and open-ended review of the state 
of the art. 
M. F. Caropreso, D. Inkpen, S. Khan and F. Keshtkar. 
2009. Novice Friendly Natural Language Genera-
tion Template Authoring Environment. Proceeding 
of the Canadian Artificial Intelligence Conference 
2009, Kelowna, BC, pp.195-198. 
H. Dalianis. 1996. Concise Natural Language Genera-
tion from Formal Specifications, Ph.D. Thesis, 
(Teknologie Doktorsavhandling), Department of 
Computer and Systems Sciences, Royal Institute of 
Technology/ Stockholm University. Report Series 
No. 96-008, ISSN 1101-8526, SRN SU-
KTH/DSV/R 96/8 SE.  
K. van Deemter, E. Krahmer and M. Theune. 2005. 
Real versus Template-Based Natural Language 
Generation: A False Opposition? In Computational 
Linguistics, 31(1): 15-24.  
D. French, C. Hale, C. Johnson and G. Farr. 1999. 
Internet Based Learning: An introduction and 
framework for higher education and business. Lon-
don, UK: Kogan Page.  
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems (Studies in Natural 
Language Processing), Cambridge University 
Press. 
E. Reiter. 2007. SimpleNlg package: 
http://www.csd.abdn.ac.uk/ereiter/simplnlg 
62
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 35?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
A Corpus-based Method for Extracting Paraphrases of Emotion Terms
Fazel Keshtkar
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
akeshtka@site.uOttawa.ca
Diana Inkpen
University of Ottawa
Ottawa, ON, K1N 6N5, Canada
diana@site.uOttawa.ca
Abstract
Since paraphrasing is one of the crucial tasks
in natural language understanding and gener-
ation, this paper introduces a novel technique
to extract paraphrases for emotion terms, from
non-parallel corpora. We present a bootstrap-
ping technique for identifying paraphrases,
starting with a small number of seeds. Word-
Net Affect emotion words are used as seeds.
The bootstrapping approach learns extraction
patterns for six classes of emotions. We use
annotated blogs and other datasets as texts
from which to extract paraphrases, based on
the highest-scoring extraction patterns. The
results include lexical and morpho-syntactic
paraphrases, that we evaluate with human
judges.
1 Introduction
Paraphrases are different ways to express the same
information. Algorithms to extract and automati-
cally identify paraphrases are of interest from both
linguistic and practical points of view. Many ma-
jor challenges in Natural Language Processing ap-
plications, for example multi-document summariza-
tion, need to avoid repetitive information from the
input documents. In Natural Language Genera-
tion, paraphrasing is employed to create more var-
ied and natural text. In our research, we ex-
tract paraphrases for emotions, with the goal of us-
ing them to automatically-generate emotional texts
(such as friendly or hostile texts) for conversations
between intelligent agents and characters in educa-
tional games. Paraphrasing is applied to generate
text with more variety. To our knowledge, most cur-
rent applications manually collect paraphrases for
specific applications, or they use lexical resources
such as WordNet (Miller et al, 1993) to identify
paraphrases.
This paper introduces a novel method for ex-
tracting paraphrases for emotions from texts. We
focus on the six basic emotions proposed by Ek-
man (1992): happiness, sadness, anger, disgust,
surprise, and fear.
We describe the construction of the paraphrases
extractor. We also propose a k-window algorithm
for selecting contexts that are used in the paraphrase
extraction method. We automatically learn patterns
that are able to extract the emotion paraphrases from
corpora, starting with a set of seed words. We use
data sets such as blogs and other annotated cor-
pora, in which the emotions are marked. We use
a large collection of non-parallel corpora which are
described in Section 3. These corpora contain many
instances of paraphrases different words to express
the same emotion.
An example of sentence fragments for one
emotion class, happiness, is shown in Table 1. From
them, the paraphrase pair that our method will
extract is:
"so happy to see"
"very glad to visit".
In the following sections, we give an overview of
related work on paraphrasing in Section 2. In Sec-
tion 3 we describe the datasets used in this work.
We explain the details of our paraphrase extraction
method in Section 4. We present results of our evalu-
ation and discuss our results in Section 5, and finally
in Section 6 we present the conclusions and future
work.
35
his little boy was so happy to see him
princess and she were very glad to visit him
Table 1: Two sentence fragments (candidate contexts)
from the emotion class happy, from the blog corpus.
2 Related Work
Three main approaches for collecting paraphrases
were proposed in the literature: manual collection,
utilization of existing lexical resources, and corpus-
based extraction of expressions that occur in similar
contexts (Barzilay and McKeown, 2001). Manually-
collected paraphrases were used in natural language
generation (NLG) (Iordanskaja et al, 1991). Langk-
ilde et al (1998) used lexical resources in statistical
sentence generation, summarization, and question-
answering. Barzilay and McKeown (2001) used a
corpus-based method to identify paraphrases from a
corpus of multiple English translations of the same
source text. Our method is similar to this method,
but it extracts paraphrases only for a particular emo-
tion, and it needs only a regular corpus, not a parallel
corpus of multiple translations.
Some research has been done in paraphrase ex-
traction for natural language processing and genera-
tion for different applications. Das and Smith (2009)
presented a approach to decide whether two sen-
tences hold a paraphrase relationship. They ap-
plied a generative model that generates a paraphrase
of a given sentence, then used probabilistic infer-
ence to reason about whether two sentences share
the paraphrase relationship. In another research,
Wang et. al (2009) studied the problem of extract-
ing technical paraphrases from a parallel software
corpus. Their aim was to report duplicate bugs. In
their method for paraphrase extraction, they used:
sentence selection, global context-based and co-
occurrence-based scoring. Also, some studies have
been done in paraphrase generation in NLG (Zhao
et al, 2009), (Chevelu et al, 2009). Bootstrapping
methods have been applied to various natural lan-
guage applications, for example to word sense dis-
ambiguation (Yarowsky, 1995), lexicon construction
for information extraction (Riloff and Jones, 1999),
and named entity classification (Collins and Singer,
1999). In our research, we use the bootstrapping ap-
proach to learn paraphrases for emotions.
3 Data
The text data from which we will extract paraphrases
is composed of four concatenated datasets. They
contain sentences annotated with the six basic emo-
tions. The number of sentences in each dataset
is presented in Table 2. We briefly describe the
datasets, as follows.
3.1 LiveJournal blog dataset
We used the blog corpus that Mishne collected for
his research (Mishne, 2005). The corpus contains
815,494 blog posts from Livejournal 1, a free we-
blog service used by millions of people to create
weblogs. In Livejournal, users are able to option-
ally specify their current emotion or mood. To se-
lect their emotion/mood users can choose from a list
of 132 provided moods. So, the data is annotated
by the user who created the blog. We selected only
the texts corresponding to the six emotions that we
mentioned.
3.2 Text Affect Dataset
This dataset (Strapparava and Mihalcea, 2007) con-
sists of newspaper headlines that were used in the
SemEval 2007-Task 14. It includes a development
dataset of 250 annotated headlines, and a test dataset
of 1000 news headlines. We use all of them. The an-
notations were made with the six basic emotions on
intensity scales of [-100, 100], therefore a threshold
is used to choose the main emotion of each sentence.
3.3 Fairy Tales Dataset
This dataset consists in 1580 annotated sentences
(Alm et al, 2005), from tales by the Grimm brothers,
H.C. Andersen, and B. Potter. The annotations used
the extended set of nine basic emotions of Izard
(1971). We selected only those marked with the six
emotions that we focus on.
3.4 Annotated Blog Dataset
We also used the dataset provided by Aman and Sz-
pakowicz (2007). Emotion-rich sentences were se-
lected from personal blogs, and annotated with the
six emotions (as well as a non-emotion class, that
we ignore here). They worked with blog posts and
collected directly from the Web. First, they prepared
1http://www.livejournalinc.com
36
Dataset Happiness Sadness Anger Disgust Surprise Fear
LiveJournal 7705 1698 4758 1191 1191 3996
TextAffect 334 214 175 28 131 166
Fairy tales 445 264 216 217 113 165
Annotated blog dataset 536 173 115 115 172 179
Table 2: The number of emotion-annotated sentences in each dataset.
Figure 1: High-level view of the paraphrase extraction
method.
a list of seed words for six basic emotion categories
proposed by Ekman (1992). Then, they took words
commonly used in the context of a particular emo-
tion. Finally, they used the seed words for each
category, and retrieved blog posts containing one or
more of those words for the annotation process.
4 Method for Paraphrase Extraction
For each of the six emotions, we run our method
on the set of sentences marked with the correspond-
ing emotion from the concatenated corpus. We
start with a set of seed words form WordNet Af-
fect (Strapparava and Valitutti, 2004), for each emo-
tion of interest. The number of seed words is the fol-
lowing: for happiness 395, for surprise 68, for fear
140, for disgust 50, for anger 250, and for sadness
200. Table 3 shows some of seeds for each category
of emotion.
Since sentences are different in our datasets
and they are not aligned as parallel sentences as
in (Barzilay and McKeown, 2001), our algorithm
constructs pairs of similar sentences, based on the
local context. On the other hand, we assume that,
if the contexts surrounding two seeds look similar,
then these contexts are likely to help in extracting
new paraphrases.
Figure 1 illustrates the high-level architecture of
our paraphrase extraction method. The input to the
method is a text corpus for a emotion category and
a manually defined list of seed words. Before boot-
strapping starts, we run the k-window algorithm on
every sentence in the corpus, in order to construct
candidate contexts. In Section 4.5 we explain how
the bootstrapping algorithm processes and selects
the paraphrases based on strong surrounding con-
texts. As it is shown in Figure 1, our method has
several stages: extracting candidate contexts, using
them to extract patterns, selecting the best patterns,
extracting potential paraphrases, and filtering them
to obtain the final paraphrases.
4.1 Preprocessing
During preprocessing, HTML and XML tags are
eliminated from the blogs data and other datasets,
then the text is tokenized and annotated with part
of speech tags. We use the Stanford part-of-speech
tagger and chunker (Toutanova et al, 2003) to iden-
tify noun and verb phrases in the sentences. In the
next step, we use a sliding window based on the
k-window approach, to identify candidate contexts
that contain the target seeds.
4.2 The k-window Algorithm
We use the k-window algorithm introduced by
Bostad (2003) in order to identify all the tokens
surrounding a specific term in a window with
size of ?k. Here, we use this approach to ex-
tract candidate patterns for each seed, from the
sentences. We start with one seed and truncate
all contexts around the seed within a window of
?k words before and ?k words after the seed,
until all the seeds are processed. For these exper-
iments, we set the value of k to ?5. Therefore
37
Happiness: avidness, glad, warmheartedness, exalt, enjoy, comforting, joviality, amorous, joyful,
like, cheer, adoring, fascinating, happy, impress, great, satisfaction, cheerful, charmed, romantic, joy,
pleased, inspire, good, fulfill, gladness, merry
Sadness: poor, sorry, woeful, guilty, miserable, glooming, bad, grim, tearful, glum, mourning, joyless,
sadness, blue, rueful, hamed, regret, hapless, regretful, dismay, dismal, misery, godforsaken, oppression,
harass, dark, sadly, attrition
Anger: belligerence, envious, aggravate, resentful, abominate, murderously, greedy, hatred, disdain,
envy, annoy, mad, jealousy, huffiness, sore, anger, harass, bother, enraged, hateful, irritating, hostile,
outrage, devil, irritate, angry
Disgust: nauseous, sicken, foul, disgust, nausea, revolt, hideous, horror, detestable, wicked, repel,
offensive, repulse, yucky, repulsive, queasy, obscene, noisome
Surprise: wondrous, amaze, gravel, marvel, fantastic, wonderful, surprising, marvelous, wonderment,
astonish, wonder, admiration, terrific, dumfounded, trounce
Fear: fearful, apprehensively, anxiously, presage, horrified, hysterical, timidity, horrible, timid,
fright, hesitance, affright, trepid, horrific, unassertive, apprehensiveness, hideous, scarey, cruel, panic,
scared, terror, awful, dire, fear, dread, crawl, anxious, distrust, diffidence
Table 3: Some of the seeds from WordNet Affect for each category of emotion.
the longest candidate contexts will have the form
w1, w2, w3, w4, w5, seed, w6, w7, w8, w9, w10, w11.
In the next subsection, we explain what features we
extract from each candidate context, to allow us to
determine similar contexts.
4.3 Feature Extraction
Previous research on word sense disambiguation on
contextual analysis has acknowledged several local
and topical features as good indicators of word prop-
erties. These include surrounding words and their
part of speech tags, collocations, keywords in con-
texts (Mihalcea, 2004). Also recently, other fea-
tures have been proposed: bigrams, named entities,
syntactic features, and semantic relations with other
words in the context.
We transfer the candidate phrases extracted by the
sliding k-window into the vector space of features.
We consider features that include both lexical and
syntactic descriptions of the paraphrases for all pairs
of two candidates. The lexical features include the
sequence of tokens for each phrase in the paraphrase
pair; the syntactic feature consists of a sequence of
part-of-speech (PoS) tags where equal words and
words with the same root and PoS are marked.
For example, the value of the syntactic feature for
the pair ??so glad to see?? and ??very
happy to visit?? is ?RB1 JJ1 TO V B1?
and ?RB1 JJ2 TO V B2?, where indices indicate
Candidate context: He was further annoyed by the jay bird
?PRP VBD RB VBN IN DT NN NN?,65,8,?VBD RB?,?,was,
?,?,?,He/PRP,was/VBD,further/RB,annoyed,by/IN,the/DT,
jay/NN,bird/NN,?,?,jay,?,?IN DT NN?,2,2,0,1
Table 4: An example of extracted features.
word equalities. However, based on the above ev-
idences and our previous research, we also investi-
gate other features that are well suited for our goal.
Table 5 lists the features that we used for paraphrase
extraction. They include some term frequency fea-
tures. As an example, in Table 4 we show extracted
features from a relevant context.
4.4 Extracting Patterns
From each candidate context, we extracted the fea-
tures as described above. Then we learn extraction
patterns, in which some words might be substituted
by their part-of-speech. We use the seeds to build
initial patterns. Two candidate contexts that con-
tain the same seed create one positive example. By
using each initial seed, we can extract all contexts
surrounding these positive examples. Then we se-
lect the stronger ones. We used Collins and Singer
method (Collins and Singer, 1999) to compute the
strength of each example. If we consider x as a con-
text, the strength as a positive example of x is de-
38
Features Description
F1 Sequence of part-of-speech
F2 Length of sequence in bytes
F3 Number of tokens
F4 Sequence of PoS between the seed and the first verb before the seed
F5 Sequence of PoS between the seed and the first noun before the seed
F6 First verb before the seed
F7 First noun before the seed
F8 Token before the seed
F9 Seed
F10 Token after the seed
F11 First verb after the seed
F12 First noun after the seed
F13 Sequence of PoS between the seed and the first verb after the seed
F14 Sequence of PoS between the seed and the first noun after the seed
F15 Number of verbs in the candidate context
F16 Number of nouns in the candidate context
F17 Number of adjective in the candidate context
F18 Number of adverbs in the candidate context
Table 5: The features that we used for paraphrase extraction.
fined as:
Strength(x) = count(x+)/count(x) (1)
In Equation 1, count(x+) is the number of times
context x surrounded a seed in a positive example
and count(x) is frequency of the context x. This
allows us to score the potential pattern.
4.5 Bootstrapping Algorithm for Paraphrase
Extraction
Our bootstrapping algorithm is summarized in Fig-
ure 2. It starts with a set of seeds, which are consid-
ered initial paraphrases. A set of extraction patterns
is initially empty. The algorithm generates candidate
contexts, from the aligned similar contexts. The can-
didate patterns are scored by how many paraphrases
they can extract. Those with the highest scores are
added to the set of extraction patterns. Using the ex-
tended set of extraction patterns, more paraphrase
pairs are extracted and added to the set of para-
phrases. Using the enlarged set of paraphrases, more
extraction patterns are extracted. The process keeps
iterating until no new patterns or no new paraphrases
are learned.
Our method is able to accumulate a large lexi-
con of emotion phrases by bootstrapping from the
manually initialized list of seed words. In each it-
eration, the paraphrase set is expanded with related
phrases found in the corpus, which are filtered by
using a measure of strong surrounding context sim-
ilarity. The bootstrapping process starts by select-
ing a subset of the extraction patterns that aim to
extract the paraphrases. We call this set the pattern
pool. The phrases extracted by these patterns be-
come candidate paraphrases. They are filtered based
on how many patterns select them, in order to pro-
duce the final paraphrases from the set of candidate
paraphrases.
5 Results and Evaluation
The result of our algorithm is a set of extraction pat-
terns and a set of pairs of paraphrases. Some of the
paraphrases extracted by our system are shown in
Table 6. The paraphrases that are considered correct
are shown under Correct paraphrases. As explained
in the next section, two human judges agreed that
these are acceptable paraphrases. The results con-
sidered incorrect by the two judges are shown un-
39
Algorithm 1: Bootstrapping Algorithm.
For each seed for an emotion
Loop until no more paraphrases or no more contexts are learned.
1- Locate the seeds in each sentence
2- Find similar contexts surrounding a pair of two seeds
3- Analyze all contexts surrounding the two seeds to extract
the strongest patterns
4- Use the new patterns to learn more paraphrases
Figure 2: Our bootstrapping algorithm for extracting paraphrases.
der Incorrect paraphrases. Our algorithm learnt 196
extraction patterns and produced 5926 pairs of para-
phrases. Table 7 shows the number of extraction pat-
terns and the number of paraphrase pairs that were
produced by our algorithm for each class of emo-
tions. For evaluation of our algorithm, we use two
techniques. One uses human judges to judge if a
sample of paraphrases extracted by our method are
correct; we also measures the agreement between
the judges (See Section 5.1). The second estimates
the recall and the precision of our method (See Sec-
tion 5.2. In the following subsections we describe
these evaluations.
5.1 Evaluating Correctness with Human
Judges
We evaluate the correctness of the extracted para-
phrase pairs, using the same method as Brazilay and
McKeown (2001). We randomly selected 600 para-
phrase pairs from the lexical paraphrases produced
by our algorithm: for each class of emotion we
selected 100 paraphrase pairs. We evaluated their
correctness with two human judges. They judged
whether the two expressions are good paraphrases
or not.
We provided a page of guidelines for the judges.
We defined paraphrase as ?approximate conceptual
equivalence?, the same definition used in (Barzilay
and McKeown, 2001). Each human judge had to
choose a ?Yes? or ?No? answer for each pair of para-
phrases under test. We did not include example sen-
tences containing these paraphrases. A similar Ma-
chine Translation evaluation task for word-to-word
translation was done in (Melamed, 2001).
Figure 3 presents the results of the evaluation: the
correctness for each class of emotion according to
judge A, and according to judge B. The judges were
graduate students in computational linguistics, na-
tive speakers of English.
We also measured the agreement between the two
judges and the Kappa coefficient (Siegel and Castel-
lan, 1988). If there is complete agreement between
two judges Kappa is 1, and if there is no agreement
between the judges then Kappa = 0. The Kappa
values and the agreement values for our judges are
presented in Figure 4.
The inter-judge agreement over all the para-
phrases for the six classes of emotions is 81.72%,
which is 490 out of the 600 paraphrases pairs in our
sample. Note that they agreed that some pairs are
good paraphrases, or they agreed that some pairs
are not good paraphrases, that is why the numbers
in Figure 4 are higher than the correctness numbers
from Figure 3. The Kappa coefficient compensates
for the chance agreement. The Kappa value over
all the paraphrase pairs is 74.41% which shows a
significant agreement.
Figure 3: The correctness results according the judge A
and judge B, for each class of emotion.
5.2 Estimating Recall
Evaluating the Recall of our algorithm is difficult
due to following reasons. Our algorithm is not able
to cover all the English words; it can only detect
40
Disgust
Correct paraphrases:
being a wicked::getting of evil; been rather sick::feeling rather nauseated;
feels somewhat queasy::felt kind of sick; damn being sick::am getting sick
Incorrect paraphrases:
disgusting and vile::appealing and nauseated; get so sick::some truly disgusting
Fear
Correct paraphrases:
was freaking scared::was quite frightened; just very afraid::just so scared;
tears of fright::full of terror; freaking scary::intense fear;
Incorrect paraphrases:
serious panic attack::easily scared; not necessarily fear::despite your fear
Anger
Correct paraphrases:
upset and angry::angry and pissed; am royally pissed::feeling pretty angry;
made me mad::see me angry; do to torment::just to spite
Incorrect paraphrases:
very pretty annoying::very very angry; bitter and spite::tired and angry
Happiness
Correct paraphrases:
the love of::the joy of; in great mood::in good condition;
the joy of::the glad of; good feeling::good mood
Incorrect paraphrases:
as much eagerness::as many gladness; feeling smart::feel happy
Sadness
Correct paraphrases:
too depressing::so sad; quite miserable::quite sorrowful;
strangely unhappy::so misery; been really down::feel really sad
Incorrect paraphrases:
out of pity::out of misery; akward and depressing::terrible and gloomy
Surprise
Correct paraphrases:
amazement at::surprised by; always wonder::always surprised;
still astounded::still amazed; unexpected surprise::got shocked
Incorrect paraphrases:
passion and tremendous::serious and amazing; tremendous stress::huge shock
Table 6: Examples of paraphrases extracted by our algorithm (correctly and incorrectly).
41
Class of Emotion # Paraphrases Pairs # Extraction Patterns
Disgust 1125 12
Fear 1004 31
Anger 670 47
Happiness 1095 68
Sadness 1308 25
Surprise 724 13
Total 5926 196
Table 7: The number of lexical and extraction patterns produced by the algorithm.
Figure 4: The Kappa coefficients and the agreement be-
tween the two human judges.
paraphrasing relations with words which appeared
in our corpus. Moreover, to compare directly with
an electronic thesaurus such as WordNet is not fea-
sible, because WordNet contains mostly synonym
sets between words, and only a few multi-word ex-
pressions. We decided to estimate recall manually,
by asking a human judge to extract paraphrases by
hand from a sample of text. We randomly selected
60 texts (10 for each emotion class) and asked the
judge to extract paraphrases from these sentences.
For each emotion class, the judge extracted expres-
sions that reflect the emotion, and then made pairs
that were conceptually equivalent. It was not feasi-
ble to ask a second judge to do the same task, be-
cause the process is time-consuming and tedious.
In Information Retrieval, Precision and Recall are
defined in terms of a set of retrieved documents and
a set of relevant documents 2. In the following sec-
tions we describe how we compute the Precision and
Recall for our algorithm compared to the manually
2http://en.wikipedia.org/wiki/
Category of Emotions Precision Recall
Disgust 82.33% 92.91%
Fear 82.64% 88.20%
Anger 93.67% 80.57%
Happiness 82.00% 90.89%
Sadness 82.00% 89.88%
Surprise 79.78% 89.50%
Average 84.23% 88.66%
Table 8: Precision and Recall for a sample of texts, for
each category of emotion, and their average.
extracted paraphrases.
From the paraphrases that were extracted by the
algorithm from the same texts, we counted how
many of them were also extracted by the human
judge. Equation 2 defines the Precision. On av-
erage, from 89 paraphrases extracted by the algo-
rithm, 74 were identified as paraphrases by the hu-
man judge (84.23%). See Table 8 for the values for
all the classes.
P =
#Correctly Retrieved Paraphrases by the Algorithm
All Paraphrases Retrieved by the Algorithm
(2)
For computing the Recall we count how many of
the paraphrases extracted by the human judge were
correctly extracted by the algorithm (Equation 3).
R =
#Correctly Retrieved Paraphrases by the Algorithm
All Paraphrases Retrieved by the Human Judge
(3)
5.3 Discussion and Comparison to Related
Work
To the best of our knowledge, no similar research
has been done in extracting paraphrases for emotion
terms from corpora. However, Barzilay and McKe-
own (2001) did similar work to corpus-based iden-
42
tification of general paraphrases from multiple En-
glish translations of the same source text. We can
compare the pros and cons of our method compared
to their method. The advantages are:
? In our method, there is no requirement for the
corpus to be parallel. Our algorithm uses the
entire corpus together to construct its boot-
strapping method, while in (Barzilay and McK-
eown, 2001) the parallel corpus is needed in or-
der detect positive contexts.
? Since we construct the candidate contexts
based on the k-window approach, there is no
need for sentences to be aligned in our method.
In (Barzilay and McKeown, 2001) sentence
alignment is essential in order to recognize
identical words and positive contexts.
? The algorithm in (Barzilay and McKeown,
2001) has to find positive contexts first, then
it looks for appropriate patterns to extract para-
phrases. Therefore, if identical words do not
occur in the aligned sentences, the algorithm
fails to find positive contexts. But, our al-
gorithm starts with given seeds that allow us
to detect positive context with the k-window
method.
A limitation of our method is the need for the initial
seed words. However, obtaining these seed words
is not a problem nowadays. They can be found in
on line dictionaries, WordNet, and other lexical re-
courses.
6 Conclusion and Future Work
In this paper, we introduced a method for corpus-
based extraction of paraphrases for emotion terms.
We showed a method that used a bootstrapping tech-
nique based on contextual and lexical features and
is able to successfully extract paraphrases using a
non-parallel corpus. We showed that a bootstrapping
algorithm based on contextual surrounding context
features of paraphrases achieves significant perfor-
mance on our data set.
In future work, we will extend this techniques to
extract paraphrases from more corpora and for more
types of emotions. In terms of evaluation, we will
use the extracted paraphrases as features in machine
learning classifiers that classify candidate sentences
into classes of emotions. If the results of the classifi-
cation are good, this mean the extracted paraphrases
are of good quality.
References
Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.
2005. Emotions from text: machine learning for text-
based emotion prediction. In Proceedings of the Hu-
man Language Technology Conference Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP 2005).
Saima Aman and Stan Szpakowicz. 2007. Identifying
expressions of emotion in text. In TSD, pages 196?
205.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceeding
of ACL/EACL, 2001, Toulouse.
Thorstein Bostad. 2003. Sentence Based Automatic Sen-
timent Classification. Ph.D. thesis, University of Cam-
bridge, Computer Speech Text and Internet Technolo-
gies (CSTIT), Computer Laboratory, Jan.
Jonathan Chevelu, Thomas Lavergne, Yves Lepage, and
Thierry Moudenc. 2009. Introduction of a new para-
phrase generation tool based on Monte-Carlo sam-
pling. In Proceedings of ACL-IJCNLP 2009, Singa-
pore, pages 249?25.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora.
Dipanjan Das and Noah A. Smith. 2009. Paraphrase
identification as probabilistic quasi-synchronous
recognition. In Proceedings of ACL-IJCNLP 2009,
Singapore, pages 468?476.
Paul Ekman. 1992. An argument for basic emotions.
Cognition and Emotion, 6:169?200.
L. Iordanskaja, Richard Kittredget, and Alain Polguere,
1991. Natural Language Generation in Artificial In-
telligence and Computational Linguistics. Kluwer
Academic.
Carroll E. Izard. 1971. The Face of Emotion. Appleton-
Century-Crofts., New York.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL.
Ilya Dan Melamed. 2001. Empirical Methods for Ex-
ploiting Parallel Texts. MIT Press.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Natural Language
Learning (CoNLL 2004), Boston, May.
43
George Miller, Richard Beckwith, Christiane Fellbaum,
Derek Gross, and Katherine Miller, 1993. Introduc-
tion to Wordnet: An On-Line Lexical Database. Cog-
nitive Science Laboratory, Princeton University, Au-
gust.
Gilad Mishne. 2005. Experiments with mood classifica-
tion in blog posts. ACM SIGIR.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence, page 10441049.
The AAAI Press/MIT Press.
Sidney Siegel and John Castellan, 1988. Non Parametric
Statistics for Behavioral Sciences. . McGraw-Hill.
Carlo Strapparava and Rada Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of the 4th
International Workshop on the Semantic Evaluations
(SemEval 2007), Prague, Czech Republic, June 2007.
Carlo Strapparava and Alessandro Valitutti. 2004.
Wordnet-affect: an affective extension of wordnet. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC 2004),
Lisbon, May 2004, pages 1083?1086.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of HLT-NAACL, pages 252?259.
Xiaoyin Wang, David Lo, Jing Jiang, Lu Zhang, and
Hong Mei. 2009. Extracting paraphrases of tech-
nical terms from noisy parallel software corpora. In
Proceedings of ACL-IJCNLP 2009, Singapore, pages
197?200.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics, pages 189?196.
Shiqi Zhao, Xiang Lan, Ting Liu, , and Sheng Li.
2009. Application-driven statistical paraphrase gen-
eration. In Proceedings of ACL-IJCNLP 2009, Singa-
pore, pages 834?842.
44
