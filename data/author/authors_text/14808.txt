Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852?1857,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Event Role Extraction using Domain-Relevant Word Representations
Emanuela Boros?
??
Romaric Besanc?on
?
Olivier Ferret
?
Brigitte Grau
??
?
CEA, LIST, Vision and Content Engineering Laboratory, F-91191, Gif-sur-Yvette, France
?
LIMSI, rue John von Neumann, Campus Universitaire d?Orsay, F-91405 Orsay cedex
?
ENSIIE, 1 square de la r?esistance F-91025
?
Evry cedex
firstname.lastname@cea.fr firstname.lastname@limsi.fr
Abstract
The efficiency of Information Extraction
systems is known to be heavily influenced
by domain-specific knowledge but the cost
of developing such systems is consider-
ably high. In this article, we consider the
problem of event extraction and show that
learning word representations from unla-
beled domain-specific data and using them
for representing event roles enable to out-
perform previous state-of-the-art event ex-
traction models on the MUC-4 data set.
1 Introduction
In the Information Extraction (IE) field, event ex-
traction constitutes a challenging task. An event
is described by a set of participants (i.e. at-
tributes or roles) whose values are text excerpts.
The event extraction task is related to several sub-
tasks: event mention detection, candidate role-
filler extraction, relation extraction and event tem-
plate filling. The problem we address here is the
detection of role-filler candidates and their associ-
ation with specific roles in event templates. For
this task, IE systems adopt various ways of ex-
tracting patterns or generating rules based on the
surrounding context, local context and global con-
text (Patwardhan and Riloff, 2009). Current ap-
proaches for learning such patterns include boot-
strapping techniques (Huang and Riloff, 2012a;
Yangarber et al., 2000), weakly supervised learn-
ing algorithms (Huang and Riloff, 2011; Sudo et
al., 2003; Surdeanu et al., 2006), fully supervised
learning approaches (Chieu et al., 2003; Freitag,
1998; Bunescu and Mooney, 2004; Patwardhan
and Riloff, 2009) and other variations. All these
methods rely on substantial amounts of manually
annotated corpora and use a large body of lin-
guistic knowledge. The performance of these ap-
proaches is related to the amount of knowledge
engineering deployed and a good choice of fea-
tures and classifiers. Furthermore, the efficiency
of the system relies on the a priori knowledge of
the applicative domain (the nature of the events)
and it is generally difficult to apply a system on
a different domain with less annotated data with-
out reconsidering the design of the features used.
An important step forwards is TIER
light
(Huang
and Riloff, 2012a) that targeted the minimization
of human supervision with a bootstrapping tech-
nique for event roles detection. Also, PIPER (Pat-
wardhan and Riloff, 2007; Patwardhan, 2010) dis-
tinguishes between relevant and irrelevant regions
and learns domain-relevant extraction patterns us-
ing a semantic affinity measure. Another possi-
ble approach for dealing with this problem is to
combine the use a restricted set of manually anno-
tated data with a much larger set of data extracted
in an unsupervised way from a corpus. This ap-
proach was experimented for relations in the con-
text of Open Information Extraction (Soderland et
al., 2010) but not for extracting events and their
participants to our knowledge.
In this paper, we propose to approach the task
of labeling text spans with event roles by auto-
matically learning relevant features that requires
limited prior knowledge, using a neural model to
induce semantic word representations (commonly
referred as word embeddings) in an unsupervised
fashion, as in (Bengio et al., 2006; Collobert and
Weston, 2008). We exploit these word embed-
dings as features for a supervised event role (mul-
ticlass) classifier. This type of approach has been
proved efficient for numerous tasks in natural lan-
guage processing, including named entity recog-
nition (Turian et al., 2010), semantic role label-
ing (Collobert et al., 2011), machine translation
(Schwenk and Koehn, 2008; Lambert et al., 2012),
word sense disambiguation (Bordes et al., 2012) or
sentiment analysis (Glorot et al., 2011; Socher et
al., 2011) but has never been used, to our knowl-
1852
edge, for an event extraction task. Our goal is two-
fold: (1) to prove that using as only features word
vector representations makes the approach com-
petitive in the event extraction task; (2) to show
that these word representations are scalable and
robust when varying the size of the training data.
Focusing on the data provided in MUC-4 (Lehnert
et al., 1992), we prove the relevance of our ap-
proach by outperforming state-of-the-art methods,
in the same evaluation environment as in previous
works.
2 Approach
In this work, we approach the event extraction task
by learning word representations from a domain-
specific data set and by using these representa-
tions to identify the event roles. This idea relies
on the assumption that the different words used
for a given event role in the text share some se-
mantic properties, related to their context of use
and that these similarities can be captured by spe-
cific representations that can be automatically in-
duced from the text, in an unsupervised way. We
then propose to rely only on these word repre-
sentations to detect the event roles whereas, in
most works (Riloff, 1996; Patwardhan and Riloff,
2007; Huang and Riloff, 2012a; Huang and Riloff,
2012b), the role fillers are represented by a set
of different features (raw words, their parts-of-
speech, syntactic or semantic roles in the sen-
tence).
Furthermore, we propose two additional contri-
butions to the construction of the word representa-
tions. The first one is to exploit limited knowledge
about the event types (seed words) to improve the
learning procedure by better selecting the dictio-
nary. The second one is to use a max operation
1
on
the word vector representations in order to build
noun phrase representations (since slot fillers are
generally noun phrases), which represents a better
way of aggregating the semantic information born
by the word representations.
2.1 Inducing Domain-Relevant Word
Representations
In order to induce the domain-specific word rep-
resentations, we project the words into a 50-
dimensional word space. We chose a single
1
This max operation consists in taking, for each compo-
nent of the vector, the max value of this component for each
word vector representation.
layer neural network (NN) architecture that avoids
strongly engineered features, assumes little prior
knowledge about the task, but is powerful enough
to capture relevant domain information. Follow-
ing (Collobert et al., 2011), we use an NN which
learns to predict whether a given text sequence
(short word window) exists naturally in the consid-
ered domain. We represent an input sequence of n
words as ?w
i
? = ?w
i?(n/2)
. . . , w
i
, . . . w
i+(n/2)
?.
The main idea is that each sequence of words in
the training set should receive a higher score than
a sequence in which one word is replaced with
a random one. We call the sequence with a ran-
dom word corrupted (
?
?w
i
?) and denote as correct
(?w
i
?) all the sequences of words from the data
set. The goal of the training step is then to min-
imize the following loss function for a word w
i
in the dictionary D: C
w
i
=
?
w
i
?D
max(0, 1 ?
g(?w
i
?)+g(
?
?w
i
?)), where g(?) is the scoring func-
tion given by the neural network. Further details
and evaluations of these embeddings can be found
in (Bengio et al., 2003; Bengio et al., 2006; Col-
lobert and Weston, 2008; Turian et al., 2010). For
efficiency, words are fed to our architecture as in-
dices taken from a finite dictionary. Obviously,
a simple index does not carry much useful infor-
mation about the word. So, the first layer of our
network maps each of these word indices into a
feature vector, by a lookup table operation. Our
first contribution intervenes in the process of the
choosing the proper dictionary. (Bengio, 2009)
has shown that the order of the words in the dic-
tionary of the neural network is not indifferent to
the quality of the achieved representations: he pro-
posed to order the dictionary by frequency and se-
lect the words for the corrupted sequence accord-
ing to this order. In our case, the most frequent
words are not always the most relevant for the task
of event role detection. Since we want to have a
training more focused to the domain specific task,
we chose to order the dictionary by word relevance
to the domain. We accomplish this by considering
a limited number of seed words for each event type
that needs to be discovered in text (e.g. attack,
bombing, kidnapping, arson). We then rate with
higher values the words that are more similar to the
event types words, according to a given semantic
similarity, and we rank them accordingly. We use
the ?Leacock Chodorow? similarity from Word-
net 3.0 (Leacock and Chodorow, 1998). Initial ex-
perimental results proved that using this domain-
1853
oriented order leads to better performance for the
task than the order by frequency.
2.2 Using Word Representations to Identify
Event Roles
After having generated for each word their vec-
tor representation, we use them as features for the
annotated data to classify event roles. However,
event role fillers are not generally single words but
noun phrases that can be, in some cases, identi-
fied as named entities. For identifying the event
roles, we therefore apply a two-step strategy. First,
we extract the noun chunks using SENNA
2
parser
(Collobert et al., 2011; Collobert, 2011) and we
build a representation for these chunks defined as
the maximum, per column, of the vector represen-
tations of the words it contains. Second, we use
a statistical classifier to recognize the slot fillers,
using this representation as features. We chose
the extra-trees ensemble classifier (Geurts et al.,
2006), which is a meta estimator that fits a num-
ber of randomized decision trees (extra-trees) on
various sub-samples of the data set and use averag-
ing to improve the predictive accuracy and control
over-fitting.
3 Experiments and Results
3.1 Task Description
We conducted the experiments on the official
MUC-4 training corpus that consists of 1,700 doc-
uments and instantiated templates for each doc-
ument. The task consists in extracting informa-
tion about terrorist events in Latin America from
news articles. We classically considered the fol-
lowing 4 types of events: attack, bombing, kid-
napping and arson. These are represented by tem-
plates containing various slots for each piece of
information that should be extracted from the doc-
ument (perpetrators, human targets, physical tar-
gets, etc). Following previous works (Huang and
Riloff, 2011; Huang and Riloff, 2012a), we only
consider the ?String Slots? in this work (other slots
need different treatments) and we group certain
slots to finally consider the five slot types PerpInd
(individual perpetrator), PerpOrg (organizational
perpetrator), Target (physical target), Victim (hu-
man target name or description) and Weapon (in-
strument id or type). We used 1,300 documents
(DEV) for training, 200 documents (TST1+TST2)
2
Code and resources can be found at http://ml.
nec-labs.com/senna/
for tuning, and 200 documents (TST3+TST4) as
the blind test set. To compare with similar works,
we do not evaluate the template construction and
only focus on the identification of the slot fillers:
for each answer key in a reference template, we
check if we find it correctly with our extraction
method, using head noun matching (e.g., the vic-
tim her mother Martha Lopez Orozco de Lopez is
considered to match Matha Lopez), and merging
duplicate extractions (so that different extracted
slot fillers sharing the same head noun are counted
only once). We also took into account the answer
keys with multiple values in the reference, deal-
ing with conjunctions (when several victims are
named, we need to find all of them) and disjunc-
tions (when several names for the same organiza-
tion are possible, we need to find any of them).
Our results are reported as Precision/Recall/F1-
score for each event role separately and averaged
on all roles.
3.2 Experiments
In all the experiments involving our model, we es-
tablished the following stable choices of parame-
ters: 50-dimensional vectors obtained by training
on sequences of 5 words, which is consistent with
previous studies (Turian et al., 2010; Collobert
and Weston, 2008). All the hyper-parameters of
our model (e.g. learning rate, size of the hidden
layer, size of the word vectors) have been chosen
by finetuning our event extraction system on the
TST1+TST2 data set. For DRVR-50 and W2V-50,
the embeddings were built from the whole training
corpus (1,300 documents) and the dictionary was
made of all the words of this corpus under their
inflected form.
We used the extra-trees ensemble classifier im-
plemented in (Pedregosa et al., 2011), with hyper-
parameters optimized on the validation data: for-
est of 500 trees and the maximum number of
features to consider when looking for the best
split is
?
number features. We present a 3-
fold evaluation: first, we compare our system with
state-of-the-art systems on the same task, then we
compare our domain-relevant vector representa-
tions (DRVR-50) to more generic word embed-
dings (C&W50, HLBL-50)
3
and finally to another
3
C&W-50 are described in (Collobert and Weston,
2008), HLBL-50 are the Hierarchical log-bilinear embed-
dings (Mnih and Hinton, 2007), provided by (Turian et
al., 2010), available at http://metaoptimize.com/
projects/wordreprs induced from the Reuters-RCV1
1854
State-of-the-art systems
PerpInd PerpOrg Target Victim Weapon Average
(Riloff, 1996) 33/49/40 53/33/41 54/59/56 49/54/51 38/44/41 45/48/46
(Patwardhan and Riloff, 2007) 39/48/43 55/31/40 37/60/46 44/46/45 47/47/47 44/36/40
(Patwardhan and Riloff, 2009) 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
(Huang and Riloff, 2011) 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
(Huang and Riloff, 2012a) 47/51/47 60/39/47 37/65/47 39/53/45 53/55/54 47/53/50
(Huang and Riloff, 2012b) 54/57/56 55/49/51 55/68/61 63/59/61 62/64/63 58/60/59
Models based on word embeddings
C&W-50 80/55/65 64/65/64 76/72/74 53/63/57 85/64/73 68/63/65
HLBL-50 81/53/64 63/67/65 78/72/75 53/63/58 93/64/75 69/62/66
W2V-50 79/57/66 88/71/79 74/72/73 69/75/71 97/65/78 77/68/72
DRVR-50 79/57/66 91/74/81 79/57/66 77/75/76 92/58/81 80/67/73
Table 1: Accuracy of ?String Slots? on the TST3 + TST4 test set P/R/F1 (Precision/Recall/F1-Score)
word representation construction on the domain-
specific data (W2V-50)
4
.
Figure 1: F1-score results for event role labeling
on MUC-4 data, for different size of training data,
of ?String Slots? on the TST3+TST4 with differ-
ent parameters, compared to the learning curve of
TIER (Huang and Riloff, 2012a). The grey points
represent the performances of other IE systems.
Figure 1 presents the average F1-score results,
computed over the slots PerpInd, PerpOrg, Tar-
get, Victim and Weapon. We observe that mod-
els relying on word embeddings globally outper-
form the state-of-the-art results, which demon-
strates that the word embeddings capture enough
semantic information to perform the task of event
newswire corpus
4
W2V-50 are the embeddings induced from the MUC4
data set using the negative sampling training algorithm
(Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et
al., 2013c), available at https://code.google.com/
p/word2vec/
role labeling on ?String Slots? without using any
additional hand-engineered features. Moreover,
our representations (DRVR-50) clearly surpass the
models based on generic embeddings (C&W-50
and HLBL-50) and obtain better results than W2V-
50, based the competitive model of (Mikolov et
al., 2013a), even if the difference is small. We
can also note that the performance of our model
is good even with a small amount of training data,
which makes it a good candidate to easily develop
an event extraction system on a new domain.
Table 1 provides a more detailed analysis of the
comparative results. We can see in this table that
our results surpass those of previous systems (0.73
vs. 0.59) with, particularly, a consistently higher
precision on all roles, whereas recall is smaller for
certain roles (Target and Weapon). To further ex-
plore the impact of these representations, we com-
pared our word embeddings with other word em-
beddings (C&W-50, HLBL-50) and report the re-
sults in Figure 1 and Table 1. The results show
that our model also outperforms the models using
others word embeddings (F1-score of 0.73 against
0.65, 0.66). This proves that a model learned
on a domain-specific data set does indeed pro-
vide better results, even if its size is much smaller
(whereas it is usually considered that neural mod-
els require often important training data). Finally,
we also achieve slightly better results than W2V-50
with other word representations built on the same
corpus, which shows that the choices made for the
word representation construction, such as the use
of domain information for word ordering, tend to
have a positive impact.
1855
4 Conclusions and Perspectives
We presented in this paper a new approach for
event extraction by reducing the features to only
use unsupervised word representations and a small
set of seed words. The word embeddings induced
from a domain-specific corpus bring improvement
over state-of-art models on the standard MUC-
4 corpus and demonstrate a good scalability on
different sizes of training data sets. Therefore,
our proposal offers a promising path towards eas-
ier and faster domain adaptation. We also prove
that using a domain-specific corpus leads to bet-
ter word vector representations for this task than
using other publicly-available word embeddings
(even if they are induced from a larger corpus).
As future work, we will reconsider the archi-
tecture of the neural network and we will refo-
cus on creating a deep learning model while tak-
ing advantage of a larger set of types of infor-
mation such as syntactic information, following
(Levy and Goldberg, 2014), or semantic informa-
tion, following (Yu and Dredze, 2014).
References
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137?1155.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastian
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
DawnE. Holmes and LakhmiC. Jain, editors, Inno-
vations in Machine Learning, volume 194 of Studies
in Fuzziness and Soft Computing, pages 138?186.
Springer Berlin Heidelberg.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and trends in Machine Learning,
2(1).
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2012. Joint learning of words
and meaning representations for open-text seman-
tic parsing. In Fifteenth International Conference
on Artificial Intelligence and Statistics (AISTATS
2012), pages 127?135.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In 42nd Annual Meeting on As-
sociation for Computational Linguistics (ACL-04),
pages 438?445.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In 41st international Annual Meeting on Association
for Computational Linguistics (ACL-2003), pages
216?223.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In 25th In-
ternational Conference of Machine learning (ICML-
08), pages 160?167. ACM.
Ronan Collobert, Jason Weston, L?eon Battou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient
discriminative parsing. In 14th International Con-
ference on Artificial Intelligence and Statistics (AIS-
TATS 2011).
Dayne Freitag. 1998. Information extraction from
HTML: Application of a general machine learning
approach. In AAAI?98, pages 517?523.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3?42.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale senti-
ment classification: A deep learning approach. In
28th International Conference on Machine Learning
(ICML-11), pages 513?520.
Ruihong Huang and Ellen Riloff. 2011. Peeling back
the layers: Detecting event role fillers in secondary
contexts. In ACL 2011, pages 1137?1147.
Ruihong Huang and Ellen Riloff. 2012a. Bootstrapped
training of event extraction classifiers. In 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL 2012), pages
286?295.
Ruihong Huang and Ellen Riloff. 2012b. Modeling
textual cohesion for event extraction. In 26th Con-
ference on Artificial Intelligence (AAAI 2012).
Patrik Lambert, Holger Schwenk, and Fr?ed?eric Blain.
2012. Automatic translation of scientific documents
in the hal archive. In LREC 2012, pages 3933?3936.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and Wordnet similarity for word
sense identification. In Christiane Fellbaum, edi-
tor, WordNet: An electronic lexical database., pages
265?283. MIT Press.
Wendy Lehnert, Claire Cardie, David Fisher, John Mc-
Carthy, Ellen Riloff, and Stephen Soderland. 1992.
University of Massachusetts: MUC-4 test results
and analysis. In 4th Conference on Message under-
standing, pages 151?158.
1856
Omer Levy and Yoav Goldberg. 2014. Dependency-
based word embeddings. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2014), Short Papers, pages 302?308, Bal-
timore, Maryland, June.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In International Conference
on Learning Representations (ICLR 20013), work-
shop track.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26 (NIPS 2013), pages 3111?3119.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In NAACL-HLT 2013, pages
746?751.
Andriy Mnih and Geoffrey Hinton. 2007. Three
new graphical models for statistical modelling. In
24th International Conference of Machine learning
(ICML 2007), pages 641?648. ACM.
Siddharth Patwardhan and Ellen Riloff. 2007. Ef-
fective information extraction with semantic affinity
patterns and relevant regions. In 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL 2007), pages 717?727.
Siddharth Patwardhan and Ellen Riloff. 2009. A uni-
fied model of phrasal and sentential evidence for in-
formation extraction. In 2009 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2009), pages 151?160.
Siddharth Patwardhan. 2010. Widening the field of
view of information extraction through sentential
event recognition. Ph.D. thesis, University of Utah.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In AAAI?96, pages
1044?1049.
Holger Schwenk and Philipp Koehn. 2008. Large
and diverse language models for statistical machine
translation. In IJCNLP 2008, pages 661?666.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011. Parsing natural scenes and nat-
ural language with recursive neural networks. In
28th International Conference on Machine Learning
(ICML-11), pages 129?136.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93?102.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representa-
tion model for automatic ie pattern acquisition. In
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL-03), pages 224?231.
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno.
2006. A hybrid approach for the acquisition of
information extraction patterns. In EACL-2006
Workshop on Adaptive Text Extraction and Mining
(ATEM 2006), pages 48?55.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In 48th international
Annual Meeting on Association for Computational
Linguistics (ACL 2010), pages 384?394.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition
of domain knowledge for information extraction. In
18th Internation Conference on Computational Lin-
guistics (COLING 2000), pages 940?946.
Mo Yu and Mark Dredze. 2014. Improving lexical em-
beddings with semantic knowledge. In 52nd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2014), Short Papers, pages 545?550,
Baltimore, Maryland, June.
1857
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 189?195,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Sentimatrix ? Multilingual Sentiment Analysis Service 
 
Alexandru-Lucian G?nsc?1, Emanuela Boro?1, Adrian Iftene1, Diana Trandab??1,  
Mihai Toader2, Marius Cor?ci2, Cenel-Augusto Perez1, Dan Cristea1, 3 
1
?Al. I. Cuza? University, Faculty of Computer Science, Iasi, Romania 
2Intelligentics, Cluj-Napoca, Romania 
3Institute of Computer Science, Romanian Academy, Iasi, Romania 
{lucian.ginsca, emanuela.boros, adiftene, dtrandabat, augusto.perez, 
dcristea}@info.uaic.ro, {mtoader, marius}@intelligentics.ro 
 
 
Abstract 
This paper describes the preliminary results 
of a system for extracting sentiments 
opinioned with regard with named entities. 
It also combines rule-based classification, 
statistics and machine learning in a new 
method. The accuracy and speed of 
extraction and classification are crucial. 
The service oriented architecture permits 
the end-user to work with a flexible 
interface in order to produce applications 
that range from aggregating consumer 
feedback on commercial products to 
measuring public opinion on political 
issues from blog and forums. The 
experiment has two versions available for 
testing, one with concrete extraction results 
and sentiment calculus and the other with 
internal metrics validation results. 
1 Motivation 
Nowadays, big companies and organizations spend 
time and money in order to find users? opinions 
about their products, the impact of their marketing 
decisions, or the overall feeling about their support 
and maintenance services. This analysis helps in 
the process of establishing new trends and policies 
and determines in which areas investments must be 
made. One of the focuses of our work is helping 
companies build such analysis in the context of 
users? sentiment identification. Therefore, the 
corpus we work on consists of articles of 
newspapers, blogs, various entries of forums, and 
posts in social networks. 
Sentiment analysis, i.e. the analysis and 
classification of the opinion expressed by a text on 
its subject matter, is a form of information 
extraction from text, which recently focused a lot 
of research and growing commercial interest. 
This paper describes Sentimatrix, a sentiment 
analysis service, doing sentiment extraction and 
associating these analyses with named entities, in 
different languages. We seek to explore how 
sentiment analysis methods perform across 
languages, especially Romanian. The main 
applications that this system experiments with are 
monitoring the Internet before, during and after a 
campaign/message release and obtaining consumer 
feedback on different topics/products. 
In Section 2 we briefly discuss a state of the art 
in sentiment analysis, the system?s architecture is 
described in Section 3 and in Section 4 we focus 
on identifying opinions on Romanian. 
Subsequently, we present the experiment results, 
analysis and discussion in Sections 5 and 6. Future 
work and conclusions are briefly described in 
Section 7. 
2 Sentimatrix compared with state-of-
the-art 
A comprehensive state of the art in the field of 
sentiment analysis, together with potential 
applications of such opinion identification tools, is 
presented in (Pang and Lee, 2008). 
Starting from the early 1990s, the research on 
sentiment-analysis and point of views generally 
assumed the existence of sub-systems for rather 
sophisticated NLP tasks, ranging from parsing to 
the resolution of pragmatic ambiguities (Hearst, 
1992; Wiebe 1990 and 1994). In Sentimatrix, in 
order to identify the sentiment a user expresses 
about a specific product or company, the company 
name must be first identified in the text. Named 
189
entity recognition (NER) systems typically use 
linguistic grammar-based techniques or statistical 
models (an overview is presented in (Nadeau and 
Satoshi Sekine. 2007)). Hand-crafted grammar-
based systems typically obtain better precision, but 
at the cost of lower recall and months of work by 
experienced computational linguists. Besides, the 
task is hard to adapt to new domains. Various 
sentiment types and levels have been considered, 
starting from the ?universal? six level of emotions 
considered in (Ovesdotter Alm, 2005; Liu et al, 
2003; Subasic and Huettner, 2001): anger, disgust, 
fear, happiness, sadness, and surprise. For 
Sentimatrix, we adapted this approach to five 
levels of sentiments: strong positive, positive, 
neutral, negative and strong negative.  
The first known systems relied on relatively 
shallow analysis based on manually built 
discriminative word lexicons (Tong 2001), used to 
classify a text unit by trigger terms or phrases 
contained in a lexicon. The lack of sufficient 
amounts of sentiment annotated corpora led the 
researchers to incorporate learning components 
into their sentiment analysis tools, usually 
supervised classification modules, (e.g., 
categorization according to affect), as initiated in 
(Wiebe and Bruce 1995). 
Much of the literature on sentiment analysis has 
focused on text written in English. Sentimatrix is 
designed to be, as much as possible, language 
independent, the resources used being easily 
adaptable for any language. 
Some of the most known tools available 
nowadays for NER and Opinion Mining are: 
Clarabridge (www.clarabridge.com), RavenPack 
(ravenpack.com), Lexalytics (www.lexalytics.com) 
OpenAmplify (openamplify.com), Radian6 
(www.radian6.com), Limbix (lymbix.com), but 
companies like Google, Microsoft, Oracle, SAS, 
are also deeply involved in this task. 
3 System components 
In Figure 1, the architecture and the main modules 
of our system are presented: preprocessing, named 
entity extraction and opinion identification 
(sentiment extraction per fragment).  
The final production system is based on service 
oriented architecture in order to allow users 
flexible customization and to enable an easier way 
for marketing technology. Each module of the 
system (Segmenter, Tokenizer, Language Detector, 
Entity Extractor, and Sentiment Extractor) can be 
exposed in a user-friendly interface.  
 
Figure 1. System architecture 
3.1 Preprocessing 
The preprocessing phase is made out of a text 
segmentator and a tokenizer. Given a text, we 
divide it into paragraphs, every paragraph is split 
into sentences, and every phrase is tokenized. Each 
token is annotated with two pieces of information: 
its lemma (for Romanian it is obtained from our 
resource with 76,760 word lemmas corresponding 
to 633,444 derived forms) and the normalized form 
(translated into the proper diacritics1). 
3.2 Language Detection 
Language detection is a preprocessing step 
problem of classifying a sample of characters 
based on its features (language-specific models). 
Currently, the system supports English, Romanian 
and Romanian without Diacritics. This step is 
needed in order to correctly identify a sentiment or 
a sentiment modifier, as the named entity detection 
depends on this. We combined three methods for 
                                                          
1
 In Romanian online texts, two diacritics are commonly used, 
but only one is accepted by the official grammar. 
190
identifying the language: N-grams detection, 
strictly 3-grams detection and lemma correction.  
The 3-grams classification method uses corpus 
from Apache Tika for several languages. The 
Romanian 3-gram profile for this method was 
developed from scratch, using our articles archive. 
The language detection in this case performs 
simple distance measurement between every 
language profile that we have and the test 
document profile. The N-grams classification 
method implies, along with computing frequencies, 
a posterior Naive Bayes implementation. The third 
method solves the problematic issue of short 
phrases language detection and it implies looking 
through the lemmas of several words to obtain the 
specificity of the test document. 
3.3 Named Entity Recognition 
The Named Entity Recognition component for 
Romanian language is created using linguistic 
grammar-based techniques and a set of resources. 
Our component is based on two modules, the 
named entity identification module and the named 
entity classification module. After the named entity 
candidates are marked for each input text, each 
candidate is classified into one of the considered 
categories, such as Person, Organization, Place, 
Country, etc. 
 
Named Entity Extraction: After the pre-
processing step, every token written with a capital 
letter is considered to be a named entity candidate.  
For tokens with capital letters which are the first 
tokens in phrases, we consider two situations:  
1. this first token of a phrase is in our stop word 
list (in this case we eliminate it from the 
named entities candidate list),  
2. the first token of a phrase is in our common 
word list. In the second situation there are 
considered two cases:  
a. this common word is followed by lowercase 
words (then we check if the common word 
can be found in the list of trigger words, like 
university, city, doctor, etc.),  
b. this common word is followed by uppercase 
words (in this case the first word of the 
sentence is kept in the NEs candidate list, 
and in a further step it will be decided if it 
will be combined with the following word in 
order to create a composed named entity).  
Named Entities Classification: In the 
classification process we use some of rules utilized 
in the unification of NEs candidates along with the 
resource of NEs and several rules specifically 
tailored for classification. Thus, after all NEs in the 
input text are identified and, if possible, compound 
NEs have been created, we apply the following 
classification rules: contextual rules (using 
contextual information, we are able to classify 
candidate NEs in one of the categories 
Organization, Company, Person, City and Country 
by considering a mix between regular expressions 
and trigger words) and resource-based rules (if no 
triggers were found to indicate what type of entity 
we have, we start searching our databases for the 
candidate entity). 
 
Evaluation: The system?s Upper Bound and its 
performance in real context are evaluated for each 
of the two modules (identification and 
classification) and for each named entity type. The 
first part of the evaluation shows an upper bound 
of 95.76% for F-measure at named entity 
extraction and 95.71% for named entity 
classification. In real context the evaluation shows 
a value of 90.72% for F-measure at named entity 
extraction and a value of 66.73% for named entity 
classification. The results are very promising, and 
they are being comparable with the existing 
systems for Romanian, and even better for Person 
recognition. 
4 Identify users opinions on Romanian 
4.1 Resources 
In such a task as sentiment identification, linguistic 
resources play a very important role. The core 
resource is a manually built list of words and 
groups of words that semantically signal a positive 
or a negative sentiment. From now on, we will 
refer to such a word or group of words as 
?sentiment trigger?. Certain weights have been 
assigned to these words after multiple revisions. 
The weights vary from -3, meaning strong negative 
to +3, which translates to a strong positive. There 
are a total of 3,741 sentiment triggers distributed to 
weight groups as can be observed in Figure 2. The 
triggers are lemmas, so the real number of words 
that can be identified as having a sentiment value 
is much higher. 
191
This list is not closed and it suffers modifications, 
especially by adding new triggers, but in certain 
cases, if a bad behavior is observed, the weights 
may also be altered.  
 
 
Figure 2. Number of sentiment words by weight groups 
 
We define a modifier as a word or a group of 
words that can increase or diminish the intensity of 
a sentiment trigger. We have a manually built list 
of modifiers. We consider negation words a special 
case of modifiers that usually have a greater impact 
on sentiment triggers. So, we also built a small list 
of negation words.   
4.2 Formalism 
General definitions: We define a sentiment 
segment as follows: 
  = (	
, , 		) 
 
sSG is a tuple in which the first two elements are 
optional.  
Let NL be the set of negation words that we use, 
ML the set of modifiers and TL the set of sentiment 
triggers.  We define two partially ordered sets: 
  = (, ?), 