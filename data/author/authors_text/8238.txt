Squibs and Discussions 
Nonminimal Derivations in Unification-based 
Parsing 
Noriko Tomuro* 
DePaul University 
Steven L. Lytinen t 
DePaul University 
Shieber's abstract parsing algorithm (Shieber 1992)for unification grammars is an extension of 
Earley's algorithm (Earley 1970)for context-free grammars to feature structures. In this paper, 
we show that, under certain conditions, Shieber ' salgorithm produces what we call a nonminimal 
derivation: aparse tree which contains additional features that are not in the licensing productions. 
While Shieber's definition of parse tree allows for such nonminimal derivations, we claim that they 
should be viewed as invalid. We describe the sources of the nonminimal derivation problem, and 
propose aprecise definition of minimal parse tree, as well as a modification to Shieber's algorithm 
which ensures minimality, although at some computational cost. 
1. Introduction 
Unification grammar is a term often used to describe a family of feature-based gram- 
mar formalisms, including GPSG (Gazdar et al 1985), PATR-II (Shieber 1986), DCG 
(Pereira and Warren 1980), and HPSG (Pollard and Sag 1994). In an effort to formalize 
the common elements of unification-style grammars, Shieber (1992) developed a logic 
for describing them, and used this logic to define an abstract parsing algorithm. The 
algorithm uses the same set of operations as Earley's (1970) algorithm for context-free 
grammars, but modified for unification grammars. 
In this paper, we show that, under certain conditions, Shieber's algorithm produces 
unintended, spurious parses in addition to the intended ones. We call these spurious 
parses nonminimal derivations (or nonminimal parse trees), because they contain 
extra features which are not in the productions that license the parse, aWe claim that 
such nonminimal derivations are invalid. The basis of our claim is that the unifica- 
tion operation as set union preserves minimality; thus any correct unification-based 
parsing algorithm should produce parses that contain all and only features from the 
licensing productions (i.e., minimal derivations or minimal parse trees). Nonminimal 
derivations are also undesirable in practice because, given a parse tree, we cannot ell 
whether a particular feature should be in the model or not unless we reconstruct the 
whole tree. 
Despite the nonminimal derivations, Shieber (1992) proved the correctness of his 
algorithm. As it turned out, his definition of parse tree, which his proof relied on, was 
* School of Computer  Science, Telecommunications and Information Systems, Chicago, IL 60604. E-mail: 
tomuro@cs.depaul.edu 
t School of Computer  Science, Telecommunications and Information Systems, Chicago, IL 60604. E-maih 
lytinen@cs.depaul.edu 
1 In this paper, we use "nonminimal  derivations" synonymous ly  with "nonminimal  parses". Normal ly 
the notions of derivation and parse tree are different. However, in this paper we focus on parse trees as 
the final result of derivation, thus we mean that a derivation is nonminimal  when its result is a 
nonminimal  parse, in contrast o a minimal  derivation which produces a minimal  parse. Unfortunately, 
formal definitions of min imal  and nonmin imal  derivations are outside the scope of this short paper; 
interested readers are encouraged to read Tomuro (1999). 
(~) 2001 Association for Computat ional  Linguistics 
Computational Linguistics Volume 27, Number 2 
((cat) - S 
/ (1 cat) =" NP 
.J (2 cat) -- VP 
P0 = (2, ~0 : \] (head) -- (2 head) 
/ / head subj) - (1 head} 
~, (head agr> - (1 head agr> 
((cat) - VP 
J <1 cat/----" V 
P2 : (1, q)2 : ~ (head) -- (1 head} > 
I, (head type) - intrans ,/ 
Figure 1 
Examples of productions. 
((cat) -- NP 
pl = ("John",~l : ~ (head agr pers) - 3rd } > 
~, (head agr num)-  singJ 
((cat) -- V "\] 
~l (head agr pers) - 3rd / P3 = ("sleeps", ,I~ 3 : \] (head agr num) - sing ) 
I, (head tense} - pres 
not constrain ing enough to disal low nonmin imal  derivat ions.  To solve this twofo ld  
prob lem,  we propose  an alternate def init ion of min ima l  parse tree for unif icat ion gram-  
mars,  and present  a modi f icat ion to Shieber 's  a lgor i thm which ensures minimality.  
It is impor tant  to note that the same spur ious  parses also occur in context-free 
parsing,  specif ically in Ear ley's  algor i thm. However ,  since the only in format ion a con- 
st ituent carries in context-free grammar  is the grammar  symbol ,  the spur ious  der iva-  
t ions only produce  exactly the same results as the normal  ones. When the a lgor i thm 
is extended to unif icat ion grammar ,  however ,  these spur ious  parses are a prob lem.  
2. Unification Grammar and Parse Trees 
Shieber (1992) defines a unif icat ion grammar  as a 3-tuple (G, P, p0), where  ~ is the 
vocabu lary  of the grammar ,  P is the set of product ions ,  and P0 E P is the start pro-  
duct ion.  G contains L, a set of labels  (feature names);  C, a set of constants (feature 
values); and W, a set of terminals. There are two k inds of product ions  in P: phrasal 
and lexical. A phrasa l  product ion  is a 2-tuple (a, ~),  where  a is the arity of the rule (the 
number  of r ight-hand-s ide \[RHS\] constituents),  and ~ is a logical formula.  Typically, 
q~ is a conjunct ion of equat ions of the form pl - p2 or pl -" c, where  pl, p2 E L* are 
paths,  and c E C. In an equat ion,  any  path  wh ich  begins wi th  an integer i (1 < i < a) 
represents the ith RHS const i tuent of the rule. 2 A lexical p roduct ion  is a 2-tuple (w, ~), 
where  w E W and q~ is the same as above,  except that there are no RHS constituents. 
F igure 1 shows some example  phrasa l  and lexical product ions  (P0 cor responds  to the 
context-free rule S --+ NP  VP and is the start product ion) .  Then a mode l  M relates to 
a formula  q~ by  a satisfaction relat ion ~ as usual  (M ~ ~), and when q~ is the formula  
in a product ion  p = (a, ~),  p is said to l icense M. 
Based on the logic above,  Shieber defines a parse tree and the language of a 
g rammar  expressed in his formal ism. To define a val id parse tree, he first def ines the 
set of possible parse trees I1 = Ui>_0 Hi for a g iven grammar  G, where  each Eli is def ined 
as follows: 
Definition 
A parse tree r is a mode l  that is a member  of the infinite un ion of sets of bounded-  
depth  parse trees FI = Ui_>0 I1i, where  each IIi is def ined as: 
2 Shieber (1992) also uses a path that begins with 0 for the left-hand-side (LHS) constituent of a rule. In 
this paper, we omit the 0 arcs and place the features of the LHS constituent directly at the root. This 
change does not affect the formalism for the purpose of this paper. 
278 
Tomuro and Lytinen Nonminimal Derivations 
. 
. 
rio is the set of models 7- for which there is a lexical production 
p = <w, q)) E G such that 7- ~ 4< 
I I i ( i  > 0) is the set of models 7- for which there is a phrasal production 
p = (a, q~) C G such that 7- ~ ~ and, for all 1 < i < a, 7-/{i) is defined and 
7-/<i} C Uj<iIIy. 
In the second condition, the extraction operator, denoted by / ,  retrieves the feature 
structure found at the end of a particular path; so for instance 7-/<1) retrieves the first 
subconstituent on the RHS of the production that licenses 7-. In the definition above, 
II0 contains all models that satisfy any lexical production in the grammar, while Hi 
contains all models that satisfy a phrasal production, and whose subconstituents are 
all i n  UjGi I\]j. 
To specify what constitutes a valid parse for a particular sentence, the next step is 
to define the yield of a parse tree. It is defined recursively as follows: if 7- is licensed by 
some lexical production p = {w, q~/, then the yield of 7- is w; or if 7- is licensed by some 
phrasal production {a, q~} and O~ 1 . . . . .  (X a are the yields of 7-/(1) . . . . .  7-/<a) respectively, 
then the yield of 7- is ~1 ...  %. 
Finally, Shieber defines a valid parse tree 7- c II for sentence Wl . . .  wn as follows: 
o 
2. 
The yield of 7- is Wl . . .  Wn 
7- is licensed by the start production po 
Notice that this definition allows extra features in a parse tree, because a parse tree 
7- is defined by the satisfaction relation (7- ~ ~), which allows the existence of features 
in the model that are not in the licensing production's formula. Given this definition, 
for any valid parse tree 7-, we can construct another parse tree 7-' by simply adding an 
arbitrary (nonnumeric) feature to any node in 7-. Such a parse tree T' is nonminimal 
because extra features are nonminimal with respect o the minimal features in the 
licensing productions. We will return to the issue of minimal and nonminimal parse 
trees in Section 4. 
3. The Abstract Parsing Algorithm 
Based on the logic described above, Shieber defines an abstract parsing algorithm as a 
set of four logical deduction rules. Each rule derives a new item, from previous items 
and/or  productions in the grammar. An item is a 5-tuple {i,j, p, M, d), where i and j are 
indices into the sentence and specify which words in the sentence have been used to 
construct the item; p is the production used to construct the item; M is a model; and d 
is the position of the "dot"; i.e., how many subconstituents in p have been completed 
so far. 
The logical rules of the abstract algorithm are shown in Figure 2. The Initial Item 
rule produces the first item, and is constructed from the start production P0. It spans 
none of the input (i and j are both 0), and its model is the minimal model (ram) of P0. 
The Prediction rule is essentially the top-down rewriting of the expectation (a 
subconstituent just after the dot) in a prior item. In this rule, the extraction of M/(d  + 
1 / retrieves the d + 1st submodel in M (i.e., expectation). The function p, which is 
left underspecified as a parameter in the abstract algorithm, filters out some features 
predefined in the various instantiations of the algorithm. Here, it is applied to the 
expectation, by which it effectively controls the top-down predictive power of the 
279 
Computational Linguistics Volume 27, Number 2 
INITIAL ITEM: {O,O, po, mm(~o),O) 
PREDICTION: 
SCANNING: 
li, j,p = la, ~l,M,d) 
(j,j, p', p(M/(d+l)) t3 mm(~'), 0) ' where d K a and p' = (a',O') ? P 
(i,j,p = (a, ~},M,d} 
{i,j+lip, M t_l (mm(~2') \ {d+l ) ) ,d+l}  ' where  d < a and  (wj+l, O'} ? P 
COMPLETION: li'j'P = la' ~l 'M'd) (j,k,p' = (a',/I~'),M',a' / where d < a 
I {i, kip, M El (M' \ {d+l) ,d+l) 
Figure 2 
Shieber's parsing operations. 
I0 = (O,O, po, mm(420),O) 
11 = (O, 1,po, Ml,1) 
12 = (1,1,p2,M2,0 I 
I3 = (1,2,p2,M3,1) 
I4 = (0, 2, p0, M4, 2) 
ag 5 ?yP? 
pers~ -n~ p mtrans 
3rd sing 
Figure 3 
Items produced in the parse of John sleeps, and the final parse. 
algorithm and provides flexibility to the instantiated algorithms. Then the expectation 
is unified with a production (~'), which can consistently rewrite it. By this operation, 
some features in the expectation may be propagated own in the production. 
The remaining two rules advance the dot in a prior item, by unifying the sub- 
constituent to the right of the dot with either a lexical item from the input string (the 
Scanning rule) or some other completed higher-level item (the Completion rule). Both 
rules perform the correct unification by utilizing the embedding operator (signified 
by \), which places a model M under a path p (M\p). 
We illustrate these operators with a simple step-by-step example parse. Consider 
the grammar that consists of the rules presented in Figure 1. Using this grammar, 
Figure 3 shows the parse of the sentence John sleeps. First, the Initial Item operator 
is applied, producing item I0, whose model is mm(~o). Next, the Scanning operator 
scans the word John, producing 11. The Prediction operator then produces 12. Next, 
the word sleeps is scanned (since the first subconstituent of the model in 12 is a V), 
producing 13. Finally, since the item in 13 is complete (d = 1, the arity of production 
p2), Completion is applied to items 11 and/3, producing 14. Model M4 is the final parse 
of the sentence. 
4. Nonminimal Derivations 
In Section 2, we noted that Shieber's definition of parse trees allows them to be non- 
minimal. We consider these to be invalid based on a principle that, since the unification 
operation as set union preserves minimality (as proved in Shieber, \[1992\]), repeated 
applications of unification using licensing productions hould result in parses that 
contain features only from those productions and nothing more. In this section, we 
280 
Tomuro and Lytinen Nonminimal Derivations 
((cat) - VP 
I (1 cat) -- VP 
p4 = (2,~4: { (2 cat} -- ADV } 
| (head) - (1 head) / 
( (head modified) - true ) 
Figure 4 
A phrasal production that results in a nonminimal derivation. 
I~ = (1,1,p4,M~, 0} 
/~' = (1,1, p2,M~r, 0) 
I~ = (1,2,p2,M~, 1) 
I~ = {0, 2, p0, M~, 2} 
M4 ~ ." 
cat~-43 
c S a l t  
NP nea~/ l~Vub~V t 
ag~.. 7se.t~pe~modified 
per~ n~ lntrans t}ue 
3rd sing 
Figure 5 
Nonminimal derivation of John sleeps. 
formally define minimal and nonminimal parse trees, and show an example in which 
nonminimal parse trees are produced by Shieber's algorithm. 
Our definition of minimal parse tree is to a large extent similar to Shieber's def- 
inition, but to ensure minimality, our definition uses the equality relation instead of 
D, and inductively specifies a minimal parse tree bottom-up. 
Definition 
Given a grammar G, a minimal parse tree r admitted by G is a model that is a member 
of the infinite union of sets of bounded-depth parse trees 11' = Oi>0 IIl, where each 
171 is defined as: 
. 
2. 
For each lexical production p = (w, ~b) E G, mm(~) E 11'o. 
For each phrasal production p = (a, ~} E G, let rl . . . . .  ra E Uj<i I1;. If 
r = mm(~) l i t1\(1) t3.. .  I lr l \(a}, then r E 1I;. 
It is obvious that 1I' is a subset of 17 in Shieber's definition. Then, a nonminimal parse 
tree is defined as a model that is a member of the difference of the two sets (II - 1I'). 3 
Here is a simple example in which a nonminimal parse is produced in Shieber's 
algorithm. Say that we add the production in Figure 4 to the grammar in the previous 
section. The intent of this production is to mark the verb with the feature modified if an 
adverb follows. Using this grammar, Shieber's algorithm will produce a nonminimal 
parse for the sentence John sleeps, in addition to the minimal parse shown in the 
previous section. 4 The nonminimal parse, shown in Figure 5, arises as follows: after 
scanning John, Prediction can produce items I~ and I~', first using production p4 (thus 
inserting /head modified} - true into the model), and then P2. Scanning the word 
3 Note that using subsumption (which we will discuss in Section 5) here does not work, for instance by 
saying "a model r"  is a nonminimal parse tree if r "  E 17 and there exists r '  E II such that r '  _< r"",  
because some r" 's  are minimal. See the example in Section 5. 
4 Here, we are assuming that the filtering function/9 is the identity function. 
281 
Computational Linguistics Volume 27, Number 2 
sleeps then produces I~ from I~ I. Completion then can be applied directly to 11 and 11 by 
skipping a completion using I~ and I~, thereby producing item I~. The feature modified 
remains in I~, even though an adverb was never encountered in the sentence. The 
final parse M~, shown in Figure 5, is clearly nonminimal according to our definition 
because of this feature. 
Note that the example grammar can be changed to prevent he nonminimal parse, 
by moving the feature modified off of the head path in ff~4 (i.e., (modified / - true 
instead of (head modified / - true), sHowever, the point of the example is not to argue 
whether or not well-designed grammars will produce erroneous parses. A formally 
defined parser (see the discussion below) should in principle produce correct parses 
regardless of the grammar used; otherwise, the grammar formalism (i.e., Shieber's logic 
for unification grammars) must be revised and properly constrained to allow only the 
kinds of productions with which the parser produces correct results. 
In general, nonminimal derivations may arise whenever two or more predictions 
that are not mutually exclusive can be produced at the same point in the sentence; 
i.e., two prediction items (i, i, p, M, 0 / and (i, i, p', M ~, 0 / are produced such that M 
M / and M and M ~ are unifiable. In the example, items 12 = (1,1, p2, M2, 0/ and I~ -- 
(1,1, P4, M~, 0) (as well as I2 and I~ ~ = (1,1, p2, M~ ~, 0/) are two such items. Since the two 
predictions did not have any conflicting features from the beginning, a situation may 
occur where a completion generated from one prediction can fill the other prediction 
without causing conflict. When this happens, features that were in the other prediction 
but not the original one become nonminimal in the resulting model. 
As to what causes nonminimal situations, we speculate that there are a number 
of possibilRies. First, nonminimal derivations occur when a prediction is filled by a 
complete item that was not generated from the prediction. This mismatch will not 
happen if parsing is done in one direction only (e.g. purely top-down or bottom-up 
parsing). Thus, the mixed-direction parsing strategy is a contributing factor. 
Second, wrong complete items are retrieved because Shieber's item-based algo- 
rithm makes all partial results available during parsing, as if they are kept in a global 
structure (such as a chart in chart parsing). But if the accessibility of items were some- 
how restricted, prediction-completion mismatch would not happen. In this respect, 
other chart-based algorithms for unification grammars which adopt mixed-direction 
parsing strategy, including head-corner parsing (van Noord 1997) and left-corner pars- 
ing (Alshawi 1992), are subject o the same problem. 
Third, extra features can only appear when the grammar contains rules which 
interact in a certain way (such as rules P2 and P4 above). If the grammar contained 
no such rules, or if p (the filtering function applied in Prediction) filtered out those 
features, even the prediction-completion mismatch would not produce nonminimal 
derivations. 
As we stated in the beginning of this section, we consider nonminimal parses to 
be invalid on the basis of minimality. It then immediately follows that any parsing 
algorithm that produces nonminimal parses is considered to be unsound; in particular, 
Shieber's algorithm is unsound. However, since nonminimal parse trees have the same 
yield as their minimal counterparts, his algorithm does indeed recognize xactly the 
language of a given grammar. So, Shieber's algorithm is sound as a recognizer, 6 but 
not as a transducer or parser (as in van Noord, \[1997\]) where the correctness of output 
models (i.e., parse trees) is critical. In other words, Shieber's algorithm is correct up to 
5 Note that adding (head modified) -- false to ~2 (VP --* V) or ~3 (sleeps) isnot feasible, because they 
cannot specify the modified feature at their level, 
6 In fact, Shieber hints at this: "The process of parsing (more properly, recognition)..." (Shieber 1992, 78). 
282 
Tomuro and Lytinen Nonminimal Derivations 
licensing, but incorrect on the basis of a stronger criteria of minimality. Thus, to guar-  
antee correctness based on minimality, we need another algorithm; such an a lgor i thm 
is exactly the solution to the nonmin imal  der ivat ion problem. 
5. Practical Techniques 
Before present ing our solution to the nonmin imal  der ivat ion problem, we discuss 
several possible practical techniques to get around the prob lem in implemented sys- 
tems. These are known techniques, which have been appl ied to solve other problems 
in unif ication-based systems. However ,  most  of them only offer partial solutions to 
the nonmin imal  derivat ion problem. First, whenever  Shieber's a lgor i thm produces a 
nonmin imal  derivation, it also produces a corresponding minimal  der ivat ion (Tomuro 
1999). Thus, one possible solution is to use subsumpt ion  to discard items that are more 
specific than any other items that are produced.  Subsumpt ion  has often been used in 
unif ication-based systems to pack items or models  (e.g., A lshawi  1992). However,  
s imple subsumpt ion  may filter out val id parses for some grammars,  thus sacrificing 
completeness. 7 
Another  possibil ity is to filter out problematic features in the Prediction step by 
using the funct ion p. However,  automatic detection of such features (i.e., automatic 
derivat ion of p) is undecidable for the same reason as the prediction nontermination 
problem (caused by left recursion) for unif ication grammars  (Shieber 1985). Manual  
detection is also problematic:  when a grammar  is large, part icular ly if semantic fea- 
tures are included, complete detection is nearly impossible. As for the techniques 
developed so far which (partially) solve predict ion nonterminat ion (e.g., Shieber 1985; 
Haas 1989; Samuelsson 1993), they do not apply  to nonmin imal  derivations because 
nonmin imal  derivations may arise wi thout  left recursion or recursion in genera l  s One 
way  is to define p to filter out all features except the context-free backbone of predic- 
tions. However ,  this severely restricts the range of possible instantiations of Shieber's 
algorithm. 9 
A third possibil ity is to manual ly  fix the grammar  so that nonmin imal  derivations 
do not occur, as we noted in Section 4. However ,  this approach is problematic for the 
same reason as the manua l  der ivat ion of p ment ioned above. 
6. Modified Algorithm 
Finally, we propose an algor i thm that does not produce nonmin imal  derivations. It is a 
modif icat ion of Shieber's a lgor i thm that incorporates parent pointers. Figure 6 shows 
7 For example, when there are two predictions M1 and M2 for category C and a production p where 
M1 : {<cat> -- C, <x> - a}, M2 : {<cat> - C, <y> - b}, and p = <1, {<cat> - C, <1 cat> "- D, <x> - a}> 
respectively, the resulting model M2 ~ = {<cat> - C, <1 cat> - D, <x> -- a, <y> -- b} will have strictly more 
information than the other esulting model MI' = {<cat> ~ C, <1 cat> - D, <x> - a}, although both 
models are minimal. 
8 We do not show any particular example here, but if we change the left-recursive VP rule in the earlier 
example to a non-left-recursive rule, for instance VP --* VP2 ADV, and add some rules, a nonrninimal 
parse will indeed arise. 
Note also that some (but not all) cases of prediction ontermination will produce nonminimal 
derivations. Those cases occur when there is a prediction for a category, and repeated applications of 
some left-recursive rule(s) generate predictions for the same category that are not mutually exclusive to 
the original prediction or each other. 
9 In head-corner parsing, Sikkel (1997) proposes the use of transitive features: features that propagate 
only through ead arcs. However, this method oes not solve nonminimal derivations either, because 
problematic features may be subfeatures of a head (such as the example case shown earlier), which will 
not be filtered. 
283 
Computational Linguistics Volume 27, Number 2 
INITIAL ITEM: 
PREDICTION: 
(id, nil, (O,O, po, mm( ~o),O) ' where id is a new symbol 
(id, pid, (i,j,p = (a, ~),M,d) ) 
(id', id, (j,j, p', p(M/ (d+l) ) U mm(~I,'), 0))' 
where id I is a new symbol, and d ( a and pl = (ar,~t) C P 
SCANNING: 
COMPLETION: 
(id, pid, (i,j,p = (a, ~),M,d) ) 
(id, pid, (i,j+l,p,M U mm( ~') \ (d+l),d+l)) ' where d< a and (wj+D ~') E P 
(id, pid,(i,j,p,M,d)) (id",id,(j,k,p',M',a')) where d < a 
(ia, pie, (i,k,p, UU (U' \ (d+l)),d+l)) ' 
Figure 6 
Shieber's parsing operations modified. 
the modified algorithm. In the figure, an item is represented by a nested 3-tuple, where 
the first argument is the self index, the second is the parent index/pointer, and the 
third is the old 5-tuple used in Shieber's original algorithm. A parent pointer, then, 
is set in Prediction--the r sulting item has the index of the antecedent item (id) as 
its parent. By generating a new symbol for the self index in every Prediction item 
(id'), parent pointers in those items are threaded to form a prediction path. Then in 
Completion, the parent pointer is used to restrict he antecedent items: the complete 
item (on the right) must have the prior expectation (on the left) as its parent (id), 
thereby ensuring a prediction path to be precisely restored. 
While this modified algorithm offers a complete solution on the level of logic, it 
has some undesirable implications in implemented systems. The most prominent one 
is that the parent pointer scheme makes implementation f memoizat ion rather diffi- 
cult. Normally, memoization is used to avoid storing duplicate items that are identical; 
however, in the modified algorithm, many items that are otherwise identical will have 
different parent pointers, thereby changing the polynomial time algorithm (O(n3); Ear- 
ley \[1970\]) to an exponential one. To avoid computational inefficiency, a way must be 
devised for items that are identical except for parent poInters to share information, 
especially models, and thus avoid the expense of duplicate identical unification opera- 
tions. One possibility is to represent the 5-tuple from Shieber's original algorithm by a 
separate structure and have an index to it in the new 3-tuple item. This way, not only 
can the items be shared, they can still be memoized in the usual way as well. Another 
possibility is to adopt an efficiency technique along the line of selective memoization 
(van Noord 1997). Implementation a d empirical analysis is our future research. 
Whatever the practical performance will turn out to be, it is important to note 
that the proposed algorithm is a formal solution that guarantees minimality for any 
grammar defined in Shieber's logic. Moreover the algorithm preserves the same gen- 
erality and flexibility as Shieber's: a mixed top-down, bottom-up arsing with the 
filtering function p to allow various instantiations of the algorithm to characterize 
their algorithms. 
References 
Alshawi, H., editor. 1992. The Core Language 
Engine. MIT Press. 
Earley, J. 1970. An efficient context-free 
parsing algorithm. Communications ofthe 
ACM, 13(2). 
Gazdar, G., E. Klein, G. Pullum, and I. Sag. 
1985. Generalized Phrase Structure Grammar. 
Blackwell Publishing. 
Haas, A. 1989. A parsing algorithm for 
unification grammar. Computational 
Linguistics, 15(4):219-232. 
284 
Tomuro and Lytinen Nonminimal Derivations 
Pereira, F. and D. Warren. 1980. Definite 
clause grammars for language analysis. 
Arti~'cial Intelligence, 13:231-278. 
Pollard, C. and I. Sag. 1994. Head-driven 
Phrase Structure Grammar. CSLI. University 
of Chicago Press. 
Samuelsson, C. 1993. Avoiding 
non-termination in unification grammars. 
In Natural Language Understanding and Logic 
Programming IV. 
Shieber, S. 1985. Using restriction to extend 
parsing algorithms for complex-feature- 
based formalisms. In Proceedings ofthe 23rd 
Annual Meeting, Association for 
Computational Linguistics. 
Shieber, S. 1986. An Introduction to 
UniX'cation-Based Approaches to Grammar. 
CSLI. University of Chicago Press. 
Shieber, S. 1992. Constraint-based Grammar 
Formalisms. MIT Press. 
Sikkel, K. 1997. Parsing Schemata. 
Springer?Verlag. 
Tomuro, N. 1999. Left-Corner Parsing 
Algorithm for UniX'cation Grammars. Ph.D. 
thesis, DePaul University. 
van Noord, G. 1997. An efficient 
implementation f the head-corner parser. 
Computational Linguistics, 23(3):425-456. 
285 
Tree-cut and A Lexicon based on Systematic Polysemy
Noriko Tomuro
DePaul University
School of Computer Science, Telecommunications and Information Systems
243 S. Wabash Ave.
Chicago, IL 60604
tomuro@cs.depaul.edu
Abstract
This paper describes a lexicon organized around sys-
tematic polysemy: a set of word senses that are
related in systematic and predictable ways. The
lexicon is derived by a fully automatic extraction
method which utilizes a clustering technique called
tree-cut. We compare our lexicon to WordNet
cousins, and the inter-annotator disagreement ob-
served between WordNet Semcor and DSO corpora.
1 Introduction
In recent years, the granularity of word senses
for computational lexicons has been discussed fre-
quently in Lexical Semantics (for example, (Kilgar-
ri, 1998a; Palmer, 1998)). This issue emerged as a
prominent problem after previous studies and ex-
ercises in Word Sense Disambiguation (WSD) re-
ported that, when ne-grained sense denitions such
as those in WordNet (Miller, 1990) were used, en-
tries became very similar and indistinguishable to
human annotators, thereby causing disagreement on
correct tags (Kilgarri, 1998b; Veronis, 1998; Ng et
al., 1999). In addition to WSD, the selection of sense
inventories is fundamentally critical in other Natural
Language Processing (NLP) tasks such as Informa-
tion Extraction (IE) and Machine Translation (MT),
as well as in Information Retrieval (IR), since the
dierence in the correct sense assignments aects re-
call, precision and other evaluation measures.
In response to this, several approaches have been
proposed which group ne-grained word senses in
various ways to derive coarse-grained sense groups.
Some approaches utilize an abstraction hierarchy de-
ned in a dictionary (Kilgarri, 1998b), while others
utilize surface syntactic patterns of the functional
structures (such as predicate-argument structure for
verbs) of words (Palmer, 1998). Also, the current
version of WordNet (1.6) encodes groupings of sim-
ilar/related word senses (or synsets) by a relation
called cousin.
Another approach to grouping word senses is to
utilize a linguistic phenomenon called systematic
polysemy: a set of word senses that are related in sys-
tematic and predictable ways.
1
For example, ANIMAL
and MEAT meanings of the word \chicken" are re-
lated because chicken as meat refers to the esh of
a chicken as a bird that is used for food.
2
This rela-
tion is systematic, since many ANIMAL words such as
\duck" and \lamb" have a MEAT meaning. Another
example is the relation QUANTITY-PROCESS observed
in nouns such as \increase" and \supply".
Sense grouping based on systematic polysemy is
lexico-semantically motivated in that it expresses
general human knowledge about the relatedness of
word meanings. Such sense groupings have advan-
tages compared to other approaches. First, related
senses of a word often exist simultaneously in a
discourse (for example the QUANTITY and PROCESS
meanings of \increase" above). Thus, systematic
polysemy can be eectively used in WSD (and WSD
evaluation) to accept multiple or alternative sense
tags (Buitelaar, personal communication). Second,
many systematic relations are observed between
senses which belong to dierent semantic categories.
So if a lexicon is dened by a collection of sepa-
rate trees/hierarchies (such as the case of Word-
Net), systematic polysemy can express similarity be-
tween senses that are not hierarchically proximate.
Third, by explicitly representing (inter-)relations be-
tween senses, a lexicon based on systematic poly-
semy can facilitate semantic inferences. Thus it is
useful in knowledge-intensive NLP tasks such as dis-
course analysis, IE and MT. More recently, (Gonzalo
et al, 2000) also discusses potential usefulness of sys-
tematic polysemy for clustering word senses for IR.
However, extracting systematic relations from
large sense inventories is a dicult task. Most of-
ten, this procedure is done manually. For example,
WordNet cousin relations were identied manually
by the WordNet lexicographers. A similar eort was
also made in the EuroWordnet project (Vossen et
1
Systematic polysemy (in the sense we use in this paper) is
also referred to as regular polysemy (Apresjan, 1973) or logical
polysemy (Pustejovsky, 1995).
2
Note that systematic polysemy should be contrasted
with homonymy, which refers to words which have more
than one unrelated sense (e.g. FINANCIAL INSTITUTION and
SLOPING LAND meanings of the word \bank").
al., 1999). The problem is not only that manual
inspection of a large, complex lexicon is very time-
consuming, it is also prone to inconsistencies.
In this paper, we describes a lexicon organized
around systematic polysemy. The lexicon is derived
by a fully automatic extraction method which uti-
lizes a clustering technique called tree-cut (Li and
Abe, 1998). In our previous work (Tomuro, 2000),
we applied this method to a small subset of Word-
Net nouns and showed potential applicability. In the
current work, we applied the method to all nouns
and verbs in WordNet, and built a lexicon in which
word senses are partitioned by systematic polysemy.
We report results of comparing our lexicon with the
WordNet cousins as well as the inter-annotator dis-
agreement observed between two semantically an-
notated corpora: WordNet Semcor (Landes et al,
1998) and DSO (Ng and Lee, 1996). The results are
quite promising: our extraction method discovered
89% of the WordNet cousins, and the sense parti-
tions in our lexicon yielded better  values (Car-
letta, 1996) than arbitrary sense groupings on the
agreement data.
2 The Tree-cut Technique
The tree-cut technique is an unsupervised learning
technique which partitions data items organized in a
tree structure into mutually-disjoint clusters. It was
originally proposed in (Li and Abe, 1998), and then
adopted in our previous method for automatically
extracting systematic polysemy (Tomuro, 2000). In
this section, we give a brief summary of this tree-cut
technique using examples from (Li and Abe, 1998)'s
original work.
2.1 Tree-cut Models
The tree-cut technique is applied to data items that
are organized in a structure called a thesaurus tree.
A thesaurus tree is a hierarchically organized lexicon
where leaf nodes encode lexical data (i.e., words) and
internal nodes represent abstract semantic classes.
A tree-cut is a partition of a thesaurus tree. It is
a list of internal/leaf nodes in the tree, and each
node represents a set of all leaf nodes in a subtree
rooted by the node. Such a set is also considered as a
cluster.
3
Clusters in a tree-cut exhaustively cover all
leaf nodes of the tree, and they are mutually disjoint.
For instance, Figure 1 shows an example thesaurus
tree and one possible tree-cut [AIRCRAFT, ball, kite,
puzzle], which is indicated by a thick curve in the
gure. There are also four other possible tree-cuts
for this tree: [airplane, helicopter, ball, kite, puzzle],
[airplane, helicopter, TOY], [AIRCRAFT, TOY] and
[ARTIFACT].
In (Li and Abe, 1998), the tree-cut technique
was applied to the problem of acquiring general-
3
A leaf node is also a cluster whose cardinality is 1.
ized case frame patterns from a corpus. Thus, each
node/word in the tree received as its value the num-
ber of instances where the word occurred as a case
role (subject, object etc.) of a given verb. Then the
acquisition of a generalized case frame was viewed as
a problem of selecting the best tree-cut model that
estimates the true probability distribution, given a
sample corpus data.
Formally, a tree-cut model M is a pair consisting
of a tree-cut   and a probability parameter vector
 of the same length,
M = ( ;) (1)
where   and  are:
  = [C
1
; ::; C
k
]; = [P (C
1
); ::; P (C
k
)] (2)
where C
i
(1  i  k) is a cluster in the tree-
cut, P (C
i
) is the probability of a cluster C
i
, and
P
k
i=1
P (C
i
) = 1. Note that P (C) is the prob-
ability of cluster C = fn
1
; ::; n
m
g as a whole,
that is, P (C) =
P
m
j=1
P (n
j
). For example, sup-
pose a corpus contains 10 instances of verb-object
relation for the verb \y", and the frequencies
of object nouns n, denoted f(n), are as follows:
f(airplane) = 5; f(helicopter) = 3; f(ball) =
0; f(kite) = 2; f(puzzle) = 0. Then, the set of tree-
cut models for the example thesaurus tree shown in
Figure 1 includes ([airplane, helicopter, TOY], [.5,
.3, .2]) and ([AIRCRAFT, TOY], [.8, .2]).
2.2 The MDL Principle
To select the best tree-cut model, (Li and Abe, 1998)
uses the Minimal Description Length (MDL). The
MDL is a principle of data compression in Informa-
tion Theory which states that, for a given dataset,
the best model is the one which requires the min-
imum length (often measured in bits) to encode
the model (the model description length) and the
data (the data description length) (Rissanen, 1978).
Thus, the MDL principle captures the trade-o be-
tween the simplicity of a model, which is measured
by the number of clusters in a tree-cut, and the good-
ness of t to the data, which is measured by the
estimation accuracy of the probability distribution.
The calculation of the description length for a
tree-cut model is as follows. Given a thesaurus tree
T and a sample S consisting of the case frame in-
stances, the total description length L(M;S) for a
tree-cut model M = ( ;) is
L(M;S) = L( ) + L(j ) + L(Sj ;) (3)
where L( ) is the model description length, L(j )
is the parameter description length (explained
shortly), and L(Sj ;) is the data description
length. Note that L( ) + L(j ) essentially corre-
sponds to the usual notion of the model description
length.
ARTIFACT
AIRCRAFT TOY
airplane helicopter ball kite puzzle
? L(?|?) L(S|?,?)  L(M,S)
[A]                   1.66 11.60 13.26
[AC,TOY]                 3.32    14.34 17.66
[ap,heli,TOY]            4.98    14.44 19.42
[AC,ball,kite,puz]       6.64 4.96   11.60 
[ap,hel,ball,kite,puz] 8.31     5.06   13.37
0.8
0.0 0.2 0.0
5 3 0 02frequency
Figure 1: The MDL lengths and the nal tree-cut
Each length in L(M;S) is calculated as follows.
4
The model description length L( ) is
L( ) = log
2
jGj (4)
where G is the set of all cuts in T , and jGj denotes
the size of G. This value is a constant for all mod-
els, thus it is omitted in the calculation of the total
length.
The parameter description length L(j ) indi-
cates the complexity of the model. It is the length
required to encode the probability distribution of the
clusters in the tree-cut  . It is calculated as
L(j ) =
k
2
 log
2
jSj (5)
where k is the length of , and jSj is the size of S.
Finally, the data description length L(Sj ;) is
the length required to encode the whole sample data.
It is calculated as
L(Sj ;) =  
X
n2S
log
2
P (n) (6)
where, for each n 2 C and each C 2  ,
P (n) =
P (C)
jCj
and P (C) =
f(C)
jSj
(7)
Note the equation (7) essentially computes the Max-
imum Likelihood Estimate (MLE) for all n.
5
A table in Figure 1 shows the MDL lengths for all
ve tree-cut models. The best model is the one with
the tree-cut [AIRCRAFT, ball, kite, puzzle].
3 Clustering Systematic Polysemy
Using the tree-cut technique described above, our
previous work (Tomuro, 2000) extracted systematic
polysemy from WordNet. In this section, we give a
summary of this method, and describe the cluster
pairs obtained by the method.
4
For justication and detailed explanation of these formu-
las, see (Li and Abe, 1998).
5
In our previous work, we used entropy instead of MLE.
That is because the lexicon represents true population, not
samples; thus there is no additional data to estimate.
3.1 Extraction Method
In our previous work, systematically related word
senses are derived as binary cluster pairs, by apply-
ing the extraction procedure to a combination of two
WordNet (sub)trees. This process is done in the fol-
lowing three steps. In the rst step, all leaf nodes
of the two trees are assigned a value of either 1, if
a node/word appears in both trees, or 0 otherwise.
6
In the second step, the tree-cut technique is applied
to each tree separately, and two tree-cuts (or sets of
clusters) are obtained. To search the best tree-cut
for a tree (i.e., the model which requires the mini-
mum total description length), a greedy algorithm
called Find-MDL described in (Li and Abe, 1998)
is used to speed up the search. Finally in the third
step, clusters in those two tree-cuts are matched up,
and the pairs which have substantial overlap (more
than three overlapping words) are selected as sys-
tematic polysemies.
Figure 2 shows parts of the nal tree-cuts for the
ARTIFACT and MEASURE classes. Note in the gure,
bold letters indicate words which are polysemous in
the two trees (i.e., assigned a value 1).
3.2 Modication
In the current work, we made a minor modication
to the extraction method described above, by re-
moving nodes that are assigned a value 0 from the
trees. The purpose was to make the tree-cut tech-
nique less sensitive to the structure of a tree and
produce more specic clusters dened at deeper lev-
els.
7
The MDL principle inherently penalizes a com-
plex tree-cut by assigning a long parameter length.
Therefore, shorter tree-cuts partitioned at abstract
levels are often preferred. This causes a problem
when the tree is bushy, which is the case with Word-
Net trees. Indeed, many tree-cut clusters obtained
in our previous work were from nodes at depth 1
(counting the root as depth 0) { around 88% (122
6
Prior to this, each WordNet (sub)tree is transformed into
a thesaurus tree, since WordNet tree is a graph rather than a
tree, and internal nodes as well as leaf nodes carry data. In
the transformation, all internal nodes in a WordNet tree are
copied as leaf nodes, and shared subtrees are duplicated.
7
Removing nodes with 0 is also warranted since we are not
estimating values for those nodes (as explained in footnote 5).
MEASURE
INDEFINITE
QUANTITY
LINEAR
MEASURE
yard
LINEAR
UNIT
foot
CONTAINERFUL
bottle
bucket
spoon
DEFINITE
QUANTITY TIMEPERIOD
mile
0.07
0.120.330.36
loadblockbit
ounce
quarter
flashmorning
knot
0.53
ARTIFACT
foot
STRUCTURE INSTRUMEN-
TALITY
yard
ROD
bottle bucket
CONTAINER
spoonVESSEL
base building
ARTICLE
IMPLEMENT
UTENSIL
mixer porcelain
0.1
TABLEWARE
spoon
dish
plate
0.02
DEVICE
foot
knot
Figure 2: Parts of the nal tree-cuts for ARTIFACT and MEASURE
Table 1: Automatically Extracted Cluster Pairs
Category Basic Underspecied Cluster
classes classes pairs
Nouns 24 99 2,377
Verbs 10 59 1,710
Total 34 158 4,077
out of total 138 clusters) obtained for 5 combinations
of WordNet noun trees. Note that we did not allow
a cluster at the root of a tree; thus, depth 1 is the
highest level for any cluster. After the modication
above, the proportion of depth 1 clusters decreased
to 49% (169 out of total 343 clusters) for the same
tree combinations.
3.3 Extracted Cluster Pairs
We applied the modied method described above to
all nouns and verbs in WordNet. We rst parti-
tioned words in the two categories into basic classes.
A basic class is an abstract semantic concept, and
it corresponds to a (sub)tree in the WordNet hier-
archies. We chose 24 basic classes for nouns and 10
basic classes for verbs, from WordNet Top categories
for nouns and lexicographers' le names for verbs
respectively. Those basic classes exhaustively cover
all words in the two categories encoded in Word-
Net. For example, basic classes for nouns include
ARTIFACT, SUBSTANCE and LOCATION, while basic
classes for verbs include CHANGE, MOTION and STATE.
For each part-of-speech category, we applied our
extraction method to all combinations of two ba-
sic classes. Here, a combined class, for instance
ARTIFACT-SUBSTANCE, represents an underspecied
semantic class. We obtained 2,377 cluster pairs in
99 underspecied classes for nouns, and 1,710 cluster
pairs in 59 underspecied classes for verbs. Table 1
shows a summary of the number of basic and under-
specied classes and cluster pairs extracted by our
method.
Although the results vary among category combi-
nations, the accuracy (precision) of the derived clus-
ter pairs was rather low: 50 to 60% on average, based
on our manual inspection using around 5% randomly
chosen samples.
8
This means our automatic method
over-generates possible relations. We speculate that
this is because in general, there are many homony-
mous relations that are 'systematic' in the English
language. For example, in the ARTIFACT-GROUP
class, a pair [LUMBER, SOCIAL GROUP] was extracted.
Words which are common in the two clusters are
\picket", \board" and \stock". Since there are
enough number of such words (for our purpose), our
automatic method could not dierentiate them from
true systematic polysemy.
4 Evaluation: Comparison with
WordNet Cousins
To test our automatic extraction method, we com-
pared the cluster pairs derived by our method to
WordNet cousins. The cousin relation is relatively
new in WordNet, and the coverage is still incom-
plete. Currently a total of 194 unique relations are
encoded. A cousin relation in WordNet is dened
between two synsets, and it indicates that senses of
a word that appear in both of the (sub)trees rooted
by those synsets are related.
9
The cousins were man-
8
Note that the relatedness between clusters was deter-
mined solely by our subjective judgement. That is because
there is no existing large-scale lexicon which encodes related
senses completely for all words in the lexicon. (Note that
WordNet cousin relation is encoded only for some words).
Although the distinction between related vs. unrelated mean-
ings is sometimes unclear, systematicity of the related senses
among words is quite intuitive and has been well studied in
Lexical Semantics (for example, (Apresjan, 1973; Nunberg,
1995; Copestake and Briscoe, 1995)). A comparison with
WordNet cousin is discussed in the next section 4.
9
Actually, cousin is one of the three relations which in-
dicate the grouping of related senses of a word. Others are
sister and twin. In this paper, we use cousin to refer to all
relations listed in \cousin.tps" le (available in a WordNet
distribution).
ually identied by the WordNet lexicographers.
To compare the automatically derived cluster
pairs to WordNet cousins, we used the hypernym-
hyponym relation in the trees, instead of the number
or ratio of the overlapping words. This is because
the levels at which the cousin relations are dened
dier quite widely, from depth 0 to depth 6, thus the
number of polysemous words covered in each cousin
relation signicantly varies. Therefore, it was di-
cult to decide on an appropriate threshold value for
either criteria.
Using the hypernym-hyponym relation, we
checked, for each cousin relation, whether there was
at least one cluster pair that subsumed or was sub-
sumed by the cousin. More specically, for a cousin
relation dened between nodes c1 and c2 in trees
T1 and T2 respectively and a cluster pair dened
between nodes r1 and r2 in the same trees, we de-
cided on the correspondence if c1 is a hypernym or
hyponym of r1, and c2 is a hypernym or hyponym
r2 at the same time.
Based on this criteria, we obtained a result indi-
cating that 173 out of the 194 cousin relations had
corresponding cluster pairs. This makes the recall
ratio 89%, which we consider to be quite high.
In addition to the WordNet cousins, our auto-
matic extraction method discovered several interest-
ing relations. Table 2 shows some examples.
5 A Lexicon based on Systematic
Relations
Using the extracted cluster pairs, we partitioned
word senses for all nouns and verbs in WordNet, and
produced a lexicon. Recall from the previous section
that our cluster pairs are generated for all possible
binary combinations of basic classes, thus one sense
could appear in more than one cluster pair. For ex-
ample, Table 3 shows the cluster pairs (and a set of
senses covered by each pair, which we call a sense
cover) extracted for the noun \table" (which has 6
senses in WordNet). Also as we have mentioned ear-
lier in section accuracy-result, our cluster pairs con-
tain many false positives ones. For those reasons, we
took a conservative approach, by disallowing transi-
tivity of cluster pairs.
To partition senses of a word, we rst assign each
sense cover a value which we call a connectedness. It
is dened as follows. For a given word w which has n
senses, let S be the set of all sense covers generated
for w. Let c
ij
denote the number of sense covers in
which sense i (s
i
) and sense j (s
j
) occurred together
in S (where c
ii
= 0 for all 1  i  n), and d
ij
=
P
n
k=1
c
ik
+c
kj
C
, where k 6= i, k 6= j, c
ik
> 0, c
kj
> 0,
and C =
P
i;j
c
ij
. A connectedness of a sense cover
sc 2 S, denoted CN
sc
, where sc = (s
l
; ::; s
m
) (1 
Table 3: Extracted Relations for \table"
Sense Cover Cluster Pair CN
(1 4) [ARRANGEMENT, NAT OBJ] 1.143
(1 5) [ARRANGEMENT, SOC GROUP] 1.143
(2 3) [FURNITURE] 4.429
(2 3 4) [FURNITURE, NAT OBJ] 7.429
(2 3 5) [FURNITURE, SOC GROUP] 7.714
(2 3 6) [FURNITURE, FOOD] 7.429
(4 5) [NAT OBJ, SOC GROUP] 1.429
(5 6) [SOC GROUP, FOOD] 1.286
l < m  n) is dened as:
CN
sc
=
m
X
i=l
m
X
j=1
c
ij
+ d
ij
(8)
Intuitively, c
ij
represents the weight of a direct re-
lation, and d
ij
represents the weight of an indirect
relation between any two senses i and j. The idea
behind this connectedness measure is to favor sense
covers that have strong intra-relations. This mea-
sure also eectively takes into account a one-level
transitivity in d
ij
. As an example, the connectedness
of (2 3 4) is the summation of c
23
; c
34
; c
24
; d
23
; d
34
and d
24
. Here, c
23
= 4 because sense 2 and 3 co-
occur in four sense covers, and c
34
= c
24
= 1. Also,
d
23
=
(c
24
+c
43
)+(c
25
+c
53
)+(c
26
+c
63
)
C
=
2+2+2
14
= :429
(omitting cases where either or both c
ik
and c
kj
are
zero), and similarly d
34
= :5 and d
24
= :5. Thus,
CN
(234)
= 4+1+1+ :429+ :5+ :5= 7:429. Table 3
shows the connectedness values for all sense covers
for \table".
Then, we partition the senses by selecting a set of
non-overlapping sense covers which maximizes the
total connectedness value. So in the example above,
the set f(1 4),(2 3 5)g yields the maximum con-
nectedness. Finally, senses that are not covered by
any sense covers are taken as singletons, and added
to the nal sense partition. So the sense partition
for \table" becomes f(1 4),(2 3 5),(6)g.
Table 4 shows the comparison between Word-
Net and our new lexicon. As you can see,
our lexicon contains much less ambiguity: the
ratio of monosemous words increased from 84%
(88,650/105,461.84) to 92% (96,964/105,461.92),
and the average number of senses for polysemous
words decreased from 2.73 to 2.52 for nouns, and
from 3.57 to 2.82 for verbs.
As a note, our lexicon is similar to CORELEX
(Buitelaar, 1998) (or CORELEX-II presented in
(Buitelaar, 2000)), in that both lexicons share the
same motivation. However, our lexicon diers from
CORELEX in that CORELEX looks at all senses of
a word and groups words that have the same sense
distribution pattern, whereas our lexicon groups
Table 2: Examples of Automatically Extracted Systematic Polysemy
Underspecied Class Cluster Pair Common Words
ACTION-LOCATION [ACTION, POINT] \drop", \circle", \intersection", \dig",
\crossing", \bull's eye"
ARTIFACT-GROUP [STRUCTURE, PEOPLE] \house", \convent", \market", \center"
ARTIFACT-SUBSTANCE [FABRIC, CHEMICAL COMPOUND] \acetate", \nylon", \acrylic", \polyester"
COMMUNICATION-PERSON [VOICE, SINGER] \soprano", \alto", \tenor", \baritone"
[WRITING, RELIGIOUS PERSON] \John", \Matthew", \Jonah", \Joshua",
\Jeremiah"
Table 4: WordNet vs. the New Lexicon
Category WordNet New
Nouns Monosemous 82,892 88,977
Polysemous 12,243 6,158
Total words 95,135 95,135
Ave # senses 2.73 2.52
Verbs Monosemous 5,758 7,987
Polysemous 4,568 2,339
Total words 10,326 10,326
Ave # senses 3.57 2.82
Total Monosemous 88,650 96,964
Polysemous 16,811 8,497
Total words 105,461 105,461
word senses that have the same systematic relation.
Thus, our lexicon represents systematic polysemy at
a ner level than CORELEX, by pinpointing related
senses within each word.
6 Evaluation: Inter-annotator
Disagreement
To test if the sense partitions in our lexicon con-
stitute an appropriate (or useful) level of granular-
ity, we applied it to the inter-annotator disagree-
ment observed in two semantically annotated cor-
pora: WordNet Semcor (Landes et al, 1998) and
DSO (Ng and Lee, 1996). The agreement between
those corpora is previously studied in (Ng et al,
1999). In our current work, we rst re-produced
their agreement data, then used our sense partitions
to see whether or not they yield a better agreement.
In this experiment, we extracted 28,772 sen-
tences/instances for 191 words (consisting of 121
nouns and 70 verbs) tagged in the intersection of
the two corpora. This constitutes the base data set.
Table 5 shows the breakdown of the number of in-
stances where tags agreed and disagreed.
10
As you
10
Note that the numbers reported in (Ng et al, 1999) are
slightly more than the ones reported in this paper. For in-
stance, the number of sentences in the intersected corpus re-
ported in (Ng et al, 1999) is 30,315. We speculate the dis-
crepancies are due to the dierent sentence alignment meth-
Table 5: Agreement between Semcor and DSO
Category Agree Disagree Total Ave. 
Nouns 6,528 5,815 12,343 .268
Verbs 7,408 9,021 16,429 .260
Total 13,936 14,836 28,772 .264
(%) (48.4) (51.6) (100.0)
can see, the agreement is not very high: only around
48%.
11
This low agreement ratio is also reected in a mea-
sure called the  statistic (Carletta, 1996; Bruce and
Wiebe, 1998; Ng et al, 1999).  measure takes into
account chance agreement, thus better representing
the state of disagreement. A  value is calculated
for each word, on a confusion matrix where rows
represent the senses assigned by judge 1 (DSO) and
columns represent the senses assigned by judge 2
(Semcor). Table 6 shows an example matrix for the
noun \table".
A  value for a word is calculated as follows. We
use the notation and formula used in (Bruce and
Wiebe, 1998). Let n
ij
denote the number of in-
stances where the judge 1 assigned sense i and the
judge 2 assigned sense j to the same instance, and
n
i+
and n
+i
denote the marginal totals of rows and
columns respectively. The formula is:
k =
P
i
P
ii
 
P
i
P
i+
P
+i
1 
P
i
P
i+
P
+i
(9)
where P
ii
=
n
ii
n
++
(i.e., proportion of n
ii
, the number
of instances where both judges agreed on sense i, to
the total instances), P
i+
=
n
i+
n
++
and P
+i
=
n
+i
n
++
.
The  value is 1.0 when the agreement is perfect
(i.e., values in the o-diagonal cells are all 0, that
is,
P
i
P
ii
= 1), or 0 when the agreement is purely
ods used in the experiments.
11
(Ng et al, 1999) reports a higher agreement of 57%. We
speculate the discrepancy might be from the version of Word-
Net senses used in DSO, which was slightly dierent from the
standard delivery version (as noted in (Ng et al, 1999)).
Table 6: Confusion Matrix for the noun \table" ( = :611)
Judge 2 (Semcor)
1 2 3 4 5 6 Total
1 43 0 0 0 0 0 43 (= n
1+
)
2 6 17 3 0 0 0 26 (= n
2+
)
Judge 1 3 0 0 0 0 0 0 0 (= n
3+
)
(DSO) 4 1 0 0 0 0 0 1 (= n
4+
)
5 0 0 0 0 0 0 0 (= n
5+
)
6 2 2 1 0 0 0 5 (= n
6+
)
Total 52 19 4 0 0 0 75
(= n
+1
) (= n
+2
) (= n
+3
) (= n
+4
) (= n
+5
) (= n
+6
) (= n
++
)
Table 7: Reduced Matrix for \table" ( = :699)
1,4 2,3,5 6 Total
1,4 44 0 0 44
2,3,5 6 20 0 26
6 2 3 0 5
Total 52 23 0 75
by chance (i.e., values in a row (or column) are uni-
formly distributed across rows (or columns), that is,
P
ii
= P
i+
P
+i
for all 1  i  M , where M is the
number of rows/columns).  also takes a negative
value when there is a systematic disagreement be-
tween the two judges (e.g., some values in the diago-
nal cells are 0, that is, P
ii
= 0 for some i). Normally,
  :8 is considered a good agreement (Carletta,
1996).
By using the formula above, the average  for the
191 words was .264, as shown in Table 5.
12
This
means the agreement between Semcor and DSO is
quite low.
We selected the same 191 words from our lexicon,
and used their sense partitions to reduce the size of
the confusion matrices. For each word, we computed
the  for the reduced matrix, and compared it with
the  for a random sense grouping of the same parti-
tion pattern.
13
For example, the partition pattern of
f(1 4),(2 3 5),(6)g for \table" mentioned earlier
(where Table 7 shows its reduced matrix) is a multi-
nomial combination
 
6
2 3 1

. The  value for a ran-
dom grouping is obtained by generating 5,000 ran-
dom partitions which have the same pattern as the
corresponding sense partition in our lexicon, then
taking the mean of their 's. Then we measured the
possible increase in  by our lexicon by taking the
dierence between the paired  values for all words
(i.e., 
w
by our sense partition - 
w
by random par-
tition, for a word w), and performed a signicance
12
(Ng et al 1999)'s result is slightly higher:  = :317.
13
For this comparison, we excluded 23 words whose sense
partitions consisted of only 1 sense cover. This is reected in
the total number of instances in Table 8.
Table 8: Our Lexicon vs. Random Partitions
Category Total Our Lexicon Random
Ave.  Ave. 
Nouns 10,980 .247 .217
Verbs 14,392 .283 .262
Total 25,372 .260 .233
test, with a null hypothesis that there was no signif-
icant increase. The result showed that the P-values
were 4.17 and 2.65 for nouns and verbs respectively,
which were both statistically signicant. Therefore,
the null hypothesis was rejected, and we concluded
that there was a signicant increase in  by using
our lexicon.
As a note, the average 's for the 191 words from
our lexicon and their corresponding random parti-
tions were .260 and .233 respectively. Those values
are in fact lower than that for the original WordNet
lexicon. There are two major reasons for this. First,
in general, combining any arbitrary senses does not
always increase . In the given formula 9,  actually
decreases when the increase in
P
i
P
ii
(i.e., the diag-
onal sum) in the reduced matrix is less than the in-
crease in
P
i
P
i+
P
+i
(i.e., the marginal product sum)
by some factor.
14
This situation typically happens
when senses combined are well distinguished in the
original matrix, in the sense that, for senses i and j,
n
ij
and n
ji
are 0 or very small (relative to the total
frequency). Second, some systematic relations are in
fact easily distinguishable. Senses in such relations
often denote dierent objects in a context, for in-
stance ANIMAL and MEAT senses of \chicken". Since
our lexicon groups those senses together, the 's for
the reduce matrices decrease for the reason we men-
tioned above. Table 8 shows the breakdown of the
average  for our lexicon and random groupings.
14
This is because
P
i
P
i+
P
+i
is subtracted in both the nu-
merator and the denominator in the  formula. Note that
both
P
i
P
ii
and
P
i
P
i+
P
+i
always increase when any ar-
bitrary senses are combined. The factor mentioned here is
1 
P
i
P
ii
1 
P
i
P
i+
P
+i
.
7 Conclusions and Future Work
As we reported in previous sections, our tree-cut
extraction method discovered 89% of the Word-
Net cousins. Although the precision was rela-
tively low (50-60%), this is an encouraging re-
sult. As for the lexicon, our sense partitions con-
sistently yielded better  values than arbitrary
sense groupings. We consider these results to
be quite promising. Our data is available at
www.depaul.edu/ntomuro/research/naacl-01.html.
It is signicant to note that cluster pairs and sense
partitions derived in this work are domain indepen-
dent. Such information is useful in broad-domain
applications, or as a background lexicon (Kilgarri,
1997) in domain specic applications or text catego-
rization and IR tasks. For those tasks, we anticipate
that our extraction methods may be useful in deriv-
ing characteristics of the domains or given corpus,
as well as customizing the lexical resource. This is
our next future research.
For other future work, we plan to investigate an
automatic way of detecting and ltering unrelated
relations. We are also planning to compare our sense
partitions with the systematic disagreement ob-
tained by (Wiebe, et al, 1998)'s automatic classier.
Acknowledgments
The author wishes to thank Steve Lytinen at
DePaul University and the anonymous reviewers for
very useful comments and suggestions.
References
Apresjan, J. (1973). Regular Polysemy. Linguistics ,
(142).
Bruce, R. and Wiebe, J. (1998). Word-sense Dis-
tinguishability and Inter-coder Agreement. In
Proceedings of the COLING/ACL-98, Montreal,
Canada.
Buitelaar, P. (1998). CORELEX: Systematic Poly-
semy and Underspecication. Ph.D. dissertation,
Department of Computer Science, Brandeis Uni-
versity.
Buitelaar, P. (2000). Reducing Lexical Semantic
Complexity with Systematic Polysemous Classes
and Underspecication. In Proceedings of the
ANLP/NAACL-00 Workshop on Syntactic and
Semantic Complexity in Natural Language Pro-
cessing, Seattle, WA.
Carletta, J. (1996). Assessing Agreement on Clas-
sication Tasks: The Kappa Statistic, Computa-
tional Linguistics , 22(2).
Copestake, A. and Briscoe, T. (1995). Semi-
productive Polysemy and Sense Extension. Jour-
nal of Semantics , 12.
Gonzalo, J., Chugur, I. and Verdejo, F. (2000).
Sense Clusters for Information Retrieval: Evi-
dence from Semcor and the InterLingual Index.
In Proceedings of the ACL-2000 Workshop on
Word Senses and Multilinguality, Hong-Kong.
Kilgarri, A. (1997). Foreground and Background
Lexicons and Word Sense Disambiguation for In-
formation Extraction. In Proceedings of the In-
ternational Workshop on Lexically Driven Infor-
mation Extraction.
Kilgarri, A. (1998a). SENSEVAL: An Exercise
in Evaluating Word Sense Disambiguation Pro-
grams. In Proceedings of the LREC.
Kilgarri, A. (1998b). Inter-tagger Agreement. In
Advanced Papers of the SENSEVAL Workshop,
Sussex, UK.
Landes, S., Leacock, C. and Tengi, R. (1998).
Building Semantic Concordance. In WordNet:
An Electronic Lexical Database, The MIT Press.
Li, H. and Abe, N. (1998). Generalizing Case
Frames Using a Thesaurus and the MDL Prin-
ciple, Computational Linguistics, 24(2).
Miller, G. (eds.) (1990). WORDNET: An Online
Lexical Database. International Journal of Lex-
icography , 3(4).
Ng, H.T., and Lee, H.B. (1996). Integrating Mul-
tiple Knowledge Sources to Disambiguate Word
Sense. In Proceedings of the ACL-96, Santa Cruz,
CA.
Ng, H.T., Lim, C. and Foo, S. (1999). A Case
Study on Inter-Annotator Agreement for Word
Sense Disambiguation. In Proceedings of the
ACL SIGLEX Workshop on Standardizing Lexi-
cal Resources, College Park, MD.
Nunberg, G. (1995). Transfers of Meaning. Journal
of Semantics , 12.
Palmer, M. (1998). Are Wordnet sense distinctions
appropriate for computational lexicons? In Ad-
vanced Papers of the SENSEVAL Workshop, Sus-
sex, UK.
Pustejovsky, J. (1995). The Generative Lexicon,
The MIT Press.
Rissanen, J. (1978). Modeling by Shortest Data
Description. Automatic, 14.
Tomuro, N. (2000). Automatic Extraction of Sys-
tematic Polysemy Using Tree-cut. In Proceedings
of the ANLP/NAACL-00 Workshop on Syntactic
and Semantic Complexity in Natural Language
Processing, Seattle, WA.
Veronis, J. (1998). A Study of Polysemy Judge-
ments and Inter-annotator Agreement. In Ad-
vanced Papers of the SENSEVAL Workshop, Sus-
sex, UK.
Vossen, P., Peters, W. and Gonzalo, J. (1999). To-
wards a Universal Index of Meaning. In Proceed-
ings of the ACL SIGLEX Workshop on Standard-
izing Lexical Resources, College Park, MD.
Wiebe, J., Bruce, R. and O'Hara, T. (1999). De-
velopment and Use of a Gold-Standard Data Set
for Subjectivity Classications. In Proceedings of
the ACL-99, College Park, MD.
Automatic Extraction of Systematic Polysemy Using Tree-cut 
Nor iko  Tomuro  
DePaul  University 
School of Computer  Science, Telecommunicat ions and Informat ion Systems 
243 S. Wabash Ave. 
Chicago, IL 60604 
tomuro@cs.depaul.edu 
Abst rac t  
This paper describes an automatic method for 
extracting systematic polysemy from a hierar- 
chically organized semantic lexicon (WordNet). 
Systematic polysemy is a set of word senses 
that are related in systematic and predictable 
ways. Our method uses a modification of a tree 
generalization technique used in (Li and Abe, 
1998), and generates a tree-cut, which is a list 
of clusters that partition a tree. We compare 
the systematic relations extracted by our auto- 
matic method to manually extracted WordNet 
cousins. 
1 In t roduct ion  
In recent years, several on-line broad-coverage 
semantic lexicons became available, including 
LDOCE (Procter, 1978), WordNet (Miller, 
1990) and HECTOR .(Kilgarriff, 1998a). 
These lexicons have been used as a domain- 
independent semantic resource as well as an 
evaluation criteria in various Natural Language 
Processing (NLP) tasks, such as Information 
Retrieval (IR), Information Extraction (IE) and 
Word Sense Disambiguation (WSD). 
However, those lexicons are rather complex. 
For instance, WordNet (version 1.6) contains a 
total of over 120,000 words and 170,000 word 
senses, which are grouped into around 100,000 
synsets (synonym sets). In addition to the size, 
word entries in those lexicon are often polyse- 
mous. For instance, 20% of the words in Word- 
net have more than one sense, and the average 
number of senses of those polysemous words is 
around 3. Also, the distinction between word 
senses tends to be ambiguous and arbitrary. 
For example, the following 6 senses are listed 
in WordNet for the noun "door": 
1. door  - a swinging or sliding barrier 
2. door  - the space in a wall 
3. door  - anything providing a means of 
access (or escape) 
4. door  - a swinging or sliding barrier that 
will close off access into a car 
5. door  - a house that is entered via a door 
6. door  - a room that is entered via a door 
Because of the high degree of ambiguity, using 
such complex semantic lexicons brings some se- 
rious problems to the performance of NLP sys- 
tems. The first, obvious problem is the com- 
putational intractability: increased processing 
time needed to disambiguate multiple possibili- 
ties will necessarily slow down the system. An- 
other problem, which has been receiving atten- 
tion in the past few years, is the inaccuracy: 
when there is more than one sense applicable in 
a given context, different systems (or human in- 
dividuals) may select different senses as the cor- 
rect sense. Indeed, recent studies in WSD show 
that, when sense definitions are fine-grained, 
similar senses become indistinguishable to hu- 
man annotators and often cause disagreement 
on the correct tag (Ng et al, 1999; Veronis, 
1998; Kilgarriff, 1998b). Also in IR and IE 
tasks, difference in the correct sense assignment 
will surely degrade recall and precision of the 
systems. Thus, it is apparent that, in order for 
a lexicon to be useful as an evaluation criteria 
for NLP systems, it must represent word senses 
at the level of granularity that captures human 
intuition. 
In Lexical Semantics, everal approaches have 
been proposed which organize a lexicon based 
on systematic polysemy: 1 a set of word senses 
that are related in systematic and predictable 
ISystematic polysemy (in the sense we use in this 
paper) is also referred to as regular polysemy (Apresjan, 
1973) or logical polyseray (Pustejovsky, 1995). 
20 
ways (e.g. ANIMAL and MEAT meanings of the 
word "chicken"). 2 In particular, (Buitelaar, 
1997, 1998) identified systematic relations that 
exist between abstract semantic concepts in 
the WordNet noun hierarchy, and defined a 
set of underspecified semantic classes that rep- 
resent the relations. Then he extracted all 
polysemous nouns in WordNet according to 
those underspecified classes and built a lexicon 
called CORELEX. For example, a CORELEX 
class AQU (which represents a relation between 
ARTIFACT and QUANTITY) contains words such 
as "bottle", "bucket" and "spoon". 
Using the abstract semantic lasses and orga- 
nizing a lexicon based on systematic polysemy 
addresses the two problems mentioned above in 
the following ways. For the first problem, using 
the abstract classes can reduce the size of the 
lexicon by combining several related senses into 
one sense; thus computation becomes more effi- 
cient. For the second problem, systematic poly- 
semy does reflect our general intuitions on word 
meanings. Although the distinction between 
systematic vs. non-systematic relations (or re- 
lated vs. unrelated meanings) is sometimes un- 
clear, systematicity of the related senses among 
words is quite intuitive and has been well stud- 
ied in Lexical Semantics (for example, (Apres- 
jan, 1973; Cruse, 1986; Nunberg, 1995; Copes- 
take and Briscoe, 1995)). 
However, there is one critical issue still to 
be addressed: the level of granularity at which 
the abstract classes are defined. The prob- 
lem is that, when the granularity of the ab- 
stract classes is too coarse, systematic rela- 
tions defined at that level may not hold uni- 
formly at more fine-grained levels (Vossen et 
al., 1999). For instance, the CORELEX class 
AQU mentioned above also contains a word 
"dose" .3 Here, the relation between the senses 
of "dose" is different from that of "bottle", 
"bucket" and "spoon", which can be labeled as 
CONTAINER-CONTAINERFUL relation. We argue 
that human intuitions can distinguish meanings 
2Note that systematic polysemy should be contrasted 
with homonymy which refers to words which have more 
than one unrelated sense (e.g. FINANCIAL_INSTITUTION 
and SLOPING_LAND meanings ofthe word "bank"). 
3Senses of "dose" in WordNet are: (1) a measured 
portion of medicine taken at any one time, and (2) 
the quantity of an active agent (substance orradiation) 
taken in or absorbed at any one time. 
ARTIFACT 
AIRCRAFT TOY 
/ \  / l \  
airplane helicopter ball kite puzzle 
Figure 1: An example thesaurus tree 
at this level, where differences between the sys- 
tematic relations are rather clear, and therefore 
lexicons that encode word senses at this level of 
granularity have advantages over fine-grained as 
well as coarse-grained lexicons in various NLP 
tasks. 
Another issue we like to address is the ways 
for extracting systematic polysemy. Most of- 
ten, this procedure is done manually. For ex- 
ample, the current version of WordNet (1.6) 
encodes the similarity between word senses (or 
synsets) by a relation called cousin. But those 
cousin relations were identified manually by the 
WordNet lexicographers. A similar effort was 
also made in the EuroWordnet project (Vossen 
et al, 1999). However, manually inspecting a
large, complex lexicon is very time-consuming 
and often prone to inconsistencies. 
In this paper, we propose a method which au- 
tomatically extracts ystematic polysemy from 
a hierarchically organized semantic lexicon 
(WordNet). Our method uses a modification of 
a tree generalization technique used in (Li and 
Abe, 1998), and generates a tree-cut, which is a 
list of clusters that partition a tree. Then, we 
compare the systematic relations extracted by 
our automatic method to the WordNet cousins. 
Preliminary results show that our method dis- 
covered most of the WordNet cousins as well as 
some more interesting relations. 
2 T ree  Genera l i za t ion  us ing Tree-cut  
and  MDL 
Before we present our method, we first give a 
brief summary of the tree-cut echnique which 
we adopted from (Li and Abe, 1998). This tech- 
nique is used to acquire generalized case frame 
patterns from a corpus using a thesaurus tree. 
2.1 Tree-cut  Models  
A thesaurus tree is a hierarchically organized 
lexicon where leaf nodes encode lexical data 
21 
(i.e., words) and internal nodes represent ab- 
stract semantic lasses. A tree-cut is a partition 
of a thesaurus tree. It is a list of internal/leaf 
nodes in the tree, and each node represents a 
set of all leaf nodes in a subtree rooted by the 
node. Such set is also considered as a clus- 
ter. 4 Clusters in a tree-cut exhaustively cover 
all leaf nodes of the tree, and they are mutu- 
ally disjoint. For example, for a thesaurus tree 
in Figure 1, there are 5 tree-cuts: \[airplane, he- 
licopter, ball, kite, puzzle\], \[AIRCRAFT, ball, 
kite, puzzle\], \[airplane, helicopter, TOY\], \[AIR- 
CRAFT, TOY\] and \[ARTIFACT\]. Thus, a tree- 
cut corresponds to one of the levels of abstrac- 
tion in the tree. 
Using a thesaurus tree and the idea of tree- 
cut, the problem of acquiring generalized case 
frame patters (for a fixed verb) from a corpus 
is to select the best tree-cut hat accounts for 
both observed and unobserved case frame in- 
stances. In (Li and Abe, 1998), this generaliza- 
tion problem is viewed as a problem of select- 
ing the best model for a tree-cut hat estimates 
the true probability distribution, given a sample 
corpus data. 
Formally, a tree-cut model M is a pair consist- 
ing of a tree-cut F and a probability parameter 
vector O of the same length, 
M = (F, O) 
where F and ? are: 
(1) 
F=\[Cx,..,Ck\],O=\[P(C,),..,P(Ck)\] (2) 
words, that is, P(C) = ~=1 P(nj). Here, com- 
pared to knowing all P(nj) (where 1 < j < m) 
individually, knowing one P(C) can only facil- 
itate an estimate of uniform probability distri- 
bution among members as the best guess, that 
is, P(nj) = P(C) for all j. Therefore, in general, m 
when clusters C1..Cm are merged and general- 
ized to C according to the thesaurus tree, the 
estimation of a probability model becomes less 
accurate. 
2.2 The  MDL Pr inc ip le  
To select the best tree-cut model, (Li and Abe, 
1998) uses the Minimal Description Length 
(MDL) principle (Rissanen, 1978). The MDL is 
a principle of data compression i  Information 
Theory which states that, for a given dataset, 
the best model is the one which requires the 
minimum length (often measured in bits) to en- 
code the model (the model description length) 
and the data (the data description length). For 
the problem of case frame generalization, the 
MDL principle fits very well in that it captures 
the trade-off between the simplicity of a model, 
which is measured by the number of clusters in 
a tree-cut, and the goodness of fit to the data, 
which is measured by the estimation accuracy 
of the probability distribution. 
The calculation of the description length for 
a tree-cut model is as follows. Given a the- 
saurus tree T and a sample S consisting of 
the case frame instances, the total description 
length L(M, S) for a tree-cut model M = (F, 0) 
is 
where Ci (1 < i < k) is a cluster in the tree- 
cut, P(Ci) is the probability of a cluster Ci, 
and ~/k=l P(Ci) = 1. For example, suppose 
a corpus contained 10 instances of verb-object 
relation for the verb "fly", and the frequency 
of object noun n, denoted f(n), are as follows: 
f ( airpl ane ) -- 5, f ( helicopter ) = 3, f ( bal l ) = 
O, f(kite) -- 2, f(puzzle) = 0. Then, the set of 
tree-cut models for the thesaurus tree shown in 
Figure 1 includes (\[airplane, helicopter, TOY\], 
\[0.5, 0.3, 0.2\]) and (\[AIRCRAFT, TOY\], \[0.8, 
0.2\]). 
Note that P(C) is the probability of cluster 
C = {nl, .., nm) as a whole. It is essentially the 
sum of all (true) probabilities of the member  
4A leaf node is also a cluster whose cardinality is 1. 
L(M,S)=L(F)+L(eT)+L(SJF, e) (3) 
where L(F) is the model description length, 
L(OIF) is the parameter description length (ex- 
plained shortly), and L(SIF , O) is the data de- 
scription length. Note that L(F) + L(OIF ) es- 
sentially corresponds to the usual notion of the 
model description length. 
Each  length in L(M, S) is calculated as fol- 
lows. 5 The model description length L(F) is 
L( r )  = log21GI (4) 
where G is the set of all cuts in T, and IG I de- 
notes the size of G. This value is a constant for 
? SFor justification and detailed explanation of these 
formulas, ee (Li and Abe, 1998). 
22 
all models, thus it is omitted in the calculation 
of the total length. 
The parameter description length L(OIF ) in- 
dicates the complexity of the model. It is the 
length required to encode the probability dis- 
tribution of the clusters in the tree-cut F. It is 
calculated as 
k 
L(Olr)  = x Zog21Sl (5) 
where k is the length of ?, and IS\[ is the size of 
S. 
Finally, the data description length L(SIF, O) 
is the length required to encode the whole sam- 
ple data. It is calculated as 
L(S IF ,  e )  = - log2P(n) (6) 
nES 
where, for each n E C and each C E F, 
and 
P(n) -  P(C) 
ICl (7) 
P(C) -  f (c)  (8) 
ISl 
Note here that, in (7), the probability of C is di- 
vided evenly among all n in C. This way, words 
that are not observed in the sample receive a 
non-zero probability, and the data sparseness 
problem is avoided. 
Then, the best model is the one which re- 
quires the minimum total description length. 
Figure 2 shows the MDL lengths for all five 
tree-cut models that can be produced for the 
thesaurus tree in Figure 1. The best model is 
the one with the tree-cut \[AIRCRAFT, ball, kite, 
puzzle\] indicated by a thick curve in the figure. 
3 C lus ter ing  Systemat ic  Po lysemy 
3.1 Generalization Technique 
Using the generalization technique in (Li and 
Abe, 1998) described in the previous section, 
we wish to extract systematic polysemy au- 
tomatically from WordNet. Our assumption 
is that, if a semantic concept is systemati- 
cally related to another concept, words that 
have one sense under one concept (sub)tree are 
likely to have another sense under the other 
concept (sub)tree. To give an example, Fig- 
ure 3 shows parts of WordNet noun trees for 
ARTIFACT and MEASURE, where subtrees under 
CONTAINER and C0NTAINERFUL respectively con- 
tain "bottle", "bucket" and "spoon". Note a 
dashed line in the figure indicates an indirect 
link for more than one level. 
Based on this assumption, it seems system- 
atic polysemy in the two trees can be extracted 
straight-forwardly by clustering each tree ac- 
cording to polysemy as a feature, and by match- 
ing of clusters taken from each tree. 6 To this 
end, the notion of tree-cut and the MDL prin- 
ciple seem to comprise an excellent ool. 
However, we must be careful in adopting Li 
and Abe's technique directly: since the problem 
which their technique was applied to is funda- 
mentally different from ours, some procedures 
used in their problem may not have any inter- 
pretation in our problem. Although both prob- 
lems are essentially a tree generalization prob- 
lem, their problem estimates the true probabil- 
ity distribution from a random sample of exam- 
ples (a corpus), whereas our problem does not 
have any additional data to estimate, since all 
data (a lexicon) is already known. This differ- 
ence raises the following issue. In the calcu- 
lation of the data description length in equa- 
tion (6), each word in a cluster, observed or un- 
observed, is assigned an estimated probability, 
which is a uniform fraction of the probability 
of the cluster. This procedure does not have 
interpretation if it is applied to our problem. 
Instead, we use the distribution of feature fre- 
quency proportion of the clusters, and calculate 
the data description length by the following for- 
mula: 
k 
L(SIF, e) = - f(Ci) ? log2P(Ci) (9) 
i=l 
where F = \[C1,.., Ck\], 0 = \[P(C,),.., P(Ck)\]. 
This corresponds to the length required to en- 
code all words in a cluster, for all clusters 
in a tree-cut, assuming Huffman's algorithm 
(Huffman, 1952) assigned a codeword of length 
- log2P(Ci) to each cluster C/ (whose propor- 
6We could also combine two (or possibly more) trees 
into one tree and apply clustering over that tree once. 
In this paper, we describe clustering of two trees for ex- 
ample purpose. 
23 
F 
\[A\] 
\[AC,TOY\] 
\[ap,heli,TOY\] 
\[AC,ball,kite,puz\]. 
\[ap,hel,ball,kite,puz\] 
L(eIF) L(SIF.e) L(M,S) 
1.66 11.60 13.26 
3.32 14.34 17.66 
4.98 14.44 19.42 
6.64 4.96 11.60 
8.31 5.06 13.37 
ARTIFACT 
/ \ k.o.o/o.2 o.o 
/ \ / ~k  
airplane helicopter ball kite puzzle 
Figure 2: The MDL lengths and the final tree-cut 
ARTIFACT MEASURE 
CONTAINER MEDICINE dose CONTAINERFUL 
VESSEL spoon dose bottle bucket spoon / \  
bottle bucket 
Figure 3: Parts of WordNet trees ARTIFACT and MEASURE 
tion is P(C i )  = .~_~_d~ Isl J" 
All other notions and formulas are applicable 
to our problem without modification. 
3.2 C lus ter ing  Method  
Our clustering method uses the the modified 
generalization technique described in the last 
section to generate tree-cuts. But before we ap- 
ply the method, we must transform the data in 
Wordnet. This is because WordNet differs from 
a theaurus tree in two ways: it is a graph rather 
than a tree, and internal nodes as well as leaf 
nodes carry data, First, we eliminate multiple 
inheritance by separating shared subtrees. Sec- 
ond, we bring down every internal node to a 
leaf level by creating a new duplicate node and 
adding it as a child of the old node (thus making 
the old node an internal node). 
After trees are transformed, our method ex- 
tracts systematic polysemy by the following 
three steps. In the first step, all leaf nodes of 
the two trees are marked with either 1 or 0 (1 
if a node/word appears in both trees, or 0 oth- 
erwise), 
In the second step, the generalization tech- 
nique is applied to each tree, and two tree-cuts 
are obtained. To search for the best tree-cut, 
instead of computing the description length for 
M1 possible tree-cuts in a tree, a greedy dy- 
namic programming algorithm is used. This 
algorithm , called F ind-MDL in (Li and Abe, 
1998), finds the best tree-cut for a tree by recur- 
sively finding the best tree-cuts for all of its sub- 
trees and merging them from bottom up. This 
algorithm is quite efficient, since it is basically a 
depth-first search with minor overhead for com- 
puting the description length. 
Finally in the third step, clusters from the two 
tree-cuts are matched up, and the pairs which 
have substantial overlap are selected as system- 
atic polysemy. 
Figure 4 shows parts of the final tree-cuts 
for ARTIFACT and MEASURE obtained by our 
method. ~In both trees, most of the clusters in 
the tree-cuts are from nodes at depth 1 (count- 
ing the root as depth 0). That is because the 
tree-cut echnique used in our method is sensi- 
tive to the structure of the tree. More specifi- 
cally, the MDL principle inherently penalizes a
complex tree-cut by assigning a long parame- 
ter length. Therefore, unless the entropy of the 
feature distribution is large enough to make the 
data length overshadow the parameter length, 
simpler tree-cuts partitioned at abstract levels 
are preferred. This situation tends to happen 
often when the tree is bushy and the total fea- 
ture frequency is low. This was precisely the 
case with ARTIFACT and MEASURE, where both 
Tin the figure, bold letters indicate words which are  
polysemous in the two t ree .  
24 
ARTIFACT 
0,  
STRUCTURE INSTRUMEN- ARTICLE 
TALITY / i \  
base !building J /  ~ MEDICINE~ 10.02 
foot IMPLEMENT DEVICE CONTAINER / ~ TABLEWARE / \  
UTENSIL ROD 
J \  
mixer porcelain 
yard 
/ '"... I ~  inhalant dose / l ~  
foot VESSEL spoon spoon dish plate 
k .o t  / \  
bottle bucket 
0.36 
DEFINITE 
QUANTITY 
bit block 
ounce 
bottle bucket spoon 
MEASURE 
QUANTITY . ~k MEASURE / PERIOD 
CONTAINERFUL dose load LINEAR morning flash sixties UNIT l / ~  quarter 
mile knot yard foot 
Figure 4: Parts of the final tree-cuts for ARTIFACT and MEASURE 
trees were quite bushy, and only 4% and 14% of 
the words were polysemous in the two categories 
respectively. 
4 Eva luat ion  
To test our method, we chose 5 combinations 
from WordNet noun Top categories (which we 
call top relation classes), and extracted clus- 
ter pairs which have more than 3 overlapping 
words. Then we evaluated those pairs in two 
aspects: related vs. unrelated relations, and 
automatic vs. manual clusters. 
4.1 Re la ted  vs. Unre la ted  C lusters  
Of the cluster pairs we extracted automatically, 
not all are systematically related; some are un- 
related, homonymous relations. They are essen- 
tially false positives for our purposes. Table 1 
shows the number of related and unrelated re- 
lations in the extracted cluster pairs. 
Although the results vary among category 
combinations, the ratio of the related pairs is 
rather low: less than 60% on average. There are 
several reasons for this. First, there are some 
pairs whose relations are spurious. For exam- 
ple, in ARTIFACT-GROUP class, a pair \[LUMBER, 
SOCIAL_GROUP\] was extracted. Words which 
are common in the two clusters are "picket", 
"board" and "stock". This relation is obviously 
homonymous. 
Second, some clusters obtained by tree-cut 
are rather abstract, so that pairing two ab- 
stract clusters results in an unrelated pair. For 
example, in ARTIFACT-MEASURE class, a pair 
\[INSTRUMENTALITY, LINEAR_UNIT\] was selected. 
Words which are common in the two clus- 
ters include "yard", "foot" and "knot" (see 
the previous Figure 4). Here, the concept 
INSTRUMENTALITY is very general (at depth 
1), and  it also contains many (polysemous) 
words. So, matching this cluster with an- 
other abstract cluster is likely to yield a pair 
which has just enough overlapping words but 
whose relation is not systematic. In the case 
of \[INSTRUMENTALITY, LINEAR_UNIT\], the situ- 
ation is even worse, because the concept of 
LINEAR_UNIT in MEASURE represents a collection 
of terms that were chosen arbitrarily in the his- 
25 
Table 1: Related vs. Unrelated Relations 
Top relation class Related Unrelated 
ACTION-L0CATION 
ARTIFACT-GROUP 
ARTIFACT-MEASURE 
ARTIFACT-SUBSTANCE 
COMMUNICATION-PERSON 
10 1 
18 9 
7 19 
19 12 
12 11 
Total 66 52 
Total 
11 
27 
26 
31 
23 
118 
% of 
related 
90.9 
66.7 
26.9 
61.3 
52.2 
55.9 
tory of the English language. 
4.2 Automat ic  vs. Manua l  C lus ters  
To compare the cluster pairs our method ex- 
tracted automatically to manually extracted 
clusters, we use WordNet cousins. A cousin 
relation is relatively new in WordNet, and the 
coverage is still incomplete. However, it gives 
us a good measure to see whether our auto- 
matic method discovered systematic relations 
that correspond to human intuitions. 
A cousin relation in WordNet is defined be- 
tween two synsets (currently in the noun trees 
only), and it indicates that senses of a word that 
appear in both of the (sub)trees rooted by those 
synsets are related, s The cousins were manuMly 
extracted by the WordNet lexicographers. Ta- 
ble 2 shows the number of cousins listed for each 
top relation class and the number of cousins our 
automatic method recovered (in the 'Auto' col- 
umn). As you see, the total recall ratio is over 
80% (27/33~ .82). 
In the right three columns of Table 2, we also 
show the breakdown of the recovered cousins, 
whether each recovered one was an exact match, 
or it was more general or specific than the cor- 
responding WordNet cousin. From this, we 
can see that more than half of the recovered 
cousins were more general than the WordNet 
cousins. That is partly because some WordNet 
cousins have only one or two common words. 
For example, a WordNet cousin \[PAINTING, 
COLORING_MATERIAL\] in ARTIFACT-SUBSTANCE 
has only one common word "watercolor". Such 
SActually, cousin is one of the three relations which 
indicate the grouping of related senses of a word. Others 
are sister and twin. In this paper, we use cousin to refer 
to all relations listed in "cousin.tps" file (available in a 
WordNet distribution). 
26 
a minor relation tends to be lost in our tree gen- 
eralization procedure. However, the main rea- 
son is the difficulty mentioned earlier in the pa- 
per: the problem of applying the tree-cut ech- 
nique to a bushy tree when the data is sparse. 
In addition to the WordNet cousins, our auto- 
matic extraction method discovered several in- 
teresting relations. Table 3 shows some exam- 
ples, 
5 Conc lus ions  and  Future  Work  
In this paper, we proposed an automatic 
method for extracting systematic polysemy 
from WordNet. As we reported, preliminary re- 
sults show that our method identified almost 
all WordNet cousins as well as some new ones. 
One difficulty is that applying the generaliza- 
tion technique using the MDL principle to the 
bushy WordNet trees seems to yield a tree-cut 
at rather abstract level. 
For future work, we plan to compare the 
systematic relations extracted by our automatic 
method to corpus data. In particular, we like 
to test whether our method extracts the same 
groups of senses which human annotators 
disagreed (Ng et al, 1999). We also like to test 
whether our method agrees with the finding 
that multiple senses which occur in a discourse 
are often systematically polysemous (Krovetz, 
1998). 
Re ferences  
Apresjan, J. (1973). Regular Polysemy. Lin- 
guistics, (142). 
Buitelaar, P. (1997). A Lexicon for Underspec- 
ified Semantic Tagging. In Proceedings off the 
A CL S IGLEX Workshop on Tagging Text 
Table 2: Automatic Clusters vs. WordNet Cousins 
Top relation class WN cousin 
ACTION-LOCATION 
ARTIFACT-GROUP 
ARTIFACT-MEASURE 
ARTIFACT-SUBSTANCE 
COMMUNICATION-PERSON 
Total 33 
Auto  Exact Gen Spec 
2 1 0 1 0 
6 6 1 5 0 
1 1 0 1 0 
15 13 3 9 1 
9 6 5 1 0 
27 9 17 1 
Table 3: Examples of Automatically Extracted Systematic Polysemy 
Top relation class 
ACTION-LOCATION 
Relation 
\[ACTION, POINT\] 
Common Words 
\[VOICE, SINGER\] 
"drop", "circle", "intersection", dig", 
"crossing", "bull's eye" 
ARTIFACT-GROUP \[STRUCTURE, P OPLE\] "house", "convent", "market", "center" 
ARTIFACT-SUBSTANCE \[FABRIC, CHEMICAL_COMPOUND\] "acetate", "nylon", "acrylic", "polyester" 
COMMUNI CATI 0N-PERSON 
\[WRITING, RELIGIOUS-PERSON\] 
"soprano", "alto", "tenor", "baritone" 
"John", "Matthew", "Jonah", "Joshua", 
"Jeremiah" 
with Lexical Semantics, Washington, D.C., 
pp. 25-33. 
Buitelaar, P. (1998). CORELEX: Systematic 
Polysemy and Underspecification. Ph.D. dis- 
sertation, Department of Computer Science, 
Brandeis University. 
Copestake, A. and Briscoe, T. (1995). Semi- 
productive Polysemy and Sense Extension. 
Journal of Semantics, 12. 
Cruse, D. (1986). Lexical Semantics, Cam- 
bridge University Press. 
Huffman, D. A. (1952). A Model for the Con- 
struction of Minimum Redundancy Codes. 
In Proceedings ofthe IRE, 40. 
Kilgarriff, A. (1998a)~ SENSEVAL: An Exer- 
cise in Evaluating Word Sense Disambigua- 
tion Programs. In Proceedings of the LREC 
Kilgarriff, A. (1998b). Inter-tagger Agree- 
ment. In Advanced Papers of the SENSE- 
VAL Workshop, Sussex, UK. 
Krovetz, R. (1998). More than One Sense Per 
Discourse. In Advanced Papers of the SEN- 
SEVAL Workshop, Sussex, UK. 
Li, H. and Abe, N. (1998). Generalizing Case 
Frames Using a Thesaurus and the MDL 
Principle, Computational Linguistics, 24(2), 
pp. 217-244 
Miller, G. (eds.) (1990). WORDNET: An On- 
line Lexical Database. International Journal 
of Lexicography, 3 (4). 
Ng, H.T., Lim, C. and Foo, S. (1999). A 
Case Study on Inter-Annotator Agreement 
for Word Sense Disambiguationl In Proceed- 
ings of the A CL SIGLEX Workshop on Stan- 
dardizing Lexical Resources, College Park, 
MD. 
Nunberg, G. (1995). Transfers of Meaning. 
Journal of Semantics, 12. 
Procter, P. (1978). Longman dictionary of 
Contemporary English, Longman Group. 
Pustejovsky, J. (1995). The Generative Lexi- 
con, The MIT Press. 
Rissanen, J. (1978). Modeling by Shortest 
Data Description. Automatic, 14. 
Veronis, J. (1998). A Study of Polysemy Judge- 
ments and Inter-annotator Agreement. In 
Advanced Papers of the SENSEVAL Work- 
shop, Sussex, UK. 
Vossen, P., Peters, W. and Gonzalo, J. (1999). 
Towards a Universal Index of Meaning. In 
Proceedings of the A CL SIGLEX Workshop 
on Standardizing Lexical Resources, College 
Park, MD. 
27 
Question Terminology and Representation for
Question Type Classication
Noriko Tomuro
DePaul University
School of Computer Science Telecommunications and Information Systems
 S Wabash Ave
Chicago IL  USA
tomurocsdepauledu
Abstract
Question terminology is a set of terms which ap
pear in keywords idioms and xed expressions
commonly observed in questions This paper
investigates ways to automatically extract ques
tion terminology from a corpus of questions and
represent them for the purpose of classifying by
question type Our key interest is to see whether
or not semantic features can enhance the repre
sentation of strongly lexical nature of question
sentences We compare two feature sets one
with lexical features only and another with a
mixture of lexical and semantic features For
evaluation we measure the classication accu
racy made by two machine learning algorithms
C and PEBLS by using a procedure called
domain crossvalidation which eectively mea
sures the domain transferability of features
 Introduction
In Information Retrieval 	IR
 text categoriza
tion and clustering documents are usually in
dexed and represented by domain terminology
terms which are particular to the domaintopic
of a document However when documents must
be retrieved or categorized according to criteria
which do not correspond to the domains such as
genre 	text style
 	Kessler et al  Finn et
al 
 or subjectivity 	eg opinion vs fac
tual description
 	Wiebe 
 we must use
dierent domainindependent features to index
and represent documents In those tasks selec
tion of the features is in fact one of the most
critical factors which aect the performance of
a system
Question type classication is one of such
tasks where categories are question types 	eg
howto why and where
 In recent years
question type has been successfully used in
many QuestionAnswering 	QA
 systems for
determining the kind of entity or concept be
ing asked and extracting an appropriate answer
	Voorhees  Harabagiu et al  Hovy
et al 
 Just like genre question types
cut across domains for instance we can ask
howto questions in the cooking domain the
legal domain etc However features that consti
tute question types are dierent from those used
for genre classication 	typically partofspeech
or metalingusitic features
 in that features are
strongly lexical due to the large amount of id
iosyncrasy 	keywords idioms or syntactic con
structions
 that is frequently observed in ques
tion sentences For example we can easily think
of question patterns such as What is the best
way to  and What do I have to do to  In
this regard terms which identify question type
are considered to form a terminology of their
own which we dene as question terminology
Terms in question terminology have some
characteristics First they are mostly domain
independent noncontent words Second they
include many closedclass words 	such as in
terrogatives modals and pronouns
 and some
openclass words 	eg the noun way and the
verb do
 In a way question terminology is a
complement of domain terminology
Automatic extraction of question terminology
is a rather dicult task since question terms are
mixed in with content terms Another compli
cating factor is paraphrasing  there are many
ways to ask the same question For example
 How can I clean teapots
 In what way can we clean teapots
 What is the best way to clean teapots
 What method is used for cleaning teapots
 How do I go about cleaning teapots
In this paper we present the results of our
investigation on how to automatically extract
question terminology from a corpus of questions
and represent them for the purpose of classi
fying by question type It is an extension of
our previous work 	Tomuro and Lytinen 

where we compared automatic and manual tech
niques to select features from questions but
only 	stemmed
 words were considered for fea
tures The focus of the current work is to in
vestigate the kinds of features rather than
selection techniques which are best suited for
representing questions for classication Specif
ically from a large dataset of questions we au
tomatically extracted two sets of features one
set consisting of terms 	ie lexical features

only and another set consisting of a mixture of
terms and semantic concepts 	ie semantic fea
tures
 Our particular interest is to see whether
or not semantic concepts can enhance the repre
sentation of strongly lexical nature of question
sentences To this end we apply two machine
learning algorithms 	C 	Quinlan 
 and
PEBLS 	Cost and Salzberg 

 and com
pare the classication accuracy produced for the
two feature sets The results show that there is
no signicant increase by either algorithm by
the addition of semantic features
The original motivation behind our work on
question terminology was to improve the re
trieval accuracy of our system called FAQFinder
	Burke et al  Lytinen and Tomuro 

FAQFinder is a webbased natural language
QA system which uses Usenet Frequently
Asked Questions 	FAQ
 les to answer users
questions Figures  and  show an example ses
sion with FAQFinder First the user enters a
question in natural language The system then
searches the FAQ les for questions that are
similar to the users Based on the results of
the search FAQFinder displays a maximum of
 FAQ questions which are ranked the highest
by the systems similarity measure Currently
FAQFinder incorporates question type as one of
the four metrics in measuring the similarity be
tween the users question and FAQ questions

In the present implementation the system uses
a small set of manually selected words to deter
mine the type of a question The goal of our
work here is to derive optimal features which
would produce improved classication accuracy

The other three metrics are vector similarity seman
tic similarity and coverage Lytinen and Tomuro 
Figure  User question entered as a natural
language query to FAQFinder
Figure  The  bestmatching FAQ questions
 Question Types
In our work we dened  question types below
 DEF definition  PRC procedure
 REF reference  MNR manner
 TME time  DEG degree
 LOC location 	 ATR atrans

 ENT entity  INT interval
 RSN reason  YNQ yesno
Descriptive denitions of these types are
found in 	Tomuro and Lytinen 
 Table
 shows example FAQ questions which we had
used to develop the question types Note that
our question types are general question cate
gories They are aimed to cover a wide variety
of questions entered by the FAQFinder users
 Selection of Feature Sets
In our current work we utilized two feature sets
one set consisting of lexical features only 	LEX

and another set consisting of a mixture of lexi
cal features and semantic concepts 	LEXSEM

Obviously there are many known keywords id
ioms and xed expressions commonly observed
in question sentences However categorization
of some of our  question types seem to de
pend on openclass words for instance What
does mpg mean 	DEF
 and What does Bel
gium import and export 	REF
 To distin
guish those types semantic features seem eec
tive Semantic features could also be useful as
backo features since they allow for generaliza
tion For example in WordNet 	Miller 

the noun knowhow is encoded as a hypernym
of method methodology solution and
technique By selecting such abstract con
cepts as semantic features we can cover a va
riety of paraphrases even for xed expressions
and supplement the coverage of lexical features
We selected the two feature sets in the follow
ing two steps In the rst step using a dataset
of  example questions taken from Interrogative Reformulation Patterns and Acquisition of Question
Paraphrases
Noriko Tomuro
DePaul University
School of Computer Science, Telecommunications and Information Systems
243 S. Wabash Ave.
Chicago, IL 60604 U.S.A.
tomuro@cs.depaul.edu
Abstract
We describe a set of paraphrase patterns
for questions which we derived from a
corpus of questions, and report the result
of using them in the automatic recogni-
tion of question paraphrases. The aim
of our paraphrase patterns is to factor out
different syntactic variations of interroga-
tive words, since the interrogative part of
a question adds a syntactic superstructure
on the sentence part (i.e., the rest of the
question), thereby making it difficult for
an automatic system to analyze the ques-
tion. The patterns we derived are rules
which map surface syntactic structures to
semantic case frames, which serve as the
canonical representation of questions. We
also describe the process in which we
acquired question paraphrases, which we
used as the test data. The results obtained
by using the patterns in paraphrase recog-
nition were quite promising.
1 Introduction
The phenomenon of paraphrase in human languages
is essentially the inverse of ambiguity ? a given
sentence could ambiguously have several meanings,
while any given meaning could be formulated into
several paraphrases using various words and syntac-
tic constructions. For this reason, paraphrase poses
a great challenge for many Natural Language Pro-
cessing (NLP) tasks, just as ambiguity does, notably
in text summarization and NL generation (Barzilay
and Lee, 2003; Pang et al, 2003).
The problem of paraphrase is important in
Question-Answering systems as well, because the
systems must return the same answer to questions
which ask for the same thing but are expressed in
different ways. Recently there have been several
work which utilized reformulations of questions as
a way to fill the chasm between words in a question
and those in a potential answer sentence (Hermjakob
et al, 2002; Murata and Isahara, 2001; Agichtei et
al., 2001). In general, paraphrasing a question, be it
for recognition or generation, is more difficult than
a declarative sentence, because interrogative words
carry a meaning of their own, which is subject to re-
formulation, in addition to the rest (or the sentence
part) of the question. Reformulations of the interrog-
ative part of questions have some interesting char-
acteristics which are distinct from reformulations of
the sentence part or declarative sentences. First,
paraphrases of interrogatives are strongly lexical and
idiosyncratic, containing many keywords, idioms or
fixed expressions. For example, for a question ?How
can I clean teapots?? one can easily think of some
variations of the ?how? part while fixing the sentence
part:
- ?In what way should I clean teapots??
- ?What do I have to do to clean teapots??
- ?What is the best way to clean teapots??
- ?What method is used for cleaning teapots??
- ?How do I go about cleaning teapots??
- ?What is involved in cleaning teapots?
- ?What should I do if I want to clean teapots?
Second, reformulation patterns of interrogatives
seem to be governed by question types. For exam-
ple, the variation patterns above apply to almost all
?how-to? questions, while ?why? questions undergo
a different set of transformations (e.g. ?Why ..?,
?For what reason ..?, ?What was the reason why ..?
etc.). Also, further observations suggest that ques-
tions of the same question type have the same se-
mantic empty category: something (or some things)
which a question is asking.
In this paper, we describe the set of para-
phrase/reformulation patterns we derived from a
corpus of questions, and report the result of using
them in the automatic recognition of question para-
phrases. We also describe the process in which we
acquired paraphrases, which we used as the test data.
Our approaches to constructing those resources were
manual ? the transformation patterns were derived
by inspecting an existing large corpus of questions,
and the paraphrases were collected by asking web
users to type in reformulations of sample questions.
Our work here is focused on the reformulations of
the interrogative part of questions in contrast to other
work in question-answering where major emphases
are placed on the reformulations of phrases or words
in the sentence part (Lin and Pantel, 2001; Herm-
jakob et al, 2002). The patterns we derived are
essentially rules which map surface syntactic struc-
tures to semantic case frame representations. We use
those case frame representations when we compare
questions for similarity. The results obtained by the
use of the patterns in paraphrase recognition were
quite promising.
The motivation behind the work we present here
is to improve the retrieval accuracy of our system
called FAQFinder (Burke et al, 1997). FAQFinder is
a web-based, natural language question-answering
system which uses Usenet Frequently Asked Ques-
tions (FAQ) files to answer users? questions. Each
FAQ file contains a list of question-and-answer
(Q&A) pairs on a particular subject. Given a user?s
question as a query, FAQFinder tries to find an an-
swer by matching the user?s question against the
question part of each Q&A pair, and displays 5 FAQ
questions which are ranked the highest by the sys-
tem?s similarity measure. Thus, FAQFinder?s task is
to identify FAQ questions which are the best para-
phrases of the user?s question. Figure 1 shows a
screen snapshot of FAQFinder where a user?s query
Figure 1: The 5 best-matching FAQ questions re-
turned by FAQFinder
?What do I have to do to clean teapots?? is matched
against the Q&A pairs in ?drink tea faq?. The cur-
rent similarity measure used in the system is a com-
bination of four independent metrics: term vector
similarity, coverage, semantic similarity, and ques-
tion type similarity (Lytinen and Tomuro, 2002). Al-
though those metrics are additive and complemental
to each other, they cannot capture the relations and
interactions between them. The idea of paraphrase
patterns proposed in this paper is a first step in devel-
oping an alternative, integrated similarity measure
for question sentences.
2 Paraphrasing Patterns for Questions
2.1 Training Data
Paraphrasing patterns were extracted from a large
corpus of question sentences which we had used in
our previous work (Tomuro and Lytinen, 2001; Lyti-
nen and Tomuro, 2002). It consisted of 12938 exam-
ple questions taken from 485 Usenet FAQ files. In
the current work, we used a subset of that corpus
consisting of examples whose question types were
PRC (procedure), RSN (reason) or ATR (atrans).
Those question types are members of the 12 ques-
tion types we had defined in our previous work (To-
muro and Lytinen, 2001). As described in that paper,
PRC questions are typical ?how-to? questions and
RSN questions are ?why? questions. The type ATR
;(1) how can/do .. anyVerb
(defpattern prc-how 1
(:WH how) (:S <NPS>) (:V <V>) (:O <NPO>)
=>
(:proc ?) (:actor <NPS>) (:verb <V>) (:theme <NPO>))
;(2) how can/do .. obtain
(defpattern atr-1-how-obtainV 3
(:WH how) (:S <NPS>) (:V <obtainV>) (:O <NPO>)
=>
(:source ?) (:proc ?) (:actor <NPS>) (:verb <obtainV>) (:theme <NPO>))
;(3) what is the .. method for obtaining
(defpattern atr-1-what-is-method 4
(:WH what) (:S NIL) (:V <beV>) (:O <methodN>) (:VG <obtainV>) (:NP <NPO>)
=>
(:source ?) (:proc ?) (:actor I) (:verb <obtainV>) (:theme <NPO>))
;(4) who sells
(defpattern atr-who-sourceNP 4
(:WH who) (:S NIL) (:V <sellV>) (:O <NPO>)
=>
(:source ?) (:proc ?) (:actor I) (:verb obtain) (:theme <NPO>))
Figure 2: Example Paraphrase Patterns
(for ATRANS in Conceptual Dependency (Schank,
1973)) is essentially a special case of PRC, where
the (desire for the) transfer of possession is strongly
implied. An example question of this type would
be ?How can I get tickets for the Indy 500??. Not
only do ATR questions undergo the paraphrasing
patterns of PRC questions, they also allow reformu-
lations which ask for the (source or destination) lo-
cation or entity of the thing(s) being sought, for in-
stance, ?Where can I get tickets for the Indy 500??
and ?Who sells tickets for the Indy 500??. We
had observed that such ATR questions were in fact
asked quite frequently in question-answering sys-
tems.1 Also those question types seem to have a
richer set of paraphrasing patterns than other types
(such as definition or simple reference questions
given in TREC competitions (Voorhees, 2000)) with
regard to the interrogative reformulation. In the cor-
pus, there were 2417, 1022 and 968 questions of
type PRC, RSN, ATR respectively, and they consti-
tuted the training data in the current work.
1Although we did not use it in the current work, we
also had access to the user log of AskJeeves system
(http://www.askjeeves.com). We observed that a large
portion of the user questions were ATR questions.
2.2 Paraphrase Patterns
The aim of our paraphrasing patterns is to account
for different syntactic variations of interrogative
words. As we showed examples in section 1, the
interrogative part of a question adds a syntactic su-
perstructure to the sentence part, thereby making it
difficult for an automatic system to get to the core of
the question. By removing this syntactic overhead,
we can derive the canonical representations of ques-
tions, and by using them we can perform a many-
to-one matching instead of many-to-many when we
compare questions for similarity.
In the pre-processing stage, we first applied a
shallow parser to each question in the training data
and extracted its phrase structure. The parser we
used is customized for interrogative sentences, and
its complexity is equivalent to a finite-state machine.
The output of the parser is a list of phrases in which
each phrase is labeled with its syntactic function
in the question (subject, verb, object etc.). Passive
questions are converted to active voice in the last
step of the parser by inverting the subject and object
noun phrases. Then using the pre-processed data,
we manually inspected all questions and defined pat-
terns which seemed to apply to more than two in-
stances. By this enumeration process, we derived
a total of 127 patterns, consisting of 18, 23 and 86
patterns for PRC, RSN and ATR respectively.
Each pattern is expressed in the form of a rule,
where the left-hand side (LHS) expresses the phrase
structure of a question, and the right-hand side
(RHS) expresses the semantic case frame represen-
tation of the question. When a rule is matched
against a question, the LHS of the rule is compared
with the question first, and if they match, the RHS is
generated using the variable binding obtained from
the LHS. Figure 2 shows some example patterns.
In a pattern, both LHS and RHS are a set of slot-
value tuples. In each tuple, the first element, which
is always prefixed with :, is the slot name and the
remaining elements are the values. Slots names
which appear on the LHS (:S, :V, :O, etc.) relate
to syntactic phrases, while those on the RHS (:ac-
tor, :theme, :source etc.) indicate semantic cases. A
slot value could be either a variable, indicated by a
symbol enclosed in .. (e.g. NPS), or a con-
stant (e.g. how). A variable could be either con-
strained (e.g. obtainV) or unconstrained (e.g.
NPS, NPO). Constrained variables are de-
fined separately, and they specify that a phrase to
be matched must satisfy certain conditions. Most
of the conditions are lexical constraints ? a phrase
must contain a word of a certain class. For instance,
obtainV denotes a word class ?obtainV? and it
includes words such as ?obtain?, ?get?, ?buy? and
?purchase?. Word classes are groupings of words
appeared in the training data which have similar
meanings (i.e., synonyms), and they were developed
in tandem with the paraphrase patterns. Whether
constrained or unconstrained, a variable gets bound
with one or more words in the matched question (if
possible for constrained variables). A constant indi-
cates a word and requires the word to exist in the tu-
ple. ?NIL? and ??? are special constants where ?NIL?
requires the tuple (phrase in the matched question)
to be empty, and ??? indicates that the slot is an
empty category. Each rule is also given a priority
level (e.g. 3 in pattern (2)), with a large number in-
dicating a high priority.
In the example patterns shown in Figure 2, pat-
tern (1) matches a typical ?how-to? question such
as ?How do I make beer??. Its meaning, accord-
ing to the case frame generated by the RHS, would
be ?I? for the actor, ?make? for the verb, ?beer? for
the theme, and the empty category is :proc (for pro-
Figure 3: Paraphrase Entry Site
cedure). Patterns (2) through (4) are rules for ATR
questions. Notice they all have two empty categories
? :proc and :source ? as consistent with our defini-
tion of type ATR. Also notice the semantic case roles
are taken from various syntactic phrases: pattern (2)
takes the actor and theme from syntactic subject and
object straight-forwardly, while pattern (3), which
matches a question such as ?What is a good way to
buy tickets for the Indy 500?, takes the theme from
the object in the infinitival phrase (:NP) and fills the
actor with ?I? which is implicit in the question. Pat-
tern (4), which matches a question such as ?Who
sells tickets for the Indy 500?, changes the verb to
?obtain? as well as filling the implicit actor with ?I?.
This way, ATR paraphrases are mapped to identical
case frames (modulo variable binding).
3 Acquisition of Question Paraphrases
To evaluate the question paraphrase patterns, we
used the set of question paraphrases which we had
acquired in our previous work (Tomuro and Lytinen,
2001) for the test data. In that work, we obtained
question paraphrases in the following way. First we
selected a total of 35 questions from 5 FAQ cate-
gories: astronomy, copyright, gasoline, mutual-
fund and tea. Then we created a web site where
users could enter paraphrases for any of the 35 ques-
tions. Figure 3 shows a snapshot of the site when the
astronomy FAQ is displayed.2 After keeping the site
public for two weeks, a total of 1000 paraphrases
were entered. Then we inspected each entry and dis-
carded ill-formed ones (such as keywords or boolean
queries) and incorrect paraphrases. This process left
us with 714 correct paraphrases (including the orig-
inal 35 questions).
Figure 4 shows two sets of example paraphrases
entered by the site visitors. In each set, the first
sentence in bold-face is the original question (and
its question type). In the paraphrases of the first
question, we see more variations of the interroga-
tive part of ATR questions. For instance, 1c ex-
plicitly refers to the source location/entity as ?store?
and 1d uses ?place?. Those words are essentially
hyponyms/specializations of the concept ?location?.
Paraphrases of the second question, on the other
hand, show variations in the sentence part of the
questions. The expression ?same face? in the origi-
nal question is rephrased as ?one side? (2a), ?same
side? (2b), ?not .. other side? (2c) and ?dark
side? (2f). The verb is changed from ?show? to
?face? (2b), ?see? (2c, 2d) and ?look? (2e). Those
rephrasings are rather subtle, requiring deep seman-
tic knowledge and inference beyond lexical seman-
tics, that is, the common-sense knowledge.
To see the kinds of rephrasing the web users en-
tered, we categorized the 679 (= 714 - 35) para-
phrased questions roughly into the following 6 cate-
gories.3
(1) Lexical substitution ? synonyms; involves no
or minimal sentence transformation
(2) Passivization
(3) Verb denominalization ? e.g. ?destroy? vs.
?destruction?
(4) Lexical semantics & inference ? e.g. ?show?
vs. ?see?
(5) Interrogative reformation ? variations in the in-
terrogative part
(6) Common-sense ? e.g. ?dark side of the Moon?
Table 1 shows the breakdown by those categories.
As you see, interrogative transformation had the
2In order to give a context to a question, we put a link
(?wanna know the answer??) to the actual Q&A pair in the
FAQ file for each sample question.
3If a paraphrase fell under two or more categories, the one
with the highest number was chosen.
Table 1: Breakdown of the paraphrases by para-
phrase category
Category # of paraphrases
(1) Lexical substitution 168 (25 %)
(2) Passivization 37 (5 %)
(3) Verb denominalization 18 (3 %)
(4) Lexical semantics & inference 107 (16 %)
(5) Interrogative reformation 339 (50 %)
(6) Common-sense 10 (1 %)
Total 679 (100 %)
largest proportion. This was partly because all trans-
formations to questions that start with ?What? were
classified as this category. But the data indeed con-
tained many instances of transformation between
different interrogatives (why  how  where 
who etc.). From the statistics above, we can thus
see the importance of understanding the reformula-
tions of the interrogatives. As for other categories,
lexical substitution had the next largest proportion.
This means a fair number of users entered rela-
tively simple transformations. On this, (Lin and Pan-
tel, 2001) makes a comment on manually generated
paraphrases (as versus automatically extracted para-
phrases): ?It is difficult for humans to generate a di-
verse list of paraphrases, given a starting formula-
tion and no context?. Our data is in agreement with
their observations indeed.
4 Evaluation
Using the paraphrase data described in the previous
section, we evaluated our question reformulation
patterns on coverage and in the paraphrase recogni-
tion task. From the data, we selected all paraphrases
derived from the original questions of type PRC,
RSN and ATR. There were 306 such examples, and
they constituted the testset for the evaluation.
4.1 Coverage
We first applied the transformation patterns to all ex-
amples in the testset and generated their case frame
representations. In the 306 examples, 289 of them
found at least one pattern. If an example matched
with two or more patterns, the one with the highest
priority was selected. Thus the coverage was 94%.
However after inspecting the results, we observed
that in some successful matches, the syntactic struc-
ture of the question did not exactly correspond to
1. Where can I get British tea in the United States? [ATR]
a. How can I locate some British tea in the United States?
b. Who sells English tea in the U.S.?
c. What stores carry British tea in the United States?
d. Where is the best place to find English tea in the U.S.?
e. Where exactly should I go to buy British tea in the U.S.?
f. How can an American find British tea?
2. Why does the Moon always show the same face to the Earth? [RSN]
a. What is the reason why the Moon show only one side to the Earth?
b. Why is the same side of the Moon facing the Earth all the time?
c. How come we do not see the other side of the Moon from Earth?
d. Why do we always see the same side of the Moon?
e. Why do the Moon always look the same from here?
f. Why is there the dark side of Moon?
Figure 4: Examples of question paraphrases entered by the web users
the pattern as intended. For example, ?How can I
learn to drink less tea and coffee??4 matched the
pattern (1) shown in Figure 2 and produced a frame
where ?I? was the actor, ?learn? was the verb and
the theme was null (because the shallow parser an-
alyzed ?to drink less tea and coffee? to be a verb
modifier). Although the difficulty with this example
was incurred by inadequate pre-processing or inher-
ent difficulty in shallow parsing, the end result was a
spurious match nonetheless. In the 289 matches, 15
of them were such false matches.
As for the 17 examples which failed to match
with any patterns, one example is ?What internet re-
sources exist regarding copyright??5 ? there were
patterns that matched the interrogative part (?What
internet resources?), but all of them had constrained
variables for the verb which did not match ?exist?.
Other failed matches were because of elusive para-
phrasing. For example, for an original question
?Why is evaporative emissions a problem??, web
users entered ?What?s up with evaporative emis-
sions?? and ?What is wrong with evaporative emis-
sions??. Those paraphrases seem to be keyed off
from ?problem? rather than ?why?.
4The original question for this paraphrase was ?How can I
get rid of a caffeine habit??.
5This question can be paraphrased as ?Where can I find in-
formation about copyright on the internet??
4.2 Paraphrase Recognition
Using the case frame representations derived from
the first experiment, we applied a frame similarity
measure for all pairs of frames. This measure is
rather rudimentary, and we are planning to fine-tune
it in the future work. This measure focuses on the
effect of paraphrase patterns ? how much the canon-
ical representations, after the variations of interrog-
atives are factored out, can bring closer the (true)
paraphrases (i.e., questions generated from the same
original question), thereby possibly improving the
recognition of paraphrases.
The frame similarity between a pair of frames is
defined as a weighted sum of two similarity scores:
one for the interrogative part (which we call inter-
rogative similarity) and another for the sentence part
(which we call case role similarity). The interrog-
ative similarity is obtained by computing the av-
erage slot-wise correspondence of the empty cate-
gories (slots whose value is ???), where the corre-
spondence value of a slot is 1 if both frames have
??? for the slot or 0 otherwise. The case role simi-
larity, on the other hand, is obtained by computing
the distance between two term vectors, where terms
are the union of words that appeared in the remain-
ing slots (i.e., non-empty category slots) of the two
frames. Those terms/words are considered as a bag
of words (as in Information Retrieval), irrespective
of the order or the slots in which they appeared. We
00.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Rejection
R
ec
al
l FrSim_0.5
FrSim_0.0
Sent
Figure 5: Recall vs. Rejection
chose this scheme for the non-empty category slots
because our current work does not address the issue
of paraphrases in the sentence part of the questions
(as we mentioned earlier). Value of each term in a
frame is either 1 if the word is present in the frame
or 0 otherwise, and the cosine of the two vectors is
returned as the distance. The final frame similarity
value, after applying weights which sum to 1, would
be between 0 and 1, where 1 indicates the strongest
similarity.6
Using the frame similarity measure, we computed
two versions ? one with 0.5 for the weight of the in-
terrogative similarity and another with 0.0. In addi-
tion, we also computed a baseline metric, sentence
similarity. It was computed as the term vector simi-
larity where terms in the vectors were taken from the
phrase representation of the questions (i.e., syntactic
phrases generated by the shallow parser). Thus the
terms here included various wh-interrogative words
as well as words that were dropped or changed in
the paraphrase patterns (e.g. words instantiated with
methodN in pattern (3) in Figure 2). This metric
produces a value between 0 and 1, thus it is compa-
rable to the frame similarity.
The determination of whether or not two frames
(or questions) are paraphrase of each other depends
on the threshold value ? if the similarity value is
above a certain threshold, the two frames/questions
are determined to be paraphrases. With the 306 case
frames in the testset, there were a total of 46665 (=


) distinct combinations of frames, and 3811
6If either one of the frames is null (for which the pattern-
matching failed), the frame similarity is 0.
of them were (true) paraphrases. After computing
the three metrics (two versions of frame similarity,
plus sentence similarity) for all pairs, we evaluated
their performance by examining the trade-off be-
tween recall and rejection for varying threshold val-
ues. Recall is defined in the usual way, as the ratio of
true positives # classified as paraphrase# true paraphrases , and re-
jection is defined as the ratio of true negatives 
# classified as non-paraphrase
# true non-paraphrases . We chose to use
rejection instead of precision or accuracy because
those measures are not normalized for the number of
instances in the classification category (# true para-
phrases vs. # true non-paraphrases); since our test-
set had a skewed distribution (8% paraphrases, 92%
non-paraphrases), those measures would have only
given scores in which the results for paraphrases was
overshadowed by those for non-paraphrases.
Figure 5 shows the recall vs. rejection curves for
the three metrics. As you see, both versions of the
frame similarity (FrSim 0.5 and FrSim 0.0 in the
figure) outperformed the sentence similarity (Sent),
suggesting that the use of semantic representation
was very effective in recognizing paraphrases com-
pared to syntactic representation. For example, Fr-
Sim 0.5 correctly recognized 90% of the true para-
phrases while making only a 10% error in recogniz-
ing false positives, whereas Sent made a slightly
over 20% error in achieving the same 90% recall
level. This is a quite encouraging result.
The figure also shows that FrSim 0.5 performed
much better than FrSim 0.0. This means that ex-
plicit representation of empty categories (or question
types) contributed significantly to the paraphrase
recognition. This also underscores the importance
of considering the formulations of interrogatives in
analyzing question sentences.
5 Conclusions and Future Work
In this paper, we showed that automatic recogni-
tion of question paraphrases can benefit from un-
derstanding the various formulations of the interrog-
ative part. Our paraphrase patterns remove those
variations and produce canonical forms which re-
flect the meaning of the questions (i.e., case frames).
Not only does this semantic representation facili-
tates simple and straight-forward ways to compute
the similarity of questions, it also produces more ac-
curate results than syntactic phrase representation.
Our immediate future work is to define paraphrase
patterns for other question types. While doing so,
we would also like to look into ways to automati-
cally extract patterns. A good starting point would
be (Agichtei et al, 2001), which looked for common
n-grams anchored at the beginning of questions.
Once the syntactic superstructure of the interrog-
ative part is factored out, the next task is to tackle
reformulations of the sentence part of questions.
Lately several interesting efforts have been made to
extract paraphrase expressions automatically, for in-
stance (Lin and Pantel, 2001; Shinyama et al, 2002).
We would like to experiment doing the same with
the web as the resource.
Finally, we would like to synthesize the reformu-
lation patterns of the two parts of questions and de-
velop unified paraphrase patterns. Then we will in-
corporate this new approach in FAQFinder and con-
duct end-to-end question-answering experiments in
order to see how much the use of paraphrase patterns
can improve the performance of the system.
References
E. Agichtei, S. Lawrence, and L. Gravano. 2001. Learn-
ing search engine specific query transformations for
question answering. In Proceedings of the 10th In-
ternational World Wide Web Conference (WWW10),
Hong Kong.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proceedings of the DARPA Human Lan-
guage Technologies (HLT-2003).
R. Burke, K. Hammond, V. Kulyukin, S. Lytinen, N. To-
muro, and S. Schoenberg. 1997. Question answering
from frequently asked question files: Experiences with
the faqfinder system. AI Magazine, 18(2).
U. Hermjakob, E. Abdessamad, and D. Marcu. 2002.
Natural language based reformulation resource and
web exploitation for question answering. In Proceed-
ings of TREC-2002.
D. Lin and P. Pantel. 2001. Discovery of inference rules
for question answering. Natural Language Engineer-
ing, 7(4):343?360.
S. Lytinen and N. Tomuro. 2002. The use of question
types to match questions in faqfinder. In Papers from
the 2002 AAAI Spring Symposium on Mining Answers
from Texts and Knowledge Bases.
M. Murata and H. Isahara. 2001. Universal model
for paraphrasing using transformation based on a de-
fined criteria. In Proceedings of the workshop on Au-
tomatic Paraphrasing at NLP Pacific Rim (NLPRS-
2001), Tokyo, Japan.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceedings
of the DARPA Human Language Technologies (HLT-
2003).
R. Schank. 1973. Identification of conceptualiza-
tions underlying natural language. In R. Schank and
K. Colby, editors, Computer Models of Thought and
Language. Freeman.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news
articles. In Proceedings of Human Language Technol-
ogy Conference (HLT-2002).
N. Tomuro and S. Lytinen. 2001. Selecting features for
paraphrasing question sentences. In Proceedings of
the workshop on Automatic Paraphrasing at NLP Pa-
cific Rim (NLPRS-2001), Tokyo, Japan.
E. Voorhees. 2000. The trec-9 question answering track
report. In Proceedings of TREC-9.
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 42?50,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Construction of Disambiguated Folksonomy Ontologies Using Wikipedia
Noriko Tomuro and Andriy Shepitsen
DePaul University, College of Digital Media
243 S. Wabash, Chicago, IL USA
tomuro@cs.depaul.edu, ashepits@cdm.depaul.edu
Abstract
One of the difficulties in using Folk-
sonomies in computational systems is tag
ambiguity: tags with multiple meanings.
This paper presents a novel method for
building Folksonomy tag ontologies in
which the nodes are disambiguated. Our
method utilizes a clustering algorithm
called DSCBC, which was originally de-
veloped in Natural Language Processing
(NLP), to derive committees of tags, each
of which corresponds to one meaning or
domain. In this work, we use Wikipedia
as the external knowledge source for the
domains of the tags. Using the commit-
tees, an ambiguous tag is identified as one
which belongs to more than one commit-
tee. Then we apply a hierarchical agglom-
erative clustering algorithm to build an on-
tology of tags. The nodes in the derived
ontology are disambiguated in that an am-
biguous tag appears in several nodes in
the ontology, each of which corresponds
to one meaning of the tag. We evaluate the
derived ontology for its ontological den-
sity (how close similar tags are placed),
and its usefulness in applications, in par-
ticular for a personalized tag retrieval task.
The results showed marked improvements
over other approaches.
1 Introduction
In recent years, there has been a rapid growth in
social tagging systems ? so-called Folksonomies
where users assign keywords or tags to categorize
resources. Typically, the sources of folksonomies
are web resources, and virtually any kind of infor-
mation available on the Internet, ranging from web
pages (e.g. Delicious (delicious.com)), scientific ar-
ticles (e.g. Bibsonomy (www.bibsonomy.org)) to me-
dia resources (e.g. Flickr (www.flickr.com), Last.fm
(www.last.fm)). Although tags in folksonomies are
essentially semantic concepts, they have distinct
characteristics as compared to conventional se-
mantic resources which are often used in Natu-
ral Language Processing (NLP), such as WordNet
(Miller, 1990). First, folksonomy tags are unre-
stricted ? users are free to choose any words or
set of characters to formulate tags. One significant
problem arising from such free-formedness is tag
ambiguity: tags that have several meanings (e.g.
?Java? as coffee or a programming language or an
island in Indonesia). Second, folksonomy tags are
unstructured ? tags assigned to a given resource
are simply enumerated in a list (although often-
times using a varying font size to indicate popu-
larity), and no special organization or categoriza-
tion of the tags is made (by the Folksonomy site).
There have been several work recently which ex-
tracted structures from folksonomy tags and con-
structed ontologies (e.g. (Clough et al, 2005),
(Schmitz, 2006)). However, most of them evalu-
ate the effect of the extracted structures only in the
context of specific applications, for instance gen-
erating user recommendations (e.g. (Shepitsen et
al., 2008)).
In this work, we develop a novel method for
constructing ontologies from folksonomy tags.
In particular, we employ a clustering algorithm
called Domain Similarity Clustering By Commit-
tee (DSCBC) (Tomuro et al, 2007). DSCBC is an
extension of an algorithm called CBC (Pantel and
Lin, 2002), and was originally developed for lexi-
cal semantics in NLP to automatically derive sin-
gle/unambiguous word meanings (as committees)
from ambiguous words. In this work, DSCBC is
effectively adopted to derive disambiguated folk-
sonomy tag committees, where a committee in this
context is a cluster of tags in which the members
share the same or very similar concept in one of
their meanings. By using DSCBC, an ambiguous
tag is identified as one which belongs to more than
42
one committee. One of the key ideas in DSCBC is
the notion of feature domain similarity: the sim-
ilarity between the features themselves, obtained
a priori from sources external to the dataset used
at hand. For example, if data instances x and y
are represented by features f1 and f2, the feature
domain similarity refers to the similarity between
f1 and f2 (not between x and y). DSCBC uti-
lizes this feature domain similarity to derive clus-
ters whose domains are ?close?, thereby produc-
ing unambiguous committees. In this work, we in-
corporate Wikipedia as the external knowledge re-
source, and use the similarity between Wikipedia
articles to derive the committees of disambiguated
tags. Finally using the tag committees derived by
DSCBC, we build an ontology of tags by using a
modified hierarchical agglomerative clustering al-
gorithm. Ambiguous tags are mapped to several
nodes in this ontology.
Note that in this paper, we refer to the structure
derived by the hierarchical clustering algorithm as
an ?ontology? instead of a ?taxonomy?. That is be-
cause, in the algorithm, the parent-child relation is
determined by a similarity measure only, therefore
sometimes does not correspond to the subsump-
tion relation in the strict sense.
For evaluation, we construct an ontology from
the Delicious tags, and measure the quality (onto-
logical density) of the derived ontology by com-
paring with the ontologies obtained without using
Wikipedia. We also use the derived ontology in
a personalized information retrieval task. The re-
sults show that our method achieved marked im-
provements over other approaches.
2 Related Work
Several efforts have been made recently which fo-
cused on extracting structures from folksonomies.
Clough (Clough et al, 2005) and Schmitz
(Schmitz, 2006) derived hierarchical structures
from image folksonomies (St. Andrew collection
(specialcollections.st-and.ac.uk/photcol.htm) and Flickr,
respectively). In addition to the hierarchi-
cal relation, they also derived other relations
such as ?type of?, ?aspect of?, ?same-as?, etc.
Mika (Mika, 2007) and Heymann (Heymann and
Garcia-Molina, 2006) proposed an automatic cre-
ation of tags in folksonomy networks based on
the tag co-occurrences among resources and users.
They then used a graph clustering algorithm to
connect tags which were used by the same users
and for the same resources to identify tag ?clouds?
and communities of like-minded users. However,
none of those work used NLP techniques, nor did
they deal with the tag ambiguity problem; Often-
times, highly ambiguous tags are even removed
from the data.
In our previous work (Shepitsen et al, 2008),
we used a standard hierarchical agglomerative
clustering algorithm to build a tag hierarchy. We
also considered only the most popular sense of an
ambiguous tag and ignored all other senses.
Wikipedia has been attracting much atten-
tion in the recent NLP research. For exam-
ple, Wikipedia as a lexical resource was ex-
ploited for thesauri construction (Milne et al,
2006) and for word sense disambiguation (Mi-
halcea and Csomai, 2007). Other NLP tasks in
which Wikipedia was utilized to provide contex-
tual and domain/encyclopedia knowledge include
question-answering (Ahn et al, 2004) and infor-
mation extraction (Culotta et al, 2006). In a simi-
lar vein, (Gabrilovich and Markovitch, 2006) also
used Wikipedia to improve the accuracy for text
categorization. An interesting text retrieval appli-
cation was done by Gurevych (Gurevych et al,
2007), in whichWikipedia was utilized to improve
the retrieval accuracy in matching the professional
interests of job applicants with the descriptions of
professions/careers.
The work presented in this paper applies an
NLP technique (the DSCBC algorithm), which in-
corporates the domain knowledge (Wikipedia) as
a critical component, to the task of extracting se-
mantic structure, in particular an ontology, from
folksonomies. Our method is novel, and the ex-
perimental results indicate that the derived ontol-
ogy was of high semantic quality.
3 Deriving Unambiguous Tag
Committees
The DSCBC algorithm, which we had developed
in our previous work (Tomuro et al, 2007), is
an extension of CBC Clustering (Pantel and Lin,
2002), modified to produce unambiguous clusters
when the data contained ambiguous instances. As-
suming the instances are represented by vectors of
features/domains, consider the following data:
a b c d
x: 1 1 0 0
y: 1 0 1 0
z: 1 0 0 1
43
where x, y, z are data instances, and a, b, c, d
are features. In most clustering algorithms, fea-
tures are assumed to be independent to each other,
or their dependencies are ignored. So in the ex-
ample, x is equally likely clustered with y or z,
because the similarity between x and y, and x and
z are the same (based on the Euclidean distance,
for example). However if we have a priori, gen-
eral knowledge about the features that b?s domain
is more similar to that of c than to d, it is better
to cluster x and y instead of x and z, because the
{x, y} cluster is ?tighter? than the {x, z} cluster
with respect to the domains of the features.
3.1 Feature Domain Similarity
In DSCBC, the general knowledge about the fea-
tures is incorporated as a measure called Fea-
ture Domain Similarity: the similarity between
the features themselves, obtained a priori from
sources external to the dataset used at hand. In this
work, we used Wikipedia as the external knowl-
edge source, and as the features to represent the
folksonomy tags. To this end, we first obtained the
most recent dump of Wikipedia and clustered the
articles to reduce the size of the data. We call such
a cluster of Wiki articles a Wiki concept. Cluster-
ing was based on the similarity of the terms which
appeared in the articles. Detailed descriptions of
the Wikipedia data and this clustering process are
given in section 5.1. Then given a set of folkson-
omy tags T , a set of folksonomy resources R and
a set of Wiki concepts W , we defined a matrix M
of size |T | ? |W |, where the rows are tags and the
columns/features are Wiki concepts. Each entry
in this matrix, for a tag t ? T and a Wiki con-
cept w ? W , was computed as the cosine between
two term vectors: one for t where the features are
terms used in (all of) the resources in R to which
t was assigned (by the folksonomy users), and an-
other for w where the features are terms used in
(all of) the Wiki articles in w. Thus, the matrix
M contains the similarity values for a given tag
to all Wikipedia concepts, thereby identifying the
(Wikipedia) domains of the tag.
Using the matrix M , we define the feature do-
main similarity between two tags f and g, denoted
fdSim(f, g), as:
fdSim(f, g) =
?
i
?
j fi ? gj ? cos(wi, wj)
?
?
i f2i ?
?
i g2i
where fi is the similarity of the tag f to the ith
Wiki concept (and likewise for g), and cos(wi, wj)
is the cosine (thus similarity) between the ith and
jth Wiki concepts. In this formula, the domain
knowledge is incorporated not only through the
way a tag is represented (as a vector of Wiki con-
cepts), but also directly by cos(wi, wj), the simi-
larity between Wiki concepts themselves.
In addition to Feature Domain Similarity, we
also incorporated a measure of reference tight-
ness for folksonomy tags and Wiki concepts. This
metric measures and takes advantage of the link
structure in the folksonomy system as well as
Wikipedia. For example, when a tag was assigned
to several web pages in the folksonomy system,
some of those pages may be reachable from each
other through hyperlinks ? in which case, we can
consider the tag?s domains are tight. Likewise for
Wiki concepts, if a folksonomy tag is ?similar?
to several Wiki concepts (for which the similar-
ity value is above some threshold), some of those
Wiki concepts may be reachable in the Wikipedia
structure ? then we can consider the tag?s domains
are tight as well. Furthermore, based on the notion
of reference tightness within a set of resources, we
define the connectedness between two sets of re-
sources as the fraction of the resources (web pages
orWiki concepts) in one set which are reachable to
resources in another set. We define the reference
tightness between two sets of resources S and U ,
denoted srt(S, U), as follows.
srt(S, U) =
?
s?S,u?U reach(s, u) + reach(u, s)
?
s?S nRef(s) +
?
u?U nRef(u)
where nRef(k) is the number of outgoing refer-
ence links in the resource k, and reach(a, b) is an
indicator function which returns 1 if any reference
link from the resource in a is reachable from any
resource in b or 0 otherwise. There are two terms
in the numerator because the reachability relation
is directional.
3.2 The DSCBC Algorithm
Using the notions of feature domain similarity and
reference tightness, we define the similarity be-
tween two tags f and g as follows.
dsSim(f, g) = ? ? fdSim(f, g)
+(1 ? ?) ? srt(Rf , Rg)
where Rf is the set of references from all web
pages to which the tag f is assigned, srt(Rf , Rg)
is the reference tightness between Rf and Rg, and
44
? is a weighting coefficient. In our experiments
(discussed in section 5), we set ? to be 0.8 based
on the results of the preliminary runs.
The DSCBC algorithm is shown in Algo-
rithm 1. DSCBC is an unsupervised clustering
algorithm which automatically derives a set of
committees. A committee is a group of folkson-
omy tags which are very similar to each other. In
Phase I, a set of preliminary tag clusters are first
created. In Phase II, some of those tag clusters are
selected as committees ? those which are dissimi-
lar/orthogonal to all other committees selected so
far. Then in Phase III, each tag is assigned to com-
mittees which are similar to the tag. The dsSim
function is used in Phase I and II to measure
the similarity between clusters and committees
respectively. In Phase III, an ambiguous tag is
assigned to one of more committees, where each
time the features of the assigned committee are
removed from the tag. Thus, ambiguous tags are
identified as those which belong to more than one
committee.
4 Building Folksonomy Tag Ontology
After obtaining the committees by DSCBC, we or-
ganize the tags into a ontology by using a modified
hierarchical agglomerative clustering algorithm.1
We first compute the pair-wise similarity between
any two tags and sort those pairs according to the
similarity values. Then we take the most similar
pair and create the first cluster. Afterwards, we it-
erate through the whole tag/cluster pairs and sub-
stitute all instances in which either tag is a mem-
ber, if the tag is not ambiguous, by the obtained
cluster, and repeat the process until the list of pairs
is empty. The committees derived by DSCBC are
utilized to identify ambiguous tags ? when a tag
belonged to more than one committee. When we
process an ambiguous tag, we first find its ?core
meaning? by finding the committee to which the
tag is most similar, then remove all (non-zero) fea-
tures that are encoded in committee from all in-
stances left in the dataset. With this scheme, we
can cover all senses of an ambiguous tag, for all
such tags, during ontology generation. The simi-
larity is computed using the dsSim function de-
scribed in the previous section; the only difference
that, if one member of a pair is a cluster, it is rep-
1Our algorithm is essentially a modification of the
Average-Link Clustering by (OConnor andHerlocker, 2001).
Input: Set of tags T. Tuning coefficients:
n - number of the most similar tags chosen for
the target tag
q - number of features for finding the centroid
? - similarity threshold for adding tags to
committees
? - similarity threshold for assigning tags to
committees
Output: Set of committees C. Set of tags T
where each t ? T is assigned to
committees in C.
Phase I. Finding set of clusters L
foreach ti ? T do
Select a set k of n most similar tj : i 6= j
add k to L if it is not already in L.
end
Phase II. Find Communities C
foreach c ? L do
Find the centroid of c using only q
features shared by most of tags in the
cluster
Add c to C if its similarity to every other
cluster is lower than ?
end
Phase III. Assign tags to committees
foreach t ? T do
Assign t to committee c in C if the
similarity is higher than ?
end
Algorithm 1: Clustering tags using DSCBC
45
resented by its centroid. Figure 1 shows an exam-
ple folksonomy ontology. The modified hierarchi-
cal agglomerative clustering algorithm is shown in
Algorithm 2.
Sport
Chess Fitness Soccer
Fi
sh
er
_g
en
Ru
ss
io
n_
bo
ok
s
66_games Iceland Spassky
G
ym
_c
om
pl
ex
Lo
os
in
g_
we
ig
ht
Po
ol
s
Tr
ad
e_
m
ea
ls
Figure 1: Example Folksonomy Ontology
Input: Set of tags T. Set of Committees C.
Output: An ontology of folksonomy tags.
L is a list containing pairs of tag/clusters with
associated similarity, initially empty.
foreach ti ? T do
Compute the similarity to all other tags tj
(i 6= j), and add a pair ?ti, tj? in L.
end
while L is not empty do
1. Sort L by the similarity of the pairs.
2. Pop the pair with the highest similarity
from L. Let it ?ti, ??. ? can be a single
tag or a cluster of tags.
3. Make ti the parent of ?.
4. Join ti with ?, and create a new cluster
?.
if ti belongs to more than one committee
in C then
1. Find the committee c which is the
most similar to ti.
2. Remove all features intersecting
with c from ti.
end
else
1. Substitute all instances of ti in the
pairs in L by ?.
end
end
Algorithm 2: Ontology Construction Algorithm
5 Experimental Evaluations
We applied our proposed algorithm to data from
a real-world social tagging system Delicious and
derived a tag ontology. Then we evaluated the de-
rived ontology on two aspects: the density of the
ontology, and the usefulness of the ontology in a
personalized Information Retrieval (IR) task. Note
that in the experiments, we determined the values
for all tuning coefficients in the algorithms during
the preliminary test runs.
5.1 Datasets
We first crawled the Delicious site and ob-
tained data consisting of 29,918 users, 6,403,442
resources and 1,035,177 tags. In this data,
47,184,492 annotations were made by just one
user, or for one resource, or by one tag. This dis-
tribution followed the Zipf?s law ? small numbers
of tags were in frequent use and large numbers of
tags were rarely used. Our intuitions were that the
effect of using the semantic/encyclopedia knowl-
edge from Wikipedia would probably be better re-
flected in the low frequency ?long tail? part of
the Zipf?s distribution rather than the high fre-
quency part. Likewise for users, we have dis-
covered in our previous research that search per-
sonalization algorithms often produce different re-
sults for users with rich profiles and for users who
have sparse profiles. This problem is known as the
?Cold Start? problem in search personalization: a
new user has very little information/history in the
profile, therefore the system cannot reliably infer
his/her interests. Since our experiments included
a personalized IR task, we decided to extract two
subsets from the data: one set containing high fre-
quency tags assigned by users with rich profiles
(randomly selected 1,000 most frequent tags en-
tered by 100 high profile users), and another con-
taining low frequency tags assigned by users with
sparse profiles (randomly selected 1,000 least fre-
quent tags entered by 100 sparse profile users). We
refer to the former set as the ?Frequent Set? and
the latter set as the ?Long Tail Set?. The total
number of resources in each dataset was 16,635
and 3,356 respectively.
Then for both datasets, we applied a part-
of speech tagger to all resources and extracted
all nouns (and discarded all other parts of
speech). We also applied the Porter Stemmer
(tartarus.org/?martin/PorterStemmer) to eliminate terms
with inflectional variations. Finally, we repre-
46
sented each resource page as a vector of stemmed
terms, and the values were term frequencies.
As for Wikipedia, we used its English
version available from BitTorrent Network
(www.bittorrent.com). The original data (the most
recent dump, as of 24 July, 2008) contained
13,916,311 pages. In order to reduce the size
to make the computation feasible, we randomly
chose 75,000 pages (which contained at least 50
words) and applied the Maximal Complete Link
clustering algorithm to further reduce the size.
After clustering, we obtained a total of 43,876
clusters, most of which contained one or twoWiki
articles, but some of which had several articles.
We call such a Wiki article cluster Wiki concept.
As with the tag datasets, for each Wiki article
we applied the Porter Stemmer to reduce the num-
ber of the terms. Then we represented each Wiki
concept page as a vector of stemmed terms, and
the values were term frequencies.
5.2 Evaluation 1: Ontological Density
For the first evaluation, we evaluated the derived
Delicious tag ontology directly by measuring the
topological closeness of similar semantic concepts
in the ontology. To that end, we developed a no-
tion of ontological density: all tags assigned to a
specific resource should be located close to each
other in the ontology. For instance, a web resource
java.sun.com in Delicious is assigned with various
tags such as ?Java?, ?Programming? and ?Technol-
ogy?. Those tags should be concentrated in one
place rather than scattered over various sections in
the ontology. By measuring the distance as the
number of edges in the ontology between tags as-
signed to a specific resource, we can obtain an es-
timate of the ontology density for the resource.
Then finding the average density of all resources
can give us an approximation of the overall den-
sity of the ontology?s quality.
But here a difficulty arises for ambiguous tags
? when a tag is ambiguous and located in several
places in the ontology. In those cases, we chose
the sense (an ontology node) which is the clos-
est to the unambiguous tags assigned to the same
resource. For example, Figure 2 shows a part of
the ontology where an ambiguous tag ?NLP? (with
two senses) is mapped: 1) Natural Language Pro-
cessing (the left one in the figure), and 2) Neuro-
linguistic programming (the right one in the fig-
ure). The target web resource is tagged with three
tags: two unambiguous tags ?POS? and ?Porter?,
and an ambiguous tag ?NLP?. To identify the sense
of ?NLP? for this resource, we count the number
of edges from the two unambiguous tags (?POS?,
?Porter?) to both ?NLP? tag nodes, and select the
one which has the shortest distance. In the figure,
the first sense has the total distance of 4 (= 2 edges
from ?Pos? + 2 edges from ?Porter?), while the sec-
ond sense has the distance 10 (= 5 edges from
?Pos? + 5 edges from ?Porter?). Therefore, we
select the first sense (?Natural Language Process-
ing?) as the meaning of ?NLP? for this resource.
Communic
Research
Psychology
Mind
Linguistics
Language
Dictionary
NLPTwitter
NLP POS Porter
POS Porter NLPWeb-resource
Web2.0 Media
Figure 2: Example of Ambiguous Tags in the On-
tology
Formally we define the density of the ontology
T for the set of resourcesR, denotedDens(T, R),
as the average density over all resources in R, as
follows.
Dens(T, R) = 1|R|
?
r?R
density(r, T )
where density(r, T ) denotes the density for the
given resource r for the ontology T , defined as:
density(r, T ) = nTags(r) ? 1argmini,j dist(node(i, T ), node(j,T ))
and nTags(r) is the number of tags assigned to
r, node(k, T ) is the node in T for the kth tag (as-
signed to r), and dist(n1, n2) is the number of
edges between nodes n1 and n2 in T . So the
density for the given resource is essentially the
inverse of the minimum distance among the tags
assigned to it. We computed the density value
for the ontology derived by our approach (?On-
tology Enhanced with Wiki Concepts?) and com-
pared with the ontologies obtained by using only
the resources (where a tag vector is presented by
47
the stemmed terms in the resources to which the
tag is assigned), and only the tags (where a tag
vector is presented by the resource to which they
were assigned). Figures 3 and 4 show the results,
for the two datasets. For both datasets, the dif-
ferences between the three ontologies were statis-
tically significant (at p=0.05), indicating that the
encyclopedia knowledge obtained from Wikipedia
was indeed effective in deriving a semantically
dense ontology.
Here, one observation is that the relative im-
provement was more significant for the ?Frequent
Set? than the ?Long Tail Set?. The reason is be-
cause frequent tags are generally more ambigu-
ous than less frequent tags (as with words in gen-
eral), therefore the effect of tag disambiguation by
DSCBC was more salient, relatively, for the fre-
quent tags.
0,035
0,037
0,039
0,041
0,043
0,045
De
ns
ity
Ontology Enhanced
withWiki Concepts
Ontology Enhanced
by Resources
Ontology Based
on Tags
Figure 3: Ontological Density for ?Frequent Set?
Ontology Enhanced
with Wiki Concepts
Ontology Enhanced
by Resources
Ontology Based
on Tags
0,1
0,05
0,15
0,2
0,25
0,3
De
ns
ity
Figure 4: Ontological Density for ?Long Tail Set?
5.3 Evaluation 2: Personalized Information
Retrieval
For the second evaluation, we used the derived De-
licious ontology in an IR task and measured its
utility. In particular, we personalized the search
results for a given user by utilizing the tag ontol-
ogy as a way to present the user profile and infer
his/her information needs.
Using the derived ontology, we search in the on-
tology for the query tag entered by a specific user.
We first match the ontology with the user?s profile
and derive a score distribution for the nodes in the
tree which reflects the user?s general interest. To
do so, we take each tag in the user?s profile as the
initial activation point, then spread the activation
up and down the ontology tree, for all tags.
To spread activation from a given node, we
use two parameters: decay factor, which deter-
mines the amount of the interest to be transfered
to the parent/child of the current node; and damp-
ing threshold - if the interest score becomes less
than this value we stop further iteration. Thus the
resulting score distributionof the tree is effectively
personalized to the user?s general interest.
Using the obtained score distribution of a given
user, we search the tree for a query tag (of this
user). In the same way as the tags in the profile, we
spread activation over the ontology from the node
to which the tag belongs, but this time we add a
weight to emphasize the relative importance of the
query tag compared to the tags from the profile,
because the query reflects the user?s current infor-
mation needs. Finally we feed the preference vec-
tor to the modified FolkRank algorithm (Hotho et
al., 2006) to retrieve and rank the relevant web re-
sources which reflect the user-specific preferences.
Figure 5 shows the overall scheme of the person-
alized ranked retrieval using an ontological user
profile.
Sport
Chess Fitness Soccer
Fi
sh
er
_g
en
Ru
ss
io
n_
bo
ok
s
66_games Iceland Spassky
G
ym
_c
om
pl
ex
Lo
os
in
g_
we
ig
ht
Po
ol
s
Tr
ad
e_
m
ea
ls
Sport
Chess Fitness Soccer
Fi
sh
er
_g
en
Ru
ss
io
n_
bo
ok
s
66_games Iceland Spassky
G
ym
_c
om
pl
ex
Lo
os
in
g_
we
ig
ht
Po
ol
s
Tr
ad
e_
m
ea
ls
TnSpasskiy Loosing_weight
Users
Tags
Preference vector
Ranked Resourses
Resourses
...
User Profiles
Spreading
Activation
Figure 5: Ranked Retrieval in Folksonomies using
Ontological User Profile
We evaluated the retrieval results by 5-fold
cross validation. Given a test user profile, we used
48
the leave-one-out method for tags ? we removed a
target tag from the user profile and treated it as a
query. All resources which the user assigned with
that tag was the relevant set. For the final results,
we computed the F-score, which is defined as stan-
dard:
F = 2 ? Precision ? RecallPrecision + Recall
Figure 6 and 7 show the F-scores for the two
datasets. Note that ?TopN? indicates the top N
retrieved resources. As you can see, the ontol-
ogy enhanced with the Wiki concepts was able to
better reflect the users? interest and produced sig-
nificant improvements compared to the ontologies
built only with the Delicious resources. Moreover,
the improvements were much more significant for
the ?Long Tail Set? than the ?Frequent Set?, as
consistent with our intuitions ? Wikipedia?s en-
cyclopedia knowledge helped enhance the infor-
mation about the less-frequent tags (assigned by
the users with sparse profiles), thereby overcom-
ing the ?Cold Start? problem in search personal-
ization.
0
0 10 15
0,05
0,03
0,08
0,1
0,13
0,15
TopN
F-
va
lu
e
20 25 30 35 40 45 50 60 65 70 75 80 85 90 95 10055
OntologyEnhanced
by Resources
Ontology Based
on Tags
Ontology Enhanced
with Wiki Concepts
Figure 6: F-score of the Ontology for ?Frequent
Set?
TopN
Ontology Enhanced
by Resources
Ontology Based
on Tags
Ontology Enhanced
with Wiki Concepts
0
0 10 15
0,1
0,05
0,15
0,2
0,25
0,3
0,35
0,4
0,45
0,5
20 25 30 35 40 45 50 60 65 70 75 80 85 90 95 10055
F-
va
lu
e
Figure 7: F-score of the Ontology for ?Long Tail
Set?
6 Conclusions and Future Work
In this paper, we presented a novel method for dis-
ambiguating tags and incorporating encyclopedia
knowledge from Wikipedia in building folkson-
omy ontologies for social tagging systems. We
applied our method to the data from Delicious and
showed that, not only was the derived ontology se-
mantically more dense (i.e., similar tags/concepts
are clustered in close proximity), it also proved to
be very effective in a search personalization task
as well.
For future work, we are planning on investigat-
ing different ways of incorporating the link struc-
tures of Wikipedia and web pages in the tag sim-
ilarity function (in DSCBC). Possible ideas in-
clude adding different weights on various types of
links (or links appearing in various sections of a
page/article), and using distance in the reachabil-
ity relation, for example using the work done in
Wikipedia Mining (Nakayama et al, 2008).
Finally, we are planning on applying informa-
tion extraction or summarization techniques on
Wikipedia articles to focus on sentences which
provide relevant and important information about
the subject.
References
D. Ahn, V. Jijkoun, G. Mishene, K. Muller, M. DeR-
ijke, and S. Schlobach. 2004. Using Wikipedia at
the TREC QA Track. In Proceedings of the 13th
Text Retrieval Conference (TREC 2004).
P. Clough, H. Joho, and M. Sanderson. 2005. Auto-
matically Organizing Images Using Concept Hier-
archies. In Proceedings of the SIGIR Workshop on
Multimedia Information Retrieval.
A. Culotta, A. Mccallum, and J.Betz. 2006. Integrat-
ing Probabilistic Extraction Models and Data Min-
ing to Discover Relations and Patterns in Text. In
Proceedings of the Human Language Technology
Conference.
E. Gabrilovich and S. Markovitch. 2006. Over-
coming the Brittleness Bottleneck UsingWikipedia:
Enhancing Text Categorization with Encyclopedic
Knowledge. In Proceedings of the National Con-
ference on Artificial Intelligence.
I. Gurevych, C. Muler, and T. Zesch. 2007. What to
be? - Electronic Career Guidance Based on Seman-
tic Relatedness. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics.
P. Heymann and H. Garcia-Molina. 2006. Collab-
orative Creation of Communal Hierarchical Tax-
onomies in Social Tagging Systems. Technical Re-
port 2006-10, Computer Science Department, April.
49
A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme.
2006. Folkrank: A Ranking Algorithm for Folk-
sonomies. In Proceedings of the FGIR.
R. Mihalcea and A. Csomai. 2007. Wikify!: Linking
Documents to Encyclopedic Knowledge. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management.
P. Mika. 2007. Ontologies Are Us: A Unified Model
of Social Networks and Semantics. Web Semantics:
Science, Services and Agents on the World Wide
Web, 5(1).
G. Miller. 1990. WordNet: An Online Lexical
Database. International Journal of Lexicography,
3(4).
D. Milne, O. Medelyan, and I. Witten. 2006. Mining
Domain-Specific Thesauri from Wikipedia: A Case
Study. In Proceedings of the 2006 IEEE/WIC/ACM
International Conference on Web Intelligence.
K. Nakayama, T. Hara, and S. Nishio. 2008.
Wikipedia Mining - Wikipedia as a Corpus for
Knowledge Extraction. In Proceedings of Annual
Wikipedia Conference (Wikimania).
M. OConnor and J. Herlocker. 2001. Clustering
Items for Collaborative Filtering. In Proceedings of
SIGIR-2001 Workshop on Recommender Systems.
P. Pantel and D. Lin. 2002. Discovering Word Senses
from Text. In Proceedings of the 8th ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD-02).
P. Schmitz. 2006. Inducing Ontology From Flickr
Tags. In Proceedings of the CollaborativeWeb Tag-
ging Workshop (WWW 06).
A. Shepitsen, J. Gemmell, B. Mobasher, and R. Burke.
2008. Personalized Recommendation in Social Tag-
ging Systems UsingHierarchical Clustering. InPro-
ceedings of the 2008 ACM conference on Recom-
mender Systems.
N. Tomuro, S. Lytinen, K. Kanzaki, and H. Isahara.
2007. Clustering Using Feature Domain Similarity
to Discover Word Senses for Adjectives. In Pro-
ceedings of the 1st IEEE International Conference
on Semantic Computing (ICSC-2007).
50
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1924?1929,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining Visual and Textual Features for Information Extraction from
Online Flyers
Emilia Apostolova
BrokerSavant Inc
2506 N. Clark St.
Chicago, IL 60614
emilia@brokersavant.com
Noriko Tomuro
DePaul University
243 S. Wabash Ave.
Chicago, IL 60604
tomuro@cs.depaul.edu
Abstract
Information in visually rich formats such
as PDF and HTML is often conveyed
by a combination of textual and visual
features. In particular, genres such as
marketing flyers and info-graphics often
augment textual information by its color,
size, positioning, etc. As a result, tradi-
tional text-based approaches to informa-
tion extraction (IE) could underperform.
In this study, we present a supervised ma-
chine learning approach to IE from on-
line commercial real estate flyers. We
evaluated the performance of SVM clas-
sifiers on the task of identifying 12 types
of named entities using a combination of
textual and visual features. Results show
that the addition of visual features such
as color, size, and positioning significantly
increased classifier performance.
1 Introduction
Since the Message Understanding Conferences in
the 1990s (Grishman and Sundheim, 1996; Chin-
chor and Robinson, 1997), Information Extraction
(IE) and Named Entity Recognition (NER) ap-
proaches have been applied and evaluated on a va-
riety of domains and textual genres. The majority
of the work, however, focuses on the journalistic,
scientific, and informal genres (newswires, scien-
tific publications, blogs, tweets, and other social
media texts) (Nadeau and Sekine, 2007) and deals
with purely textual corpora. As a result, the fea-
ture space of NER systems involves purely tex-
tual features, typically word attributes and char-
acteristics (orthography, morphology, dictionary
lookup, etc.), their contexts and document features
(surrounding word window, local syntax, docu-
ment/corpus word frequencies, etc.) (Nadeau and
Sekine, 2007).
At the same time, textual information is often
presented in visually rich formats, e.g. HTML and
PDF. In addition to text, these formats use a vari-
ety of visually salient characteristics, (e.g. color,
font size, positioning) to either highlight or aug-
ment textual information. In some genres and do-
mains, a textual representation of the data, exclud-
ing visual features is often not enough to accu-
rately identify named entities of interest or extract
relevant information. Marketing materials, such
as online flyers or HTML emails, often contain
a plethora of visual features and text-based NER
approaches lead to poor results. In this paper, we
present a supervised approach that uses a combi-
nation of textual and visual features to recognize
named entities in online marketing materials.
2 Motivation and Problem Definition
A number of broker-based industries (e.g. com-
mercial real estate, heavy equipment machinery,
etc.) lack a centralized searchable database with
industry offerings. In particular, the commercial
real estate industry (unlike residential real estate)
does not have a centralized database or an estab-
lished source of information. Commercial real
estate brokers often need to rely on networking,
chance, and waste time with a variety of commer-
cial real estate databases that often present out-
dated information. While brokers do not often up-
date third party inventory databases, they do create
marketing materials (usually PDF flyers) that con-
tain all relevant listing information. Virtually all
commercial real estate offerings come with pub-
licly available marketing material that contains all
relevant listing information. Our goal is to harness
this source of information (the marketing flyer)
and use it to extract structured listing information.
Figure 1 shows an example of a commercial
real estate flyer. The commercial real estate fly-
ers are often distributed as PDF documents, links
to HTML pages, or visually rich HTML-based
1924
Figure 1: An example of a commercial real estate
flyer
c
? Kudan Group Real Estate.
emails. They typically contain all relevant listing
information such as the address and neighborhood
of the offering, the names and contact information
of the brokers, the type of space offered (build-
ing, land, unit(s) within a building), etc. Similar to
other info-graphics, relevant information could be
easily pinpointed by visual clues. For example, the
listing street address in Figure 1 (1629 N. Halsted
St., upper left corner) can be quickly identified and
distinguished from the brokerage firm street ad-
dress (156 N. Jefferson St., upper right corner) due
to its visual prominence (font color, size, position-
ing).
In this study we explored a supervised machine
learning approach to the task of identifying list-
ing information from commercial real estate fly-
ers. In particular, we focused on the recognition
of 12 types of named entities as described in Table
1 below.
3 Related Work
Nadeau and Satoshi (2007) present a survey of
NER and describe the feature space of NER re-
search. While they mention multi-media NER in
the context of video/text processing, all described
features/approaches focus only on textual repre-
sentation.
Broker Name The contact information of all
Broker Email listing brokers, including full name,
Broker Phone email address, phone number.
Company Phone The brokerage company phone
number.
Street The address information of the
City listing address including street or
Neighborhood intersection, city, neighborhood,
State state, and zip code.
Zip
Space Size Size and attributes of relevant spaces
Space Type (e.g. 27,042 SF building, 4.44 acres
site, etc.); Mentions of space type
descriptors, e.g. building, land/lot,
floor, unit. This excludes space type
and size information of non-essential
listing attributes (e.g. basement size
or parking lot size).
Confidential Any mentions of confidentiality.
Table 1: Types and descriptions of named enti-
ties relevant to extracting listing information from
commercial real estate flyers.
The literature on Information Extraction from
HTML resources is dominated by various ap-
proaches based on wrapper induction (Kushmer-
ick, 1997; Kushmerick, 2000). Wrapper induc-
tions rely on common HTML structure (based on
the HTML DOM) and formatting features to ex-
tract structured information from similarly format-
ted HTML pages. This approach, however, is not
applicable to the genres of marketing materials
(PDF and HTML) since they typically do not share
any common structure that can be used to iden-
tify relevant named entities. Laender et al. (2002)
present a survey of data extraction techniques and
tools from structured or semi-structured web re-
sources.
Cai et al. (2003) present a vision-based segmen-
tation algorithm of web pages that uses HTML
layout features and attempts to partition the page
at the semantic level. In (Burget and Rudolfova,
2009) authors propose web-page block classifica-
tion based on visual features. Yang and Zhang
(2001) build a content tree of HTML documents
based on visual consistency inferred semantics.
Burget (2007) proposes a layout based informa-
tion extraction from HTML documents and states
that this visual approach is more robust than tradi-
tional DOM-based methods.
Changuel et al.(2009a) describe a system for
automatically extracting author information from
web-pages. They use spatial information based on
the depth of the text node in the HTML DOM tree.
In (Changuel et al., 2009b) and (Hu et al., 2006),
1925
the authors proposed a machine learning method
for title extraction and utilize format information
such as font size, position, and font weight. In
(Zhu et al., 2007) authors use layout information
based on font size and weight for NER for auto-
mated expense reimbursement.
While the idea of utilizing visual features based
on HTML style has been previously suggested,
this study tackles a non-trivial visually rich dataset
that prevents the use of previously suggested sim-
plistic approaches to computing HTML features
(such as relying on the HTML DOM tree or sim-
plistic HTML style rendering). In addition, we in-
troduce the use of RGB color as a feature and nor-
malize it approximating human perception.
4 Dataset and Method
The dataset consists of 800 randomly selected
commercial real estate flyers spanning 315 US
locations, 75 companies, and 730 brokers. The
flyers were collected from various online sources
and were originally generated using a variety of
HTML and PDF creator tools. The collection rep-
resents numerous flyer formats and layouts, com-
mercial real estate property types (industrial, re-
tail, office, land, etc.), and transactions (invest-
ment, sale, lease).
All flyers were converted to a common format
(HTML)
1
. The HTML versions of all documents
were then annotated by 2 annotators. Figure 2
shows an example of an annotated flyer. Annota-
tion guidelines were developed and the 2 annota-
tors were able to achieve an inter-annotator agree-
ment of 91%
2
. The named entities with lowest
inter-annotator agreement were entities describ-
ing Space Size and Type because of the some-
what complex rules for determining essential list-
ing space information. For example, one of the
space size/type rules reads as follows: If the list-
ing refers to a building and mentions the lot size, include
both the land size, the building size, and corresponding space
types. Do not include individual parts of the building (e.g.
office/basement) as separate spaces. If the listing refers to a
UNIT within the building, not the whole building, then DO
NOT include the land site as a separate space.
A supervised machine learning approach was
1
PDFs were converted to HTML using the PDFTO-
HTML conversion program http://pdftohtml.
sourceforge.net/.
2
The inter-annotator agreement was measured as F1-score
using one of the annotator?s named entities as the gold stan-
dard set and the other as a comparison set.
Figure 2: The HTML versions of the flyers were
annotated by 2 annotators using a custom web-
based annotation tool.
then applied to the task of identifying the 12
named entities shown in Table 1. Flyers were con-
verted to text using an HTML parser while pre-
serving some of the white space formatting. The
text was tokenized and the task was then modeled
as a BIO classification task, classifiers identify the
Beginning, the Inside, and Outside of the text seg-
ments. We first used a traditional set of text-based
features for the classification task. Table 2 lists
the various text-based features used. In all cases,
a sliding window including the 5 preceding and 5
following tokens was used as features.
Feature Name Description
Token A normalized string representation of
the token. All tokens were converted
to lower case and all digits were
converted to a common format.
Token Orth The token orthography. Possible values
are lowercase (all token characters are
lower case), all capitals (all token
characters are upper case), upper initial
(the first token character is upper case,
the rest are lower case), mixed (any
mixture of upper and lower case letters
not included in the previous categories).
Token Kind Possible values are word, number,
symbol, punctuation.
Regex type Regex-based rules were used to mark
chunks as one of 3 regex types:
email, phone number, zip code.
Gazetteer Text chunks were marked as possible
US cities or states based on US Census
Bureau city and state data.
www.census.gov/geo/maps-data/data/gazetteer2013.html.
Table 2: List of text-based features used for the
NER task. A sliding window of the 5 preceding
and 5 following tokens was used for all features.
1926
As noted previously, human annotators were
able to quickly spot named entities of interest
solely because of their visual characteristics. For
example, a text-only version of the flyer shown in
Figure 1, stripped of all rich formatting, will make
it quite difficult to distinguish the listing address
(shown in prominent size, position, and color)
from the brokerage company address, which is
rarely prominent as it is not considered important
information in the context of the flyer. Similarly,
the essential size information for the listing shown
on Figure 2 appears prominently on the first page
(square footage of the offered restaurant), while
non-essential size information, such as the size of
the adjacent parking lot or basement, tend to ap-
pear in smaller font on subsequent flyer pages.
To account for such visual characteristics we at-
tempted to also include visual features associated
with text chunks. We used the computed HTML
style attributes for each DOM element containing
text. Table 3 lists the computed visual features.
Feature Name Description
Font Size The computed font-size attribute of
the surrounding HTML DOM element,
normalized to 7 basic sizes (xx-small,
x-small, small, medium, large, x-large,
xx-large).
Color The computed color attribute of the
surrounding HTML DOM element.
The RGB values were normalized
to a set of 100 basic colors. We
converted the RGB values to the
YUV color space, and then used
Euclidian distance to find the
most similar basic color
approximating human perception.
Y Coordinate The computed top attribute of the
surrounding HTML DOM element, i.e.
the y-coordinate in pixels. The pixel
locations was normalized to 150 pixel
increments (roughly 1/5th of the
visible screen for the most common
screen resolution.)
Table 3: List of visual features used for the NER
task. A sliding window of 5 preceding and 5 fol-
lowing DOM elements were used for all features.
Computing the HTML style attributes is a com-
plex task since they are typically defined by a
combination of CSS files, in-lined HTML style
attributes, and browser defaults. The complex-
ities of style definition, inheritance, and over-
writing are handled by browsers
3
. We used the
3
We attempted to use an HTML renderer from the Cobra
java toolkit http://lobobrowser.org/cobra.jsp
to compute HTML style attributes. However, this renderer
Chrome browser to compute dynamically the style
of each DOM element and output it as inline
style attributes. To achieve this we program-
matically inserted a javascript snippet that inlines
the computed style and saves the new version of
the HTML on the local file system utilizing the
HTML5 saveAs interface
4
. Details on how we
normalized the style attribute values for font size,
RGB color, and Y coordinate are shown in Table
3.
We then applied Support Vector Machines
(SVM) (Vapnik, 2000) on the NER task using the
LibSVM library (Chang and Lin, 2011). We chose
SVMs as they have been shown to perform well
on a variety of NER tasks, for example (Isozaki
and Kazawa, 2002; Takeuchi and Collier, 2002;
Mayfield et al., 2003; Ekbal and Bandyopadhyay,
2008). We used a linear kernel model with the
default parameters. The multi-class problem was
converted to binary problems using the one-vs-
others scheme. 80% of the documents were used
for training, and the remaining 20% for testing.
5 Results
Results are shown in Table 4. We compared clas-
sifier performance using only textual features (first
3 columns), versus performance using both textual
and visual features (next 3 columns). Results were
averaged over 2 runs of randomly selected train-
ing/test documents with 80%/20% ratio. We used
an exact measure which considers an answer to be
correct only if both the entity boundaries and en-
tity type are accurately predicted.
The addition of visual features significantly
5
increased the overall F1-score from 83 to 87%.
As expected, performance gains are more signif-
icant for named entities that are typically visu-
ally salient and are otherwise difficult (or impossi-
ble) to identify in a text-only version of the fly-
ers. Named Entities referring to listing address
information showed the most significant improve-
ments. In particular, the F1-score for mentions of
Neighborhoods (typically prominently shown on
the first page of the flyers) improved by 19%; F1-
score for mentions of the listing State improved by
9%; and Street, City, Zip by roughly 4% each, all
produced poor results on our dataset and failed to accurately
compute the pixel location of text elements.
4
https://github.com/eligrey/FileSaver.
js
5
The difference is statistically significant with p value <
0.0001% using Z-test on two proportions.
1927
Named Entity Pt Rt Ft Pv+t Rv+t Fv+t S
Broker Name 82.7 91.7 87.0 95.0 91.6 93.2 Y
Broker Email 92.3 92.8 92.6 97.2 90.2 93.6 N
Broker Phone 90.2 86.1 88.1 94.7 85.2 89.7 N
Company Ph. 95.2 67.4 78.9 89.8 65.4 75.7 N
Street 87.4 70.5 78.1 87.3 77.3 82.0 Y
City 92.5 88.5 90.5 94.9 92.8 93.8 Y
Neighborhood 68.2 52.8 59.5 85.3 72.9 78.6 Y
State 77.4 97.5 86.3 95.8 95.0 95.4 Y
Zip 89.7 94.5 92.1 96.1 97.1 96.6 Y
Space Size 80.2 65.0 71.8 87.0 70.6 77.9 Y
Space Type 76.0 74.7 75.3 78.6 72.2 75.3 N
Confidential 100 60.0 75.0 75.0 85.7 79.9 N
OVERALL 84.8 81.3 83.0 91.2 83.2 87.0 Y
Table 4: Results from applying SVM using the
textual features described in Table 2, as well as
both the textual and visual features described in
Tables 2 and 3. t=textual features only, v+t=visual
+ textual features, P=Precision, R=Recall, F=F1-
score, S=Significant Difference
statistically significant. Visual clues are also typi-
cally used when identifying relevant size informa-
tion and, as expected, performance improved sig-
nificantly by roughly 6%. The difference in per-
formance for mentions used to describe confiden-
tial information is not statistically significant
6
be-
cause such mentions rarely occurred in the dataset.
Similarly, performance differences for Company
Phone, Broker Phone, Broker Email, and Space
Type are not statistically significant. In all of
these cases, visual features did not influence per-
formance and text-based features proved adequate
predictors.
6 Conclusion
We have shown that information extraction in cer-
tain genres and domains spans different media -
textual and visual. Ubiquitous online and dig-
ital formats such as PDF and HTML often ex-
ploit the interaction of textual and visual elements.
Information is often augmented or conveyed by
non-textual features such as positioning, font size,
color, and images. However, traditionally, NER
approaches rely exclusively on textual features
and as a result could perform poorly in visually
rich genres such as online marketing flyers or info-
graphics. We have evaluated the performance gain
on the task of NER from commercial real estate
flyers by adding visual features to a set of tradi-
tional text-based features. We used SVM classi-
fiers for the task of identifying 12 types of named
entities. Results show that overall visual features
improved performance significantly.
6
p value = 0.7323% using Z-test on two proportions.
References
Radek Burget and Ivana Rudolfova. 2009. Web
page element classification based on visual features.
In Intelligent Information and Database Systems,
2009. ACIIDS 2009. First Asian Conference on,
pages 67?72. IEEE.
Radek Burget. 2007. Layout based information extrac-
tion from html documents. In Document Analysis
and Recognition, 2007. ICDAR 2007. Ninth Inter-
national Conference on, volume 2, pages 624?628.
IEEE.
Deng Cai, Shipeng Yu, Ji-Rong Wen, and Wei-Ying
Ma. 2003. Extracting content structure for web
pages based on visual representation. In Web Tech-
nologies and Applications, pages 406?417. Springer.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/
?
cjlin/libsvm.
Sahar Changuel, Nicolas Labroche, and Bernadette
Bouchon-Meunier. 2009a. Automatic web pages
author extraction. In Flexible Query Answering Sys-
tems, pages 300?311. Springer.
Sahar Changuel, Nicolas Labroche, and Bernadette
Bouchon-Meunier. 2009b. A general learning
method for automatic title extraction from html
pages. In Machine Learning and Data Mining in
Pattern Recognition, pages 704?718. Springer.
Nancy Chinchor and Patricia Robinson. 1997. Muc-7
named entity task definition. In Proceedings of the
7th Conference on Message Understanding.
Asif Ekbal and Sivaji Bandyopadhyay. 2008. Named
entity recognition using support vector machine: A
language independent approach. International Jour-
nal of Computer Systems Science & Engineering,
4(2).
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage understanding conference-6: A brief history. In
COLING, volume 96, pages 466?471.
Yunhua Hu, Hang Li, Yunbo Cao, Li Teng, Dmitriy
Meyerzon, and Qinghua Zheng. 2006. Automatic
extraction of titles from general documents using
machine learning. Information processing & man-
agement, 42(5):1276?1293.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. In Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7. Association for Computational Linguis-
tics.
Nicholas Kushmerick. 1997. Wrapper induction for
information extraction. Ph.D. thesis, University of
Washington.
1928
Nicholas Kushmerick. 2000. Wrapper induction: Ef-
ficiency and expressiveness. Artificial Intelligence,
118(1):15?68.
Alberto HF Laender, Berthier A Ribeiro-Neto, Alti-
gran S da Silva, and Juliana S Teixeira. 2002. A
brief survey of web data extraction tools. ACM Sig-
mod Record, 31(2):84?93.
James Mayfield, Paul McNamee, and Christine Piatko.
2003. Named entity recognition using hundreds of
thousands of features. In Proceedings of the seventh
conference on Natural language learning at HLT-
NAACL 2003-Volume 4, pages 184?187. Association
for Computational Linguistics.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Koichi Takeuchi and Nigel Collier. 2002. Use of
support vector machines in extended named entity
recognition. In proceedings of the 6th conference
on Natural language learning-Volume 20, pages 1?
7. Association for Computational Linguistics.
Vladimir Vapnik. 2000. The nature of statistical learn-
ing theory. springer.
Yudong Yang and HongJiang Zhang. 2001. Html page
analysis based on visual cues. In Document Analysis
and Recognition, 2001. Proceedings. Sixth Interna-
tional Conference on, pages 859?864. IEEE.
Guangyu Zhu, Timothy J Bethea, and Vikas Krishna.
2007. Extracting relevant named entities for auto-
mated expense reimbursement. In Proceedings of
the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1004?
1012. ACM.
1929
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 283?287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation
and Speculation Scopes
Emilia Apostolova
DePaul University
Chicago, IL USA
emilia.aposto@gmail.com
Noriko Tomuro
DePaul University
Chicago, IL USA
tomuro@cs.depaul.edu
Dina Demner-Fushman
National Library of Medicine
Bethesda, MD USA
ddemner@mail.nih.gov
Abstract
Detecting the linguistic scope of negated and
speculated information in text is an impor-
tant Information Extraction task. This paper
presents ScopeFinder, a linguistically moti-
vated rule-based system for the detection of
negation and speculation scopes. The system
rule set consists of lexico-syntactic patterns
automatically extracted from a corpus anno-
tated with negation/speculation cues and their
scopes (the BioScope corpus). The system
performs on par with state-of-the-art machine
learning systems. Additionally, the intuitive
and linguistically motivated rules will allow
for manual adaptation of the rule set to new
domains and corpora.
1 Motivation
Information Extraction (IE) systems often face
the problem of distinguishing between affirmed,
negated, and speculative information in text. For
example, sentiment analysis systems need to detect
negation for accurate polarity classification. Simi-
larly, medical IE systems need to differentiate be-
tween affirmed, negated, and speculated (possible)
medical conditions.
The importance of the task of negation and spec-
ulation (a.k.a. hedge) detection is attested by a num-
ber of research initiatives. The creation of the Bio-
Scope corpus (Vincze et al, 2008) assisted in the de-
velopment and evaluation of several negation/hedge
scope detection systems. The corpus consists of
medical and biological texts annotated for negation,
speculation, and their linguistic scope. The 2010
i2b2 NLP Shared Task1 included a track for detec-
tion of the assertion status of medical problems (e.g.
affirmed, negated, hypothesized, etc.). The CoNLL-
2010 Shared Task (Farkas et al, 2010) focused on
detecting hedges and their scopes in Wikipedia arti-
cles and biomedical texts.
In this paper, we present a linguistically moti-
vated rule-based system for the detection of nega-
tion and speculation scopes that performs on par
with state-of-the-art machine learning systems. The
rules used by the ScopeFinder system are automat-
ically extracted from the BioScope corpus and en-
code lexico-syntactic patterns in a user-friendly for-
mat. While the system was developed and tested us-
ing a biomedical corpus, the rule extraction mech-
anism is not domain-specific. In addition, the lin-
guistically motivated rule encoding allows for man-
ual adaptation to new domains and corpora.
2 Task Definition
Negation/Speculation detection is typically broken
down into two sub-tasks - discovering a nega-
tion/speculation cue and establishing its scope. The
following example from the BioScope corpus shows
the annotated hedging cue (in bold) together with its
associated scope (surrounded by curly brackets):
Finally, we explored the {possible role of 5-
hydroxyeicosatetraenoic acid as a regulator of arachi-
donic acid liberation}.
Typically, systems first identify nega-
tion/speculation cues and subsequently try to
identify their associated cue scope. However,
the two tasks are interrelated and both require
1https://www.i2b2.org/NLP/Relations/
283
syntactic understanding. Consider the following
two sentences from the BioScope corpus:
1) By contrast, {D-mib appears to be uniformly ex-
pressed in imaginal discs }.
2) Differentiation assays using water soluble phor-
bol esters reveal that differentiation becomes irreversible
soon after AP-1 appears.
Both sentences contain the word form appears,
however in the first sentence the word marks a hedg-
ing cue, while in the second sentence the word does
not suggest speculation.
Unlike previous work, we do not attempt to iden-
tify negation/speculation cues independently of their
scopes. Instead, we concentrate on scope detection,
simultaneously detecting corresponding cues.
3 Dataset
We used the BioScope corpus (Vincze et al, 2008)
to develop our system and evaluate its performance.
To our knowledge, the BioScope corpus is the
only publicly available dataset annotated with nega-
tion/speculation cues and their scopes. It consists
of biomedical papers, abstracts, and clinical reports
(corpus statistics are shown in Tables 1 and 2).
Corpus Type Sentences Documents Mean Document Size
Clinical 7520 1954 3.85
Full Papers 3352 9 372.44
Paper Abstracts 14565 1273 11.44
Table 1: Statistics of the BioScope corpus. Document sizes
represent number of sentences.
Corpus Type Negation Cues Speculation Cues Negation Speculation
Clinical 872 1137 6.6% 13.4%
Full Papers 378 682 13.76% 22.29%
Paper Abstracts 1757 2694 13.45% 17.69%
Table 2: Statistics of the BioScope corpus. The 2nd and 3d
columns show the total number of cues within the datasets; the
4th and 5th columns show the percentage of negated and spec-
ulative sentences.
70% of the corpus documents (randomly selected)
were used to develop the ScopeFinder system (i.e.
extract lexico-syntactic rules) and the remaining
30% were used to evaluate system performance.
While the corpus focuses on the biomedical domain,
our rule extraction method is not domain specific
and in future work we are planning to apply our
method on different types of corpora.
4 Method
Intuitively, rules for detecting both speculation and
negation scopes could be concisely expressed as a
Figure 1: Parse tree of the sentence ?T cells {lack active NF-
kappa B } but express Sp1 as expected? generated by the Stan-
ford parser. Speculation scope words are shown in ellipsis. The
cue word is shown in grey. The nearest common ancestor of all
cue and scope leaf nodes is shown in a box.
combination of lexical and syntactic patterns. For
example, O?zgu?r and Radev (2009) examined sample
BioScope sentences and developed hedging scope
rules such as:
The scope of a modal verb cue (e.g. may, might, could)
is the verb phrase to which it is attached;
The scope of a verb cue (e.g. appears, seems) followed
by an infinitival clause extends to the whole sentence.
Similar lexico-syntactic rules have been also man-
ually compiled and used in a number of hedge scope
detection systems, e.g. (Kilicoglu and Bergler,
2008), (Rei and Briscoe, 2010), (Velldal et al,
2010), (Kilicoglu and Bergler, 2010), (Zhou et al,
2010).
However, manually creating a comprehensive set
of such lexico-syntactic scope rules is a laborious
and time-consuming process. In addition, such an
approach relies heavily on the availability of accu-
rately parsed sentences, which could be problem-
atic for domains such as biomedical texts (Clegg and
Shepherd, 2007; McClosky and Charniak, 2008).
Instead, we attempted to automatically extract
lexico-syntactic scope rules from the BioScope cor-
pus, relying only on consistent (but not necessarily
accurate) parse tree representations.
We first parsed each sentence in the training
dataset which contained a negation or speculation
cue using the Stanford parser (Klein and Manning,
2003; De Marneffe et al, 2006). Figure 1 shows the
parse tree of a sample sentence containing a nega-
tion cue and its scope.
Next, for each cue-scope instance within the sen-
tence, we identified the nearest common ancestor
284
Figure 2: Lexico-syntactic pattern extracted from the sentence
from Figure 1. The rule is equivalent to the following string
representation: (VP (VBP lack) (NP (JJ *scope*) (NN *scope*)
(NN *scope*))).
which encompassed the cue word(s) and all words in
the scope (shown in a box on Figure 1). The subtree
rooted by this ancestor is the basis for the resulting
lexico-syntactic rule. The leaf nodes of the resulting
subtree were converted to a generalized representa-
tion: scope words were converted to *scope*; non-
cue and non-scope words were converted to *; cue
words were converted to lower case. Figure 2 shows
the resulting rule.
This rule generation approach resulted in a large
number of very specific rule patterns - 1,681 nega-
tion scope rules and 3,043 speculation scope rules
were extracted from the training dataset.
To identify a more general set of rules (and in-
crease recall) we next performed a simple transfor-
mation of the derived rule set. If all children of a
rule tree node are of type *scope* or * (i.e. non-
cue words), the node label is replaced by *scope*
or * respectively, and the node?s children are pruned
from the rule tree; neighboring identical siblings of
type *scope* or * are replaced by a single node of
the corresponding type. Figure 3 shows an example
of this transformation.
(a) The children of nodes JJ/NN/NN are
pruned and their labels are replaced by
*scope*.
(b) The children
of node NP are
pruned and its la-
bel is replaced by
*scope*.
Figure 3: Transformation of the tree shown in Figure 2. The
final rule is equivalent to the following string representation:
(VP (VBP lack) *scope* )
The rule tree pruning described above reduced the
negation scope rule patterns to 439 and the specula-
tion rule patterns to 1,000.
In addition to generating a set of scope finding
rules, we also implemented a module that parses
string representations of the lexico-syntactic rules
and performs subtree matching. The ScopeFinder
module2 identifies negation and speculation scopes
in sentence parse trees using string-encoded lexico-
syntactic patterns. Candidate sentence parse sub-
trees are first identified by matching the path of cue
leaf nodes to the root of the rule subtree pattern. If an
identical path exists in the sentence, the root of the
candidate subtree is thus also identified. The candi-
date subtree is evaluated for a match by recursively
comparing all node children (starting from the root
of the subtree) to the rule pattern subtree. Nodes
of type *scope* and * match any number of nodes,
similar to the semantics of Regex Kleene star (*).
5 Results
As an informed baseline, we used a previously de-
veloped rule-based system for negation and spec-
ulation scope discovery (Apostolova and Tomuro,
2010). The system, inspired by the NegEx algorithm
(Chapman et al, 2001), uses a list of phrases split
into subsets (preceding vs. following their scope) to
identify cues using string matching. The cue scopes
extend from the cue to the beginning or end of the
sentence, depending on the cue type. Table 3 shows
the baseline results.
Correctly Predicted Cues All Predicted Cues
Negation P R F F
Clinical 94.12 97.61 95.18 85.66
Full Papers 54.45 80.12 64.01 51.78
Paper Abstracts 63.04 85.13 72.31 59.86
Speculation
Clinical 65.87 53.27 58.90 50.84
Full Papers 58.27 52.83 55.41 29.06
Paper Abstracts 73.12 64.50 68.54 38.21
Table 3: Baseline system performance. P (Precision), R (Re-
call), and F (F1-score) are computed based on the sentence to-
kens of correctly predicted cues. The last column shows the
F1-score for sentence tokens of all predicted cues (including er-
roneous ones).
We used only the scopes of predicted cues (cor-
rectly predicted cues vs. all predicted cues) to mea-
2The rule sets and source code are publicly available at
http://scopefinder.sourceforge.net/.
285
sure the baseline system performance. The base-
line system heuristics did not contain all phrase cues
present in the dataset. The scopes of cues that are
missing from the baseline system were not included
in the results. As the baseline system was not penal-
ized for missing cue phrases, the results represent
the upper bound of the system.
Table 4 shows the results from applying the full
extracted rule set (1,681 negation scope rules and
3,043 speculation scope rules) on the test data. As
expected, this rule set consisting of very specific
scope matching rules resulted in very high precision
and very low recall.
Negation P R F A
Clinical 99.47 34.30 51.01 17.58
Full Papers 95.23 25.89 40.72 28.00
Paper Abstracts 87.33 05.78 10.84 07.85
Speculation
Clinical 96.50 20.12 33.30 22.90
Full Papers 88.72 15.89 26.95 10.13
Paper Abstracts 77.50 11.89 20.62 10.00
Table 4: Results from applying the full extracted rule set on the
test data. Precision (P), Recall (R), and F1-score (F) are com-
puted based the number of correctly identified scope tokens in
each sentence. Accuracy (A) is computed for correctly identi-
fied full scopes (exact match).
Table 5 shows the results from applying the rule
set consisting of pruned pattern trees (439 negation
scope rules and 1,000 speculation scope rules) on the
test data. As shown, overall results improved signif-
icantly, both over the baseline and over the unpruned
set of rules. Comparable results are shown in bold
in Tables 3, 4, and 5.
Negation P R F A
Clinical 85.59 92.15 88.75 85.56
Full Papers 49.17 94.82 64.76 71.26
Paper Abstracts 61.48 92.64 73.91 80.63
Speculation
Clinical 67.25 86.24 75.57 71.35
Full Papers 65.96 98.43 78.99 52.63
Paper Abstracts 60.24 95.48 73.87 65.28
Table 5: Results from applying the pruned rule set on the test
data. Precision (P), Recall (R), and F1-score (F) are computed
based on the number of correctly identified scope tokens in each
sentence. Accuracy (A) is computed for correctly identified full
scopes (exact match).
6 Related Work
Interest in the task of identifying negation and spec-
ulation scopes has developed in recent years. Rele-
vant research was facilitated by the appearance of a
publicly available annotated corpus. All systems de-
scribed below were developed and evaluated against
the BioScope corpus (Vincze et al, 2008).
O?zgu?r and Radev (2009) have developed a super-
vised classifier for identifying speculation cues and
a manually compiled list of lexico-syntactic rules for
identifying their scopes. For the performance of the
rule based system on identifying speculation scopes,
they report 61.13 and 79.89 accuracy for BioScope
full papers and abstracts respectively.
Similarly, Morante and Daelemans (2009b) de-
veloped a machine learning system for identifying
hedging cues and their scopes. They modeled the
scope finding problem as a classification task that
determines if a sentence token is the first token in
a scope sequence, the last one, or neither. Results
of the scope finding system with predicted hedge
signals were reported as F1-scores of 38.16, 59.66,
78.54 and for clinical texts, full papers, and abstracts
respectively3. Accuracy (computed for correctly
identified scopes) was reported as 26.21, 35.92, and
65.55 for clinical texts, papers, and abstracts respec-
tively.
Morante and Daelemans have also developed a
metalearner for identifying the scope of negation
(2009a). Results of the negation scope finding sys-
tem with predicted cues are reported as F1-scores
(computed on scope tokens) of 84.20, 70.94, and
82.60 for clinical texts, papers, and abstracts respec-
tively. Accuracy (the percent of correctly identified
exact scopes) is reported as 70.75, 41.00, and 66.07
for clinical texts, papers, and abstracts respectively.
The top three best performers on the CoNLL-
2010 shared task on hedge scope detection (Farkas
et al, 2010) report an F1-score for correctly identi-
fied hedge cues and their scopes ranging from 55.3
to 57.3. The shared task evaluation metrics used
stricter matching criteria based on exact match of
both cues and their corresponding scopes4.
CoNLL-2010 shared task participants applied a
variety of rule-based and machine learning methods
3F1-scores are computed based on scope tokens. Unlike our
evaluation metric, scope token matches are computed for each
cue within a sentence, i.e. a token is evaluated multiple times if
it belongs to more than one cue scope.
4Our system does not focus on individual cue-scope pair de-
tection (we instead optimized scope detection) and as a result
performance metrics are not directly comparable.
286
on the task - Morante et al (2010) used a memory-
based classifier based on the k-nearest neighbor rule
to determine if a token is the first token in a scope se-
quence, the last, or neither; Rei and Briscoe (2010)
used a combination of manually compiled rules, a
CRF classifier, and a sequence of post-processing
steps on the same task; Velldal et al(2010) manu-
ally compiled a set of heuristics based on syntactic
information taken from dependency structures.
7 Discussion
We presented a method for automatic extraction
of lexico-syntactic rules for negation/speculation
scopes from an annotated corpus. The devel-
oped ScopeFinder system, based on the automati-
cally extracted rule sets, was compared to a base-
line rule-based system that does not use syntac-
tic information. The ScopeFinder system outper-
formed the baseline system in all cases and exhib-
ited results comparable to complex feature-based,
machine-learning systems.
In future work, we will explore the use of statisti-
cally based methods for the creation of an optimum
set of lexico-syntactic tree patterns and will evalu-
ate the system performance on texts from different
domains.
References
E. Apostolova and N. Tomuro. 2010. Exploring surface-
level heuristics for negation and speculation discovery
in clinical texts. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
81?82. Association for Computational Linguistics.
W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm
for identifying negated findings and diseases in dis-
charge summaries. Journal of biomedical informatics,
34(5):301?310.
A.B. Clegg and A.J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC bioinformatics,
8(1):24.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006. Citeseer.
R. Farkas, V. Vincze, G. Mo?ra, J. Csirik, and G. Szarvas.
2010. The CoNLL-2010 Shared Task: Learning to
Detect Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning (CoNLL-
2010): Shared Task, pages 1?12.
H. Kilicoglu and S. Bergler. 2008. Recognizing specu-
lative language in biomedical research articles: a lin-
guistically motivated perspective. BMC bioinformat-
ics, 9(Suppl 11):S10.
H. Kilicoglu and S. Bergler. 2010. A High-Precision
Approach to Detecting Hedges and Their Scopes.
CoNLL-2010: Shared Task, page 70.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
D. McClosky and E. Charniak. 2008. Self-training for
biomedical parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies: Short Papers,
pages 101?104. Association for Computational Lin-
guistics.
R. Morante and W. Daelemans. 2009a. A metalearning
approach to processing the scope of negation. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 21?29. As-
sociation for Computational Linguistics.
R. Morante and W. Daelemans. 2009b. Learning the
scope of hedge cues in biomedical texts. In Proceed-
ings of the Workshop on BioNLP, pages 28?36. Asso-
ciation for Computational Linguistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
A. O?zgu?r and D.R. Radev. 2009. Detecting speculations
and their scopes in scientific text. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume 3, pages
1398?1407. Association for Computational Linguis-
tics.
M. Rei and T. Briscoe. 2010. Combining manual rules
and supervised learning for hedge cue and scope detec-
tion. In Proceedings of the 14th Conference on Natu-
ral Language Learning, pages 56?63.
E. Velldal, L. ?vrelid, and S. Oepen. 2010. Re-
solving Speculation: MaxEnt Cue Classification and
Dependency-Based Scope Rules. CoNLL-2010:
Shared Task, page 48.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
bioinformatics, 9(Suppl 11):S9.
H. Zhou, X. Li, D. Huang, Z. Li, and Y. Yang. 2010.
Exploiting Multi-Features to Detect Hedges and Their
Scope in Biomedical Texts. CoNLL-2010: Shared
Task, page 106.
287
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 73?76
Manchester, August 2008
The "Close-Distant" Relation of Adjectival Concepts                
Based on Self-Organizing Map 
Kyoko Kanzaki, Hitoshi Isahara 
National Institute of Information and 
Communications Technology 
3-5, Hikaridai, Seikacho, 
Sorakugun, Kyoto, 619-0289,  
Japan 
{kanzaki,isahara}@nict.go.jp
Noriko Tomuro 
School of Computer Science, Telecom-
munications and Information Systems 
DePaul University 
Chicago, IL 60604 
U.S.A 
tomuro@cs.depaul.edu 
 
Abstract 
In this paper we aim to detect some as-
pects of adjectival meanings. Concepts of 
adjectives are distributed by SOM (Self-
Organizing map) whose feature vectors 
are calculated by MI (Mutual Informa-
tion). For the SOM obtained, we make 
tight clusters from map nodes, calculated 
by cosine. In addition, the number of 
tight clusters obtained by cosine was in-
creased using map nodes and Japanese 
thesaurus. As a result, the number of ex-
tended clusters of concepts was 149 clus-
ters. From the map, we found 8 adjectival 
clusters in super-ordinate level and some 
tendencies of similar and dissimilar clus-
ters. 
1 Introduction 
This paper aims to find a diversity range of ad-
jectival meanings from a coordinate map in 
which  "close-distant" relationships between ad-
jectival classes is reflected. In related research 
over adjectives, Alonge et.al (2000), Solar (2003), 
Marrafa and Mendes (2006) suggested that 
WordNet and EuroWordNet lack sufficient ad-
jectival classes and semantic relations, and  ex-
tended the resources over such relations. 
For the sake of identifying the diversity of ad-
jectival meanings, it is necessary to analyze ad-
jectival semantics via "close-distant" relation-
ships extracted from texts. In our work on ex-
tracting adjective semantics, we consider abstract 
nouns as semantic proxies of adjectives. For the 
clustering method, we utilized a self-organizing 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
map (SOM) based on a neural network model 
(Kohonen, 1997). One of the features of SOM is 
that it assigns words coordinates, allowing for 
the possibility of visualizing word similarity. 
SOM has two advantages for our task. One is 
that we can utilize the map nodes of words to 
locate members of clusters that clustering meth-
ods have failed to classify. The other is that the 
map shows the relative relations of whole clus-
ters of adjectival concepts. By observing such a 
map in which the relations of clusters are re-
flected, we can analyze the diversity of adjectival 
meaning. 
2 Abstract Nouns that Categorize Ad-
jectives 
Collocations between adjectives and nouns in 
?concrete value and its concept? relations can be 
used to represent adjectival semantics. Nemoto 
(1969) indicated that expressions such as ?iro ga 
akai (the color is red)? and ?hayasa ga hayai 
(literally, the speed is fast)? are a kind of 
tautology. Some studies have suggested that 
some abstract nouns collocating with adjectives 
are hypernymic concepts (or concepts) of those 
adjectives, and that some semantic relations 
between abstract nouns and adjectives represent 
a kind of repetition of meaning. 
   This paper defines such abstract nouns as the 
semantic categorization of an adjective (or an 
adjectival concept). 
The data for this study was obtained by ex-
tracting adjectives co-occurring with abstract 
nouns in 100 novels, 100 essays, and 42 years of 
newspaper articles. 
We extracted the abstract nouns according to 
the procedure described by Kanzaki et.al (2006). 
Here, they evaluated the category labels of adjec-
tives obtained by the proposed procedure and 
found that for 63% of the adjectives, the ex-
73
tracted categories were found to be appropriate. 
We constructed a list as follows: 
Abstract Nouns:  
Adjectives modifying abstract nouns   
KIMOCHI (feeling):  
ureshii (glad), kanashii (sad), 
shiawasena (happy) ? 
In this list,  ?KIMOCHI (feeling)? is defined by 
?ureshii (glad), kanashii (sad), and shiawasena 
(happy)?, for example. Here, each abstract noun 
conveys the concept or hypernym of the given 
adjectives. 
Next we classify these abstract nouns based on 
their co-occurring adjectives using SOM. 
3. A Map of Adjective Semantics  
3.1 Input Data 
In our SOM, we use adjectives which occur more 
than four times in our corpus. The number of 
such adjectives was 2374. Then we identified 
361 abstract nouns that co-occurred with four or 
more of the adjectives. The maximum number of 
co-occurring adjectives for a given abstract noun 
in the corpus was 1,594. 
    In the data, each abstract noun was defined by 
a feature vector, in the form of noun co-
occurrences represented by pointwise mutual 
information (Manning and Schutze, 1999). Mu-
tual information (MI) is an information theoric 
measure and has been used in many NLP tasks, 
including clustering words (e.g. Lin and Pantel, 
2002). 
3.2 SOM 
Kohonen?s self-organizing map (SOM) is an un-
supervised learning method, where input in-
stances are projected onto a grid/map of nodes 
arranged in an n-dimensional space. Input in-
stances are usually high-dimensional data, while 
the map is usually two-dimensional (i.e., n = 2). 
Thus, SOM essentially reduces the dimensional-
ity of the data, and can be used as an effective 
tool for data visualization ? projecting complex, 
high-dimensional data onto a low-dimensional 
map. SOM can also be utilized for clustering. 
Each node in a map represents a cluster and is 
associated with a reference vector of m-
dimensions, where m is the dimension of the in-
put instances. During learning, input instances 
are mapped to a map node whose (current) refer-
ence vector is the closest to the instance vector 
(where SOM uses Euclidean distance as the 
measure of similarity by default), and the refer-
ence vectors are gradually smoothed so that the 
differences between the reference vector and the 
instance vectors mapped to the node are mini-
mized. This way, instances mapped to the same 
node form a cluster, and the reference vector es-
sentially corresponds to the centroid of the clus-
ter. 
SOM maps are self-organizing in the sense 
that input instances that are similar are gradually 
pulled closer during learning and assigned to 
nodes that are topographically close to one an-
other on the map. The mapping from input in-
stances to map nodes is one-to-one (i.e., one in-
stance is assigned to exactly one node), but from 
map nodes to instances, the mapping is one-to-
many (i.e., one map node is assigned to zero, one, 
or more instances). 
The input data was the set of 361 abstract 
nouns defined by the 2,374 co-occurring adjec-
tives, as described in the previous section. These 
abstract nouns were distributed visually on the 2-
dimensional map based on co-occurring adjec-
tives. This map is a ?map of adjective semantics? 
because the abstract nouns are identified as prox-
ies for adjective semantics.  
As mentioned before, similar words are lo-
cated in neighboring nodes on the 2-dimensional 
map. The next step is to identify similar clusters 
on the map. 
4. Clusters of Adjective Semantics 
4.1 Tight Clusters from the Map Nodes 
In SOMs, each node represents a cluster, i.e. a set 
of nouns assigned to the same node. These nouns 
are very similar and can be considered to be 
synonyms. However, nouns that are similar 
might map to different nodes because the algo-
rithm?s self-organization is sensitive to the pa-
rameter settings. To account for this, and also to 
obtain a more (coarse-grained) qualitative de-
scription of the map, tight clusters?clusters of 
map nodes whose reference vectors are signifi-
cantly close?were extracted. All groupings of 
map nodes whose average cosine coefficient be-
tween the reference vectors in the group was 
greater than 0.96 were extracted (Salton and 
McGill, 1983).  
4.2 Result  
The total number of clusters was 213. Excluding 
singleton clusters, the number of clustes was 81. 
229 concepts were classified into 81 clusters, 
with 132 concepts not classified into any cluster. 
74
In order to evaluate the quality of the concep-
tual classification, we utilized the ?Bunruigoi-
hyou?  Japanese thesaurus (National Institute of 
Japanese Language, 1964). In ?Bunruigoihyou,? 
each category is assigned a 5-digit category 
number, with close numbers indicating similar 
categories.  
Among the 81 with two or more concepts, the 
number of clusters containing words with the 
same class was 36. That is, for 44% of the clus-
ters, the constituent nouns had the same ?Bun-
ruigoihyou? class label. The ratio of concept 
agreement between "Bunruigiohyou? and our 
obtained clusters was found to be  20.87/81=0.25.  
We also compared tight clusters by performing  
hierarchical clustering with the k-means algo-
rithm. 
The results of the hierarchical clustering were as 
follows: 
1) The rate of clusters agreeing with ?Bunruigoi-
hyou?: 30/96 = 0.31 
2) The average rate of agreement for each tight 
cluster: 21.07/96 = 0.21 
In the case of k-means: 
3)The rate of clusters agreeing with ?Bunruigoi-
hyou?: 33/143 = 0.23 
4) The average rate of agreement for each tight   
cluster: 28.37/143 = 0.198 
From these results, we can observe that clus-
ters obtained with cosine similarity agree more 
with the Japanese thesaurus than the other two 
methods. Therefore, in terms of quality, clusters 
obtained by cosine similarity seem to be superior 
to the others. 
4.3 Using the Position of Map Nodes 
However, even for the result obtained with co-
sine similarity, 132 concepts were not classified 
into any clusters. Additionally, the clusters ap-
pear to be overly fine grained: most tight clusters 
include 1, 2 or 3 concepts. In order to find simi-
lar concepts that cosine similarity failed to clus-
ter together, we used the position information of 
the map nodes.  
After we plotted clusters obtained by cosine 
similarity on the map, we checked for singleton 
concepts located near a cluster which are mem-
bers of the same ?Bunruigoihyou? class.  Also, 
we checked to see if concepts in clusters located 
at neighboring nodes could be clustered together  
using the category numbers of ?Bunruigoihyou. ? 
By extending the clusters, we generated a total 
of 149 clusters, including 68 with two or more 
elements and 81 singleton clusters. 
5. Interpreting the Adjectival Clusters 
In our final map, 361 concepts were distributed 
based on 2374 adjectives into 149 clusters. 
Among the 149 clusters, 68 contained two or 
more concepts.  
5.1 ?Close-Distant? Relations of Clusters and 
Adjectives 
In the final map, clusters at the superordinate 
level are located around the center of the map. 
Upper level concepts tend to agree with clusters 
in ?Bunruigoihyou.?  For examples, ?image and 
impression,? ?situation and state?, ?feeling and 
mood? are located around the center of the map. 
 
 
 
 
 
 
 
 
 
 
Cluster1 (Center of the map): koto (matter), 
in?shou (impression), men (side of some-
thing or someone), and kankaku 
(sense/feeling) 
Cluster2: seishitsu (characteristics of some-
one/something), yousou (aspect)  
Cluster3: kanten (viewpoint), tachiba (stand-
point), bun?ya (domain) 
Cluster4: taido (attitude), yarikata (way of do-
ing) 
Cluster5: gaikan, gaiken, sugata (outlook and 
appearance of someone/something) 
Cluster6:  fun?iki, kuuki, kehai (atmosphere) 
Cluster7:  kimochi, kanji (feeling) 
Cluster8:  joutai (state), joukyou (situation) 
 
In our experiment, at the top level, adjectival 
concepts seem to be divided into 8 basic clusters. 
From the distribution of the map, we find ?close-
distant? relationships between clusters, that is 
clusters located far from each other tend to be 
semantically disparate. In terms of adjective se-
mantics, the semantic relationship between ?ki-
mochi, kanji (feeling)? (Cluster7) and ?seishitsu 
(characteristics of someone/something), yousou 
(aspect)? are distant. 
However, ?kimochi, kanji (feeling)? (Cluster7) 
has a close relation to ?fun?iki, kuuki, kehai 
(atmosphere) ? (Cluster6) and also  ?joutai (state), 
joukyou (situation)? (Cluster8). 
Fig7. Cluster 7 on the map 
1
2 
4 3 
5
Center of a map 6
7
8
75
1. In our experiment, 77 adjectives belonged 
to one or two clusters. Though there is the 
possibility of data sparseness, there is also 
the possibility that the meanings of these 
adjectives are specific. Examples of adjec-
tives belonging to specific clusters are as 
follows: 
 
Adjectives in distant relationships; 
- Clusters 2: keisandakai (seeing everything in 
terms of money), ken?meina 
(wise), ? 
- Cluster 7: akkenai (disappointing/easily), kiya-
sui (feel at home),? 
 
Adjectives in close relationships; 
- Cluster 6: ayashigena (fishy) 
- Cluster7: akkenai (disappointing /easily), kiya-
sui (feel at home) 
- Cluster8: meihakuna (obvious), omoshiroi (in-
teresting), makkurana (dark) 
 
Japanese adjectives are often said to represent 
?kanjou (mental state)?, ?joutai (state),? ?seisitsu 
(characteristics)? and ?teido (degree)?, in addi-
tion to ?positive/negative image.? In our experi-
ment, the SOM unearthed not only these adjecti-
val meanings, but also ?inshou (impression)?, 
?taido (attitude)?, ?kanten (viewpoint)? and 
?sugata (outlook)?, which seem to be discrimina-
tive meanings of adjectives. 
6. Future work 
We classified 361 concepts based on 2374 adjec-
tives using a self-organizing map. Since the 
SOM shows the distribution visually, it provides 
not only clusters of adjectives but also ?close-
distant? relationships between clusters. As a re-
sult, adjectival concepts at the superordinate 
level are divided into 8 main clusters. The results 
not only verify previous work but also suggest 
new discriminative adjective classes. One of the 
advantages of SOM is that it presents its outputs 
visually. As a result, we can explore ?close- dis-
tant? relationships between clusters, and  analyze 
the meaning of each. In addition to increasing the 
range of adjectival classes and improving our 
method, our method provides the means to ana-
lyze concepts which did not agree with those in 
existing thesauri such as ?Bunruigoihyou?, the 
EDR dictionary or Japanese Word Net. 
 
 
 
References 
Alonge, Antonietta., Francesca Bertagna, Nicoletta 
Calzolari, Andriana Roventini and Antonio Zam-
polli. 2000. Encoding Information on Adjectives in 
a Lexical-semantic Net for Computational Applica-
tions, Proceedings of the 1st Conference of the 
North American Chapter of the Association for 
Computational Linguistics(NAACL-00) :42-49 
Kyoko Kanzaki,Qing Ma, Eiko Yamamoto and 
Hitoshi Isahara, 2006, Semantic Analysis of 
Abstract Nouns to Compile a Thesaurus of 
Adjectives, In Proceedings of The Interna-
tional Conference on Language Resources 
and Evaluation (LREC-06) 
Kohonen, Teuvo. 1997. Self-Organizing Maps, Sec-
ond Edition, Springer. 
Lin, Dekang., and Patrick Pantel. 2002. Concept Dis-
covery from Text, Proceedings of the 19th Interna-
tional Conference on Computational Linguis-
tics(COLING-02): 768-774  
Manning, Christopher D., and Hinrich Sh?tze. 1999. 
Foundations of Statistical Natural language Proc-
essing, The MIT Press. 
Marrafa, Palmira., and Sara Mendes. 2006. Modeling 
Adjectives in Computational Relational Lexica, 
Proceedings of the COLING/ACL2006:555-562 
National Institute for Japanese Language. 1964. Bun-
ruigoihyou (Word List by Semantic Principles). 
Nemoto, Kesao. 1969. The combination of the noun 
with ?ga-Case? and the adjective, Language re-
search 2 for the computer, National Language Re-
search Institute: 63-73 (in Japanese) 
Salton, Gerard., and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval.  McGraw 
Hill. 
Solar, Clara. 2003. Extension of Spanish WordNet, 
Proceedings of the third International WordNet 
Conference(GWC-06):213-219 
 
76
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 81?82,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Exploring Surface-level Heuristics for Negation and Speculation
Discovery in Clinical Texts
Emilia Apostolova
DePaul University
Chicago, IL USA
emilia.aposto@gmail.com
Noriko Tomuro
DePaul University
Chicago, IL USA
tomuro@cs.depaul.edu
Abstract
We investigate the automatic identification
of negated and speculative statements in
biomedical texts, focusing on the clinical
domain. Our goal is to evaluate the perfor-
mance of simple, Regex-based algorithms
that have the advantage of low compu-
tational cost, simple implementation, and
do not rely on the accurate computation
of deep linguistic features of idiosyncratic
clinical texts. The performance of the
NegEx algorithm with an additional set of
Regex-based rules reveals promising re-
sults (evaluated on the BioScope corpus).
Current and future work focuses on a boot-
strapping algorithm for the discovery of
new rules from unannotated clinical texts.
1 Motivation
Finding negated and speculative (hedging) state-
ments is an important subtask for biomedical In-
formation Extraction (IE) systems. The task of
hedge detection is of particular importance in the
sub-genre of clinical texts which tend to avoid un-
qualified negations or assertions.
Negation/Speculation discovery is typically
broken down into two subtasks - discovering the
negation/speculation cue (a phrase or a syntactic
pattern) and establishing its scope. While a num-
ber of cue and scope discovery algorithms have
been developed, high performing systems typi-
cally rely on machine learning and more involved
feature creation. Deep linguistic feature creation
could pose problems, as the idiosyncrasies of clin-
ical texts often confuse off-the-shelf NLP feature
generation tools (e.g. relying on proper punctu-
ation and grammaticality). In addition, computa-
tionally expensive algorithms could pose problems
for high-volume IE systems.
In contrast, simple Regex-based algorithms
have demonstrated larger practical significance as
they offer reasonable performance at a low devel-
opment and computational cost. NegEx1 (Chap-
man et al, 2001), a simple rule-based algorithm
developed for the discovery of negation of findings
and diseases in discharge summaries, has been im-
plemented in a number of BioNLP systems, in-
cluding Metamap2, CaTIES3, and Mayo Clinic?s
Clinical IE System (Savova et al, 2008). In
NegEx, a list of phrases split into subsets are used
to identify cues and their corresponding scopes
(token widows preceding or following the cues).
2 Method
Negation/Speculation in general English could be
expressed by almost any combination of mor-
phologic, syntactic, semantic, and discourse-level
means. However, the scientific ?dryness? of the
biomedical genre and clinical texts in particular,
limits language variability and simplifies the task.
We evaluated the performance of the NegEx al-
gorithm on the BioScope corpus (Szarvas et al,
2008). BioScope corpus statistics are shown in Ta-
bles 1 and 2.
Corpus Type Sentences Documents Mean Document Size
Radiology Reports 7520 1954 3.85
Biological Full Papers 3352 9 372.44
Biological Paper Abstracts 14565 1273 11.44
Table 1: Statistics of the BioScope corpus. Document sizes
represent number of sentences.
Corpus Type Negation Cues Speculation Cues Negation Speculation
Rad Reports 872 1137 6.6% 13.4%
Full Papers 378 682 13.76% 22.29%
Paper Abstracts 1757 2694 13.45% 17.69%
Table 2: The percentage of speculative sentences (last col-
umn) is larger than the percentage of negated sentences.
We first evaluated the performance of an un-
modified version of the NegEx algorithm on the
task of cue detection (Table 3). Without any tuning
or modifications, NegEx performed well on identi-
fying negation cues across all documents, achiev-
1
http://code.google.com/p/negex/
2
c?The National Library of Medicine
3
http://caties.cabig.upmc.edu/Wiki.jsp?page=Home
81
ing an F-score of 90% on the clinical texts. For
the task of identifying speculation cues, we sim-
ply used the NegEx Conditional Possibility Phrase
list (35 speculative cue phrases). The overall per-
formance of this simplistic approach revealed poor
results.
TP FP FN Precision Recall F-score
Negation
Rad Reports 836 131 36 86.45 95.87 90.92
Full Papers 307 74 71 80.58 81.22 80.9
Paper Abstracts 1390 211 367 86.82 79.11 82.79
Speculation
Rad Reports 62 1 1075 98.41 5.45 10.33
Full Papers 1 0 681 100.0 0.15 0.3
Paper Abstracts 0 5 2694 0.0 0.0 0
Table 3: NegEx performance on identifying Negation and
Speculation Cues (non-exact boundary). (TP=true positive,
FP=false positive, FN=false negative)
As shown in Figure 1, speculation cues ex-
hibit wider variability and a rule matching only
35 phrases proved inefficient. To enrich the list of
speculation cues, we used hedging cues from the
FlySlip corpus of speculative sentences4. Without
any synonym expansion or fine-tuning, the per-
formance of speculation cue detection improved
significantly as shown in Table 4, achieving an F-
score of 86% on the clinical dataset5.
Figure 1: The number of occurrences (Y axis) of the 228
unique speculation cues and the 45 unique negation cues of
the BioScope corpus (X axis).
Corpus TP FP FN Precision Recall F-score
Rad Reports 903 52 234 94.55 79.42 86.33
Full Papers 439 553 243 44.25 64.37 52.45
Paper Abstracts 1741 1811 953 49.01 64.63 55.75
Table 4: NegEx performance on identifying speculation
cues (non-exact boundary) with the addition of the FlySlip
hedging cues.
We next measured the performance of NegEx
on scope detection. Newly introduced speculation
cues from the FlySlip corpus were automatically
classified into preceding or following their scope
based the position of of their annotated ?topic?. Ta-
ble 5 shows the results of scope identification.
3 Discussion
Our results show that a simple, surface-level algo-
rithm could be sufficient for the task of negation
4
http://www.wiki.cl.cam.ac.uk/rowiki/NaturalLanguage/FlySlip/Flyslip-resources
5
To avoid fine-tuning cues on the corpus we did not set aside a training subset of
the BioScope corpus for speculation cue enhancements and instead used an independent
hedging corpus (FlySlip).
TP FP FN Precision Recall F-score
Negation
Rad Reports 4003 267 140 94.12 97.61 95.18
Full Papers 2129 1835 525 54.45 80.12 64.01
Paper Abstracts 10049 6023 1728 63.04 85.13 72.31
Speculation
Rad Reports 2817 1459 2471 65.87 53.27 58.90
Full Papers 3313 2372 2958 58.27 52.83 55.41
Paper Abstracts 17219 6329 9477 73.12 64.50 68.54
Table 5: NegEx performance on identifying scopes of cor-
rectly identified cues. Precision and recall are computed
based on the number of correctly identified scope tokens
excluding punctuation (i.e. number of tokens within cue
scopes). Best results were achieved with no scope window
size (i.e. using sentence boundaries).
and hedge detection in clinical texts. Using the
NegEx algorithm and the FlySlip hedging corpus,
without any modifications or additions, we were
able to achieve an impressive F-score of 90.92%
and 86.33% for negation and speculation cue dis-
covery respectively6. We are currently expand-
ing the set of speculation cues using an unan-
notated dataset of clinical texts and a bootstrap-
ping algorithm (Medlock, 2008). The algorithm
is based on the intuition that speculative cues tend
to co-occur and this redundancy could be explored
to probabilistically discover new cues from high-
confidence existing ones. We are also exploring
the discovery of degree of speculativeness (e.g.
very unlikely vs very likely).
While NegEx performed well on the task of
identifying negation scope (F-score 95.18), further
work is needed on the discovery of speculation
scopes (F-score 58.90). As hedging cues require
a more fine-tuned set of rules, in future work we
will evaluate linguistically motivated approaches
(Kilicoglu and Bergler, 2008) for the creation of a
set of surface-level speculation scope rules.
References
W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, and B.G. Buchanan.
2001. A simple algorithm for identifying negated findings and diseases in
discharge summaries. Journal of biomedical informatics, 34(5):301?310.
H. Kilicoglu and S. Bergler. 2008. Recognizing speculative language in
biomedical research articles: a linguistically motivated perspective. BMC
bioinformatics, 9(Suppl 11):S10.
B. Medlock. 2008. Exploring hedge identification in biomedical literature.
Journal of Biomedical Informatics, 41(4):636?654.
G.K. Savova, K. Kipper-Schuler, J.D. Buntrock, and C.G. Chute. 2008.
UIMA-based Clinical Information Extraction System. In Proc. UIMA for
NLP Workshop. LREC.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008. The BioScope corpus:
annotation for negation, uncertainty and their scope in biomedical texts.
In Proceedings of the Workshop on Current Trends in Biomedical Natural
Language Processing, pages 38?45. Association for Computational Lin-
guistics.
6
The enhanced speculation cue phrase lists and a UIMA-based NegEx implementation
are available upon request.
82
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 118?121,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Domain Adaptation of Coreference Resolution for Radiology Reports
Emilia Apostolova, Noriko Tomuro, Pattanasak Mongkolwat*, Dina Demner-Fushman?
College of Computing and Digital Media, DePaul University, Chicago, IL
*Department of Radiology, Northwestern University Medical School, Chicago, IL
?Communications Engineering Branch, National Library of Medicine, Bethesda, MD
emilia.aposto@gmail.com, tomuro@cs.depaul.edu,
p-mongkolwat@northwestern.edu, ddemner@mail.nih.gov
Abstract
In this paper we explore the applicability of
existing coreference resolution systems to a
biomedical genre: radiology reports. Analysis
revealed that, due to the idiosyncrasies of the
domain, both the formulation of the problem
of coreference resolution and its solution need
significant domain adaptation work. We refor-
mulated the task and developed an unsuper-
vised algorithm based on heuristics for coref-
erence resolution in radiology reports. The
algorithm is shown to perform well on a test
dataset of 150 manually annotated radiology
reports.
1 Introduction
Coreference resolution is the process of determin-
ing whether two expressions in natural language re-
fer to the same entity in the world. General purpose
coreference resolution systems typically cluster all
mentions (usually noun phrases) in a document into
coreference chains according to the underlying ref-
erence entity. A number of coreference resolution
algorithms have been developed for general texts. To
name a few, Soon et al (2001) employed machine
learning on the task and achieved an F-score of 62.6
and 60.4 on the MUC-6 (1995) and MUC-7 (1997)
coreference corpora respectively. Ng et al (2002)
improved this learning framework and achieved F-
scores of 70.4 and 63.4 respectively on the same
datasets.
There are also a number of freely available off-
the-shelf coreference resolution modules developed
for the general domain. For example, BART (Vers-
ley et al, 2008) is an open source coreference reso-
lution system which provides an implementation of
the Soon et al algorithm (2001). The Stanford De-
terministic Coreference Resolution System (Raghu-
nathan et al, 2010) uses an unsupervised sieve-like
approach to coreference resolution. Similarly, the
GATE Information Extraction system (Cunningham
et al, 2002) includes a rule-based coreference reso-
lution module consisting of orthography-based pat-
terns and a pronominal coreferencer (matching pro-
nouns to the most recent referent).
While coreference resolution is a universal dis-
course problem, both the scope of the problem and
its solution could vary significantly across domains
and text genres. Newswire coreference resolution
corpora (such as the MUC corpus) and general pur-
pose tools do not always fit the needs of specific do-
mains such as the biomedical domain well.
The importance and distinctive characteristics of
coreference resolution for biomedical articles has
been recognized, for example (Castano et al, 2002;
Gasperin, 2006; Gasperin et al, 2007; Su et al,
2008). Within the biomedical field, clinical texts
have been noted as a genre that needs specialized
coreference corpora and methodologies (Zheng et
al., 2011). The importance of the task for the clini-
cal domain has been attested by the 2011 i2b2 NLP
shared task (Informatics for Integrating Biology and
the Bedside1) which provided an evaluation plat-
form for coreference resolution for clinical texts.
However, even within the clinical domain, coref-
erence in different sub-genres could vary signifi-
1https://www.i2b2.org/NLP/Coreference/
118
cantly. In this paper we demonstrate the idiosyn-
crasies of the task of coreference resolution in a
clinical domain sub-genre, radiology reports, and
describe an unsupervised system developed for the
task.
2 Coreference Resolution for Radiology
Reports
Radiology reports have some unique characteristics
that preclude the use of coreference resolution mod-
ules or algorithms developed for the general biomed-
ical domain or even for other types of clinical texts.
The radiology report is a clinical text used to com-
municate medical image findings and observations
to referring physicians. Typically, radiology reports
are produced by radiologists after examining medi-
cal images and are used to describe the findings and
observations present in the accompanied images.
The radiology report accompanies an imaging
study and frequently refers to artifacts present in
the image. In radiology reports, artifacts present
in the image exhibit discourse salience, and as a
result are often introduced with definite pronouns
and articles. For example, consider the sentence
The pericardial space is clear. The definite noun
phrase the pericardial space does not represent an
anaphoric (or cataphoric) discourse entity and has
no antecedent. In contrast, coreference resolution
in general texts typically considers definite noun
phrases to be anaphoric discourse entities and at-
tempts to find their antecedents.
Another important distinction between general
purpose coreference resolution and the coreference
resolution module needed by an NLP system for
clinical texts is the scope of the task. General pur-
pose coreference resolution systems typically cluster
all mentions in a document into coreference chains.
Such comprehensive mention clustering is often not
necessary for the purposes of clinical text NLP sys-
tems. Biomedical Information Extraction systems
typically first identify named entities (medical con-
cepts) and map them to unambiguous biomedical
standard vocabularies (e.g. UMLS2 or RadLex3 in
the radiological domain). While multiple mentions
of the same named entity could exist in a document,
2http://www.nlm.nih.gov/research/umls/
3http://www.radlex.org/
in most cases these mentions were previously as-
signed to the same medical concept. For example,
multiple report mentions of ?the heart? or ?the lung?
will normally be mapped to the same medical con-
cept and clustering of these mentions into corefer-
ence chains is typically not needed.
3 Task Definition
Analysis revealed that the coreference resolution
task could be simplified and still meet the needs of
most Information Extraction tasks relevant to the ra-
diological domain. Due to their nature, texts de-
scribing medical image finding and observations do
not contain most pronominal references typically
targeted by coreference resolution systems. For ex-
ample, no occurrence of personal pronouns (e.g. he,
I), possessive pronouns (e.g. his, my), and indefi-
nite pronouns (e.g. anyone, nobody) was found in
the validation dataset. Demonstrative pronouns and
non-pleonastic ?it? mentions were the only pronom-
inal references observed in the dataset4. The fol-
lowing examples demonstrate the use of demonstra-
tive pronouns and the non-pleonastic ?it? pronoun
(shown in bold):
There is prominent soft tissue swelling involving
the premaxillary tissues. This measures approxi-
mately 15 mm in thickness and extends to the infe-
rior aspect of the nose.
There is a foreign object in the proximal left main-
stem bronchus on series 11 image 17 that was not
present on the prior study. It has a somewhat ovoid
to linear configuration.
Following these observations, the coreference res-
olution task has been simplified as follows. Corefer-
ence chains are assigned only for demonstrative pro-
nouns and ?it? noun phrases. The coreference reso-
lution task then involves selecting for each mention
a single best antecedent among previously annotated
named entities (medical concepts) or the NULL an-
tecedent.
4 Dataset
A total of 300 radiology reports were set aside for
validation and testing purposes. The dataset consists
4Pleonastic ?it? refers to its use as a ?dummy? pronoun, e.g.
It is raining, while non-pleonastic use of the pronoun refers to
a specific entity.
119
Figure 1: A sample DICOM image from an imaging
study described by the following radiology report snip-
pet: . . . FINDINGS: Targeted sonography of the upper in-
ner left breast was performed. At the site of palpable ab-
normality, at the 11 o?clock position 3 cm from the nipple,
there is an oval circumscribed, benign-appearing hypoe-
choic mass measuring 2.0 x 1.6 x 1.4 cm. There is mild
internal blood flow. It is surrounded by normal appearing
glandular breast tissue.. . .
of 100 Computed Tomography Chest reports, 100
Ultrasound Breast reports, and 100 Magnetic Res-
onance Brain reports, all randomly selected based
on their report types from a dataset of more than
100,000 de-identified reports spanning a period of
9 years5. These three types of reports represent
a diverse dataset covering representative imaging
modalities and body regions. Figure 1 shows a sam-
ple Breast Ultrasound DICOM6 image and its asso-
ciated radiology report.
The reports were previously tagged (using an au-
tomated system) with medical concepts and their
semantic types (e.g. anatomical entity, disorder,
imaging observation, etc.). Half of the dataset (150
reports) was manually annotated with coreference
chains using the simplified task definition described
above. The other half of the dataset was used for
validation of the system described next.
5 Method and Results
The coreference resolution task involves selecting
for each mention a single best antecedent among
previously annotated named entities or the NULL
antecedent. Mentions are demonstrative pronoun
phrases or definite noun phrases containing previ-
ously annotated named entities.
5The collection is a proprietary dataset belonging to North-
western University Medical School.
6Digital Imaging and Communications in Medicine, c? The
National Electrical Manufacturers Association.
We implemented an algorithm for the task de-
scribed above which was inspired by the work of
Haghighi and Klein (2009). The algorithm first iden-
tifies mentions within each report and orders them
linearly according to the position of the mention
head. Then it selects the antecedent (or the NULL
antecedent) for each mention as follows:
1. The possible antecedent candidates are first fil-
tered based on a distance constraint. Only mentions
of interest belonging to the preceding two sentences
are considered. The rationale for this filtering step is
that radiology reports are typically very concise and
less cohesive than general texts. Paragraphs often
describe multiple observations and anatomical enti-
ties sequentially and rarely refer to mentions more
distant than the preceding two sentences.
2. The remaining antecedent candidates are then
filtered based on a syntactic constraint: the co-
referent mentions must agree in number (singular or
plural based on the noun phrase head).
3. The remaining antecedent candidates are then
filtered based on a semantic constraint. If the two
mentions refer to named entities, the named entities
need to have the same semantic category7.
4. After filtering, the closest mention from the set
of remaining possible antecedents is selected. If the
set is empty, the NULL antecedent is selected.
Pairwise coreference decisions are considered
transitive and antecedent matches are propagated
transitively to all paired co-referents.
The algorithm was evaluated on the manually an-
notated test dataset. Results (Table 1) were com-
puted using the pairwise F1-score measure: preci-
sion, recall, and F1-score were computed over all
pairs of mentions in the same coreference cluster.
Precision Recall F1-score
74.90 48.22 58.66
Table 1: Pairwise coreference resolution results.
The system performance is within the range of
state-of-the-art supervised and unsupervised coref-
erence resolution systems8. F1-scores could range
7The same semantic type in the case of UMLS concepts or
the same parent in the case of RadLex concepts.
8Source code for the described system will be made avail-
able upon request.
120
between 39.8 and 67.3 for various methods and
test sets (Haghighi and Klein, 2009). The simpli-
fication of the coreference resolution problem de-
scribed above allowed us to focus only on corefer-
ence chains of interest to clinical text Information
Extraction tasks and positively influenced the out-
come. In addition, our goal was to focus on high
precision results as opposed to optimizing the over-
all F1-score. This guarantees that coreference reso-
lution errors will result in mostly omissions of coref-
erence pairs and will not introduce information ex-
traction inaccuracies.
6 Conclusion
In this paper, we presented some of the challenges
involved in the task of adapting coreference resolu-
tion for the domain of clinical radiology. We pre-
sented a domain-specific definition of the corefer-
ence resolution task. The task was reformulated and
simplified in a practical manner that ensures that the
needs of biomedical information extraction systems
are still met. We developed an unsupervised ap-
proach to the task of coreference resolution of radi-
ology reports and demonstrate state-of-the-art preci-
sion and reasonable recall results. The developed
system is made publicly available to the NLP re-
search community.
References
J. Castano, J. Zhang, and J. Pustejovsky. 2002. Anaphora
resolution in biomedical literature. In International
Symposium on Reference Resolution. Citeseer.
D.H. Cunningham, D.D. Maynard, D.K. Bontcheva, and
M.V. Tablan. 2002. GATE: A Framework and Graph-
ical Development Environment for Robust NLP Tools
and Applications.
C. Gasperin, N. Karamanis, and R. Seal. 2007. Annota-
tion of anaphoric relations in biomedical full-text arti-
cles using a domain-relevant scheme. In Proceedings
of DAARC, volume 2007. Citeseer.
C. Gasperin. 2006. Semi-supervised anaphora resolution
in biomedical texts. In Proceedings of the Workshop
on Linking Natural Language Processing and Biology:
Towards Deeper Biological Literature Analysis, pages
96?103. Association for Computational Linguistics.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152?1161. Association for Compu-
tational Linguistics.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010. A
multi-pass sieve for coreference resolution. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
J. Su, X. Yang, H. Hong, Y. Tateisi, J. Tsujii, M. Ash-
burner, U. Leser, and D. Rebholz-Schuhmann. 2008.
Coreference resolution in biomedical texts: a machine
learning approach. Ontologies and Text Mining for
Life Sciences 08.
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
Bart: A modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies: Demo Session, pages 9?12. As-
sociation for Computational Linguistics.
J. Zheng, W.W. Chapman, R.S. Crowley, and G.K.
Savova. 2011. Coreference resolution: A review of
general methodologies and applications in the clinical
domain. Journal of biomedical informatics.
121
