Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1104?1115,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Hypotheses Selection Criteria in a Reranking Framework for Spoken
Language Understanding
Marco Dinarelli
LIMSI-CNRS
B.P. 133, 91403 Orsay Cedex
France
marcod@limsi.fr
Sophie Rosset
LIMSI-CNRS
B.P. 133, 91403 Orsay Cedex
France
rosset@limsi.fr
Abstract
Reranking models have been successfully ap-
plied to many tasks of Natural Language Pro-
cessing. However, there are two aspects of
this approach that need a deeper investiga-
tion: (i) Assessment of hypotheses generated
for reranking at classification phase: baseline
models generate a list of hypotheses and these
are used for reranking without any assess-
ment; (ii) Detection of cases where rerank-
ing models provide a worst result: the best
hypothesis provided by the reranking model
is assumed to be always the best result. In
some cases the reranking model provides an
incorrect hypothesis while the baseline best
hypothesis is correct, especially when base-
line models are accurate. In this paper we
propose solutions for these two aspects: (i)
a semantic inconsistency metric to select pos-
sibly more correct n-best hypotheses, from a
large set generated by an SLU basiline model.
The selected hypotheses are reranked apply-
ing a state-of-the-art model based on Partial
Tree Kernels, which encode SLU hypothe-
ses in Support Vector Machines with com-
plex structured features; (ii) finally, we apply
a decision strategy, based on confidence val-
ues, to select the final hypothesis between the
first ranked hypothesis provided by the base-
line SLU model and the first ranked hypothe-
sis provided by the re-ranker. We show the ef-
fectiveness of these solutions presenting com-
parative results obtained reranking hypothe-
ses generated by a very accurate Conditional
Random Field model. We evaluate our ap-
proach on the French MEDIA corpus. The re-
sults show significant improvements with re-
spect to current state-of-the-art and previous
re-ranking models.
1 Introduction
Discriminative reranking is a widely used approach
for several Natural Language Processing (NLP)
tasks: Syntactic Parsing (Collins, 2000), Named En-
tity Recognition (Collins, 2000; Collins and Duffy,
2001), Semantic Role Labelling (Moschitti et al,
2008), Machine Translation (Shen et al, 2004),
Question Answering (Moschitti et al, 2007). Re-
cently reranking approaches have been successfully
applied also to Spoken Language Understanding
(SLU) (Dinarelli et al, 2009b).
Discriminative Reranking combines two models:
a first SLU model is used to generate a ranked list
of n-best hypotheses; a reranking model sorts the
list based on a different score and the final result
is the new top ranked hypothesis. The advantage of
reranking approaches is in the possibility to learn di-
rectly complex dependencies in the output domain,
as this is provided in the hypotheses generated by
the baseline model.
In previous approaches complex features are ex-
tracted from the hypotheses for both training and
classification phase, but there are very few stud-
ies on approaches that can be applied to search in
the hypotheses space generated by the baseline SLU
model. Moreover, to keep overall computational
cost reasonable, the size of the n-best list is typically
small (few tens). This is a limitation since the larger
is the hypotheses space generated, the more likely is
to find a better hypothesis. On the other hand, re-
ranking a large set of hypotheses is computationally
1104
expensive, thus a strategy to select the best hypothe-
ses to be re-ranked would overcome this problem.
Another aspect of reranking that deserves to be
deeper studied is its applicability. Although a
reranking model improves the baseline model in the
overall performance, in some cases the reranked best
hypotheses can contain more mistakes than the base-
line best hypothesis. A strategy to decide when the
reranking model should be applied and when the
first hypothesis of the baseline model is more accu-
rate would improve reranking performances.
In this paper, we propose two new models for
improving discriminative reranking: (a) a seman-
tic inconsistency metric that can be applied to SLU
hypotheses to select those that are more likely to
be correct; (b) a model selection strategy based on
the confidence scores provided by the baseline SLU
model and the reranker. This provides a decision
function that detects if the original top ranked hy-
pothesis is more accurate than the reranked best hy-
pothesis.
Our re-ranking strategies turn out to be effective
on very accurate baseline models based on state-of-
the-art Conditinal Random Fields (CRF) implemen-
tation (Lavergne et al, 2010). We evaluate our ap-
proach on the well-known French MEDIA corpus
for SLU (Bonneau-Maynard et al, 2006). The re-
sults show that our approach significantly improves
both ?traditional? reranking approaches and state-
of-the-art SLU models.
The remainder of the paper is organized as fol-
lows: in Section 2 we introduce the SLU task. Sec-
tion 3 describes our discriminative reranking frame-
work for SLU, in particular the baseline model
adopted, in sub-section 3.1, and the reranking
model, in sub-section 3.2. Section 4 describes
the two strategies proposed in this paper for SLU
reranking, whereas the experiments to evaluate our
approaches are described in Section 5. Finally, after
a discussion in Section 6, in Section 7 we draw some
conclusions.
2 Spoken Language Understanding
Spoken Language Understanding is the task of rep-
resenting and extracting the meaning of natural lan-
guage sentences. Designing a general meaning rep-
resentation which can capture the semantics of a
spoken language is very complex. Therefore, in
practice, the meaning representations depend on the
specific application domain being modeled.
For the corpus used in this work, the semantic rep-
resentation is defined in an ontology described in
(Bonneau-Maynard et al, 2006). As an example,
given the following natural language sentence trans-
lated from the MEDIA corpus:
?Good morning I would like to book an hotel room in Lon-
don?
The semantic representation extraction for the
SLU task is performed in two steps:
1. Automatic Concept Labeling
Null{Good morning} command-task{I would like to book}
object-bd{an hotel room} localization-city{in London}
2. Attribute-Value Extraction
command-task[reservation] object-bd[hotel] localization-
city[London]
command-task, object-bd and localization-city
are three domain concepts, called also ?attributes?,
defined in the ontology and Null is the concept for
words not associated to any concept. As shown in
the example, Null concepts are removed from the fi-
nal output since they don?t bring any semantic con-
tent with respect to the application domain. reserva-
tion, hotel and London are the normalized attribute
values, defined also in the application ontology. This
representation is usually called attribute-value repre-
sentation.
In the last decade several probabilistic models
have been proposed for the Automatic Concept La-
beling step: in (Raymond et al, 2006) a conceptual
language model encoded in Stochastic Finite State
Transducers (SFST) is proposed. In (Raymond and
Riccardi, 2007), the SFST-based model is compared
with Support Vector Machines (SVM) (Vapnik,
1998) and Conditional Random Fields (CRF) (Laf-
ferty et al, 2001). Moreover, in (Hahn et al, 2008a)
two more models are applied to SLU: a Maximum
Entropy (EM) model and a model coming from
the Statistical Machine Translation (SMT) commu-
nity (it is actually a log-linear combination of SMT
models). Among these models, CRF has shown in
general superior performances on sequence labeling
tasks like Named Entity Recognition (NER) (Tjong
Kim Sang and De Meulder, 2003), Grapheme-to-
Phoneme transcription (Sejnowski and Rosenberg,
1105
1987) and also Spoken Language Understanding
(Hahn et al, 2008a).
In addition to individual systems, more recently
also some system combination approaches have
been tried on SLU. In (Hahn et al, 2010), two such
approaches are compared, one based on weighted
ROVER (Fiscus, 1997) while the other is the rerank-
ing approach proposed in (Dinarelli et al, 2009b).
Both system combination approaches are applied on
the MEDIA corpus, thus we will refer to (Hahn et
al., 2010) for a comparison with our approach.
Like the other tasks mentioned above, SLU is usu-
ally a supervised learning task, this means that mod-
els are learned from annotated data. This is an im-
portant aspect to take into account when designing
SLU systems. In this respect accurate SLU models
can in part alleviate the problem of manually anno-
tating data.
The second step of SLU, that is Attribute Value
Extraction (from now on AVE) is performed with
two approaches: a) Rule-based approaches apply
Regular Expressions (RE) to map the words realiz-
ing a concept into a normalized value. Regular ex-
pressions are defined for each attribute-value pair.
Given a concept and its realizing surface form, if a
RE for that concept matches the surface, the corre-
sponding value is returned.
An example of surfaces that can be mapped into
the value hotel given the concept object-bd is:
1. an hotel room
2. a hotel room
3. the hotel
...
Note that these surfaces share the same keyword
for the concept object-bd, which is ?hotel?. Thus,
a possible rule extracted from data, for the concept
object-bd can be:
Robject?bd(S) =
if S = ?an hotel room? or
S = ?a hotel room? or
S = ?the hotel? then
return ?hotel?
end
This kind of rules can be easily refined using reg-
ular expressions, so that they can capture all possible
linguistic patterns containing the triggering keyword
(?hotel? in the example).
b) The other approach used for attribute value ex-
traction is based on probabilistic models. In this case
the model learns from data the conditional probabil-
ity of values V , given the concept C and the cor-
responding sequence of words W realizing the con-
cept: P (V |W,C).
The most meaningful work about AVE ap-
proaches in SLU tasks is (Hahn et al, 2010).
The model used in this work for Automatic Con-
cept Labeling is based on CRF. For the Attribute-
Value Extraction phase we use a combination of
rule based and probabilistic approaches. The first
is made of regular expressions, as explained above.
The probabilistic approach is based again on CRF.
3 Reranking Framework
This section describes the different models involved
in the pipeline realising our reranking framework:
? Conditional Random Fields
? Semantic Inconsistency Metric for hypotheses
selection, which is optional and is applied only
at the classification phase
? Support Vector Machines with Partial Tree Ker-
nel
? Decision Strategy to detect when the top ranked
hypothesis of the baseline model is more accu-
rate than the reranked best hypothesis
It is important to underline that the phases in-
volved in the reranking framewrok are distinguished
for a matter of clarity. In principle, the phases
from the hypotheses selection to the last, the deci-
sion strategy, can be thought of as a whole reranking
model.
In the next two subsection we describe the two
models used for hypotheses generation and for
reranking: CRF and SVM with kernel methods. The
two improvements proposed in this paper and listed
above are presented in a dedicated section (4).
3.1 Conditional Random Fields
CRFs have been proposed for the first time for se-
quence segmentation and labeling tasks in (Lafferty
et al, 2001). This model belongs to the family of
exponential or log-linear models. Its main charac-
teristics are the possibility to include a huge number
1106
of features, like the Maximum Entropy (ME) model,
but computing global conditional probabilities nor-
malized at sentence level, instead of position level
like in ME. In particular this last point results very
effective since it solves the label bias problem, as
pointed out in (Lafferty et al, 2001).
Given a sequence of N words WN1 = w1, ..., wN
and its corresponding sequence of concepts CN1 =
c1, ..., cN , CRF trains the conditional probabilities
P (CN1 |WN1 ) =
1
Z
N?
n=1
exp
( M?
m=1
?m ? hm(cn?1, cn, wn+2n?2)
)
(1)
where ?m are the training parameters.
hm(cn?1, cn, wn+2n?2) are the feature functions
capturing conditional dependencies of concepts and
words. Z is a probability normalization factor in
order to model well defined probability distribution:
Z =
?
c?N1
N?
n=1
H(c?n?1, c?n, wn+2n?2) (2)
here c?n?1 and c?n are the concepts hypoth-
esized for the previous and current words,
H(c?n?1, c?n, wn+2n?2) is an abbreviation for?M
m=1 ?m ? hm(cn?1, cn, wn+2n?2).
The CRF model used for the Attribute-Value Ex-
traction phase learns in the same way the conditional
probability P (V N1 |CN1 ,WN1 ). In particular we use
attributes-words concatenations on the source side
and attribute values on the target side.
Two particular effective implementations of CRFs
have been recently proposed. One is described in
(Hahn et al, 2009) and uses a margin based criterion
for probabilities estimation. The other is described
in (Lavergne et al, 2010) and has been implemented
in the software wapiti1. The latter solution in partic-
ular trains the model using two different regulariza-
tion factors at the same time:
Gaussian prior, used as l2 regularization and used
in many softwares to avoid overfitting;
Laplacian prior, used as l1 regularization (Riezler
and Vasserman, 2010), which has the effect to filter
out features with very low scores.
1available at http://wapiti.limsi.fr
The two regularization parameters are used to-
gether in the model implementing the so-called elas-
tic net regularization (Zou and Hastie, 2005):
l(?) + ?1???1 +
?2
2 ???
2
2 (3)
? is the set of parameters of the model introduced
in equation 1, l(?) is the minus-logarithm of equa-
tion 1, used as loss function for training CRF. ???1
and ???2 are the l1 and l2 regularization, respec-
tively, while ?1 and ?2 are two parameters that can
be optimized as usual on development data or with
cross validation.
As explained in (Lavergne et al, 2010), using l1
regularization is an effective way for feature selec-
tion in CRF at training time. Note that other ap-
proaches have been proposed for feature selection,
e.g. in (McCallum, 2003). This type of features se-
lection, performed directly at training time, yields
very accurate models, since only the most meaning-
ful features are kept in the final model, which guar-
antee a strong robustness on unseen data.
In this work we refer in particular to the CRF im-
plementation described in (Lavergne et al, 2010).
3.2 SVM and Kernel Methods
Our reranking model is based on SVM (Vapnik,
1998) with the use of the Partial Tree Kernel defined
in (Moschitti, 2006).
SVMs are well-known machine learning algo-
rithms belonging to the class of maximal-margin lin-
ear classifiers (Vapnik, 1998). The model represents
a hyperplane which separates the training examples
with a maximum margin. The hyperplane is learned
using optimization theory and is represented in the
dual form as a linear combination of training exam-
ples:?
i=1..l yi?i ~xi~x + b = 0,
where ~xi, i ? [1, .., l] are training examples rep-
resenting objects oi and o in any feature space, yi is
the label associated with ~xi and ?i are the lagrange
multipliers. The dual form of the hyperplane shows
that SVM training depends on the inner product be-
tween instances. Kernel methods theory (Shawe-
Taylor and Cristianini, 2004), allows us to substitute
the inner product with a so-called kernel function,
computing the same result: K(oi, o) = ~xi ? ~x.
1107
The interesting aspect of using such formulation
is the possibility to compare objects in arbitrar-
ily complex feature spaces implicitly, i.e. without
knowing the features to be used. Since in real world
scenarios data cannot be classified using a simple
linear classifier, kernel methods can be used to carry
out learning in complex feature spaces. In this work
we use the Partial Tree Kernel (PTK) (Moschitti,
2006).
3.3 Reranking Model
In order to give an effective representation to SLU
hypotheses in SVM, since we are using PTK, we
need to represent as trees SLU hypotheses like the
one described in section 2.
This problem is easily solved by transforming the
hypotheses into trees like the one depicted in fig-
ure 1. Although there may be more formal solutions
to represent semantic information of SLU hypothe-
ses as trees, we would like to remark that the tree
structure shown in figure 1 contains all the key in-
formation needed for our purposes: the first level of
the tree represents the concept sequence annotated
on surface form. The second level of the tree al-
low to implicitly represent the segmentation of each
concept, while the third level, i.e. the leaves, are the
input words. Moreover, from figure 1 we removed
word categories associated to words in order to keep
the figure readable. Word categories are provided
together with the corpus as an application knowl-
edge base. They comprise domain categories like
city names, hotel names, street names etc., and some
domain independent categories like numbers, dates,
months etc. The categories are used at the same level
of words, they provide a generalization over words
and alleviate the effect of Out-of-Vocabulary (OOV)
words.
The CRF model used as baseline generates the
n most likely conceptual annotations for each input
sentence. These are ranked by the global conditional
probability of the concept sequence, given the input
word sequence of CRF. The n-best list produced by
the baseline model is the list of candidate hypotheses
H1, H2, .., Hn used in the reranking step.
The candidate hypotheses are organized into
pairs, e.g. (H1, H2) or (H1, H3). We build train-
ing pairs such that a reranker can learn to select the
best one between the two hypotheses in a pair, i.e.
the more correct hypothesis with respect to a refer-
ence annotation and a given metric. In particular,
we compute the edit distance of each hypothesis in
the list, with respect to the manual annotation taken
from the corpus. The best hypothesis Hb is used
to build positive instances for the reranker as pairs
(Hb, Hi) for i ? [1..n] and i 6= b, negative instances
are built as (Hi, Hb), with same constraints on index
i. This means that, if n hypotheses are generated for
a sentence, 2 ? n instances are generated from them.
Note that by construction of pairs the model is sym-
metric, this provides a property that will be exploited
at classification phase, as described in (Shen et al,
2003b).
Hypotheses are then converted into trees like the
one shown in figure 1. Pairs of trees ek = (ti,k, tj,k),
for k varying along all the training or classification
instances, are given as input to the SVM model to
train the reranker using the following reranking ker-
nel:
KR(e1, e2) = PTK(t1,1, t1,2) + PTK(t2,1, t2,2) (4)
? PTK(t1,1, t2,2)? PTK(t2,1, t1,2),
where e1 and e2 are two pairs of trees to be com-
pared.
The reranking kernel in equation 4, consisting in
summing four different kernels, has been proposed
in (Shen et al, 2003b) for syntactic parsing rerank-
ing, where the basic kernel was a Tree Kernel, and
the idea was taken in turn from (Heibrich et al,
2000), where pairs where used to learn preference
ranking. The same idea appears also, in a slightly
different form, in early work about reranking, e.g.
(Collins and Duffy, 2002). The same reranking
schema has been used also in (Shen et al, 2004)
for reranking different candidate hypotheses for ma-
chine translation.
For classification, observing that the model is
symmetric and exploiting kernel properties, we can
use, as classification instances, simple hypotheses
instead of pairs. More precisely we use pairs where
the second hypothesis is empty, i.e. (Hi, 0), for
i ? [1..n]. This simplification allow a relatively fast
classification phase, since only n instances are gen-
erated for each sentence, instead of n2. This simpli-
fication has been proposed in (Shen et al, 2003b).
1108
Figure 1: An example of semantic tree constructed from an SLU hypothesis from the MEDIA corpus and used in PTK
4 Hypotheses Selection Criteria
This section describes the main contribution of our
work: first, a semantic inconsistency metric based
on the AVE phase of SLU and allowing to select hy-
potheses generated by the baseline model; second, a
strategy to decide, after the reranking phase, if it is
more likely that the baseline best hypothesis is more
accurate than the best reranked hypothesis and al-
lowing to recover the mistake. Similar ideas have
been proposed in (Dinarelli et al, 2010), here we
propose a significant evolution and we give a much
wider description and evaluation.
4.1 Hypotheses Selection via Attribute Value
Extraction (AVE)
In previous reranking approaches (Collins, 2000;
Collins and Duffy, 2002; Shen et al, 2003a; Shen
et al, 2003b; Shen et al, 2004; Collins and Koo,
2005; Kudo et al, 2005; Dinarelli et al, 2009b), few
hypotheses are generated with the baseline model,
ranked by the model probability. These are then
used for the reranking model. An interesting strat-
egy to improve reranking performance is the selec-
tion of the best set of hypotheses to be reranked.
In this work we propose a semantic inconsistency
metric (SIM) based on the attribute-value extraction
phase that allows to select better n-best hypotheses.
We combine the scores provided by the rule based
approach and the CRF approach for AVE, comput-
ing a confidence measure.
The rule-based approach for AVE is defined by
a set of rules that map concepts and their realiz-
ing words into the corresponding value. The rules
are extracted from the training data, thus they are
defined to extract correct values from well formed
phrases annotated with correct concepts. This means
that when the corresponding words are annotated
with a wrong concept, the extracted value will prob-
ably be wrong. We use this property to compute a
semantic inconsistency value for hypotheses, which
in turn allows to select hypotheses with higher prob-
abilities to be correct.
We show the application of SIM using the same
example of Section 2. For space issues we ab-
breviate command-task with com-task, object-bd
with obj-bd and localization-city with loc-city. We
also suppose to have already removedNull concepts.
From the same sentence, the three first hypotheses
that may be generated by the baseline model are:
1. obj-bd{I would like to book} obj-bd{an hotel room} loc-
city{in London}
2. com-task{I would like to book} obj-bd{an hotel room} loc-
city{in London}
3. com-task{I would like to book} obj-bd{an hotel} obj-
bd{room} loc-city{in London}
Two of these annotations show typical errors of an
SLU model:
(i) wrong concepts annotation: in the first hypothe-
sis the phrase ?I would like to book? is erroneously
annotated as obj-bd;
(ii) wrong concept segmentation: in the third hy-
pothesis the phrase ?an hotel room? is split in two
concepts.
If we apply the AVE module to these hypotheses
the result is:
1. obj-bd[] obj-bd[hotel] loc-city[london]
2. cmd-task[reservation] obj-bd[hotel] loc-city[london]
3. cmd-task[reservation] obj-bd[hotel] obj-bd[] loc-city[london]
As we can see the first concept obj-bd in the first
hypothesis has an empty value since it was incor-
rectly annotated and, therefore, it is not supported
1109
MEDIA training dev test
# sentences 12,908 1,259 3,005
words concepts words concepts words concepts
# tokens 94,466 43,078 10,849 4,705 25,606 11,383
# vocabulary 2,210 99 838 66 1,276 78
# singletons 798 16 338 4 494 10
# OOV rate [%] ? ? 1.33 0.02 1.39 0.04
Table 1: Statistics of the MEDIA training and evaluation sets used for all experiments.
by words from which the AVE module can extract
a correct value. In this case, the output of AVE is
empty. In the same way, in the third hypothesis, the
AVE module cannot extract a correct value from the
phrase ?room? since it doesn?t contain any keyword
for a obj-bd concept.
For each hypothesis, our SIM simply counts the
number of wrong (or empty) values. In the example
above, we have 1, 0 and 1 for the three hypothe-
sis, respectively. Accordingly, the most accurate hy-
pothesis under SIM is the second, which is also the
correct one.
In order to combine the SIM score computed by
the rule-based AVE module with the score provided
by the CRF AVE model, we consider per-concept
scores from both approaches. In particular, we for-
malize the definition of the SIM metric above on a
concept ci as SIM(ci, w1,...,mi ). The value of SIM
is simply 0 if the rule-based AVE module can extract
a value from the surface form w1,...,mi realizing the
concept ci. 1 otherwise. For each concept in a hy-
pothesis, we compute its semantic consistency s(ci)
as
s(ci) =
P (vi|ci, w1,...,mi )
SIM(ci, w1,...,mi ) + 1
(5)
where P (vi|ci, w1,...,mi ) is the conditional prob-
ability output by the CRF model for the value vi,
given the concept ci and its realizing surfacew1,...,mi .
Equation 5 means that the CRF score provided for a
given value is halved if SIM returns 1, i.e. if the
AVE module cannot extract any value. Otherwise
the score output by the CRF AVE model is kept
unchanged. The semantic inconsistency metric of
an hypothesis Hk containing the concept sequence
CN1 = c1, ..., cN is then defined as
S(Hk) =
N?
i=1
s(ci) (6)
Using S(Hk) as semantic inconsistency metric,
we generate a huge number of hypotheses with the
baseline model and we select only the top n-best. We
use these hypotheses in the discriminative reranking
model, instead of the original n-best generated by
the CRF model. For simplicity, in general context
we denote S(Hk) as SIM.
4.2 Wrong Rerank Rejection
After the reranking model is applied, the first hy-
pothesis is selected as final result. This choice as-
sumes that the new hypothesis is more accurate than
the one provided by the baseline model. In gen-
eral this assumption is not true. Indeed, a reranking
model must be carefully tuned in order to correctly
rerank wrong first best hypotheses but keeping the
original baseline best for correct hypotheses. When
the baseline model is relatively accurate, the latter
case occurs in most of the cases. In this situation it
becomes hard to train an accurate reranking model.
Our idea to overcome this problem is to apply the
reranking model and then post-process results to de-
tect when the original best hypothesis is actually bet-
ter than the reranked best.
For this purpose we propose a simple strategy
based on the scores computed by the two models in-
volved in reranking: CRF for the baseline and SVM
with PTK for reranking.
Let Hcrf and HRR be the best hypothesis of the
CRF and reranking (RR) models, respectively. Let
Scrf (Hcrf ) and Scrf (HRR) be the scores of the
CRF model for Hcrf and HRR. In the same way,
let SRR(Hcrf ) and SRR(HRR) be the scores of the
reranking model on the same hypotheses. We define
the confidence margin of the CRF model the quan-
tity: Mcrf = Scrf (Hcrf )? Scrf (HRR).
In the same way we define the confidence mar-
gin of the RR model: MRR = SRR(Hcrf ) ?
SRR(HRR).
We compute two thresholds Tcrf and TRR for the
1110
Average score Feature type
0.0528186 Pref2
0.044189 CATEGORY-2
0.0355579 CATEGORY
0.0354006 Pref3-2
0.0338949 Pref4-2
0.0332647 Suff3-2
0.0314831 Suff2
0.030613 Suff4-2
... ...
0.0165602 Suff1
0.000579602 Pref1
Table 2: Ranks of average score given by the CRF model to feature
types
two margins with respect to error rate minimization
(with a ?line search? algorithm).
We select the final best interpretation hypothesis
for a given sentence with the decision function:
BestHypothesis =
{ HRR if Mcrf ? Tcrf and MRR ? TRR
Hcrf otherwise.
Since this strategy allows to recover from rerank-
ing mistakes, we call it Wrong Rerank Rejection
(WRR).
5 Experiments
The data used in our experiments are taken from
the French MEDIA corpus (Bonneau-Maynard et
al., 2006). The corpus is made of 1.250 Human-
Machine dialogs acquired with a Wizard-of-Oz ap-
proach in the domain of informtation and reservation
of French hotels. The data are split into training, de-
velopment and test set. Statistics of the corpus are
presented in table 1.
For our CRFmodels, both Automatic Concept An-
notation and Attribute Value Extraction SLU phases,
we used wapiti2 (Lavergne et al, 2010). The CRF
model for the first SLU phase integrates a tradi-
tional set of features like word prefixes and suffixes
(of length up to 5), plus some Yes/No features like
?Does the word start with capital letter ??, ?Does
the word contain non alphanumeric characters ??,
?Is the word preceded by non alphanumeric char-
acteris ?? etc. The CRF model for AVE integrates
only words, prefixes and suffixes (length 3 and 4)
concatenated with concepts. Since in this case la-
bels are attribute values, which are a huge set with
2available at http://wapiti.limsi.fr
MEDIA Text Input DEV TEST
Model Attr Attr+Val Attr Attr+Val
CRF 12.1% 14.8% 11.5% 13.8%
CRF+RR 12.0% 14.6% 11.5% 13.7%
CRF+RRSIM 11.7% 13.9% 11.3% 13.4%
CRF+RRWRR 11.2% 13.4% 11.3% 13.0%
Table 3: Results of baseline CRF model and reranking models on
MEDIA text input
respect to concepts (7?00 VS 99), using a lot of fea-
tures would make model training problematic. De-
spite the reduced set of features, training error rate
at both token and sentence level is under 1%. We
didn?t carry out optimization for parameters ?1 and
?2 of the elastic net (see section 3.1), default values
lead in most cases to very accurate models.
Reranking models based on SVM and PTK have
been trained with ?SVM-Light-TK?3. Kernel param-
eters M and SVM parameter C have been optimized
on the development set, as well as thresholds for the
WRR (see section 4.2).
Concerning hypotheses generation, for training
we generate 100 hypotheses, we select the best with
respect to the edit distance and the reference anno-
tation and we keep a total of 10 hypotheses to build
pairs. For classification, with the ?standard? rerank-
ing approach we generate and we keep the 10 best
hypotheses. While using SIM for hypotheses selec-
tion, we generate 1.000 hypotheses and we keep the
10 best with respect to SIM. 1.000 is the best thresh-
old between oracle accuracy and computational cost
for evaluating the hypotheses.
Experiments have been performed on both man-
ual and automatic transcriptions of dialog turns. For
automatic transcriptions the WER of the ASR is
30.3% on development set and 31.4% on test set.
All results are reported in terms of Concept Er-
ror Rate (CER), which is the same as WER, but it is
computed on concept sequences. In all cases we give
results for both attributes only and attributes and val-
ues extraction
5.1 Results
In order to understand feature relevance, in table 2
we report feature types ranked by the average score
given by the CRF model. Each type correspond to
features at any position with respect to the target
3available at http://disi.unitn.it/moschitti/Tree-Kernel.htm
1111
(a) M kernel parameter VS CER (on
attribute-value extraction)
(b) C SVM parameter VS training time (c) C SVM parameter VS CER (on
attribute-value extraction)
Figure 2: Optimization of the PTK M parameter and C parameter of SVM
MEDIA Speech Input DEV TEST
Model Attr Attr+Val Attr Attr+Val
CRF 24.1% 29.1% 23.7% 27.6%
CRF+RR 23.9% 29.1% 23.5% 27.6%
CRF+RRSIM 23.9% 28.3% 23.2% 26.8%
CRF+RRWRR 23.3% 27.5% 22.7% 26.1%
Table 4: Results of baseline CRF model and reranking models on
MEDIA speech input
word, with label unigrams. In contrast observation
unigrams are distinguished from bigrams using suf-
fixes -1 and -2 respectively. Feature types wrd are
words converted to lower case, Wrd are words kept
with original capitalization. Feature types Pren are
word prefixes of length n, Sufn are word suffixes of
length n. CATEGORY features are word categories
(see section 3.3). As we can see from the table,
although feature relevance depends of course from
the task, surprisingly word prefixes of length 2 are
the most meaningful features. As expected, CATE-
GORY features are also very relevant features, since
they provide a strong generalization over words. An-
other expected outcome is the fact that prefixes and
suffixes of length 1 are the least relevant features.
In figure 2(a), 2(b) and 2(c) we show the curves
resulting from optimization of parameters of rerank-
ing models. In particular we optimized the M kernel
parameter (? decay factor, see (Moschitti, 2006) for
details), and the C SVM parameter, i.e. the scale
factor for the soft margin (please refer to (Vapnik,
1998) for SVM details). Figure 2(b) shows the learn-
ing time as a function of the C SVM parameter. This
gives an idea of how long takes training our rerank-
ing models.
In table 3 and 4 we report comparative results
over the baseline CRF model, the baseline rerank-
ing model (CRF+RR) and the reranking models ob-
tained applying the two improvements proposed in
this work (CRF+RRSIM and CRF+RRWRR). As
we can see, the baseline reranking model does not
improve significantly the baseline CRF model. This
outcome is expected since we don?t use any other in-
formation in the reranking model than the semantic
tree shown in figure 1. Previous approaches like for
example (Collins and Duffy, 2002), use the baseline
model score as feature, as that the reranking model
cannot do worst than the baseline model. As we
pointed out in section 4.2, this solution require a fine
tuning of the reranking model, especially when the
baseline model is relatively accurate. In our case,
the CRF model has a Sentence Error Rate of 25.0%
on the MEDIA test set. This means that 75% of
the times the best hypothesis of CRF is correct. In
turn this implies that the reranking model must not
rerank 75% of times and rerank the other 25% of
times, someway contrasting the evidence provided
by the baseline model score. In contrast, using our
WRR strategy, we can tune the reranking model to
maximize reranking effect and recover from rerank-
ing errors applying WRR. As shown in tables 3 and
4, we consistently improve CRF baseline as well
as reranking baseline CRF+RR, especially applying
both SIM and WRR (CRF+RRWRR). Comparing
our results with those reported in (Hahn et al, 2010),
we can see that our model reaches, and even im-
1112
MEDIA Test set OER[%] correct found/present
Model
CRF 9.5 2359/2657
CRF+RR 9.5 2375/2657
CRF+RRSIM 7.5 2381/2758
CRF+RRWRR 7.5 2444/2758
Table 5: Analysis over 10-best hypotheses for CRF baseline and the
reranking models showing the effect of hypotheses selection
MEDIA Text Input DEV TEST
Model Pair Attr+Val Attr+Val
CRF vs. CRF+RR 0.2235 0.4075
CRF vs. CRF+RRSIM 0.0299 0.065
CRF vs. CRF+RRWRR 0.0044 1.9998E-4
CRF+RR vs. CRF+RRSIM 0.002 5.9994E-4
CRF+RR vs. CRF+RRWRR 4.9995E-4 9.999E-5
CRF+RRSIM vs. CRF+RRWRR 0.1355 0.0031
Table 6: Significance tests on results of models described in this
work. The significance test is based on computationally-intensive ran-
domizations as described in (Yeh and Church, 2000).
proves in some cases, state-of-the-art performance.
This is particularly meaningful since best results re-
ported in (Hahn et al, 2010) are obtained combining
6 different SLU models.
In table 5 we report some statistics to show the
effect of SIM on the 10-best hypotheses list. It is
particularly interesting to see that when hypothe-
ses selection is applied, oracle error rate (OER)
drops of 2% points from an already accurate OER
of 9.5%. This is reflected also by the number of ora-
cles present in the 10-best list without applying and
applying SIM. We pass from 2657 without SIM to
2758 applying our hypotheses selection metric.
Finally, in table 6 we report statistical signifi-
cance tests over the models described in this work.
We used the significance test described in (Yeh
and Church, 2000), it is based on computationally-
intensive randomizations of data and tests the null
hypothesis, i.e. the lower the score, the higher the
statistical significance of results difference. Scores
in table 5 reflect results given in terms of CER. We
can see that when the difference between results is
small, this is not statistically significant, when the
score is above 0.05, the difference between the two
corresponding models is not significant. We can thus
conclude that the reranking model we propose, using
hypotheses selection and reranking errors recover,
significantly improves baseline CRFmodel and ?tra-
ditional? reranking models.
6 Discussion
Although the new ideas proposed in this paper are
effective and interesting, an important issue is their
applicability to other tasks and domains. In this re-
spect, it is sufficient to note that our ideas comes
from the multi-stage nature of the task and of the
proposed reranking framework. SLU is performed
in two intertwined steps, since attribute values are
extracted from syntactic chunks annotated with con-
cept in the first step. This allows to use the model for
the second step to validate the output of the first step,
and vice versa, which is the principle of our hypothe-
ses selection metric. There are many other tasks,
in NLP and in other domains, that can be modeled
with multiple steps and thus the same idea of ?val-
idation? of the output of one step with the other?s
model output can be applied. An example is syntac-
tic parsing, where in most cases parsing is performed
upon POS tagging output.
7 Conclusions
In this paper we propose two improvements for
reranking models to be integrated in a reranking
framework for Spoken Language Understanding.
The reranking model is based on a CRF baseline
model and Support Vector Machines with the Par-
tial Tree Kernel for the reraning model. The two
improvements we propose are: i) hypotheses selec-
tion criteria, used before applying reranking to select
better hypotheses amongst those generated by CRF.
ii) a strategy to recover from reranking errors called
Wrong Rerank Rejection.
We presented a full set of comparative results
showing the viability of our approach. We can reach
performances of state-of-the-art models, improving
them in some cases, especially on automatic tran-
scriptions coming from ASR (speech input).
In particular, the effectiveness of hypotheses se-
lection is shown reporting the improvement of the
Oracle Error Rate on the 10-best hypotheses list.
Acknowledgments
This work has been funded by OSEO under the
Quaero program.
1113
References
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, pages 142?147,
Morristown, NJ, USA. Association for Computational
Linguistics.
Ellen M. Voorhees. 2001. The trec question answering
track. Nat. Lang. Eng., 7:361?378, December.
X. Carreras and Lluis Marquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling.
R. De Mori, F. Bechet, D. Hakkani-Tur, M. McTear,
G. Riccardi, and G. Tur. 2008. Spoken language un-
derstanding: A survey. IEEE Signal Processing Mag-
azine, 25:50?58.
Sylvain Galliano, Guillaume Gravier, and Maura
Chaubard. 2009. The ester 2 evaluation campaign
for the rich transcription of french radio broadcasts.
In Proceedings of the International Conference of
the Speech Communication Assosiation (Interspeech),
Brighton, U.K.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of the Conference of the Association for
Computational Linguistics (ACL), page 363370, Ann
Arbor, MI.
Michael Collins and Terry Koo. 2005. Discriminative re-
ranking for natural language parsing. Computational
Linguistic (CL), 31(1):25?70.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning (ICML), pages 282?289, Williamstown,
MA, USA, June.
Brigitte Krenn and Christer Samuelsson. 1997. The lin-
guist?s guide to statistics - don?t panic.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513. As-
sociation for Computational Linguistics, July.
Stefan Hahn, Patrick Lehnen, Georg Heigold, and Her-
mann Ney. 2009. Optimizing crfs for slu tasks in vari-
ous languages using modified training criteria. In Pro-
ceedings of the International Conference of the Speech
Communication Assosiation (Interspeech), Brighton,
U.K.
Stefan Riezler and Alexander Vasserman. 2010. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the Elastic Net. Journal of the
Royal Statistical Society B, 67:301?320.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields. In 19th Conference on
Uncertainty in Artificial Intelligence.
Lawrence R. Rabiner. 1989. A tutorial on hidden markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257?286.
Mark Johnson. 1998. Pcfg models of linguistic tree rep-
resentations. Computational Linguistics, 24:613?632.
Olivier Galibert, Ludovic Quintard, Sophie Rosset, Pierre
Zweigenbaum, Claire Ndellec, Sophie Aubin, Lau-
rent Gillard, Jean-Pierre Raysz, Delphine Pois, Xavier
Tannier, Louise Delger, and Dominique Laurent.
2010. Named and specific entity detection in var-
ied data: The quro named entity baseline evalu-
ation. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Olivier Galibert. 2009. Approches et me?thodologies pour
la re?ponse automatique a` des questions adapte?es un
cadre interactif en domaine ouvert. Ph.D. thesis, Uni-
versit Paris Sud, Orsay.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. The Automatic
Content Extraction (ACE) Program?Tasks, Data, and
Evaluation. Proceedings of LREC 2004, pages 837?
840.
He?le`ne Bonneau-Maynard, Christelle Ayache, F. Bechet,
A Denis, A Kuhn, Fabrice Lefe`vre, D. Mostefa,
M. Qugnard, S. Rosset, and J. Servan, S. Vilaneau.
2006. Results of the french evalda-media evaluation
campaign for literal understanding. In LREC, pages
2054?2059, Genoa, Italy, May.
Christian Raymond, Frdric Bchet, Renato De Mori, and
Graldine Damnati. 2006. On the use of finite state
transducers for semantic interpretation. Speech Com-
munication, 48(3-4):288?304, March-April.
Christian Raymond and Giuseppe Riccardi. 2007. Gen-
erative and discriminative algorithms for spoken lan-
guage understanding. In Proceedings of the Interna-
tional Conference of the Speech Communication As-
sosiation (Interspeech), pages 1605?1608, Antwerp,
Belgium, August.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons.
T. J. Sejnowski and C. S. Rosenberg. 1987. Parallel net-
works that learn to pronounce English text. Complex
Systems, 1:145?168.
1114
Stefan Hahn, Marco Dinarelli, Christian Raymond, Fab-
rice Lefe`vre, Patrick Lehen, Renato De Mori, Alessan-
dro Moschitti, Hermann Ney, and Giuseppe Riccardi.
2010. Comparing stochastic approaches to spoken
language understanding in multiple languages. IEEE
Transactions on Audio, Speech and Language Pro-
cessing (TASLP), 99.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2009b. Re-ranking models based on small
training data for spoken language understanding. In
Conference of Empirical Methods for Natural Lan-
guage Processing, pages 11?18, Singapore, August.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2010. Hypotheses Selection for Reranking
Semantic Annotation. In IEEE Workshop of Spoken
Language Technology (SLT), Berkeley, USA.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of ECML 2006, pages 318?329, Berlin,
Germany.
M. Collins and N. Duffy. 2002. New Ranking Algo-
rithms for Parsing and Tagging: Kernels over Discrete
structures, and the voted perceptron. In Proceedings of
the Association for Computational Linguistics, pages
263?270.
Libin Shen, Anoop Sarkar, and Aravind K. Joshi. 2003.
Using LTAG Based Features in Parse Reranking. In
Proceedings of EMNLP?06.
Herbrich, Ralf and Graepel, Thore and Obermayer,
Klaus. 2000. Large Margin Rank Boundaries for Or-
dinal Regression. In Advances in Large Margin Clas-
sifiers.
Libin Shen, and Aravind K. Joshi. 2003. An SVM Based
Voting Algorithm with Application to Parse Rerank-
ing. In Proceedings of CoNLL 2003.
Libin Shen, Anoop Sarkar, and Franz J. Och. 2004. Dis-
criminative reranking for machine translation. In HLT-
NAACL, pages 177?184.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features.
In Proceedings of ACL?05.
Stefan Hahn, Patrick Lehnen, and Hermann Ney. 2008a.
System combination for spoken language understand-
ing. In Proceedings of the International Conference of
the Speech Communication Assosiation (Interspeech),
pages 236?239, Brisbane, Australia.
J. G. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recogniser output voting er-
ror reduction (ROVER). In Proceedings 1997 IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU), pages 347?352, Santa Barbara,
CA, December.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In ICML, pages 175?182.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, pages 625?632.
MIT Press.
Rush, Alexander M. and Sontag, David and Collins,
Michael and Jaakkola, Tommi. 2010. On dual decom-
position and linear programming relaxations for nat-
ural language processing. In Empirical Methods for
Natural Language Processing (EMNLP). Cambridge,
Massachusetts, USA.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question/answer clas-
sification. In Proceedings of ACL?07, Prague, Czech
Republic.
Alexander Yeh and Kelmeth Church. 2000. More accu-
rate tests for the statistical significance of result differ-
ences.
1115
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 174?184,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Tree Representations in Probabilistic Models for Extended Named
Entities Detection
Marco Dinarelli
LIMSI-CNRS
Orsay, France
marcod@limsi.fr
Sophie Rosset
LIMSI-CNRS
Orsay, France
rosset@limsi.fr
Abstract
In this paper we deal with Named En-
tity Recognition (NER) on transcriptions of
French broadcast data. Two aspects make
the task more difficult with respect to previ-
ous NER tasks: i) named entities annotated
used in this work have a tree structure, thus
the task cannot be tackled as a sequence la-
belling task; ii) the data used are more noisy
than data used for previous NER tasks. We
approach the task in two steps, involving
Conditional Random Fields and Probabilis-
tic Context-Free Grammars, integrated in a
single parsing algorithm. We analyse the
effect of using several tree representations.
Our system outperforms the best system of
the evaluation campaign by a significant
margin.
1 Introduction
Named Entity Recognition is a traditinal task of
the Natural Language Processing domain. The
task aims at mapping words in a text into seman-
tic classes, such like persons, organizations or lo-
calizations. While at first the NER task was quite
simple, involving a limited number of classes (Gr-
ishman and Sundheim, 1996), along the years
the task complexity increased as more complex
class taxonomies were defined (Sekine and No-
bata, 2004). The interest in the task is related to
its use in complex frameworks for (semantic) con-
tent extraction, such like Relation Extraction ap-
plications (Doddington et al 2004).
This work presents research on a Named Entity
Recognition task defined with a new set of named
entities. The characteristic of such set is in that
named entities have a tree structure. As conce-
quence the task cannot be tackled as a sequence
labelling approach. Additionally, the use of noisy
data like transcriptions of French broadcast data,
makes the task very challenging for traditional
NLP solutions. To deal with such problems, we
adopt a two-steps approach, the first being real-
ized with Conditional Random Fields (CRF) (Laf-
ferty et al 2001), the second with a Probabilistic
Context-Free Grammar (PCFG) (Johnson, 1998).
The motivations behind that are:
? Since the named entities have a tree struc-
ture, it is reasonable to use a solution com-
ing from syntactic parsing. However pre-
liminary experiments using such approaches
gave poor results.
? Despite the tree-structure of the entities,
trees are not as complex as syntactic trees,
thus, before designing an ad-hoc solution for
the task, which require a remarkable effort
and yet it doesn?t guarantee better perfor-
mances, we designed a solution providing
good results and which required a limited de-
velopment effort.
? Conditional Random Fields are models ro-
bust to noisy data, like automatic transcrip-
tions of ASR systems (Hahn et al 2010),
thus it is the best choice to deal with tran-
scriptions of broadcast data. Once words
have been annotated with basic entity con-
stituents, the tree structure of named entities
is simple enough to be reconstructed with
relatively simple model like PCFG (Johnson,
1998).
The two models are integrated in a single pars-
ing algorithm. We analyze the effect of the use of
174
Zahra
name.first
Abouch
name.last
pers.ind
Zahrnme .n faisnthn Anhb
omh.
mtuomnh
.nAahcA
atlpu.A
Figure 1: Examples of structured named entities annotated on the
data used in this work
several tree representations, which result in differ-
ent parsing models with different performances.
We provide a detailed evaluation of our mod-
els. Results can be compared with those obtained
in the evaluation campaign where the same data
were used. Our system outperforms the best sys-
tem of the evaluation campaign by a significant
margin.
The rest of the paper is structured as follows: in
the next section we introduce the extended named
entities used in this work, in section 3 we describe
our two-steps algorithm for parsing entity trees,
in section 4 we detail the second step of our ap-
proach based on syntactic parsing approaches, in
particular we describe the different tree represen-
tations used in this work to encode entity trees
in parsing models. In section 6 we describe and
comment experiments, and finally, in section 7,
we draw some conclusions.
2 Extended Named Entities
The most important aspect of the NER task we
investigated is provided by the tree structure of
named entities. Examples of such entities are
given in figure 1 and 2, where words have been re-
move for readability issues and are: (?90 persons
are still present at Atambua. It?s there that 3 employ-
ees of the High Conseil of United Nations for refugees
have been killed yesterday morning?):
90 personnes toujours pre?sentes a`
Atambua c? est la` qu? hier matin ont
e?te? tue?s 3 employe?s du haut commis-
sariat des Nations unies aux re?fugie?s ,
le HCR
Words realizing entities in figure 2 are in bold,
and they correspond to the tree leaves in the
picture. As we see in the figures, entities
can have complex structures. Beyond the use
of subtypes, like individual in person (to give
pers.ind), or administrative in organization
(to give org.adm), entities with more specific con-
tent can be constituents of more general enti-
ties to form tree structures, like name.first and
Zah rnme.f
airstf
hr.AabiAfrot taie f uiecirbuluep
fuieAbafeApeh
Zah dutb taie
rpAabi
lst.A.rhh
rnme.f
airstf

Figure 2: An example of named entity tree corresponding to en-
tities of a whole sentence. Tree leaves, corresponding to sentence
words have been removed to keep readability
Quaero training dev
# sentences 43,251 112
words entities words entities
# tokens 1,251,432 245,880 2,659 570
# vocabulary 39,631 134 891 30
# components ? 133662 ? 971
# components dict. ? 28 ? 18
# OOV rate [%] ? ? 17.15 0
Table 1: Statistics on the training and development sets of the
Quaero corpus
name.last for pers.ind or val (for value) and ob-
ject for amount.
These named entities have been annotated on
transcriptions of French broadcast news coming
from several radio channels. The transcriptions
constitute a corpus that has been split into train-
ing, development and evaluation sets.The evalu-
ation set, in particular, is composed of two set
of data, Broadcast News (BN in the table) and
Broadcast Conversations (BC in the table). The
evaluation of the models presented in this work
is performed on the merge of the two data types.
Some statistics of the corpus are reported in ta-
ble 1 and 2. This set of named entities has been
defined in order to provide more fine semantic in-
formation for entities found in the data, e.g. a
person is better specified by first and last name,
and is fully described in (Grouin, 2011) . In or-
der to avoid confusion, entities that can be associ-
ated directly to words, like name.first, name.last,
val and object, are called entity constituents, com-
ponents or entity pre-terminals (as they are pre-
terminals nodes in the trees). The other entities,
like pers.ind or amount, are called entities or non-
terminal entities, depending on the context.
3 Models Cascade for Extended Named
Entities
Since the task of Named Entity Recognition pre-
sented here cannot be modeled as sequence la-
belling and, as mentioned previously, an approach
175
Quaero test BN test BC
# sentences 1704 3933
words entities words entities
# tokens 32945 2762 69414 2769
# vocabulary 28 28
# components ? 4128 ? 4017
# components dict. ? 21 ? 20
# OOV rate [%] 3.63 0 3.84 0
Table 2: Statistics on the test set of the Quaero corpus, divided in
Broadcast News (BN) and Broadcast Conversations (BC)
Figure 3: Processing schema of the two-steps approach proposed
in this work: CRF plus PCFG
coming from syntactic parsing to perform named
entity annotation in ?one-shot? is not robust on
the data used in this work, we adopt a two-steps.
The first is designed to be robust to noisy data and
is used to annotate entity components, while the
second is used to parse complete entity trees and
is based on a relatively simple model. Since we
are dealing with noisy data, the hardest part of the
task is indeed to annotate components on words.
On the other hand, since entity trees are relatively
simple, at least much simpler than syntactic trees,
once entity components have been annotated in a
first step, for the second step, a complex model is
not required, which would also make the process-
ing slower. Taking all these issues into account,
the two steps of our system for tree-structured
named entity recognition are performed as fol-
lows:
1. A CRF model (Lafferty et al 2001) is used
to annotate components on words.
2. A PCFG model (Johnson, 1998) is used
to parse complete entity trees upon compo-
nents, i.e. using components annotated by
CRF as starting point.
This processing schema is depicted in figure 3.
Conditional Random Fields are described shortly
in the next subsection. PCFG models, constituting
the main part of this work together with the analy-
sis over tree representations, is described more in
details in the next sections.
3.1 Conditional Random Fields
CRFs are particularly suitable for sequence la-
belling tasks (Lafferty et al 2001). Beyond the
possibility to include a huge number of features
using the same framework as Maximum Entropy
models (Berger et al 1996), CRF models en-
code global conditional probabilities normalized
at sentence level.
Given a sequence of N words WN1 =
w1, ..., wN and its corresponding components se-
quence EN1 = e1, ..., eN , CRF trains the condi-
tional probabilities
P (EN1 |W
N
1 ) =
1
Z
NY
n=1
exp
 
MX
m=1
?m ? hm(en?1, en, w
n+2
n?2)
!
(1)
where ?m are the training parameters.
hm(en?1, en, w
n+2
n?2) are the feature functions
capturing dependencies of entities and words. Z
is the partition function:
Z =
X
e?N1
NY
n=1
H(e?n?1, e?n, w
n+2
n?2) (2)
which ensures that probabilities sum up to one.
e?n?1 and e?n are components for previous and cur-
rent words, H(e?n?1, e?n, w
n+2
n?2) is an abbreviation
for
?M
m=1 ?m ? hm(en?1, en, w
n+2
n?2), i.e. the set
of active feature functions at current position in
the sequence.
In the last few years different CRF implemen-
tations have been realized. The implementation
we refer in this work is the one described in
(Lavergne et al 2010), which optimize the fol-
lowing objective function:
?log(P (EN1 |W
N
1 )) + ?1???1 +
?2
2
???22 (3)
???1 and ???22 are the l1 and l2 regulariz-
ers (Riezler and Vasserman, 2004), and together
in a linear combination implement the elastic net
regularizer (Zou and Hastie, 2005). As mentioned
in (Lavergne et al 2010), this kind of regulariz-
ers are very effective for feature selection at train-
ing time, which is a very good point when dealing
with noisy data and big set of features.
176
4 Models for Parsing Trees
The models used in this work for parsing en-
tity trees refer to the models described in (John-
son, 1998), in (Charniak, 1997; Caraballo and
Charniak, 1997) and (Charniak et al 1998), and
which constitutes the basis of the maximum en-
tropy model for parsing described in (Charniak,
2000). A similar lexicalized model has been pro-
posed also by Collins (Collins, 1997). All these
models are based on a PCFG trained from data
and used in a chart parsing algorithm to find the
best parse for the given input. The PCFG model
of (Johnson, 1998) is made of rules of the form:
? Xi ? XjXk
? Xi ? w
where X are non-terminal entities and w are
terminal symbols (words in our case).1 The prob-
ability associated to these rules are:
pi?j,k =
P (Xi ? Xj , Xk)
P (Xi)
(4)
pi?w =
P (Xi ? w)
P (Xi)
(5)
The models described in (Charniak, 1997;
Caraballo and Charniak, 1997) encode probabil-
ities involving more information, such as head
words. In order to have a PCFG model made of
rules with their associated probabilities, we ex-
tract rules from the entity trees of our corpus. This
processing is straightforward, for example from
the tree depicted in figure 2, the following rules
are extracted:
S? amount loc.adm.town time.dat.rel amount
amount? val object
time.date.rel? name time-modifier
object? func.coll
func.coll? kind org.adm
org.adm? name
Using counts of these rules we then compute
maximum likelihood probabilities of the Right
Hand Side (RHS) of the rule given its Left Hand
Side (LHS). Also binarization of rules, applied to
1These rules are actually in Chomsky Normal Form, i.e.
unary or binary rules only. A PCFG, in general, can have any
rule, however, the algorithm we are discussing convert the
PCFG rules into Chomsky Normal Form, thus for simplicity
we provide directly such formulation.
Figure 4: Baseline tree representations used in the PCFG parsing
model
Figure 5: Filler-parent tree representations used in the PCFG pars-
ing model
have all rules in the form of 4 and 5, is straight-
forward and can be done with simple algorithms
not discussed here.
4.1 Tree Representations for Extended
Named Entities
As discussed in (Johnson, 1998), an important
point for a parsing algorithm is the representation
of trees being parsed. Changing the tree represen-
tation can change significantly the performances
of the parser. Since there is a large difference be-
tween entity trees used in this work and syntac-
tic trees, from both meaning and structure point
of view, it is worth performing an analysis with
the aim of finding the most suitable representa-
tion for our task. In order to perform this analy-
sis, we start from a named entity annotated on the
words de notre president , M. Nicolas Sarkozy(of
our president, Mr. Nicolas Sarkozy). The corre-
sponding named entity is shown in figure 4. As
decided in the annotation guidelines, fillers can be
part of a named entity. This can happen for com-
plex named entities involving several words. The
representation shown in figure 4 is the default rep-
resentation and will be referred to as baseline. A
problem created by this representation is the fact
that fillers are present also outside entities. Fillers
of named entities should be, in principle, distin-
guished from any other filler, since they may be
informative to discriminate entities.
Following this intuition, we designed two dif-
ferent representations where entity fillers are con-
177
Figure 6: Parent-context tree representations used in the PCFG
parsing model
Figure 7: Parent-node tree representations used in the PCFG pars-
ing model
textualized so that to be distinguished from the
other fillers. In the first representation we give to
the filler the same label of the parent node, while
in the second representation we use a concatena-
tion of the filler and the label of the parent node.
These two representations are shown in figure 5
and 6, respectively. The first one will be referred
to as filler-parent, while the second will be re-
ferred as parent-context. A problem that may be
introduced by the first representation is that some
entities that originally were used only for non-
terminal entities will appear also as components,
i.e. entities annotated on words. This may intro-
duce some ambiguity.
Another possible contextualization can be to
annotate each node with the label of the parent
node. This representation is shown in figure 7
and will be referred to as parent-node. Intuitively,
this representation is effective since entities an-
notated directly on words provide also the en-
tity of the parent node. However this representa-
tion increases drastically the number of entities,
in particular the number of components, which
in our case are the set of labels to be learned by
the CRF model. For the same reason this repre-
sentation produces more rigid models, since label
sequences vary widely and thus is not likely to
match sequences not seen in the training data.
Finally, another interesting tree representation
is a variation of the parent-node tree, where en-
tity fillers are only distinguished from fillers not
in an entity, using the label ne-filler, but they are
not contextualized with entity information. This
representation is shown in figure 8 and it will be
Figure 8: Parent-node-filler tree representations used in the PCFG
parsing model
referred to as parent-node-filler. This representa-
tion is a good trade-off between contextual infor-
mation and rigidity, by still representing entities
as concatenation of labels, while using a common
special label for entity fillers. This allows to keep
lower the number of entities annotated on words,
i.e. components.
Using different tree representations affects both
the structure and the performance of the parsing
model. The structure is described in the next sec-
tion, the performance in the evaluation section.
4.2 Structure of the Model
Lexicalized models for syntactic parsing de-
scribed in (Charniak, 2000; Charniak et al 1998)
and (Collins, 1997), integrate more information
than what is used in equations 4 and 5. Consider-
ing a particular node in the entity tree, not includ-
ing terminals, the information used is:
? s: the head word of the node, i.e. the most
important word of the chunk covered by the
current node
? h: the head word of the parent node
? t: the entity tag of the current node
? l: the entity tag of the parent node
The head word of the parent node is defined
percolating head words from children nodes to
parent nodes, giving the priority to verbs. They
can be found using automatic approaches based
on words and entity tag co-occurrence or mutual
information. Using this information, the model
described in (Charniak et al 1998) is P (s|h, t, l).
This model being conditioned on several pieces
of information, it can be affected by data sparsity
problems. Thus, the model is actually approxi-
mated as an interpolation of probabilities:
P (s|h, t, l) =
?1P (s|h, t, l) + ?2P (s|ch, t, l)+
?3P (s|t, l) + ?4P (s|t) (6)
178
where ?i, i = 1, ..., 4, are parameters of the
model to be tuned, and ch is the cluster of head
words for a given entity tag t. With such model,
when not all pieces of information are available to
estimate reliably the probability with more con-
ditioning, the model can still provide a proba-
bility with terms conditioned with less informa-
tion. The use of head words and their percola-
tion over the tree is called lexicalization. The
goal of tree lexicalization is to add lexical infor-
mation all over the tree. This way the probabil-
ity of all rules can be conditioned also on lexi-
cal information, allowing to define the probabili-
ties P (s|h, t, l) and P (s|ch, t, l). Tree lexicaliza-
tion reflects the characteristics of syntactic pars-
ing, for which the models described in (Charniak,
2000; Charniak et al 1998) and (Collins, 1997)
were defined. Head words are very informative
since they constitute keywords instantiating la-
bels, regardless if they are syntactic constituents
or named entities. However, for named entity
recognition it doesn?t make sense to give prior-
ity to verbs when percolating head words over the
tree, even more because head words of named en-
tities are most of the time nouns. Moreover, it
doesn?t make sense to give priority to the head
word of a particular entity with respect to the oth-
ers, all entities in a sentence have the same im-
portance. Intuitively, lexicalization of entity trees
is not straightforward as lexicalization of syntac-
tic trees. At the same time, using not lexicalized
trees doesn?t make sense with models like 6, since
all the terms involve lexical information. Instead,
we can use the model of (Johnson, 1998), which
define the probability of a tree ? as:
P (?) =
Y
X??
P (X ? ?)C? (X??) (7)
here the RHS of rules has been generalized with
?, representing RHS of both unary and binary
rules 4 and 5. C? (X ? ?) is the number of times
the rule X ? ? appears in the tree ? . The model
7 is instantiated when using tree representations
shown in Fig. 4, 5 and 6. When using representa-
tions given in Fig. 7 and 8, the model is:
P (? |l) (8)
where l is the entity label of the parent node.
Although non-lexicalized models like 7 and 8
have shown less effective for syntactic parsing
than their lexicalized couter-parts, there are evi-
dences showing that they can be effective in our
task. With reference to figure 4, considering the
entity pers.ind instantiated by Nicolas Sarkozy,
our algorithm detects first name.first for Nicolas
and name.last for Sarkozy using the CRF model.
As mentioned earlier, once the CRF model has de-
tected components, since entity trees have not a
complex structure with respect to syntactic trees,
even a simple model like the one in equation 7
or 8 is effective for entity tree parsing. For ex-
ample, once name.first and name.last have been
detected by CRF, pers.ind is the only entity hav-
ing name.first and name.last as children. Am-
biguities, like for example for kind or qualifier,
which can appear in many entities, can affect the
model 7, but they are overcome by the model 8,
taking the entity tag of the parent node into ac-
count. Moreover, the use of CRF allows to in-
clude in the model much more features than the
lexicalized model in equation 6. Using features
like word prefixes (P), suffixes (S), capitalization
(C), morpho-syntactic features (MS) and other
features indicated as F2, the CRF model encodes
the conditional probability:
P (t|w,P, S, C,MS, F ) (9)
where w is an input word and t is the corre-
sponding component.
The probability of the CRF model, used in the
first step to tag input words with components,
is combined with the probability of the PCFG
model, used to parse entity trees starting from
components. Thus the structure of our model is:
P (t|w,P, S, C,MS, F ) ? P (?) (10)
or
P (t|w,P, S, C,MS, F ) ? P (? |l) (11)
depending if we are using the tree representa-
tion given in figure 4, 5 and 6 or in figure 7 and 8,
respectively. A scale factor could be used to com-
bine the two scores, but this is optional as CRFs
can provide normalized posterior probabilities.
2The set of features used in the CRF model will be de-
scribed in more details in the evaluation section.
179
5 Related Work
While the models used for named entity detection
and the set of named entities defined along the
years have been discussed in the introduction and
in section 2, since CRFs and models for parsing
constitute the main issue in our work, we discuss
some important models here.
Beyond the models for parsing discussed in
section 4, together with motivations for using or
not in our work, another important model for syn-
tactic parsing has been proposed in (Ratnaparkhi,
1999). Such model is made of four Maximum
Entropy models used in cascade for parsing at
different stages. Also this model makes use of
head words, like those described in section 4, thus
the same considerations hold, moreover it seems
quite complex for real applications, as it involves
the use of four different models together. The
models described in (Johnson, 1998), (Charniak,
1997; Caraballo and Charniak, 1997), (Charniak
et al 1998), (Charniak, 2000), (Collins, 1997)
and (Ratnaparkhi, 1999), constitute the main in-
dividual models proposed for constituent-based
syntactic parsing. Later other approaches based
on models combination have been proposed, like
e.g. the reranking approach described in (Collins
and Koo, 2005), among many, and also evolutions
or improvements of these models.
More recently, approaches based on log-linear
models have been proposed (Clark and Curran,
2007; Finkel et al 2008) for parsing, called also
?Tree CRF?, using also different training criteria
(Auli and Lopez, 2011). Using such models in our
work has basically two problems: one related to
scaling issues, since our data present a large num-
ber of labels, which makes CRF training problem-
atic, even more when using ?Tree CRF?; another
problem is related to the difference between syn-
tactic parsing and named entity detection tasks,
as mentioned in sub-section 4.2. Adapting ?Tree
CRF? to our task is thus a quite complex work, it
constitutes an entire work by itself, we leave it as
feature work.
Concerning linear-chain CRF models, the
one we use is a state-of-the-art implementation
(Lavergne et al 2010), as it implements the
most effective optimization algorithms as well as
state-of-the-art regularizers (see sub-section 3.1).
Some improvement of linear-chain CRF have
been proposed, trying to integrate higher order
target-side features (Tang et al 2006). An inte-
gration of the same kind of features has been tried
also in the model used in this work, without giv-
ing significant improvements, but making model
training much harder. Thus, this direction has not
been further investigated.
6 Evaluation
In this section we describe experiments performed
to evaluate our models. We first describe the set-
tings used for the two models involved in the en-
tity tree parsing, and then describe and comment
the results obtained on the test corpus.
6.1 Settings
The CRF implementation used in this work is de-
scribed in (Lavergne et al 2010), named wapiti.3
We didn?t optimize parameters ?1 and ?2 of the
elastic net (see section 3.1), although this im-
proves significantly the performances and leads
to more compact models, default values lead in
most cases to very accurate models. We used a
wide set of features in CRF models, in a window
of [?2,+2] around the target word:
? A set of standard features like word prefixes
and suffixes of length from 1 to 6, plus some
Yes/No features like Does the word start with
capital letter?, etc.
? Morpho-syntactic features extracted from
the output of the tool tagger (Allauzen and
Bonneau-Maynard, 2008)
? Features extracted from the output of the se-
mantic analyzer (Rosset et al (2009)) pro-
vided by the tool WMatch (Galibert, 2009).
This analysis morpho-syntactic information as
well as semantic information at the same level
of named entities. Using two different sets of
morpho-syntactic features results in more effec-
tive models, as they create a kind of agreement
for a given word in case of match. Concerning
the PCFG model, grammars, tree binarization and
the different tree representations are created with
our own scripts, while entity tree parsing is per-
formed with the chart parsing algorithm described
in (Johnson, 1998).4
3available at http://wapiti.limsi.fr
4available at http://web.science.mq.edu.au/
?mjohnson/Software.htm
180
CRF PCFG
Model # features # labels # rules
baseline 3,041,797 55 29,611
filler-parent 3,637,990 112 29,611
parent-context 3,605,019 120 29,611
parent-node 3,718,089 441 31,110
parent-node-filler 3,723,964 378 31,110
Table 3: Statistics showing the characteristics of the different
models used in this work
6.2 Evaluation Metrics
All results are expressed in terms of Slot Error
Rate (SER) (Makhoul et al 1999) which has a
similar definition of word error rate for ASR sys-
tems, with the difference that substitution errors
are split in three types: i) correct entity type with
wrong segmentation; ii) wrong entity type with
correct segmentation; iii) wrong entity type with
wrong segmentation; here, i) and ii) are given half
points, while iii), as well as insertion and deletion
errors, are given full points. Moreover, results are
given using the well known F1 measure, defined
as a function of precision and recall.
6.3 Results
In this section we provide evaluations of the mod-
els described in this work, based on combination
of CRF and PCFG and using different tree repre-
sentations of named entity trees.
6.3.1 Model Statistics
As a first evaluation, we describe some statis-
tics computed from the CRF and PCFG models
using the tree representations. Such statistics pro-
vide interesting clues of how difficult is learning
the task and which performance we can expect
from the model. Statistics for this evaluation are
presented in table 3. Rows corresponds to the dif-
ferent tree representations described in this work,
while in the columns we show the number of fea-
tures and labels for the CRF models (# features
and # labels), and the number of rules for PCFG
models (# rules).
As we can see from the table, the number
of rules is the same for the tree representations
baseline, filler-parent and parent-context, and
for the representations parent-node and parent-
node-filler. This is the consequence of the con-
textualization applied by the latter representa-
tions, i.e. parent-node and parent-node-filler
create several different labels depending from
the context, thus the corresponding grammar
DEV TEST
Model SER F1 SER F1
baseline 20.0% 73.4% 14.2% 79.4%
filler-parent 16.2% 77.8% 12.5% 81.2%
parent-context 15.2% 78.6% 11.9% 81.4%
parent-node 6.6% 96.7% 5.9% 96.7%
parent-node-filler 6.8% 95.9% 5.7% 96.8%
Table 4: Results computed from oracle predictions obtained with
the different models presented in this work
DEV TEST
Model SER F1 SER F1
baseline 33.5% 72.5% 33.4% 72.8%
filler-parent 31.3% 74.4% 33.4% 72.7%
parent-context 30.9% 74.6% 33.3% 72.8%
parent-node 31.2% 77.8% 31.4% 79.5%
parent-node-filler 28.7% 78.9% 30.2% 80.3%
Table 5: Results obtained with our combined algorithm based on
CRF and PCFG
will have more rules. For example, the rule
pers.ind? name.first name.last can
appear as it is or contextualized with func.ind,
like in figure 8. In contrast the other tree repre-
sentations modify only fillers, thus the number of
rules is not affected.
Concerning CRF models, as shown in table 3,
the use of the different tree representations results
in an increasing number of labels to be learned by
CRF. This aspect is quite critical in CRF learn-
ing, as training time is exponential in the number
of labels. Indeed, the most complex models, ob-
tained with parent-node and parent-node-filler
tree representations, took roughly 8 days for train-
ing. Additionally, increasing the number of labels
can create data sparseness problems, however this
problem doesn?t seem to arise in our case since,
apart the baseline model which has quite less fea-
tures, all the others have approximately the same
number of features, meaning that there are actu-
ally enough data to learn the models, regardless
the number of labels.
6.3.2 Evaluations of Tree Representations
In this section we evaluate the models in terms
of the evaluation metrics described in previous
section, Slot Error Rate (SER) and F1 measure.
In order to evaluate PCFG models alone, we
performed entity tree parsing using as input ref-
erence transcriptions, i.e. manual transcriptions
and reference component annotations taken from
development and test sets. This can be consid-
ered a kind of oracle evaluations and provides us
an upper bound of the performance of the PCFG
models. Results for this evaluation are reported in
181
Participant SER
P1 48.9
P2 41.0
parent-context 33.3
parent-node 31.4
parent-node-filler 30.2
Table 6: Results obtained with our combined algorithm based on
CRF and PCFG
table 4. As it can be intuitively expected, adding
more contextualization in the trees results in more
accurate models, the simplest model, baseline,
has the worst oracle performance, filler-parent
and parent-context models, adding similar con-
textualization information, have very similar ora-
cle performances. Same line of reasoning applies
to models parent-node and parent-node-filler,
which also add similar contextualization and have
very similar oracle predictions. These last two
models have also the best absolute oracle perfor-
mances. However, adding more contextualization
in the trees results also in more rigid models, the
fact that models are robust on reference transcrip-
tions and based on reference component annota-
tions, doesn?t imply a proportional robustness on
component sequences generated by CRF models.
This intuition is confirmed from results re-
ported in table 5, where a real evaluation of our
models is reported, using this time CRF out-
put components as input to PCFG models, to
parse entity trees. The results reported in ta-
ble 5 show in particular that models using base-
line, filler-parent and parent-context tree repre-
sentations have similar performances, especially
on test set. Models characterized by parent-node
and parent-node-filler tree representations have
indeed the best performances, although the gain
with respect to the other models is not as much
as it could be expected given the difference in
the oracle performances discussed above. In par-
ticular the best absolute performance is obtained
with the model parent-node-filler. As we men-
tioned in subsection 4.1, this model represents the
best trade-off between rigidity and accuracy using
the same label for all entity fillers, but still distin-
guishing between fillers found in entity structures
and other fillers found in words not instantiating
any entity.
6.3.3 Comparison with Official Results
As a final evaluation of our models, we pro-
vide a comparison of official results obtained at
the 2011 evaluation campaign of extended named
entity recognition (Galibert et al 2011; 2) Re-
sults are reported in table 6, where the other two
participants to the campaign are indicated as P1
and P2. These two participants P1 and P2, used
a system based on CRF, and rules for deep syn-
tactic analysis, respectively. In particular, P2 ob-
tained superior performances in previous evalua-
tion campaign on named entity recognition. The
system we proposed at the evaluation campaign
used a parent-context tree representation. The
results obtained at the evaluation campaign are
in the first three lines of Table 6. We compare
such results with those obtained with the parent-
node and parent-node-filler tree representations,
reported in the last two rows of the same table. As
we can see, the new tree representations described
in this work allow to achieve the best absolute per-
formances.
7 Conclusions
In this paper we have presented a Named Entity
Recognition system dealing with extended named
entities with a tree structure. Given such represen-
tation of named entities, the task cannot be mod-
eled as a sequence labelling approach. We thus
proposed a two-steps system based on CRF and
PCFG. CRF annotate entity components directly
on words, while PCFG apply parsing techniques
to predict the whole entity tree. We motivated
our choice by showing that it is not effective to
apply techniques used widely for syntactic pars-
ing, like for example tree lexicalization. We pre-
sented an analysis of different tree representations
for PCFG, which affect significantly parsing per-
formances.
We provided and discussed a detailed evalua-
tion of all the models obtained by combining CRF
and PCFG with the different tree representation
proposed. Our combined models result in better
performances with respect to other models pro-
posed at the official evaluation campaign, as well
as our previous model used also at the evaluation
campaign.
Acknowledgments
This work has been funded by the project Quaero,
under the program Oseo, French State agency for
innovation.
182
References
Ralph Grishman and Beth Sundheim. 1996. Mes-
sage Understanding Conference-6: a brief history.
In Proceedings of the 16th conference on Com-
putational linguistics - Volume 1, pages 466?471,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Satoshi Sekine and Chikashi Nobata. 2004. Defini-
tion, Dictionaries and Tagger for Extended Named
Entity Hierarchy. In Proceedings of LREC.
G. Doddington, A. Mitchell, M. Przybocki,
L. Ramshaw, S. Strassel, and R. Weischedel.
2004. The Automatic Content Extraction (ACE)
Program?Tasks, Data, and Evaluation. Proceedings
of LREC 2004, pages 837?840.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Karn Fort, Olivier Galibert, Ludovic Quintard.
2011. Proposal for an extension or traditional
named entities: From guidelines to evaluation, an
overview. In Proceedings of the Linguistic Annota-
tion Workshop (LAW).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Pro-
ceedings of the Eighteenth International Confer-
ence on Machine Learning (ICML), pages 282?289,
Williamstown, MA, USA, June.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,
24:613?632.
Stefan Hahn, Marco Dinarelli, Christian Raymond,
Fabrice Lefe`vre, Patrick Lehen, Renato De Mori,
Alessandro Moschitti, Hermann Ney, and Giuseppe
Riccardi. 2010. Comparing stochastic approaches
to spoken language understanding in multiple lan-
guages. IEEE Transactions on Audio, Speech and
Language Processing (TASLP), 99.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. COMPU-
TATIONAL LINGUISTICS, 22:39?71.
Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Stefan Riezler and Alexander Vasserman. 2004. In-
cremental feature selection and l1 regularization
for relaxed maximum-entropy modeling. In Pro-
ceedings of the International Conference on Em-
pirical Methods for Natural Language Processing
(EMNLP).
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the Elastic Net. Journal of the
Royal Statistical Society B, 67:301?320.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the fourteenth national conference
on artificial intelligence and ninth conference on
Innovative applications of artificial intelligence,
AAAI?97/IAAI?97, pages 598?603. AAAI Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139, San
Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Sharon A. Caraballo and Eugene Charniak. 1997.
New figures of merit for best-first probabilistic chart
parsing. Computational Linguistics, 24:275?298.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, ACL ?98, pages 16?23, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-based best-first chart parsing. In
In Proceedings of the Sixth Workshop on Very Large
Corpora, pages 127?133. Morgan Kaufmann.
Alexandre Allauzen and He?le?ne Bonneau-Maynard.
2008. Training and evaluation of pos taggers on the
french multitag corpus. In Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
Olivier Galibert. 2009. Approches et me?thodologies
pour la re?ponse automatique a` des questions
adapte?es a` un cadre interactif en domaine ouvert.
Ph.D. thesis, Universite? Paris Sud, Orsay.
Rosset Sophie, Galibert Olivier, Bernard Guillaume,
Bilinski Eric, and Adda Gilles. The LIMSI mul-
tilingual, multitask QAst system. In Proceed-
ings of the 9th Cross-language evaluation forum
conference on Evaluating systems for multilin-
gual and multimodal information access, CLEF?08,
pages 480?487, Berlin, Heidelberg, 2009. Springer-
Verlag.
Azeddine Zidouni, Sophie Rosset, and Herve? Glotin.
2010. Efficient combined approach for named en-
tity recognition in spoken language. In Proceedings
of the International Conference of the Speech Com-
munication Assosiation (Interspeech), Makuhari,
Japan
John Makhoul, Francis Kubala, Richard Schwartz,
and Ralph Weischedel. 1999. Performance mea-
sures for information extraction. In Proceedings of
DARPA Broadcast News Workshop, pages 249?252.
Adwait Ratnaparkhi. 1999. Learning to Parse Natural
Language with Maximum Entropy Models. Journal
of Machine Learning, vol. 34, issue 1-3, pages 151?
175.
183
Michael Collins and Terry Koo. 2005. Discriminative
Re-ranking for Natural Language Parsing. Journal
of Machine Learning, vol. 31, issue 1, pages 25?70.
Clark, Stephen and Curran, James R. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Journal of Computational Lin-
guistics, vol. 33, issue 4, pages 493?552.
Finkel, Jenny R. and Kleeman, Alex and Manning,
Christopher D. 2008. Efficient, Feature-based,
Conditional Random Field Parsing. Proceedings
of the Association for Computational Linguistics,
pages 959?967, Columbus, Ohio.
Michael Auli and Adam Lopez 2011. Training a Log-
Linear Parser with Loss Functions via Softmax-
Margin. Proceedings of Empirical Methods for
Natural Language Processing, pages 333?343, Ed-
inburgh, U.K.
Tang, Jie and Hong, MingCai and Li, Juan-Zi and
Liang, Bangyong. 2006. Tree-Structured Con-
ditional Random Fields for Semantic Annotation.
Proceedgins of the International Semantic Web
Conference, pages 640?653, Edited by Springer.
Olivier Galibert; Sophie Rosset; Cyril Grouin; Pierre
Zweigenbaum; Ludovic Quintard. 2011. Struc-
tured and Extended Named Entity Evaluation in Au-
tomatic Speech Transcriptions. IJCNLP 2011.
Marco Dinarelli, Sophie Rosset. Models Cascade for
Tree-Structured Named Entity Detection IJCNLP
2011.
184
Proceedings of the ACL 2014 Student Research Workshop, pages 34?40,
Baltimore, Maryland USA, June 22-27 2014. c?2014 Association for Computational Linguistics
A Mapping-Based Approach for General Formal
Human Computer Interaction Using Natural Language
Vincent Letard
LIMSI CNRS
letard@limsi.fr
Sophie Rosset
LIMSI CNRS
rosset@limsi.fr
Gabriel Illouz
LIMSI CNRS
illouz@limsi.fr
Abstract
We consider the problem of mapping nat-
ural language written utterances express-
ing operational instructions1 to formal lan-
guage expressions, applied to French and
the R programming language. Developing
a learning operational assistant requires
the means to train and evaluate it, that is,
a baseline system able to interact with the
user. After presenting the guidelines of
our work, we propose a model to repre-
sent the problem and discuss the fit of di-
rect mapping methods to our task. Finally,
we show that, while not resulting in excel-
lent scores, a simple approach seems to be
sufficient to provide a baseline for an in-
teractive learning system.
1 Introduction
Technical and theoretical advances allow achiev-
ing more and more powerful and efficient opera-
tions with the help of computers. However, this
does not necessarily make it easier to work with
the machine. Recent supervised learning work
(Allen et al, 2007; Volkova et al, 2013) exploited
the richness of human-computer interaction for
improving the efficiency of a human performed
task with the help of the computer.
Contrary to most of what was proposed so far,
our long term goal is to build an assistant system
learning from interaction to construct a correct for-
mal language (FL) command for a given natural
language (NL) utterance, see Table 1. However,
designing such a system requires data collection,
and early attempts highlighted the importance of
usability for the learning process: a system that is
hard to use (eg. having very poor performance)
1We call operational instruction the natural language ex-
pression of a command in any programming language.
would prevent from extracting useful learning ex-
amples from the interaction. We thus need to pro-
vide the system with a basis of abilities and knowl-
edge to allow both incremental design and to keep
the interest of the users, without which data turn
to be way more tedious to collect. We assume that
making the system usable requires the ability to
provide help to the user more often than it needs
help from him/her, that is an accuracy over 50%.
We hypothesize that a parametrized direct
mapping between the NL utterances and the FL
commands can reach that score. A knowledge set
K is built from parametrized versions of the asso-
ciations shown in Table 1. The NL utterance U
best
from K that is the closest to the request-utterance
according to a similarity measure is chosen and its
associated command C(U
best
) is adapted to the
parameters of the request-utterance and returned.
For example, given the request-utterance U
req
:
?Load the file data.csv?, the system should rank
the utterances of K by similarity with U
req
. Con-
sidering the associations represented in Table 1,
the first utterance should be the best ranked, and
the system should return the command:
?var1 <- read.csv("data.csv")?.
Note that several commands can be proposed at
the same time to give the user alternate choices.
We use Jaccard, tf-idf, and BLEU similarity
measures, and consider different selection strate-
gies. We highlight that the examined similarity
measures show enough complementarity to permit
the use of combination methods, like vote or sta-
tistical classification, to improve a posteriori the
efficiency of the retrieval.
2 Related Work
2.1 Mapping Natural Language to Formal
Language
Related problems have been previously processed
using different learning methods. Branavan (2009,
34
NL utterances FL commands (in R)
1 Charge les donne?es depuis ?res.csv? var1=read.csv("res.csv")Load the data from ?res.csv?
2 Trace l?histogramme de la colonne 2 de tab plot(hist(tab[[2]]))Draw a bar chart with column 2 of tab
3 Dessine la re?partition de la colonne 3 de tab plot(hist(tab[[3]]))Draw the distribution of column 3 of tab
4 Somme les colonnes 3 et 4 de tab var2=c(sum(tab[3]),sum(tab[4]))Compute the sum of columns 3 and 4 of tab
5 Somme les colonnes 3 et 4 de tab var3=sum(c(tab[[3]],tab[[4]]))Compute the sum of columns 3 and 4 of tab
Table 1: A sample of NL utterances to FL commands mapping
These examples specify the expected command to be returned for each utterance. The tokens in bold
font are linked with the commands parameters, cf. section 4. Note that the relation between utterances
and commands is a n to n. Several utterances can be associated to the same command and conversely.
2010) uses reinforcement learning to map En-
glish NL instructions to a sequence of FL com-
mands. The mapping takes high-level instructions
and their constitution into account. The scope
of usable commands is yet limited to graphical
interaction possibilities. As a result, the learn-
ing does not produce highly abstract schemes. In
the problematic of interactive continuous learning,
Artzi and Zettlemoyer (2011) build by learning a
semantic NL parser based on combinatory cate-
gorial grammars (CCG). Kushman and Barzilay
(2013) also use CCG in order to generate regu-
lar expressions corresponding to their NL descrip-
tions. This constructive approach by translation
allows to generalize over learning examples, while
the expressive power of regular expressions cor-
respond to the type-3 grammars of the Chomsky
hierarchy. This is not the case for the program-
ming languages since they are at least of type-2.
Yu and Siskind (2013) use hidden Markov mod-
els to learn a mapping between object tracks from
a video sequence and predicates extracted from
a NL description. The goal of their approach is
different from ours but the underlying problem of
finding a map between objects can be compared.
The matched objects constitute here a FL expres-
sion instead of a video sequence track.
2.2 Machine Translation
Machine translation usually refers to transforming
a NL sentence from a source language to another
sentence of the same significance in another natu-
ral language, called target language. This task is
achieved by building an intermediary representa-
tion of the sentence structure at a given level of
abstraction, and then encoding the obtained object
into the target language. While following a dif-
ferent goal, one of the tasks of the XLike project
(Marko Tadic? et al, 2012) was to examine the
possibility of translating statements from NL (En-
glish) to FL (Cycl). Adapting such an approach
to operational formal target language can be inter-
esting to investigate, but we will not focus on that
track for our early goal.
2.3 Information Retrieval
The issue of information retrieval systems can be
compared with the operational assistant?s (OA),
when browsing its knowledge. Question an-
swering systems in particular (Hirschman and
Gaizauskas, 2001), turn out to be similar to OA
since both types of systems have to respond to a
NL utterance of the user by generating an accu-
rate reaction (which is respectively a NL utterance
containing the wanted information, or the execu-
tion of a piece of FL code). However, as in (Toney
et al, 2008), questions answering systems usually
rely on text mining to retrieve the right informa-
tion. Such a method demands large sets of anno-
tated textual data (either by hand or using an au-
tomatic annotator). Yet, tutorials, courses or man-
uals which could be used in order to look for re-
sponses for operational assistant systems are het-
erogeneous and include complex or implicit ref-
erences to operational knowledge. This makes
the annotation of such data difficult. Text min-
ing methods are thus not yet applicable to oper-
ational assistant systems but could be considered
once some annotated data is collected.
35
3 Problem Formulation
As we introduced in the first section, we represent
the knowledge K as a set of examples of a binary
relation R : NL ? FL associating a NL utter-
ance to a FL command. If we consider the simple
case of a functional and injective relation, each
utterance is associated to exactly one command.
This is not realistic since it is possible to reformu-
late nearly any NL sentence. The case of a non in-
jective relation covers better the usual cases: each
command can be associated with one or more ut-
terances, this situation is illustrated by the second
and third examples of Table 1. Yet, the real-life
case should be a non injective nor functional rela-
tion. Not only multiple utterances can refer to a
same command, but one single utterance can also
stand for several distinct commands (see the fourth
and fifth examples2 in Table 1). We must consider
all these associations when matching a request-
utterance U
req
for command retrieval in K .
At this point, several strategies can be used to
determine what to return, with the help of the sim-
ilarity measure ? : NL ? NL ? R between two
NL utterances. Basically, we must determine if
a response should be given, and if so how many
commands to return. To do this, two potential
strategies can be considered for selecting the as-
sociated utterances in K .
The first choice focuses on the number of re-
sponses that are given for each request-utterance.
The n first commands according to the rankings of
their associated utterances in K are returned. The
rank r of a given utterance U is computed with:
r(U |U
req
) =
?
?
?
U
?
? K : ?(U
req
, U
?
) > ?(U
req
, U)
?
?
?
(1)
The second strategy choice can be done by de-
termining an absolute similarity threshold below
which the candidate utterances from K and their
associated sets of commands are considered too
different to match. The resulting set of commands
is given by:
Res = {C ? FL : (U,C) ? K,?(U
req
, U) < t} (2)
with t the selected threshold. Once selected the
set of commands to be given as response, if there
are more than one, the choice of the one to execute
can be done interactively with the help of the user.
2The command 4 returns a vector of the sums of each col-
umn, while the command 5 returns the sum of the columns as
a single integer.
4 Approach
We are given a simple parsing result of both the ut-
terance and the command. The first step to address
is the acquisition of examples and the way to up-
date the knowledge. Then we examine the meth-
ods for retrieving a command from the knowledge
and a given request-utterance.
Correctly mapping utterances to commands re-
quires at least to take their respective parameters
into account (variable names, numeric values, and
quoted strings). We build generic representations
of utterances and commands by identifying the pa-
rameters in the knowledge example pair (see Ta-
ble 1), and use them to reconstruct the command
with the parameters of the request-utterance.
4.1 Retrieving the Commands
We applied three textual similarity measures to
our model in order to compare their strengths and
weaknesses on our task: the Jaccard similarity co-
efficient (Jaccard index), a tf-idf (Term frequency-
inverse document frequency) aggregation, and the
BLEU (Bilingual Evaluation Understudy) mea-
sure.
4.1.1 Jaccard index
The Jaccard index measures a similarity between
two sets valued in the same superset. For the
present case, we compare the set of words of the
input NL instruction and the one of the compared
candidate instruction, valued in the set of possible
tokens. The adapted formula for two sentences S
1
and S
2
results in:
J(s
1
, s
2
) =
|W (s
1
) ?W (s
2
)|
|W (s
1
) ?W (s
2
)|
(3)
where W (S) stands for the set of words of the
sentence S. The Jaccard index is a baseline to
compare co-occurences of unigrams, and should
be efficient mainly with corpora containing few
ambiguous examples.
4.1.2 tf-idf
The tf-idf measure permits, given a word, to clas-
sify documents on its importance in each one, re-
garding its importance in the whole set. This mea-
sure should be helpful to avoid noise bias when it
comes from frequent terms in the corpus. Here,
the documents are the NL utterances from K , and
they are classified regarding the whole request-
utterance, or input sentence s
i
. We then use the
36
following aggregation of the tf-idf values for each
word of s
i
.
tfidf
S
(s
i
, s
c
) =
1
|W (s
i
)|
X
w?W (s
i
)
tfidf(w, s
c
, S) (4)
with S = {s|(s, com) ? K}, where s
i
is the input
sentence, s
c
? S is the compared sentence, and
where the tf-idf is given by:
tfidf(w, s
c
, S) = f(w, s
c
)idf(w, S) (5)
idf(w,S) = log
?
|S|
|{s ? S|w ? s}|
?
(6)
where at last f(w, s) is the frequency of the word
w in the sentence s. As we did for the Jaccard in-
dex, we performed the measures on both raw and
lemmatized words. On the other hand, getting rid
of the function words and closed class words is not
here mandatory since the tf-idf measure already
takes the global word frequency into account.
4.1.3 The BLEU measure
The bilingual evaluation understudy algorithm
(Papineni et al, 2002) focuses on n-grams co-
occurrences. This algorithm can be used to dis-
card examples where the words ordering is too far
from the candidate. It computes a modified pre-
cision based on the ratio of the co-occurring n-
grams within candidate and reference sentences,
on the total size of the candidate normalized by n.
P
BLEU
(s
i
, S) =
X
gr
n
?s
i
max
s
c
?S
occ(gr
n
, s
c
)
grams(s
i
, n)
(7)
where grams(s, n) = |s| ? (n? 1) is the number
of n-grams in the sentence s and occ(gr
n
, s) =
?
gr
n
?
?s
[gr
n
= gr
n
?
] is the number of occur-
rences of the n-gram gr
n
in s. BLEU also uses
a brevity penalty to prevent long sentences from
being too disadvantaged by the n-gram based pre-
cision formula. Yet, the scale of the length of the
instructions in our corpus is sufficiently reduced
not to require its use.
4.2 Optimizing the similarity measure
We applied several combinations of filters to the
utterances compared before evaluating their sim-
ilarity. We can change the set of words taken
into account, discarding or not the non open-class
words3. Identified non-lexical references such as
3Open-class words include nouns, verbs, adjectives, ad-
verbs and interjections.
variable names, quoted character strings and nu-
meric values can also be discarded or transformed
to standard substitutes. Finally, we can apply or
not a lemmatization4 on lexical tokens.By discard-
ing non open-class words, keeping non-lexical ref-
erences and applying the lemmatization, the sec-
ond utterance of Table 1 would then become:
draw bar chart column xxVALxx xxVARxx
5 Experimental Setup
5.1 Parsing
The NL utterances first pass through an arith-
metic expression finder to completely tag them be-
fore the NL analyzer. They are then parsed us-
ing WMATCH, a generic rule-based engine for
language analysis developed by Olivier Galibert
(2009). This system is modular and dispose of
rules sets for both French and English. As an ex-
ample, the simplified parsing result of the first ut-
terance of Table 1 looks like:
<_operation>
<_action> charge|_?V </_action>
<_det> les </_det>
<_subs> donne?es|_?N </_subs>
<_prep> depuis </_prep>
<_unk> "res.csv" </_unk>
</_operation>
Words tagged as unknown are considered as po-
tential variable or function names. We also added
a preliminary rule to identify character strings and
count them among the possibly linked features of
the utterance. The commands are normalized by
inserting spaces between every non semantically
linked character pair and we identify numeric val-
ues, variable/function names and character strings
as features.
Only generative forms of the commands are
associated to utterances in the knowledge. This
form consists in a normalized command with unre-
solved references for every parameter linked with
the learning utterance. These references are re-
solved at the retrieving phase by matching with the
tokens of the request-utterance.
5.2 Corpus Constitution
Our initial corpus consists in 605 associations be-
tween 553 unique NL utterances in French and
240 unique R commands.
4Lemmatization is the process of transforming a word to
its canonical form, or lemma, ignoring the inflections. It can
be performed with a set of rules or with a dictionary. The
developed system uses a dictionary.
37
The low number of documents describing a
majority of R commands and their heterogeneity
make automatic example gathering not yet achiev-
able. These documentations are written for human
readers having global references on the task. Thus,
we added each example pair manually, making
sure that the element render all the example infor-
mation and that the format correspond to the cor-
pus specifications. Those specifications are meant
to be the least restrictive, that is: a NL utterance
must be written as to ask for the execution of the
associated R task. It therefore should be mostly
in the imperative form and reflect, for experienced
people, a usual way they would express the con-
cerned operation for non specialists.
5.3 Evaluation Metrics
The measures that can contribute to a relevant
evaluation of the system depend on its purpose.
Precision and recall values of information retrieval
systems are computed as follows:
P =
# correct responses
# responses given (8)
R =
# correct responses
# responses in K (9)
Note that the recall value is not as important as for
information retrieval: assuming that the situation
showed by the fourth and fifth associations of Ta-
ble 1 are not usual5, there should be few different
valid commands for a given request-utterance, and
most of them should be equivalent. Moreover, the
number of responses given is fixed (so is the num-
ber of responses in K), the recall thus gives the
same information as the precision, with a linear
coefficient variation.
These formulae can be applied to the ?command
level?, that is measuring the accuracy of the sys-
tem in terms of its good command ratio. However,
the user satisfaction can be better measured at the
?utterance level? since it represents the finest gran-
ularity for the user experience. We define the ut-
terance precision uP as:
uP =
# correct utterances
# responses given (10)
where ?# correct utterances? stands for the num-
ber of request-utterances for which the system pro-
vided at least one good command.
5Increasing the tasks covering of the corpus will make
these collisions more frequent, but this hypothesis seems rea-
sonable for a first approach.
6 Results and Discussion
The system was tested on 10% of the corpus (61
associations). The set of known associations K
contains 85% of the corpus (514 associations), in-
stead of 90% in order to allow several distinct
drawings (40 were tested), and thus avoid too
much noise.
6.1 Comparing similarity measures
As shown in Table 2 the tf-idf measure outper-
forms the Jaccard and BLEU measures, whichever
filter combination is applied. The form of the ut-
terances in the corpus causes indeed the repetition
of a small set of words across the associations.
This can explain why the inverse document fre-
quency is that better.
non-lexical included not included
lemmatize yes no yes no
Jaccard 36.5 36.5 21.2 23.0
tf-idf 48.0 51.9 36.5 40.4
BLEU 30.8 32.7 26.9 30.8
chance 1.9
Table 2: Scores of precision by utterance (uP ),
providing 3 responses for each request-utterance.
The lemmatization and the inclusion of non
open-class words (not shown here) does not seem
to have a clear influence on uP , whereas including
the non-lexical tokens allows a real improvement.
This behaviour must result from the low length av-
erage (7.5 words) of the utterances in the corpus.
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9
Number of responses
Ut
te
ra
n
ce
 p
re
cis
io
n 
(uP
)
measure
tfidf
tfidf_inl
Figure 1: Utterance precision (uP ) for a fixed
number of responses by utterance. The tfidf inl
curve includes the non-lexical tokens.
Note that uP is obtained with Equation 10, which
explains the increase of the precision along the
number of responses.
38
Figure 1 shows the precision obtained with tfidf
while increasing the number of commands given
for each request-utterance. It comes out that it
is useful to propose at least 3 commands to the
user. It would not be interesting, though, to offer a
choice of more than 5 items, because the gain on
uP would be offset by the time penalty for retriev-
ing the good command among the proposals.
6.2 Allowing silence
We also tested the strategy of fixing an absolute
threshold to decide between response and silence.
Given a request-utterance and an associated order-
ing of K according to ?, the system will remain
silent if the similarity of the best example in K is
below the defined threshold.
Surprisingly, it turned out that for every mea-
sure, the 6 best similar responses at least were all
wrong. This result seems to be caused by the ex-
istence, in the test set of commands uncovered by
K , of some very short utterances that contain only
one or two lexical tokens.
6.3 Combinations
0.0
0.2
0.4
0.6
0.8
1 2 3 4 5 6 7 8 9
Number of responses
Ut
te
ra
n
ce
 p
re
cis
io
n 
(uP
)
method
vote
tfidf_inl
vote_oracle
learning
Figure 2: Comparison of the combinations with
the tf-idf inl method. Oracle and actual vote are
done using tf-idf, Jaccard, and BLEU, with and
without non-lexical tokens. The training set for
learning is the result of a run on K .
Having tested several methods giving differ-
ent results, combining these methods can be very
interesting depending on their complementarity.
The oracle vote using the best response among
the 6 best methods shows an encouraging progres-
sion margin (cf. Figure 2). The actual vote it-
self outperforms the best method for giving up to
3 responses (reaching 50% for only 2 responses).
However, the curve position is less clear for more
responses, and tests must be performed on other
drawings of K to measure the noise influence.
The complementarity of the methods can also
be exploited by training a classification model to
identify when a method is better than the others.
We used the similarity values as features and the
measure that gave a good response as the refer-
ence class label (best similarity if multiple, and
?none? class if no good response). This setup was
tested with the support vector machines using lib-
svm (Chang and Lin, 2011) and results are shown
in Figure 2. As expected, machine learning per-
forms poorly on our tiny corpus. The accuracy
is under 20% and the system only learned when
to use the best method, and when to give no re-
sponse. Still, it manages to be competitive with
the best method and should be tested again with
more data and multiple drawings of K .
7 Conclusion and Future Work
The simple mapping methods based on similar-
ity ranking showed up to 60% of utterance pre-
cision6 remaining below a reasonable level of user
sollicitation, which validate our prior hypothesis.
A lot of approaches can enhance that score, such
as adding or developing more suitable similarity
measures (Achananuparp et al, 2008), combining
learning and vote or learning to rerank utterances.
However, while usable as a baseline, these
methods only allow poor generalization and really
need more corpus to perform well. As we pointed
out, the non-functionality of the mapping relation
also introduces ambiguities that cannot be solved
using the only knowledge of the system.
Thanks to this baseline method, we are now able
to collect more data by developing an interactive
agent that can be both an intelligent assistant and
a crowdsourcing platform. We are currently de-
veloping a web interface for this purpose. Finally,
situated human computer interaction will allow the
real-time resolving of ambiguities met in the re-
trieval with the help of the user or with the use of
contextual information from the dialogue.
Aknowledgements
The authors are grateful to every internal and ex-
ternal reviewer for their valuable advices. We also
would like to thank Google for the financial sup-
port for the authors participation to the conference.
6The corpus will soon be made available.
39
References
Palakorn Achananuparp, Xiaohua Hu, and Xiajiong
Shen. 2008. The Evaluation of Sentence Similar-
ity Measures. In Data Warehousing and Knowledge
Discovery, Springer.
James Allen, Nathanael Chambers, George Ferguson,
Lucian Galescu, Hyuckchul Jung, Mary Swift, and
William Tayson. 2007. PLOW: A Collaborative
Task Learning Agent. In Proceedings of the 22nd
National Conference on Artificial Intelligence.
Yoav Artzi, and Luke S. Zettlemoyer. 2011. Boot-
strapping semantic parsers from conversations. Pro-
ceedings of the conference on empirical methods in
natural language processing.
S.R.K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading Between the Lines: Learn-
ing to Map High-level Instructions to Commands. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
S.R.K. Branavan, Harr Chen, Luke S. Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP.
Chih-Chung Chang, and Chih-Jen Lin. 2011. LIB-
SVM: A Library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology
Olivier Galibert. 2009. Approches et me?thodologies
pour la re?ponse automatique a` des questions
adapte?es a` un cadre interactif en domaine ouvert.
Doctoral dissertation, Universite? Paris Sud XI.
Lynette Hirschman, and Robert Gaizauskas. 2001.
Natural language question answering: The view
from here. Natural Language Engineering 7. Cam-
bridge University Press.
Nate Kushman, and Regina Barzilay. 2013. Using Se-
mantic Unification to Generate Regular Expressions
from Natural Language. In Proceedings of the Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Marko Tadic?, Boz?o Bekavac, ?Zeljko Agic?, Matea
Srebac?ic?, Das?a Berovic?, and Danijela Merkler.
2012. Early machine translation based semantic an-
notation prototype XLike project www.xlike.org .
Dave Toney, Sophie Rosset, Aure?lien Max, Olivier
Galibert, and e?ric Billinski. 2008. An Evaluation of
Spoken and Textual Interaction on the RITEL Inter-
active Question Answering System In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation.
Svitlana Volkova, Pallavi Choudhury, Chris Quirk, Bill
Dolan, and Luke Zettlemoyer. 2013. Lightly Su-
pervised Learning of Procedural Dialog System In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics.
Haonan Yu, and Jeffrey Mark Siskind. 2013.
Grounded Language Learning from Video De-
scribed with Sentences. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics.
40
The PEACE SLDS understanding evaluation paradigm of the French
MEDIA campaign
Laurence Devillers, He?le`ne Maynard, Patrick Paroubek, Sophie Rosset
LIMSI-CNRS
Bt 508 University of Paris XI - BP 133 F-91403 ORSAY Cedex, France
fdevil,hbm,pap,rossetg@limsi.fr
Abstract
This paper presents a paradigm for
evaluating the context-sensitive under-
standing capability of any spoken lan-
guage dialog system: PEACE (French
acronym for Paradigme d?Evaluation
Automatique de la Compre?hension hors
et En-contexte). This paradigm will be
the basis of the French Technolangue
MEDIA project, in which dialog sys-
tems from various academic and indus-
trial sites will be tested in an evaluation
campaign coordinated by ELRA/ELDA
(over the next two years). Despite pre-
vious efforts such as EAGLES, DISC,
AUPELF ARCB2 or the ongoing Ameri-
can DARPA COMMUNICATOR project,
the spoken dialog community still lacks
common reference tasks and widely
agreed upon methods for comparing
and diagnosing systems and techniques.
Automatic solutions are nowadays be-
ing sought both to make possible the
comparison of different approaches by
means of reliable indicators with generic
evaluation methodologies and also to re-
duce system development costs. How-
ever achieving independence from both
the dialog system and the task per-
formed seems to be more and more a
utopia. Most of the evaluations have
up to now either tackled the system as
a whole, or based the measurements
on dialog-context-free information. The
PEACE proposal aims at bypassing some
of these shortcomings by extracting,
from real dialog corpora, test sets that
synthesize contextual information.
1 Introduction
Generally speaking common reference
tasks (Whittaker et al, 2002) and methods
to compare and diagnose spoken language dialog
systems (SLDS) and spoken dialog techniques
are lacking despite previous efforts futher dis-
cussed in the next section such as EAGLES,
DISC, AUPELF ARCB2 or the ongoing American
project DARPA COMMUNICATOR. Without
an objective assessment of dialog systems, it is
difficult to reuse previous work and to advance
theories. The assessment of a dialog system is
complex in part to the high integration factor
and tight coupling between the various modules
present in any SLDS, for which unfortunately
today, no common accepted reference architecture
exists. Nevertheless, a major problem remains the
dynamic nature of dialog. Consequently to these
shortcomings, researchers are often unable to
provide principled design and system capabilities
for technology transfer. In other research areas,
such as speech recognition and information re-
trieval, common reference tasks have been highly
effective in sharing research costs and efforts. A
similar development is highly needed in the dialog
community.
In this contribution which addresses only a part
of the SLDS evaluation problem, a paradigm for
evaluating the context-sensitive understanding ca-
pability of any spoken language dialog system is
proposed. PEACE (Devillers et al, 2002a) de-
scribed in section 3, is based on test sets extracted
from real corpora, and has three main aspects: it
is generic, contextual and it offers diagnostic ca-
pabilities. Here genericity is envisaged in a con-
text of information dialogs access. The diagnos-
tic aspect is important in order to determine the
different qualities of the systems under test. The
contextual aspect of evaluation is a crucial point
since dialog is dynamic by nature. We propose
to simulate/synthesize the contextual information.
The PEACE paradigm will be tested in the French
Technolangue MEDIA project and will serve as
basis in the comparison and diagnostic evaluation
of systems presented by various academic and in-
dustrial sites (section 4). ELRA/ELDA is the co-
ordinator of the larger scope evaluation campaign
EVALDA, which includes the MEDIA campaign
that began in January 2003.
2 Overview of SLDS evaluation
Without an attempt to be exhaustive, we overview
some recent efforts for evaluation of SLDS.
The objective of the European DISC project was
to write the best-practice guidelines for SLDS de-
velopment and evaluation of its time. DISC has
collected a systematic list of bottom-up evalua-
tion criteria, each corresponding to a partially or-
dered list of properties likely to be encountered
in any SLDS. This properties are positioned on a
grid defining an SLDS abstract architecture and re-
late to various phases of the generic DISC SLDS
development life-cycle (Dybkj?r and al., 1998).
They are complemented by a standard evaluation
pattern made of 10 generic questions (e.g. ?Which
symptoms need to be observed?? ) which has been
instantiated for all the evaluation criteria. If the
DISC results are quite extensive and presented in
an homogeneous way, they do not provide a di-
rect answer to the question of SLDS evaluation.
Its contribution lies more at the specification level.
Although the approach and the goals of the Euro-
pean EAGLES project were different, one could
forward the same remark about the results of the
speech evaluation work group (D. Gibbon, 1997).
In (Fraser, 1998), one find a set of evaluation cri-
teria for voice oriented products and services, or-
ganized in four broad categories.: 1) voice com-
mand, 2) document generation, 3) phone services
4) other.
To the best of our knowledge, the MADCOW
(Multi Site Data COllection Working group) co-
ordination group set up in the USA by ARPA in
the context of the ATIS (Air Travel Information
Services) task to collect corpora, was the first to
propose a common infrastructure for SLDS auto-
matic evaluation (MADCOW, 1992), which also
addressed the problem of language understand-
ing evaluation, based on system answer compar-
ison. Unfortunately no direct diagnostic informa-
tion can be produced, since understanding is ap-
preciated by gauging the distance from the answer
to a pair of minimal and a maximal reference an-
swers. In ATIS, the protocol was only been ap-
plied to context free sentences. Up to now it has
been one of the most used by the community since
it is relatively objective and generic because it re-
lies on counts of explicit information and allows
for a certain variation in the answers. On the other
hand, the method displays a bias toward silence
and does not give the means to appreciate error
severity.
In ARISE (Automatic Railway Information Sys-
tems for Europe) (Lamel, 1998), a corpus of
roughly 10,000 calls has been used in conjunc-
tion with user debriefing questionnaire analysis to
diagnose different versions of a phone informa-
tion server. The hand-tagging objective measures
of the corpus include understanding error counts
(glass box methodology). Although it provides
fine grained diagnostic information, this procedure
cannot be easily generalized since it requires hand-
annotated corpus and access to the internal repre-
sentation of the system.
Two metrics have been developped at MIT
(Glass et al, 2000): the Query Density (QD)
and the Concept Efficiency (CE), which measure
respectively over the course of a dialogue: the
mean number of new concepts introduced per user
query, and the number of turns necessary for each
concept to be understood by the system. Con-
cepts are generated automatically for each utter-
ance with a parsable orthographic transcription as
a series of keyword-value pairs. The higher the
QD, the more effectively a user is able to commu-
nicate information to the system. The CE is an in-
dicator of recognition or understanding errors; the
higher it is, the fewer times a user needs to repeat
himself. These metrics were evaluated on single
systems (JUPITER and and MERCURY); to com-
pare different systems of the same type, one would
need a common ontology. In (Glass et al, 2000),
the authors believe that CE should be related to
user frustation, but to show it they would need to
use the PARADISE framework.
PARADISE (Walker et al, 1998) can be seen
as a sort of meta-paradigm which correlates ob-
jective and subjective measurements. Its ground-
ing hypothesis states that the goal of any SLDS is
to achieve user-satisfaction, which in turn can be
predicted through task success and various interac-
tion costs. With the help of the kappa coefficient
(Carletta, 1996) proposes to represent the dialog
success independently from the task intrinsic com-
plexity, thus opening the way to task generic com-
parative evaluation. PARADISE has been tested
in the COMMUNICATOR project (Walker et al,
2001) with 9 systems working on the same task
over different databases. With four basic measures
(e.g. task completion) the protocol has been able
to predict 37% of user satisfaction variation, and
42% with the help of a few extra measurements on
dialog acts and subtasks. One critic, one can make
about PARADISE concern its cost (real user tests
are costly) and the use of subjective assessment.
The adaption of the DQR text understanding
evaluation methodology (Sabatier et al, 2000) to
speech resulted in a generic and qualitative proce-
dure. Each element of its test set holds three parts,
the Declaration to define the context, a Question
which bears on point present in the context and the
Response. The test set is organized through seven
levels of test, from basic explicit understanding
to semantic interpretation and reply pertinence as-
sessment. This protocol is task and system generic
but test set construction is not straightforward and
the bias introduced by the wording of the question
is difficult to assess.
Recently the GDR-13 work group of CNRS
on spoken dialog understanding, has proposed an
evaluation methodology for literal understanding.
According to (Antoine and al., 2002), DEFI tries
to remedy two important weaknesses of the MAD-
COW methodology, namely the lack of genericity
and the lack of diagnostic information, by craft-
ing system specific test sets from a primary set of
enunciations representative of the task (provided
by the developers). Secondary enunciations are
then derived from the primary ones in order to ex-
hibit particular language phenomena. Afterwards,
the systems are evaluated by their developers us-
ing specific test set and their own metrics. The
various results can be mapped over a generic ab-
stract architecture for comparison (although this
mapping is still unspecified at the time of writ-
ing). DEFI has already been used in one evalua-
tion campaign, with 5 systems presented by 4 lab-
oratories. (Antoine and al., 2002) has reported the
following weaknesses of the protocol: how to con-
trol the bias introduced by the derivation of enun-
ciations, how to guaranty that derived enunciation
will remain in the task scope (this prevented some
system from being evaluated over the complete
test set) and finally how to restrict and organize
the language phenomena used in the test set.
3 The PEACE paradigm
We first describe the paradigm and relate prelim-
inary experiments with PEACE. This paradigm
which is as basement for the MEDIA project will
be refined by all the partners and use for an evalua-
tion campaign between seven systems of industrial
and academic sites.
3.1 Description
The PEACE paradigm relies on the idea that for
database querying tasks, it is possible to define a
common semantic representation, onto which all
the systems are able to convert their own repre-
sentation (Moore, 1994). The paradigm based on
data extracted from real corpus, includes both lit-
eral and contextual understanding test sets. More
precisely, it provides:
 the definition of a semantic representation
(see 3.1.1),
 the definition of a model for dialogic contexts
(see 3.1.2),
 the definition and typology of linguistic phe-
nomena and dialogic functions used to selec-
tively diagnoze the system language capabil-
ities (anaphora resolution, constraints relax-
ation, etc.) (see 3.1.3),
 a data structuring method. The format of the
annotated data will be adapted to language
resource standard annotations implemented
(see 3.1.4),
 and evaluation metrics with the correspond-
ing evaluation tool (see 3.1.5).
3.1.1 Generic semantic representation
The difficulty of choosing a semantic represen-
tation lies in finding a complete and simple repre-
sentation of a user utterance meaning in a unified
format. A frame Attribute Value Representation
(AVR) has been chosen, allowing a fast and re-
liable annotation. The values are either numeric
units, proper names, or semantic classes, that
group together lexical units which are synonyms
for the task. The order of the (attribute, value)
pairs in the semantic representation matches their
respective position in the utterance. A modal in-
formation (positive (+) and negative(-)) is also as-
signed to each (attribute, value) pair. The semantic
representation of an utterance consists then in a list
of triplets of the form (mode, attribute, normalized
value). An example is given in figure 1. In order
to take into account for long-time dependencies or
to allow multiple referenced objects, the semantic
representation may be enriched by adding a refer-
ence value to each triplet for the representation of
links between 2 attributes of the utterance.
Attributes can grouped into different classes:
 the database attributes (the most frequent)
correspond to the attributes of the database
tables (e.g. category for an hotel);
 the modifier attributes are associated to
the database concepts. Their values are
used to modify the database concept in-
terpretation values (e.g. the attribute
category-modifier with possible val-
ues: >; <, =, Max, Min);
 the discursive attributes are introduced to
handle various aspects of dialogic interaction
User c?est pas Paris c?est Passy
Query it is not Paris it is Passy
(LU) AVR (-, place, Paris)
(+, place, Passy)
Figure 1: Example of a semantic representation of an ut-
terance with positive and negative information for the ARISE
task. Place is an database attribute,Paris and Passy are
values and +/- modal markers.
(e.g. commandwith values cancelation, cor-
rection, error specification: : :, or response
with values yes or no);
 the argument attribute which represents the
topic at the focus of the utterance.
When dealing with information retrieval appli-
cations, defining the database and modifier at-
tributes and the appropriate values can be done
in a rather straightforward way. Most of those
attributes are derived directly from the informa-
tion stored in the database. Furthermore, most of
the discursive attributes are domain-independent.
Some database attributes remain unchanged across
many tasks, such as those dealing with dates or
prices.
This semantic representation has been used at
LIMSI for PARIS-SITI TASK (touristic informa-
tion) and ARISE TASK (traintable information)
both with triplet representation. More recently in
the context of the AMITIES project, quadruplets
were used.
3.1.2 Contextual understanding modeling
Contextual understanding evaluation provides
information about the capability of the system
to take into account the dialog history in order
to properly interpret the user query. Contextual
understanding evaluation is rarely performed be-
cause of the dynamic nature of the dialog make
the dialog context depend on the system?s dialog
strategy.
Nevertheless PEACE proposes a system-
independent way to evaluate local contextual
interpretation. Given U
1
:::U
t
the user inter-
actions, and S
1
:::S
t
the answers of the agent
or system, the context a time t is a function
f(U
1
; S
1
; U
2
; S
2
; :::U
t
; S
t
). In the PEACE
paradigm, a paraphrase of the context is derived
from the semantic representation (Bonneau-
Maynard et al, 2000).
The dialog contexts are extracted from real di-
alogs in three steps. First, the internal semantic
frames representing the dialog contexts are auto-
matically extracted from the log files of the ses-
sion recordings. Secondly, the semantic frames
are converted into AVR format and then hand-
corrected to faithfully represent the dialog history.
The last step consists in the writing of a sentence
for each context (the context paraphrase), which
results in the same AVR representation as the one
of the dialog context.
Two possibilities may be investigated for build-
ing the paraphrase from the internal semantic rep-
resentation of the dialog context. A rule-based or
template-based natural language generation mod-
ule can be used to automatically produce the para-
phrase. The paraphrase can also be obtained
by concatenating the sentences preceding the ex-
tracted dialog state. In both cases, a manual veri-
fication is needed.
3.1.3 A typology of linguistic phenomena and
dialogic functions
For dialog system evaluation, it is essential to
build test sets randomly extracted from real cor-
pus. For dialog system diagnosis, it is also crucial
to build test sets labeled with the linguistic phe-
nomena and dialogic functions. Thus, the capabil-
ities of system?s contextual understanding can be
assessed for the main linguistic and dialogic dif-
ficulties such as, for instance, anaphora or ellipsis
resolution.
3.1.4 A data structuring method
Two types of units, one for literal understanding
(LU), the other for contextual understanding (CU)
are defined. The format of the annotated data will
be adapted to language resource standard annota-
tions implemented in XML, e.g. (Geoffrois et al,
2000), (Ide and Romary, 2002).
Each unit is extracted from a real dialog cor-
pus. LU units are composed of the user query,
the corresponding audio signal, an automatic tran-
scription obtained with a recognition system, and
finally the literal semantic representation of the ut-
terance (see Figure 1). CU units are composed of
Context je voudrais un ho?tel 4
paraphrase e?toiles dans le neuvie`me
I would like a 4 category
hotel in the ninth
(LU) AVR (+, argument, hotel)
(+, district, 9)
(+, category, 4)
User la me?me cate?gorie dans
query un autre arrondissement
the same category in
another district
(LU) AVR (+, other, district)
(+, same, category)
(CU) AVR (+, argument, hotel)
(-, district, 9)
(+, category, 4)
Figure 2: Example of a contextual understanding unit com-
posed of a context paraphrase, a user query and the resulting
AVR. AVR of context paraphrase and user query are given in
TYPEWRITING MODE. Ellipsis (?in the ninth?) and anaphora
(?same category?, ?another district?) may be observed.
the dialog context (given by the paraphrase), the
user query and the resulting AVR of the user query
in the given context (see Figure 2). Those units are
also labeled with linguistic and dialogic phenom-
ena.
3.1.5 Evaluation metrics and scoring tool
Common evaluation metrics are essential for
analyzing the system capabilities. The scoring tool
for AVR comparison is able to compare between
two AVR frame representation sets. For evalu-
ation, system outputs translated in AVR format
composed one set, the other one contains the AVR
references which are manually annotated. Both
frame sets have the form of a list of AVRs (fixed
length records). Each record is composed of three
or four fields (mode, attribute, value, reference).
The comparison consists in applying a set of pre-
defined operators each assigned with a cost value.
The comparison process looks for operator lists
to be applied to the test frame in order to obtain
the reference frame that minimizes the final cost
value. For a global evaluation, the classical opera-
tors from speech evaluation (DELetion, INSertion
and SUBstitution) may be used (as used for first
two values of Accuracy percentage in Table 1).
With our scoring tool the definition of new opera-
tors is quite easy. It is then also possible to distin-
guish between different types of errors by defining
specific operators (as used to estimate Topic iden-
tification in Table 1), or by using different cost val-
ues (for example a substitution is often considered
more costly for dialog management).
3.2 Example use of PEACE
In order to validate the evaluation paradigm, a
set of approximatively 1,700 literal units and a
set of 100 contextual units has been used for
the PARIS-SITI task (Bonneau-Maynard and Dev-
illers, 2000). Results for both literal and contex-
tual understanding test sets are given in Table 1. In
order to observe the ability of the systems to deal
with recognition errors, each literal understand-
ing unit also contains the ASR transcription of the
original user utterance. The various measures of
understanding accuracy are computed as the ratio
between the sum of the number of deleted, inserted
and substituted attributes, and the total number of
AVR attributes in the test set. The possibility of
an automatic evaluation of the LU accuracy and
the ability of the scoring tool to point out the er-
rors allowed us to easily improve the literal un-
derstanding accuracy from 89.0% to 93.5%. Due
to a 26.5% ASR error rate, the LU accuracy goes
down from 93.5% to 72% after ASR transcription.
The contextual understanding accuracy on the 100
test units is 82.6% on exact transcription. For
instance, anaphoric references are relatively well
solved, with 80.4% accuracy on the 50 units con-
taining at least one anaphoric reference. For each
example, the anaphoric referenced object is gen-
erally correctly identified and remaining errors are
often due to a bad history constraint management.
3.3 Discussing the PEACE paradigm
The PEACE paradigm enables automatic evalua-
tion of literal and contextual dialog understand-
ing. The evaluation paradigm makes the distinc-
tion between different types of errors, allowing a
qualitative and diagnostic analysis of the perfor-
mances of a speech understanding module. Very
few evaluation paradigms propose automatic di-
agnosis of contextual interpretation (Glass et al,
2000). The proposed methodology is based on
#Units #Attr. %Acc. Prec.
LU exact 1 681 3 991 93.5% 0.7
LU ASR . 1 681 3 991 72.0% 1.4
Topic id. 680 833 94.3% 1.6
Modifier id. 323 445 95.7% 1.9
CU exact 100 430 86.8% 3.2
Anaphoric 50 245 84.4% 4.5
resolution
Ellipsis 25 106 85.3% 6.7
resolution
Table 1: Literal understanding (LU) accuracy on both exact
and ASR transcription, and contextual understanding (CU)
accuracy. Second column indicates the number of units in-
cluded in the test set (i.e # of user utterances), third col-
umn gives the total number of attributes in the correct AVR
test sets. Details, using specific operators, are given for
argument (topic) and modifier identification for LU on
exact transcription, and for anaphoric reference and ellipsis
resolution for CU. Last column gives the 95% precision of
the accuracy estimation (Montacie? and Chollet, 1997)
.
semi-automatically built reference test sets, and
therefore is much more time effective than manual
evaluation. Furthermore, it provides reproducible
tests.
Although the semantic representation is task de-
pendent, the example described above shows the
feasibility of the paradigm for any dialog system
interfacing to a database. Robustness to many
linguistic phenomena such as repetitions, hesita-
tions or auto-corrections may be evaluated with
this method. XML coding will facilitate the gener-
icity and the reusability of the test sets, by al-
lowing the selection of the dialogic contexts to be
studied.
The representation of the dialog context with a
single paraphrase, derived from a ?flat? structured
AVR, may have some limitations in case of long-
time dialog dependencies. It does not allow for
memorizing all the steps of the dialog. For ex-
ample, if the speaker says first ?I would like a
2 star hotel?, then ?no I prefer 3 stars? and fi-
nally says ?give me again my first choice?, the
CU unit cannot take into account this succession
of queries. However, this kind of interaction is
rarely observed in dialogue corpora: the user usu-
ally repeats the constraint value (?give me again
a 2 star hotel?). To represent more precisely the
dialog state, the representation of the dialog con-
text should incorporate some meta-information in-
spired for example from the DAMSL annotation
standard 1 (Devillers et al, 2002b).
Another point is the representativity of the test
sets. This may be considered as a limitation as
far as PEACE paradigm is built on the idea that
the test units are extracted from real dialogs. Ob-
viously, the larger the test sets are, the better. A
diagnostic evaluation may need a very large test
corpora to validate system performance against the
wide range of phenomena present in spontaneous
dialog.
The ability to automatically diagnose the per-
formances of contextual understanding modules
on local difficulties such as ellipsis, negations,
anaphoric reference or constraint relaxation is one
of the major advantages of the PEACE paradigm,
which has not been investigated by other method-
ologies. This is why it has been chosen for the
MEDIA project described in the next section.
4 The MEDIA project
The MEDIA project proposes a paradigm based
on a reference task and on test sets extracted from
real corpora for evaluating literal and contextual
understanding in dialog systems. The PEACE
paradigm will serve as basis for the MEDIA
project. The consortium is composed of IRIT,
LIA, LIMSI, LORIA, VALORIA for the French
academic sites and France Telecom R&D and
TELIP for the industrial sites. The scientific com-
mittee contains representatives of AT&T (USA),
Tilburg University (Netherlands), IBM, IMAG,
LIUM and VECSYS (France).
The project has four main parts. First, the selec-
tion of reference task such as for example a task
of web-based travel agency. The reference task
has to correspond to a real-life application allow-
ing real user tests. Secondly, multi-level represen-
tation such as the semantic representation, the ty-
pology of linguistic phenomena and dialogic func-
tions, the dialog context model... will be com-
monly refined and adapted to the reference task.
The third part deals with the recording and la-
beling of a dialog corpus which will be used for
1http://www.cs.rochester.edu/research/trains/annotation
both system adaptation and test set selection. The
last part is the organisation of the evaluation cam-
paigns by ELRA/ELDA for the participating sites.
ELRA/ELDA is the coordinator of a larger
scope project: EVALDA which includes among
others, the MEDIA project. ELDA with VEC-
SYS will provide transcribed and annotated cor-
pora and evaluation tools according to consortium
specifications. The recording of 1200 French di-
alogs (240 speakers, 5 dialogs each, 15k user
queries) is planned. Three sets of LU and CU
units will be built from this corpus. A large size
adaptation set will be used by the participants to
adapt their system to the task and the semantic
representation. The development set (around 1K
LU (resp. CU) units) will be used to validate the
evaluation protocole. The size of the test set is
planned to be around 3K LU (resp. CU) units. Var-
ious approaches are currently used at the partici-
pating sites; stochastic or syntactic and semantic
rule-based modeling. The project started in Jan-
uary 2003 and will last two years.
5 Conclusion
Assessing the dialog system understanding capa-
bilities requires to evaluate the transition between
successive states of the dialog. At least, we must
be able to test a sequence of two states at any
point in the dialog. The dynamic and interac-
tive nature of the dialog makes construction and
reuse of test sets difficult. Furthermore, to eval-
uate one particular dialog transition, the system
has to be put in a particular state corresponding to
the original dialog context. The variable describ-
ing the dialog state can be composed of complex
information such as the current semantic frame
(list of triplets (mode,attribute,value) or quadru-
ples (mode, attribute, value, reference)), the dialog
history semantic frame and potentially other infor-
mation like recognition scores, dialog acts, etc.
The PEACE paradigm allows the evaluation of
two successive simplified dialog states. It has been
successfully tested with test samples focusing on
linguistic difficulties of literal and contextual un-
derstanding. For these tests, the dialog state is
the dialog history semantic frame. The contextual
understanding modeling in PEACE is system inde-
pendent since the context is given by a paraphrase
of queries. PEACE allows a diagnostic evaluation
of specific semantic attributes and particular lin-
guistic phenomena.
In our opinion, it is crucial for the dialog com-
munity to agree on a common reference task and
reference test sets in order to be able to compare
and diagnose dialog systems. Both evaluation with
real users and artificial simulation of successive
dialog states using test sets extracted from real cor-
pora have to be carried out in parallel. The use of
test sets reduces the global cost of dialog system
evaluation, moreover such tests are reproducible.
The PEACE protocol will be used as basis for
the French Technolangue MEDIA project in a two
year evaluation campaign where dialog systems
from both academia and industry will be evalu-
ated. In other domains, it could be related with
(Hirschman, 2000) propositions for Question An-
swering evaluation.
References
J.Y. Antoine and al. 2002. Predictive and objective evalua-
tion of speech understanding: the ?challenge? evaluation
campaign of the i3 speech workgroup of th french cnrs. In
LREC2002, Spain, May. ELRA.
H. Bonneau-Maynard and L. Devillers. 2000. A framework
for evaluating contextual understanding. In ICSLP.
H. Bonneau-Maynard, L. Devillers, and S. Rosset. 2000.
Predictive performance of dialog systems. In LREC2000,
volume 1, pages 177?181, Athens, Greece, May. ELRA.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistics. Computational Linguistics,
2(22):249?254.
R. Winski D. Gibbon, R. Moore. 1997. Handbook of Stan-
dards and Ressources for Spoken Language Ressources.
Mouton de Gruyter, New York.
L. Devillers, H. Maynard, and P. Paroubek. 2002a.
Me?thodologies d?e?valuation des syste`mes de dia-
logue parle? : re?flexions et expe?riences autour de la
compre?hension. In Traitement Automatique des Langues,
volume 43, pages 155?184.
L. Devillers, S. Rosset, H. Bonneau-Maynard, and L. Lamel.
2002b. Annotations for dynamic diagnosis of the dialog
state. In LREC2002, Spain, May. ELRA.
L. Dybkj?r and al. 1998. The disc approach to spo-
ken language systems development and evaluation. In
LREC1998), volume 1, pages 185?189, Spain, May.
ELRA.
N. Fraser. 1998. Spoken Language System Assessment, vol-
ume 3. Mouton de Gruyter, New York.
E. Geoffrois, C. Barras, S. Bird, and Z. Wu. 2000. Tran-
scribing with annotation graphs. In LREC2000, volume 2,
pages 1517?1521, Greece, May. ELRA.
J. Glass, J. Polifroni, S. Seneff, and V. Zue. 2000. Data
collection and performance evaluation of spoken dialogue
systems: the MIT experience.
Lynette Hirschman. 2000. Reading comprehension and
question answering new evaluation paradigms for human
language technology. In LREC2000 Workshop ?Using
Evaluation within HLT Programs: Results and Trends?,
pages 54?59, Greece, May. ELRA.
N. Ide and L. Romary. 2002. Towards multimodal content
representation. In LREC 2002.
L. Lamel. 1998. Spoken language dialog system develop-
ment and evaluation at limsi. In Actes de l?International
Symposium on Spoken Dialogue, Sydney, Australia,
November.
MADCOW. 1992. Multi-site data collection for a spoken
language corpus. In DARPA Speech and Natural Lan-
guage Workshop.
C. Montacie? and G. Chollet. 1997. Syste`mes de re?fe?rence
pour l?e?valuation d?applications et la caracte?risation de
bases de donne?es en reconnaissance de la parole. In
16e`me JEP.
R.C. Moore. 1994. Semantic evaluation for spoken-language
systems. In DARPA Speech and Natural Language Work-
shop.
P. Sabatier, Ph. Blache, J. Guizol, F. Le?vy, A. Nazarenko,
and S. N?Guema. 2000. e?valuer des syste`mes de
compre?hension de textes. In Ressources et Evaluation en
Inge?nierie Linguistique, pages 265?275. Chibout K. et al
(Eds) Duculot.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Eval-
uating spoken dialogue agents with paradise: 2 cases stud-
ies. Computer Speech and Language, 3(12):317?347.
M. Walker, R. Passonneau, and J.E. Boland. 2001. Quantita-
tive and qualitative evaluation of darpa communicatorspo-
ken dialog systems. In Actes du 39me ACL, pages 515?
522, Toulouse, France, July. ACL.
S. Whittaker, L. Terveen, and B. Nardi. 2002. Reference task
agenda for HCI. In ISLE workshop 2002.
Proceedings of the Fifth Law Workshop (LAW V), pages 92?100,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Proposal for an Extension of Traditional Named Entities:
From Guidelines to Evaluation, an Overview
Cyril Grouin?, Sophie Rosset?, Pierre Zweigenbaum?
Kar?n Fort?,? , Olivier Galibert?, Ludovic Quintard?
?LIMSI?CNRS, France ?INIST?CNRS, France ?LIPN, France ?LNE, France
{cyril.grouin,sophie.rosset,pierre.zweigenbaum}@limsi.fr
karen.fort@inist.fr, {olivier.galibert,ludovic.quintard}@lne.fr
Abstract
Within the framework of the construction of a
fact database, we defined guidelines to extract
named entities, using a taxonomy based on an
extension of the usual named entities defini-
tion. We thus defined new types of entities
with broader coverage including substantive-
based expressions. These extended named en-
tities are hierarchical (with types and compo-
nents) and compositional (with recursive type
inclusion and metonymy annotation). Human
annotators used these guidelines to annotate a
1.3M word broadcast news corpus in French.
This article presents the definition and novelty
of extended named entity annotation guide-
lines, the human annotation of a global corpus
and of a mini reference corpus, and the evalu-
ation of annotations through the computation
of inter-annotator agreements. Finally, we dis-
cuss our approach and the computed results,
and outline further work.
1 Introduction
Within the framework of the Quaero project?a mul-
timedia indexing project?we organized an evalu-
ation campaign on named entity extraction aiming
at building a fact database in the news domain, the
first step being to define what kind of entities are
needed. This campaign focused on broadcast news
corpora in French. While traditional named enti-
ties include three major classes (persons, locations
and organizations), we decided to extend the cov-
erage of our campaign to new types of entities and
to broaden their main parts-of-speech from proper
names to substantives, this extension being neces-
sary for ever-increasing knowledge extraction from
documents. We thus produced guidelines to specify
the way corpora had to be annotated, and launched
the annotation process.
In this paper, after covering related work (Sec-
tion 2), we describe the taxonomy we created (Sec-
tion 3) and the annotation process and results (Sec-
tion 4), including the corpora we gathered and the
tools we developed to facilitate annotation. We then
present inter-annotator agreement measures (Sec-
tion 5), outline limitations (Section 6) and conclude
on perspectives for further work (Section 7).
2 Related work
2.1 Named entity definitions
Named Entity recognition was first defined as recog-
nizing proper names (Coates-Stephens, 1992). Since
MUC-6 (Grishman and Sundheim, 1996; SAIC,
1998), named entities have been proper names
falling into three major classes: persons, locations
and organizations.
Proposals were made to sub-divide these entities
into finer-grained classes. The ?politicians? sub-
class was proposed for the ?person? class by (Fleis-
chman and Hovy, 2002) while the ?cities? subclass
was added to the ?location? class by (Fleischman,
2001; Lee and Lee, 2005).
The CONLL conference added a miscellaneous
type that includes proper names falling outside the
previous classes. Some classes have thus sometimes
been added, e.g. the ?product? class by (Bick, 2004;
Galliano et al, 2009).
92
Specific entities are proposed and handled in
some tasks: ?language? or ?shape? for question-
answering systems in specific domains (Rosset et
al., 2007), ?email address? or ?phone number? to
process electronic messages (Maynard et al, 2001).
Numeric types are also often described and used.
They include ?date?, ?time?, and ?amount? types
(?amount? generally covers money and percentage).
In specific domains, entities such as gene, protein,
are also handled (Ohta, 2002), and campaigns are or-
ganized for gene detection (Yeh et al, 2005). At the
same time, extensions of named entities have been
proposed: (Sekine, 2004) defined a complete hierar-
chy of named entities containing about 200 types.
2.2 Named Entities and Annotation
As for any other kind of annotation, some aspects are
known to lead to difficulties in obtaining coherence
in the manual annotation process (Ehrmann, 2008;
Fort et al, 2009). Three different classes of prob-
lems are distinguished: (1) selecting the correct cat-
egory in cases of ambiguity, where one entity can
fall into several classes, depending on the context
(?Paris? can be a town or a person name); (2) detect-
ing the boundaries (in a person designation, is only
the proper name to be annotated or the trigger ?Mr?
too?) and (3) annotating metonymies (?France? can
be a sports team, a country, etc.).
In the ACE Named Entity task (Doddington et al,
2004), a complex task, the obtained inter-annotator
agreement was 0.86 in 2002 and 0.88 in 2003. Some
tasks obtain better agreement. Desmet and Hoste
(2010) described the Named Entity annotation real-
ized within the Sonar project, where Named Entity
are clearly simpler. They follow the MUC Named
Entity definition with the subtypes as proposed
by ACE. The agreement computed over the Sonar
Dutch corpus ranges from 0.91 to 0.97 (kappa val-
ues) depending of the emphasized elements (span,
main type, subtype, etc.).
3 Taxonomy
3.1 Guidelines production
Having in mind the objective of building a fact
database through the extraction of named entities
from texts, we defined a richer taxonomy than those
used in other information extraction works.
Following (Bonneau-Maynard et al, 2005; Alex
et al, 2010), the annotation guidelines were first
written from December 2009 to May 2010 by three
researchers managing the manual annotation cam-
paign. During guidelines production, we evaluated
the feasibility of this specific annotation task and the
usefulness of the guidelines by annotating a small
part of the target corpus. Then, these guidelines
were delivered to the annotators. They consist of a
description of the objects to annotate, general anno-
tation rules and principles, and more than 250 pro-
totypical and real examples extracted from the cor-
pus (Rosset et al, 2010). Rules are important to set
the general way annotations must be produced. Ad-
ditionally, examples are essential for human annota-
tors to grasp the annotation rationale more easily.
Indeed, while producing the guidelines, we knew
that the given examples would never cover all possi-
ble cases because of the specificity of language and
of the ambiguity of formulations and situations de-
scribed in corpora, as shown in (Fort et al, 2009).
Nevertheless, guidelines examples must be consid-
ered as a way to understand the final objective of
the annotation work. Thanks to numerous meetings
from May to November 2010, we gathered feedback
from the annotators (four annotators plus one anno-
tation manager). This feedback allowed us to clarify
and extend the guidelines in several directions. The
guidelines are 72 pages long and consist of 3 major
parts: general description of the task and the prin-
ciples (25% of the overall document), presentation
of each type of named entity (57%), and a simpler
?cheat sheet? (18%).
3.2 Definition
We decided to use the three general types of
named entities: name (person, location, organi-
zation) as described in (Grishman and Sundheim,
1996; SAIC, 1998), time (date and duration), and
quantity (amount). We then included named entities
extensions proposed by (Sekine, 2004; Galliano et
al., 2009) (respectively products and functions) and
we extended the definition of named entities to ex-
pressions which are not composed of proper names
(e.g., phrases built around substantives). The ex-
tended named entities we defined are both hierar-
chical and compositional. For example, type pers
(person) is split into two subtypes, pers.ind (indi-
93
Person Function
pers.ind (individual
person)
pers.coll (group of
persons)
func.ind (individual
function)
func.coll (collectivity
of functions)
Location Product
administrative
(loc.adm.town,
loc.adm.reg,
loc.adm.nat,
loc.adm.sup)
physical
(loc.phys.geo,
loc.phys.hydro,
loc.phys.astro)
facilities
(loc.fac),
oronyms
(loc.oro),
address
(loc.add.phys,
loc.add.elec)
prod.object
(manufac-
tured object)
prod.serv
(transporta-
tion route)
prod.fin
(financial
products)
prod.doctr
(doctrine)
prod.rule
(law)
prod.soft
(software)
prod.art prod.media prod.award
Organization Time
org.adm (administra-
tion)
org.ent (services)
Amount
amount (with unit or general object), includ-
ing duration
time.date.abs
(absolute date),
time.date.rel (relative
date)
time.hour.abs
(absolute hour),
time.hour.rel (relative
hour)
Table 1: Types (in bold) and subtypes (in italic)
vidual person) and pers.coll (collective person), and
pers entities are composed of several components,
among which are name.first and name.last.
3.3 Hierarchy
We used two kinds of elements: types and compo-
nents. The types with their subtypes categorize a
named entity. While types and subtypes were used
before (ACE, 2000; Sekine, 2004; ACE, 2005; Gal-
liano et al, 2009), we consider that structuring the
contents of an entity (its components) is important
too. Components categorize the elements inside a
named entity.
Our taxonomy is composed of 7 main types
(person, function, location, product, organization,
amount and time) and 32 subtypes (Table 1). Types
and subtypes refer to the general category of a
named entity. They give general information about
the annotated expression. Almost each type is then
specified using subtypes that either mark an opposi-
tion between two major subtypes (individual person
vs. collective person), or add precisions (for exam-
ple for locations: administrative location, physical
location, etc.).
This two-level representation of named entities,
with types and components, constitutes a novel ap-
proach.
Types and subtypes To deal with the intrinsic am-
biguity of named entities, we defined two specific
transverse subtypes: 1. other for entities with a dif-
ferent subtype than those proposed in the taxon-
omy (for example, prod.other for games), and 2. un-
known when the annotator does not know which sub-
type to use.
Types and subtypes constitute the first level of an-
notation. They refer to a general segmentation of
the world into major categories. Within these cate-
gories, we defined a second level of annotation we
call components.
Components Components can be considered as
clues that help the annotator to produce an anno-
tation: either to determine the named entity type
(e.g. a first name is a clue for the pers.ind named
entity subtype), or to set the named entity bound-
aries (e.g. a given token is a clue for the named en-
tity, and is within its scope, while the next token is
not a clue and is outside its scope). Components are
second-level elements, and can never be used out-
side the scope of a type or subtype element. An en-
tity is thus composed of components that are of two
kinds: transverse components and specific compo-
nents (Table 2). Transverse components can be used
in several types of entities, whereas specific compo-
nents can only be used in one type of entity.
94
Transverse components
name (name of the entity), kind (hyperonym of the entity), qualifier (qualifying adjective), demonym
(inhabitant or ethnic group name), demonym.nickname (inhabitant or ethnic group nickname), val
(a number), unit (a unit), extractor (an element in a series), range-mark (range between two values),
time-modifier (a time modifier).
pers.ind loc.add.phys time.date.abs/rel amount
name.last, name.first,
name.middle, pseudonym,
name.nickname, title
address-number, po-box,
zip-code,
other-address-component
week, day, month, year,
century, millennium,
reference-era
object
prod.award
award-cat
Table 2: Transverse and specific components
3.4 Composition
Another original point in this work is the compo-
sitional nature of the annotations. Entities can be
compositional for three reasons: (i) a type contains a
component; (ii) a type includes another type, used as
a component; and (iii) in cases of metonymy. Dur-
ing the Ester II evaluation campaign, there was an
attempt to use compositionality in named entities for
two categories: persons and functions, where a per-
son entity could contain a function entity.
<pers.hum> <func.pol> pr?sident </func.pol>
<pers.hum> Chirac </pers.hum> </pers.hum>
Nevertheless, the Ester II evaluation did not take
this inclusion into account and only focused on
the encompassing annotation (<pers.hum> pr?sident
Chirac </pers.hum>). We drew our inspiration from
this experience, and allowed the annotators and the
systems to use compositionality in the annotations.
Cases of inclusion can be found in the function
type (Figure 1), where type func.ind, which spans
the whole expression, includes type org.adm, which
spans the single word ?budget?. In this case, we con-
sider that the designation of this function (?ministre
du budget?) includes both the kind (?ministre?) and
nouveau
qualifier
ministre
kind
du Budget
name
org.adm
func.ind
, Fran?ois
name.first
Baroin
name.last
pers.ind
Figure 1: Multi-level annotation of entity types (red tags)
and components (blue tags): new minister of budget ,
Fran?ois Baroin.
the name (?budget?) of the ministry, which itself is
typed as is relevant (org.adm). Recursive cases of
embedding can be found when a subtype includes
another named entity annotated with the same sub-
type (org.ent in Figure 2).
le collectif
kind
des associations
kind
des droits de l' Homme
name
prod.rule
au Sahara
name
loc.phys.geo
loc.adm.sup
org.ent
org.ent
Figure 2: Recursive embedding of the same subtype:
Collective of the Human Rights Organizations in Sahara.
Cases of metonymy include strict metonymy (a
term is substituted with another one in a relation
of contiguity) and antonomasia (a proper name is
used as a substantive or vice versa). In such cases,
the entity must be annotated with both types, first
(inside) with the intrinsic type of the entity, then
(outside) with the type that corresponds to the re-
sult of the metonymy. Basically, country names
correspond to ?national administrative? locations
(loc.adm.nat) but they can also designate the admin-
istration (org.adm) of the country (Figure 3).
depuis
time-modifier
plusieurs
val
mois
unit
amount
time.date.rel
, la Russie
name
loc.adm.nat
org.adm
Figure 3: Annotation with a metonymic use of country
?Russia? as its government: for several months , Russia...
95
3.5 Boundaries
Our definition of the scope of entities excludes rel-
ative clauses, subordinate clauses, and interpolated
clauses: the annotation of an entity must end before
these clauses. If an interpolated clause occurs inside
an entity, its annotation must be split. Moreover, two
distinct persons sharing the same last name must be
annotated as two separate entities (Figure 4); we in-
tend to use relations between entities to gather these
segments in the next step of the project.
depuis
utmi-oefrl
vifr-eua
il n.s,etui
utmi-oefrl
Rprveu
utmi-strl
vifr-eua
Figure 4: Separate (coordinated) named entities.
4 Annotation process
4.1 Corpus
We managed the annotation of a corpus of about one
hundred hours of transcribed speech from several
French-speaking radio stations in France and Mo-
rocco. Both news and entertainment shows were
transcribed, including dialogs, with speaker turns.1
Once annotated, the corpus was split into a de-
velopment corpus: one file from a French radio sta-
tion;2 a training corpus: 188 files from five French
stations3 and one Moroccan station;4 and a test cor-
pus: 18 files from two French stations already stud-
ied in the training corpus5 and from unseen sources,
both radio6 and television,7 in order to evaluate the
robustness of systems. These data have been used in
the 2011 Quaero named entity evaluation campaign.
1Potential named entities may be split across several seg-
ments or turns.
2News from France Culture.
3News from France Culture (refined language), France Info
(news with short news headlines), France Inter (generalist radio
station), Radio Classique (classical music and economic news),
RFI (international radio broadcast out of France).
4News from RTM (generalist French speaking radio).
5News from France Culture, news and entertainment from
France Inter.
6A popular entertainment show from Europe 1.
7News from Arte (public channel with art and culture),
France 2 (public generalist channel), and TF1 (private gener-
alist popular channel).
This corpus allows us to perform different evalua-
tions, depending of the knowledge the systems have
of the source (source seen in the training corpus vs.
unseen source), the kind of show (news vs. enter-
tainment), the language style (popular vs. refined),
and the type of media (radio vs. television).
4.2 Tools for annotators
To perform our test annotations (see Section 2.2),
we developed a very simple annotation tool as an in-
terface based on XEmacs. We provided the human
annotators with this tool and they decided to use it
for the campaign, despite the fact that it is very sim-
ple and that we told them about other, more generic,
annotation tools such as GATE8 or Glozz.9 This is
probably due to the fact that apart from being very
simple to install and use, it has interesting features.
The first feature is the insertion of annotations
using combinations of keyboard shortcuts based on
the initial of each type, subtype and component
name. For example, combination F2 key + initial
keys is used to annotate a subtype (pers.ind, etc.),
F3 + keys for a transverse component (name, kind,
etc.), F4 + keys for a specific component (name.first,
etc.), and F5 to delete the annotation selected with
the cursor (both opening and closing tags).
The second feature is boundary management: if
the annotator puts the cursor over the token to anno-
tate, the annotation tool will handle the boundaries
of this token; opening and closing tags will be in-
serted around the token.
However, it presents some limitations: tags are
inserted in the text (which makes visualization more
complex, especially for long sentences or in cases
of multiple annotations on the same entity), no per-
sonalization is offered (tags are of only one color),
and there is no function to express annotator uncer-
tainty (the user must choose among several possible
tags the one that fits the best;10 while producing the
guidelines, we did not consider it could be of inter-
est: as a consequence, no uncertainty management
was implemented). Therefore, this tool allows users
to insert tags rapidly into a text, but it offers no exter-
nal resources, as real annotation tools (e.g. GATE)
often do.
8http://gate.ac.uk/
9http://www.glozz.org/
10Uncertainty can be found in cases of lack of context.
96
These simplistic characteristics combined with a
fast learning curve allow the annotators to rapidly
annotate the corpora. Annotators were allowed not
to annotate the transverse component name (only if
it was the only component in the annotated phrase,
e.g. ?Russia? in Figure 3, blue tag) and to annotate
events, even though we do not focus on this type
of entity as of yet. We therefore also provided a
normalization tool which adds the transverse com-
ponent name in these instances, and which removes
event annotations.
4.3 Corpus annotation
Global annotation It took four human annotators
two months and a half to annotate the entire corpus
(10 man-month). These annotators were hired grad-
uate students (MS in linguistics). The overall corpus
was annotated in duplicate. Regular comparisons of
annotations were performed and allowed the anno-
tators to develop a methodology, which was subse-
quently used to annotate the remaining documents.
Mini reference corpus To evaluate the global an-
notation, we built a mini reference corpus by ran-
domly selecting 400 sentences from the training cor-
pus and distributing them into four files. These files
were annotated by four graduate human annotators
from two research institutes (Figure 5) with two hu-
mans per institute, in about 10 hours per annotator.
	








Figure 5: Creation of mini reference corpus and compu-
tation of inter-annotator agreement. Institute 1 = LIMSI?
CNRS, Institute 2 = INIST?CNRS
First, we merged the annotations of each file
within a given institute (1.5h per pair of annotators),
then merged the results across the two institutes
(2h). Finally, we merged the results with the anno-
tations of the hired annotators (8h). We thus spent
about 90 hours to annotate and merge annotations in
this mini reference corpus (0.75 man-month).
4.4 Annotation results
Our broadcast news corpus includes 1,291,225
tokens, among which there are 954,049 non-
punctuation tokens. Its annotation contains 113,885
named entities and 146,405 components (Table 3),
i.e. one entity per 8.4 non-punctuation tokens, and
one component per 6.5 non-punctuation tokens.
There is an average of 6 annotations per line.
PPPPPPPPInf.
Data
Training Test
# shows 188 18
# lines 43,289 5,637
# words 1,291,225 108,010
# entity types 113,885 5,523
# distinct types 41 32
# components 146,405 8,902
# distinct comp. 29 22
Table 3: Statistics on annotated corpora.
5 Inter-Annotator Agreement
5.1 Procedure
During the annotation campaign, we measured sev-
eral criteria on a regular basis: inter-annotator agree-
ment and disagreement. We used them to correct er-
roneous annotations, and mapped these corrections
to the original annotations. We also used these mea-
sures to give the annotators feedback on the en-
countered problems, discrepancies, and residual er-
rors. Whereas we performed these measurements all
along the annotation campaign, this paper focuses
on the final evaluation on the mini reference corpus.
5.2 Metrics
Because human annotation is an interpretation pro-
cess (Leech, 1997), there is no ?truth? to rely on. It
is therefore impossible to really evaluate the validity
of an annotation. All we can and should do is to eval-
uate its reliability, i.e. the consistency of the anno-
tation across annotators, which is achieved through
computation of the inter-annotator agreement (IAA).
97
The best way to compute it is to use one of
the Kappa family coefficients, namely Cohen?s
Kappa (Cohen, 1960) or Scott?s Pi (Scott, 1955),
also known as Carletta?s Kappa (Carletta, 1996),11
as they take chance into account (Artstein and Poe-
sio, 2008). However, these coefficients imply a
comparison with a ?random baseline? to establish
whether the correlation between annotations is sta-
tistically significant. This baseline depends on the
number of ?markables?, i.e. all the units that could
be annotated.
In the case of named entities, as in many others,
this ?random baseline? is known to be difficult?if
not impossible?to identify (Alex et al, 2010). We
wish to analyze this in more detail, to see how we
could actually compute these coefficients and what
information it would give us about the annotation.
Markables Annotators Both institutes
F = 0.84522 F = 0.91123
U1: n-grams
? = 0.84522 ? = 0.91123
pi = 0.81687 pi = 0.90258
U2: n-grams ? 6
? = 0.84519 ? = 0.91121
pi = 0.81685 pi = 0.90257
U3: NPs
? = 0.84458 ? = 0.91084
pi = 0.81628 pi = 0.90219
U4: Ester entities
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
U5: Pooling
? = 0.71300 ? = 0.82607
pi = 0.71210 pi = 0.82598
Table 4: Inter-Annotator Agreements (? stands for Co-
hen?s Kappa, pi for Scott?s Pi, and F for F-measure). IAA
values were computed by taking as the reference the hired
annotators? annotation or that obtained by merging from
both institutes (see Figure 5).
In the present case, we could consider that, poten-
tially, all the noun phrases can be annotated (row U3
in Table 4, based on the PASSAGE campaign (Vil-
nat et al, 2010)). Of course, this is a wrong approx-
imation as named entities are not necessarily noun
phrases (e.g., ?? partir de l?automne prochain?, from
next autumn).
We could also consider all n-grams of tokens in
the corpus (row U1). However, it would be more
11For more details on terminology issues, we refer to the in-
troduction of (Artstein and Poesio, 2008).
relevant to limit their size. For a maximum size of
six, we get the results shown in row U2. All this, of
course, is artificial, as the named entity annotation
process is not random.
To obtain results that are closer to reality, we
could use numbers of named entities from previous
named entity annotation campaigns (row U4 based
on the Ester II campaign (Galliano et al, 2009)), but
as we consider here a largely extended version of
those, the results would again be far from reality.
Another solution is to consider as ?markables? all
the units annotated by at least one of the annotators
(row U5). In this particular case, units not annotated
by any of the annotators (i.e. silence) are overlooked.
The lowest IAA will be the one computed with
this last solution, while the highest IAA will be
equal to the F-measure (i.e. the measure computed
with all the markables as shown in row U1 in Ta-
ble 4). We notice that the first two solutions (U1
and U2 with n-grams) are not acceptable because
they are far from reality; even extended named en-
tities are sparse annotations, and just considering
all tokens as ?markables? is not suitable. The last
three ones seem to be more relevant because they
are based on an observed segmentation on similar
data. Still, the U3 solution (NPs) overrates the num-
ber of markables because not all noun phrases are
extended named entities. Although the U4 solution
(Ester entities) is based on the same corpus used for
a related task, it underrates the number of markables
because that task produced 16.3 times less annota-
tions. Finally the U5 solution (pooling) gives the
lower bound for the ? estimation which is an in-
teresting information but may easily undervalue the
quality of the annotation.
As (Hripcsak and Rothschild, 2005) showed, in
our case ? tends towards the F-measure when the
number of negative cases tends towards infinity. Our
results show that it is hard to build a justifiable hy-
pothesis on the number of markables which is larger
than the number of actually annotated entities while
keeping ? significantly under the F-measure. But
building no hypothesis leads to underestimating the
? value.
This reinforces the idea of using the F-measure
as the main inter-annotator agreement measure for
named entity annotation tasks.
98
6 Limitations
We used syntax to define some components (e.g. a
qualifier is an adjective) and to set the scope of en-
tities (e.g. stop at relative clauses). Nevertheless,
this syntactic definition cannot fit all named enti-
ties, which are mainly defined according to seman-
tics: the phrase ?dans les mois qui viennent? (?in
the coming months?) expresses an entity of type
time.date.rel where the relative clause ?qui vien-
nent? is part of the entity and contributes the time-
modifier component.
The distinction between some types of entities
may be fuzzy, especially for the organizations (is
the Social Security an administrative organization or
a company?) and for context-dependent annotations
(is lemonde.fr a URL, a media, or a company?). As a
consequence, some entity types might be converted
into specific components in a future revision, e.g. the
func type could become a component of the pers
type, where it would become a description of the
function itself instead of the person who performs
this function (Figure 6).
depuisptm
-its
oftrlits
vaienr
tn.pl,num
Rpeulits
depuisptm
oftr
vaienr
tn.pl,num
Rpeulits
Figure 6: Possible revision: current annotation (left),
transformation of func from entity to component (right).
7 Conclusion and perspectives
In this paper, we presented an extension of the tra-
ditional named entity categories to new types (func-
tions, civilizations) and new coverage (expressions
built over a substantive). We created guidelines
that were used by graduate annotators to annotate
a broadcast news corpus.
The organizers also annotated a small part of the
corpus to build a mini reference corpus. We evalu-
ated the human annotations with our mini-reference
corpus: the actual computed ? is between 0.71 et
0.85 which, given the complexity of the task, seems
to indicate a good annotation quality. Our results are
consistent with other studies (Dandapat et al, 2009)
in demonstrating that human annotators? training is
a key asset to produce quality annotations.
We also saw that guidelines are never fixed, but
evolve all along the annotation process due to feed-
back between annotators and organizers; the rela-
tionship between guidelines producers and human
annotators evolved from ?parent? to ?peer? (Akrich
and Boullier, 1991). This evolution was observed
during the annotation development, beyond our ex-
pectations. These data have been used for the 2011
Quaero Named Entity evaluation campaign.
Extensions and revisions are planned. Our first
goal is to add a new type of named entity for all
kinds of events; guidelines are being written and hu-
man annotation tests are ongoing. We noticed that
some subtypes are more difficult to disambiguate
than others, especially org.adm and org.ent (defi-
nition and examples in the guidelines are not clear
enough). We shall make decisions about this kind
of ambiguity, either by merging these subtypes or by
reorganizing the distinctions within the organization
type. We also plan to link the annotated entities us-
ing relations; further work is needed to define more
precisely the way we will perform these annotations.
Moreover, the taxonomy we defined was applied to
a broadcast news corpus, but we intend to use it in
other corpora. The annotation of an old press corpus
was performed according to the same process. Its
evaluation will start in the coming months.
Acknowledgments
We thank all the annotators who did such a great
work on this project, as well as Sabine Barreaux
(INIST?CNRS) for her work on the reference cor-
pus.
This work was partly realized as part of the
Quaero Programme, funded by Oseo, French State
agency for innovation and by the French ANR Etape
project.
References
ACE. 2000. Entity detection and tracking,
phase 1, ACE pilot study. Task definition.
http://www.nist.gov/speech/tests/ace/phase1/doc/summary-
v01.htm.
ACE. 2005. ACE (Automatic Con-
tent Extraction) English annotation guide-
lines for entities version 5.6.1 2005.05.23.
http://www.ldc.upenn.edu/Projects/ACE/docs/English-
Entities-Guidelines_v5.6.1.pdf.
99
Madeleine Akrich and Dominique Boullier. 1991. Le
mode d?emploi, gen?se, forme et usage. In Denis
Chevallier, editor, Savoir faire et pouvoir transmettre,
pages 113?131. ?d. de la MSH (collection Ethnologie
de la France, Cahier 6).
Beatrice Alex, Claire Grover, Rongzhou Shen, and Mijail
Kabadjov. 2010. Agile Corpus Annotation in Prac-
tice: An Overview of Manual and Automatic Annota-
tion of CVs. In Proc. of the Fourth Linguistic Annota-
tion Workshop, pages 29?37, Uppsala, Sweden. ACL.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Computa-
tional Linguistics, 34(4):555?596.
Eckhard Bick. 2004. A named entity recognizer for dan-
ish. In LREC?04.
H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-
ache, Anne Kuhn, and Djamel Mostefa. 2005. Seman-
tic Annotation of the French Media Dialog Corpus. In
InterSpeech, Lisbon.
Jean Carletta. 1996. Assessing Agreement on Classifi-
cation Tasks: the Kappa Statistic. Computational Lin-
guistics, 22:249?254.
Sam Coates-Stephens. 1992. The analysis and acquisi-
tion of proper names for the understanding of free text.
Computers and the Humanities, 26:441?456.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological Mea-
surement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex Linguistic An-
notation - No Easy Way Out! A Case from Bangla
and Hindi POS Labeling Tasks. In Proc. of the Third
Linguistic Annotation Workshop, Singapour. ACL.
Bart Desmet and V?ronique Hoste. 2010. Towards a
balanced named entity corpus for dutch. In LREC.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
(ACE) program tasks, data, and evaluation. In Proc. of
LREC.
Maud Ehrmann. 2008. Les entit?s nomm?es, de la lin-
guistique au TAL : statut th?orique et m?thodes de
d?sambigu?sation. Ph.D. thesis, Univ. Paris 7 Diderot.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proc. of
COLING, volume 1, pages 1?7. ACL.
Michael Fleischman. 2001. Automated subcategoriza-
tion of named entities. In Proc. of the ACL 2001 Stu-
dent Research Workshop, pages 25?30.
Kar?n Fort, Maud Ehrmann, and Adeline Nazarenko.
2009. Towards a Methodology for Named Entities An-
notation. In Proceeding of the 3rd ACL Linguistic An-
notation Workshop (LAW III), Singapore.
Sylvain Galliano, Guillaume Gravier, and Laura
Chaubard. 2009. The ESTER 2 evaluation campaign
for the rich transcription of French radio broadcasts.
In Proc of Interspeech 2009.
Ralph Grishman and Beth Sundheim. 1996. Message
Understanding Conference - 6: A brief history. In
Proc. of COLING, pages 466?471.
George Hripcsak and Adam S. Rothschild. 2005. Tech-
nical brief: Agreement, the f-measure, and reliability
in information retrieval. JAMIA, 12(3):296?298.
Seungwoo Lee and Gary Geunbae Lee. 2005. Heuris-
tic methods for reducing errors of geographic named
entities learned by bootstrapping. In IJCNLP, pages
658?669.
Geoffrey Leech. 1997. Introducing corpus annotation.
In Geoffrey Leech Roger Garside and Tony McEnery,
editors, Corpus annotation: Linguistic information
from computer text corpora, pages 1?18. Longman,
London.
Diana Maynard, Valentin Tablan, Cristian Ursu, Hamish
Cunningham, and Yorick Wilks. 2001. Named en-
tity recognition from diverse text types. In Recent Ad-
vances in NLP 2001 Conference, Tzigov Chark.
Tomoko Ohta. 2002. The genia corpus: An annotated
research abstract corpus in molecular biology domain.
In Proc. of HLTC, pages 73?77.
Sophie Rosset, Olivier Galibert, Gilles Adda, and Eric
Bilinski. 2007. The LIMSI participation to the QAst
track. In Working Notes for the CLEF 2007 Workshop,
Budapest, Hungary.
Sophie Rosset, Cyril Grouin, and Pierre Zweigenbaum.
2010. Entit?s nomm?es : guide d?annotation Quaero,
November. T3.2, presse ?crite et orale.
SAIC. 1998. Proceedings of the seventh message under-
standing conference (MUC-7).
William A Scott. 1955. Reliability of Content Analysis:
The Case of Nominal Scale Coding. Public Opinion
Quaterly, 19(3):321?325.
Satoshi Sekine. 2004. Definition, dictionaries and tagger
of extended named entity hierarchy. In Proc. of LREC.
Anne Vilnat, Patrick Paroubek, Eric Villemonte de la
Clergerie, Gil Francopoulo, and Marie-Laure Gu?not.
2010. Passage syntactic representation: a minimal
common ground for evaluation. In Proc. of LREC.
Alex Yeh, Alex Morgan, Marc Colosimo, and Lynette
Hirschman. 2005. BioCreAtIvE task 1A: gene men-
tion finding evaluation. BMC Bioinformatics, 6(1).
100
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 87?96,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Methods Combination and ML-based Re-ranking of Multiple
Hypothesis for Question-Answering Systems
Arnaud Grappy
LIMSI-CNRS
arnaud.grappy@limsi.fr
Brigitte Grau
LIMSI-CNRS
ENSIIE
brigitte.grau@limsi.fr
Sophie Rosset
LIMSI-CNRS
sophie.rosset@limsi.fr
Abstract
Question answering systems answer cor-
rectly to different questions because they
are based on different strategies. In order
to increase the number of questions which
can be answered by a single process, we
propose solutions to combine two question
answering systems, QAVAL and RITEL.
QAVAL proceeds by selecting short pas-
sages, annotates them by question terms,
and then extracts from them answers which
are ordered by a machine learning valida-
tion process. RITEL develops a multi-level
analysis of questions and documents. An-
swers are extracted and ordered according
to two strategies: by exploiting the redun-
dancy of candidates and a Bayesian model.
In order to merge the system results, we de-
veloped different methods either by merg-
ing passages before answer ordering, or by
merging end-results. The fusion of end-
results is realized by voting, merging, and
by a machine learning process on answer
characteristics, which lead to an improve-
ment of the best system results of 19 %.
1 Introduction
Question-answering systems aim at giving short
and precise answers to natural language ques-
tions. These systems are quite complex, and
include many different components. Question-
Answering systems are generally organized
within a pipeline which includes at a high level
at least three components: questions processing,
snippets selection and answers extraction. But
each module of these systems is quite different.
They are based on different knowledge sources
and processing. Even if the global performance of
these systems are similar, they show great dispar-
ity when examining local results. Moreover there
is no question-answering system able to answer
correctly to all possible questions. Considering all
QA evaluation campaigns in French like CLEF,
EQUER or Qu?ro, or for other languages like
TREC, no system obtained 100% correct answers
at first rank. A new direction of research was built
upon these observations: how can we combine
correct answers provided by different systems?
This work deals with this issue1 . In this paper
we describe different experiments concerning the
combination of QA systems. We used two differ-
ent available systems, QAVAL and RITEL, while
RITEL includes two different answer extraction
strategies. We propose to merge the results of
these systems at different levels. First, at an in-
termediary step (for example, between snippet se-
lection and answer extraction). This approach al-
lows to evaluate a fusion process based on the in-
tegration of different strategies. Another way to
proceed is to execute the fusion at the end of each
system. The aim is then to choose between all the
candidate answers the best one for each question.
Such an approach has been successfully applied
in the information retrieval field, with the defini-
tion of different functions for combining results
of search engines (Shaw and Fox, 1994). How-
ever, in QA, the problem is different as answers to
questions are not made of a list of answers, but are
made of excerpts of texts, which may be different
in their writing, but which correspond to a unique
and same answer. Thus, we propose fusion meth-
ods that rely on the information generally com-
puted by QA systems, such as score, rank, an-
1This work was partially financed by OSEO under the
Quro program
87
swer redundancy, etc. We defined new voting and
scoring functions, and a machine learning system
to combine these features. Most of the strategies
presented here allow a clear improvement (up to
19 %) on the first ranked correct answers.
In the following, related work is presented in
the section 2. We then describe the different sys-
tems used in this work (Section 3.1 and 3.2). The
proposed approach are presented (Section 4 and
5). The methods and the different systems are
then evaluated on the same corpus.
2 Related work
QA system hybridization often consists in merg-
ing end-results. The first studies presented here
aim at merging the results of different strate-
gies for finding answers in the same set of doc-
uments. (Jijkoun and Rijke, 2004) developed sev-
eral strategies for answering questions, based on
different paradigms for extracting answers. They
search for answers in a knowledge base or by ap-
plying extraction patterns or by selecting the n-
grams the closest to the question words. They de-
fined different methods for recognizing the simi-
larity of two answers: equality, inclusion and an
edit distance. The merging of answers is realized
by summing the confidence scores of similar an-
swers and leads to improve the number of right
answers at first rank of 31 %.
(Tellez-Valero et al, 2010) combine the out-
put of QA systems, whose strategy is not known.
They only dispose of the provided answers asso-
ciated with a supporting snippet. Merging is done
by a machine learning approach, which combines
different criteria such as the question category, the
expected answer type, the compatibility between
the provided answer and the question, the system
which was applied and the rate of question terms
in the snippet. When applying this module on the
CLEF QA systems which were run on the Span-
ish data, they obtain a better MRR2 value than the
best system from 0.62 up to 0.73.
In place of diversifying the answering strate-
gies, another possibility is to apply a same strat-
egy on different collections. (Aceves-Pe?rez et al,
2008) apply classical merging strategies to mul-
tilingual QA systems, by merging answers ac-
cording to their rank or by combining their con-
fidence scores, normalized or not. They show that
2Mean Reciprocal Rank
the combination of normalized scores obtains re-
sults which are better than a monolingual system
(MRR from 0.64 up to 0.75). They also tested
hybridization at the passage level by extracting
answers from the overall set of passages which
proved to be less relevant than answer merging.
(Chalendar et al, ) combine results obtained by
searching the Web in parallel to a given collec-
tion. The combination which consists in boosting
answers if they are found by the two systems is
very effective, as it is less probable to find same
incorrect answers on different documents.
The hybridization we are interested in concerns
the merging of different strategies and different
system capabilities in order to improve the final
result. We tested different hybridization levels,
and different merging methods. One is closed
to (Tellez-Valero et al, 2010) as it is based on
a validation module. Other are voting and scor-
ing methods which have been defined according
to our task, and are compared to classical merg-
ing scheme which have been proposed in infor-
mation retrieval (Shaw and Fox, 1994), ComSum
and CombMNZ.
3 The Question-Answering systems
3.1 The QAVAL system
3.1.1 General overview
QAVAL(Grappy et al, 2011) is made of se-
quential modules, corresponding to five main
steps (see Fig. 1). The question analysis provides
main characteristics for retrieving passages and
for guiding the validation process. Short passages
of about 300-character long are obtained directly
from the search engine Lucene and are annotated
with question terms and their weighted variants.
They are then parsed by a syntactic parser and en-
riched with the question characteristics, which al-
lows QAVAL to compute the different features for
validating or discarding candidate answers.
A specificity of QAVAL relies on its validation
module. Candidate answers are extracted accord-
ing to the expected answer type, i.e. a named en-
tity or not. In case of a named entity, all the named
entities corresponding to the expected type are
extracted while, in the second case, QAVAL ex-
tracts all the noun phrases which are not question
phrases. As many candidate answers can be ex-
tracted, a first step consists in recognizing obvious
false answers. Answers from a passage that does
88
Que
stio
n 
an
aly
sis
Pas
sa
ge 
se
lec
tion
An
sw
er
va
lida
tion
 
an
d 
ran
kin
g
Can
did
ate
 
an
sw
er
ex
tra
ctio
n
An
no
tat
ion
 
an
d 
syn
tac
tic
an
aly
sis
of p
as
sa
ges
Que
stio
ns
an
sw
ers
an
sw
ers
an
sw
ers
Do
cu
m
en
ts
An
sw
er
ran
kin
g
An
sw
er
fus
ion
QA
VA
L
RIT
EL
 
Sta
nda
rd
RIT
EL
 
Pro
bab
ilis
tic
5 a
ns
we
rs
 

Que
stio
n 
an
aly
sis
An
no
tat
ion
 
an
d 
syn
tac
tic
an
aly
sis
of p
as
sa
ges
Pas
sa
ge 
se
lec
tion
Can
did
ate
 
an
sw
er
ex
tra
ctio
n
An
sw
er
ran
kin
g
Hy
brid
iza
tion
poi
nt
Figure 1: The QAVAL and RITEL systems and their
possible hybridizations
not contain all the named entities of the question
are discarded. The remaining answers are then
ranked based on a learning method which com-
bines features characterizing the passage and the
candidate answer it provides. The QAVAL sys-
tem has been evaluated on factual questions and
obtains good results.
3.1.2 Answer ranking by validation
A machine based learning validation module
provides scores to each candidate answer. Fea-
tures relative to passages aim at evaluating in
which part a passage conveys the same meaning
as the question. They are based on lexical fea-
tures, as the rate of question words in the passage,
their POS tag, the main terms of the question, etc.
Features relative to the answer represent the
property that an answer has to be of an expected
type, if explicitly required, and to be related to
the question terms. Another kind of criterion con-
cerns the answer redundancy: the most frequent
an answer is, the most relevant it is. Answer type
verification is applied for questions which give an
explicit type for the answer, as in ?Which presi-
dent succeeded Georges W. Bush?? that expects
as answer the name of a president, more specific
than the named entity type PERSON. This mod-
ule (Grappy and Grau, 2010) combines results
given by different kinds of verifications, based
on named entity recognizers and searches in cor-
pora. To evaluate the relation degree of an answer
with the question terms, QAVAL computes i) the
longest chain of consecutive common words be-
tween the question plus the answer and the pas-
sage; ii) the average distance between the answer
and each of the question words in the passage.
Other criteria are the passage rank given by us-
ing results of the passage analysis, the question
category, i.e. definition, characterization of an en-
tity, verb modifier or verb complement, etc.
3.2 The RITEL systems
3.3 General overview
The RITEL system (see Figure 1) which we used
in these experiments is fully described in (Bernard
et al, 2009). This system has been devel-
oped within the framework of the Ritel project
which aimed at building a human-machine dia-
logue system for question-answering in open do-
main (Toney et al, 2008).
The same multilevel analysis is carried out on
both queries and documents. The objective of this
analysis is to find the bits of information that may
be of use for search and extraction, called perti-
nent information chunks. These can be of dif-
ferent categories: named entities, linguistic enti-
ties (e.g., verbs, prepositions), or specific entities
(e.g., scores). All words that do not fall into such
chunks are automatically grouped into chunks via
a longest-match strategy. The analysis is hierar-
chical, resulting in a set of trees. Both answers
and important elements of the questions are sup-
posed to be annotated as one of these entities.
The first step of the QA system itself is to build
a search descriptor (SD) that contains the impor-
tant elements of the question, and the possible
answer types with associated weights. Answer
types are predicted through rules based on com-
binations of elements of the question. On all sec-
ondary and mandatory chunks, the possible trans-
formations (synonym, morphological derivation,
etc.) are indicated and weighted in the SD. Docu-
ments are selected using this SD. Each element of
the document is scored with the geometric mean
of the number of occurrences of all the SD ele-
ments that appear in it, and sorted by score, keep-
ing the n-best. Snippets are extracted from the
document using fixed-size windows and scored
using the geometrical mean of the number of oc-
89
currences of all the SD elements that appear in the
snippet, smoothed by the document score.
3.3.1 Answer selection and ranking
Two different strategies are implemented in RI-
TEL. The first one is based on distance between
question words and candidate answer, named RI-
TEL Standard. The second one is based on a
Bayesian model, named RITEL Probabilistic.
Distance-based answer scoring The snippets
are sorted by score and examined one by one in-
dependently. Every element in a snippet with a
type found in the list of expected answer types of
the SD is considered an answer candidate. RITEL
associates to each candidate answer a score which
is the sum of the distances between itself and the
elements of the SD. That score is smoothed with
the snippet score through a ?-ponderated geomet-
ric mean. All the scores for the different instances
of the same element are added together. The enti-
ties with the best scores then win. The scores for
identical (type,value) pairs are added together and
give the final scoring to the candidate answers.
Answer scoring through Bayesian modeling
This method of answer scoring is built upon a
Bayesian modeling of the process of estimating
the quality of an answer candidate. This approach
relies on multiple elementary models including
element co-occurrence probabilities, question el-
ement appearance probability in the context of a
correct answer and out of context answer proba-
bility. The model parameters are either estimated
on the documents or are set empirically. This sys-
tem has not better result than the distance-based
one but is interesting because it allows to obtain
different correct answers.
3.4 Systems combination
The systems we used in these experiments are
very different especially with respect to the pas-
sage selection and the answer extraction and scor-
ing methods. The QAVAL system proceeds to
the passage selection before any analysis while
the two RITEL systems do a complete and multi-
level analysis on the documents before the pas-
sage selection. Concerning the answer extraction
and scoring, the QAVAL system uses an answer
validation process based on machine learning ap-
proach while the answer extraction of the RITEL-
S system uses a distance-based scoring and the
RITEL-P Bayesian models. It seems then inter-
esting to combine these various approaches in a
in-system way (see Section 4): (1) the passages
selected by the QAVAL system are provided as
document collection to the RITEL systems; (2)
the candidate answers provided by the RITEL
systems are given to the answer validation mod-
ule of the QAVAL system.
We also worked, in a more classical way, on
interleaving results of answer selection methods
(see Section 5 and 6). These methods make use of
the various information provided by the different
systems along with all candidate answers.
4 Internal combination
4.1 QAVAL snippets used by RITEL
The RITEL system proceeds to a complete analy-
sis of the document which is used during the doc-
ument and selection extraction procedure and ob-
tains 80.3% of the questions having a correct an-
swer in at least one passage. The QAVAL system
extracts short passages (150) using Lucene and
obtains a score of 88%. We hypothesized that the
RITEL?s fine-grained analysis could better work
on small collection than on the overall document
collection (combination 1 Fig. 1). We consider
the passages extracted by the QAVAL system be-
ing a new collection for the RITEL system. First,
the analysis is done on this new collection and
the analysis result is indexed. Then the gen-
eral question-answering procedures are applied:
question analysis, SD construction, document and
snippet extraction and then answer selection and
ranking. The two answer extraction methods have
been applied and the results are presented in the
Table 1. This simple approach does not allow any
All documents QAVAL? snippets
Ritel-S Ritel-P Ritel-S Ritel-P
top-1 34.0% 22.4% 29.9% 22.4%
MRR 0.41 0.29 0.38 0.32
top-20 61.2% 48.7% 54.4% 49.7%
Table 1: Results of Ritel systems (Ritel-S used
the distance-based answer scoring, Ritel-P used the
Bayesian modeling) working on the QAVAL? snippets.
improvement. Actually all the results are worsen-
ing, except maybe for the Ritel-P systems (which
is actually not the best one). One of our hypoth-
esis is that the QAVAL snippets are too short and
90
do not fit the criteria used by the RITEL system.
4.2 Answer validation
In QAVAL, answer ranking is done by an an-
swer validation module (fully described in sec-
tion 3.1). The candidate answers ranked by this
module are associated to a confidence score. The
objective of this answer validation module is to
decide whether the candidate answer is correct or
not given an associated snippet. The objective is
to use this answer validation module on the candi-
date answers and the snippets provided by all the
systems (combination 2 Fig. 1). Unfortunately,
this method did not obtain better results than the
best system. We assume that this module being
learnt on the QAVAL data only is not robust to
different data and more specifically to the passage
length which is larger in RITEL than in QAVAL.
A possible improvement could be to add answers
found by the RITEL system in the training base.
5 Voting methods and scores
combination
These methods are based on a comparison be-
tween the candidate answers: are they identical ?
An observation that can be made concerning the
use of a strict equality between answers is that in
some cases, 2 different answers can be more or
less identical. For example if one system returns
?Sarkozy? and another one ?Nicolas Sarkozy? we
may want to consider these two answers as iden-
tical. We based the comparison of answers on the
notion of extended equality. For that, we used
morpho-syntactic information such as the lemmas
and the part of speech of each words of the an-
swers. The TreeTagger tool3 has been used. An
answer R1 is then considered as included in an
answer R2 if all non-empty words of R1 are in-
cluded in R2. Two words having the same lemma
are considered as identical. For example ?chanta?
and ?chanterons? are identical because they share
the same lemma ?chanter?. Adjectives, proper
names and substantives are considered as non-
empty words. Following this definition, two an-
swers R1 and R2 are considered identical if R1 is
included in R2 and R2 in R1.
3www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
5.1 Merge based on candidate answer rank
The first information we used takes into account
the rank of the candidate answers. The hypothesis
beyond this is that the systems often provide the
correct answer at first position, if they found it.
5.1.1 Simple interleaving
The first method, and probably the simplest, is
to merge the candidate answers provided by all
the systems: the first candidate answer of the first
system is ranked in the first position; the first an-
swer of the second system is ranked in the sec-
ond position; the second answer of the first sys-
tem is ranked in the third position, and so on. If
one answer was already merged (because ranked
at a higher rank by another system), it is not used.
We choose to base the systems order given their
individual score. The first system is QAVAL, the
second RITEL-S and the third RITEL-P. Follow-
ing that method, the accuracy (percentage of cor-
rect answers at first rank) is the one obtained by
the best system. But we assume that the MRR at
the top-n (with n > 1) would be improved.
5.1.2 Sum of the inverse of the rank
The simple interleaving method does not take
into account the answer rank provided by the dif-
ferent systems. However, this information may
be relevant and was used in order to merge can-
didate answer extracted from different document
collection, Web articles and news paper (Chalen-
dar et al, ). In our case, answers are extracted
from the same document collection by the dif-
ferent systems. Then it is possible that the same
wrong answers will be extracted by the different
systems.
A first possible method to take into account
the rank provided by the systems is to weight the
candidate answer using this information. For a
same answer provided by the different systems,
the weight is the sum of the inverse of the rank
given by the systems. To compare the answers the
strict equality is applied. If a system ranks an an-
swer at the first position and another system ranks
the same answer at the second position, the weight
is 1.5 (1 + 12 ). The following equation express in
a more formalized way this method.
weight =
? 1
rank
Comparing to the previous method, that one
should allow to place more correct answers at the
first rank.
91
5.2 Using confidence scores
In order to rank all their candidate answers, the
systems used a confidence score associated to
each candidate answer. We then wanted to use
these confidence scores in order to re-rank all the
candidate answers provided by all the systems.
But this is only possible if all systems produce
comparable scores. This is not the case. QAVAL
produces scores ranging from -1 to +1. RITEL-
P, being probabilistic, produces a score between 0
and +1. And RITEL-S does not use strict interval
and the scores are potentially ranged from ?? to
+?. The following normalization (a linear re-
gression) has been applied to the RITEL-S and
RITEL-P scores in order to place it in the range
-1 to 1.
valuenormalized =
2 ? valueorigin
valMin ? valMax
? 1
5.2.1 Sum of confidence scores
In order to compare our methods with classi-
cal approaches, we used two methods presented
in (Shaw and Fox, 1994):
? CombSum which adds the different confi-
dence scores of an answer given by the dif-
ferent systems;
? CombMNZ which adds the confidence
scores of the different systems and multiply
the obtained value by the number of systems
having found the considered answer.
5.2.2 Hybrid method
An hybrid method combining the rank and the
confidence score has been defined. The weight is
the sum of two elements: the higher confidence
score and a value taking into account the rank
given by the different systems. This value is de-
pendent on the number of answers, the type of the
equality (the answers are included or equal) which
results in the form of a bonus, and the rank of the
different considered answers. The weight of an
answer a to a question q is then:
w(a) = s(a) +
?
be ? (|a(q)| ?
?
r(a)) (1)
with be the equality bonus, w the weight, s, the
score and r the rank.
The equality bonus, found empirically, is given
for each systems pair. The value is 3 if the two
answers are equal, 2 if an answer is included in
the other and 1 otherwise. When an answer is
found by two or more systems, the higher con-
fidence score is kept. The result of this method is
that the answers extracted by more than one sys-
tem are favored. An answer found by only one
system, even with a very high confidence score,
may be downgraded.
6 Machine-learning-based method for
answer re-ranking
To solve a re-ranking problem, machine learn-
ing approaches can be used (for example (Mos-
chitti et al, 2007)). But in most of the cases,
the objective is to re-rank answers provided by
one system, that means to re-rank multiple hy-
potheses from one system. In our case, we want
to re-rank multiple answers from different sys-
tems. We decided to use an SVM-based approach,
namely SVMrank (Joachims, 2006), which is well
adapted to our problem. An important aspect is
then to choose the pertinent features for such a
task. Our objective is to consider robust enough
features to deal with different systems? answers
without introducing biases. Two classes of char-
acteristic should be able to give a useful represen-
tation of the answers: those related to the answer
itself and those related to the question.
6.1 Answer characteristics
First of all, we should use the rank and the score
as we did in the preceding merging methods. The
problem may appear here because not all candi-
date answers are found by the different systems.
In that case, the score and the rank given to these
systems is then -2. It guarantees us that the fea-
tures are out of the considered range [?1,+1].
Considering that, it may be useful to know which
system provided the considered answer. For each
answer all systems having found that answer are
indicated. Moreover this information may help
to distinguish answers coming from for example
QAVAL and RITEL-S or RITEL-P from answers
coming from RITEL-S and RITEL-P. The two RI-
TEL systems share most of the modules and their
answers may have the same problems. Concern-
ing the answer, another aspect may be of interest:
how many time this answer has been found? The
question is not, how many times the answer ap-
pears in the documents but how many times the
answer appears in a context allowing this answer
92
to be considered as a candidate answer. We used
the number of different snippets selected by the
systems in which that answer was found.
6.2 Question characteristics
When observing the results obtained by the sys-
tems on different questions, we observed that the
?kind? of the question has an impact on the sys-
tems? performance. More specifically, it is largely
accepted in the community that at least two crite-
ria are of importance: the length of the question,
and the type of the expected answer (EAT).
Question length We may consider that the length
of the questions is more or less a good indicator
for the complexity level of the question. The num-
ber of non-empty words of the question can then
be a interesting feature.
Expected answer type One of the task of the
question processing, in a classical Question-
Answering system, is to decide of which type will
be the answer. For example, for a question like
Who is the president of France? the type of the
expected answer will be a named entity of the
class person and for a question like what wine to
drink with seafood? that the EAT is not a named
entity. (Grappy, 2009) observed that the QAVAL
system is better when the EAT is of a named entity
class. It is possible that adding this information
will, during the learning phase, positively weight
an answer coming from RITEL when the EAT is
not a named entity.
The value of this feature indicates the compat-
ibility of the answer and the EAT. We used the
method presented in (Grappy and Grau, 2010) and
already used for the answer validation module of
the QAVAL system. This method is based on a
ML-based combination of different methods us-
ing named entity dictionaries, wikipedia knowl-
edge, etc. This system gives a confidence score,
ranging from -1 to +1 which indicates the con-
fidence the system has in compatibility between
the answer and the EAT. In some cases, the ques-
tion processing module may indicate if the EAT
is of a more fine-grained entity. For example, the
question Who is the president of France? is not
only waiting for a person but more precisely for a
person having the function of a president. A new
feature is then added. If the EAT is a fine-grained
named entity, then the value is 1 and -1 otherwise.
7 Experiments and results
7.1 Data and observations
For the training of the SVM model, we used
the answers to 104 questions provided by the
2009 Quaero evaluation campaign (Quintard et
al., 2010). Only 104 questions have been used be-
cause we need to have at least one correct answer
provided by at least one system in the training
base for each question. Models have been trained
using 5, 10, 15 and 20 answers for each system.
For the evaluation, we used 147 factoid ques-
tions used in the 2010 Quaero4 evaluation cam-
paign. The document collection is made of
500,000 Web pages5. We used the Mean Re-
ciprocal Rank (MRR) as it is a usual metric in
Question-Answering on the first five candidate
answers. The MRR is the average of the recip-
rocal ranks of all considered answers. We also
used the top-1 metric which indicates the number
of correct answers ranked at the first position.
The baseline results, provided by each of the
three systems, are presented in Table 2. QAVAL
and RITEL-S have quite similar results which are
higher than those obtained by the RITEL-P sys-
tem. We can observe that, within the 20 top ranks,
38% of the questions have an answer given by
all the systems, 76 % by at least 2 systems and
21% receive no correct answers. The best possi-
ble result that could be obtained by a perfect fu-
sion method is also indicated in this table (0.79 of
MRR and 79% for top-1). Such a method would
lead to rank first each correct answer found by at
least a system. Figure 2 presents the answer repar-
System MRR % top-1 (#)
QAVAL 0.45 36 (53)
RITEL-S 0.41 32 (47)
RITEL-P 0.26 18 (27)
Perfect fusion 0.79 79 (115)
Table 2: Baseline results
tition between ranks 2 and 20 (the numbers of cor-
rect answers in first rank are given in Table 2).
This figure shows that the systems ranked the cor-
rect answer mostly in the first positions. That
means that these systems are relatively effective
for re-ranking their own candidate answers. Very
4http://www.quaero.org
5crawled by Exalead http://www.exalead.com/
93
few correct answers are ranked after the tenth po-
sition. Following these observations, the evalua-
tions are done on the first 10 candidate answers.
2 3 4 5 6 7 8 9 10 200
2
4
6
8
10
12
14
16
18
20
22
QAVALRITEL-S RITTEL-PSVM
3 4 5 6 7 8 9 1
Figure 2: Answer repartition
7.2 Results and analysis
Table 3 presents the results obtained with the dif-
ferent merging methods: simple interleaving (In-
ter.), Sum of the inverse of the rank, CombSum,
CombMNZ, hybrid method (Hyb. Meth.) and
SVM model. In order to evaluate the impact of the
RITEL-P (which achieved less good results), the
results are given using two (QAVAL and RITEL-
S) or three systems.
Method MRR % Top-1 (#)
(2 sys. / 3 sys.) (2 sys. / 3 sys.)
Inter. 0.47 / 0.45 36 (53) / 36 (53)
? 1
rang 0.48 / 0.46 38 (56)/ 36 (53)
CombSum 0.46 / 0.44 38 (56) / 34 (50)
CombMNZ 0.46/ 0.44 38 (56) / 35 (51)
Hyb. meth. 0.49 /0.44 40 (58) / 34 (50)
SVM 0.48 / 0.51 39 (57) / 42 (62)
QAVAL 0.44 36 (53)
Table 3: General results.
As shown in Table 3, the different methods
improve the results and the best method is the
SVM-based model which allows an improvement
of 19% of correct answer at first rank. This re-
sult is significantly better than the baseline result
and this method can be considered as very effec-
tive. Figure 2 shows the results of this model. In
order to validate our choice of using the SVM-
Rank model, we also tested the use of a com-
bination of decision trees, as QAVAL obtained
# candidate answers % Top-1 (#)
20 39 (58)
15 39 (58)
10 43 (63)
5 37 (55)
Table 4: Impact of the number of candidate answers
normalization MRR # Top-1
without 0.49 58 (39%)
with 0.51 63 (43%)
Table 5: Impact of the normalization
good results with this classifier in the validation
module. We obtained a MRR of 0.44 which is
obviously lower than the result obtained by the
SVM method. Generally speaking, the methods
taking into account the answer rank allow better
results than the methods using the answer confi-
dence score. Another interesting observation is
that the interleaving methods obtained better re-
sults when not using the RITEL-P system while
the SVM one obtained better results when using
the three systems. We assume that these two sys-
tems, RITEL-S and RITEL-P are too similar to
provide strict useful information, but that a ML-
based approach is able to generalize such infor-
mation.
In order to validate our choice of using only
the first ten candidate answers, we did some more
tests using 5, 10, 15 and 20 candidate answers.
Table 4 shows the results obtained with the SVM
model. We can see that is is better to consider
10 candidate answers. Beyond the first 10 can-
didate answers it is difficult to re-rank the cor-
rect answer without adding unsustainable noise.
Moreover most of the correct answers are in the
first ten candidates.
In order to validate the confidence score nor-
malization, we did experiments with and without
this normalization. Table 5 presents results which
validate our choice.
To better understand how the fusion is made,
we observed the repartition of the correct answers
at the first rank and at the top five ranks according
to the number of systems which extracted them
(figure 3 and figure 4). We do this for the three
best fusion approaches: the ML method with 3
systems, the hybrid method and the sum of the in-
verse of the ranks with two systems. As we can
94
Feuille1
Page 1
SVM Hybrid sum 1/rank
%
5%
10%
15%
20%
25%
30%
35%
40%
45%
50%
27%
12%
36% 33%
4%
3% 4%
1 system
2 systems
3 systems
Figure 3: First rank Feuille1
Page 1
SVM Hybrid sum 1/rank
%
15%
05%
25%
35%
45%
75%
65%
26%
14%
40% 43%
10% 15% 10%
1 system
0 systems
2 systems
Figure 4: Top five ranks
see, in most of the cases, the three approaches of-
ten rank the correct answers found by all the sys-
tems. The best approach is the SVM-based one.
It ranks 98 % of the correct answers given by the
3 systems in top 5 ranks. It also ranks better cor-
rect answers given by 2 systems (60% are ranked
in the top 5 ranks versus about 48 % with the two
other methods).
The rank-based method is globally reliable for
selecting correct answers in the top 5 ranks. This
behavior is consistent with the fact that our QA
systems, when they found a correct answer, gen-
erally rank it in first positions.
Some correct answers given by only one sys-
tem remain in the first position, and about 10%
of them remain in the top 5 ranks and are not su-
perseded by common wrong answers. However
the major part of these correct single-system an-
swers are discarded after the 5 first ranks (39% of
them by the SVM method, 45% by the rank-based
method and 53% by the hybrid method). In that
case, a ML method is a better solution for decid-
ing, however an improvement would be possible
only if other features could be found for a better
characterization of a correct answer, or maybe by
enlarging the training base.
According to these results, we also can expect
that with more QA systems, a fusion approach
would be more effective.
8 Conclusion
Improving QA systems is a very difficult task,
given the variability of the pairs (question / an-
swering passages), the complexity of the pro-
cesses and the variability of they performances.
Thus, an improvement can be searched by the hy-
bridization of different QA systems. We studied
hybridization at different levels, internal combi-
nation of processes and merging of end-results.
The first combination type did not proved to be
useful, maybe because each system has its global
coherence leading their modules to be more in-
terdependent than expected. Thus it appears
that combining different strategies is better re-
alized with the combination of their end-results,
specially when these strategies obtain good re-
sults. We proposed different combination meth-
ods, based on the confidence scores, the answer
rank, that are adapted to the QA context, and
a ML-method which considers more features for
characterizing the answers. This last method ob-
tains the better results, even if the simpler ones
also show good results. The proposed methods
can be applied to other QA systems, as the fea-
tures used are generally provided by the systems.
References
R.M. Aceves-Pe?rez, M. Montes-y Go?mez, L. Vil-
lasen?or-Pineda, and L.A. Uren?a-Lo?pez. 2008. Two
approaches for multilingual question answering:
Merging passages vs. merging answers. Interna-
tional Journal of Computational Linguistics & Chi-
nese Language Processing, 13(1):27?40.
G. Bernard, S. Rosset, O. Galibert, E. Bilinski, and
G. Adda. 2009. The LIMSI participation to the
QAst 2009 track. In Working Notes of CLEF 2009
Workshop, Corfu, Greece, October.
G. De Chalendar, T. Dalmas, F. Elkateb-gara, O. Fer-
ret, B. Grau, M. Hurault-plantet, G. Illouz, L. Mon-
ceaux, I. Robba, and A. Vilnat. The question an-
swering system QALC at LIMSI: experiments in
using Web and WordNet.
Arnaud Grappy and Brigitte Grau. 2010. Answer type
validation in question answering systems. In Adap-
95
tivity, Personalization and Fusion of Heterogeneous
Information, RIAO ?10, pages 9?15.
Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco,
Anne-Laure Ligozat, Isabelle Robba, and Anne Vil-
nat. 2011. Selecting answers to questions from web
documents by a robust validation process. In The
2011 IEEE/WIC/ACM International Conference on
Web Intelligence.
Arnaud Grappy. 2009. Validation de rponses dans un
systme de questions rponses. Ph.D. thesis, Universit
Paris Sud, Orsay.
Valentin Jijkoun and Maarten De Rijke. 2004. Answer
Selection in a Multi-Stream Open Domain Question
Answering System. In Proceedings 26th European
Conference on Information Retrieval (ECIR?04),
volume 2997 of LNCS, pages 99?111. Springer.
Thorsten Joachims. 2006. Training linear SVMs
in linear time. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD ?06, pages 217?
226, New York, NY, USA. ACM.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
Syntactic and Shallow Semantic Kernels for Ques-
tion Answer Classification. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 776?783, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Ludovic Quintard, Olivier Galibert, Gilles Adda,
Brigitte Grau, Dominique Laurent, Veronique
Moriceau, Sophie Rosset, Xavier Tannier, and Anne
Vilnat. 2010. Question Answering on Web Data:
The QA Evaluation in Quaero. In LREC?10, Val-
letta, Malta, May.
Joseph A. Shaw and Edward A. Fox. 1994. Combina-
tion of multiple searches. In TREC-2. NIST SPE-
CIAL PUBLICATION SP.
Alberto Tellez-Valero, Manuel Montes Gomez,
Luis Villasenor Pineda, and Anselmo Penas. 2010.
Towards multi-stream question answering using
answer validation. Informatica, 34(1):45?54.
Dave Toney, Sophie Rosset, Aurlien Max, Olivier Gal-
ibert, and Eric Bilinski. 2008. An Evaluation of
Spoken and Textual Interaction in the RITEL Inter-
active Question Answering System. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, May.
96
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 168?177,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Automatic Named Entity Pre-Annotation
for Out-of-Domain Human Annotation
Sophie Rosset?, Cyril Grouin?, Thomas Lavergne?,? , Mohamed Ben Jannet?,?,?,?
Je?re?my Leixa, Olivier Galibert? , Pierre Zweigenbaum?.
?LIMSI?CNRS ?Universite? Paris-Sud ?LNE
?LPP, Universite? Sorbonne Nouvelle ELDA
{rosset,grouin,lavergne,ben-jannet,pz}@limsi.fr
leixa@elda.org, olivier.galibert@lne.fr
Abstract
Automatic pre-annotation is often used to
improve human annotation speed and ac-
curacy. We address here out-of-domain
named entity annotation, and examine
whether automatic pre-annotation is still
beneficial in this setting. Our study de-
sign includes two different corpora, three
pre-annotation schemes linked to two an-
notation levels, both expert and novice an-
notators, a questionnaire-based subjective
assessment and a corpus-based quantita-
tive assessment. We observe that pre-
annotation helps in all cases, both for
speed and for accuracy, and that the sub-
jective assessment of the annotators does
not always match the actual benefits mea-
sured in the annotation outcome.
1 Introduction
Human corpus annotation is a difficult, time-
consuming, and hence costly process. This mo-
tivates research into methods which reduce this
cost (Leech, 1997). One such method consists of
automatically pre-annotating the corpus (Marcus
et al, 1993; Dandapat et al, 2009) using an ex-
isting system, e.g., a POS tagger, syntactic parser,
named entity recognizer, according to the task for
which the annotations aim to provide a gold stan-
dard. The pre-annotations are then corrected by
the human annotators. The underlying hypothe-
sis is that this should reduce annotation time while
possibly at the same time increasing annotation
completeness and consistency.
We study here corpus pre-annotation in a spe-
cific setting, out-of-domain named entity annota-
tion, in which we examine specific questions that
we present below. We produced corpora and an-
notation guidelines for named entities which are
both hierarchical and compositional (Grouin et al,
2011),1 and which we used in contrastive stud-
ies of news texts in French (Rosset et al, 2012).
We want to rely on the same named entity def-
initions for studies on two types of data we did
not cover: parliament debates (Europarl corpus)
and regional, contemporary written news (L?Est
Re?publicain), both in French. To help the annota-
tion process we could reuse our system (Dinarelli
and Rosset, 2011), but needed first to examine
whether a system trained on one type of text (our
first Broadcast News data) could be used to pro-
duce a useful pre-annotation for different types of
text (our two corpora).
We therefore set up the present study in which
we aim to answer the following questions linked
to this point and to related annotation issues:
? can a system trained on data from one spe-
cific domain be useful on data from another
domain in a pre-annotation task?
? does this pre-annotation help human annota-
tors or bias them?
? what importance can we give to the annota-
tors? subjective assessment of the usefulness
of the pre-annotation?
? can we observe differences in the use of pre-
annotation depending on the level of exper-
tise of human annotators?
Moreover, as the aforementioned annotation
scheme is based on two annotation levels (entities
and components), we want to answer these ques-
tions taking into account these two levels.
We first examine related work on pre-annotation
(Section 2), then present our corpora and annota-
tion task (Section 3). We describe and discuss ex-
periments in Section 4, and make subjective and
1Corpora, guidelines and tools are available through
ELRA under references ELRA-S0349 and ELRA-W0073.
168
quantitative observations in Sections 5 and 6. Fi-
nally, we conclude and present some perspectives
in Section 7.
2 Related Work
Facilitating human annotations has been the topic
of a large amount of research. Two different
approaches can be distinguished: active learn-
ing (Ringger et al, 2007; Settles et al, 2008) and
pre-annotation (Marcus et al, 1993; Dandapat et
al., 2009). Our work falls into the latter type.
Pre-annotation can be used in several ways. The
first is to provide annotations to be corrected by
human annotators (Fort and Sagot, 2010). A vari-
ant consists of merging multiple automatic anno-
tations before having them corrected by human
curators to produce a gold-standard (Rebholz-
Schuhmann et al, 2011). The second type con-
sists of providing clues to help human annotators
perform the annotation task (Mihaila et al, 2013).
This work addresses the first type, a single-
system pre-annotation with human correction. An
objective is to examine whether a system trained
on one type of text can be useful to pre-annotate
texts of a different type. Most previous studies
have been performed on well-behaved tasks such
as part-of-speech tagging on in-domain data, i.e.,
the model used for pre-annotating the target data
had been trained on similar data. For instance, Fort
and Sagot (2010) provide a precise evaluation of
the usefulness of pre-annotation and compare the
impact of different quality levels in POS taggers
on the Penn TreeBank corpus. They first trained
different models on the training part of the cor-
pus and applied them to the test corpus. The pre-
annotated test corpus was then corrected by hu-
mans. They reported gains in accuracy and inter-
annotator agreement. The study focused on the
minimal quality (accuracy threshold) of automatic
annotation that would prove useful for human an-
notation. They reported a gain for human annota-
tion when accuracy ranged from 66.5% to 81.6%.
On the contrary, for a semantic-frame annotation
task, Rehbein et al (2009) observed no significant
gain in quality and speed of annotation even when
using a state-of-the-art system.
Generally speaking, annotators find the pre-
annotation stage useful (Rehbein et al, 2009;
South et al, 2011; Huang et al, 2011). Anno-
tation managers consider that a bias may occur
depending on how much human annotators trust
the pre-annotation (Rehbein et al, 2009; Fort and
Sagot, 2010; South et al, 2011). In their frame-
semantic argument structure annotation, Rehbein
et al (2009) addressed a specific question consid-
ering a two-level annotation scheme: is the pre-
annotation of frame assignment (low-level anno-
tation) useful for annotating semantic roles (high-
level annotation)? Although for the low-level an-
notation task they observed a significant difference
in quality of final annotation, for the high-level
task they found no difference.
Most of these studies used a pre-annotation sys-
tem trained on the same kind of data as those
which were to be annotated manually. Neverthe-
less some system-oriented studies have focused on
the results obtained by systems trained on one type
of corpus and applied to another type of corpus,
e.g., for a Latin POS tagger (Poudat and Longre?e,
2009; Skj?rholt, 2011) or for a CoNLL named en-
tity tagger for German (Faruqui and Pado?, 2010)
for which the authors noticed noticed a reduc-
tion of the F-measure when going from in-domain
(newswire data, F=0.782 for their best system) to
out-of-domain (Europarl data, F=0.656).
One of our objectives is then to examine
whether a system trained on one type of text can
be useful to pre-annotate texts of a different type.
We set up experiments to study precisely the pos-
sible induced bias and whether the level of experi-
ence of the annotators would make a difference in
such a context. In this study, we used two different
kinds of corpora, which were both different from
the corpus used to train the pre-annotation system.
3 Task and corpus description
3.1 Task
In this work, we used the structured named entity
definition we proposed in a previous study (Grouin
et al, 2011): entities are both hierarchical (types
have subtypes) and compositional (types and com-
ponents are included in entities) as in Figure 1.
func.coll
org.ent
name
BEIde la
kind
analystes financiersles
Figure 1: Multi-level annotation of entity sub-
types (red tags) and components (blue tags): the
financial analysts of the EBI
169
This taxonomy of entity types is composed of
7 types (person, location, organization, amount,
time, production and function) and 32 sub-types
(individual person pers.ind vs. group of persons
pers.coll; administrative organization org.adm vs.
services org.ent; etc.). Types and subtypes consti-
tute the first level of annotation.
Within these categories, components are
second-level elements (kind, name, first.name,
etc.), and can never be used outside the scope of a
type or subtype element.
3.2 Corpora
Two French corpora were sampled from larger
ones:
Europarl: Prepared speech (Parliament
Debates?Europarl): 15,306 word extract;
Press: Local, contemporary written news (L?Est
Re?publicain): 11,146 word extract.
These corpora were automatically annotated us-
ing the system described in (Dinarelli and Rosset,
2011). This system relies on a Conditional Ran-
dom Field (CRF) model for the detection of com-
ponents and on a probabilistic context-free gram-
mar (PCFG) model for types and sub-types. These
models have been trained on Broadcast News data.
This system achieved a Slot Error Rate (Makhoul
et al, 1999) of 37.0% on Broadcast conversation
and 29.7% on Broadcast news, and ranked first in
the Quaero evaluation campaign (Galibert et al,
2011).
4 Experiments
In this section we present the protocol we designed
to study the usefulness of pre-annotation under
different conditions, and its overall results.
4.1 Protocol
We defined the following protocol, similar to the
one used in Rehbein et al (2009).
Corpora. Four versions of our two corpora were
prepared: (i) raw text, (ii) pre-annotation of
types, (iii) pre-annotation of components, and
(iv) full pre-annotation of both types and compo-
nents. Each of these four versions was split into
four quarters.
Annotators. Eight human annotators were in-
volved in this task. Among them, four are con-
sidered as expert annotators (they annotated cor-
pora in the previous years) while the four re-
maining ones are novice annotators (this was the
first time they annotated such corpora; they were
given training sessions before starting actual anno-
tation). We defined four pairs of annotators, where
each pair was composed of an expert and a novice
annotator.
Quarter allocation. We allocated each corpus
quarter in such a way that each pair of annotators
processed, in each corpus, material from each one
of the four pre-annotated versions (see Table 3).
The same allocation was made in both corpora.
4.2 Results
For each corpus part, a reference was built based
on a majority vote by confronting all annotations.
The resulting reference corpus is presented in Ta-
ble 1.
Corpus # comp. # types # entities # words
P
re
ss
Q1 481 310 791 3047
Q2 367 246 673 2628
Q3 495 327 822 2971
Q4 413 282 695 2600
E
ur
op
ar
l Q1 362 259 621 3926
Q2 309 221 530 3809
Q3 378 247 625 3604
Q4 413 299 712 3967
Table 1: General description of the reference an-
notations: number of components, types, entities
(the sum of components and types), and words
Table 2 presents the performance of the au-
tomatic pre-annotation system against the refer-
ence corpus. We used the well known F-measure
and in addition the Slot Error Rate as it allows
to weight different error classes (deletions, inser-
tions, type or frontier errors). Fort and Sagot
(2010) reported a gain in human annotation when
pre-annotation accuracy ranged from 66.5% to
81.6%. Given their results we can hope for a gain
in both accuracy and annotation time when using
pre-annotation.
Table 3 presents all results obtained by each an-
notators given each pre-annotation condition (raw,
components, types and full) in terms of precision,
recall and F-measure.
170
Corpus #
Raw text Components Types Full
R P F R P F R P F R P F
Press
Q1
0.874 0.777 0.823 0.876 0.741 0.803 0.824 0.870 0.846 0.852 0.800 0.825
0.810 0.766 0.788 0.815 0.777 0.796 0.645 0.724 0.683 0.844 0.785 0.813
Q2
0.765 0.796 0.780 0.870 0.773 0.819 0.822 0.801 0.812 0.917 0.773 0.839
0.558 0.654 0.602 0.826 0.775 0.800 0.815 0.777 0.795 0.816 0.752 0.783
Q3
0.835 0.715 0.771 0.888 0.809 0.847 0.884 0.796 0.837 0.887 0.859 0.873
0.792 0.689 0.736 0.904 0.780 0.837 0.876 0.771 0.820 0.780 0.827 0.803
Q4
0.802 0.757 0.779 0.845 0.876 0.860 0.900 0.702 0.789 0.914 0.840 0.876
0.794 0.727 0.759 0.696 0.715 0.705 0.812 0.701 0.752 0.802 0.757 0.779
Europarl
Q1
0.809 0.728 0.766 0.800 0.568 0.665 0.776 0.862 0.817 0.754 0.720 0.736
0.754 0.720 0.736 0.720 0.609 0.660 0.687 0.607 0.644 0.736 0.638 0.683
Q2
0.776 0.792 0.784 0.782 0.617 0.690 0.797 0.645 0.713 0.821 0.526 0.641
0.563 0.498 0.529 0.802 0.619 0.699 0.698 0.553 0.617 0.769 0.566 0.652
Q3
0.747 0.459 0.569 0.749 0.624 0.681 0.805 0.800 0.803 0.735 0.744 0.739
0.732 0.598 0.658 0.736 0.717 0.726 0.822 0.738 0.777 0.808 0.734 0.769
Q4
0.742 0.624 0.678 0.874 0.760 0.813 0.732 0.480 0.580 0.743 0.608 0.669
0.721 0.566 0.634 0.695 0.652 0.672 0.707 0.600 0.649 0.738 0.603 0.664
Table 3: Overall recall, precision and F-measure for each pair of annotators (blue: pair #1, ocre: pair
#2, green: pair #3, white: pair #4) on each corpus quarter (Q1, Q2, Q3, Q4), depending on the kind of
pre-annotation (raw text, only components, only types, full pre-annotation). Expert annotator is on the
upper line of each quarter, novice annotator is on the lower line. Boldface indicates the best F-measure
for each novice and expert annotator among all pre-annotation tasks in a given corpus quarter
Corpus
Components Types Full
F SER F SER F SER
P
re
ss
Q1 72.4 37.9 63.5 46.3 68.9 41.0
Q2 77.2 32.2 66.8 43.5 73.1 36.6
Q3 76.1 34.1 68.3 41.7 73.1 36.9
Q4 76.1 33.3 63.3 45.7 71.0 38.2
E
ur
op
ar
l Q1 61.9 49.9 57.5 55.4 60.1 52.2
Q2 61.2 51.3 54.6 54.3 58.5 52.5
Q3 61.6 50.1 53.3 55.7 58.2 52.2
Q4 57.1 57.0 48.1 59.7 53.3 58.1
Broad. 88.3 29.1 73.1 39.1 73.2 33.1
Table 2: F-measure and Slot Error Rate achieved
by the automatic system on each kind of annota-
tion and on in-domain broadcast data
We also computed inter-annotator agreement
(IAA) for each corpus considering two groups of
annotators, experts and novices. We consider that
the inter-annotator agreement is somewhere be-
tween the F-measure and the standard IAA con-
sidering as markables all the units annotated by at
least one of the annotators (Grouin et al, 2011).
We computed Scott?s Pi (Scott, 1955), and Co-
hen?s Kappa (Cohen, 1960). The former considers
one model for all annotators while the latter con-
siders one model per annotator. In our case, these
two values are almost the same, which means that
the proportions and kinds of annotations are very
similar across experts and novices. Figure 2 shows
the IAA (Cohen?s Kappa and F-measure) obtained
on the two corpora given the four pre-annotation
conditions (no pre-annotation, components, types,
and full pre-annotation). As we can see, IAA is
systematically higher for the Press corpus than
for the Europarl corpus, which can be linked
to the higher performance of the automatic pre-
annotation system on this corpus. We also can see
that pre-annotation always improves agreement
and that full pre-annotation yields the best result.
We observe that, as expected, pre-annotation leads
human annotators to obtain higher consistency.
5 Subjective assessment
An important piece of information in any anno-
tation campaign is the feelings of the annotators
about the task. This can give interesting clues
about the expected quality of their work and on the
usefulness of the pre-annotation step. We asked
the annotators a few questions concerning sev-
eral features of this project, such as the annotation
171
 0.5
 0.6
 0.7
 0.8
 0.9
 1
raw comp types full
Press: Cohen's kappaPress: F-measureEuroparl: Cohen's kappaEuroparl: F-measure
Figure 2: Cohen?s Kappa (red and blue) and F-
measure (green and pink) measuring agreement of
experts and novices on Press and Europarl corpora
in four pre-annotation conditions. Each measure
compares the concatenated annotations of the four
experts with the four novices.
manual, or how they assessed the benefits of pre-
annotation in the different corpora (Section 5.1).
Another important point is the experience of the
annotators, which we also examine in the light of
theirs answers to the questionnaire (Section 5.2).
5.1 Questionnaire
The questionnaire submitted to the annotators con-
tained 4 questions, dealing with their feedback on
the annotation process:
1. According to you, which level of pre-
annotation has been the most helpful during
the annotation process? Types, components,
or both?
2. To what extent would you say that pre-
annotation helped you in terms of precision
and speed? Did it produce many errors you
had to correct?
3. If you had to choose between the Europarl
corpus and the Press corpus, could you say
that one has been easier to annotate than the
other?
4. Concerning the annotation manual, are there
topics that you would like to change, or cor-
rect? In the same way, which named entities
caused you the most difficulties to deal with?
All 8 annotators answered these questions. We
summarize below what we found in their answers.
5.1.1 Level of pre-annotation
Most of the annotators preferred the corpora that
were pre-annotated with types only. The reason,
for the most part, is that a pre-annotation of types
allows the annotator to work faster on their files,
because guessing the components from the types
is easier than guessing types from components.2
Indeed, the different types of entities defined in
the manual always imply the same components,
be they specific (to one entity type) or transverse
(common to several entity types). On the contrary,
a transverse component, such as <kind>, can be
part of any type of named entity. The other rea-
son for this choice of pre-annotation concerns the
readability brought to the corpora. An annotation
with types only is easier to read than an annotation
with components, and less exhausting after many
hours of work on the texts.
5.1.2 Gain in precision and speed
What motivated the answers to the second ques-
tion mainly concerns the accuracy of the different
pre-annotation methods. While all of them pre-
sented errors that needed to be corrected, the pre-
annotation of types was the one that they felt pre-
sented the smaller number of errors. Thus, annota-
tors spent less time reviewing the corpora in search
of errors, compared to the other pre-annotated cor-
pora (with components, and with both types and
components), where more errors had to be spot-
ted and corrected. This search for incorrect pre-
annotations impacted the time spent on each cor-
pus. Indeed, most annotators declared that pre-
annotation with types was quicker to deal with
than other pre-annotation schemes.
5.1.3 Corpus differences
About one half of the annotators agreed that the
Europarl corpus had been more difficult to anno-
tate. Despite obvious differences in register, sen-
tence structure and vocabulary between the two
corpora, Europarl seemed more redundant and
complex than the other corpus. For instance, one
of the annotators declared:
The Europarl corpus is more difficult
to annotate in the sense that the exist-
ing types and components do not al-
ways match the realities found in the
corpus, either because their definitions
2This feeling is supported by results about ambiguity pre-
sented in Fort et al (2012).
172
cannot apply exactly, or because the re-
quired types and components are miss-
ing (mainly for frequencies: ?five times
per year?).
The other half of the annotators did not feel any
specific difficulties in annotating one corpus or the
other. According to them, both corpora are the
same in terms of register and sentence structure.
5.1.4 Improvements in guidelines
All of the annotators were unanimous in think-
ing that two points need to be modified in the
manual. First of all, the distinction between the
<org.adm> and <org.ent> subtypes is too diffi-
cult to apprehend, above all in the Europarl corpus
where these entities are too ambiguous to be anno-
tated correctly. Secondly, the distinction between
the <pers> and <func> types has also been diffi-
cult to deal with. The other remarks about poten-
tial changes mainly concerned the introduction of
explicit rules for frequencies, which are recurrent
in the Europarl corpus.
5.2 Experience
As mentioned earlier in Section 4.1, we will now
see if the differences in experience between an-
notators impacted their difficulty in annotating the
corpora. First of all, when we look at the answers
given to question 3, we notice that both novice and
expert annotators consider the Europarl corpus the
most difficult to annotate. Most of their answers
deal with the redundancy and the formal register
of the data. Moreover, as everyone answered in
question 4, both <func> and <org> entities have
to be modified to be easier to understand and to
use. This unanimous opinion about what needs
to be reviewed in the manual allows us to think
that the annotators? level of experience has a low
impact on their apprehension of the corpora, both
Europarl and Press. To confirm this, we can look
at the answers given to questions 1 and 2, as indi-
cated in the previous paragraph. As has been ex-
plained, every annotator correctly pointed at the
many errors found in pre-annotation, regardless
of their experience. Besides, the assessment of
the benefits of pre-annotation is the same for al-
most everyone, regardless of their experience too:
both novice and expert annotators agree that pre-
annotation with type adds efficiency and speed to
annotation.
To conclude, according to our observations
based on the questionnaire, we cannot assert that
there has been a difference between novice and ex-
pert annotators. Both groups agreed on the same
difficulties, pointed at the same errors, and crit-
icized the same entities, saying that their defini-
tions needed to be clarified.
6 Quantitative observations
In this section we provide results of quantitative
observations in order to support, or not, the anno-
tators? subjective assessment.
6.1 Corpus statistics
The annotators reported different feelings depend-
ing on the corpora. Some of them reported that
the Europarl corpus was more difficult to annotate,
with more complex sentence structures, or usage
of fewer proper nouns.
To explore these differences, we computed
some statistics over the two original, un-annotated
corpora (which are much larger than the samples
annotated in this experiment) as well as over the
original broadcast news corpus used to train the
pre-annotation system. Each of these corpora con-
tains several million words.
Table 4 reports simple statistics about sentences
in the three corpora. Based on these statistics,
while the Europarl (Euro) corpus is very similar to
the original Broadcast News (BN), the Press cor-
pus shows differences: sentences are 20% shorter,
with fewer but larger chunks, confirming the im-
pression of simpler, less convoluted sentences.
BN Press Euro
Mean sentence length 30.2 23.9 29.7
Mean chunk count 10.9 6.7 10.4
Mean chunk length 2.7 3.6 2.8
Table 4: Sentence summary of the three corpora
Looking more closely at the contents of these
sentences, Figure 3 summarizes the proportions of
grammatical word classes. The sentiment of ex-
tensive naming of entities in the Press corpus is
confirmed by the four times higher rate of proper
nouns. On the other hand, entities are more often
referred to using nouns with an optional adjective
in the Europarl corpus, leading to a more frequent
usage of the latter.
173
Figure 3: Frequency of word classes in the three
corpora (BN = Broadcast News, Est = Press, Euro
= Europarl). TOOL = grammatical words, PCT/NB
= punctuation and numbers, ADJ/ADV = adjectives
and adverbs, NAM = proper name, NOM = noun,
VER = verb.
6.2 Influence of pre-annotation on the
behaviour of annotators
As already mentioned, it is often reported that a
bias may occur depending on human confidence
in the pre-annotation (Fort and Sagot, 2010; Re-
hbein et al, 2009; South et al, 2011). An im-
portant unknown is always the influence of pre-
annotation on the behaviour of annotators, and at
which point pre-annotation induces more errors
than it helps. This may obviously depend on pre-
annotation quality. Table 5 summarizes the er-
ror rates of the automatic annotator in the stud-
ied data (Press + Europarl) and in comparison to
in-domain data. Insertions (Ins) are extra anno-
tations, deletions (Del) missing annotations, and
substitutions (Subs) are annotations that are incor-
rect in type, boundaries, or both. We can see that
Domain Pre-annotation Ins Del Subs
Components 4.4% 33.6% 7.8%
Out Types 7.0% 36.2% 12.7%
Full 5.5% 34.6% 9.7%
In Full 3.7% 23.4% 10.6%
Table 5: Pre-annotation errors and comparison
with in-domain (Broadcast News) data
going out-of-domain increased deletions, proba-
bly through a lack of knowledge of domain vo-
cabulary. But it did not influence the other error
rates significantly. It is also noticeable that dele-
tion is the type of error most produced by the sys-
tem, with every third entity missed. Automatic,
full pre-annotation of Press + Europarl obtains a
precision of 0.79 and a recall of 0.56.
Human annotator performance can then be mea-
sured over the same three error types (Table 6). We
Pre-annotation Ins Del Subs
Raw 8.9% 18.9% 12.8%
Components 5.9% 16.7% 11.3%
Types 7.1% 16.5% 12.0%
Full 7.1% 16.5% 10.1%
Table 6: Mean human annotation error levels for
each pre-annotation scheme
can see that annotation quality was systematically
improved by pre-annotation, with the best global
result obtained by full pre-annotation. In addition
there was no increase in deletions (had the human
stopped looking at the unannotated text) or inser-
tions (had the human always trusted the system) as
might have been feared. This may be a side effect
of the high deletion rate, making it obvious to the
human that the system was missing things. In any
case, the annotation was clearly beneficial in our
experiment with no ill effects seen in error rates
compared to the gold standard.
6.3 Is pre-annotation useful and to whom?
All annotators asserted that pre-annotation is use-
ful, specifically with types. In this section, we pro-
vide observations concerning variations in annota-
tion both in terms of accuracy (F-measure is used)
and duration.
Raw Comp. Types Full
Experts 0.748 0.786 0.778 0.791
Novices 0.682 0.737 0.721 0.742
Table 7: Mean F-measure of experts and novices,
for each pre-annotation scheme
Raw Comp. Types Full
Experts 109.0 52.5 64.0 39.13
Novices 151.7 135.5 117.9 103.88
Table 8: Mean duration (in minutes) of annotation
for experts and novices, for each pre-annotation
scheme (two corpus quarters)
Tables 7 and 8 confirm the hypothesis that auto-
matic pre-annotation helps annotators to annotate
174
faster and to be more efficient. All pre-annotation
levels (components, types and both) seem to be
helpful for both experts and novices. Experts
reached a higher accuracy (F=0.791) and they
were more than twice faster with components or
full pre-annotation. Similarly, novices performed
better when working on a full pre-annotation
(F=0.742) and reached a faster working time
(48mn less than with no pre-annotation). This last
observation contradicts the annotators? reported
experience: the annotators felt more comfortable
and faster with a types-only pre-annotation than
with full pre-annotation (see Section 5.1.2). The
results show that full pre-annotation was the best
choice for both quality and speed.
These results confirm that pre-annotation is use-
ful, even with a moderate level of performance of
the system. Does it help to annotate components
and types equally? To answer this question, we
computed the F-measure of novices and experts
for both components and types separately (see Fig-
ure 4).
 60
 65
 70
 75
 80
 85
 90
raw comp types full
types/novicescomponents/novicestypes/expertscomponents/experts
Figure 4: Mean F-measure on each pre-annotation
level for expert and novice annotators
For experts we can see that all pre-annotation
levels allow them to improve their performance on
both types and components. However for novices,
pre-annotation with types does not improve their
performance in labeling components. We also no-
tice that pre-annotation in both types and compo-
nents allows experts and novices to reach their best
performance for both types and components.
7 Conclusion and Perspectives
Conclusion. In this paper, we studied the inter-
est of a pre-annotation process for a complex an-
notation task with only an out-of-domain annota-
tion system available. We also designed our exper-
iments to check whether the level of experience of
the annotators made a difference in such a context.
The experiment produced in the end a high-quality
gold standard (8-way merge including 2 versions
without pre-annotation) which enabled us to mea-
sure quantitatively the performance of every pre-
annotation scheme.
We noticed that the pre-annotation system
proved relatively precise for such a complex task,
with 79% correct pre-annotations, but with a poor
recall at 56%. This may be a good operating point
for a pre-annotation system to reduce bias though.
In our quantitative experiments we found that
the fullest pre-annotation helped most, both in
terms of quality and annotation speed, even though
the quality of the pre-annotation system varied de-
pending on the annotation layer. This contradicted
the feelings of the annotators who thought that a
type-only pre-annotation was the most efficient.
This shows that in such a setting self-evaluation
cannot be trusted. On the other hand their remarks
about the problems in the annotation guide itself
seemed rather pertinent.
When it comes to experts vs. novices, we noted
that their behaviour and remarks were essentially
identical. Experts were both better and faster
at annotating, but had similar reactions to pre-
annotation and essentially the same feelings.
In conclusion, even with an out-of-domain sys-
tem, a pre-annotation step proves extremely useful
in both annotation speed and annotation quality,
and at least in our setting, with a reasonably pre-
cise system (at the expense of recall) no bias was
detectable. In addition, no matter what the anno-
tators feel, as long as precision is good enough,
the more pre-annotations the better. Pre-filtering
either of our two levels did not help.
Perspectives. Based upon this conclusion, we
plan to use automatic pre-annotation in further an-
notation work, beginning with the present corpora.
As a first use, we plan to propose a few changes
to the annotation principles in the guidelines we
used. To annotate existing corpora with these
changes, automatic pre-annotation will be useful.
As a second piece of future work, we plan to
annotate new corpora with the existing annotation
framework. We also plan to add new types of
named entities (e.g., events) to extend the anno-
tation of existing annotated corpora, using the pre-
annotation process to reduce the overall workload.
175
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program and by the French ANR
VERA project.
References
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Sandipan Dandapat, Priyanka Biswas, Monojit Choud-
hury, and Kalika Bali. 2009. Complex linguistic an-
notation ? no easy way out! A case from Bangla and
Hindi POS labeling tasks. In Proc of 3rd Linguistic
Annotation Workshop (LAW-III), pages 10?18, Sun-
tec, Singapore, August. ACL.
Marco Dinarelli and Sophie Rosset. 2011. Models
cascade for tree-structured named entity detection.
In Proc of IJCNLP, pages 1269?1278, Chiang Mai,
Thailand.
Manaal Faruqui and Sebastian Pado?. 2010. Training
and evaluating a German named entity recognizer
with semantic generalization. In Proc of Konvens,
Saarbru?cken, Germany.
Kare?n Fort and Beno??t Sagot. 2010. Influence of pre-
annotation on POS-tagged corpus development. In
Proc of 4th Linguistic Annotation Workshop (LAW-
IV), pages 56?63, Uppsala, Sweden. ACL.
Kare?n Fort, Adeline Nazarenko, and Sophie Rosset.
2012. Modeling the complexity of manual anno-
tation tasks: a grid of analysis. In Proceedings of
COLING 2012, pages 895?910, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Olivier Galibert, Sophie Rosset, Cyril Grouin, Pierre
Zweigenbaum, and Ludovic Quintard. 2011. Struc-
tured and extended named entity evaluation in au-
tomatic speech transcriptions. In Proc of IJCNLP,
Chiang Mai, Thailand.
Cyril Grouin, Sophie Rosset, Pierre Zweigenbaum,
Kare?n Fort, Olivier Galibert, and Ludovic Quin-
tard. 2011. Proposal for an extension of tradi-
tional named entities: From guidelines to evalua-
tion, an overview. In Proc of 5th Linguistic Anno-
tation Workshop (LAW-V), pages 92?100, Portland,
OR. ACL.
Minlie Huang, Aure?lie Ne?ve?ol, and Zhiyong Lu.
2011. Recommending MeSH terms for annotating
biomedical articles. Journal of the American Medi-
cal Informatics Association, 18(5):660?7.
Geoffrey Leech. 1997. Introducing corpus annota-
tion. In Roger Garside, Geoffrey Leech, and Tony
McEnery, editors, Corpus annotation: Linguistic in-
formation from computer text corpora, pages 1?18.
Longman, London.
John Makhoul, Francis Kubala, Richard Schwartz, and
Ralph Weischedel. 1999. Performance measures for
information extraction. In Proc. of DARPA Broad-
cast News Workshop, pages 249?252.
Mitchell P Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn TreeBank. Com-
putational Linguistics, 19(2):313?330.
Claudiu Mihaila, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. Biocause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14:2.
Ce?line Poudat and Doninique Longre?e. 2009. Vari-
ations langagie`res et annotation morphosyntaxique
du latin classique. Traitement Automatique des
Langues, 50(2):129?148.
Dietrich Rebholz-Schuhmann, Antonio Jimeno, Chen
Li, Senay Kafkas, Ian Lewin, Ning Kang, Peter Cor-
bett, David Milward, Ekaterina Buyko, Elena Beiss-
wanger, Kerstin Hornbostel, Alexandre Kouznetsov,
Rene? Witte, Jonas B Laurila, Christopher JO Baker,
Cheng-Ju Kuo, Simone Clematide, Fabio Rinaldi,
Richa?rd Farkas, Gyo?rgy Mo?ra, Kazuo Hara, Laura I
Furlong, Michael Rautschka, Mariana Lara Neves,
Alberto Pascual-Montano, Qi Wei, Nigel Collier,
Md Faisal Mahbub Chowdhury, Alberto Lavelli,
Rafael Berlanga, Roser Morante, Vincent Van Asch,
Walter Daelemans, Jose? L Marina, Erik van Mulli-
gen, Jan Kors, and Udo Hahn. 2011. Assessment of
NER solutions against the first and second CALBC
silver standard corpus. J Biomed Semantics, 2.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of partial
automatic pre-labeling for frame-semantic annota-
tion. In Proc of 3rd Linguistic Annotation Workshop
(LAW-III), pages 19?26, Suntec, Singapore. ACL.
Eric Ringger, Peter McClanahan, Robbie Haertel,
George Busby, Marc Carmen, James Carroll, Kevin
Seppi, and Deryle Lonsdale. 2007. Active learning
for part-of-speech tagging: Accelerating corpus an-
notation. In Proc of Linguistic Annotation Workshop
(LAW), pages 101?108. ACL.
Sophie Rosset, Cyril Grouin, Kare?n Fort, Olivier Gal-
ibert, Juliette Kahn, and Pierre Zweigenbaum. 2012.
Structured named entities in two distinct press cor-
pora: Contemporary broadcast news and old news-
papers. In Proc of 6th Linguistic Annotation Work-
shop (LAW-VI), pages 40?48, Jeju, South Korea.
ACL.
William A Scott. 1955. Reliability of content analysis:
The case of nominal scale coding. Public Opinion
Quaterly, 19(3):321?325.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proc
of the NIPS Workshop on Cost-Sensitive Learning.
176
Arne Skj?rholt. 2011. More, faster: Accelerated cor-
pus annotation with statistical taggers. Journal for
Language Technology and Computational Linguis-
tics, 26(2):151?163.
Brett R South, Shuying Shen, Robyn Barrus, Scott L
DuVall, O?zlem Uzuner, and Charlene Weir. 2011.
Qualitative analysis of workflow modifications used
to generate the reference standard for the 2010
i2b2/VA challenge. In Proc of AMIA.
177
