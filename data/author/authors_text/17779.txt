Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 57?66,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Head Finalization Reordering for Chinese-to-Japanese
Machine Translation
Han Dan+ Katsuhito Sudoh? Xianchao Wu??
Kevin Duh?? Hajime Tsukada? Masaaki Nagata?
+The Graduate University For Advanced Studies, Tokyo, Japan
?NTT Communication Science Laboratories, NTT Corporation
+handan@nii.ac.jp, ?wuxianchao@baidu.com, ?kevinduh@is.naist.jp
?{sudoh.katsuhito, tsukada.hajime, nagata.masaaki}@lab.ntt.co.jp
Abstract
In Statistical Machine Translation, reorder-
ing rules have proved useful in extracting
bilingual phrases and in decoding during
translation between languages that are struc-
turally different. Linguistically motivated
rules have been incorporated into Chinese-
to-English (Wang et al, 2007) and English-
to-Japanese (Isozaki et al, 2010b) transla-
tion with significant gains to the statistical
translation system. Here, we carry out a lin-
guistic analysis of the Chinese-to-Japanese
translation problem and propose one of the
first reordering rules for this language pair.
Experimental results show substantially im-
provements (from 20.70 to 23.17 BLEU)
when head-finalization rules based on HPSG
parses are used, and further gains (to 24.14
BLEU) were obtained using more refined
rules.
1 Introduction
In state-of-the-art Statistical Machine Translation
(SMT) systems, bilingual phrases are the main
building blocks for constructing a translation given
a sentence from a source language. To extract
those bilingual phrases from a parallel corpus,
the first step is to discover the implicit word-
to-word correspondences between bilingual sen-
tences (Brown et al, 1993). Then, a symmetriza-
tion matrix is built (Och and Ney, 2004) by us-
ing word-to-word alignments, and a wide variety
?Now at Baidu Japan Inc.
? Now at Nara Institute of Science and Technology
(NAIST)
of heuristics can be used to extract the bilingual
phrases (Zens et al, 2002; Koehn et al, 2003).
This method performs relatively well when the
source and the target languages have similar word
order, as in the case of French, Spanish, and En-
glish. However, when translating between lan-
guages with very different structures, as in the case
of English and Japanese, or Japanese and Chinese,
the quality of extracted bilingual phrases and the
overall translation quality diminishes.
In the latter scenario, a simple but effective strat-
egy to cope with this problem is to reorder the
words of sentences in one language so that it re-
sembles the word order of another language (Wu
et al, 2011; Isozaki et al, 2010b). The advan-
tages of this strategy are two fold. The first ad-
vantage is at the decoding stage, since it enables
the translation to be constructed almost monoton-
ically. The second advantage is at the training
stage, since automatically estimated word-to-word
alignments are likely to be more accurate and sym-
metrization matrices reveal more evident bilingual
phrases, leading to the extraction of better quality
bilingual phrases and cleaner phrase tables.
In this work, we focus on Chinese-to-Japanese
translation, motivated by the increasing interaction
between these two countries and the need to im-
prove direct machine translation without using a
pivot language. Despite the countries? close cul-
tural relationship, their languages significantly dif-
fer in terms of syntax, which poses a severe diffi-
culty in statistical machine translation. The syntac-
tic relationship of this language pair has not been
carefully studied before in the machine translation
57
field, and our work aims to contribute in this direc-
tion as follows:
? We present a detailed syntactic analysis of
several reordering issues in Chinese-Japanese
translation using the information provided by
an HPSG-based deep parser.
? We introduce novel reordering rules based on
head-finalization and linguistically inspired
refinements to make words in Chinese sen-
tences resemble Japanese word order. We em-
pirically show its effectiveness (e.g. 20.70 to
24.23 BLEU improvement).
The paper is structured as follows. Section 2 in-
troduces the background and gives an overview of
similar techniques related to this work. Section 3
describes the proposed method in detail. Exper-
imental evaluation of the performance of the pro-
posed method is described in section 4. There is an
error analysis on the obtained results in section 5.
Conclusions and a short description on future work
derived from this research are given in the final
section.
2 Background
2.1 Head Finalization
The structure of languages can be characterized
by phrase structures. The head of a phrase is the
word that determines the syntactic category of the
phrase, and its modifiers (also called dependents)
are the rest of the words within the phrase. In En-
glish, the head of a phrase can be usually found
before its modifiers. For that reason, English is
called a head-initial language (Cook and Newson,
1988). Japanese, on the other hand, is head-final
language (Fukui, 1992), since the head of a phrase
always appears after its modifiers.
In certain applications, as in the case of ma-
chine translation, word reordering can be a promis-
ing strategy to ease the task when working with
languages with different phrase structures like En-
glish and Japanese. Head Finalization is a success-
ful syntax-based reordering method designed to re-
order sentences from a head-initial language to re-
semble the word order in sentences from a head-
final language (Isozaki et al, 2010b). The essence
of this rule is to move the syntactic heads to the
end of its dependency by swapping child nodes in
a phrase structure tree when the head child appears
before the dependent child.
Isozaki et al (2010b) proposed a simple method
of Head Finalization, by using an HPSG-based
deep parser for English (Miyao and Tsujii, 2008)
to obtain phrase structures and head information.
The score results from several mainstream evalua-
tion methods indicated that the translation quality
had been improved; the scores of Word Error Rate
(WER) and Translation Edit Rate (TER) (Snover
et al, 2006) had especially been greatly reduced.
2.2 Chinese Deep Parsing
Syntax-based reordering methods need parsed sen-
tences as input. Isozaki et al (2010b) used Enju,
an HPSG-based deep parser for English, but they
also discussed using other types of parsers, such
as word dependency parsers and Penn Treebank-
style parsers. However, to use word dependency
parsers, they needed an additional heuristic rule to
recover phrase structures, and Penn Treebank-style
parsers are problematic because they output flat
phrase structures (i.e. a phrase may have multiple
dependents, which causes a problem of reorder-
ing within a phrase). Consequently, compared to
different types of parsers, Head-Final English per-
forms the best on the basis of English Enju?s pars-
ing result.
In this paper, we follow their observation, and
use the HPSG-based parser for Chinese (Chinese
Enju) (Yu et al, 2011) for Chinese syntactic pars-
ing. Since Chinese Enju is based on the same pars-
ing model as English Enju, it provides rich syn-
tactic information including phrase structures and
syntactic/semantic heads.
Figure 1 shows an example of an XML output
from Chinese Enju for the sentence ?wo (I) qu (go
to) dongjing (Tokyo) he (and) jingdu (Ky-
oto).? The label <cons> and <tok> represent
the non-terminal nodes and terminal nodes, respec-
tively. Each node is identified by a unique ?id?
and has several attributes. The attribute ?head?
indicates which child node is the syntactic head.
In this figure, <head=?c4? id=?c3?> means that
the node that has id=?c4? is the syntactic head of
the node that has id=?c3?.
58
Figure 1: An XML output for a Chinese sentence from
Chinese Enju. For clarity, we only draw information
related to the phrase structure and the heads.
2.3 Related Work
Reordering is a popular strategy for improving
machine translation quality when source and tar-
get languages are structurally very different. Re-
searchers have approached the reordering problem
in multiple ways. The most basic idea is pre-
ordering (Xia and McCord, 2004; Collins et al,
2005), that is, to do reordering during preprocess-
ing time, where the source side of the training and
development data and sentences from a source lan-
guage that have to be translated are first reordered
to ease the training and the translation, respec-
tively. In (Xu et al, 2009), authors used a depen-
dency parser to introduce manually created pre-
ordering rules to reorder English sentences when
translating into five different SOV(Subject-Object-
Verb) languages. Other authors (Genzel, 2010; Wu
et al, 2011) use automatically generated rules in-
duced from parallel data. Tillmann (2004) used a
lexical reordering model, and Galley et al (2004)
followed a syntactic-based model.
In this work, however, we are centered in the
design of manual rules inspired by the Head Final-
ization (HF) reordering (Isozaki et al, 2010b). HF
reordering is one of the simplest methods for pre-
ordering that significantly improves word align-
ments and leads to a better translation quality. Al-
though the method is limited to translation where
the target language is head-final, it requires neither
training data nor fine-tuning. To our knowledge,
HF is the best method to reorder languages when
translating into head-final languages like Japanese.
The implementation of HF method for English-
to-Japanese translation appears to work well. A
reasonable explanation for this is the close match
between the concept of ?head? in this language
pair. However, for Chinese-to-Japanese, there are
differences in the definitions of numbers of impor-
tant syntactic concepts, including the definition of
the syntactic head. We concluded that the diffi-
culties we encountered in using HF to Chinese-to-
Japanese translation were the result of these differ-
ences in the definition of ?head?. As we believe
that such differences are also likely to be observed
in other language pairs, the present work is gener-
ally important for head-initial to head-final trans-
lation as it shows a systematic linguistic analysis
that consistently improves the effectivity of the HF
method.
3 Syntax-based Reordering Rules
This section describes our method for syntax-
based reordering for Chinese-to-Japanese transla-
tion. We start by introducing Head Finalization
for Chinese (HFC), which is a simple adaptation
of Isozaki et al (2010b)?s method for English-to-
Japanese translation. However, we found that this
simple method has problems when applied to Chi-
nese, due to peculiarities in Chinese syntax. In
Section 3.2, we analyze several distinctive cases of
the problem in detail. And following this analysis,
Section 3.3 proposes a refinement of the original
HFC, with a couple of exception rules for reorder-
ing.
3.1 Head Finalization for Chinese (HFC)
Since Chinese and English are both known to be
head-initial languages1, the reordering rule intro-
duced in (Isozaki et al, 2010b) ideally would re-
order Chinese sentences to follow the word order
1As Gao (2008) summarized, whether Chinese is a head-
initial or a head-final language is open for debate. Neverthe-
less, we take the view that most Chinese sentence structures
are head-initial since the written form of Chinese mainly be-
haves as an head-initial language.
59
Figure 2: Simple example for Head-Final Chinese. The left figure shows the parsing tree of the original sentence
and its English translation. The right figure shows the reordered sentence along with its Japanese translation.
( ?*? indicate the syntactic head).
of their Japanese counterparts.
Figure 2 shows an example of a head finalized
Chinese sentence based on the output from Chi-
nese Enju shown in Figure 1. Notice that the
coordination exception rule described in (Isozaki
et al, 2010b) also applies to Chinese reordering.
This exception rule says that child nodes are not
swapped if the node is a coordination2. Another
exception rule is for punctuation symbols, which
are also preserved in their original order. In this
case, as can be seen in the example in Figure 2, the
nodes of c3, c6, and c8 had not been swapped with
their dependency. In this account, only the verb
?qu? had been moved to the end of the sentence,
following the same word order as its Japanese
translation.
3.2 Discrepancies in Head Definition
Head Finalization relies on the idea that head-
dependent relations are largely consistent among
different languages while word orders are differ-
ent. However, in Chinese, there has been much
debate on the definition of head3, possibly because
Chinese has fewer surface syntactic features than
other languages like English and Japanese. This
causes some discrepancies between the definitions
2Coordination is easily detected in the output of
Enju; it is marked by the attributes xcat="COOD" or
schema="coord-left/right" as shown in Figure 1.
3In this paper, we only consider the syntactic head.
of the head in Chinese and Japanese, which leads
to undesirable reordering of Chinese sentences.
Specifically, in preliminary experiments we ob-
served unexpected reorderings that are caused by
the differences in the head definitions, which we
describe below.
3.2.1 Aspect Particle
Although Chinese has no syntactic tense marker,
three aspect particles following verbs can be used
to identify the tense semantically. They are ?le0?
(did), ?zhe0? (doing), and ?guo4? (done), and
their counterparts in Japanese are ?ta?, ?teiru?,
and ?ta?, respectively. Both the first word and
third word can represent the past tense, but the
third one is more often used in the past perfect.
The Chinese parser4 treated aspect particles as
dependents of verbs, whereas their Japanese coun-
terparts are identified as the head. For exam-
ple in Table 15, ?qu? (go) and ?guo? (done)
aligned with ?i? and ?tta?, respectively. How-
ever, since ?guo? is treated as a dependent of
?qu?, by directly implementing the Head Final
Chinese (HFC), the sentence will be reordered like
4The discussions in this section presuppose the syntactic
analysis done by Chinese Enju, but most of the analysis is
consistent with the common explanation for Chinese syntax.
5English translation (En); Chinese original sentence
(Ch); reordered Chinese by Head-Final Chinese (HFC); re-
ordered Chinese by Refined Head-Final Chinese (R-HFC)
and Japanese translation (Ja).
60
HFC in Table 1, which does not follow the word
order of the Japanese (Ja) translation. In contrast,
the reordered sentence from refined-HFC (R-HFC)
can be translated monotonically.
En I have been to Tokyo.
Ch wo qu guo dongjing.
HFC wo dongjing guo qu.
R-HFC wo dongjing qu guo.
Ja watashi (wa) Tokyo (ni) i tta.
Table 1: An example for Aspect Particle. Best word
alignment Ja-Ch (En): ?watashi? ? ?wo?(I); ?Tokyo? ?
?dongjing? (Tokyo); ?i? ? ?qu? (been); ?tta? ? ?guo?
(have).
3.2.2 Adverbial Modifier ?bu4?
Both in Chinese and Japanese, verb phrase mod-
ifiers typically occur in pre-verbal positions, espe-
cially when the modifiers are adverbs. Since ad-
verbial modifiers are dependents in both Chinese
and Japanese, head finalization works perfectly for
them. However, there is an exceptional adverb,
?bu4?, which means negation and is usually trans-
lated into ?nai?, which is always at the end of the
sentence in Japanese and thus is the head. For ex-
ample in Table 2, the word ?kan? (watch) will be
identified as the head and the word ?bu? is its de-
pendent; on the contrary, in the Japanese transla-
tion (Ja), the word ?nai?, which is aligned with
?bu?, will be identified as the head. Therefore,
the Head Final Chinese is not in the same order,
but the reordered sentence by R-HFC obtained the
same order with the Japanese translation.
En I do not watch TV.
Ch wo bu kan dianshi.
HFC wo dianshi bu kan.
R-HFC wo dianshi kan bu.
Ja watashi (wa) terebi (wo) mi nai.
Table 2: An example for Adverbial Modifier bu4.
Best word alignment Ja-Ch (En): ?watashi? ? ?wo? (I);
?terebi? ? ?dianshi? (TV); ?mi? ? ?kan? (watch); ?nai?
? ?bu? (do not).
3.2.3 Sentence-final Particle
Sentence-final particles often appear at the end
of a sentence to express a speaker?s attitude:
e.g. ?ba0, a0? in Chinese, and ?naa, nee? in
Japanese. Although they appear in the same posi-
tion in both Chinese and Japanese, in accordance
with the differences of head definition, they are
identified as the dependent in Chinese while they
are the head in Japanese. For example in Table 3,
since ?a0? was identified as the dependent, it had
been reordered to the beginning of the sentence
while its Japanese translation ?nee? is at the end
of the sentence as the head. Likewise, by refining
the HFC, we can improve the word alignment.
En It is good weather.
Ch tianqi zhenhao a.
HFC a tianqi zhenhao.
R-HFC tianqi zhenhao a.
Ja ii tennki desu nee.
Table 3: An example for Sentence-final Particle.
Best word alignment Ja-Ch (En): ?tennki? ? ?tianqi?
(weather); ?ii? ? ?zhenhao? (good); ?nee? ? ?a? (None).
3.2.4 Et cetera
In Chinese, there are two expressions for rep-
resenting the meaning of ?and other things? with
one Chinese character: ?deng3? and ?deng3
deng3?, which are both identified as dependent
of a noun. In contrast, in Japanese, ?nado? is al-
ways the head because it appears as the right-most
word in a noun phrase. Table 4 shows an example.
En Fruits include apples, etc.
Ch shuiguo baokuo pingguo deng.
HFC shuiguo deng pingguo baokuo.
R-HFC shuiguo pingguo deng baokuo.
Ja kudamono (wa) ringo nado (wo)
fukunde iru.
Table 4: An example for Et cetera. Best word alignment
Ja-Ch (En): ?kudamono? ? ?shuiguo? (Fruits); ?ringo?
? ?pingguo? (apples); ?nado? ? ?deng? (etc.); ?fukunde
iru? ? ?baokuo? (include).
61
AS Aspect particle
SP Sentence-final particle
ETC et cetera (i.e. deng3 and deng3 deng3)
IJ Interjection
PU Punctuation
CC Coordinating conjunction
Table 5: The list of POSs for exception reordering rules
3.3 Refinement of HFC
In the preceding sections, we have discussed syn-
tactic constructions that cause wrong application
of Head Finalization to Chinese sentences. Fol-
lowing the observations, we propose a method to
improve the original Head Finalization reordering
rule to obtain better alignment with Japanese.
The idea is simple: we define a list of POSs,
and when we find one of them as a dependent
child of the node, we do not apply reordering. Ta-
ble 5 shows the list of POSs we define in the cur-
rent implementation6. While interjections are not
discussed in detail, we should obviously not re-
order to interjections because they are position-
independent. The rules for PU and CC are ba-
sically equivalent to the exception rules proposed
by (Isozaki et al, 2010b).
4 Experiments
The corpus we used as training data comes
from the China Workshop on Machine Transla-
tion (CWMT) (Zhao et al, 2011). This is a
Japanese-Chinese parallel corpus in the news do-
main, containing 281, 322 sentence pairs. We also
collected another Japanese-Chinese parallel cor-
pus from news containing 529, 769 sentences and
merged it with the CWMT corpus to create an ex-
tended version of the CWMT corpus. We will re-
fer to this corpus as ?CWMT ext.? We split an in-
verted multi-reference set into a development and a
test set containing 1, 000 sentences each. In these
two sets, the Chinese input was different, but the
Japanese reference was identical. We think that
this split does not pose any severe problem to the
comparison fairness of the experiment, since no
new phrases are added during tuning and the ex-
perimental conditions remain equal for all tested
6The POSs are from Penn Chinese Treebank.
Ch Ja
CWMT
Sentences 282K
Run. words 2.5M 3.2M
Avg. sent. leng. 8.8 11.5
Vocabulary 102K 42K
CWMT ext.
Sentences 811K
Run. words 14.7M 17M
Avg. sent. leng. 18.1 20.9
Vocabulary 249K 95K
Dev.
Sentences 1000
Run. words 29.9K 35.7K
Avg. sent. leng. 29.9 35.7
OoV w.r.t. CWMT 485 106
OoV w.r.t. CWMT ext. 244 53
Test
Sentences 1000
Run. words 25.8K 35.7K
Avg. sent. leng. 25.8 35.7
OoV w.r.t. CWMT 456 106
OoV w.r.t. CWMT ext. 228 53
Table 6: Characteristics of CWMT and extended
CWMT Chinese-Japanese corpus. Dev. stands for De-
velopment, OoV for ?Out of Vocabulary? words, K for
thousands of elements, and M for millions of elements.
Data statistics were collected after tokenizing.
methods. Detailed Corpus statistics can be found
in Table 6.
To parse Chinese sentences, we used Chinese
Enju (Yu et al, 2010), an HPSG-based parser
trained with the Chinese HPSG treebank converted
from Penn Chinese Treebank. Chinese Enju re-
quires segmented and POS-tagged sentences to
do parsing. We used the Stanford Chinese seg-
menter (Chang et al, 2008) and Stanford POS-
tagger (Toutanova et al, 2003) to obtain the seg-
mentation and POS-tagging of the Chinese side of
the training, development, and test sets.
The baseline system was trained following
the instructions of recent SMT evaluation cam-
paigns (Callison-Burch et al, 2010) by using the
MT toolkit Moses (Koehn et al, 2007) in its de-
fault configuration. Phrase pairs were extracted
from symmetrized word alignments and distor-
tions generated by GIZA++ (Och and Ney, 2003)
using the combination of heuristics ?grow-diag-
final-and? and ?msd-bidirectional-fe?. The lan-
guage model was a 5-gram language model es-
timated on the target side of the parallel cor-
pora by using the modified Kneser-Ney smooth-
ing (Chen and Goodman, 1999) implemented in
62
the SRILM (Stolcke, 2002) toolkit. The weights
of the log-linear combination of feature functions
were estimated by using MERT (Och, 2003) on the
development set described in Table 6.
The effectiveness of the reorderings proposed
in Section 3.3 was assessed by using two preci-
sion metrics and two error metrics on translation
quality. The first evaluation metric is BLEU (Pap-
ineni et al, 2002), a very common accuracy metric
in SMT that measures N -gram precision, with a
penalty for too short sentences. The second eval-
uation metric was RIBES (Isozaki et al, 2010a), a
recent precision metric used to evaluate translation
quality between structurally different languages. It
uses notions on rank correlation coefficients and
precision measures. The third evaluation metric is
TER (Snover et al, 2006), another error metric that
computes the minimum number of edits required
to convert translated sentences into its correspond-
ing references. Possible edits include insertion,
deletion, substitution of single words, and shifts of
word sequences. The fourth evaluation metric is
WER, an error metric inspired in the Levenshtein
distance at word level. BLEU, WER, and TER
were used to provide a sense of comparison but
they do not significantly penalize long-range word
order errors. For this reason, RIBES was used to
account for this aspect of translation quality.
The baseline system was trained and tuned us-
ing the same configuration setup described in this
section, but no reordering rule was implemented at
the preprocessing stage.
Three systems have been run to translate the test
set for comparison when the systems were trained
using the two training data sets. They are the
baseline system, the system consisting in the na??ve
implementation of HF reordering, and the system
with refined HFC reordering rules. Assessment of
translation quality can be found in Table 7.
As can be observed in Table 7, the translation
quality, as measured by precision and error met-
rics, was consistently and significantly increased
when the HFC reordering rule was used and was
significantly improved further when the refinement
proposed in this work was used. Specifically, the
BLEU score increased from 19.94 to 20.79 when
the CWMT corpus was used, and from 23.17 to
24.14 when the extended CWMT corpus was used.
AS SP ETC IJ PU COOD
3.8% 0.8% 1.3% 0.0%* 21.0% 38.3%
Table 8: Weighted recall of each exception rule during
reordering on CWMT ext. training data, dev data, and
test data. (* actual value 0.0016%.)
Table 8 shows the recall of each exception rule
listed in Section 3, and was computed by counting
the times an exception rule was triggered divided
by the number of times the head finalization rule
applied. Data was collected for CWMT ext. train-
ing, dev and test sets. Although the exception rules
related to aspect particles, Et cetera, sentence-final
particles and interjections have a comparatively
lower frequency of application than punctuation
or coordination exception rules, the improvements
they led to are significant.
5 Error Analysis
In Section 3 we have analyzed syntactic differ-
ences between Chinese and Japanese that led to
the design of an effective refinement. A manual
error analysis of the results of our refined reorder-
ing rules showed that some more reordering issues
remain and, although they are not side effects of
our proposed rule, they are worth mentioning in
this separate section.
5.1 Serial Verb Construction
Serial verb construction is a phenomenon occur-
ring in Chinese, where several verbs are put to-
gether as one unit without any conjunction be-
tween them. The relationship between these
verbs can be progressive or parallel. Apparently,
Japanese has a largely corresponding construc-
tion, which indicates that no reordering should
be applied. An example to illustrate this fact in
Chinese is ?weishi (maintain) shenhua (deepen)
zhongriguanxi (Japan-China relations) de
(of) gaishan (improvement) jidiao (basic
tone).?7 The two verbs ?weishi? (in Japanese,
iji) and ?shenhua? (in Japanese, shinka) are
used together, and they follow the same order as
in Japanese: ?nicchukankei (Japan-China re-
7English translation: Maintain and deepen the improved
basic tone of Japan-China relations.
63
CWMT CWMT ext.
BLEU RIBES TER WER BLEU RIBES TER WER
baseline 16.74 71.24 70.86 77.45 20.70 74.21 66.10 72.36
HFC 19.94 73.49 65.19 71.39 23.17 75.35 61.38 67.74
refined HFC 20.79 75.09 64.91 70.39 24.14 77.17 59.67 65.31
Table 7: Evaluation of translation quality of a test set when CWMT and CWMT extended corpus were used for
training. Results are given in terms of BLEU, RIBES, TER, and WER for baseline, head finalization, and proposed
refinement of head finalization reordering rules.
lations) no (of) kaizan (improvement) kityo
(basic tone) wo iji (maintain) shinka (deepen)
suru (do).?
5.2 Complementizer
A ?complementizer? is a particle used to intro-
duce a complement. In English, a very common
complementizer is the word ?that? when making a
clausal complement, while in Chinese it can de-
note other types of word, such as verbs, adjec-
tives or quantifiers. The complementizer is iden-
tified as the dependent of the verb that it modi-
fies. For instance, a Chinese sentence: ?wo (I)
mang wan le (have finished the work).? This
can be translated into Japanese: ?watashi (I) wa
shigoto (work) wo owa tta (have finished).? In
Chinese, the verb ?mang? is the head while ?wan?
is the complementizer, and its Japanese counter-
part ?owa tta? has the same word order.
However, during the reordering, ?mang? will be
placed at the end of the sentence and ?wan? in the
beginning, leading to an inconsistency with respect
to the Japanese translation where the complemen-
tizer ?tta? is the head.
5.3 Verbal Nominalization and Nounal
Verbalization
As discussed by Guo (2009), compared to English
and Japanese, Chinese has little inflectional mor-
phology, that is, no inflection to denote tense, case,
etc. Thus, words are extremely flexible, making
verb nominalization and noun verbalization appear
frequently and commonly without any conjugation
or declension. As a result, it is difficult to do dis-
ambiguation during POS tagging and parsing. For
example, the Chinese word ?kaifa? may have
two syntactic functions: verb (develop) and noun
(development). Thus, it is difficult to reliably tag
without considering the context. In contrast, in
Japanese, ?suru? can be used to identify verbs.
For example, ?kaihatu suru? (develop) is a
verb and ?kaihatu? (development) is a noun.
This ambiguity is prone to not only POS tagging
error but also parsing error, and thus affects the
identification of heads, which may lead to incor-
rect reordering.
5.4 Adverbial Modifier
Unlike the adverb ?bu4? we discussed in Sec-
tion 3.2, the ordinary adverbial modifier comes
directly before the verb it modifies both in Chi-
nese and Japanese, but not in English. Nev-
ertheless, in accordance with the principle of
identifying the head for Chinese, the adverb
will be treated as the dependent and it will
not be reordered following the verb it modi-
fied. As a result, the alignment between adverbs
and verbs is non-monotonic. This can be ob-
served in the Chinese sentence ?guojia (coun-
try) yanli (severely) chufa (penalize) jiage
(price) weifa (violation) xingwei (behavior)?8,
and its Japanese translation: ?kuni (country) wa
kakaku (price) no ihou (violation) koui (be-
havior) wo kibisiku (severely) syobatu (penal-
ize).? Both in Chinese and Japanese, the adverbial
modifier ?yanli? and ?kibisiku? are directly
in front of the verb ?chufa? and ?syobatu?, re-
spectively. However, the verb in Chinese is identi-
fied as the head and will be reordered to the end of
the sentence without the adverb.
8English translation: The country severely penalizes vio-
lations of price restrictions.
64
5.5 POS tagging and Parsing Errors
There were word reordering issues not caused
solely by differences in syntactic structures. Here
we summarize two that are difficult to remedy dur-
ing reordering and that are hard to avoid since re-
ordering rules are highly dependent on the tagger
and parser.
? POS tagging errors
In Chinese, for example, the word ?Iran?
was tagged as ?VV? or ?JJ? instead of ?NR?.
This led to identifying ?Iran? as a head in
accordance with the head definition in Chi-
nese, and it was reordered undesirably.
? Parsing errors
For example, in the Chinese verb phrase
?touzi (invest) 20 yi (200 million)
meiyuan (dollars)?, ?20? and ?yi? were
identified as dependent of ?touzi? and
?meiyuan?, respectively, which led to an
unsuitable reordering for posterior word
alignment.
6 Conclusion and Future Work
In the present work, we have proposed novel
Chinese-to-Japanese reordering rules inspired
in (Isozaki et al, 2010b) based on linguistic analy-
sis on Chinese HPSG and differences among Chi-
nese and Japanese. Although a simple implemen-
tation of HF to reorder Chinese sentences per-
forms well, translation quality was substantially
improved further by including linguistic knowl-
edge into the refinement of the reordering rules.
In Section 5, we found more patterns on reorder-
ing issues when reordering Chinese sentences to
resemble Japanese word order. The extraction of
those patterns and their effective implementation
may lead to further improvements in translation
quality, so we are planning to explore this possi-
bility.
In this work, syntactic information from a deep
parser has been used to reorder words better. We
believe that using semantic information can fur-
ther increase the expressive power of reordering
rules. With that objective, Chinese Enju can be
used since it provides the semantic head of nodes
and can interpret sentences by using their semantic
dependency.
Acknowledgments
This work was mainly developed during an intern-
ship at NTT Communication Science Laborato-
ries. We would like to thank Prof. Yusuke Miyao
for his invaluable support on this work.
References
P.F. Brown, S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of ma-
chine translation. In Computational Linguistics, vol-
ume 19, pages 263?311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, and Omar Zaidan, editors. 2010. Pro-
ceedings of the joint 5th workshop on Statistical Ma-
chine Translation and MetricsMATR. Association
for Computational Linguistics, July.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word seg-
mentation for machine translation performance. In
Proceedings of the 3rd Workshop on SMT, pages
224?232, Columbus, Ohio. Association for Compu-
tational Linguistics.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech and Language,
4(13):359?393.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
ACL ?05, pages 531?540, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Vivian James Cook and Mark Newson. 1988. Chom-
sky?s Universal Grammar: An introduction. Oxford:
Basil Blackwell.
Naoki Fukui. 1992. Theory of Projection in Syntax.
CSLI Publisher and Kuroshio Publisher.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. Whats in a translation rule?
In Proceedings of HLT-NAACL.
Qian Gao. 2008. Word order in mandarin: Reading and
speaking. In Proceedings of the 20th North Ameri-
can Conference on Chinese Linguistics (NACCL-20),
volume 2, pages 611?626.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ?10,
65
pages 376?384, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yuqing Guo. 2009. Treebank-based acquisition of
Chinese LFG resources for parsing and generation.
Ph.D. thesis, Dublin City University.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of Empirical Methods on Nat-
ural Language Processing (EMNLP).
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for sov languages. In Proceedings of
WMTMetricsMATR, pages 244?251.
P. Koehn, F. J. Och, and D. Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings
HLT/NAACL?03, pages 48?54.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the ACL Demo and Poster Sessions, 2007, pages
177?180, June 25?27.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic hpsg parsing. Computa-
tional Linguistics, 34:35?80, March.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st annual conference of the Association for Com-
putational Linguistics, 2003, pages 160?167, July 7?
12.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the
40th annual conference of the Association for Com-
putational Linguistics, 2002, pages 311?318, July 6?
12.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on Spoken Language Pro-
cessing, 2002, pages 901?904, September 16?20.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004: Short Papers, HLT-
NAACL-Short ?04, pages 101?104, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings OF HLT-NAACL, pages 252?259.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 737?
745, Prague, Czech Republic, June. Association for
Computational Linguistics.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proceedings of 5th International Joint Con-
ference on Natural Language Processing, pages 29?
37, Chiang Mai, Thailand, November. Asian Feder-
ation of Natural Language Processing.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
?04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
NAACL ?09, pages 245?253, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Kun Yu, Yusuke Miyao, Xiangli Wang, Takuya Mat-
suzaki, and Jun ichi Tsujii. 2010. Semi-
automatically developing chinese hpsg grammar
from the penn chinese treebank for deep parsing. In
COLING (Posters)?10, pages 1417?1425.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the dif-
ficulties in chinese deep parsing. In Proceedings of
the 12th International Conference on Parsing Tech-
nologies, pages 48?57.
R. Zens, F.J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of
KI?02, pages 18?32.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th china workshop on machine translation
(cwmt2011). The 7th China Workshop on Machine
Translation (CWMT2011).
66
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 25?33,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using Unlabeled Dependency Parsing for Pre-reordering
for Chinese-to-Japanese Statistical Machine Translation
Dan Han1,2 Pascual Mart??nez-Go?mez2,3 Yusuke Miyao1,2
Katsuhito Sudoh4 Masaaki Nagata4
1The Graduate University For Advanced Studies
2National Institute of Informatics, 3The University of Tokyo
4NTT Communication Science Laboratories, NTT Corporation
{handan,pascual,yusuke}@nii.ac.jp
{sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
Chinese and Japanese have a different sen-
tence structure. Reordering methods are
effective, but need reliable parsers to ex-
tract the syntactic structure of the source
sentences. However, Chinese has a loose
word order, and Chinese parsers that ex-
tract the phrase structure do not perform
well. We propose a framework where only
POS tags and unlabeled dependency parse
trees are necessary, and linguistic knowl-
edge on structural difference can be en-
coded in the form of reordering rules. We
show significant improvements in transla-
tion quality of sentences from news do-
main, when compared to state-of-the-art
reordering methods.
1 Introduction
Translation between Chinese and Japanese lan-
guages gains interest as their economic and polit-
ical relationship intensifies. Despite their linguis-
tic influences, these languages have different syn-
tactic structures and phrase-based statistical ma-
chine translation (SMT) systems do not perform
well. Current word alignment models (Och and
Ney, 2003) account for local differences in word
order between bilingual sentences, but fail at cap-
turing long distance word alignments. One of
the main problems in the search of the best word
alignment is the combinatorial explosion of word
orders, but linguistically-motivated heuristics can
help to guide the search.
This work explores syntax-informed pre-
reordering for Chinese; that is, we obtain syntactic
structures of Chinese sentences, reorder the words
to resemble the Japanese word order, and then
translate the reordered sentences using a phrase-
based SMT system. However, Chinese parsers
have difficulties in extracting reliable syntactic in-
formation, mainly because Chinese has a loose
word order and few syntactic clues such as inflec-
tion and function words.
On one hand, parsers implementing head-driven
phrase structure grammars infer a detailed con-
stituent structure, and such a rich syntactic struc-
ture can be exploited to design well informed re-
ordering methods. However, inferring abundant
syntactic information often implies introducing er-
rors, and reordering methods that heavily rely on
detailed information are sensitive to those parsing
errors (Han et al, 2012).
On the other hand, dependency parsers are com-
mitted to the simpler task of finding dependency
relations and dependency labels, which can also be
useful to guide reordering (Xu et al, 2009). How-
ever, reordering methods that rely on those depen-
dency labels will also be prone to errors, specially
in the case of Chinese since it has a richer set of
dependency labels when compared to other lan-
guages. Since improving parsers for Chinese is
challenging, we thus aim at reducing the influence
of parsing errors in the reordering procedure.
We present a hybrid approach that boosts the
performance of phrase-based SMT systems by
pre-reordering the source language using unla-
beled parse trees augmented with constituent
information derived from Part-of-Speech tags.
Specifically, we propose a framework to pre-
reorder a Subject-Verb-Object (SVO) language,
in order to improve its translation to a Subject-
Object-Verb (SOV) language, where the only re-
quired syntactic information are POS tags and un-
labeled dependency parse trees. We test the per-
formance of our pre-reordering method and com-
pare it to state-of-the-art reordering methods in the
news domain for Chinese.
In the next section, we describe similar work on
pre-reordering methods for language pairs that in-
25
volve either Chinese or Japanese, and explain how
our method builds upon them. From a linguis-
tic perspective, we describe in section 3 our ob-
servations of reordering issues between Chinese
and Japanese and detail how our framework solves
those issues. In section 4 we assess to what extent
our pre-reordering method succeeds in reordering
words in Chinese sentences to resemble the order
of Japanese sentences, and measure its impact on
translation quality. The last section is dedicated to
discuss our findings and point to future directions.
2 Related Work
Although there are many works on pre-reordering
methods for other languages to English translation
or inverse (Xia and McCord, 2004; Xu et al, 2009;
Habash, 2007; Wang et al, 2007; Li et al, 2007;
Wu et al, 2011), reordering method for Chinese-
to-Japanese translation, which is a representative
of long distance language pairs, has received little
attention.
The most related work to ours is in (Han et al,
2012), in which the authors introduced a refined
reordering approach by importing an existing re-
ordering method for English proposed in (Isozaki
et al, 2010b). These reordering strategies are
based on Head-driven phrase structure grammars
(HPSG) (Pollard and Sag, 1994), in that the re-
ordering decisions are made based on the head of
phrases. Specifically, HPSG parsers (Miyao and
Tsujii, 2008; Yu et al, 2011) are used to extract the
structure of sentences in the form of binary trees,
and head branches are swapped with their depen-
dents according to certain heuristics to resemble
the word order of the target language. However,
those strategies are sensitive to parsing errors, and
the binary structure of their parse trees impose
hard constraints in sentences with loose word or-
der. Moreover, as Han et al (2012) noted, reorder-
ing strategies that are derived from the HPSG the-
ory may not perform well when the head definition
is inconsistent in the language pair under study. A
typical example for the language pair of Chinese
and Japanese that illustrates this phenomenon is
the adverb ?bu4?, which is the dependent of its
verb in Chinese but the head in Japanese.
The work in (Xu et al, 2009) used an English
dependency parser and formulated handcrafted re-
ordering rules with dependency labels, POS tags
and weights as triplets and implemented them re-
cursively into sentences. This design, however,
limited the extensibility of their method. Our ap-
proach follows the idea of using dependency tree
structures and POS tags, but we discard the infor-
mation on dependency labels since we did not find
them informative to guide our reordering strate-
gies in our preliminary experiments, partly due to
Chinese showing less dependencies and a larger
label variability (Chang et al, 2009).
3 Methodology
In Subject-Verb-Object (SVO) languages, objects
usually follow their verbs, while in Subject-
Object-Verb (SOV) languages, objects precede
them. Our objective is to reorder words in Chinese
sentences (SVO) to resemble the word order of
Japanese sentences (SOV). For that purpose, our
method consists in moving verbs to the right-hand
side of their objects. However, it is challenging
to correctly identify the appropriate verbs and ob-
jects that trigger a reordering, and this section will
be dedicated to that end.
More specifically, the first step of our method
consists in identifying the appropriate verb (and
certain words close to it) that need to be moved to
the right-hand side of its object argument. Verbs
(and those accompanying words) will move as a
block, preserving the relative order among them.
We will refer to them as verbal blocks (Vbs). The
second step will consist in identifying the right-
most argument object of the verb under considera-
tion, and moving the verbal block to the right-hand
side of it. Finally, certain invariable grammatical
particles in the original vicinity of the verb will
also be reordered, but their positions will be de-
cided relative to their verb.
In what follows, we describe in detail how to
identify verbal blocks, their objects and the invari-
able grammatical particles that will play a role in
our reordering method. As mentioned earlier, the
only information that will be used to perform this
task will be the POS tags of the words and their
unlabeled dependency structures.
3.1 Identifying verbal blocks (Vbs)
Verbal blocks are composed of a head (Vb-H)
and possibly accompanying dependents (Vb-D).
In the Chinese sentence ?wo3 (I) chi1 le5 (ate) li2
(pear).?1, ?chi1? refers to the English verb ?eat?
1In this paper, we represent a Chinese character by using
Pinyin plus a tone number (there are 5 tones in Chinese). In
the example, ?chi1(eat)? is a verb and ?le5(-ed)? is an aspect
particle that adds preterit tense to the verb.
26
Vb-H VV VE VC VA P
Vb-D AD AS SP MSP CC VV VE VC VA
BEI LB SB
RM-D NN NR NT PN OD CD M FW CC
ETC LC DEV DT JJ SP IJ ON
Oth-DEP LB SB CS
Table 1: Lists of POS tags in Chinese used to iden-
tify blocks of words to reorder (Vb-H, Vb-D, BEI
lists), the POS tags of their dependents (RM-D
lists) which indicate the reordering position, and
invariable grammatical particles (Oth-DEP) that
need to be reordered.
and the aspect particle ?le5? adds a preterit tense
to the verb. The words ?chi1 le5? are an example
of verbal block that should be reordered as a block
without altering its inner word order, i.e. ?wo3
(I) li2 (pear) chi1 le5 (ate).?, which matches the
Japanese SOV order.
Possible heads of verbal blocks (Vb-H) are
verbs (words with POS tags VV, VE, VC and VA),
or prepositions (words with POS tag P). The Vb-H
entry of Table 1 contains the list of POS tags for
heads of verbal blocks. We use prepositions for
Vb-H identification since they behave similarly to
verbs in Chinese and should be moved to the right-
most position in a prepositional phrase to resemble
the Japanese word order. There are three condi-
tions that a word should meet to be considered as
a Vb-H:
i) Its POS tag is in the set of Vb-H in Table 1.
ii) It is a dependency head, which indicates that
it may have an object as a dependent.
iii) It has no dependent whose POS tag is in the
set of BEI in Table 1. BEI particles indicate
that the verb is in passive voice and should
not be reordered since it already resembles
the Japanese order.
Chinese language does not have inflection, con-
jugation, or case markers (Li and Thompson,
1989). For that reason, some adverbs (AD), as-
pect particles (AS) or sentence-final particles (SP)
are used to signal modality, indicate grammati-
cal tense or add aspectual value to verbs. Words
in this category preserve the order when translat-
ing to Japanese, and they will be candidates to be
part of the verbal block (Vb-D) and accompany
the verb when it is reordered. Other words in this
category are coordinating conjunctions (CC) that
connect multiple verbs, and both resultative ?de5?
(DER) and manner ?de5? (DEV). The full list of
POS tags used to identify Vb-Ds can be found in
Table 1. To be a Vb-D, there are three necessary
conditions as well:
i) Its POS tag is in the Vb-D entry in Table 1.
ii) It is a dependent of a word that is already in
the Vb.
iii) It is next to its dependency head or only a
coordination conjunction is in between.
To summarize, to build verbal blocks (Vbs) we
first find the words that meet the three Vb-H con-
ditions. Then, we test the Vb-D conditions on the
words adjacent to the Vb-Hs and extend the verbal
blocks to them if they meet the conditions. This
process is iteratively applied to the adjacent words
of a block until no more words can be added to the
verbal block, possibly nesting other verbal blocks
if necessary.
Figure 1a 2 shows an example of a dependency
tree of a Chinese sentence that will be used to il-
lustrate Vb identification. By observing the POS
tags of the words in the sentence, only the words
?bian1 ji4 (edit)? and ?chu1 ban3 (publish)? have
a POS tag (i.e. VV) in the Vb-H entry of Table 1.
Moreover, both words are dependency heads and
do not have any dependent whose POS tag is in
the BEI entry of Table 1. Thus, ?bian1 ji4 (edit)?
and ?chu1 ban3 (publish)? will be selected as Vb-
Hs and form, by themselves, two separate incipi-
ent Vbs. We arbitrarily start building the Vb from
the word ?chu1 ban3 (publish)?, by analyzing its
adjacent words that are its dependents.
We observe that only ?le5 (-ed)? is adjacent to
?chu1 ban3 (publish)?, it is its dependent, and its
POS tag is in the Vb-D list. Since ?le5 (-ed)?
meets all three conditions stated above, ?le5 (-ed)?
will be included in the Vb originated by ?chu1
ban3 (publish)?. The current Vb thus consists of
the sequence of tokens ?chu1 ban3 (publish)? and
?le5 (-ed)?, and the three conditions for Vb-D are
tested on the adjacent words of this block. Since
the adjacent words (or words separated by a coor-
dinating conjunction) do not meet the conditions,
the block is not further extended. Figure 1b shows
the dependency tree where the Vb block that con-
sists of the words ?chu1 ban3 (publish)? and ?le5
(-ed)? is represented by a rectangular box.
By checking in the same way, there are three
dependents that meet the requirements of being
2For all the dependency parsing trees in this paper, arrows
are pointing from heads to their dependents.
27
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT.o .o
.o.o .o .o .o .o .o
(a) Original dependency tree
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .?? .?? .? .?? .? .? .? .? .?.School .has already .edit (-ed) .and .publish .-ed .a . .book
.NN .AD .VV .CC .VV .AS .CD .M .NN .PU
.ROOT .o.o .o .o.o
(b) Vbs in rectangular boxes
..xue2 xiao4 .yi3 jing1 .bian1 ji4 .he2 .chu1 ban3 .le5 .yi1 .ben3 .shu1 ..?? .? .? .? .?? .?? .? .?? .? .?.School .a . .book .has already .edit (-ed) .and .publish .-ed
(c) Merged and reordered Vb
Figure 1: An example that shows how to de-
tect and reorder a Verbal block (Vb) in a sen-
tence. In the first two figures 1a and 1b, Chi-
nese Pinyin, Chinese tokens, word-to-word En-
glish translations, and POS tags of each Chinese
token are listed in four lines. In Figure 1c, there
are Chinese Pinyin, reordered Chinese sentence
and its word-to-word English counterpart.
Vb-Ds for ?bian1 ji4 (edit)?: ?yi3 jing1 (has al-
ready)?, ?he2 (and)? and ?chu1 ban4 (publish)?
and hence this Vb consists of three tokens and one
Vb. The outer rectangular box in Figure 1b shows
that the Vb ?bian1 ji4 (edit)? as the Vb-H. Fig-
ure 1c shows an image of how this Vb will be
reordered while the inner orders are kept. Note
that the order of building Vbs from which Vb-Hs,
?chu1 ban3 (publish)? or ?bian1 ji4 (edit)? will not
affect any change of the final result.
3.2 Identifying objects
In the most general form, objects are dependents
of verbal blocks3 that act as their arguments.
While the simplest objects are nouns (N) or pro-
nouns (PN), they can also be comprised of noun
phrases or clauses (Downing and Locke, 2006)
such as nominal groups, finite clauses (e.g. that
clauses, wh-clauses) or non-finite clauses (e.g. -
ing clauses), among others.
For every Vb in a verb phrase, clause, or sen-
tence, we define the right-most object dependent
(RM-D) as the word that:
3Dependents of verbal blocks are dependents of any word
within the verbal block.
..ta1 .chi1 .le5 .wu3 fan4 . .qu4 .xue2 xiao4 ..? .? .? .?? .? .? .?? .?.he .eat .-ed .lunch .(and) .go(to) .school ..PN .VV .AS .NN .PU .VV .NN .PU
.ROOT.o .o .o.o
.o.o .o
? ?? ? ? ? ?? ? ?he lunch eat -ed school go(to)
V ?????? O V ???? OS
O ????? V O ???? VS
English Translation: He ate lunch, and went to school.
Figure 2: An example of a Chinese sentence with
a coordination of verb phrases as predicate. Sub-
ject(S), verbs(V), and objects(O) are displayed for
both verb phrases. Lines between the original Chi-
nese sentence and the reordered Chinese sentence
indicate the reordering trace of Verbal blocks(Vb).
i) its POS tag is in the RM-D entry of Table 1,
ii) its dependency head is inside of the verbal
block, and
iii) is the right-most object among all objects of
the verbal block.
All verbal blocks in the phrase, clause, or sen-
tence will move to the right-hand side of their cor-
respondent RM-Ds recursively. Figure 1b and Fig-
ure 1c show a basic example of object identifica-
tion. The Chinese word corresponding to ?shu1
(book)? is a dependent of a word within the verbal
block and its POS tag is within the RM-D entry
list of Table 1 (i.e. NN). For this reason, ?shu1
(book)? is identified as the right-most dependent
of the verbal block (Vb), and the Vb will move to
the right-hand side of it to resemble the Japanese
word order.
A slightly more complex example can be found
in Figure 2. In this example, there is a coordina-
tion structure of verb phrases, and the dependency
tree shows that the first verb, ?chi1 (eat)?, ap-
pears as the dependency head of the second verb,
?qu4 (go)?. The direct right-most object depen-
dent (RM-D) of the first verb, ?chi1 (eat)?, is the
word ?wu3 fan4 (lunch)?, and the verb ?chi1 (eat)?
will be moved to the right-hand side of its object
dependent.
There are cases, however, where there is no co-
ordination structure of verb phrases but a simi-
lar dependency relation occurs between two verbs.
Figure 3 illustrates one of these cases, where the
main verb ?gu3 li4 (encourage)? has no direct de-
28
..xue2 xiao4 .gu3 li4 .xue2 sheng1 .can1 yu3 .she4 hui4 .shi2 jian4 ..?? .?? .?? .?? .?? .?? .?.school .encourage .student .participate .social .practice.NN .VV .NN .VV .NN .NN .PU
.o .ROOT
.o
.o.o .o .o
?? ?? ?? ?? ?? ?? ?school student social practice participate encourage
S ???? V ?????? O
S ???? V ?????????? O
S ?????? O ??????? V
S ???????????? O ??????????? VEnglish Translation: School encourages student to participate in social practice.
Figure 3: An example of a Chinese sentence in
which an embedded clause appears as the object
of the main verb. Subjects (S), verbs (V), and ob-
jects (O) are displayed for both the sentence and
the clause. Lines between the original Chinese
sentence and the reordered Chinese sentence in-
dicate the reordering trace of Verbal blocks (Vb).
pendent that can be considered as an object since
no direct dependent has a POS tag in the RM-D en-
try of Table 1. Instead, an embedded clause (SVO)
appears as the object argument of the main verb,
and the main verb ?gu3 li4 (encourage)? appears
as the dependency head of the verb ?can1 yu2 (par-
ticipate)?.
In the news domain, reported speech is a fre-
quent example that follows this pattern. In our
method, if the main verb of the sentence (labeled
as ROOT) has dependents but none of them is a
direct object, we move the main verb to the end of
the sentence. As for the embedded clause ?xue2
sheng1 (student) can1 yu2 (participate) she4 hui4
(social) shi2 jian4 (practice)?, the verbal block of
the clause is the word ?can1 yu2 (participate)?
and its object is ?shi2 jian4 (practice)?. Apply-
ing our reordering method, the clause order results
in ?xue2 sheng1 (student) she4 hui4 (social) shi2
jian4 (practice) can1 yu2 (participate)?. The result
is an SOV sentence with an SOV clause, which
resembles the Japanese word order.
3.3 Identifying invariable grammatical
particles
In Chinese, certain invariable grammatical parti-
cles that accompany verbal heads have a different
word order relative to their heads, when compared
to Japanese. Those particles are typically ?bei4?
particle (POS tags LB and SB) and subordinating
conjunctions (POS tag CS). Those particles appear
on the left-hand side of their dependency heads in
Chinese, and they should be moved to the right-
hand side of their dependency heads for them to
resemble the Japanese word order. Reordering in-
variable grammatical particles in our framework
can be summarized as:
i) Find dependents of a verbal head (Vb-H)
whose POS tags are in the Oth-DEP entry of
Table 1.
ii) Move those particles to the right-hand side of
their (possibly reordered) heads.
iii) If there is more than one such particle, move
them keeping the relative order among them.
3.4 Summary of the reordering framework
Based on the definitions above, our dependency
parsing based pre-reordering framework can be
summarized in the following steps:
1. Obtain POS tags and an unlabeled depen-
dency tree of a Chinese sentence.
2. Obtain reordering candidates: Vbs.
3. Obtain the object (RM-D) of each Vb.
4. Reorder each Vb in two exclusive cases by
following the order:
(a) If RM-D exists, reorder Vb to be the
right-hand side of RM-D.
(b) If Vb-H is ROOT and its RM-D does not
exist, reorder Vb to the end of the sen-
tence.
(c) If none of above two conditions is met,
no reordering happens.
5. Reorder grammatical particles (Oth-DEPs) to
the right-hand side of their corresponding
Vbs.
Note that, unlike other works in reordering dis-
tant languages (Isozaki et al, 2010b; Han et al,
2012; Xu et al, 2009), we do not prevent chunks
from crossing punctuations or coordination struc-
tures. Thus, our method allows to achieve an
authentic global reordering in reported speech,
which is an important reordering issue in news do-
mains.
In order to illustrate our method, a more compli-
cated Chinese sentence example is given in Fig-
ure 4, which includes the unlabeled dependency
29
..xin1wen2 .bao2dao3 . .sui2zhe5 .jing1ji4 .de5 .fa1zhan3 . .sheng4dan4jie2 .zhu2jian4 .jin4ru4 .le5 .zhong1guo2 . .cheng2wei2 .shang1jia1 .jia1qiang2 .li4cu4 .mai3qi4 .de5 .yi1 .ge4 .ji2ri4 ..?? .?? .? .?? .?? .? .?? .? .??? .?? .?? .? .?? .? .?? .?? .?? .?? .?? .? .? .? .?? .?.news .report . .with .economic .?s .development . .Christmas .gradually .enter .-ed .China . .become .businesses .strengthen .urge .purchase .?s .one .kind .festival ..NN .VV .PU .P .NN .DEG .NN .PU .NN .AD .VV .AS .NR .PU .VV .NN .VV .VV .NN .DEC .CD .M .NN .PU
.ROOT
.o.o .o .o .o.o.o
.o .o .o .o.o .o
.o
.o
.o .o.o .o .o .o.o.o
?? ? ?? ? ?? ?? ? ??? ?? ?? ?? ? ? ?? ?? ?? ?? ? ? ? ???? ?? ?
???? ?? ?? ?? ??? ?????? ? ??? ?? ? ??? ? ?? ? ? ?? ?? ? ?? ? ?? ? ?????? ???Entire English translation: News reports, with the economic development, Christmas has gradually entered into China, and becomes one of the festivals that businesses use to promote commerce.
Figure 4: Dependency parse tree of a complex Chinese sentence example, and word alignments for
reordered sentence with its Japanese counterpart. The first four lines are Chinese Pinyin, tokens, word-
to-word English translations, and the POS tags of each Chinese token. The fifth line shows the reordered
Chinese sentence while the sixth line is the segmented Japanese translation. The entire English transla-
tion for the sentence is showed in the last line.
parsing tree of the original Chinese sentence, and
the word alignment between reordered Chinese
sentence and its Japanese counterpart, etc.
Based on both POS tags and the unlabeled de-
pendency tree, first step of our method is to obtain
all Vbs. For all heads in the tree, according to the
definition of Vb introduced in Section 3.1, there
are six tokens which will be recognized as the can-
didates of Vb-Hs, that is ?bao4 dao3 (report)?,
?sui2 zhe5 (with)?, ?jin4 ru4 (enter)?, ?cheng2
wei2 (become)?, ?jia1 qiang2 (strengthen)?, and
?li4 cu4 (urge)?. Then, for each of the candidate,
its direct dependents will be checked if they are
Vb-Ds. For instance, for the verb of ?jin4 ru4 (en-
ter)?, its dependents of ?zhu2 jian4 (gradually)?
and ?le5 (-ed)? will be considered as the Vb-Ds.
For the case of ?jia1 qiang2 (strengthen)?, instead
of being a Vb-H, it will be recognized as Vb-D
of the Vb ?li4 cu4 (urge)? since it is one of the
direct dependents of ?li4 cu4 (urge)? with a qual-
ified POS tag for Vb-D. Therefore, there are five
Vbs in total, which are ?bao4 dao3 (report)?, ?sui2
zhe5 (with)?, ?zhu2 jian4 (gradually) jin4 ru4 (en-
ter) le5 (-ed)?, ?cheng2 wei2 (become)?, and ?jia1
qiang2 (strengthen) li4 cu4 (urge)?.
The next step is to identify RM-D for each
Vb, if there is one. By checking all conditions,
four Vbs have their RM-Ds: ?fa1 zhan3 (develop-
ment)? is the RM-D of the Vb ?sui2 zhe5 (with)?;
?zhong1 guo2 (China)? is the RM-D of the Vb
?zhu2 jian4 (gradually) jin4 ru4 (enter) le5 (-ed)?;
?jie2 ri4 (festival)? is the RM-D of the Vb ?cheng2
wei2 (become)?; ?mai3 qi4 (purchase)? is the RM-
D of the Vb ?jia1 qiang2 (strengthen) li4 cu4
(urge)?.
After obtaining all RM-Ds, we find those Vbs
that have RM-Ds and move them to right of their
RM-Ds. As for the case of ?bao4 dao3 (report)?,
since it is the root and does not have any matched
RM-D, it will be moved to the end of the sen-
tence, before any final punctuation. Finally, since
there is no any invariable grammatical particle in
the sentence that need to be reordered, reordering
has been finished. From the alignments between
the reordered Chinese and its Japanese translation
showed in the figure, an almost monotonic word
alignment has been achieved.
For comparison purposes, particle seed words
had been inserted into the reordered sentences in
the same way as the Refined-HFC method, which
is using the information of predicate argument
structure output by Chinese Enju (Yu et al, 2011).
We therefore can not entirely disclaim the use
of the HPSG parser at the present stage in our
method. However, we believe that dependency
parser can provide enough information for insert-
ing particles.
4 Experiments
We conducted experiments to assess how our pro-
posed dependency-based pre-reordering for Chi-
nese (DPC) impacts on translation quality, and
compared it to a baseline phrase-based system
and a Refined-HFC pre-reordering for Chinese to
Japanese translation.
We used two Chinese-Japanese training data
30
News CWMT+News
BLEU RIBES BLEU RIBES
Baseline 39.26 84.83 38.96 85.01
Ref-HFC 39.22 84.88 39.26 84.68
DPC 39.93 85.23 39.94 85.22
Table 3: Evaluation of translation quality of two
test sets when CWMT, News and the combination
of both corpora were used for training.
sets of parallel sentences, namely an in-house-
collected Chinese-Japanese news corpus (News),
and the News corpus augmented with the
CWMT (Zhao et al, 2011) corpus. We extracted
disjoint development and test sets from News cor-
pus, containing 1, 000 and 2, 000 sentences re-
spectively. Table 2 shows the corpora statistics.
We used MeCab 4 (Kudo and Matsumoto, 2000)
and the Stanford Chinese segmenter 5 (Chang et
al., 2008) to segment Japanese and Chinese sen-
tences. POS tags of Chinese sentences were ob-
tained using the Berkeley parser 6 (Petrov et al,
2006), while dependency trees were extracted us-
ing Corbit 7 (Hatori et al, 2011). Following the
work in (Han et al, 2012), we re-implemented
the Refined-HFC using the Chinese Enju to ob-
tain HPSG parsing trees. For comparison purposes
with the work in (Isozaki et al, 2010b), particle
seed words were inserted at a preprocessing stage
for Refined-HFC and our DPC method.
DPC and Refined-HFC pre-reordering strate-
gies were followed in the pipeline by a standard
Moses-based baseline system (Koehn et al, 2007),
using a default distance reordering model and a
lexicalized reordering model ?msd-bidirectional-
fe?. A 5-gram language model was built using
SRILM (Stolcke, 2002) on the target side of the
corresponding training corpus. Word alignments
were extracted using MGIZA++ (Gao and Vogel,
2008) and the parameters of the log-linear combi-
nation were tuned using MERT (Och, 2003).
Table 3 summarizes the results of the Baseline
system (no pre-reordering nor particle word inser-
tion), the Refined-HFC (Ref-HFC) and our DPC
method, using the well-known BLEU score (Pap-
ineni et al, 2002) and a word order sensitive met-
ric named RIBES (Isozaki et al, 2010a).
4http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html
5http://nlp.stanford.edu/software/segmenter.shtml
6http://nlp.cs.berkeley.edu/Software.shtml
7http://triplet.cc/software/corbit
As it can be observed, our DPC method obtains
around 0.7 BLEU points of improvement when
compared to the second best system in both cor-
pora. When measuring the translation quality in
terms of RIBES, our method obtains an improve-
ment of 0.3 and 0.2 points when compared to the
second best system in News and CWMT + News
corpora, respectively. We suspect that corpus di-
versity might be one of the reasons for Refined-
HFC not to show any advantage in this setting.
We tested the significance of BLEU improve-
ment for Refined-HFC and DPC when compared
to the baseline phrase-based system. Refined-HFC
tests obtained p-values 0.355 and 0.135 on News
and CWMT + News corpora, while our proposed
DPC method obtained p-values 0.002 and 0.0,
which indicates significant improvements over the
phrase-based system.
5 Conclusions
In the present paper, we have analyzed the dif-
ferences in word order between Chinese and
Japanese sentences. We captured the regulari-
ties of ordering differences between Chinese and
Japanese sentences, and proposed a framework to
reorder Chinese sentences to resemble the word
order of Japanese.
Our framework consists in three steps. First,
we identify verbal blocks, which consist of Chi-
nese words that will move all together as a block
without altering their relative inner order. Sec-
ond, we identify the right-most object of the verbal
block, and move the verbal block to the right of it.
Finally, we identify invariable grammatical parti-
cles in the original vicinity of the verbal block and
move them relative to their dependency heads.
Our framework only uses the unlabeled depen-
dency structure of sentences and POS tag informa-
tion of words. We compared our system to a base-
line phrase-based SMT system and a refined head-
finalization system. Our method obtained a Chi-
nese word order that is more similar to Japanese
word order, and we showed its positive impact on
translation quality.
6 Discussion and future work
In the literature, there are mainly two types of
parsers that have been used to extract sentence
structure and guide reordering. The first type cor-
responds to parsers that extract phrase structures
(i.e. HPSG parsers). These parsers infer a rich
31
News CWMT+News
Chinese Japanese Chinese Japanese
Training
Sentences 342, 050 621, 610
Running words 7,414,749 9,361,867 9,822,535 12,499,112
Vocabulary 145,133 73,909 214,085 98,333
News Devel.
Sentences 1, 000 ?
Running words 46,042 56,748 ? ?
Out of Vocab. 255 54 ? ?
News Test
Sentences 2, 000 ?
Running words 51,534 65,721 ? ?
Out of Vocab. 529 286 ? ?
Table 2: Basic statistics of our corpora. News Devel. and News Test were used to tune and test the
systems trained with both training corpora. Data statistics were collected after tokenizing and filtering
out sentences longer than 64 tokens.
annotation of the sentence in terms of semantic
structure or phrase heads. Other reordering strate-
gies use a different type of parsers, namely depen-
dency parsers. These parsers extract dependency
information among words in the sentence, often
consisting in the dependency relation between two
words and the type of relation (dependency label).
Reordering strategies that use syntactic infor-
mation have proved successful, but they are likely
to magnify parsing errors if their reordering rules
heavily rely on abundant parse information. This
is aggravated when reordering Chinese sentences,
due to its loose word order and large variety of
possible dependency labels.
In this work, we based our study of ordering
differences between Chinese and Japanese solely
on dependency relations and POS tags. This con-
trasts with the work in (Han et al, 2012) that re-
quires phrase structures, phrase-head information
and POS tags, and the work in (Xu et al, 2009)
that requires dependency relations, dependency la-
bels and POS tags.
In spite of the fact that our method uses less syn-
tactic information, it succeeds at reordering sen-
tences with reported speech even in presence of
punctuation symbols. It is worth saying that re-
ported speech is very common in the news domain,
which might be one of the reasons of the supe-
rior translation quality achieved by our reordering
method. Our method also accounted for ordering
differences in serial verb constructions, comple-
mentizers and adverbial modifiers, which would
have required an increase in the complexity of the
reordering logic in other methods.
To the best of our knowledge, dependency
parsers are more common than HPSG parsers
across languages, and our method can potentially
be applied to translate under-resourced languages
into other languages with a very different sentence
structure, as long as they count with dependency
parsers and reliable POS taggers.
Implementing our method for other languages
would first require a linguistic study on the re-
ordering differences between the two distant lan-
guage pairs. However, some word ordering differ-
ences might be consistent across SVO and SOV
language pairs (such as verbs going before or after
their objects), but other ordering differences may
need special treatment for the language pair under
consideration (i.e. Chinese ?bei? particles).
There are two possible directions to extend the
present work. The first one would be to refine the
current method to reduce its sensitivity to POS tag-
ging or dependency parse errors, and to extend our
linguistic study on ordering differences between
Chinese and Japanese languages. The second di-
rection would be to manually or automatically find
common patterns of ordering differences between
SVO and SOV languages. The objective would be
then to create a one-for-all reordering method that
induces monotonic word alignments between sen-
tences from distant language pairs, and that could
also be easily extended to account for the unique
characteristics of the source language of interest.
Acknowledgments
We would like to thank Dr. Takuya Matsuzaki for
his precious advice on this work and Dr. Jun Ha-
tori for his support on using Corbit.
32
References
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Proc.
of the 3rd Workshop on SMT, pages 224?232.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D Manning. 2009. Discriminative re-
ordering with Chinese grammatical relations fea-
tures. In Proc. of the Third Workshop on Syntax and
Structure in Statistical Translation, pages 51?59.
Angela Downing and Philip Locke. 2006. English
grammar: a university course. Routledge.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57.
Nizar Habash. 2007. Syntactic preprocessing for sta-
tistical machine translation. In Proc. of Machine
Translation Summit XI, pages 215?222.
Dan Han, Katsuhito Sudoh, Xianchao Wu, Kevin Duh,
Hajime Tsukada, and Masaaki Nagata. 2012. Head
finalization reordering for Chinese-to-Japanese ma-
chine translation. In Proc. of the Sixth Workshop on
Syntax, Semantics and Structure in Statistical Trans-
lation, pages 57?66.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Ju-
nichi Tsujii. 2011. Incremental joint POS tagging
and dependency parsing in Chinese. In Proc. of
5th International Joint Conference on Natural Lan-
guage Processing, pages 1216?1224.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010a. Automatic
evaluation of translation quality for distant language
pairs. In Proc. of EMNNLP.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010b. Head finalization: A simple re-
ordering rule for SOV languages. In Proc. of WMT-
MetricsMATR, pages 244?251.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proc. of ACL ?07, Demonstration Sessions, pages
177?180.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proc. of the EMNLP/VLC-2000, pages
18?25.
Charles N Li and Sandra Annear Thompson. 1989.
Mandarin Chinese: A functional reference gram-
mar. Univ of California Press.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A probabilistic ap-
proach to syntax-based reordering for statistical ma-
chine translation. In Proc. of ACL, page 720.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature for-
est models for probabilistic HPSG parsing. Compu-
tational Linguistics, 34:35?80.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51.
Franz J. Och. 2003. Minimum error rate training
for statistical machine translation. In Proc. of ACL,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COL-
ING and the 44th ACL, pages 433?440.
Carl Jesse Pollard and Ivan A. Sag. 1994. Head-
driven phrase structure grammar. The University
of Chicago Press and CSLI Publications.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the 7th interna-
tional conference on Spoken Language Processing,
2002, pages 901?904.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proc. of the 2007 Joint Con-
ference on EMNLP-CoNLL, pages 737?745.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
pre-ordering rules from predicate-argument struc-
tures. In Proc. of 5th International Joint Conference
on Natural Language Processing, pages 29?37.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proc. of the 20th international
conference on Computational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
SMT for subject-object-verb languages. In Proc. of
HLT: NA-ACL 2009, pages 245?253.
Kun Yu, Yusuke Miyao, Takuya Matsuzaki, Xiangli
Wang, and Junichi Tsujii. 2011. Analysis of the
difficulties in Chinese deep parsing. In Proc. of the
12th International Conference on Parsing Technolo-
gies, pages 48?57.
Hong-Mei Zhao, Ya-Juan Lv, Guo-Sheng Ben, Yun
Huang, and Qun Liu. 2011. Evaluation report
for the 7th China workshop on machine translation
(CWMT2011). The 7th China Workshop on Ma-
chine Translation (CWMT2011).
33
