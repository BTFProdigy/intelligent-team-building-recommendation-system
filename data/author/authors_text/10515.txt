Proceedings of the 12th Conference of the European Chapter of the ACL, pages 567?575,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Text-to-text Semantic Similarity for Automatic Short Answer Grading
Michael Mohler and Rada Mihalcea
Department of Computer Science
University of North Texas
mgm0038@unt.edu, rada@cs.unt.edu
Abstract
In this paper, we explore unsupervised
techniques for the task of automatic short
answer grading. We compare a number of
knowledge-based and corpus-based mea-
sures of text similarity, evaluate the effect
of domain and size on the corpus-based
measures, and also introduce a novel tech-
nique to improve the performance of the
system by integrating automatic feedback
from the student answers. Overall, our
system significantly and consistently out-
performs other unsupervised methods for
short answer grading that have been pro-
posed in the past.
1 Introduction
One of the most important aspects of the learn-
ing process is the assessment of the knowledge
acquired by the learner. In a typical examination
setting (e.g., an exam, assignment or quiz), this
assessment implies an instructor or a grader who
provides students with feedback on their answers
to questions that are related to the subject mat-
ter. There are, however, certain scenarios, such
as the large number of worldwide sites with lim-
ited teacher availability, or the individual or group
study sessions done outside of class, in which an
instructor is not available and yet students need an
assessment of their knowledge of the subject. In
these instances, we often have to turn to computer-
assisted assessment.
While some forms of computer-assisted assess-
ment do not require sophisticated text understand-
ing (e.g., multiple choice or true/false questions
can be easily graded by a system if the correct so-
lution is available), there are also student answers
that consist of free text which require an analy-
sis of the text in the answer. Research to date has
concentrated on two main subtasks of computer-
assisted assessment: the grading of essays, which
is done mainly by checking the style, grammati-
cality, and coherence of the essay (cf. (Higgins
et al, 2004)), and the assessment of short student
answers (e.g., (Leacock and Chodorow, 2003; Pul-
man and Sukkarieh, 2005)), which is the focus of
this paper.
An automatic short answer grading system is
one which automatically assigns a grade to an an-
swer provided by a student through a comparison
with one or more correct answers. It is important
to note that this is different from the related task of
paraphrase detection, since a requirement in stu-
dent answer grading is to provide a grade on a cer-
tain scale rather than a binary yes/no decision.
In this paper, we explore and evaluate a set of
unsupervised techniques for automatic short an-
swer grading. Unlike previous work, which has
either required the availability of manually crafted
patterns (Sukkarieh et al, 2004; Mitchell et al,
2002), or large training data sets to bootstrap such
patterns (Pulman and Sukkarieh, 2005), we at-
tempt to devise an unsupervised method that re-
quires no human intervention. We address the
grading problem from a text similarity perspec-
tive and examine the usefulness of various text-
to-text semantic similarity measures for automati-
cally grading short student answers.
Specifically, in this paper we seek answers to
the following questions. First, given a number
of corpus-based and knowledge-based methods as
previously proposed in the past for word and text
semantic similarity, what are the measures that
work best for the task of short answer grading?
Second, given a corpus-based measure of similar-
ity, what is the impact of the domain and the size
of the corpus on the accuracy of the measure? Fi-
nally, can we use the student answers themselves
to improve the quality of the grading system?
2 Related Work
There are a number of approaches that have been
proposed in the past for automatic short answer
grading. Several state-of-the-art short answer
graders (Sukkarieh et al, 2004; Mitchell et al,
2002) require manually crafted patterns which, if
matched, indicate that a question has been an-
swered correctly. If an annotated corpus is avail-
567
able, these patterns can be supplemented by learn-
ing additional patterns semi-automatically. The
Oxford-UCLES system (Sukkarieh et al, 2004)
bootstraps patterns by starting with a set of key-
words and synonyms and searching through win-
dows of a text for new patterns. A later implemen-
tation of the Oxford-UCLES system (Pulman and
Sukkarieh, 2005) compares several machine learn-
ing techniques, including inductive logic program-
ming, decision tree learning, and Bayesian learn-
ing, to the earlier pattern matching approach with
encouraging results.
C-Rater (Leacock and Chodorow, 2003)
matches the syntactical features of a student
response (subject, object, and verb) to that of a
set of correct responses. The method specifically
disregards the bag-of-words approach to take
into account the difference between ?dog bites
man? and ?man bites dog? while trying to detect
changes in voice (?the man was bitten by a dog?).
Another short answer grading system, AutoTu-
tor (Wiemer-Hastings et al, 1999), has been de-
signed as an immersive tutoring environment with
a graphical ?talking head? and speech recogni-
tion to improve the overall experience for students.
AutoTutor eschews the pattern-based approach en-
tirely in favor of a bag-of-words LSA approach
(Landauer and Dumais, 1997). Later work on Au-
toTutor (Wiemer-Hastings et al, 2005; Malatesta
et al, 2002) seeks to expand upon the original bag-
of-words approach which becomes less useful as
causality and word order become more important.
These methods are often supplemented with
some light preprocessing, e.g., spelling correc-
tion, punctuation correction, pronoun resolution,
lemmatization and tagging. Likewise, in order to
facilitate their goals of providing feedback to the
student more robust than a simple ?correct? or ?in-
correct,? several systems break the gold-standard
answers into constituent concepts that must indi-
vidually be matched for the answer to be consid-
ered fully correct (Callear et al, 2001). In this way
the system can determine which parts of an answer
a student understands and which parts he or she is
struggling with.
Automatic short answer grading is closely re-
lated to the task of text similarity. While more
general than short answer grading, text similarity
is essentially the problem of detecting and com-
paring the features of two texts. One of the earli-
est approaches to text similarity is the vector-space
model (Salton et al, 1997) with a term frequency
/ inverse document frequency (tf.idf) weighting.
This model, along with the more sophisticated
LSA semantic alternative (Landauer and Dumais,
1997), has been found to work well for tasks such
as information retrieval and text classification.
Another approach (Hatzivassiloglou et al,
1999) has been to use a machine learning algo-
rithm in which features are based on combina-
tions of simple features (e.g., a pair of nouns ap-
pear within 5 words from one another in both
texts). This method also attempts to account for
synonymy, word ordering, text length, and word
classes.
Another line of work attempts to extrapolate
text similarity from the arguably simpler prob-
lem of word similarity. (Mihalcea et al, 2006)
explores the efficacy of applying WordNet-based
word-to-word similarity measures (Pedersen et al,
2004) to the comparison of texts and found them
generally comparable to corpus-based measures
such as LSA.
An interesting study has been performed at the
University of Adelaide (Lee et al, 2005), compar-
ing simpler word and n-gram feature vectors to
LSA and exploring the types of vector similarity
metrics (e.g., binary vs. count vectors, Jaccard
vs. cosine vs. overlap distance measure, etc.).
In this case, LSA was shown to perform better
than the word and n-gram vectors and performed
best at around 100 dimensions with binary vectors
weighted according to an entropy measure, though
the difference in measures was often subtle.
SELSA (Kanejiya et al, 2003) is a system that
attempts to add context to LSA by supplementing
the feature vectors with some simple syntactical
features, namely the part-of-speech of the previous
word. Their results indicate that SELSA does not
perform as well as LSA in the best case, but it has
a wider threshold window than LSA in which the
system can be used advantageously.
Finally, explicit semantic analysis (ESA)
(Gabrilovich and Markovitch, 2007) uses
Wikipedia as a source of knowledge for text
similarity. It creates for each text a feature vector
where each feature maps to a Wikipedia article.
Their preliminary experiments indicated that ESA
was able to significantly outperform LSA on some
text similarity tasks.
3 Data Set
In order to evaluate the methods for short answer
grading, we have created a data set of questions
from introductory computer science assignments
with answers provided by a class of undergradu-
ate students. The assignments were administered
as part of a Data Structures course at the Univer-
sity of North Texas. For each assignment, the stu-
dent answers were collected via the WebCT online
learning environment.
568
The evaluations reported in this paper are car-
ried out on the answers submitted for three of the
assignments in this class. Each assignment con-
sisted of seven short-answer questions.1 Thirty
students were enrolled in the class and submitted
answers to these assignments. Thus, the data set
we work with consists of a total of 630 student an-
swers (3 assignments x 7 questions/assignment x
30 student answers/question).
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both hu-
man judges were graduate computer science stu-
dents; one was the teaching assistant in the Data
Structures class, while the other is one of the au-
thors of this paper. Table 1 shows two question-
answer pairs with three sample student answers
each. The grades assigned by the two human
judges are also included.
The evaluations are run using Pearson?s corre-
lation coefficient measured against the average of
the human-assigned grades on a per-question and
a per-assignment basis. In the per-question set-
ting, every question and the corresponding student
answer is considered as an independent data point
in the correlation, and thus the emphasis is placed
on the correctness of the grade assigned to each
answer. In the per-assignment setting, each data
point is an assignment-student pair created by to-
taling the scores given to the student for each ques-
tion in the assignment. In this setting, the em-
phasis is placed on the overall grade a student re-
ceives for the assignment rather than on the grade
received for each independent question.
The correlation between the two human judges
is measured using both settings. In the per-
question setting, the two annotators correlated at
(r=0.6443). For the per-assignment setting, the
correlation was (r=0.7228).
A deeper look into the scores given by the
two annotators indicates the underlying subjectiv-
ity in grading short answer assignments. Of the
630 grades given, only 358 (56.8%) were exactly
agreed upon by the annotators. Even more strik-
ing, a full 107 grades (17.0%) differed by more
than one point on the five point scale, and 19
grades (3.0%) differed by 4 points or more. 2
1In addition, the assignments had several programming
exercises which have not been considered in any of our ex-
periments.
2An example should suffice to explain this discrepancy in
annotator scoring: Question: What does a function signature
include? Answer: The name of the function and the types of
the parameters. Student: input parameters and return type.
Scores: 1, 5. This example suggests that the graders were
not always consistent in comparing student answers to the in-
structor answer. Additionally, the instructor answer may be
insufficient to account for correct student answers, as ?return
Furthermore, on the occasions when the annota-
tors disagreed, the same annotator gave the higher
grade 79.8% of the time.
Over the course of this work, much attention
was given to our choice of correlation metric.
Previous work in text similarity and short-answer
grading seems split on the use of Pearson?s and
Spearman?s metric. It was not initially clear
that the underlying assumptions necessary for the
proper use of Pearson?s metric (e.g. normal dis-
tribution, interval measurement level, linear cor-
relation model) would be met in our experimental
setup. We considered both Spearman?s and sev-
eral less often used metrics (e.g. Kendall?s tau,
Goodman-Kruskal?s gamma), but in the end, we
have decided to follow previous work using Pear-
son?s so that our scores can be more easily com-
pared.3
4 Automatic Short Answer Grading
Our experiments are centered around the use of
measures of similarity for automatic short answer
grading. In particular, we carry out three sets
of experiments, seeking answers to the following
three research questions.
First, what are the measures of semantic sim-
ilarity that work best for the task of short an-
swer grading? To answer this question, we run
several comparative evaluations covering a num-
ber of knowledge-based and corpus-based mea-
sures of semantic similarity. While previous work
has considered such comparisons for the related
task of paraphrase identification (Mihalcea et al,
2006), to our knowledge no comprehensive eval-
uation has been carried out for the task of short
answer grading which includes all the similarity
measures proposed to date.
Second, to what extent do the domain and the
size of the data used to train the corpus-based
measures of similarity influence the accuracy of
the measures? To address this question, we run
a set of experiments which vary the size and do-
main of the corpus used to train the LSA and the
ESA metrics, and we measure their effect on the
accuracy of short answer grading.
Finally, given a measure of similarity, can we
integrate the answers with the highest scores and
improve the accuracy of the measure? We use
a technique similar to the pseudo-relevance feed-
back method used in information retrieval (Roc-
chio, 1971) and augment the correct answer with
type? does seem to be a valid component of a ?function sig-
nature? according to some literature on the web.
3Consider this an open call for discussion in the NLP
community regarding the proper usage of correlation metrics
with the ultimate goal of consistency within the community.
569
Sample questions, correct answers, and student answers Grade
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior of portions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
more manageable problems. 4, 4
Table 1: Two sample questions with short answers provided by students and the grades assigned by the
two human judges
the student answers receiving the best score ac-
cording to a similarity measure.
In all the experiments, the evaluations are run
on the data set described in the previous section.
The results are compared against a simple baseline
that assigns a grade based on a measurement of
the cosine similarity between the weighted vector-
space representations of the correct answer and the
candidate student answer. The Pearson correla-
tion for this model, using an inverse document fre-
quency derived from the British National Corpus
(BNC), is r=0.3647 for the per-question evaluation
and r=0.4897 for the per-assignment evaluation.
5 Text-to-text Semantic Similarity
We run our comparative evaluations using eight
knowledge-based measures of semantic similarity
(shortest path, Leacock & Chodorow, Lesk, Wu
& Palmer, Resnik, Lin, Jiang & Conrath, Hirst &
St. Onge), and two corpus-based measures (LSA
and ESA). For the knowledge-based measures, we
derive a text-to-text similarity metric by using the
methodology proposed in (Mihalcea et al, 2006):
for each open-class word in one of the input texts,
we use the maximum semantic similarity that can
be obtained by pairing it up with individual open-
class words in the second input text. More for-
mally, for each word W of part-of-speech class C
in the instructor answer, we find maxsim(W,C):
maxsim(W,C) = maxSIMx(W,wi)
where wi is a word in the student answer of class
C and the SIMx function is one of the functions
described below. All the word-to-word similarity
scores obtained in this way are summed up and
normalized with the length of the two input texts.
We provide below a short description for each of
these similarity metrics.
5.1 Knowledge-Based Measures
The shortest path similarity is determined as:
Simpath =
1
length (1)
where length is the length of the shortest path be-
tween two concepts using node-counting (includ-
ing the end nodes).
The Leacock & Chodorow (Leacock and
Chodorow, 1998) similarity is determined as:
Simlch = ? log
length
2 ?D (2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D
is the maximum depth of the taxonomy.
The Lesk similarity of two concepts is defined as
a function of the overlap between the correspond-
ing definitions, as provided by a dictionary. It is
based on an algorithm proposed by Lesk (1986) as
a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) simi-
larity metric measures the depth of two given con-
cepts in the WordNet taxonomy, and the depth of
the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
returns the information content (IC) of the LCS of
two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
570
The measure introduced by Lin (Lin, 1998) builds
on Resnik?s measure of similarity, and adds a
normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
Finally, we consider the Hirst & St. Onge (Hirst
and St-Onge, 1998) measure of similarity, which
determines the similarity strength of a pair of
synsets by detecting lexical chains between the
pair in a text using the WordNet hierarchy.
5.2 Corpus-Based Measures
Corpus-based measures differ from knowledge-
based methods in that they do not require any en-
coded understanding of either the vocabulary or
the grammar of a text?s language. In many of
the scenarios where CAA would be advantageous,
robust language-specific resources (e.g. Word-
Net) may not be available. Thus, state-of-the-art
corpus-based measures may be the only available
approach to CAA in languages with scarce re-
sources.
One corpus-based measure of semantic similar-
ity is latent semantic analysis (LSA) proposed by
Landauer (Landauer and Dumais, 1997). In LSA,
term co-occurrences in a corpus are captured by
means of a dimensionality reduction operated by a
singular value decomposition (SVD) on the term-
by-document matrix T representing the corpus.
For the experiments reported in this section, we
run the SVD operation on several corpora includ-
ing the BNC (LSA BNC) and the entire English
Wikipedia (LSA Wikipedia).4
Explicit semantic analysis (ESA) (Gabrilovich
and Markovitch, 2007) is a variation on the stan-
dard vectorial model in which the dimensions of
the vector are directly equivalent to abstract con-
cepts. Each article in Wikipedia represents a con-
cept in the ESA vector. The relatedness of a term
to a concept is defined as the tf*idf score for the
term within the Wikipedia article, and the related-
ness between two words is the cosine of the two
concept vectors in a high-dimensional space. We
refer to this method as ESA Wikipedia.
4Throughout this paper, the references to the Wikipedia
corpus refer to a version downloaded in September 2007.
5.3 Implementation
For the knowledge-based measures, we use the
WordNet-based implementation of the word-to-
word similarity metrics, as available in the Word-
Net::Similarity package (Patwardhan et al, 2003).
For latent semantic analysis, we use the InfoMap
package.5 For ESA, we use our own imple-
mentation of the ESA algorithm as described in
(Gabrilovich and Markovitch, 2006). Note that
all the word similarity measures are normalized so
that they fall within a 0?1 range. The normaliza-
tion is done by dividing the similarity score pro-
vided by a given measure with the maximum pos-
sible score for that measure.
Table 2 shows the results obtained with each of
these measures on our evaluation data set.
Measure Correlation
Knowledge-based measures
Shortest path 0.4413
Leacock & Chodorow 0.2231
Lesk 0.3630
Wu & Palmer 0.3366
Resnik 0.2520
Lin 0.3916
Jiang & Conrath 0.4499
Hirst & St-Onge 0.1961
Corpus-based measures
LSA BNC 0.4071
LSA Wikipedia 0.4286
ESA Wikipedia 0.4681
Baseline
tf*idf 0.3647
Table 2: Comparison of knowledge-based and
corpus-based measures of similarity for short an-
swer grading
6 The Role of Domain and Size
One of the key considerations when applying
corpus-based techniques is the extent to which size
and subject matter affect the overall performance
of the system. In particular, based on the underly-
ing processes involved, the LSA and ESA corpus-
based methods are expected to be especially sen-
sitive to changes in domain and size. Building the
language models depends on the relatedness of the
words in the training data which suggests that, for
instance, in a computer science domain the terms
?object? and ?oriented? will be more closely re-
lated than in a more general text. Similarly, a large
amount of training data will lead to less sparse
5http://infomap-nlp.sourceforge.net/
571
vector spaces, which in turn is expected to affect
the performance of the corpus-based methods.
With this in mind, we developed two training
corpora for use with the corpus-based measures
that covered the computer science domain. The
first corpus (LSA slides) consists of several online
lecture notes associated with the class textbook,
specifically covering topics that are used as ques-
tions in our sample. The second domain-specific
corpus is a subset of Wikipedia (LSA Wikipedia
CS) consisting of articles that contain any of the
following words: computer, computing, computa-
tion, algorithm, recursive, or recursion.
The performance on the domain-specific cor-
pora is compared with the one observed on the
open-domain corpora mentioned in the previ-
ous section, namely LSA Wikipedia and ESA
Wikipedia. In addition, for the purpose of running
a comparison with the LSA slides corpus, we also
created a random subset of the LSA Wikipedia
corpus approximately matching the size of the
LSA slides corpus. We refer to this corpus as LSA
Wikipedia (small).
Table 3 shows an overview of the various cor-
pora used in the experiments, along with the Pear-
son correlation observed on our data set.
Measure - Corpus Size Correlation
Training on generic corpora
LSA BNC 566.7MB 0.4071
LSA Wikipedia 1.8GB 0.4286
LSA Wikipedia (small) 0.3MB 0.3518
ESA Wikipedia 1.8GB 0.4681
Training on domain-specific corpora
LSA Wikipedia CS 77.1MB 0.4628
LSA slides 0.3MB 0.4146
ESA Wikipedia CS 77.1MB 0.4385
Table 3: Corpus-based measures trained on cor-
pora from different domains and of different sizes
Assuming a corpus of comparable size, we ex-
pect a measure trained on a domain-specific cor-
pus to outperform one that relies on a generic one.
Indeed, by comparing the results obtained with
LSA slides to those obtained with LSA Wikipedia
(small), we see that by using the in-domain com-
puter science slides we obtain a correlation of
r=0.4146, which is higher than the correlation
of r=0.3518 obtained with a corpus of the same
size but open-domain. The effect of the domain
is even more pronounced when we compare the
performance obtained with LSA Wikipedia CS
(r=0.4628) with the one obtained with the full LSA
Wikipedia (r=0.4286).6 The smaller, domain-
6The difference was found significant using a paired t-test
specific corpus performs better, despite the fact
that the generic corpus is 23 times larger and is a
superset of the smaller corpus. This suggests that
for LSA the quality of the texts is vastly more im-
portant than their quantity.
When using the domain-specific subset of
Wikipedia, we observe decreased performance
with ESA compared to the full Wikipedia space.
We suggest that for ESA the high-dimensionality
of the concept space7 is paramount, since many re-
lations between generic words may be lost to ESA
that can be detected latently using LSA.
In tandem with our exploration of the effects
of domain-specific data, we also look at the effect
of size on the overall performance. The main in-
tuitive trends are there, i.e., the performance ob-
tained with the large LSA-Wikipedia is better than
the one that can be obtained with LSA Wikipedia
(small). Similarly, in the domain-specific space,
the LSA Wikipedia CS corpus leads to better per-
formance than the smaller LSA slides data set.
However, an analysis carried out at a finer grained
scale, in which we calculate the performance ob-
tained with LSA when trained on 5%, 10%, ...,
100% fractions of the full LSA Wikipedia corpus,
does not reveal a close correlation between size
and performance, which suggests that further anal-
ysis is needed to determine the exact effect of cor-
pus size on performance.
7 Relevance Feedback based on Student
Answers
The automatic grading of student answers im-
plies a measure of similarity between the answers
provided by the students and the correct answer
provided by the instructor. Since we only have
one correct answer, some student answers may be
wrongly graded because of little or no similarity
with the correct answer that we have.
To address this problem, we introduce a novel
technique that feeds back from the student an-
swers themselves in a way similar to the pseudo-
relevance feedback used in information retrieval
(Rocchio, 1971). In this way, the paraphrasing that
is usually observed across student answers will en-
hance the vocabulary of the correct answer, while
at the same time maintaining the correctness of the
gold-standard answer.
Briefly, given a metric that provides similarity
scores between the student answers and the cor-
rect answer, scores are ranked from most similar
(p<0.001).
7In ESA, all the articles in Wikipedia are used as dimen-
sions, which leads to about 1.75 million dimensions in the
ESA Wikipedia corpus, compared to only 55,000 dimensions
in the ESA Wikipedia CS corpus.
572
to least. The words of the top N ranked answers
are then added to the gold standard answer. The
remaining answers are then rescored according the
the new gold standard vector. In practice, we hold
the scores from the first run (i.e., with no feed-
back) constant for the top N highest-scoring an-
swers, and the second-run scores for the remaining
answers are multiplied by the first-run score of the
Nth highest-scoring answer. In this way, we keep
the original scores for the top N highest-scoring
answers (and thus prevent them from becoming ar-
tificially high), and at the same time, we guarantee
that none of the lower-scored answers will get a
new score higher than the best answers.
The effects of relevance feedback are shown in
Figure 9, which plots the Pearson correlation be-
tween automatic and human grading (Y axis) ver-
sus the number of student answers that are used
for relevance feedback (X axis).
Overall, an improvement of up to 0.047 on
the 0-1 Pearson scale can be obtained by using
this technique, with a maximum improvement ob-
served after about 4-6 iterations on average. Af-
ter an initial number of high-scored answers, it is
likely that the correctness of the answers degrades,
and thus the decrease in performance observed af-
ter an initial number of iterations. Our results in-
dicate that the LSA and WordNet similarity met-
rics respond more favorably to feedback than the
ESA metric. It is possible that supplementing the
bag-of-words in ESA (with e.g. synonyms and
phrasal differences) does not drastically alter the
resultant concept vector, and thus the overall ef-
fect is smaller.
8 Discussion
Our experiments show that several knowledge-
based and corpus-based measures of similarity
perform comparably when used for the task of
short answer grading. However, since the corpus-
based measures can be improved by account-
ing for domain and corpus size, the highest per-
formance can be obtained with a corpus-based
measure (LSA) trained on a domain-specific cor-
pus. Further improvements were also obtained
by integrating the highest-scored student answers
through a relevance feedback technique.
Table 4 summarizes the results of our experi-
ments. In addition to the per-question evaluations
that were reported throughout the paper, we also
report the per-assignment evaluation, which re-
flects a cumulative score for a student on a single
assignment, as described in Section 3.
Overall, in both the per-question and per-
assignment evaluations, we obtained the best per-
formance by using an LSA measure trained on
Correlation
Measure per-quest. per-assign.
Baselines
tf*idf 0.3647 0.4897
LSA BNC 0.4071 0.6465
Relevance Feedback based on Student Answers
WordNet shortest path 0.4887 0.6344
LSA Wikipedia CS 0.5099 0.6735
ESA Wikipedia full 0.4893 0.6498
Annotator agreement 0.6443 0.7228
Table 4: Summary of results obtained with vari-
ous similarity measures, with relevance feedback
based on six student answers. We also list the
tf*idf and the LSA trained on BNC baselines (no
feedback), as well as the annotator agreement up-
per bound.
a medium size domain-specific corpus obtained
from Wikipedia, with relevance feedback from
the four highest-scoring student answers. This
method improves significantly over the tf*idf
baseline and also over the LSA trained on BNC
model, which has been used extensively in previ-
ous work. The differences were found to be sig-
nificant using a paired t-test (p<0.001).
To gain further insights, we made an additional
analysis where we determined the ability of our
system to make a binary accept/reject decision. In
this evaluation, we map the 0-5 human grading of
the data set to an accept/reject annotation by us-
ing a threshold of 2.5. Every answer with a grade
higher than 2.5 is labeled as ?accept,? while ev-
ery answer below 2.5 is labeled as ?reject.? Next,
we use our best system (LSA trained on domain-
specific data with relevance feedback), and run a
ten-fold cross-validation on the data set. Specif-
ically, for each fold, the system uses the remain-
ing nine folds to automatically identify a thresh-
old to maximize the matching with the gold stan-
dard. The threshold identified in this way is used
to automatically annotate the test fold with ?ac-
cept?/?reject? labels. The ten-fold cross validation
resulted in an accuracy of 92%, indicating the abil-
ity of the system to automatically make a binary
accept/reject decision.
9 Conclusions
In this paper, we explored unsupervised tech-
niques for automatic short answer grading.
We believe the paper made three important con-
tributions. First, while there are a number of word
and text similarity measures that have been pro-
posed in the past, to our knowledge no previ-
ous work has considered a comprehensive evalu-
573
 0.35
 0.4
 0.45
 0.5
 0.55
 0  5  10  15  20
Co
rre
la
tio
n
Number of student answers used for feedback
LSA-Wiki-full
LSA-Wiki-CS
LSA-slides-CS
ESA-Wiki-full
ESA-Wiki-CS
WN-JCN
WN-PATH
TF*IDF
LSA-BNC
Figure 1: Effect of relevance feedback on performance
ation of all the measures for the task of short an-
swer grading. We filled this gap by running com-
parative evaluations of several knowledge-based
and corpus-based measures on a data set of short
student answers. Our results indicate that when
used in their original form, the results obtained
with the best knowledge-based (WordNet short-
est path and Jiang & Conrath) and corpus-based
measures (LSA and ESA) have comparable per-
formance. The benefit of the corpus-based ap-
proaches over knowledge-based approaches lies in
their language independence and the relative ease
in creating a large domain-sensitive corpus versus
a language knowledge base (e.g., WordNet).
Second, we analysed the effect of domain and
corpus size on the effectiveness of the corpus-
based measures. We found that significant im-
provements can be obtained for the LSA measure
when using a medium size domain-specific corpus
built from Wikipedia. In fact, when using LSA,
our results indicate that the corpus domain may be
significantly more important than corpus size once
a certain threshold size has been reached.
Finally, we introduced a novel technique for in-
tegrating feedback from the student answers them-
selves into the grading system. Using a method
similar to the pseudo-relevance feedback tech-
nique used in information retrieval, we were able
to improve the quality of our system by a few per-
centage points.
Overall, our best system consists of an LSA
measure trained on a domain-specific corpus built
on Wikipedia with feedback from student answers,
which was found to bring a significant absolute
improvement on the 0-1 Pearson scale of 0.14 over
the tf*idf baseline and 0.10 over the LSA BNC
model that has been used in the past.
In future work, we intend to expand our analy-
sis of both the gold-standard answer and the stu-
dent answers beyond the bag-of-words paradigm
by considering basic logical features in the text
(i.e., AND, OR, NOT) as well as the existence
of shallow grammatical features such as predicate-
argument structure(Moschitti et al, 2007) as well
as semantic classes for words. Furthermore, it may
be advantageous to expand upon the existing mea-
sures by applying machine learning techniques to
create a hybrid decision system that would exploit
the advantages of each measure.
The data set introduced in this paper, along with
the human-assigned grades, can be downloaded
from http://lit.csci.unt.edu/index.php/Downloads.
Acknowledgments
This work was partially supported by a National
Science Foundation CAREER award #0747340.
The authors are grateful to Samer Hassan for mak-
ing available his implementation of the ESA algo-
rithm.
References
D. Callear, J. Jerrams-Smith, and V. Soh. 2001.
CAA of Short Non-MCQ Answers. Proceedings of
574
the 5th International Computer Assisted Assessment
conference.
E. Gabrilovich and S. Markovitch. 2006. Overcoming
the brittleness bottleneck using Wikipedia: Enhanc-
ing text categorization with encyclopedic knowl-
edge. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), Boston.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Ex-
plicit Semantic Analysis. Proceedings of the 20th
International Joint Conference on Artificial Intelli-
gence, pages 6?12.
V. Hatzivassiloglou, J. Klavans, and E. Eskin. 1999.
Detecting text similarity over short passages: Ex-
ploring linguistic feature combinations via machine
learning. Proceedings of the Joint SIGDAT Con-
ference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile.
2004. Evaluating multiple aspects of coherence in
student essays. In Proceedings of the annual meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as rep-
resentations of contexts for the detection and correc-
tion of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, Taiwan.
D. Kanejiya, A. Kumar, and S. Prasad. 2003. Au-
tomatic evaluation of students? answers using syn-
tactically enhanced LSA. Proceedings of the HLT-
NAACL 03 workshop on Building educational appli-
cations using natural language processing-Volume
2, pages 53?60.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis the-
ory of acquisition, induction, and representation of
knowledge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Au-
tomated Scoring of Short-Answer Questions. Com-
puters and the Humanities, 37(4):389?405.
M.D. Lee, B. Pincombe, and M. Welsh. 2005. An em-
pirical evaluation of models of text document simi-
larity. Proceedings of the 27th Annual Conference
of the Cognitive Science Society, pages 1254?1259.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to
text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence
(AAAI 2006), Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of
free-text responses. Proceedings of the 6th Interna-
tional Computer Assisted Assessment (CAA) Confer-
ence.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Conference of the Association for Computa-
tional Linguistics.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003.
Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the Fourth
International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Mexico
City, February.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference
on Artificial Intelligence, pages 1024?1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic
Short Answer Marking. ACL WS Bldg Ed Apps us-
ing NLP.
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th
International Joint Conference on Artificial Intelli-
gence, Montreal, Canada.
J. Rocchio, 1971. Relevance feedback in information
retrieval. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Wong, and C.S. Yang. 1997. A vec-
tor space model for automatic indexing. In Read-
ings in Information Retrieval, pages 273?280. Mor-
gan Kaufmann Publishers, San Francisco, CA.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004.
Auto-Marking 2: An Update on the UCLES-Oxford
University research into using Computational Lin-
guistics to Score Short, Free Text Responses. In-
ternational Association of Educational Assessment,
Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tutor?s
comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535?542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton.
2005. Initial results and mixed directions for re-
search methods tutor. In AIED2005 - Supplementary
Proceedings of the 12th International Conference on
Artificial Intelligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proceedings of the 32nd Annual
Meeting of the Association for Computational Lin-
guistics, Las Cruces, New Mexico.
575
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1752?1763, Dublin, Ireland, August 23-29 2014.
A Novel Distributional Approach to Multilingual Conceptual Metaphor
Recognition
Michael Mohler and Bryan Rink and David Bracewell and Marc Tomlinson
Language Computer Corp.
Richardson, Texas, USA
{michael,bryan,david,marc}@languagecomputer.com
Abstract
We present a novel approach to the problem of multilingual conceptual metaphor recognition.
Our approach extends recent work in conceptual metaphor discovery by combining a complex
methodology for facet-based concept induction with a distributional vector space model of lin-
guistic and conceptual metaphor. In the evaluation of our system in English, Spanish, Russian,
and Farsi, we experiment with several state-of-the-art vector space models and demonstrate a
clear benefit to the fine-grained concept representation that forms the basis of our methodology
for conceptual metaphor recognition.
1 Introduction
The role of metaphor in language has been defined by Lakoff et al. (1980; 1993) as a cognitive phe-
nomenon which operates at the level of mental processes, whereby one concept or domain is viewed
systematically in terms of another. For example, the phrase ?to cure poverty? is a metaphor which subtly
conveys a wide variety of information to the listener. In order to mentally process this phrase, we must
first recognize that a metaphor is being used and that ?cure? (as a medical term) is being used figura-
tively. Then, we assume some relationship between ?poverty? and ?things that can be medically cured?
which leads to the conceptual mapping ?POVERTY as DISEASE.? This conceptual mapping enables the
listener to transfer a variety of properties and associations between the two concepts, such as their as-
sociation with a feeling of helplessness, the existence of sustained efforts to end them, the potential for
them to spread, and their mutual relationship with ill-health and death. Therefore, by identifying the con-
ceptual domains associated with this linguistic metaphor, we are able to reason about the target domain
(POVERTY) using concepts and terms associated with the source domain (DISEASE).
Any natural language processing system capable of processing metaphor in text with human-level
competence must, therefore, overcome three problems in sequence:
1. the identification of metaphorical expressions (also known as linguistic metaphors (LMs))
2. the discovery of a conceptual domain mapping or conceptual metaphor (CM) which consists of
(a) the conceptual domain of the metaphor target (e.g., POVERTY); and
(b) the conceptual domain of the metaphor source (e.g., DISEASE)
3. the real-world interpretation of the metaphorical text which uses the conceptual metaphor frame-
work to transfer knowledge between the source and target domains.
While a significant amount of recent work has presented interesting and promising methodologies for
multilingual LM identification (Shutova and Sun, 2013; Wilks et al., 2013; Strzalkowski et al., 2013),
the work presented in this paper is focused on (2), the problem of multilingual CM recognition, which
will be made to serve as the foundation for a more fine-grained interpretation of metaphor.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1752
We cast the CM recognition process as a two-part methodology which (a) selects the target domain
associated with a particular LM that has been detected; and (b) determines the source domain to which
it should be mapped in order to produce a satisfactory interpretation. In this work, we assume that the
target domains are known and belong to one of the following conceptual spaces: POVERTY, WEALTH, or
TAXATION. Pragmatically speaking, research in CM recognition presupposes some methodology for LM
identification, and to this end, we have employed an existing state-of-the-art LM identification system
which has been developed to detect linguistic metaphors in four languages: English, Spanish, Russian,
and Farsi (Bracewell et al., 2014).
In order to generate a CM which can serve as the basis for an interpretation of an LM, we have
developed an approach that is based on the following hypotheses:
CONCEPTUAL HYPOTHESIS: When an LM has been identified as a pair of lexical items that
represent the source (e.g., ?cure?) and the target (e.g., ?poverty?), we can generate a conceptual
mapping by selecting the conceptual domains that are, a priori, the most likely for the source and
target lexemes.
1
DISTRIBUTIONAL HYPOTHESIS: It is possible to decide which conceptual space better repre-
sents a given lexeme by
1. expanding the lexical space with additional terms (which we call ?grammatical co-occurrents?)
that are strongly associated with the lexeme through grammatical relations such as AGENT,
PATIENT, INSTRUMENT, and ATTRIBUTE;
2. using these lexical expansions to produce distributional vectors; and
3. uncovering the selectional constraints of particular domain facets by clustering the distribu-
tional vectors within a semantic space.
DOMAIN HYPOTHESIS: The grammatical co-occurrents of the LM are themselves very likely to
belong to the same conceptual domain as the lexeme (e.g., ?cure patient?, ?cured of AIDS?, and
?doctor cured?).
MAPPING HYPOTHESIS: The semantic space representations of both the LM source and its gram-
matically associated terms can be used to produce mappings into a high dimensional space in which
source domains are known to exist.
While other computational linguistics research in metaphor has made use of the CONCEPTUAL and
DISTRIBUTIONAL hypotheses, to our knowledge the DOMAIN and MAPPING hypotheses have not
yet been explored in combination with a distributional approach.
The remainder of this work is organized as follows. In Section 2, we discuss related work in the field
of metaphor interpretation and unsupervised concept induction. In Section 3, we introduce the overall
architecture of our CM recognition system. In Section 4, we describe our method for representing lexical
items and conceptual metaphors in a distributional vector space. Then, in Section 5, we explain our
methodology for creating and ranking clusters of LM co-occurrents which are then mapped to conceptual
metaphors within our vector space. In Section 6, we describe our experimental setup and provide the
results of our experiments. Finally, in Section 7 we present our conclusions.
2 Related Work
Research in metaphor processing can broadly be divided into two categories: metaphor identification and
metaphor interpretation. Although some recent work on metaphor interpretation has skirted the issue of
conceptual metaphor entirely by casting the problem of metaphor interpretation as an instance of lexical
paraphrase (Shutova, 2010; Bollegala and Shutova, 2013) or textual entailment (Mohler et al., 2013), the
mapping and modeling of conceptual metaphors has historically served as an important foundation for
1
If the target domains are pre-selected, this hypotheses is reduced to selecting only the most likely source domain.
1753
more robust interpretation of metaphor. Indeed, a significant amount of research in metaphor interpreta-
tion has been concentrated on the development of highly-structured, manually curated representations of
both the CM source and CM target domains. Notable in this regard are the KARMA system (Feldman
and Narayanan, 2004) which was designed to simulate neurological modeling of verbs ? both abstract
and metaphorical ? and the ISOMETA system (Beust et al., 2003) which made use of differential tables
of CM domain lexical items to drive their metaphor interpretation process. The CorMet system (Ma-
son, 2004) sought to model conceptual metaphors by detecting individual source-target mappings that
provide evidence for a known CM by quantifying the overlap between clusters of terms with a strong
selectional preference to the most representative verbs within the source and target domains. After a man-
ual inspection of the source/target cluster pairs across domains, the directionality and the systematicity
of these underlying conceptual mappings were quantified in order to produce an overall confidence in
the mapping. As part of their development of the Hamburg Metaphor Database (HMD), Reining and
L?onneker-Rodman (2007) performed a a manual categorization of lexical items into conceptual source
domains with a facet-level granularity and enriched their domains using a WordNet-based lexical expan-
sion. In the same vein, Chung et al. (2005) chose to model source domains by expanding their lexical
items by exploiting the links between WordNet glosses and the SUMO ontology.
In recent years, however, research has focused on automating the modeling and classification of con-
ceptual metaphors as much as possible in order to encourage the scaling up of metaphor research in
general. Veale and Hao (2008), as part of the Talking Points system, developed what they refer to as a
Slipnet which defines linked chains of meaning that connect a source to a target through shared (or re-
lated) attributes and actions. As a step in this process, they combined WordNet relations with pragmatic
relations extracted from text and clustered nouns according to their relation (and attribute) similarity in
order to define a weak conceptual mapping within the clusters. In a similar way, Shutova et al. (2010),
beginning with a seed set of noun/verb linguistic metaphor pairs, performed spectral clustering on a large
set of nouns and verbs in order to predict metaphors which participate in the same conceptual metaphor
mapping. In particular, she modeled verbs according to their subcategorization frames parameterized by
a model of their selectional preferences, while nouns were modeled according to the verbs with which
they frequently co-occurred in a dependency relation.
More recently, Gandy et al. (2013) approached the CM discovery problem as a set covering problem.
For a given nominal target lexeme, they began by finding all facets (i.e., verbs/adjectives) that share
a positive PMI with the target. Then, they would find the set of nouns that also have a positive PMI
with those facets, compute their confidence in each association, and heuristically select pairs of concepts
(defined as rooted WordNet synset trees) which subsume a large percentage of those nouns and cover a
large portion of the overlapping facets. Similarly, Shutova and Sun (2013) detect conceptual mappings by
performing hierarchical graph factorization clustering on a graph in which the vertices are defined to be
nouns (i.e., concepts) and the edges are weighted using Jensen-Shannon Divergence. For a given input
LM source, its likely conceptual metaphors are then discovered by determining its non-literal cluster
membership. Finally, Strzalkowski et al. (2013) discovered terms (literal and metaphoric) which often
co-occur with an LM source in a corpus and clustered those terms using WordNet and corpus statistics
to form ?ProtoSources? which could be further inspected to define CM source concepts.
Two vector-based approaches to concept representation are of particular interest in understanding the
present work. In the first of these, Sch?utze (1998) described an approach to word sense identification
using second-order co-occurrence vectors which were used to cluster first-order vectors of the in-context
terms into senses.
2
Lin (1998), in developing a methodology for evaluating the quality of thesauri,
defined a word vector space that moved beyond simple co-occurrence by integrating information about
the relations between the word and its co-occurrents. In particular, a word?s vector was defined by the
number of times that word occurred within a set of (word, relation, word) tuples. Our DepVec space
represents an extension to Lin?s space insofar as we incorporate additional information about relational
(i.e., selectional) preference.
2
While context is critical in word sense disambiguation, we hasten to point out that one mark of metaphoricity is its discon-
nect from the surrounding literal context.
1754
Figure 1: The architecture of our conceptual metaphor recognition system. This system takes a linguistic
metaphor as input, induces potential concepts using vector-space clustering, and maps these clusters onto
a conceptual metaphor domain.
3 A New Methodology for Conceptual Metaphor Recognition
Figure 1 shows the overall flow of our metaphor processing architecture. We begin with a set of doc-
uments gathered from a variety of online news-wire sources. These documents are fed to our state-
of-the-art LM detection system which employs a binary logistic regression classifier using a variety of
feature modules including imageability and concreteness estimation, topicality modeling, pattern match-
ing, semantic categorization, selectional preference violation, and source/target vector space similarity.
The methodology used in this system is beyond the scope of this work, but it is described in detail by
Bracewell et al. (2014). The LMs provided by the detection system are validated by a group of native-
language experts before being sent for CM recognition system for concept-level interpretation.
Once the LMs have been collected and validated, the CM recognition system begins by extracting,
weighting, and clustering the common grammatical contexts of the LM source term. By grammatical
context, we refer to the syntactic relations (along with their arguments) which have been found to fre-
quently co-occur with the LM source term in open text. In order to model this grammatical context, we
have syntactically parsed a wide collection of documents in each of our focus languages: English, Span-
ish, Russian, and Farsi. From these parsed documents, we have extracted the most common grammatical
co-occurrents of each word in the corpus along with the relation that connects them and the number
of times they are connected by that relation. For a given word, we refer to the set of its grammatical
co-occurrents as the ?concept candidates? associated with that word, as they represent potential concepts
within the same conceptual domain as the given word (the DOMAIN HYPOTHESIS). For example,
grammatical co-occurrents of the noun ?battle? would include many WAR concepts such as ?fought?,
?died in?, ?waged?, ?naval?, and ?losing?.
Since a conceptual domain is made up of several interacting concepts, we perform a clustering over
the grammatical co-occurrents to produce groups of terms which are likely to represent individual con-
cepts within a domain. The clustering is performed within a high-dimensional, distributional vector
space which we describe in Section 4. The clusters are then merged and aligned with a set of 51 pre-
defined source concept domains (see Table 1) that have been found to occur frequently in conceptual
metaphors about POVERTY, WEALTH, or TAXATION. For each of these known conceptual domains, we
have amassed a collection of lexical items for the purpose of modeling the domains and aligning them
to our automatically discovered domains. The collection of lexical items associated with each domain
have been further partitioned into three to five facets which provide a more fine-grained representation of
the domain. For instance, the conceptual domain of ABYSS as been subdivided into facets representing
1755
Full Source Concept List
A GOD COMPETITION ENSLAVEMENT LIGHT NATURAL PHYSICAL FORCE PORTAL
A RIGHT CONFINEMENT FOOD LOW POINT OBESITY RESOURCE
ABYSS CRIME FORCEFUL EXTRACTION MACHINE PARASITE SCHISM
ACCIDENT CROP GAME MAZE PATHWAY STRUGGLE
ADDICTION DARKNESS GEOGRAPHIC FEATURE MEDICINE PHYSICAL BURDEN VERTICAL SCALE
ANIMAL DESTROYER GOAL DIRECTED MONSTER PHYSICAL HARM VISION
BLOOD SYSTEM DISEASE HIGH POINT MORAL DUTY PHYSICAL LOCATION
BODY OF WATER ENABLER HUMAN BODY MOVEMENT PHYSICAL OBJECT
BUILDING ENERGY IMPURITY MOVEMENT ON A VERTICAL SCALE PLANT
Sample Lexical Items
ANIMAL bite, bark, claw, bird, beaver MEDICINE dosage, prescription, heal
ENSLAVEMENT servant, oppression, ruler STRUGGLE enemy, fight, combat, attack
Table 1: The 51 source conceptual domains along with some sample English lexical items for a subset
of them.
DEPTH (e.g., ?deep?, ?bottomless?), ENTRANCE (e.g., ?plunged into?, ?falling into?), and EXIT (e.g.,
?climb out of?).
3.1 Motivating Example
Table 2 shows a sample of the concept candidates associated with the word ?cure? along with the relation
that connects them. Our methodology for extracting these terms is discussed in Section 5.1.
nsubj
NIH, WHO, therapist, doctor, vaccine,
prep of
cancer, AIDS, HIV, malaria, influenza,
drug, medicine, chef, butcher seizures, allergies
dobj
cancer, polio, Goji Berries, man,
prep by
bone marrow transplant, spleen cells,
genetic defects, aging, infant, woman, acupuncture, smoking, salting,
depression, meat, fish, garlic doxycycline, drying, burying, dipping
prep without
surgery, operation, suppuration, salt
prep to
?1
need, project, brine, mineral,
chemotherapy, injections coalition, run, walk, salt, nitrite
prep in
mice, children, baby, spices, salt,
prep for
grinding, smoking, voyages, lox,
monkeys, drug trial, breakthrough, transportation, preservation, jerky,
brine, smokehouse, basement, fridge sausages, bacon, sale
Table 2: Terms that are frequently a part of the grammatical context of ?cure? along with their associated
relations
It is clear from the concept candidates shown that there are at least two coarse-grained senses of
?cure? present ? corresponding to the domains of MEDICINE and FOOD. Table 3 shows a sample
result of clustering these concept candidates. These clusters are organized according to their domain
with MEDICINE-related clusters in the left grouping, FOOD-related clusters in the top-right grouping,
and clusters not strongly related to either domain in the bottom-right grouping. Each row of the table
represents a single cluster. In addition, it can be observed that these clusters correspond to particular
semantic facets of the conceptual domain. For instance, there is a cluster that defines ?procedures which
result in medical cures? (?acupuncture?, ?surgery?, ?operation?, etc.), one that defines ?individuals who
cure food products? (?chef?, ?butcher?), and one that defines ?diseases that can (potentially) be cured?
(?cancer?, ?polio?, ?AIDS?, etc.). Our methodology for automatically inducing such clusters is described
in Section 5.2.
Once the clusters have been identified, they can be used to define a mapping from the original LM
(?cure?) onto a pre-defined set of CM source domains (the MAPPING HYPOTHESIS). In particular,
individual concept candidates are mapped to CM domains by calculating the distance between the can-
didate and one or more vectors representing each domain in a high-dimensional distributional vector
space.
4 Distributional Representations
Our method for identifying conceptual metaphor domains relies on determining when multiple words
should be grouped as belonging to the same conceptual class (the DISTRIBUTIONAL HYPOTHESIS).
Previous work in semantic similarity has shown two types of approaches to work well: (a) hand-coded
knowledge such as WordNet or SUMO, and (b) distributional approaches which rely on statistics of
1756
NIH, WHO, therapist, doctor chef, butcher
vaccine, drug, medicine, doxycycline project, coalition
spleen cells, bone marrow transplant meat, fish, sausages, jerky, bacon, lox
acupuncture, surgery, operation garlic, Goji Berries
chemotherapy, injections, suppuration smoking, salting, drying, dipping
HIV, malaria, influenza burying
cancer, polio, AIDS salt, brine, spices, nitrite, mineral
genetic defects, aging, depression smokehouse, basement, fridge
seizures, allergies run, walk
drug trial, breakthrough voyages, transportation
infant, man, woman, children, baby mice, monkeys
Table 3: Terms from Table 2 grouped into conceptual clusters ? one per line. These clusters are organized
according to their domain association: MEDICINE (left), FOOD (top-right), unclear (bottom-right).
word usage in corpora. We adopt the distributional approach in order to facilitate research in languages
(such as Farsi) for which coverage of existing knowledge bases is limited. The only requirements for our
approach are a corpus with documents written in that language and a syntactic parser for the language.
We use the Malt dependency parser to obtain syntactic parses for web documents in each language.
Table 2 of Section 3.1 shows some of the words which participate regularly with the word ?cure?
in a dependency relation. These syntactic contexts of the word ?cure? form the basis for one semantic
representation we use to find other similar words, which we will call DepVec. All of the dependency
relations for a word are used to form a vector-based distributional representation for that word. This
representation projects words which are semantically similar to one another onto vectors which are near
to each other in the vector space. In the following subsection, we describe DepVec along with LSA and
word2vec which are alternative vector space models of word meaning. These vector spaces are then used
to calculate similarities between words in order to cluster them and to align them with lexicons which
model our existing conceptual spaces.
4.1 Dependency Vectors (DepVec) space
In our DepVec vector space model, each word is represented by a vector whose elements correspond
to syntactic contexts of the word. Each element of the vector for word w corresponds to the fre-
quency of a unique dependency relation (w, r, w
?
) seen in the corpus. For example, if the relation
(whale, nsubj
?1
, swim) is extracted once, then the vector for ?whale? contains a 1 in the element
for (nsubj
?1
, swim) , and the vector for ?swim? contains a 1 for the element (nsubj, whale). This
representation corresponds that proposed by Lin (1998).
However, the use of raw frequency counts in these vectors leads to a situation in which words that
are more frequent in the corpus (e.g., ?of?, ?the?, ?one?) will have higher frequencies in the vectors by
chance alone, and so a high co-occurrence count for those words is not indicative of a significant relation
to the word. We overcome this limitation by replacing the raw frequency counts in each vector with their
corresponding G-test scores. The G-test is a measure of statistical significance for proportions, similar to
the Chi-square test, which measures the degree to which a particular triple (w, r, w
?
) was found to occur
more frequently than expected given all relations (w
??
, r, w
?
). If w
?
occurs far more often with w than
it does with other words, then it will receive a high G-test score for w. In particular, the G-test score is
computed according to the following equation:
G = 2
?
i
O
i
? ln(O
i
/E
i
)
where the index i ranges over the four cells of a 2x2 contingency table, O
i
is the observed count in cell
i, and E
i
is the expected count in the same cell.
1757
Language Source # Documents Language Source # Documents
English ClueWeb 13,361,743 Spanish ClueWeb 3,682,478
Russian ruWac 1,173,590 Farsi Online news sites 835,588
Table 4: Statistics of the corpora used to construct the vector space models
4.2 Latent Semantic Analysis (LSA)
While the DepVec model provides information about the immediate contexts a word can be expected
to occur in, it does not directly capture information about the broader contexts typical of that word,
such as topical information. Latent Semantic Analysis (LSA) is a well-studied model (Landauer and
Dumais, 1997) which does capture such topical information. The LSA model utilizes a singular value
decomposition of a TF-IDF weighted matrix representation of the term-document co-occurrences. Terms
and documents are then represented in a reduced dimensionality space using only the information from
the eigenvectors with the k largest eigenvalues.
4.3 Continuous skip-gram model (W2V)
Mikolov et al. (2013) recently presented a new method for determining distributional word representa-
tions based on a shallow neural network model. The values of the latent vector for each word are trained
to optimize prediction of the words within a 10 token window. This prediction is performed using the
term?s latent vector as the input to a series of log-linear classifiers with outputs which correspond to
probability distributions over the tokens within the context window. Each position in the context window
is assigned its own classifier weights, so that the model used for making predictions about words imme-
diately following the input term is different than the model which makes predictions about the words two
tokens after the term, and so on. Because these latent vector representations are in a low dimensionality
space (300 dimensions in our case), the training process will tend to move the representations for similar
words closer together in this space in order to maximize the predictive accuracy of their contexts.
One benefit of the continuous skip-gram model is that it creates representations which capture some
local context as in the DepVec model, which is required to make predictions about the previous and next
tokens. However, it must also encode some topical knowledge in order to make accurate predictions
about the words seven tokens away. Therefore, using the latent term representations from the continuous
skip-gram model as a vector space puts it in a convenient position in between the two others we presented.
4.4 Corpus Processing
The vector models described above were developed using web-scale corpora collected from a combina-
tion of frequently used NLP corpora and web crawls on news websites. Table 4 indicates the number of
documents used for each language along with their source. These corpora were part-of-speech tagged
with in-house POS taggers for English and Spanish, TreeTagger
3
for Russian, and hunpos
4
for Farsi. The
open-source MaltParser was used to produce dependency parses for all four languages (Nivre, 2003). De-
pendency counts for all words occurring fewer than 40 times and for triples occurring fewer than three
times were discarded to minimize noise.
5 Concept Induction and CM Recognition
In Section 4, we described our DepVec representation of terms as vectors in a high-dimensional dis-
tributional space. These vector representations encode both the dominant grammatical contexts of a
term as well as the selectional preference information associated with it in the form of G-test scores. In
this section, we describe our methodology for inducing conceptual domains for a linguistic metaphor
by adapting techniques for unsupervised word-sense induction (Erk and Pad?o, 2008; Korkontzelos and
Manandhar, 2010; Hope and Keller, 2013). In particular, we induce conceptual domains in an uncon-
strained manner by extracting the grammatical co-occurrents of an LM source term (i.e., the ?concept
candidates?) and clustering them into semantically-related concept clusters. Both the clusters and our
3
http://www.cis.uni-muenchen.de/
?
schmid/tools/TreeTagger/
4
http://code.google.com/p/hunpos/
1758
given source domains are then mapped into a distributional vector space, allowing us to compute cluster-
to-domain scores. Finally, each source domain is assigned a score based on its affinity to each individual
cluster with these affinity scores weighted according to cluster quality. This results in an overall weighted
ranking of the given source conceptual domains for the linguistic metaphor.
5.1 Extracting Concept Candidates
Given a linguistic metaphor which consists of a metaphor source, s (e.g., ?cure?), and a metaphor target,
t (e.g., ?poverty?), our system extracts a set of terms (i.e., ?concept candidates?) from the typical gram-
matical contexts of s as found in the web-scale corpus described in Section 4.4. In order to extract these
candidates, we first determine the syntactic relation, r, which exists between s and t. This relation is the
key point of interaction between the domains of the source and the target for the given LM and, as such,
it provides an indication of which terms will contribute the most to our understanding of the underly-
ing conceptual mapping. In addition, we make use of a predefined set of relations that are semantically
meaningful ? specifically the subjects and objects of verbs (i.e., ?nsubj?, ?nsubjpass?, and ?dobj?),
5
at-
tributes and verbs associated with nouns (i.e., ?amod?, ?dobj
?1
?, ?nsubj
?1
?, and ?nsubjpass
?1
?), the
terms modified by adjectives or adverbs (i.e., ?advmod
?1
? and ?amod
?1
?), and prepositional relations
(e.g., ?prep by?, ?prep of?, ?prep for?). Using this set of relations, R, we extract the set of candidate
terms, X , that have been found to co-occur with the term s within some relation r
i
? R in the prepro-
cessed, web-scale corpus described in Section 4.4 such that X = {x|(s, r
i
, x)exists in the corpus}.
To improve the quality of our extracted candidates, we apply three criteria to isolate those that best
exemplify the underlying non-metaphorical senses of s. First, we anticipate that any term in X which
does not co-occur with s at least k times will not be informative,
6
and so we remove such terms from
further processing. Next, we predict that poorly imageable terms (i.e., highly abstract terms) are likely to
represent metaphorical usages of s and so are unlikely to be integral to a given literal source domain, so
these are filtered out as well.
7
Finally, to improve our ability to map these candidates into a conceptual
domain, we remove terms that are not significantly related to any of our provided source domains (i.e.,
those that are off-topic) along with terms that are strongly related to multiple source domains (i.e., those
that are ambiguous) as these provide little evidence to distinguish the most appropriate concept for the
given LM.
8
We determine the relatedness of a term to a source domain by measuring the similarity of
the term and domain vectors in our distributional space as described in Section 5.3.
5.2 Clustering Concept Candidates
Once the candidates have been extracted, they are clustered using a hierarchical agglomerative clustering
algorithm with the distance metric defined as the cosine distance between the vectors within one of our
distributional vector spaces. Each cluster is then assigned a quality score based on its size (to prefer large
clusters with a large amount of semantic evidence), average internal distance (to prefer tighter clusters),
and co-occurrence frequency with the LM source (to prefer more closely related terms). Formally, we
define the weight associated with a given cluster using the following equation:
w(C) = (1? IDIST (C)) ? (S2(C) + FREQ(C) ? (1 + S2(C)))
S2(C) =
max(SIZE(C)
2
, k)
k
where IDIST (C) represents the average vector distance between all pairs of terms in cluster C,
FREQ(C) represents the total co-occurrence frequency of the terms in C with the original LM,
5
These dependency relation types come from the MaltParser.
6
We empirically set k to 3.
7
We estimate candidate imageability by combining the scores of the candidate?s most distributionally similar words for
which an imageability score is available in the MRC psycholinguistics database (Coltheart, 1981) using the ranked weighting
methodology described in Mohler et al. (2014).
8
Note that filtering by conceptual domain relatedness is only necessary when mapping the induced concepts to a predefined
set of source concepts.
1759
SIZE(C) represents the number of unique terms in C, and k is a tuning parameter meant to favor
large clusters.
9
Singleton clusters are discarded.
5.3 Assigning Domain Scores to Concept Candidates
We propose two methods for calculating domain scores for candidates ? one which attempts to compare
candidate vectors to a source domain directly, and and another which attempts to compare them to indi-
vidual facets of the domain. These two methods rely on representing sources [CentS], or facets [CentF],
as centroids which take the average of the vectors of each the lexemes assigned to that source (or facet).
Our three vector spaces ? DepVec, W2V, and LSA ? along with our two methods for mapping terms to
domains ? CentS and CentF ? correspond to six approaches to modeling a CM domain in some vector
space.
In each case, the result for a given candidate is a distribution over all source domain scores. This
distribution is then normalized by subtracting the mean score between the candidate vector and any of
the source concepts. Formally, we define the normalized distribution for concept candidate x as:
S(x,D
y
) = (1?DIST (x,D
y
))?
?
D
k
?D
(1?DIST (x,D
k
))
|D|
where D is defined as the set of all known source domains and DIST (x, d) is the cosine distance from
x to a CM domain d in one of our vector spaces.
5.3.1 Assigning Domain Scores to Clusters
Within a given cluster (found as described in Section 5.2), the individual concept domain scores can then
be combined to produce cluster-level domain scores. For a given cluster C
x
, the score associated with a
particular source domain D
y
is defined as follows:
S(C
x
, D
y
) =
N
?
i=1
S(C
xi
, D
y
)
?
i
where N represents the number of concepts in C
x
with a positive score for the domain D
y
, C
xi
is the
i-th highest score associated with any candidate in the cluster, and ? is a tuning parameter which bounds
the growth of the cluster-level score.
10
Any cluster with a maximum domain score that does not exceed
a threshold is discarded as being weakly related to any CM source domain.
5.3.2 Assigning Domain Scores to the Linguistic Metaphor
We then sum the cluster-level source domain scores, scaling each by its associated cluster quality weight
w(c) as computed in Section 5.2. By scaling cluster domain scores in this way, we ensure that the most
pure and discriminating clusters contribute the most to the overall LM domain scores. The final result
measuring the association between the given LM and the source domain D
y
is then defined as:
S(D
y
) =
?
C
x
?C
w(C
x
) ? S(C
x
, D
y
)
Applied across all known domains, we therefore produce a ranked and scored list of CM source do-
mains (i.e., a mapping) that are associated with the given linguistic metaphor and can be used to drive
more robust interpretation of the metaphor.
6 Evaluation
We evaluate two aspects of our end-to-end CM recognition system. First, we analyze the impact of our
choice of vector space. Specifically, we compare the use of our DepVec space to link concept candidates
9
In our experiments, k is set to 5.
10
We have used a value of ? = 2 which ensures that the result remains within the bounds [0.0,1.0].
1760
with source domains against two off-the-shelf vector space models ? the continuous skip-gram model
[W2V] (Mikolov et al., 2013)
11
and latent semantic analysis [LSA] (Landauer and Dumais, 1997). Both
alternative models were trained over the same corpus as in our DepVec space using a predefined number
of dimensions (300 for W2V; 400 for LSA). Second, we have experimented with two different metrics for
calculating the distance between a vector and a source concept ? the cosine distance to the source-level
centroid (CentS) and the cosine distance to the facet-level centroid (CentF).
Our evaluation dataset consists of a held out, unseen set of documents taken from a variety of news
articles, opinion pages, and blogs on the open web. These documents consist of 3 to 5 sentences each
and cover four of our focus languages.
12
They were then annotated by two native-proficiency speakers in
the following way. For each LM, they were instructed to choose the most closely related source concept
from our list of 51 provided. Any source concepts selected by at least one annotator were considered
correct. Since our CM recognition system produces a ranked list of source concepts, we report both the
accuracy associated with our top-ranked concept and the accuracy of the system when allowed to select
two.
Cluster Linking
English Spanish Russian Farsi
Vector Space Distance Acc@1 Acc@2 Acc@1 Acc@2 Acc@1 Acc@2 Acc@1 Acc@2
DepVec CentS 28.0% 44.1% 33.3% 43.4% 24.4% 32.6% 16.5% 27.5%
CentF 25.8% 40.9% 33.3% 49.4% 25.6% 34.9% 26.4% 40.7%
LSA CentS 34.4% 45.2% 31.0% 41.4% 27.9% 41.9% 22.0% 27.5%
CentF 38.7% 54.9% 27.6% 46.0% 29.1% 47.7% 31.9% 44.0%
W2V CentS 24.7% 36.6% 42.5% 55.2% 31.4% 43.0% 25.3% 34.1%
CentF 28.0% 44.1% 46.0% 58.6% 34.9% 48.8% 35.2% 48.4%
Table 5: The accuracy of our conceptual interpretation system. We experiment with three vector spaces
(LSA, W2V, and DepVec) and two source concept centroid representations ? source-level (CentS) and
facet-level (CentF).
These results indicate that the continuous skip-gram vector space [W2V] is well suited to the task of
cluster-level concept mapping, consistently and significantly outperforming both the LSA space and the
DepVec space in every language but English. We believe that this is a result of its probabilistic represen-
tation of local context which implicitly collects many of the same relations as the DepVec model while
incorporating the advantages associated with dimensionality reduction which has not been incorporated
into our DepVec model.
13
We further observe an unmistakable dominance of the facet-level centroid
representation over the source-level representation. Based on these results, we believe that we have suc-
cessfully demonstrated the contribution of our system?s vector-space clustering component which groups
concept candidates at a facet-level granularity.
7 Conclusion
In this paper, we have presented a novel approach to the problem of multilingual conceptual metaphor
recognition which combines facet-based concept induction with a distributional vector space represen-
tation of metaphor. We have experimentally demonstrated the advantage of our fine-grained concept
induction approach within a variety of vector space models, including our novel DepVec space. Taken
together, we hypothesize that a facet-level conceptual model represented in a relational context vec-
tor space will serve as a reliable foundation enabling high-quality metaphoric interpretation in future
metaphor research. Future work includes expanding the set of concept candidates through higher-order
dependency contexts, improved clustering techniques, and evaluating the induced clusters directly.
11
We make use of the implementation included as part of the gensim python package: http://radimrehurek.com/
gensim/
12
This dataset consists of the following counts of documents: English (92), Spanish (86), Russian (85), Farsi (90).
13
During our pilot experiments, we applied singular value decomposition (SVD) to the DepVec space without any significant
improvement to system performance.
1761
Acknowledgments
This research is supported by the Intelligence Advanced Research Projects Activity (IARPA) via De-
partment of Defense US Army Research Laboratory contract number W911NF-12-C-0025. The U.S.
Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstand-
ing any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those
of the authors and should not be interpreted as necessarily representing the official policies or endorse-
ments, either expressed or implied, of IARPA, DoD/ARL, or the U.S. Government.
References
Pierre Beust, St?ephane Ferrari, Vincent Perlerin, et al. 2003. NLP model and tools for detecting and interpreting
metaphors in domain-specific corpora. In Proceedings of the Corpus Linguistics 2003 conference, volume 16,
pages 114?123. Citeseer.
Danushka Bollegala and Ekaterina Shutova. 2013. Metaphor interpretation using paraphrases extracted from the
web. PloS one, 8(9):e74304.
D. Bracewell, M. Tomlinson, M. Mohler, and B. Rink. 2014. A tiered approach to the recognition of metaphor. In
Computational Linguistics and Intelligent Text Processing.
Siaw-Fong Chung, Kathleen Ahrens, and Chu-Ren Huang. 2005. Source domains as concept domains
in metaphorical expressions. International Journal of Computational Linguistics and Chinese Language
Processing, 10(4):553?570.
Max Coltheart. 1981. The MRC psycholinguistic database. The Quarterly Journal of Experimental Psychology,
33(4):497?505.
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 897?906. As-
sociation for Computational Linguistics.
J. Feldman and S. Narayanan. 2004. Embodied meaning in a neural theory of language. Brain and language,
89(2):385?392.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder, Newton Howard, Sergey Kanareykin, Moshe Koppel, Mark
Last, Yair Neuman, and Shlomo Argamon. 2013. Automatic identification of conceptual metaphors with limited
knowledge. In Twenty-Seventh AAAI Conference on Artificial Intelligence.
David Hope and Bill Keller. 2013. MaxMax: a graph-based soft clustering algorithm applied to word sense
induction. In Computational Linguistics and Intelligent Text Processing, pages 368?381. Springer.
Ioannis Korkontzelos and Suresh Manandhar. 2010. UoY: Graphs of unambiguous vertices for word sense in-
duction and disambiguation. In Proceedings of the 5th international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
G. Lakoff and M. Johnson. 1980. Metaphors we live by, volume 111. Chicago London.
G. Lakoff. 1993. The contemporary theory of metaphor. Metaphor and thought, 2:202?251.
T.K. Landauer and S.T. Dumais. 1997. A solution to Plato?s problem: The latent semantic analysis theory of acqui-
sition, induction, and representation of knowledge. Psychological Review; Psychological Review, 104(2):211.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Z.J. Mason. 2004. CorMet: A computational, corpus-based conventional metaphor extraction system.
Computational Linguistics, 30(1):23?44.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in
vector space. arXiv preprint arXiv:1301.3781.
Michael Mohler, Marc Tomlinson, and David Bracewell. 2013. Applying textual entailment to the interpretation
of metaphor. In IEEE Seventh International Conference on Semantic Computing (ICSC), pages 118?125. IEEE.
1762
Michael Mohler, Marc Tomlinson, David Bracewell, and Bryan Rink. 2014. Semi-supervised methods for expand-
ing psycholinguistics norms by integrating distributional similarity with the structure of WordNet. Language
Resources and Evaluation Conference 2014.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th
International Workshop on Parsing Technologies (IWPT. Citeseer.
Astrid Reining and Birte L?onneker-Rodman. 2007. Corpus-driven metaphor harvesting. In Proceedings of the
Workshop on Computational Approaches to Figurative Language, pages 5?12. Association for Computational
Linguistics.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised metaphor identification using hierarchical graph factorization
clustering. In Proceedings of NAACL-HLT, pages 978?988.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor identification using verb and noun clustering. In
Proceedings of the 23rd International Conference on Computational Linguistics, pages 1002?1010. Associa-
tion for Computational Linguistics.
Ekaterina Shutova. 2010. Automatic metaphor interpretation as a paraphrasing task. In Human Language
Technologies: The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, pages 1029?1037. Association for Computational Linguistics.
Tomek Strzalkowski, George Aaron Broadwell, Sarah Taylor, Laurie Feldman, Boris Yamrom, Samira Shaikh,
Ting Liu, Kit Cho, Umit Boz, Ignacio Cases, et al. 2013. Robust extraction of metaphors from novel data.
Meta4NLP 2013, page 67.
T. Veale and Y. Hao. 2008. A fluid knowledge representation for understanding and generating creative metaphors.
In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 945?952.
Association for Computational Linguistics.
Yorick Wilks, Lucian Galescu, James Allen, and Adam Dalton. 2013. Automatic metaphor detection using large-
scale lexical resources and conventional metaphor extraction. Meta4NLP 2013, page 36.
1763
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752?762,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning to Grade Short Answer Questions using Semantic Similarity
Measures and Dependency Graph Alignments
Michael Mohler
Dept. of Computer Science
University of North Texas
Denton, TX
mgm0038@unt.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, Ohio
bunescu@ohio.edu
Rada Mihalcea
Dept. of Computer Science
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
In this work we address the task of computer-
assisted assessment of short student answers.
We combine several graph alignment features
with lexical semantic similarity measures us-
ing machine learning techniques and show
that the student answers can be more accu-
rately graded than if the semantic measures
were used in isolation. We also present a first
attempt to align the dependency graphs of the
student and the instructor answers in order to
make use of a structural component in the au-
tomatic grading of student answers.
1 Introduction
One of the most important aspects of the learning
process is the assessment of the knowledge acquired
by the learner. In a typical classroom assessment
(e.g., an exam, assignment or quiz), an instructor or
a grader provides students with feedback on their
answers to questions related to the subject matter.
However, in certain scenarios, such as a number of
sites worldwide with limited teacher availability, on-
line learning environments, and individual or group
study sessions done outside of class, an instructor
may not be readily available. In these instances, stu-
dents still need some assessment of their knowledge
of the subject, and so, we must turn to computer-
assisted assessment (CAA).
While some forms of CAA do not require sophis-
ticated text understanding (e.g., multiple choice or
true/false questions can be easily graded by a system
if the correct solution is available), there are also stu-
dent answers made up of free text that may require
textual analysis. Research to date has concentrated
on two subtasks of CAA: grading essay responses,
which includes checking the style, grammaticality,
and coherence of the essay (Higgins et al, 2004),
and the assessment of short student answers (Lea-
cock and Chodorow, 2003; Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), which is the fo-
cus of this work.
An automatic short answer grading system is one
that automatically assigns a grade to an answer pro-
vided by a student, usually by comparing it to one
or more correct answers. Note that this is different
from the related tasks of paraphrase detection and
textual entailment, since a common requirement in
student answer grading is to provide a grade on a
certain scale rather than make a simple yes/no deci-
sion.
In this paper, we explore the possibility of im-
proving upon existing bag-of-words (BOW) ap-
proaches to short answer grading by utilizing ma-
chine learning techniques. Furthermore, in an at-
tempt to mirror the ability of humans to understand
structural (e.g. syntactic) differences between sen-
tences, we employ a rudimentary dependency-graph
alignment module, similar to those more commonly
used in the textual entailment community.
Specifically, we seek answers to the following
questions. First, to what extent can machine learn-
ing be leveraged to improve upon existing ap-
proaches to short answer grading. Second, does the
dependency parse structure of a text provide clues
that can be exploited to improve upon existing BOW
methodologies?
752
2 Related Work
Several state-of-the-art short answer grading sys-
tems (Sukkarieh et al, 2004; Mitchell et al, 2002)
require manually crafted patterns which, if matched,
indicate that a question has been answered correctly.
If an annotated corpus is available, these patterns
can be supplemented by learning additional pat-
terns semi-automatically. The Oxford-UCLES sys-
tem (Sukkarieh et al, 2004) bootstraps patterns by
starting with a set of keywords and synonyms and
searching through windows of a text for new pat-
terns. A later implementation of the Oxford-UCLES
system (Pulman and Sukkarieh, 2005) compares
several machine learning techniques, including in-
ductive logic programming, decision tree learning,
and Bayesian learning, to the earlier pattern match-
ing approach, with encouraging results.
C-Rater (Leacock and Chodorow, 2003) matches
the syntactical features of a student response (i.e.,
subject, object, and verb) to that of a set of correct
responses. This method specifically disregards the
BOW approach to take into account the difference
between ?dog bites man? and ?man bites dog? while
still trying to detect changes in voice (i.e., ?the man
was bitten by the dog?).
Another short answer grading system, AutoTutor
(Wiemer-Hastings et al, 1999), has been designed
as an immersive tutoring environment with a graph-
ical ?talking head? and speech recognition to im-
prove the overall experience for students. AutoTutor
eschews the pattern-based approach entirely in favor
of a BOW LSA approach (Landauer and Dumais,
1997). Later work on AutoTutor(Wiemer-Hastings
et al, 2005; Malatesta et al, 2002) seeks to expand
upon their BOW approach which becomes less use-
ful as causality (and thus word order) becomes more
important.
A text similarity approach was taken in (Mohler
and Mihalcea, 2009), where a grade is assigned
based on a measure of relatedness between the stu-
dent and the instructor answer. Several measures are
compared, including knowledge-based and corpus-
based measures, with the best results being obtained
with a corpus-based measure using Wikipedia com-
bined with a ?relevance feedback? approach that it-
eratively augments the instructor answer by inte-
grating the student answers that receive the highest
grades.
In the dependency-based classification compo-
nent of the Intelligent Tutoring System (Nielsen et
al., 2009), instructor answers are parsed, enhanced,
and manually converted into a set of content-bearing
dependency triples or facets. For each facet of the
instructor answer each student?s answer is labelled
to indicate whether it has addressed that facet and
whether or not the answer was contradictory. The
system uses a decision tree trained on part-of-speech
tags, dependency types, word count, and other fea-
tures to attempt to learn how best to classify an an-
swer/facet pair.
Closely related to the task of short answer grading
is the task of textual entailment (Dagan et al, 2005),
which targets the identification of a directional in-
ferential relation between texts. Given a pair of two
texts as input, typically referred to as text and hy-
pothesis, a textual entailment system automatically
finds if the hypothesis is entailed by the text.
In particular, the entailment-related works that are
most similar to our own are the graph matching tech-
niques proposed by Haghighi et al (2005) and Rus
et al (2007). Both input texts are converted into a
graph by using the dependency relations obtained
from a parser. Next, a matching score is calculated,
by combining separate vertex- and edge-matching
scores. The vertex matching functions use word-
level lexical and semantic features to determine the
quality of the match while the the edge matching
functions take into account the types of relations and
the difference in lengths between the aligned paths.
Following the same line of work in the textual en-
tailment world are (Raina et al, 2005), (MacCartney
et al, 2006), (de Marneffe et al, 2007), and (Cham-
bers et al, 2007), which experiment variously with
using diverse knowledge sources, using a perceptron
to learn alignment decisions, and exploiting natural
logic.
3 Answer Grading System
We use a set of syntax-aware graph alignment fea-
tures in a three-stage pipelined approach to short an-
swer grading, as outlined in Figure 1.
In the first stage (Section 3.1), the system is pro-
vided with the dependency graphs for each pair of
instructor (Ai) and student (As) answers. For each
753
Figure 1: Pipeline model for scoring short-answer pairs.
node in the instructor?s dependency graph, we com-
pute a similarity score for each node in the student?s
dependency graph based upon a set of lexical, se-
mantic, and syntactic features applied to both the
pair of nodes and their corresponding subgraphs.
The scoring function is trained on a small set of man-
ually aligned graphs using the averaged perceptron
algorithm.
In the second stage (Section 3.2), the node simi-
larity scores calculated in the previous stage are used
to weight the edges in a bipartite graph representing
the nodes in Ai on one side and the nodes in As on
the other. We then apply the Hungarian algorithm
to find both an optimal matching and the score asso-
ciated with such a matching. In this stage, we also
introduce question demoting in an attempt to reduce
the advantage of parroting back words provided in
the question.
In the final stage (Section 3.4), we produce an
overall grade based upon the alignment scores found
in the previous stage as well as the results of several
semantic BOW similarity measures (Section 3.3).
Using each of these as features, we use Support Vec-
tor Machines (SVM) to produce a combined real-
number grade. Finally, we build an Isotonic Regres-
sion (IR) model to transform our output scores onto
the original [0,5] scale for ease of comparison.
3.1 Node to Node Matching
Dependency graphs for both the student and in-
structor answers are generated using the Stanford
Dependency Parser (de Marneffe et al, 2006) in
collapse/propagate mode. The graphs are further
post-processed to propagate dependencies across the
?APPOS? (apposition) relation, to explicitly encode
negation, part-of-speech, and sentence ID within
each node, and to add an overarching ROOT node
governing the main verb or predicate of each sen-
tence of an answer. The final representation is a
list of (relation,governor,dependent) triples, where
governor and dependent are both tokens described
by the tuple (sentenceID:token:POS:wordPosition).
For example: (nsubj, 1:provide:VBZ:4, 1:pro-
gram:NN:3) indicates that the noun ?program? is a
subject in sentence 1 whose associated verb is ?pro-
vide.?
If we consider the dependency graphs output by
the Stanford parser as directed (minimally cyclic)
graphs,1 we can define for each node x a set of nodes
Nx that are reachable from x using a subset of the
relations (i.e., edge types)2. We variously define
?reachable? in four ways to create four subgraphs
defined for each node. These are as follows:
? N0x : All edge types may be followed
? N1x : All edge types except for subject types,
ADVCL, PURPCL, APPOS, PARATAXIS,
ABBREV, TMOD, and CONJ
? N2x : All edge types except for those in N1x plus
object/complement types, PREP, and RCMOD
? N3x : No edge types may be followed (This set
is the single starting node x)
Subgraph similarity (as opposed to simple node
similarity) is a means to escape the rigidity involved
in aligning parse trees while making use of as much
of the sentence structure as possible. Humans intu-
itively make use of modifiers, predicates, and subor-
dinate clauses in determining that two sentence en-
tities are similar. For instance, the entity-describing
phrase ?men who put out fires? matches well with
?firemen,? but the words ?men? and ?firemen? have
1The standard output of the Stanford Parser produces rooted
trees. However, the process of collapsing and propagating de-
pendences violates the tree structure which results in a tree
with a few cross-links between distinct branches.
2For more information on the relations used in this experi-
ment, consult the Stanford Typed Dependencies Manual at
http://nlp.stanford.edu/software/dependencies manual.pdf
754
less inherent similarity. It remains to be determined
how much of a node?s subgraph will positively en-
rich its semantics. In addition to the complete N0x
subgraph, we chose to include N1x and N2x as tight-
ening the scope of the subtree by first removing
more abstract relations, then sightly more concrete
relations.
We define a total of 68 features to be used to train
our machine learning system to compute node-node
(more specifically, subgraph-subgraph) matches. Of
these, 36 are based upon the semantic similarity
of four subgraphs defined by N [0..3]x . All eight
WordNet-based similarity measures listed in Sec-
tion 3.3 plus the LSA model are used to produce
these features. The remaining 32 features are lexico-
syntactic features3 defined only for N3x and are de-
scribed in more detail in Table 2.
We use ?(xi, xs) to denote the feature vector as-
sociated with a pair of nodes ?xi, xs?, where xi is
a node from the instructor answer Ai and xs is a
node from the student answer As. A matching score
can then be computed for any pair ?xi, xs? ? Ai ?
As through a linear scoring function f(xi, xs) =
wT?(xi, xs). In order to learn the parameter vec-
tor w, we use the averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002).
As training data, we randomly select a subset of
the student answers in such a way that our set was
roughly balanced between good scores, mediocre
scores, and poor scores. We then manually annotate
each node pair ?xi, xs? as matching, i.e. A(xi, xs) =
+1, or not matching, i.e. A(xi, xs) = ?1. Overall,
32 student answers in response to 21 questions with
a total of 7303 node pairs (656 matches, 6647 non-
matches) are manually annotated. The pseudocode
for the learning algorithm is shown in Table 1. Af-
ter training the perceptron, these 32 student answers
are removed from the dataset, not used as training
further along in the pipeline, and are not included in
the final results. After training for 50 epochs,4 the
matching score f(xi, xs) is calculated (and cached)
for each node-node pair across all student answers
for all assignments.
3Note that synonyms include negated antonyms (and vice
versa). Hypernymy and hyponymy are restricted to at most
two steps).
4This value was chosen arbitrarily and was not tuned in anyway
0. set w ? 0, w? 0, n? 0
1. repeat for T epochs:
2. foreach ?Ai;As?:
3. foreach ?xi, xs? ? Ai ?As:
4. if sgn(wT?(xi, xs)) 6= sgn(A(xi, xs)):
5. set w ? w+A(xi, xs)?(xi, xs)
6. set w ? w+w, n? n+ 1
7. return w/n.
Table 1: Perceptron Training for Node Matching.
3.2 Graph to Graph Alignment
Once a score has been computed for each node-node
pair across all student/instructor answer pairs, we at-
tempt to find an optimal alignment for the answer
pair. We begin with a bipartite graph where each
node in the student answer is represented by a node
on the left side of the bipartite graph and each node
in the instructor answer is represented by a node
on the right side. The score associated with each
edge is the score computed for each node-node pair
in the previous stage. The bipartite graph is then
augmented by adding dummy nodes to both sides
which are allowed to match any node with a score of
zero. An optimal alignment between the two graphs
is then computed efficiently using the Hungarian al-
gorithm. Note that this results in an optimal match-
ing, not a mapping, so that an individual node is as-
sociated with at most one node in the other answer.
At this stage we also compute several alignment-
based scores by applying various transformations to
the input graphs, the node matching function, and
the alignment score itself.
The first and simplest transformation involves the
normalization of the alignment score. While there
are several possible ways to normalize a matching
such that longer answers do not unjustly receive
higher scores, we opted to simply divide the total
alignment score by the number of nodes in the in-
structor answer.
The second transformation scales the node match-
ing score by multiplying it with the idf 5 of the in-
structor answer node, i.e., replace f(xi, xs) with
idf(xi) ? f(xi, xs).
The third transformation relies upon a certain
real-world intuition associated with grading student
5Inverse document frequency, as computed from the British Na-
tional Corpus (BNC)
755
Name Type # features Description
RootMatch binary 5 Is a ROOT node matched to: ROOT, N, V, JJ, or Other
Lexical binary 3 Exact match, Stemmed match, close Levenshtein match
POSMatch binary 2 Exact POS match, Coarse POS match
POSPairs binary 8 Specific X-Y POS matches found
Ontological binary 4 WordNet relationships: synonymy, antonymy, hypernymy, hyponymy
RoleBased binary 3 Has as a child - subject, object, verb
VerbsSubject binary 3 Both are verbs and neither, one, or both have a subject child
VerbsObject binary 3 Both are verbs and neither, one, or both have an object child
Semantic real 36 Nine semantic measures across four subgraphs each
Bias constant 1 A value of 1 for all vectors
Total 68
Table 2: Subtree matching features used to train the perceptron
answers ? repeating words in the question is easy
and is not necessarily an indication of student under-
standing. With this in mind, we remove any words
in the question from both the instructor answer and
the student answer.
In all, the application of the three transforma-
tions leads to eight different transform combina-
tions, and therefore eight different alignment scores.
For a given answer pair (Ai, As), we assemble the
eight graph alignment scores into a feature vector
?G(Ai, As).
3.3 Lexical Semantic Similarity
Haghighi et al (2005), working on the entailment
detection problem, point out that finding a good
alignment is not sufficient to determine that the
aligned texts are in fact entailing. For instance, two
identical sentences in which an adjective from one is
replaced by its antonym will have very similar struc-
tures (which indicates a good alignment). However,
the sentences will have opposite meanings. Further
information is necessary to arrive at an appropriate
score.
In order to address this, we combine the graph
alignment scores, which encode syntactic knowl-
edge, with the scores obtained from semantic sim-
ilarity measures.
Following Mihalcea et al (2006) and Mohler
and Mihalcea (2009), we use eight knowledge-
based measures of semantic similarity: shortest path
[PATH], Leacock & Chodorow (1998) [LCH], Lesk
(1986), Wu & Palmer(1994) [WUP], Resnik (1995)
[RES], Lin (1998), Jiang & Conrath (1997) [JCN],
Hirst & St. Onge (1998) [HSO], and two corpus-
based measures: Latent Semantic Analysis [LSA]
(Landauer and Dumais, 1997) and Explicit Seman-
tic Analysis [ESA] (Gabrilovich and Markovitch,
2007).
Briefly, for the knowledge-based measures, we
use the maximum semantic similarity ? for each
open-class word ? that can be obtained by pairing
it up with individual open-class words in the sec-
ond input text. We base our implementation on
the WordNet::Similarity package provided by Ped-
ersen et al (2004). For the corpus-based measures,
we create a vector for each answer by summing
the vectors associated with each word in the an-
swer ? ignoring stopwords. We produce a score in
the range [0..1] based upon the cosine similarity be-
tween the student and instructor answer vectors. The
LSA model used in these experiments was built by
training Infomap6 on a subset of Wikipedia articles
that contain one or more common computer science
terms. Since ESA uses Wikipedia article associa-
tions as vector features, it was trained using a full
Wikipedia dump.
3.4 Answer Ranking and Grading
We combine the alignment scores ?G(Ai, As) with
the scores ?B(Ai, As) from the lexical seman-
tic similarity measures into a single feature vector
?(Ai, As) = [?G(Ai, As)|?B(Ai, As)]. The fea-
ture vector ?G(Ai, As) contains the eight alignment
scores found by applying the three transformations
in the graph alignment stage. The feature vector
?B(Ai, As) consists of eleven semantic features ?
the eight knowledge-based features plus LSA, ESA
and a vector consisting only of tf*idf weights ? both
with and without question demoting. Thus, the en-
tire feature vector ?(Ai, As) contains a total of 30
features.
6http://Infomap-nlp.sourceforge.net/
756
An input pair (Ai, As) is then associated with a
grade g(Ai, As) = uT?(Ai, As) computed as a lin-
ear combination of features. The weight vector u is
trained to optimize performance in two scenarios:
Regression: An SVM model for regression (SVR)
is trained using as target function the grades as-
signed by the instructors. We use the libSVM 7 im-
plementation of SVR, with tuned parameters.
Ranking: An SVM model for ranking (SVMRank)
is trained using as ranking pairs all pairs of stu-
dent answers (As, At) such that grade(Ai, As) >
grade(Ai, At), where Ai is the corresponding in-
structor answer. We use the SVMLight 8 implemen-
tation of SVMRank with tuned parameters.
In both cases, the parameters are tuned using a
grid-search. At each grid point, the training data is
partitioned into 5 folds which are used to train a tem-
porary SVM model with the given parameters. The
regression passage selects the grid point with the
minimal mean square error (MSE), and the SVM-
Rank package tries to minimize the number of dis-
cordant pairs. The parameters found are then used to
score the test set ? a set not used in the grid training.
3.5 Isotonic Regression
Since the end result of any grading system is to give
a student feedback on their answers, we need to en-
sure that the system?s final score has some mean-
ing. With this in mind, we use isotonic regression
(Zadrozny and Elkan, 2002) to convert the system
scores onto the same [0..5] scale used by the an-
notators. This has the added benefit of making the
system output more directly related to the annotated
grade, which makes it possible to report root mean
square error in addition to correlation. We train the
isotonic regression model on each type of system
output (i.e., alignment scores, SVM output, BOW
scores).
4 Data Set
To evaluate our method for short answer grading,
we created a data set of questions from introductory
computer science assignments with answers pro-
vided by a class of undergraduate students. The as-
signments were administered as part of a Data Struc-
7http://www.csie.ntu.edu.tw/?cjlin/libsvm/
8http://svmlight.joachims.org/
tures course at the University of North Texas. For
each assignment, the student answers were collected
via an online learning environment.
The students submitted answers to 80 questions
spread across ten assignments and two examina-
tions.9 Table 3 shows two question-answer pairs
with three sample student answers each. Thirty-one
students were enrolled in the class and submitted an-
swers to these assignments. The data set we work
with consists of a total of 2273 student answers. This
is less than the expected 31 ? 80 = 2480 as some
students did not submit answers for a few assign-
ments. In addition, the student answers used to train
the perceptron are removed from the pipeline after
the perceptron training stage.
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both human
judges were graduate students in the computer sci-
ence department; one (grader1) was the teaching as-
sistant assigned to the Data Structures class, while
the other (grader2) is one of the authors of this pa-
per. We treat the average grade of the two annotators
as the gold standard against which we compare our
system output.
Difference Examples % of examples
0 1294 57.7%
1 514 22.9%
2 231 10.3%
3 123 5.5%
4 70 3.1%
5 9 0.4%
Table 4: Annotator Analysis
The annotators were given no explicit instructions
on how to assign grades other than the [0..5] scale.
Both annotators gave the same grade 57.7% of the
time and gave a grade only 1 point apart 22.9% of
the time. The full breakdown can be seen in Table
4. In addition, an analysis of the grading patterns
indicate that the two graders operated off of differ-
ent grading policies where one grader (grader1) was
more generous than the other. In fact, when the two
differed, grader1 gave the higher grade 76.6% of the
time. The average grade given by grader1 is 4.43,
9Note that this is an expanded version of the dataset used by
Mohler and Mihalcea (2009)
757
Sample questions, correct answers, and student answers Grades
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior of portions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
more manageable problems. 4, 4
Table 3: A sample question with short answers provided by students and the grades assigned by the two human judges
while the average grade given by grader2 is 3.94.
The dataset is biased towards correct answers. We
believe all of these issues correctly mirror real-world
issues associated with the task of grading.
5 Results
We independently test two components of our over-
all grading system: the node alignment detection
scores found by training the perceptron, and the
overall grades produced in the final stage. For the
alignment detection, we report the precision, recall,
and F-measure associated with correctly detecting
matches. For the grading stage, we report a single
Pearson?s correlation coefficient tracking the anno-
tator grades (average of the two annotators) and the
output score of each system. In addition, we re-
port the Root Mean Square Error (RMSE) for the
full dataset as well as the median RMSE across each
individual question. This is to give an indication of
the performance of the system for grading a single
question in isolation.10
5.1 Perceptron Alignment
For the purpose of this experiment, the scores as-
sociated with a given node-node matching are con-
verted into a simple yes/no matching decision where
positive scores are considered a match and negative
10We initially intended to report an aggregate of question-level
Pearson correlation results, but discovered that the dataset
contained one question for which each student received full
points ? leaving the correlation undefined. We believe that
this casts some doubt on the applicability of Pearson?s (or
Spearman?s) correlation coefficient for the short answer grad-
ing task. We have retained its use here alongside RMSE for
ease of comparison.
scores a non-match. The threshold weight learned
from the bias feature strongly influences the point
at which real scores change from non-matches to
matches, and given the threshold weight learned by
the algorithm, we find an F-measure of 0.72, with
precision(P) = 0.85 and recall(R) = 0.62. However,
as the perceptron is designed to minimize error rate,
this may not reflect an optimal objective when seek-
ing to detect matches. By manually varying the
threshold, we find a maximum F-measure of 0.76,
with P=0.79 and R=0.74. Figure 2 shows the full
precision-recall curve with the F-measure overlaid.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Sc
or
e
Recall
Precision
F-Measure
Threshold
Figure 2: Precision, recall, and F-measure on node-level
match detection
5.2 Question Demoting
One surprise while building this system was the con-
sistency with which the novel technique of question
demoting improved scores for the BOW similarity
measures. With this relatively minor change the av-
erage correlation between the BOW methods? sim-
758
ilarity scores and the student grades improved by
up to 0.046 with an average improvement of 0.019
across all eleven semantic features. Table 5 shows
the results of applying question demoting to our
semantic features. When comparing scores using
RMSE, the difference is less consistent, yielding an
average improvement of 0.002. However, for one
measure (tf*idf), the improvement is 0.063 which
brings its RMSE score close to the lowest of all
BOW metrics. The reasons for this are not entirely
clear. As a baseline, we include here the results of
assigning the average grade (as determined on the
training data) for each question. The average grade
was chosen as it minimizes the RMSE on the train-
ing data.
? w/ QD RMSE w/ QD Med. RMSE w/ QD
Lesk 0.450 0.462 1.034 1.050 0.930 0.919
JCN 0.443 0.461 1.022 1.026 0.954 0.923
HSO 0.441 0.456 1.036 1.034 0.966 0.935
PATH 0.436 0.457 1.029 1.030 0.940 0.918
RES 0.409 0.431 1.045 1.035 0.996 0.941
Lin 0.382 0.407 1.069 1.056 0.981 0.949
LCH 0.367 0.387 1.068 1.069 0.986 0.958
WUP 0.325 0.343 1.090 1.086 1.027 0.977
ESA 0.395 0.401 1.031 1.086 0.990 0.955
LSA 0.328 0.335 1.065 1.061 0.951 1.000
tf*idf 0.281 0.327 1.085 1.022 0.991 0.918
Avg.grade 1.097 1.097 0.973 0.973
Table 5: BOW Features with Question Demoting (QD).
Pearson?s correlation, root mean square error (RMSE),
and median RMSE for all individual questions.
5.3 Alignment Score Grading
Before applying any machine learning techniques,
we first test the quality of the eight graph alignment
features ?G(Ai, As) independently. Results indicate
that the basic alignment score performs comparably
to most BOW approaches. The introduction of idf
weighting seems to degrade performance somewhat,
while introducing question demoting causes the cor-
relation with the grader to increase while increasing
RMSE somewhat. The four normalized components
of ?G(Ai, As) are reported in Table 6.
5.4 SVM Score Grading
The SVM components of the system are run on the
full dataset using a 12-fold cross validation. Each of
the 10 assignments and 2 examinations (for a total
of 12 folds) is scored independently with ten of the
remaining eleven used to train the machine learn-
Standard w/ IDF w/ QD w/ QD+IDF
Pearson?s ? 0.411 0.277 0.428 0.291
RMSE 1.018 1.078 1.046 1.076
Median RMSE 0.910 0.970 0.919 0.992
Table 6: Alignment Feature/Grade Correlations using
Pearson?s ?. Results are also reported when inverse doc-
ument frequency weighting (IDF) and question demoting
(QD) are used.
ing system. For each fold, one additional fold is
held out for later use in the development of an iso-
tonic regression model (see Figure 3). The param-
eters (for cost C and tube width ) were found us-
ing a grid search. At each point on the grid, the data
from the ten training folds was partitioned into 5 sets
which were scored according to the current param-
eters. SVMRank and SVR sought to minimize the
number of discordant pairs and the mean absolute
error, respectively.
Both SVM models are trained using a linear ker-
nel.11 Results from both the SVR and the SVMRank
implementations are reported in Table 7 along with
a selection of other measures. Note that the RMSE
score is computed after performing isotonic regres-
sion on the SVMRank results, but that it was unnec-
essary to perform an isotonic regression on the SVR
results as the system was trained to produce a score
on the correct scale.
We report the results of running the systems on
three subsets of features ?(Ai, As): BOW features
?B(Ai, As) only, alignment features ?G(Ai, As)
only, or the full feature vector (labeled ?Hybrid?).
Finally, three subsets of the alignment features are
used: only unnormalized features, only normalized
features, or the full alignment feature set.
B CA ? Ten Folds
B CA ? Ten Folds
B CA ? Ten FoldsIR Model
SVM Model
Features
Figure 3: Dependencies of the SVM/IR training stages.
11We also ran the SVR system using quadratic and radial-basis
function (RBF) kernels, but the results did not show signifi-
cant improvement over the simpler linear kernel.
759
Unnormalized Normalized Both
IAA Avg. grade tf*idf Lesk BOW Align Hybrid Align Hybrid Align Hybrid
SVMRank
Pearson?s ? 0.586 0.327 0.450 0.480 0.266 0.451 0.447 0.518 0.424 0.493
RMSE 0.659 1.097 1.022 1.050 1.042 1.093 1.038 1.015 0.998 1.029 1.021
Median RMSE 0.605 0.973 0.918 0.919 0.943 0.974 0.903 0.865 0.873 0.904 0.901
SVR
Pearson?s ? 0.586 0.327 0.450 0.431 0.167 0.437 0.433 0.459 0.434 0.464
RMSE 0.659 1.097 1.022 1.050 0.999 1.133 0.995 1.001 0.982 1.003 0.978
Median RMSE 0.605 0.973 0.918 0.919 0.910 0.987 0.893 0.894 0.877 0.886 0.862
Table 7: The results of the SVM models trained on the full suite of BOW measures, the alignment scores, and the
hybrid model. The terms ?normalized?, ?unnormalized?, and ?both? indicate which subset of the 8 alignment features
were used to train the SVM model. For ease of comparison, we include in both sections the scores for the IAA, the
?Average grade? baseline, and two of the top performing BOW metrics ? both with question demoting.
6 Discussion and Conclusions
There are three things that we can learn from these
experiments. First, we can see from the results that
several systems appear better when evaluating on a
correlation measure like Pearson?s ?, while others
appear better when analyzing error rate. The SVM-
Rank system seemed to outperform the SVR sys-
tem when measuring correlation, however the SVR
system clearly had a minimal RMSE. This is likely
due to the different objective function in the corre-
sponding optimization formulations: while the rank-
ing model attempts to ensure a correct ordering be-
tween the grades, the regression model seeks to min-
imize an error objective that is closer to the RMSE.
It is difficult to claim that either system is superior.
Likewise, perhaps the most unexpected result of
this work is the differing analyses of the simple
tf*idf measure ? originally included only as a base-
line. Evaluating with a correlative measure yields
predictably poor results, but evaluating the error rate
indicates that it is comparable to (or better than) the
more intelligent BOW metrics. One explanation for
this result is that the skewed nature of this ?natural?
dataset favors systems that tend towards scores in
the 4 to 4.5 range. In fact, 46% of the scores output
by the tf*idf measure (after IR) were within the 4 to
4.5 range and only 6% were below 3.5. Testing on
a more balanced dataset, this tendency to fit to the
average would be less advantageous.
Second, the supervised learning techniques are
clearly able to leverage multiple BOW measures to
yield improvements over individual BOW metrics.
The correlation for the BOW-only SVM model for
SVMRank improved upon the best BOW feature
from .462 to .480. Likewise, using the BOW-only
SVM model for SVR reduces the RMSE by .022
overall compared to the best BOW feature.
Third, the rudimentary alignment features we
have introduced here are not sufficient to act as a
standalone grading system. However, even with a
very primitive attempt at alignment detection, we
show that it is possible to improve upon grade learn-
ing systems that only consider BOW features. The
correlations associated with the hybrid systems (esp.
those using normalized alignment data) frequently
show an improvement over the BOW-only SVM sys-
tems. This is true for both SVM systems when con-
sidering either evaluation metric.
Future work will concentrate on improving the
quality of the answer alignments by training a model
to directly output graph-to-graph alignments. This
learning approach will allow the use of more com-
plex alignment features, for example features that
are defined on pairs of aligned edges or on larger
subtrees in the two input graphs. Furthermore, given
an alignment, we can define several phrase-level
grammatical features such as negation, modality,
tense, person, number, or gender, which make bet-
ter use of the alignment itself.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation CAREER award #0747340. Any
opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
760
References
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 165?170. Association for
Computational Linguistics.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
M.C. de Marneffe, T. Grenager, B. MacCartney, D. Cer,
D. Ramage, C. Kiddon, and C.D. Manning. 2007.
Aligning semantic graphs for textual inference and
machine reading. In Proceedings of the AAAI Spring
Symposium. Citeseer.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6?12.
A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Ro-
bust textual inference via graph matching. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 387?394. Association for Computa-
tional Linguistics.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student
essays. In Proceedings of the annual meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as repre-
sentations of contexts for the detection and correction
of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of the International Conference on Research in
Computational Linguistics, Taiwan.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389?405.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C.D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page 48. As-
sociation for Computational Linguistics.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. Proceedings of the 6th International
Computer Assisted Assessment (CAA) Conference.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
R.D. Nielsen, W. Ward, and J.H. Martin. 2009. Recog-
nizing entailment in intelligent tutoring systems. Nat-
ural Language Engineering, 15(04):479?501.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic Short
Answer Marking. ACL WS Bldg Ed Apps using NLP.
R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,
K. Toutanova, B. MacCartney, M.C. de Marneffe, C.D.
Manning, and A.Y. Ng. 2005. Robust textual infer-
ence using diverse knowledge sources. Recognizing
Textual Entailment, page 57.
761
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal, Canada.
V. Rus, A. Graesser, and K. Desai. 2007. Lexico-
syntactic subsumption for textual entailment. Recent
Advances in Natural Language Processing IV: Se-
lected Papers from RANLP 2005, page 187.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-
Marking 2: An Update on the UCLES-Oxford Univer-
sity research into using Computational Linguistics to
Score Short, Free Text Responses. International Asso-
ciation of Educational Assessment, Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tu-
tor?s comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535?542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005.
Initial results and mixed directions for research meth-
ods tutor. In AIED2005 - Supplementary Proceedings
of the 12th International Conference on Artificial In-
telligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proceedings of the 32nd Annual Meeting
of the Association for Computational Linguistics, Las
Cruces, New Mexico.
B. Zadrozny and C. Elkan. 2002. Transforming classifier
scores into accurate multiclass probability estimates.
Edmonton, Alberta.
762
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635?642,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNT: A Supervised Synergistic Approach
to Semantic Text Similarity
Carmen Banea, Samer Hassan, Michael Mohler, Rada Mihalcea
University of North Texas
Denton, TX, USA
{CarmenBanea,SamerHassan,MichaelMohler}@my.unt.edu, rada@cs.unt.edu
Abstract
This paper presents the systems that we par-
ticipated with in the Semantic Text Similar-
ity task at SEMEVAL 2012. Based on prior
research in semantic similarity and related-
ness, we combine various methods in a ma-
chine learning framework. The three varia-
tions submitted during the task evaluation pe-
riod ranked number 5, 9 and 14 among the 89
participating systems. Our evaluations show
that corpus-based methods display a more ro-
bust behavior on the training data, yet com-
bining a variety of methods allows a learning
algorithm to achieve a superior decision than
that achievable by any of the individual parts.
1 Introduction
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vector-
space model used in information retrieval, where the
document most relevant to an input query is deter-
mined by ranking documents in a collection in re-
versed order of their similarity to the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and more recently for
extractive summarization (Salton et al, 1997), and
methods for automatic evaluation of machine trans-
lation (Papineni et al, 2002) or text summarization
(Lin and Hovy, 2003). Measures of text similarity
were also found useful for the evaluation of text co-
herence (Lapata and Barzilay, 2005).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stop-word removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is
an obvious similarity between the text segments I
own a dog and I have an animal, but most of the
current text similarity metrics will fail in identifying
any kind of connection between these texts.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus measures
such as Latent Semantic Analysis (Landauer et al,
1997), Explicit Semantic Analysis (Gabrilovich and
Markovitch, 2007), or Salient Semantic Analysis
(Hassan and Mihalcea, 2011).
In this paper, we describe the system with which
635
we participated in the SEMEVAL 2012 task on se-
mantic text similarity (Agirre et al, 2012). The sys-
tem builds upon our earlier work on corpus-based
and knowledge-based methods of text semantic sim-
ilarity (Mihalcea et al, 2006; Hassan and Mihal-
cea, 2011; Mohler et al, 2011), and combines all
these previous methods into a meta-system by us-
ing machine learning. The framework provided by
the task organizers also enabled us to perform an in-
depth analysis of the various components used in our
system, and draw conclusions concerning the role
played by the different resources, features, and al-
gorithms in building a state-of-the-art semantic text
similarity system.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
The system we proposed for the SEMEVAL 2012
Semantic Textual Similarity task builds upon both
knowledge- and corpus-based methods previously
described in (Mihalcea et al, 2006; Hassan and Mi-
halcea, 2011; Mohler et al, 2011). The predictions
of these independent systems, paired with additional
salient features, are leveraged by a meta-system that
employs machine learning. In this section, we will
elaborate further on the resources we use, our fea-
tures, and the components of our machine learning
system. We will start by describing the task setup.
3.1 Task Setup
The training data released by the task organiz-
ers consists of three datasets showcasing two sen-
tences per line and a manually assigned similarity
score ranging from 0 (no relation) to 5 (semanti-
cally equivalent). The datasets1 provided are taken
from the Microsoft Research Paraphrase Corpus
(MSRpar), the Microsoft Research Video Descrip-
tion Corpus (MSRvid), and the WMT2008 devel-
opment dataset (Europarl section)(SMTeuroparl);
they each consist of about 750 sentence pairs with
the class distribution varying with each dataset. The
testing data contains additional sentences from the
same collections as the training data as well as
from two additional unknown sets (OnWN and
SMTnews); they range from 399 to 750 sentence
pairs. The reader may refer to (Agirre et al, 2012)
for additional information regarding this task.
3.2 Resources
Wikipedia2 is a free on-line encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
web page, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential mis-
takes are quickly corrected within the collaborative
environment) of this on-line resource. The basic en-
try in Wikipedia is an article which describes an en-
tity or an event, and which, in addition to untagged
1http://www.cs.york.ac.uk/semeval-2012/
task6/data/uploads/datasets/train-readme.
txt
2www.wikipedia.org
636
content, also consists of hyperlinked text to other
pages within or outside of Wikipedia. These hyper-
links are meant to guide the reader to pages that pro-
vide additional information / clarifications, so that
a better understanding of the primary concept can
be achieved. The structure of Wikipedia in terms of
pages and hyperlinks is exploited directly by seman-
tic similarity methods such as ESA (Gabrilovich and
Markovitch, 2007), or SSA (Hassan and Mihalcea,
2011).
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
between basic units of meaning, or synsets. A synset
groups together senses of different words that share
a very similar meaning, which act in a particu-
lar context as synonyms. Each synset is accompa-
nied by a gloss or definition, and one or two ex-
amples illustrating usage in the given context. Un-
like a traditional thesaurus, the structure of Word-
Net is able to encode additional relationships be-
side synonymy, such as antonymy, hypernymy, hy-
ponymy, meronymy, entailment, etc., which vari-
ous knowledge-based methods use to derive seman-
tic similarity.
3.3 Features
Our meta-system uses several features, which can
be grouped into knowledge-based, corpus-based,
and bipartite graph matching, as described below.
The abbreviations appearing between parentheses
by each method allow for easy cross-referencing
with the evaluations provided in Table 1.
3.3.1 Knowledge-based Semantic Similarity
Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for
each open-class word in one of the input texts, we
compute the maximum semantic similarity (using
the WordNet::Similarity package (Pedersen et al,
2004)) that can be obtained by pairing it with any
open-class word in the other input text. All the
word-to-word similarity scores obtained in this way
are summed and normalized to the length of the two
input texts. We provide below a short description
for each of the similarity metrics employed by this
system3.
The shortest path (Path) similarity is determined
as:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting (including
the end nodes).
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) similarity is determined
as:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
3We point out that the similarity metric proposed by Hirst &
St. Onge was not considered due to the time constraints associ-
ated with the STS task.
637
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
Each of the measures listed above is used as a fea-
ture by our meta-system.
3.3.2 Corpus-based Semantic Similarity
Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
stand out as different, since they rely on a concept-
space representation. In these methods, the semantic
profile of a word is expressed in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch. In the
experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia
download from October 2008, with approximately
6 million articles, and more than 9.5 million hyper-
links.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. Since
encyclopedic knowledge is typically organized into
concepts (or topics), each concept is further de-
scribed using definitions and examples. ESA relies
on the distribution of words inside the encyclopedic
descriptions. It builds semantic representations for
a given word using a word-document association,
where the document represents a Wikipedia article
(concept). ESA is in effect a Vector Space Model
(VSM) built using Wikipedia corpus, where vectors
represents word-articles association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction and interpretation of words as ESA,
yet it uses salient concepts gathered from encyclo-
pedic knowledge, where a ?concept? represents an
unambiguous word or phrase with a concrete mean-
ing, and which affords an encyclopedic definition.
Saliency in this case is determined based on the
word being hyperlinked (either trough manual or au-
tomatic annotations) in context, implying that they
are highly relevant to the given text. SSA is an ex-
ample of Generalized Vector Space Model (GVSM),
where vectors represent word-concepts associations.
In order to determine the similarity of two text
fragments , we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail below.
Both variations were paired with the LSA, ESA,
and SSA systems resulting in six similarity scores
that were used as features by our meta-system,
namely LSAcos, LSAalign, ESAcos, ESAalign,
SSAcos, and SSAalign.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a + b
(8)
where ? is the number of shared terms between the
text fragments and ?i is the similarity score for the
ith pairing.
3.3.3 Bipartite Graph Matching
In an attempt to move beyond the bag-of-words
paradigm described thus far, we attempt to compute
638
a set of dependency graph alignment scores based
on previous work in automatic short-answer grading
(Mohler et al, 2011). This score, computed in two
stages, is used as a feature by our meta-system.
In the first stage, the system is provided with the
dependency graphs for each pair of sentences4. For
each node in one dependency graph, we compute a
similarity score for each node in the other depen-
dency graph based upon a set of lexical, semantic,
and syntactic features applied to both the pair of
nodes and their corresponding subgraphs (i.e. the set
of nodes reachable from a given node by following
directional governor-to-dependant links). The scor-
ing function is trained on a small set of manually
aligned graphs using the averaged perceptron algo-
rithm.
We define a total of 64 features5 to be used to train
a machine learning system to compute subgraph-
subgraph similarity. Of these, 32 are based upon the
bag-of-words semantic similarity of the subgraphs
using the metrics described in Section 3.3.1 as well
as a Wikipedia-trained LSA model. The remaining
32 features are lexico-syntactic features associated
with the parent nodes of the subgraphs and are de-
scribed in more detail in our earlier paper.
We then calculate weights associated with these
features using an averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002) trained on a set of 32 manually annotated
instructor/student answer pairs selected from the
short-answer grading corpus (MM2011). These
pairs contain 7303 node pairs (656 matches, 6647
non-matches). Once the weights are calculated, a
similarity score for each pair of nodes can be com-
puted by taking the dot product of the feature vector
with the weights.
In the second stage, the node similarity scores cal-
culated in the previous step are used to find an op-
timal alignment for the pair of dependency graphs.
We begin with a bipartite graph where each node
in one graph is represented by a node on the left
side of the bipartite graph and each node in the other
4We here use the output of the Stanford Dependency Parser
in collapse/propagate mode with some modifications as de-
scribed in our earlier work.
5With the exception of the four features based upon the Hirst
& St.Onge similarity metric, these are equivalent to the features
used in previous work.
graph is represented by a node on the right side. The
weight associated with each edge is the score com-
puted for each node-node pair in the previous stage.
The bipartite graph is then augmented by adding
dummy nodes to both sides which are allowed to
match any node with a score of zero. An optimal
alignment between the two graphs is then computed
efficiently using the Hungarian algorithm. Note that
this results in an optimal matching, not a mapping,
so that an individual node is associated with at most
one node in the other answer. After finding the opti-
mal match, we produce four alignment-based scores
by optionally normalizing by the number of nodes
and/or weighting the node-alignments according to
the idf scores of the words.6 This results in four
alignment scores listed as graphnone, graphnorm,
graphidf , graphidfnorm.
3.3.4 Baselines
As a baseline, we also utilize several lexical bag-
of-words approaches where each sentence is repre-
sented by a vector of tokens and the similarity of the
two sentences can be computed by finding the co-
sine of the angle between their representative vectors
using term frequency (tf ) or term frequency mul-
tiplied by inverse document frequency (tf.idf )6, or
by using simple overlap between the vectors? dimen-
sions (overlap).
3.4 Machine Learning
3.4.1 Algorithms
All the systems described above are used to gen-
erate a score for each training and test sample (see
Section 3.1). These scores are then aggregated per
sample, and used in a supervised learning frame-
work. We decided to use a regression model, instead
of classification, since the requirements for the task
specify that we should provide a score in the range of
0 to 5. We could have used classification paired with
bucketed ranges, yet classification does not take into
consideration the underlying ordinality of the scores
(i.e. a score of 4.5 is closer to either 4 or 5, but
farther away from 0), which is a noticeable handi-
cap in this scenario. We tried both linear and sup-
6The document frequency scores were taken from the British
National Corpus (BNC).
639
port vector regression7 by performing 10 fold cross-
validation on the train data, yet the latter algorithm
consistently performs better, no matter what kernel
was chosen. Thus we decided to use support vec-
tor regression (Smola and Schoelkopf, 1998) with a
Pearson VII function-based kernel.
Due to its different learning methodology, and
since it is suited for predicting continuous classes,
our second system uses the M5P decision tree al-
gorithm (Quinlan, 1992; Wang and Witten, 1997),
which outperforms support vector regression on the
10 fold cross-validation performed on the SMTeu-
roparl train set, while providing competitive results
on the other train sets (within .01 Pearson correla-
tion).
3.4.2 Setup
We submitted three system variations, namely
IndividualRegression, IndividualDecTree,
and CombinedRegression. The first word de-
scribes the training data; for individual, for the
known test sets we trained on the corresponding
train sets, while for the unknown test sets we trained
on all the train sets combined; for combined,
for each test set we trained on all the train sets
combined. The second word refers to the learning
methodology, where Regression stands for support
vector regression, and DecTree stands for M5P
decision tree.
4 Results and Discussion
We include in Table 1 the Pearson correlations ob-
tained by comparing the predictions of each fea-
ture to the gold standard for the three train datasets.
We notice that the corpus based metrics display a
consistent performance across the three train sets,
when compared to the other methods, including
knowledge-based. Furthermore, the best alignment
strategy (align) for corpus based models outper-
forms similarity scores based on traditional cosine
similarity. It is interesting to note that simple base-
lines such as tf , tf.idf and overlap offer signifi-
cant correlations with all the train sets without ac-
cess to additional knowledge inferred by knowledge
or corpus-based methods. In the case of the bipar-
7Implementations provided through the Weka framework
(Hall et al, 2009).
System MSRpar MSRvid SMTeuroparl
Path 0.49 0.62 0.50
LCH 0.48 0.49 0.45
Lesk 0.48 0.59 0.50
WUP 0.46 0.38 0.42
RES 0.47 0.55 0.48
Lin 0.49 0.54 0.48
JCN 0.49 0.63 0.51
LSAalign 0.44 0.57 0.61
LSAcos 0.37 0.74 0.56
ESAalign 0.52 0.70 0.62
ESAcos 0.30 0.71 0.53
SSAalign 0.46 0.61 0.65
SSAcos 0.22 0.63 0.39
graphnone 0.42 0.50 0.21
graphnorm 0.48 0.43 0.59
graphidf 0.16 0.67 0.16
graphidfnorm 0.08 0.60 0.19
tf.idf 0.45 0.63 0.41
tf 0.45 0.69 0.51
overlap 0.44 0.69 0.27
Table 1: Correlation of individual features for the training
sets with the gold standards
tite graph matching, the graphnorm variation pro-
vides the strongest correlation results across all the
datasets.
We include the evaluation results provided by the
task organizers in Table 2. They indicate that our in-
tuition in using a support vector regression strategy
was correct. While the IndividualRegression was
our strongest system on the training data, the same
ranking applies to the test data (including the addi-
tional two surprise datasets) as well, earning it the
fifth place among the 89 participating systems, with
a Pearson correlation of 0.7846.
Regarding the decision tree based learning
(IndividualDecTree), despite its more robust be-
havior on the train sets, it achieved slightly lower
outcome on the test data, at 0.7677 correlation. We
believe this happened because decision trees have a
tendency to overfit training data, as they generate a
rigid structure which is unforgiving to minor devia-
tions in the test data. Nonetheless, this second vari-
ation still ranks in the top 10% of the submitted sys-
tems.
As an alternative approach to handle unknown test
data (e.g. different distributions, genres), we opted
640
Run ALL Rank Mean RankMean MSRpar MSRvid SMTeuroparl OnWN SMTnews
IndividualRegression 0.7846 5 0.6162 13 0.5353 0.8750 0.4203 0.6715 0.4033
IndividualDecTree 0.7677 9 0.5947 25 0.5693 0.8688 0.4203 0.6491 0.2256
CombinedRegression 0.7418 14 0.6159 14 0.5032 0.8695 0.4797 0.6715 0.4033
Table 2: Evaluation results and ranking published by the task organizers
to also include the CombinedRegression strategy
as our third variation. This seems to have been fruit-
ful for MSRvid, SMTeuroparl, and the two sur-
prise datasets (ONWn and SMTnews). In the
case of SMTeuroparl, this expanded training set
achieves a better performance than learning from
the corresponding training set alne, gaining an im-
provement of 0.0776 correlation points. Unfortu-
nately, the variation has some losses, particularly for
the MSRpar dataset (0.0321), yet it is able to con-
sistently model and handle a wider variety of text
types.
5 Conclusion
This paper describes the three system variations our
team participated with in the Semantic Text Similar-
ity task in SEMEVAL 2012. Our focus has been to
produce a synergistic approach, striving to achieve a
superior result than attainable by each system indi-
vidually. We have considered a variety of methods
for inferring semantic similarity, including knowl-
edge and corpus-based methods. These were lever-
aged in a machine-learning framework, where our
preferred learning algorithm is support vector re-
gression, due to its ability to deal with continuous
classes and to dampen the effect of noisy features,
while augmenting more robust ones. While it is al-
ways preferable to use similar test and train sets,
when information regarding the test dataset is un-
available, we show that a robust performance can
be achieved by combining all train data from dif-
ferent sources into a single set and allowing a ma-
chine learner to make predictions. Overall, it was
interesting to note that corpus-based methods main-
tain strong results on all train datasets in compari-
son to knowledge-based methods. Our three systems
ranked number 5, 9 and 14 among the 89 systems
participating in the task.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS award #1018613.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the National Science Foundation.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using Wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 1606?1611, Hyderabad, India.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA Data
Mining Software: An Update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue, xx(xx).
641
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the Fifth Conference
on Language Resources and Evaluation, volume 2,
Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, D. Laham, B. Rehder, and M. E.
Schreiner. 1997. How well can passage meaning be
derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow, 1998. Combining local
context and WordNet similarity for word sense identi-
fication, pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Y. Lin and E. H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of Human Language Technology Confer-
ence (HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
english. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M.E. Lesk, 1971. The SMART Retrieval
System: Experiments in Automatic Document Process-
ing, chapter Computer evaluation of indexing and text
processing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning,
pages 491?502, Freiburg, Germany.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
642
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 221?228, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
CPN-CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge
Carmen Banea[?, Yoonjung Choi?, Lingjia Deng?, Samer Hassan?, Michael Mohler?
Bishan Yang
?
, Claire Cardie
?
, Rada Mihalcea[?, Janyce Wiebe?
[University of North Texas
Denton, TX
?University of Pittsburgh
Pittsburgh, PA
?Google Inc.
Mountain View, CA
?Language Computer Corp.
Richardson, TX
?
Cornell University
Ithaca, NY
Abstract
This article provides a detailed overview of the
CPN text-to-text similarity system that we par-
ticipated with in the Semantic Textual Similar-
ity task evaluations hosted at *SEM 2013. In
addition to more traditional components, such
as knowledge-based and corpus-based met-
rics leveraged in a machine learning frame-
work, we also use opinion analysis features to
achieve a stronger semantic representation of
textual units. While the evaluation datasets are
not designed to test the similarity of opinions,
as a component of textual similarity, nonethe-
less, our system variations ranked number 38,
39 and 45 among the 88 participating systems.
1 Introduction
Measures of text similarity have been used for a long
time in applications in natural language processing
and related areas. One of the earliest applications
of text similarity is perhaps the vector-space model
used in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their angular distance with the given query
(Salton and Lesk, 1971). Text similarity has also
been used for relevance feedback and text classifi-
cation (Rocchio, 1971), word sense disambiguation
(Lesk, 1986; Schutze, 1998), and extractive summa-
rization (Salton et al, 1997), in the automatic evalu-
ation of machine translation (Papineni et al, 2002),
?carmen.banea@gmail.com
? rada@cs.unt.edu
text summarization (Lin and Hovy, 2003), text co-
herence (Lapata and Barzilay, 2005) and in plagia-
rism detection (Nawab et al, 2011).
Earlier work on this task has primarily focused on
simple lexical matching methods, which produce a
similarity score based on the number of lexical units
that occur in both input segments. Improvements
to this simple method have considered stemming,
stopword removal, part-of-speech tagging, longest
subsequence matching, as well as various weight-
ing and normalization factors (Salton and Buckley,
1997). While successful to a certain degree, these
lexical similarity methods cannot always identify the
semantic similarity of texts. For instance, there is an
obvious similarity between the text segments ?she
owns a dog? and ?she has an animal,? yet these
methods will mostly fail to identify it.
More recently, researchers have started to con-
sider the possibility of combining the large number
of word-to-word semantic similarity measures (e.g.,
(Jiang and Conrath, 1997; Leacock and Chodorow,
1998; Lin, 1998; Resnik, 1995)) within a semantic
similarity method that works for entire texts. The
methods proposed to date in this direction mainly
consist of either bipartite-graph matching strate-
gies that aggregate word-to-word similarity into a
text similarity score (Mihalcea et al, 2006; Islam
and Inkpen, 2009; Hassan and Mihalcea, 2011;
Mohler et al, 2011), or data-driven methods that
perform component-wise additions of semantic vec-
tor representations as obtained with corpus mea-
sures such as latent semantic analysis (Landauer et
al., 1997), explicit semantic analysis (Gabrilovich
and Markovitch, 2007), or salient semantic analysis
221
(Hassan and Mihalcea, 2011).
In this paper, we describe the system variations
with which we participated in the *SEM 2013 task
on semantic textual similarity (Agirre et al, 2013).
The system builds upon our earlier work on corpus-
based and knowledge-based methods of text seman-
tic similarity (Mihalcea et al, 2006; Hassan and
Mihalcea, 2011; Mohler et al, 2011; Banea et al,
2012), while also incorporating opinion aware fea-
tures. Our observation is that text is not only similar
on a semantic level, but also with respect to opin-
ions. Let us consider the following text segments:
?she owns a dog? and ?I believe she owns a dog.?
The question then becomes how similar these text
fragments truly are. Current systems will consider
the two sentences semantically equivalent, yet to a
human, they are not. A belief is not equivalent to a
fact (and for the case in point, the person may very
well have a cat or some other pet), and this should
consequently lower the relatedness score. For this
reason, we advocate that STS systems should also
consider the opinions expressed and their equiva-
lence. While the *SEM STS task is not formulated
to evaluate this type of similarity, we complement
more traditional corpus and knowledge-based meth-
ods with opinion aware features, and use them in
a meta-learning framework in an arguably first at-
tempt at incorporating this type of information to in-
fer text-to-text similarity.
2 Related Work
Over the past years, the research community has
focused on computing semantic relatedness using
methods that are either knowledge-based or corpus-
based. Knowledge-based methods derive a measure
of relatedness by utilizing lexical resources and on-
tologies such as WordNet (Miller, 1995) to measure
definitional overlap, term distance within a graph-
ical taxonomy, or term depth in the taxonomy as
a measure of specificity. We explore several of
these measures in depth in Section 3.3.1. On the
other side, corpus-based measures such as Latent
Semantic Analysis (LSA) (Landauer et al, 1997),
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007), Salient Semantic Analysis
(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-
tual Information (PMI) (Church and Hanks, 1990),
PMI-IR (Turney, 2001), Second Order PMI (Islam
and Inkpen, 2006), Hyperspace Analogues to Lan-
guage (Burgess et al, 1998) and distributional simi-
larity (Lin, 1998) employ probabilistic approaches
to decode the semantics of words. They consist
of unsupervised methods that utilize the contextual
information and patterns observed in raw text to
build semantic profiles of words. Unlike knowledge-
based methods, which suffer from limited coverage,
corpus-based measures are able to induce a similar-
ity between any given two words, as long as they
appear in the very large corpus used as training.
3 Semantic Textual Similarity System
3.1 Task Setup
The STS task consists of labeling one sentence pair
at a time, based on the semantic similarity existent
between its two component sentences. Human as-
signed similarity scores range from 0 (no relation)
to 5 (semantivally equivalent). The *SEM 2013 STS
task did not provide additional labeled data to the
training and testing sets released as part of the STS
task hosted at SEMEVAL 2012 (Agirre et al, 2012);
our system variations were trained on SEMEVAL
2012 data.
The test sets (Agirre et al, 2013) consist of
text pairs extracted from headlines (headlines,
750 pairs), sense definitions from WordNet and
OntoNotes (OnWN, 561 pairs), sense definitions
from WordNet and FrameNet (FNWN, 189 pairs),
and data used in the evaluation of machine transla-
tion systems (SMT, 750 pairs).
3.2 Resources
Various subparts of our framework use several re-
sources that are described in more detail below.
Wikipedia1 is the most comprehensive encyclo-
pedia to date, and it is an open collaborative effort
hosted on-line. Its basic entry is an article which in
addition to describing an entity or an event also con-
tains hyperlinks to other pages within or outside of
Wikipedia. This structure (articles and hyperlinks)
is directly exploited by semantic similarity methods
such as ESA (Gabrilovich and Markovitch, 2007),
or SSA (Hassan and Mihalcea, 2011)2.
1www.wikipedia.org
2In the experiments reported in this paper, all the corpus-
based methods are trained on the English Wikipedia download
from October 2008.
222
WordNet (Miller, 1995) is a manually crafted lex-
ical resource that maintains semantic relationships
such as synonymy, antonymy, hypernymy, etc., be-
tween basic units of meaning, or synsets. These rela-
tionships are employed by various knowledge-based
methods to derive semantic similarity.
The MPQA corpus (Wiebe and Riloff, 2005) is
a newswire data set that was manually annotated
at the expression level for opinion-related content.
Some of the features derived by our opinion extrac-
tion models were based on training on this corpus.
3.3 Features
Our system variations derive the similarity score of a
given sentence-pair by integrating information from
knowledge, corpus, and opinion-based sources3.
3.3.1 Knowledge-Based Features
Following prior work from our group (Mihalcea
et al, 2006; Mohler and Mihalcea, 2009), we em-
ploy several WordNet-based similarity metrics for
the task of sentence-level similarity. Briefly, for each
open-class word in one of the input texts, we com-
pute the maximum semantic similarity4 that can be
obtained by pairing it with any open-class word in
the other input text. All the word-to-word similarity
scores obtained in this way are summed and normal-
ized to the length of the two input texts. We provide
below a short description for each of the similarity
metrics employed by this system.
The shortest path (Path) similarity is equal to:
Simpath =
1
length
(1)
where length is the length of the shortest path be-
tween two concepts using node-counting.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) (LCH) metric is equal to:
Simlch = ? log
length
2 ?D
(2)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk (Lesk) similarity of two concepts is de-
fined as a function of the overlap between the cor-
responding definitions, as provided by a dictionary.
3The abbreviation in italics accompanying each method al-
lows for cross-referencing with the results listed in Table 2.
4We use the WordNet::Similarity package (Pedersen et al,
2004).
It is based on an algorithm proposed by Lesk (1986)
as a solution for word sense disambiguation.
The Wu & Palmer (Wu and Palmer, 1994) (WUP )
similarity metric measures the depth of two given
concepts in the WordNet taxonomy, and the depth
of the least common subsumer (LCS), and combines
these figures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(3)
The measure introduced by Resnik (Resnik, 1995)
(RES) returns the information content (IC) of the
LCS of two concepts:
Simres = IC(LCS) (4)
where IC is defined as:
IC(c) = ? logP (c) (5)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The measure introduced by Lin (Lin, 1998) (Lin)
builds on Resnik?s measure of similarity, and adds
a normalization factor consisting of the information
content of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(6)
We also consider the Jiang & Conrath (Jiang and
Conrath, 1997) (JCN ) measure of similarity:
Simjnc =
1
IC(concept1) + IC(concept2)? 2 ? IC(LCS)
(7)
3.3.2 Corpus Based Features
While most of the corpus-based methods induce
semantic profiles in a word-space, where the seman-
tic profile of a word is expressed in terms of its co-
occurrence with other words, LSA, ESA and SSA
rely on a concept-space representation, thus express-
ing a word?s semantic profile in terms of the im-
plicit (LSA), explicit (ESA), or salient (SSA) con-
cepts. This departure from the sparse word-space to
a denser, richer, and unambiguous concept-space re-
solves one of the fundamental problems in semantic
relatedness, namely the vocabulary mismatch.
Latent Semantic Analysis (LSA) (Landauer et al,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
erated by a singular value decomposition (SVD)
223
on the term-by-context matrix T, where the ma-
trix is induced from a large corpus. This reduc-
tion entails the abstraction of meaning by collaps-
ing similar contexts and discounting noisy and ir-
relevant ones, hence transforming the real world
term-context space into a word-latent-concept space
which achieves a much deeper and concrete seman-
tic representation of words5.
Random Projection (RP ) (Dasgupta, 1999). In RP,
a high dimensional space is projected onto a lower
dimensional one, using a randomly generated ma-
trix. (Bingham and Mannila, 2001) show that unlike
LSA or principal component analysis (PCA), RP
is computationally efficient for large corpora, while
also retaining accurate vector similarity and yielding
comparable results.
Explicit Semantic Analysis (ESA) (Gabrilovich
and Markovitch, 2007). ESA uses encyclopedic
knowledge in an information retrieval framework to
generate a semantic interpretation of words. It relies
on the distribution of words inside Wikipedia arti-
cles, thus building a semantic representation for a
given word using a word-document association.
Salient Semantic Analysis (SSA) (Hassan and Mi-
halcea, 2011). SSA incorporates a similar seman-
tic abstraction as ESA, yet it uses salient con-
cepts gathered from encyclopedic knowledge, where
a ?concept? represents an unambiguous expression
which affords an encyclopedic definition. Saliency
in this case is determined based on the word being
hyperlinked in context, implying that it is highly rel-
evant to the given text.
In order to determine the similarity of two text
fragments, we employ two variations: the typical
cosine similarity (cos) and a best alignment strat-
egy (align), which we explain in more detail in
the paragraph below. Both variations were paired
with the ESA, and SSA systems resulting in four
similarity scores that were used as features by our
meta-system, namely ESAcos, ESAalign, SSAcos,
and SSAalign; in addition, we also used BOWcos,
LSAcos, and RPcos.
Best Alignment Strategy (align). Let Ta and Tb be
two text fragments of size a and b respectively. After
removing all stopwords, we first determine the num-
5We use the LSA implementation available at code.
google.com/p/semanticvectors/.
ber of shared terms (?) between Ta and Tb. Second,
we calculate the semantic relatedness of all possible
pairings between non-shared terms in Ta and Tb. We
further filter these possible combinations by creating
a list ? which holds the strongest semantic pairings
between the fragments? terms, such that each term
can only belong to one and only one pair.
Sim(Ta, Tb) =
(? +
?|?|
i=1 ?i)? (2ab)
a+ b
(8)
where ?i is the similarity score for the ith pairing.
3.3.3 Opinion Aware Features
We design opinion-aware features to capture sen-
tence similarity on the subjectivity level based on the
output of three subjectivity analysis systems. Intu-
itively, two sentences are similar in terms of sub-
jectivity if there exists similar opinion expressions
which also share similar opinion holders.
OpinionFinder (Wilson et al, 2005) is a publicly
available opinion extraction model that annotates the
subjectivity of new text based on the presence (or
absence) of words or phrases in a large lexicon. The
system consists of a two step process, by feeding
the sentences identified as subjective or objective
by a rule-based high-precision classifier to a high-
recall classifier that iteratively learns from the re-
maining corpus. For each sentence in a STS pair,
the two classifiers provide two predictions; a subjec-
tivity similarity score (SUBJSL) is computed as fol-
lows. If both sentences are classified as subjective
or objective, the score is 1; if one is subjective and
the other one is objective, the score is -1; otherwise
it is 0. We also make use of the output of the sub-
jective expression identifier in OpinionFinder. We
first record how many expressions the two sentences
have: feature NUMEX1 and NUMEX2. Then we
compare how many tokens these expressions share
and we normalize by the total number of expressions
(feature EXPR).
We compute the difference between the probabil-
ities of the two sentences being subjective (SUBJD-
IFF), by employing a logistic regression classifier
using LIBLINEAR (Fan et al, 2008) trained on the
MPQA corpus. The smaller the difference, the more
similar the sentences are in terms of subjectivity.
We also employ features produced by the opinion-
extraction model of Yang and Cardie (Yang and
Cardie, 2012), which is better suited to process ex-
224
pressions of arbitrary length. Specifically, for each
sentence, we extract subjective expressions and gen-
erate the following features. SUBJCNT is a binary
feature which is equal to 1 if both sentences con-
tain a subjective expression. DSEALGN marks the
number of shared words between subjective expres-
sions in two sentences, while DSESIM represents
their similarity beyond the word level. We repre-
sent the subjective expressions in each sentence as
a feature vector, containing unigrams extracted from
the expressions, their part-of-speech, their WordNet
hypernyms and their subjectivity label6, and com-
pute the cosine similarity between the feature vec-
tors. The holder of the opinion expressions is ex-
tracted with the aid of a dependency parser7. In most
cases, the opinion holder and the opinion expression
are related by the dependency relation subj. This re-
lation is used to expand the verb dependents in the
opinion expression and identify the opinion holder
or AGENT.
3.4 Meta-learning
Each metric described above provides one individ-
ual score for every sentence-pair in both the train-
ing and test set. These scores then serve as in-
put to a meta-learner, which adjusts their impor-
tance, and thus their bearing on the overall similar-
ity score predicted by the system. We experimented
with regression and decision tree based algorithms
by performing 10-fold cross validation on the 2012
training data; these types of learners are particularly
well suited to maintain the ordinality of the seman-
tic similarity scores (i.e. a score of 4.5 is closer
to either 4 or 5, implying that the two sentences
are mostly or fully equivalent, while also being far
further away from 0, implying no semantic relat-
edness between the two sentences). We obtained
consistent results when using support vector regres-
sion with polynomial kernel (Drucker et al, 1997;
Smola and Schoelkopf, 1998) (SV R) and random
subspace meta-classification with tree learners (Ho,
1998) (RandSubspace)8.
We submitted three system variations based
on the training corpus (first word in the sys-
6Label is based on the OpinionFinder subjectivity lexicon
(Wiebe et al, 2005).
7nlp.stanford.edu/software/
8Included with the Weka framework (Hall et al, 2009); we
used the default values for both algorithms.
System FNWN headlines OnWN SMT Mean
comb.RandSubSpace 0.331 0.677 0.514 0.337 0.494
comb.SVR 0.362 0.669 0.510 0.341 0.494
indv.RandSubspace 0.331 0.677 0.548 0.277 0.483
baseline-tokencos 0.215 0.540 0.283 0.286 0.364
Table 1: Evaluation results (Agirre et al, 2013).
tem name) or the learning methodology (second
word) used: comb.RandSubspace, comb.SV R and
indv.RandSubspace. For comb, training was per-
formed on the merged version of the entire 2012 SE-
MEVAL dataset. For indv, predictions for OnWN
and SMT test data were based on training on
matching OnWN and SMT 9 data from 2012, pre-
dictions for the other test sets were computed using
the combined version (comb).
4 Results and Discussion
Table 2 lists the correlations obtained between
the scores assigned by each one of the features
we used and the scores assigned by the human
judges. It is interesting to note that overall, corpus-
based measures are stronger performers compared to
knowledge-based measures. The top contenders in
the former group are ESAalign, SSAalign, LSAcos,
and RPcos, indicating that these methods are able to
leverage a significant amount of semantic informa-
tion from text. While LSAcos achieves high corre-
lations on many of the datasets, replacing the singu-
lar value decomposition operation by random pro-
jection to a lower-dimension space (RP ) achieves
competitive results while also being computation-
ally efficient. This observation is in line with prior
literature (Bingham and Mannila, 2001). Among
the knowledge-based methods, JCN and Path
achieve high performance on more than five of the
datasets. In some cases, particularly on the 2013
test data, the shortest path method (Path) peforms
better or on par with the performance attained by
other knowledge-based measures, despite its com-
putational simplicity. While opinion-based mea-
sures do not exhibit the same high correlation, we
should remember that none of the datasets displays
consistent opinion content, nor were they anno-
tated with this aspect in mind, in order for this in-
formation to be properly leveraged and evaluated.
9The SMT training set is a combination of SMTeuroparl
(in this paper abbreviated as SMTep) and SMTnews data.
225
Train 2012 Test 2012 Test 2013
Feature SMTep MSRpar MSRvid SMTep MSRpar MSRvid OnWN SMTnews FNWN headlines OnWN SMT
Knowledge-based measures
JCN 0.51 0.49 0.63 0.48 0.48 0.64 0.62 0.28 0.38 0.72 0.71 0.34
LCH 0.45 0.48 0.49 0.47 0.49 0.54 0.54 0.3 0.39 0.69 0.69 0.32
Lesk 0.5 0.48 0.59 0.5 0.47 0.63 0.64 0.4 0.4 0.71 0.7 0.33
Lin 0.48 0.49 0.54 0.48 0.48 0.56 0.57 0.27 0.28 0.65 0.66 0.3
Path 0.5 0.49 0.62 0.48 0.49 0.65 0.62 0.35 0.43 0.72 0.73 0.34
RES 0.48 0.47 0.55 0.49 0.47 0.6 0.62 0.33 0.28 0.64 0.7 0.31
WUP 0.42 0.46 0.38 0.44 0.48 0.42 0.48 0.26 0.19 0.55 0.6 0.25
Corpus-based measures
BOW cos 0.51 0.47 0.69 0.32 0.44 0.71 0.66 0.37 0.34 0.68 0.52 0.32
ESA cos 0.53 0.34 0.71 0.44 0.3 0.77 0.63 0.44 0.34 0.55 0.35 0.27
ESA align 0.55 0.56 0.75 0.49 0.52 0.78 0.69 0.38 0.46 0.71 0.47 0.34
SSA cos 0.4 0.34 0.63 0.4 0.22 0.71 0.6 0.42 0.35 0.48 0.47 0.26
SSA align 0.54 0.56 0.74 0.49 0.51 0.77 0.68 0.38 0.44 0.69 0.46 0.34
LSA cos 0.65 0.48 0.76 0.36 0.45 0.79 0.67 0.45 0.25 0.63 0.61 0.32
RP cos 0.6 0.49 0.78 0.46 0.43 0.79 0.7 0.45 0.38 0.68 0.57 0.34
Opinion-aware measures
AGENT 0.16 0.15 0.05 0.11 0.12 0.03 n/a -0.01 n/a 0.08 -0.04 0.11
DSEALGN 0.18 0.2 0.11 0.05 0.11 0.11 0.07 0.06 -0.1 0.08 0.13 0.1
DSESIM 0.12 0.15 0.05 0.1 0.08 0.07 0.04 0.08 0.05 0.08 0.04 0.08
EXPR 0.17 0.19 0.06 0.18 0.18 0.02 0.07 0 0.13 0.08 0.18 0.17
NUMEX1 0.12 0.22 -0.03 0.07 0.16 -0.05 -0.01 -0.01 -0.01 -0.03 0.08 0.1
NUMEX2 -0.25 0.19 0.01 0.06 0.14 -0.03 0.01 0.06 0.09 -0.05 0.03 0.11
SUBJCNT 0.14 0.19 0.01 0.09 0.07 0.03 0.02 0.08 0.05 0.05 0.05 0.09
SUBJDIFF -0.07 -0.07 -0.17 -0.27 -0.13 -0.22 -0.17 -0.12 -0.04 -0.12 -0.2 -0.12
SUBJSL 0.15 -0.11 0.07 0.23 0.01 0.07 0.11 -0.08 0.15 0.07 -0.03 0
Table 2: Correlation of individual features for the training and test sets with the gold standard.
Nonetheless, we notice several promising features,
such as DSEALIGN and EXPR. Lower cor-
relations seem to be associated with shorter spans
of text, since when averaging all opinion-based cor-
relations per dataset, MSRvid (x2), OnWN (x2),
and headlines display the lowest average correla-
tion, ranging from 0 to 0.03. This matches the
expectation that opinionated content can be easier
identified in longer contexts, as additional subjective
elements amount to a stronger prediction. The other
seven datasets consist of longer spans of text; they
display an average opinion-based correlation be-
tween 0.07 and 0.12, with the exception of FNWN
and SMTnews at 0.04 and 0.01, respectively.
Our systems performed well, ranking 38, 39 and
45 among the 88 competing systems in *SEM 2013
(see Table 1), with the best being comb.SVR and
comb.RandSubspace, both with a mean correlation
of 0.494. We noticed from our participation in
SEMEVAL 2012 (Banea et al, 2012), that training
and testing on the same type of data achieves the
best results; this receives further support when con-
sidering the performance of the indv.RandSubspace
variation on the OnWN data10, which exhibits a
10The SMT test data is not part of the same corpus as either
0.034 correlation increase over our next best sys-
tem (comb.RandSubspace). While we do surpass the
bag-of-words cosine baseline (baseline-tokencos)
computed by the task organizers by a 0.13 differ-
ence in correlation, we fall short by 0.124 from the
performance of the best system in the STS task.
5 Conclusions
To participate in the STS *SEM 2013 task, we con-
structed a meta-learner framework that combines
traditional knowledge and corpus-based methods,
while also introducing novel opinion analysis based
metrics. While the *SEM data is not particularly
suited for evaluating the performance of opinion fea-
tures, this is nonetheless a first step toward conduct-
ing text similarity research while also considering
the subjective dimension of text. Our system varia-
tions ranked 38, 39 and 45 among the 88 participat-
ing systems.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation CA-
REER award #0747340 and IIS awards #1018613,
SMTep or SMTnews.
226
#0208798 and #0916046. This work was sup-
ported in part by DARPA-BAA-12-47 DEFT grant
#12475008. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the authors and do not necessarily reflect
the views of the National Science Foundation or the
Defense Advanced Research Projects Agency.
References
E. Agirre, D. Cer, M. Diab, and A. Gonzalez. 2012.
Semeval-2012 task 6: A pilot on semantic textual sim-
ilarity. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), in con-
junction with the First Joint Conference on Lexical and
Computational Semantics (*SEM 2012).
E. Agirre, D. Cer, M. Diab, A. Gonzalez-Agirre, and W.
Guo. 2013. *SEM 2013 Shared Task: Semantic Tex-
tual Similarity, including a Pilot on Typed-Similarity.
In Proceedings of the Second Joint Conference on Lex-
ical and Computational Semantics (*SEM 2013), At-
lanta, GA, USA.
C. Banea, S. Hassan, M. Mohler, and R. Mihalcea. 2012.
UNT: A supervised synergistic approach to seman-
tic text similarity. In Proceedings of the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012), pages 635?642, Montreal, Canada.
E. Bingham and H. Mannila. 2001. Random projection
in dimensionality reduction: applications to image and
text data. In Proceedings of the seventh ACM SIGKDD
international conference on Knowledge discovery and
data mining (KDD 2001), pages 245?250, San Fran-
cisco, CA, USA.
C. Burgess, K. Livesay, and K. Lund. 1998. Explorations
in context space: words, sentences, discourse. Dis-
course Processes, 25(2):211?257.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1):22?29.
S. Dasgupta. 1999. Learning mixtures of Gaussians. In
40th Annual Symposium on Foundations of Computer
Science (FOCS 1999), pages 634?644, New York, NY,
USA.
H. Drucker, C. J. Burges, L. Kaufman, A. Smola, and
Vladimir Vapnik. 1997. Support vector regression
machines. Advances in Neural Information Process-
ing Systems, 9:155?161.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. 2008.
Liblinear: A library for large linear classification. The
Journal of Machine Learning Research, 9:1871?1874.
E. Gabrilovich and S. Markovitch. 2007. Comput-
ing semantic relatedness using Wikipedia-based ex-
plicit semantic analysis. In Proceedings of the 20th
AAAI International Conference on Artificial Intelli-
gence (AAAI?07), pages 1606?1611, Hyderabad, In-
dia.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and Ian H. Witten. 2009. The WEKA data
mining software: An update. SIGKDD Explorations,
11(1).
S. Hassan and R. Mihalcea. 2011. Measuring semantic
relatedness using salient encyclopedic concepts. Arti-
ficial Intelligence, Special Issue.
T. K. Ho. 1998. The Random Subspace Method for
Constructing Decision Forests. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 20(8):832?
844.
A. Islam and D. Inkpen. 2006. Second order co-
occurrence PMI for determining the semantic similar-
ity of words. In Proceedings of the 5th Conference on
Language Resources and Evaluation (LREC 06), vol-
ume 2, pages 1033?1038, Genoa, Italy, July.
A. Islam and D. Inkpen. 2009. Semantic Similarity of
Short Texts. In Nicolas Nicolov, Galia Angelova, and
Ruslan Mitkov, editors, Recent Advances in Natural
Language Processing V, volume 309 of Current Issues
in Linguistic Theory, pages 227?236. John Benjamins,
Amsterdam & Philadelphia.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
International Conference Research on Computational
Linguistics (ROCLING X), pages 9008+, September.
T. K. Landauer, T. K. L, D. Laham, B. Rehder, and M.
E. Schreiner. 1997. How well can passage meaning
be derived without using word order? a comparison of
latent semantic analysis and humans.
M. Lapata and R. Barzilay. 2005. Automatic evaluation
of text coherence: Models and representations. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, Edinburgh.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense identi-
fication. In WordNet: An Electronic Lexical Database,
pages 305?332.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In SIGDOC ?86: Pro-
ceedings of the 5th annual international conference on
Systems documentation, pages 24?26, New York, NY,
USA. ACM.
C. Lin and E. Hovy. 2003. Automatic evaluation of sum-
maries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference
(HLT-NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the Fifteenth Interna-
227
tional Conference on Machine Learning, pages 296?
304, Madison, Wisconsin.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
pages 775?780, Boston, MA, US.
G. A. Miller. 1995. WordNet: a Lexical database for
English. Communications of the Association for Com-
puting Machinery, 38(11):39?41.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
M. Mohler, R. Bunescu, and R. Mihalcea. 2011. Learn-
ing to grade short answer questions using semantic
similarity measures and dependency graph alignments.
In Proceedings of the Association for Computational
Linguistics ? Human Language Technologies (ACL-
HLT 2011), Portland, Oregon, USA.
R. M. A. Nawab, M. Stevenson, and P. Clough. 2011.
External plagiarism detection using information re-
trieval and sequence alignment: Notebook for PAN at
CLEF 2011. In Proceedings of the 5th International
Workshop on Uncovering Plagiarism, Authorship, and
Social Software Misuse (PAN 2011).
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318, Philadelphia, PA.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
P. Resnik. 1995. Using information content to evaluate
semantic similarity in a taxonomy. In In Proceedings
of the 14th International Joint Conference on Artificial
Intelligence, pages 448?453.
J. Rocchio, 1971. Relevance feedback in information re-
trieval. Prentice Hall, Ing. Englewood Cliffs, New Jer-
sey.
G. Salton and C. Buckley. 1997. Term weighting ap-
proaches in automatic text retrieval. In Readings in
Information Retrieval. Morgan Kaufmann Publishers,
San Francisco, CA.
G. Salton and M. Lesk, 1971. The SMART Retrieval Sys-
tem: Experiments in Automatic Document Processing,
chapter Computer evaluation of indexing and text pro-
cessing. Prentice Hall, Ing. Englewood Cliffs, New
Jersey.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. Infor-
mation Processing and Management, 2(32).
H. Schutze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97?124.
A. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Re-
port NC2-TR-1998-030.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the 12th European Conference on Machine Learning
(ECML?01), pages 491?502, Freiburg, Germany.
J. Wiebe and E. Riloff. 2005. Creating subjective and
objective sentence classifiers from unannotated texts.
In Proceedings of the 6th international conference on
Computational Linguistics and Intelligent Text Pro-
cessing (CICLing 2005), pages 486?497, Mexico City,
Mexico.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2-3):165?210.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
Janyce Wiebe, Yejin Choi, Claire Cardie, Ellen Riloff,
and Siddharth Patwardhan. 2005. OpinionFinder:
A system for subjectivity analysis. In Proceedings
of HLT/EMNLP on Interactive Demonstrations, pages
34?35, Vancouver, BC, Canada.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexical
selection. In Proceedings of the 32nd annual meeting
on Association for Computational Linguistics, pages
133?-138, Las Cruces, New Mexico.
B. Yang and C. Cardie. 2012. Extracting opinion expres-
sions with semi-markov conditional random fields. In
Proceedings of the conference on Empirical Meth-
ods in Natural Language Processing. Association for
Computational Linguistics.
228
Proceedings of the First Workshop on Metaphor in NLP, pages 27?35,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Semantic Signatures for Example-Based Linguistic Metaphor Detection
Michael Mohler and David Bracewell and David Hinote and Marc Tomlinson
Language Computer Corp.
Richardson, Texas, USA
{michael,david,dhinote,marc}@languagecomputer.com
Abstract
Metaphor is a pervasive feature of human lan-
guage that enables us to conceptualize and
communicate abstract concepts using more
concrete terminology. Unfortunately, it is
also a feature that serves to confound a com-
puter?s ability to comprehend natural human
language. We present a method to detect
linguistic metaphors by inducing a domain-
aware semantic signature for a given text and
compare this signature against a large index
of known metaphors. By training a suite of
binary classifiers using the results of several
semantic signature-based rankings of the in-
dex, we are able to detect linguistic metaphors
in unstructured text at a significantly higher
precision as compared to several baseline ap-
proaches.
1 Introduction
Metaphor is a widely-used literary mechanism
which allows for the comparison of seemingly un-
related concepts. It has been thoroughly studied in
both the linguistics literature (Ahrens et al, 2003;
Lakoff and Johnson, 1980; Tourangeau and Stern-
berg, 1982; Wilks, 1978) and more recently within
the field of computational linguistics.1 Although
there have been many influential theories regarding
the cognitive basis of metaphor, the most promi-
nent among them is Lakoff?s Contemporary The-
ory of Metaphor (Lakoff and Johnson, 1980; Lakoff,
1993), which popularized the idea of a conceptual
1For a broad survey of the relevant literature, see Shutova
(2010).
metaphor mapping. Within the cognitive framework
of a given conceptual mapping, terms pertaining to
one concept or domain (the source) can be used fig-
uratively to express some aspect of another concept
or domain (the target). For example, the conceptual
metaphor ?Life is a Journey? indicates a medium
within which the target concept ?life? may be more
easily discussed and understood. This particular
mapping allows us to speak of one being stuck in a
?dead-end? job, a crucial decision as being a ?fork in
the road?, or someone?s life ?taking a wrong turn?.
By allowing us to discuss an abstract target con-
cept using the vocabulary and world knowledge
associated with a more familiar source concept,
metaphor serves as a vehicle for human communica-
tion and understanding, and as such, has been found
to be extremely prevalent in natural language, oc-
curring as often as every third sentence (Shutova et
al., 2010). As a consequence of this ubiquity, it is
crucial that any system tasked with the understand-
ing of natural language be capable of detecting the
presence of metaphor in text and of modeling the
intended semantic content of the metaphoric expres-
sion. In this work, we first induce a domain-sensitive
semantic signature which we define as a set of highly
related and interlinked WordNet (Fellbaum, 1998)
senses drawn and augmented from a text that may
be used to place the text within the semantic space
of a metaphoric concept. We then employ a suite
of binary classifiers to detect metaphoricity within
a text by comparing its semantic signature to a set
of known metaphors. If the semantic signature of
the text closely matches the signature of a known
metaphor, we propose that it is likely to represent
27
Example Metaphor
Obama heard a bomb ticking in his left ear. No
Obama heard another political bomb ticking, this time in his left ear. Yes
Table 1: The top sentence describes a literal bomb ticking, while the bottom sentence uses metaphoric language to
describe an impending political disaster.
an instance of the same conceptual metaphor. To fa-
cilitate this work, we have built an index of known
metaphors within a particular target domain. We
have selected the domain of Governance which we
define broadly to include electoral politics, the set-
ting and enactment of economic policy, and the
creation, application, and enforcement of rules and
laws.
The problem of metaphor as it relates to computer
understanding is illustrated in the example sentences
of Table 1. A strictly literal reading suggests that the
two sentences are describing something very similar.
At the very least, the semantics of the phrases ?bomb
ticking? and ?in his left ear? are indistinguishable
without the added knowledge that the second sen-
tence is using metaphor to convey information about
something altogether different from explosives and
body parts. From the context of the full sentences,
it is clear that while the first sentence is straight-
forwardly describing Obama and his perception of
a literal bomb, the second is describing an impend-
ing political crisis as though it were a bomb. Rather
than a literal ?ear? this sentence uses the phrase ?in
his left ear? to suggest that the source of the crisis in
on the political ?left?. In order for an automated sys-
tem to correctly understand the intended meaning of
these sentences, it must first be aware that the text
under consideration is not to be taken literally, and
given this knowledge, it must employ all available
knowledge of the underlying conceptual mapping to
appropriately interpret the text in context.
The remainder of this work is organized as fol-
lows. In Section 2, we survey related work in se-
mantic representation and linguistic metaphor iden-
tification. Section 3 describes in detail our approach
to metaphor identification through the use of seman-
tic signatures. In Section 4, we discuss the setup of
our experiment which includes the creation of our
metaphor index as well as the extraction and anno-
tation of our training and testing data sets. Finally,
we show the results of our experiments in Section 5
and share our conclusions in Section 6.
2 Related Work
The phenomenon of metaphor has been studied
by researchers across multiple disciplines, includ-
ing psychology, linguistics, sociology, anthropol-
ogy, and computational linguistics. A number of
theories of metaphor have been proposed, includ-
ing the Contemporary Theory of Metaphor (Lakoff,
1993), the Conceptual Mapping Model (Ahrens et
al., 2003), the Structure Mapping Model (Wolff and
Gentner, 2000), and the Attribute Categorization
Hypothesis (McGlone, 1996). Based on these the-
ories, large collections of metaphors have been as-
sembled and published for use by researchers. The
Master Metaphor List (MML) (Lakoff, 1994) groups
linguistic metaphors together according to their
conceptual mapping, and the Hamburg Metaphor
Database (HMD) (Eilts and Lo?nneker, 2002) for
French and German fuses EuroWordNet synsets
with the MML source and target domains for a ro-
bust source of metaphoric semantics in those lan-
guages.
In recent years, the computational linguistics
community has seen substantial activity in the de-
tection of figurative language (Bogdanova, 2010;
Li and Sporleder, 2010; Peters and Wilks, 2003;
Shutova, 2011) one aspect of which is the iden-
tification of metaphoric expressions in text (Fass,
1991; Shutova et al, 2010; Mason, 2004). Much of
the early work on the identification of metaphor re-
lied upon hand-crafted world knowledge. The met*
(Fass, 1991) system sought to determine whether an
expression was literal or figurative by detecting the
violation of selectional preferences. Figurative ex-
pressions were then classified as either metonymic,
using hand-crafted patterns, or metaphoric, us-
ing a manually constructed database of analogies.
The CorMet (Mason, 2004) system determined the
28
source and target concepts of a metaphoric expres-
sion using domain-specific selectional preferences
mined from Internet resources. More recent work
has examined noun-verb clustering (Shutova et al,
2010) which starts from a small seed set of one-
word metaphors and results in clusters that rep-
resent source and target concepts connected via a
metaphoric relation. These clusters are then used to
annotate the metaphoricity of text.
Similar to our work, the Metaphor Interpreta-
tion, Denotation, and Acquisition System (MIDAS)
(Martin, 1990) employed a database of conventional
metaphors that could be searched to find a match
for a metaphor discovered in text. If no match
was found, the metaphoric text was replaced with a
more abstract equivalent (e.g. a hypernym) and the
database was searched again. If a match was found,
an interpretation mapping was activated, and the
novel metaphor would be added to the database for
use in future encounters. Unfortunately, this tech-
nique was limited to interpreting known metaphors
(and descendants of known metaphors) and was un-
able to detect truly novel usages. By expanding the
metaphors using a more robust semantic signature,
we attempt to transcend this limitation thereby pro-
ducing a more durable system for metaphoric exam-
ple linking.
An additional vein of metaphor research has
sought to model the human processing of metaphor
as a semantic space within which source and tar-
get concepts can be placed such that the similar-
ity between their representations within this space
(i.e. semantic vectors) can be sensibly quantified
(Katz, 1992; Utsumi, 2011). One computational
example of this approach (Kintsch, 2000) has em-
ployed latent semantic analysis (LSA) (Landauer
and Dumais, 1997) to represent the semantic space
of the metaphors in a reduced dimensionality (i.e.
using singular value decomposition). In their ap-
proach, metaphors were represented as a set of terms
found using a spreading activation algorithm in-
formed by the terms? independent vector related-
ness to the source and target concepts within some
LSA space. By contrast, we have chosen to rep-
resent the metaphoric space using WordNet senses
which have been shown in previous work(Lo?nneker,
2003) to represent a viable representation language
for metaphor. We believe that the ontological knowl-
edge encoded in the semantic relationships of Word-
Net represents an improvement over the distribu-
tional relatedness encoded within an LSA vector.
Also of relevance to the construction and use of
semantic signatures is current research on the induc-
tion of topic signatures. A topic signature is a set of
related words with associated weights which define
and indicate the distinct topics within a text. In their
work on automated summarization, Lin and Hovy
(2000) developed a method for the construction of
topic signatures which were mined from a large cor-
pus. Similarly, Harabagiu and Lacatusu (2005) ex-
plored the use of topic signatures and enhanced topic
signatures for their work on multi-document sum-
marization. By contrast, we explore the use of se-
mantic signatures which serve to enrich the seman-
tics of the source and target frame concepts being
expressed in a text for the purpose of detecting the
presence of metaphor.
3 Methodology
In this work, we approach the task of linguis-
tic metaphor detection as a classification problem.
Starting from a known target domain (i.e. Gover-
nance), we first produce a target domain signature
which represents the target-specific dimensions of
the full conceptual space. Using this domain sig-
nature, we are able to separate the individual terms
of a sentence into source frame elements and tar-
get frame elements and to independently perform a
semantic expansion for each set of elements using
WordNet and Wikipedia as described in our earlier
work (Bracewell et al, 2013). Taken together, the
semantic expansions of a text?s source frame ele-
ments and target frame elements make up the full se-
mantic signature of the text which can then be com-
pared to an index of semantic signatures generated
for a collection of manually detected metaphors. We
use as features for our classifiers a set of metrics that
are able to quantify the similarity between the given
semantic signature and the signatures of metaphors
found within the index.
3.1 Constructing a Target Domain Signature
In order to produce a semantic representation of the
text, we first build a target domain signature, which
we define as a set of highly related and interlinked
29
Figure 1: Focused crawling of Wikipedia articles pertaining to the target concept using intra-wiki links
Figure 2: Constructing the domain signature of the target concept from Wikipedia articles pertaining to the target
concept
WordNet senses that correspond to our particular
target domain with statistical reliability. For ex-
ample, in the domain of Governance the concepts
of ?law?, ?government?, and ?administrator?, along
with their associated senses in WordNet, are present
in the domain signature. We generate this signa-
ture using semantic knowledge encoded in the fol-
lowing resources: (1) the semantic network encoded
in WordNet; (2) the semantic structure implicit in
Wikipedia; and (3) collocation statistics taken from
the statistical analysis of a large corpora. In par-
ticular, we use Wikipedia as an important source
of world knowledge which is capable of provid-
ing information about concepts, such as named en-
tities, that are not found in WordNet as shown in
several recent studies (Toral et al, 2009; Niemann
and Gurevych, 2011). For example, the organi-
zation ?Bilderberg Group? is not present in Word-
Net, but can easily be found in Wikipedia where
it is listed under such categories as ?Global trade
and professional organizations?, ?International busi-
ness?, and ?International non-governmental orga-
nizations?. From these categories we can deter-
mine that the ?Bilderberg Group? is highly related
to WordNet senses such as ?professional organiza-
tion?, ?business?, ?international?, and ?nongovern-
mental organization?.
We begin our construction of the domain signa-
ture by utilizing the semantic markup in Wikipedia
to collect articles that are highly related to the tar-
get concept by searching for the target concept (and
optionally content words making up the definition
of the target concept) in the Wikipedia article titles
and redirects. These articles then serve as a ?seed
set? for a Wikipedia crawl over the intra-wiki links
present in the articles. By initiating the crawl on
these links, it becomes focused on the particular do-
main expressed in the seed articles. The crawling
process continues until either no new articles are
found or a predefined crawl depth (from the set of
seed articles) has been reached. The process is illus-
trated in Figure 1. The result of the crawl is a set
of Wikipedia articles whose domain is related to the
target concept. From this set of articles, the domain
signature can be built by exploiting the semantic in-
formation provided by WordNet.
The process of going from a set of target concept
articles to a domain signature is illustrated in Fig-
ure 2 and begins by associating the terms contained
in the gathered Wikipedia articles with all of their
possible WordNet senses (i.e. no word sense disam-
biguation is performed). The word senses are then
expanded using the lexical (e.g. derivationally re-
lated forms) and semantic relations (e.g. hypernym
and hyponym) available in WordNet. These senses
are then clustered to eliminate irrelevant senses us-
ing the graph-based Chinese Whispers algorithm
(Biemann, 2006). We transform our collection of
word senses into a graph by treating each word sense
as a vertex of an undirected, fully-connected graph
where edge weights are taken to be the product of
the Hirst and St-Onge (1998) WordNet similarity be-
30
tween the two word senses and the first-order cor-
pus cooccurrence of the two terms. In particular, we
use the normalized pointwise mutual information as
computed using a web-scale corpus.
The clusters resulting from the Chinese Whispers
algorithm contain semantically and topically similar
word senses such that the size of a cluster is directly
proportional to the centrality of the concepts within
the cluster as they pertain to the target domain. After
removing stopwords from the clusters, any clusters
below a predefined size are removed. Any cluster
with a low2 average normalized pointwise mutual
information (npmi) score between the word senses
in the cluster and the word senses in the set of terms
related to the target are likewise removed. This set
of target-related terms used in calculating the npmi
are constructed from the gathered Wikipedia articles
using TF-IDF (term frequency inverse document fre-
quency), where TF is calculated within the gathered
articles and IDF is calculated using the entire textual
content of Wikipedia. After pruning clusters based
on size and score, the set of word senses that remain
are taken to be the set of concepts that make up the
target domain signature.
3.2 Building Semantic Signatures for
Unstructured Text
After constructing a signature that defines the do-
main of the target concept, it is possible to use this
signature to map a given text (e.g. a sentence) into
a multidimensional conceptual space which allows
us to compare two texts directly based on their con-
ceptual similarity. This process begins by mapping
the words of the text into WordNet and extracting
the four most frequent senses for each term. In or-
der to improve coverage and to capture entities and
terms not found in WordNet, we also map terms
to Wikipedia articles based on a statistical measure
which considers both the text of the article and the
intra-wiki links. The Wikipedia articles are then
mapped back to WordNet senses using the text of
the categories associated with the article.
In the next step, source and target frame ele-
ments of a given text are separated using the Word-
Net senses contained in the target domain signature.
2We define low as being below an empirically defined
threshold, ? .
Terms in the text which have some WordNet sense
that is included in the domain signature are clas-
sified as target frame elements while those that do
not are considered source frame elements. Figure 3
shows an overview of the process for determining
the source and target concepts within a text. The
remainder of the signature induction process is per-
formed separately for the source and target frame el-
ements. In both cases, the senses are expanded using
the lexical and semantic relations encoded in Word-
Net, including hypernymy, domain categories, and
pertainymy. Additionally, source frame elements
are expanded using the content words found in the
glosses associated with each of the noun and verb
senses. Taken together, these concepts represent the
dimensions of a full conceptual space which can be
separately expressed as the source concept dimen-
sions and target concept dimensions of the space.
Figure 3: Example of a generated conceptual space for a
given text. In this work, only one iteration of the sense
expansion is performed.
In order to determine the correct senses for in-
clusion in the semantic signature of a text, cluster-
ing is performed using the same methodology as
in the construction of the domain signature. First,
a graph is built from the senses with edge weights
assigned based on WordNet similarity and cooccur-
rence. Then, the Chinese Whispers algorithm is used
to cluster the graph which serves to disambiguate the
senses and to prioritize which senses are examined
and incorporated into the source concept dimensions
of the conceptual space. Word senses are prioritized
by ranking the clusters based on their size and on the
highest scoring word sense contained in the cluster
using:
rank(c) = size(c) ?
(?
s score(s)
|c|
)
(1)
where c is the cluster, s is a word sense in the clus-
31
ter, and |c| is the total number of word senses in the
cluster. The senses are scored using: (1) the degree
distribution of the sense in the graph (more central
word senses are given a higher weight); and (2) the
length of the shortest path to the terms appearing in
the given text with concepts closer to the surface
form given a higher weight. Formally, score(s) is
calculated as:
score(s) =
degree(s) + dijkstra(s,R)
2
(2)
where degree(s) is degree distribution of s and
dijkstra(s,R) is the length of the shortest path in
the graph between s and some term in the original
text, R.
Clusters containing only one word sense or with
a score less than the average cluster score (?c) are
ignored. The remaining clusters and senses are
then examined for incorporation into the concep-
tual space with senses contained in higher ranked
clusters examined first. Senses are added as con-
cepts within the conceptual space when their score is
greater than the average word sense score (?s). To
decrease redundancy in the dimensions of the con-
ceptual space, neighbors of the added word sense in
the graph are excluded from future processing.
3.3 Classification
Given a semantic signature representing the place-
ment of a text within our conceptual space, it is pos-
sible to measure the conceptual distance to other sig-
natures within the same space. By mapping a set
of known metaphors into this space (using the pro-
cess described in Section 3.2), we can estimate the
likelihood that a given text contains some metaphor
(within the same target domain) by using the seman-
tic signature of the text to find the metaphors with
the most similar signatures and to measure their sim-
ilarity with the original signature.
We quantify this similarity using five related mea-
sures which are described in Table 2. Each of these
features involves producing a score that ranks ev-
ery metaphor in the index based upon the seman-
tic signature of the given text in a process similar to
that of traditional information retrieval. In particu-
lar, we use the signature of the text to build a query
against which the metaphors can be scored. For each
word sense included in the semantic signature, we
add a clause to the query which combines the vector
space model with the Boolean model so as to prefer
a high overlap of senses without requiring an identi-
cal match between the signatures.3
Three of the features simply take the score of
the highest ranked metaphor as returned by a query.
Most simply, the feature labeled Max Score (na??ve)
uses the full semantic signature for the text which
should serve to detect matches that are very simi-
lar in both the source concept dimensions and the
target concept dimensions. The features Max Score
(source) and Max Score (target) produce the query
using only the source concept dimensions of the
signature and the target concept dimensions respec-
tively.
The remaining two features score the metaphors
within the source dimensions and the target dimen-
sions separately before combining the results into a
joint score. The feature Max Score (joint) calculates
the product of the scores for each metaphor using the
source- and target-specific queries described above
and selects the maximum value among these prod-
ucts. The final feature, Joint Count, represents the
total number of metaphors with a score for both the
source and the target dimensions above some thresh-
old (?j). Unlike the more na??ve features for which a
very good score in one set of dimensions may incor-
rectly lead to a high overall score, these joint similar-
ity features explicitly require metaphors to match the
semantic signature of the text within both the source
and target dimensions simultaneously.
Altogether, these five features are used to train
a suite of binary classifiers to make a decision on
whether a given text is or is not a metaphor.
4 Experimental Setup
One crucial component of our linguistic metaphor
detection system is the index of metaphors (in the
domain of Governance) against which we com-
pare our candidate texts. As a part of this project,
we have produced an ever-growing, metaphor-rich
dataset taken from political speeches, political web-
sites (e.g. Communist Party USA, Tea Party sites,
3This functionality comes standard with the search function-
ality of Apache Lucene which we employ for the production of
our index.
32
Measure Description
Max Score (na??ve) Find the score of the metaphor that best matches the full semantic signature
Max Score (source) Find the score of the metaphor that best matches the source side of the semantic signature
Max Score (target) Find the score of the metaphor that best matches the target side of the semantic signature
Max Score (joint)
Independently score the metaphors by the target side and by the source side.
Find the metaphor with the highest product of the scores.
Joint Count
Independently score the metaphors by the target side and by the source side.
Count the number of metaphors that receive a positive score for both.
Table 2: The five features used by our metaphoricity classifiers.
etc.), and political commentary in web-zines and on-
line newspapers. Three annotators have analyzed
the raw texts and manually selected snippets of text
(with context) whenever some element in the text
seemed to have been used figuratively to describe
or stand in for another element not represented in
the text.4 Each of these metaphors is projected into
a conceptual space using the process described in
Section 3.2 and assembled into a searchable index.
For evaluation purposes, we have selected a sub-
set of our overall repository which consists of
500 raw documents that have been inspected for
metaphoricity by our annotators. We allocate 80%
of these documents for the training of our classi-
fiers and evaluate using the remaining 20%. In total,
our training data consists of 400 documents contain-
ing 1,028 positive examples of metaphor and around
16,000 negative examples. Our test set consists of
100 documents containing 4,041 sentences with 241
positive examples of metaphor and 3,800 negative
examples. For each sentence in each document, our
system attempts to determine whether the sentence
does or does not contain a metaphor within the do-
main of Governance.
We have experimented with several flavors of ma-
chine learning classification. In addition to an in-
house implementation of a binary maximum en-
tropy (MaxEnt) classifier, we have evaluated our re-
sults using four separate classifiers from the popu-
lar Weka machine learning toolkit.5 These include
an unpruned decision tree classifier (J48), a support
vector machine (SMO) approach using a quadratic
4Generally speaking, each annotator operated within a re-
gion of high precision and low recall, and the overlap between
individual annotators was low. As such, we have selected the
union of all metaphors detected by the annotators.
5http://www.cs.waikato.ac.nz/ml/weka/
kernel with parameters tuned via grid search, a rule-
based approach (JRIP), and a random forest clas-
sifier (RF). In addition, we have combined all five
classifiers into an ensemble classifier which uses a
uniformly-weighted voting methodology to arrive at
a final decision.
5 Results
We have evaluated our methodology in two ways.
First, we have performed an evaluation which high-
lights the discriminatory capabilities of our features
by testing on a balanced subset of our test data.
Next, we performed an evaluation which shows the
utility of each of our classifiers as they are applied
to real world data with a natural skew towards literal
usages.6 In both cases, we train on a balanced sub-
set of our training data using all 1,028 positive ex-
amples and a set of negative examples selected ran-
domly such that each document under consideration
contains the same number of positive and negative
examples. In an initial experiment, we trained our
classifiers on the full (skewed) training data, but the
results suggested that an error-minimizing strategy
would lead to all sentences being classified as ?lit-
eral?.
As shown in Table 3, the choice of classifier ap-
pears significant. Several of the classifiers (J48,
JRIP, and MaxEnt) maintain a high recall suggest-
ing the ability of the tree- and rule-based classifiers
to reliably ?filter out? non-metaphors. On the other
hand, other classifiers (SMO and ENSEMBLE) op-
erate in a mode of high precision suggesting that a
high confidence can be associated with their positive
classifications. In all cases, performance is signifi-
6Note that metaphors that are not related to the domain of
Governance are classified as ?literal?.
33
Classifier Precision Recall F-Measure
J48 56.1% 93.0% 70.0%
JRIP 57.7% 79.3% 66.8%
MaxEnt 59.9% 72.6% 65.7%
ENSEMBLE 72.0% 42.7% 53.7%
RF 55.8% 47.7% 51.5%
SMO 75.0% 33.6% 46.4%
All metaphor 50.0% 100.0% 66.7%
Random baseline 50.0% 50.0% 50.0%
Table 3: The results of our experiments using several ma-
chine learning classifiers while evaluating on a dataset
with 241 positive examples and 241 negative examples.
cantly better than chance as reported by our random
baseline.7
Table 4 shows the result of evaluating the same
models on an unbalanced dataset with a natural
skew towards ?literal? sentences which reflects a
more realistic use case in the context of linguistic
metaphor detection. The results suggest that, once
again, the decision tree classification accepts the
vast majority of all metaphors (93%), but also pro-
duces a significant number of false positives mak-
ing it difficult to usefully employ this classifier as
a complete metaphor detection system despite its
top-performing F-measure on the balanced dataset.
More useful is the SMO approach, which shows a
precision over twice that of the random baseline. Put
another way, a positive result from this classifier is
more than 110% more likely to be correct than a
random classification. From the standpoint of util-
ity, joining these classifiers in an ensemble config-
uration seems to combine the high precision of the
SMO classifier with the improved recall of the other
classifiers making the ensemble configuration a vi-
able choice in a real world scenario.
6 Conclusions
We have shown in this work the potential utility
of our example-based approach to detect metaphor
within a domain by comparing the semantic signa-
ture of a text with a set of known metaphors. Al-
though this technique is necessarily limited by the
coverage of the metaphors in the index, we believe
that it is a viable technique for metaphor detection
7According to Fisher?s exact test (one-tailed): RF (p <
0.02); all others (p < 0.0001).
Classifier Precision Recall F-Measure
SMO 12.7% 33.6% 18.4%
ENSEMBLE 11.2% 42.7% 17.8%
MaxEnt 8.7% 72.6% 15.6%
JRIP 8.1% 79.3% 14.8%
J48 7.6% 93.0% 14.0%
RF 7.4% 47.7% 12.7%
All metaphor 6.0% 100.0% 11.3%
Random baseline 6.0% 50.0% 10.7%
Table 4: The results of our experiments using several ma-
chine learning classifiers while evaluating on naturally
skewed dataset with 241 positive examples and 3,800
negative examples.
as more and more examples become available. In
future work, we hope to supplement our existing fea-
tures with such information as term imageability, the
transmission of affect, and selectional preference vi-
olation we believe will result in a robust system for
linguistic metaphor detection to further aid in the
computer understanding of natural language.
Acknowledgments
This research is supported by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via De-
partment of Defense US Army Research Labora-
tory contract number W911NF-12-C-0025. The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes notwith-
standing any copyright annotation thereon. Dis-
claimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoD/ARL, or the U.S. Govern-
ment. We would also like to thank our annotators
whose efforts have made this work possible.
References
K. Ahrens, S.F. Chung, and C. Huang. 2003. Concep-
tual metaphors: Ontology-based representation and
corpora driven mapping principles. In Proceedings
of the ACL 2003 workshop on Lexicon and figura-
tive language-Volume 14, pages 36?42. Association
for Computational Linguistics.
C. Biemann. 2006. Chinese whispers: an efficient graph
clustering algorithm and its application to natural lan-
34
guage processing problems. In Proceedings of the
First Workshop on Graph Based Methods for Natural
Language Processing, pages 73?80. Association for
Computational Linguistics.
D. Bogdanova. 2010. A framework for figurative lan-
guage detection based on sense differentiation. In Pro-
ceedings of the ACL 2010 Student Research Workshop,
pages 67?72. Association for Computational Linguis-
tics.
D. Bracewell, M. Tomlinson, and M. Mohler. 2013. De-
termining the conceptual space of metaphoric expres-
sions. In Computational Linguistics and Intelligent
Text Processing, pages 487?500. Springer.
C. Eilts and B. Lo?nneker. 2002. The Hamburg Metaphor
Database.
D. Fass. 1991. met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
S. Harabagiu and F. Lacatusu. 2005. Topic themes for
multi-document summarization. In Proceedings of the
28th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 202?209. ACM.
G. Hirst and D. St-Onge. 1998. Lexical chains as rep-
resentations of context for the detection and correction
of malapropism. In Christiane Fellbaum, editor, Word-
Net: An Electronic Lexical Database, pages 305?332.
MIT Press.
A.N. Katz. 1992. Psychological studies in metaphor pro-
cessing: extensions to the placement of terms in se-
mantic space. Poetics Today, pages 607?632.
W. Kintsch. 2000. Metaphor comprehension: A com-
putational theory. Psychonomic Bulletin & Review,
7(2):257?266.
G. Lakoff and M. Johnson. 1980. Metaphors we live by,
volume 111. Chicago London.
G. Lakoff. 1993. The contemporary theory of metaphor.
Metaphor and thought, 2:202?251.
G. Lakoff. 1994. Master metaphor list. University of
California.
T.K. Landauer and S.T. Dumais. 1997. A solution to
Plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review; Psychological Review,
104(2):211.
L. Li and C. Sporleder. 2010. Using gaussian mixture
models to detect figurative language in context. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 297?300.
Association for Computational Linguistics.
C. Lin and E. Hovy. 2000. The automated acquisi-
tion of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational
linguistics-Volume 1, pages 495?501. Association for
Computational Linguistics.
B. Lo?nneker. 2003. Is there a way to represent metaphors
in WordNets?: insights from the Hamburg Metaphor
Database. In Proceedings of the ACL 2003 workshop
on Lexicon and figurative language-Volume 14, pages
18?27. Association for Computational Linguistics.
J.H. Martin. 1990. A computational model of metaphor
interpretation. Academic Press Professional, Inc.
Z.J. Mason. 2004. CorMet: A computational, corpus-
based conventional metaphor extraction system. Com-
putational Linguistics, 30(1):23?44.
M.S. McGlone. 1996. Conceptual metaphors and figura-
tive language interpretation: Food for thought? Jour-
nal of memory and language, 35(4):544?565.
E. Niemann and I. Gurevych. 2011. The people?s web
meets linguistic knowledge: Automatic sense align-
ment of Wikipedia and WordNet. In Proceedings of
the 9th International Conference on Computational
Semantics (IWCS), pages 205?214. Citeseer.
W. Peters and Y. Wilks. 2003. Data-driven detection
of figurative language use in electronic language re-
sources. Metaphor and Symbol, 18(3):161?173.
E. Shutova, L. Sun, and A. Korhonen. 2010. Metaphor
identification using verb and noun clustering. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 1002?1010. Associ-
ation for Computational Linguistics.
E. Shutova. 2010. Models of metaphor in NLP. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 688?697. Asso-
ciation for Computational Linguistics.
E.V. Shutova. 2011. Computational approaches to fig-
urative language. Ph.D. thesis, University of Cam-
bridge.
S.L. Toral, M.R. Mart??nez-Torres, F. Barrero, and
F. Corte?s. 2009. An empirical study of the driving
forces behind online communities. Internet Research,
19(4):378?392.
R. Tourangeau and R.J. Sternberg. 1982. Understanding
and appreciating metaphors. Cognition, 11(3):203?
244.
A. Utsumi. 2011. Computational exploration of
metaphor comprehension processes using a semantic
space model. Cognitive science, 35(2):251?296.
Y. Wilks. 1978. Making preferences more active. Artifi-
cial Intelligence, 11(3):197?223.
P. Wolff and D. Gentner. 2000. Evidence for role-neutral
initial processing of metaphors. Journal of Experi-
mental Psychology: Learning, Memory, and Cogni-
tion, 26(2):529.
35
