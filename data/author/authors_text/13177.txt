XMG - An expressive formalism for describing tree-based grammars
Yannick Parmentier
INRIA / LORIA
Universite? Henri Poincare?
615, Rue du Jardin Botanique
54 600 Villers-Les-Nancy
France
parmenti@loria.fr
Joseph Le Roux
LORIA
Institut National
Polytechnique de Lorraine
615, Rue du Jardin Botanique
54 600 Villers-Les-Nancy
France
leroux@loria.fr
Beno??t Crabbe?
HCRC / ICCS
University of Edinburgh
2 Buccleuch Place
EH8 9LW,
Edinburgh, Scotland
bcrabbe@inf.ed.ac.uk
Abstract
In this paper1 we introduce eXtensible
MetaGrammar, a system that facilitates
the development of tree based grammars.
This system includes both (1) a formal lan-
guage adapted to the description of lin-
guistic information and (2) a compiler for
this language. It applies techniques of
logic programming (e.g. Warren?s Ab-
stract Machine), thus providing an effi-
cient and theoretically motivated frame-
work for the processing of linguistic meta-
descriptions.
1 Introduction
It is well known that grammar engineering is a
complex task and that factorizing grammar in-
formation is crucial for the rapid development,
the maintenance and the debugging of large scale
grammars. While much work has been deployed
into producing such factorizing environments for
standard unification grammars, less attention has
been paid to the issue of developing such environ-
ments for ?tree based grammars? that is, grammars
like Tree Adjoining Grammars (TAG) or Tree De-
scription Grammars where the basic unit of infor-
mation is a tree rather than a category encoded in
a feature structure.
For these grammars, two trends have emerged
to automatize tree-based grammar production:
systems based on lexical rules (see (Becker,
2000)) and systems based on combination of
classes (also called metagrammar systems, see
(Candito, 1999), (Gaiffe et al, 2002)).
1We are grateful to Claire Gardent for useful comments
on this work. This work is partially supported by an INRIA
grant.
In this paper, we present a metagrammar system
for tree-based grammars which differs from com-
parable existing approaches both linguistically and
computationally.
Linguistically, the formalism we introduce is
both expressive and extensible. In particularly, we
show that it supports the description and factor-
ization both of trees and of tree descriptions; that
it allows the synchronized description of several
linguistic dimensions (e.g., syntax and semantics)
and that it includes a sophisticated treatment of
the interaction between inheritance and identifier
naming.
Computationally, the production of a grammar
from a metagrammar is handled using power-
ful and well-understood logic programming tech-
niques. A metagrammar is viewed as an extended
definite clause grammar and compiled using a vir-
tual machine closely resembling the Warren?s Ab-
stract Machine. The generation of the trees satisfy-
ing a given tree description is furthermore handled
using a tree description solver.
The paper is structured as follows. We begin
(section 2) by introducing the linguistic formal-
ism used for describing and factorizing tree based
grammars. We then sketch the logic program-
ming techniques used by the metagrammar com-
piler (section 3). Section 4 presents some evalu-
ation results concerning the use of the system for
implementing different types of tree based gram-
mars. Section 5 concludes with pointers for fur-
ther research and improvements.
2 Linguistic formalism
As mentioned above, the XMG system produces a
grammar from a linguistic meta-description called
a metagrammar. This description is specified us-
ing the XMG metagrammar formalism which sup-
103
ports three main features:
1. the reuse of tree fragments
2. the specialization of fragments via in-
heritance
3. the combination of fragments by
means of conjunctions and disjunctions
These features reflect the idea that a metagrammar
should allow the description of two main axes: (i)
the specification of elementary pieces of informa-
tion (fragments), and (ii) the combination of these
to represent alternative syntactic structures.
Describing syntax In a tree-based metagram-
mar, the basic informational units to be handled
are tree fragments. In the XMG formalism, these
units are put into classes. A class associates a
name with a content. At the syntactic level, a con-
tent is a tree description2 . The tree descriptions
supported by the XMG formalism are defined by
the following tree description language:
Description ::= x ? y | x ?+ y | x ?? y |
x ? y | x ?+ y | x ?? y |
x[f :E] (1)
where x, y represent node variables, ? immediate
dominance (x is directly above y),?+ strict dom-
inance (x is above y), ?? large dominance (x is
above or equal to y), ? is immediate precedence,
?+ strict precedence, and ?? large precedence3 .
x[f :E] constrains feature f with associated ex-
pression E on node x (a feature can for instance
refer to the syntactic category of the node)4.
Tree fragments can furthermore be combined
using conjunction and/or disjunction. These
two operators allow the metagrammar designer to
achieve a high degree of factorization. Moreover
the XMG system also supports inheritance be-
tween classes, thus offering more flexibility and
structure sharing by allowing one to reuse and
specialize classes.
Identifiers? scope When describing a broad-
coverage grammar, dealing with identifiers scope
is a non-trivial issue.
In previous approaches to metagrammar com-
pilation ((Candito, 1999), (Gaiffe et al, 2002)),
2As we shall later see, a content can in fact be multi-
dimensional and integrate for instance both semantic and syn-
tax/semantics interface information.
3We call strict the transitive closure of a relation and large
the reflexive and transitive one.
4E is an expression, so it can be a feature structure: that?s
how top and bottom are encoded in TAG.
node identifiers had global scope. When design-
ing broad-coverage metagrammars however, such
a strategy quickly reduces modularity and com-
plexifies grammar maintenance. To start with, the
grammar writer must remember each node name
and its interpretation and in a large coverage gram-
mar the number of these node names amounts to
several hundreds. Further it is easy to use twice
the same name erroneously or on the contrary, to
mistype a name identifier, in both cases introduc-
ing errors in the metagrammar
In XMG, identifiers are local to a class and can
thus be reused freely. Global and semi-global (i.e.,
global to a subbranch in the inheritance hierar-
chy) naming is also supported however through a
system of import / export inspired from Object
Oriented Programming. When defining a class as
being a sub-class of another one, the XMG user
can specify which are the viewable identifiers (i.e.
which identifiers have been exported in the super-
class).
Extension to semantics The XMG formalism
further supports the integration in the grammar of
semantic information. More generally, the lan-
guage manages dimensions of descriptions so that
the content of a class can consists of several ele-
ments belonging to different dimensions. Each di-
mension is then processed differently according to
the output that is expected (trees, set of predicates,
etc).
Currently, XMG includes a semantic represen-
tation language based on Flat Semantics (see (Gar-
dent and Kallmeyer, 2003)):
Description ::= `:p(E1, ..., En) |
?`:p(E1, ..., En) | Ei  Ej (2)
where `:p(E1, ..., En) represents the predicate p
with parameters E1, .., En, and labeled `. ? is the
logical negation, and Ei  Ej is the scope be-
tween Ei and Ej (used to deal with quantifiers).
Thus, one can write classes whose content con-
sists of tree description and/or of semantic formu-
las. The XMG formalism furthermore supports the
sharing of identifiers across dimension hence al-
lowing for a straightforward encoding of the syn-
tax/semantics interface (see figure 1).
3 Compiling a MetaGrammar into a
Grammar
We now focus on the compilation process and on
the constraint logic programming techniques we
104
Figure 1: Tree with syntax/semantics interface
draw upon.
As we have seen, an XMG metagrammar con-
sists of classes that are combined. Provided these
classes can be referred to by means of names, we
can view a class as a Clause associating a name
with a content or Goal to borrow vocabulary from
Logic Programming. In XMG, this Goal will be
either a tree Description, a semantic Description,
a Name (class call) or a combination of classes
(conjunction or disjunction). Finally, the valua-
tion of a specific class can be seen as being trig-
gered by a query.
Clause ::= Name ? Goal (3)
Goal ::= Description | Name
| Goal ? Goal | Goal ?Goal (4)
Query ::= Name (5)
In other words, we view our metagrammar lan-
guage as a specific kind of Logic Program namely,
a Definite Clause Grammar (or DCG). In this
DCG, the terminal symbols are descriptions.
To extend the approach to the representation of
semantic information as introduced in 2, clause (4)
is modified as follows:
Goal ::= Dimension+=Description |
Name |
Goal ? Goal | Goal ? Goal
Note that, with this modification, the XMG lan-
guage no longer correspond to a Definite Clause
Grammar but to an Extended Definite Clause
Grammar (see (Van Roy, 1990)) where the sym-
bol += represents the accumulation of information
for each dimension.
Virtual Machine The evaluation of a query is
done by a specific Virtual Machine inspired by
the Warren?s Abstract Machine (see (Ait-Kaci,
1991)). First, it computes the derivations con-
tained in the description, i.e. in the Extended Def-
inite Clause Grammar, and secondly it performs
unification of non standard data-types (nodes,
node features for TAG). Eventually it produces
as an output a description, more precisely one de-
scription per dimension (syntax, semantics).
In the case of TAG, the virtual machine produces
a tree description. We still need to solve this de-
scription in order to obtain trees (i.e. the items of
the resulting grammar).
Constraint-based tree description solver The
tree description solver we use is inspired by
(Duchier and Niehren, 2000). The idea is to:
1. associate to each node x in the description an
integer,
2. then refer to x by means of the tuple
(Eqx,Upx,Downx,Leftx,Rightx) where Eqx
(respectively Upx, Downx, Leftx, Rightx) de-
notes the set of nodes in the description which
are equal, (respectively above, below, left, and
right) of x (see picture 2). Note that these sets
are set of integers.
Eq
Up
Down
Left
Right
Figure 2: node regions
The operations supported by the XMG language
(i.e. dominance, precedence, etc) are then con-
verted into constraints on these sets. For instance,
let us consider 2 nodes x and y of the description.
Assuming we associate x with the integer i and
y with j, we can translate the dominance relation
x ? y the following way5:
N i ? N j ? [N iEqUp ? N
j
Up ?
N iDown ? N
j
EqDown ?
N iLeft ? N
j
Left ?
N iRight ? N
j
Right]
This means that if x dominates y, then in a model,
(1) the set of integers representing nodes that are
equal or above x is included in the set of inte-
gers representing nodes that are strictly above y,
5N iEqUp corresponds to the disjoint union of N iEq and
N iUp, similarly for N jEqDown with N
i
Eq and N iDown.
105
(2) the dual holds, i.e. the set of integers repre-
senting nodes that are below x contains the set of
integers representing nodes that are equal or be-
low y, (3) the set of integers representing nodes
that are on the left of x is included in the set of
integers representing those on the left of y, and (4)
symmetrically for the nodes on the right6.
Parameterized constraint solver To recap 3
from a grammar-designer?s point of view, a
queried class needs not define complete trees but
rather a set of tree descriptions. The solver is then
called to generate all the matching valid minimal
trees from those descriptions. This feature pro-
vides the users with a way to concentrate on what
is relevant in the grammar, thus taking advantage
of underspecification, and to delegate the tiresome
work to the solver.
Actually, the solver can be parameterized to per-
form various checks or constraints on the tree de-
scriptions besides tree-shaping them. These pa-
rameters are called principles in the XMG termi-
nology. Some are specific to a target formalism
(e.g. TAG trees must have at most one foot node)
while others are independent. The most interesting
one is a resources/needs mechanism for node uni-
fication called color principle, see (Crabb?e and
Duchier, 2004).
At the end of this tree description solving pro-
cess we obtain the trees of the grammar. Note that
the use of constraint programming techniques to
solve tree descriptions allows us to compute gram-
mars faster than the previous approaches (see sec-
tion 4).
4 Evaluation
The XMG system has been successfully used by
linguists to develop a core TAG for French contain-
ing more than 6.000 trees. This grammar has been
evaluated on the TSNLP test-suite, with a cover-
age rate of 75 % (see (Crabb?e, 2005)). The meta-
grammar used to produce that grammar consists of
290 classes and is compiled by the XMG system
in about 16 minutes with a Pentium 4, 2.6 GHz
and 1 GB of RAM.7
XMG has also been used to produce a core
size Interaction Grammar for French (see (Perrier,
2003)).
6See (Duchier and Niehren, 2000) for details .
7Because this metagrammar is highly unspecifi ed, con-
straint solving takes about 12 min. Of course, subsets of the
grammar may be rebuilt separately.
Finally, XMG is currently used to develop a
TAG that includes a semantic dimension along the
line described in (Gardent and Kallmeyer, 2003).
5 Conclusion and Future Work
We have presented a system, XMG8, for produc-
ing broad-coverage grammars, system that offers
an expressive description language along with an
efficient compiler taking advantages from logic
and constraint programming techniques.
Besides, we aim at extending XMG to a generic
tool. That is to say, we now would like to obtain
a compiler which would propose a library of lan-
guages (each associated with a specific process-
ing) that the user would load dynamically accord-
ing to his/her target formalism (not only tree-based
formalisms, but others such as HPSG or LFG).
References
H. Ait-Kaci. 1991. Warren?s abstract machine: A tu-
torial reconstruction. In Proc. of the Eighth Interna-
tional Conference of Logic Programming.
T. Becker. 2000. Patterns in metarules. In A. Abeille
and O. Rambow, editors, Tree Adjoining Grammars:
formal, computational and linguistic aspects. CSLI
publications, Stanford.
M.H. Candito. 1999. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques lex-
icalise?es : application au franc?ais et a` l?italien.
Ph.D. thesis, Universit?e Paris 7.
B. Crabb?e and D. Duchier. 2004. Metagrammar redux.
In CSLP 2004, Copenhagen.
B. Crabb?e. 2005. Repr?esentation informatique de
grammaires fortement lexicalise?es : Application a`
la grammaire d?arbres adjoints. Ph.D. thesis, Uni-
versit?e Nancy 2.
D. Duchier and J. Niehren. 2000. Dominance
constraints with set operators. In Proceedings of
CL2000.
B. Gaiffe, B. Crabb?e, and A. Roussanaly. 2002. A new
metagrammar compiler. In Proceedings of TAG+6.
C. Gardent and L. Kallmeyer. 2003. Semantic con-
struction in ftag. In Proceedings of EACL?03.
Guy Perrier. 2003. Les grammaires d?interaction.
HDR en informatique, Universit?e Nancy 2.
P. Van Roy. 1990. Extended dcg notation: A tool for
applicative programming in prolog. Technical re-
port, Technical Report UCB/CSD 90/583, Computer
Science Division, UC Berkeley.
8XMG is freely available at http://sourcesup.
cru.fr/xmg .
106
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A constraint driven metagrammar
Joseph Le Roux
LORIA
Institut National
Polytechnique de Lorraine
615, Rue du Jardin Botanique
54 600 Villers-Le`s-Nancy
France
leroux@loria.fr
Beno??t Crabbe?
HCRC / ICCS
University of Edinburgh
2 Buccleuch Place
EH8 9LW,
Edinburgh, Scotland
bcrabbe@inf.ed.ac.uk
Yannick Parmentier
INRIA / LORIA
Universite? Henri Poincare?
615, Rue du Jardin Botanique
54 600 Villers-Le`s-Nancy
France
parmenti@loria.fr
Abstract
We present an operational framework al-
lowing to express a large scale Tree Ad-
joining Grammar (TAG) by using higher
level operational constraints on tree de-
scriptions. These constraints first meant
to guarantee the well formedness of the
grammatical units may also be viewed as
a way to put model theoretic syntax at
work through an efficient offline grammat-
ical compilation process. Our strategy pre-
serves TAG formal properties, hence en-
sures a reasonable processing efficiency.
1 Introduction
This paper is concerned with the semi-automatic
grammar development of real-scale grammars.
For natural language syntax, lexicalised TAGs are
made of thousands of trees, carrying an extreme
structural redundancy. Their development and
their maintenance is known to be cumbersome as
the size of the grammar raises significantly.
To counter the lack of generalisations inher-
ent to strong lexicalisation, various proposals for
semi-automatic grammar development have been
carried out: lexical rules or meta-rules (Becker,
2000) and metagrammars: (Candito, 1999; Gaiffe
et al, 2002; Xia, 2001). The aim of these frame-
works is twofold: expressing general facts about
the grammar of a language and factorising the in-
formation to avoid redundancy.
The metagrammar path adopts a different per-
spective from the lexical rule based grammar de-
velopment: instead of describing how a derived
tree is different from a canonical one, grammati-
cal description mainly consists of combining frag-
mentary tree descriptions or building blocks.
The paper is structured as follows. We start
in section 2 by providing motivations and back-
ground information on the framework we are us-
ing. Section 3 shows that the metagrammar frame-
work may be viewed as an offline system allowing
to express high level well-formedness constraints
on elementary grammatical structures while pre-
serving TAG computational and formal proper-
ties. Section 4 shows how to implement effi-
ciently this constraint-based approach with logic
programming techniques and finally section 5 pro-
vides an idea of the performance of the imple-
mented system.
2 eXtensible MetaGrammar (XMG)
By opposition to other metagrammatical frame-
works, XMG (Duchier et al, 2004) uses an expres-
sive though simple language, enabling a mono-
tonic description of a real scale grammar. Mono-
tonicity is important because it means that the or-
der of application of the different operations does
not matter. This is the major drawback of lexical-
rule systems. Moreover, (Crabb e?, 2005b) shows
that it is sufficiently expressive to implement con-
veniently a core TAG for French.
XMG allows the grammar writer to manipulate
tree descriptions through a control language. The
intuition behind is that a metagrammatical lan-
guage needs to provide means to describe syn-
tactic information along two methodological axis
(Crabb e?, 2005b): structure sharing and alterna-
tives. Structure sharing is the axis dedicated to
express factorisation in the grammar, whereas al-
ternatives allow to express regular alternation re-
lationships such as alternatives between the rep-
resentation of a canonical nominal subject and its
interrogative representation, or between an active
9
and a passive verb form1.
Building on this intuition the XMG language al-
lows the user to name partial tree descriptions
within classes. The name of the class can be ma-
nipulated afterwards. For instance the following
tree descriptions on the right of the arrow are as-
sociated with the names stated on the left of the
arrow2:
(1) a. CanonicalSubject ?
S
N? V
b. RelativisedSubject ?
N
N* S
N? V
c. VerbalForm ?
S
V
Naming is the main device that allows the gram-
mar writer to express and to take advantage of the
structure sharing axis mentioned above. Indeed
class names can be reused in other descriptions.
Thus names can also be used to describe alterna-
tives. To express, in our simplified example, that a
Subject is an abstract way to name a Relativised-
Subject or a CanonicalSubject, we use a choice op-
erator (?) as illustrated below:
(2) Subject ? CanonicalSubject
? RelativisedSubject
Disjunction (non-deterministic choice) is the de-
vice provided by the language to express the
methodological axis of alternatives.
Finally, names can be given to class combina-
tions. To express the composition of two tree de-
scriptions in the language, we use the ? operator.
1The passive is a semi-regular alternation, many transi-
tive verbs do not passivise. Our system presupposes a classi-
cal architecture for the computational representation of Tree
Adjoining Grammars such as XTAG, where means to ex-
press such exceptions during the anchoring process are well-
known. In what follows, we therefore consider only tree tem-
plates (or tree schematas) as our working units. Finally the
trees depicted in this paper take their inspiration from the
grammar described by (Abeill e?, 2002).
2To represent the tree descriptions mentioned in this pa-
per, we use a graphical notation. Immediate dominance is de-
picted with a straight line and precedence follows the graphi-
cal order. Note that nodes are decorated with their labels only,
ignoring the names of the variables denoting them. Note also
that we use only the reflexive transitive closure of precedence
between sibling nodes and it is explicitly stated with the sym-
bol ??.
Thus we can say that an IntransitiveVerb is made
by the composition of a Subject and a VerbalForm
as follows:
(3) IntransitiveVerb ? Subject ? VerbalForm
Given these 3 primitives, the control language
is naturally interpreted as a context free grammar
whose terminals are tree descriptions and where
our composition plays the role of concatenation.
This abstract grammar or metagrammar is further
restricted to be non recursive in order to ensure
that the generated TAG is finite.
Provided the axiom IntransitiveVerb, an inter-
preter for this language generates non determinis-
tically all the sentences of the grammar3 underly-
ing a grammatical description. Thus in our current
example the two sentences generated are those de-
picted on the left hand side of the arrows in Figure
1. On the right hand side of the arrow is depicted
the result of the composition of the tree descrip-
tions.
It remains to make clear what is actually this
composition. The grammatical classes may con-
tain information on tree descriptions and/or ex-
press composition of descriptions stated in other
classes. Tree descriptions take their inspiration
from the logic described in (Rogers and Vijay-
Shanker, 1994). Its syntax is the following:
Description ::= x ? y | x ?? y |
x ? y | x ?? y |
x[f :E]
where x, y are node variables, ? the dominance
relation, ? the precedence relation, ? denoting the
reflexive transitive closure of a relation. The last
line associates x with a feature f whose value is
the result of evaluating expression E.
Tree descriptions are interpreted as finite linear
ordered trees being the minimal models of the de-
scription.
Using tree descriptions, the above mentioned
operation of tree ?composition? breaks down to a
conjunction of formulas where variables of each
conjunct are in first approximation renamed to
avoid name collisions. Renaming is a crucial dif-
ference with previous approaches to metagrammar
(Candito, 1999; Xia, 2001) where the user had to
manage explicitly a ?global namespace?. Here a
specific attention is given to namespace manage-
ment, because this was a bottleneck for real scale
3Understood as compositions of tree fragments.
10
SN? V
Le garc?on. . .
The boy. . .
?
S
V
dort
sleeps
?
S
N? V
Le garc?on dort
The boy who sleeps
N
N* S
N? V
(Le garc?on) qui. . .
(The boy) who. . .
?
S
V
dort
sleeps
?
N
N* S
N? V
Le garc?on qui dort
The boy who sleeps
Figure 1: Interpretation of a grammatical description
grammar design. More precisely each class has
its own namespace of identifiers and namespace
merging can be triggered when a class combina-
tion occurs. This merging relies on a fine-grained
import/export mechanism.
In addition to conjunction and disjunction, XMG
is augmented with syntactic sugar to offer some
of the features other metagrammatical formalisms
propose. For instance, inheritance of classes is not
built-in in the core language but is realised through
conjunction and namespace import. Of course,
this restricts users to monotonic inheritance (spe-
cialisation) but it seems to be sufficient for most
linguists.
3 Constraining admissible structures
XMG has been tested against the development of a
large scale French Grammar (Crabb e?, 2005a). To
ease practical grammatical development we have
added several augmentations to the common tree
description language presented so far in order to
further restrict the class of admissible structures
generated by the metagrammar.
Further constraining the structures generated by
a grammar is a common practice in computational
linguistics. For instance a Lexical Functional
Grammar (Bresnan and Kaplan, 1982) further re-
stricts the structures generated by the grammar by
means of a functional uniqueness and a functional
completeness principles. These constraints further
restricts the class of admissible structures gener-
ated by an LFG grammar to verify valency condi-
tions.
For TAG and in a theoretical context, (Frank,
2002) states a set of such well formedness prin-
ciples that contribute to formulate a TAG theory
within a minimalist framework. In what remains
we describe operational constraints of this kind
that further restrict the admissibility of the struc-
ture generated by the metagrammar. By contrast
with the principles stated by (Frank, 2002), we
do not make any theoretical claim, instead we
are stating operational constraints that have been
found useful in practical grammar development.
However as already noted by (Frank, 2002) and
by opposition to an LFG framework where con-
straints apply to the syntactic structure of a sen-
tence as a whole, we formulate here constraints on
the well-formedness of TAG elementary trees. In
other words these constraints apply to units that
define themselves their own global domain of lo-
cality. In this case, it means that we can safely
ignore locality issues while formulating our con-
straints. This is theoretically weaker than formu-
lating constraints on the whole sentential structure
but this framework allows us to generate common
TAG units, preserving the formal and computa-
tional properties of TAG.
We formulate this constraint driven framework
by specifying conditions on model admissibility.
Methodologically the constraints used in the de-
velopment of the French TAG can be classified
in four categories: formal constraints, operational
constraints, language dependent constraints and
theoretical principles.
First the formal constraints are those constrain-
ing the trees generated by the model builder to
be regular TAG trees. These constraints require
the trees to be linear ordered trees with appropri-
ate decorations : each node has a category label,
leaf nodes are either terminal, foot or substitution,
there is at most one foot node, the category of the
foot note is identical to that of the root node, each
tree has at least one leaf node which is an anchor.
11
It is worth noting here that using a different set
of formal constraints may change the target for-
malism. Indeed XMG provides a different set of
formal constraints (not detailed here) that allow to
generate elementary units for another formalism,
namely Interaction Grammars.
The second kind of constraint is a single op-
erational constraint dubbed the colouration con-
straint. We found it convenient in the course
of grammar development. It consists of associ-
ating colour-based polarities to the nodes to en-
sure a proper combination of the fragmentary
tree descriptions stated within classes. Since in
our framework descriptions stated in two different
classes are renamed before being conjoined, given
a formula being the conjunction of the two follow-
ing tree descriptions :
(4)
X
W Z
X
Z Y
both the following trees are valid models of that
formula:
(5) (a)
X
W Z Y (b)
X
W Z Z Y
In the context of grammar development, however,
only (a) is regarded as a desired model. To rule out
(b) (Candito, 1999; Xia, 2001) use a naming con-
vention that can be viewed as follows4: they assign
a name to every node of the tree description. Both
further constrain model admissibility by enforcing
the identity of the interpretation of two variables
associated to the same name. Thus the description
stated in their systems can be exemplified as fol-
lows:
(6)
Xa
Wb Zc
Xa
Zc Yd
Though solving the initial formal problem, this de-
sign choice creates two additional complications:
(1) it constrains the grammar writer to manually
manage a global naming, entailing obvious prob-
lems as the size of the grammatical description
grows and (2) it prevents the user to reuse sev-
eral times the same class in a composition. This
case is a real issue in the context of grammati-
cal development since a grammar writer willing
to describe a ditransitive context with two prepo-
sitional phrases cannot reuse two times a fragment
4They actually use a different formal representation that
does not affect the present discussion.
describing such a PP since the naming constraint
will identify them.
To solve these problems we use a colouration
constraint. This constraint associates unary prop-
erties, colours, to every node of the descriptions.
A colour is taken among the set red(?R), black(?B ),
white (?W). A valid model is a model in which ev-
ery node is coloured either in red or black. Two
variables in the description interpreted by the same
node have their colours merged following the table
given in Figure 2.
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
Figure 2: Colour identification rules.
The table indicates the resulting colour after
a merge. The ? symbol indicates that this two
colours cannot be merged and hence two nodes la-
belled with these colours cannot be merged. Note
that the table is designed to ensure that merging is
not a procedural operation.
The idea behind colouration is that of saturat-
ing the tree description. The colour white repre-
sents the non saturation or the need of a node to
be combined with a resource, represented by the
colour black. Black nodes need not necessarily
be combined with other nodes. Red is the colour
used to label nodes that cannot be merged with
any other node. A sample tree description with
coloured node is as follows:
(7)
X?B
W?R Z?B
X?W
Z?W Y?R
Colours contribute to rule out the (b) case and re-
move the grammar writer the burden of managing
manually a ?global namespace?.
The third category of constraints are language
dependent constraints. In the case of French, such
constraints are clitic ordering, islands constraints,
etc. We illustrate these constraints with clitic or-
dering in French. In French clitics are non tonic
particles with two specific properties already iden-
tified by (Perlmutter, 1970): first they appear in
front of the verb in a fixed order according to their
rank (8a-8b) and second two different clitics in
front of the verb cannot have the same rank (8c).
For instance the clitics le, la have the rank 3 and
lui the rank 4.
12
SN? V??+ ?
V?
Cl?3 V?+ ?
V?
Cl?4 V?+ ?
S
V?
V ?
S
N? V?
Cl?3 Cl?4 V
S
N? V?
Cl?4 Cl?3 V
Figure 3: Clitic ordering
(8) a. Jean le3 lui4 donne
John gives it to him
b. *Jean lui4 le3 donne
*John gives to him it
c. *Jean le3 la3 donne
*John gives it it
In the French grammar of (Crabb e?, 2005a) trees
with clitics are generated with the fragments illus-
trated on the left of the arrow in Figure 35. As
illustrated on the right of the arrow, the composi-
tion may generate ill-formed trees. To rule them
out we formulate a clitic ordering constraint. Each
variable labelled with a clitic category is also la-
belled with a property, an integer representing its
rank. The constraint stipulates that sibling nodes
labelled with a rank have to be linearly ordered ac-
cording to the order defined over integers.
Overall language dependent constraints handle
cases where the information independently spec-
ified in different fragments may interact. These
interactions are a counterpart in a metagrammar to
the interactions between independently described
lexical rules in a lexical rule based system. As-
suming independent lexical rules moving canoni-
cal arguments (NP or PP) to their clitic position,
lexical rules fall short for capturing the relative or-
dering among clitics6 .
A fourth category of constraints, not imple-
mented in our system so far are obviously the lan-
guage independent principles defining the theory
underlying the grammar. Such constraints could
involve for instance a Principle of Predicate Argu-
ment Coocurrency (PPAC) or even the set of min-
imalist principles described by (Frank, 2002).
4 Efficient implementation
We describe now the implementation of our meta-
grammatical framework. In particular, we will fo-
5Colours are omitted.
6This observation was already made by (Perlmutter, 1970)
in a generative grammar framework where clitics where as-
sumed to be moved by transformations.
cus on the implementation of the constraints dis-
cussed above within XMG.
As mentioned above, a metagrammar corre-
sponds to a reduced description of the grammar.
In our case, this description consists of tree frag-
ments combined either conjunctively or disjunc-
tively. These combinations are expressed using
a language close to the Definite Clause Grammar
formalism (Pereira and Warren, 1980), except that
partial tree descriptions are used as terminal sym-
bols. In this context, a metagrammar can be re-
duced to a logic program whose execution will
lead to the computation of the trees of the gram-
mar.
To perform this execution, a compiler for our
metagrammatical language has been implemented.
This compilation is a 3-step process as shown in
Figure 4.
First, the metagrammar is compiled into in-
structions for a specific virtual machine inspired
by the Warren?s Abstract Machine (Ait-Kaci,
1991). These instructions correspond to the un-
folding of the relations7 contained in the tree de-
scriptions of the metagrammar.
Then, the virtual machine performs unifications
of structures meant to refer to corresponding in-
formation within fragments (e.g. two nodes, two
feature structures ...). Note that the XMG?s virtual
machine uses the structure sharing technique for
memory management, i.e. data are represented by
a pair pattern ? environment in which to interpret
it. The consequences are that (a) we save mem-
ory when compiling the metagrammar, and (b) we
have to perform pointer dereferencing during uni-
fication. Even if the latter is time-consuming, it
remains more efficient than structure copying as
we have to possibly deal with a certain amount of
tree descriptions.
Eventually, as a result of this instruction pro-
cessing by the virtual machine, we obtain poten-
7These relations are either dominance or precedence be-
tween node variables, or their reflexive transitive closure, or
the labelling of node variable with feature structures.
13
STEP1
(translation of concrete syntax)
INTO INSTRUCTIONS
CONCRETE SYNTAX
METAGRAMMATICAL
COMPILATION OF 
TREE DESCRIPTION SOLVING
STEP3
(unification of data structures)
STEP2
A SPECIFIC VIRTUAL MACHINE
INSTRUCTIONS BY
EXECUTION OF THE 
INPUT: MetaGrammar
Total tree descriptions OUTPUT: TAGCompiled partial tree descriptions
Figure 4: Metagrammar compilation.
tially total tree descriptions, that have to be solved
in order to produce the expected TAG.
Now, we will introduce XMG?s tree description
solver and show that it is naturally designed to pro-
cess efficiently the higher level constraints men-
tioned above. In particular, we will see that the
description solver has been designed to be easily
extended with additional parametric admissibility
constraints.
4.1 Tree descriptions solving
To find the minimal models corresponding to the
total tree descriptions obtained by accumulating
fragmentary tree descriptions, we use a tree de-
scription solver. This solver has been developed in
the Constraint Programming paradigm using the
constraint satisfaction approach of (Duchier and
Niehren, 2000). The idea is to translate relations
between node variables into constraints over sets
of integers.
Basically, we refer to a node of the input de-
scription in terms of the nodes being equals,
above, below, or on its side (see Figure 5). More
precisely, we associate each node of the descrip-
tion with an integer, then our reference to a node
corresponds to a tuple containing sets of nodes (i.e.
sets of integers).
As a first approximation, let us imagine that we
refer to a node x in a model by means of a 5-tuple
N ix = (Eq, Up, Down, Left, Right) where i is an in-
teger associated with x and Eq (respectively Up,
Down, Left, Right) denotes the set of nodes8 in the
description which are equal, (respectively above,
below, left, and right) of x.
Then we can convert the relations between
nodes of our description language into constraints
on sets of integer.
8I.e. integers.
Eq
Up
Down
Left
Right
Figure 5: Node representation.
For instance, if we consider 2 nodes x and y of
the description. Assuming we associate x with the
integer i and y with j, we can translate the domi-
nance relation x ? y the following way9:
N ix? N jy?
[N ix.EqUp ? N jy.Up?N ix.Down ? N jy.EqDown
?N ix.Left ? N jy.Left?N ix.Right ? N
j
y.Right]
This means that if the node10 x strictly dominates
y in the input description, then (i) the set of nodes
that are above or equal x in a valid model is in-
cluded in the set of those that are strictly above y
and (ii) the dual holds for the nodes that are above
and (iii) the set of nodes that are on the left of y is
included in the set of those that are on the left of x
and (iv) similarly for the right part.
Once the constraints framework is settled, we
can search for the solutions to our problem, i.e.
the variable assignments for each of the sets of in-
tegers used to refer to the nodes of the input de-
scription. This search is performed by associating
with each pair of nodes (x, y) of the input descrip-
tion a choice variable denoting the mutually ex-
clusive relations11 between these two nodes. Then
9N ix.EqUp corresponds to the disjoint union of N ix.Eq and
N ix.Up, similarly for N jx.EqDown with N
i
x.Eq and N ix.Down.
10One should read the node denoted by the variable x.
11Either x equals y, x dominates y, y dominates x, x pre-
cedes y or y precedes x.
14
we use a search strategy to explore the consistent
assignments to these choices variables (and the as-
sociated assignments for sets of integers referring
to nodes)12 . Note that the strategy used in XMG
is a first-fail strategy which leads to very good re-
sults (see section 5 below). The implementation
of this solver has been done using the constraint
programming support of the Mozart Programming
System (The Oz-Mozart Board, 2005).
4.2 Extension to higher-level constraints
solving
An important feature of our approach is that this
system of constraints over integer sets can be
extended so that we not only ensure tree well-
formedness of the outputted trees, but also the re-
spect of linguistic properties such as the unique-
ness of clitics in French, etc.
The idea is that if we extend adequately our
node representation, we can find additional con-
straints that reflects the syntactic constraints we
want to express.
Clitic uniqueness For instance, let us consider
the clitic uniqueness constraint introduced above.
We want to express the fact that in a valid model
?, there is only one node having a given property
p (i.e. a parameter of the constraint, here the cat-
egory clitic13). This can be done by introducing,
for each node x of the description, a boolean vari-
able px indicating whether the node denoting x in
the model has this property or not. Then, if we call
V?p the set of integers referring to nodes having the
property p in a model, we have:
px ? (N ix.Eq ? V?p ) 6= ?
Finally, if we represent the true value with the in-
teger 1 and false with 0, we can sum the px for
each x in the model. When this sum gets greater
than 1, we can consider that we are not building a
valid model.
Colouration constraint Another example of the
constraints introduced in section 3 is coloura-
tion. Colouration represents operational con-
straints whose effect is to control tree fragment
combination. The idea is to label nodes with a
colour between red, black and white. Then, during
12More information about the use of such choice variables
is given in (Duchier, 1999)
13In fact, the uniqueness concerns the rank of the clitics,
see (Crabb e?, 2005b), ?9.6.3.
description solving, nodes are identified according
to the rules given previously (see Figure 2).
That is, red nodes are not identified with any
other node, white nodes can be identified with a
black one. Black nodes are not identified with
each other. A valid model in this context is a satu-
rated tree, i.e. where nodes are either black (possi-
bly resulting from identifications) or red. In other
words, for every node in the model, there is at most
one red or black node with which it has been iden-
tified. The implementation of such a constraint
is done the following way. First, the tuples rep-
resenting nodes are extended by adding a integer
field RB referring to the red or black node with
which the node has been identified. Then, con-
sidering the following sets of integers: VR, VB,
VW respectively containing the integers referring
to red, black and white nodes in the input descrip-
tion, the following constraints hold:
x ? VR ? N ix.RB = i ? N ix.Eq = {i} (a)
x ? VB ? N ix.RB = i (b)
x ? VW ? N ix.RB ? V?B (c)
where V?B represents the black nodes in a model,
i.e. V?B = V? ? VB. (a) expresses the fact that for
red nodes, N ix.RB is the integer i associated with
x itself, and N ix.Eq is a set only containing i. (b)
means that for black nodes, we have that N ix.RB is
also the integer i denoting x itself, but we cannot
say anything about N ix.Eq. Eventually (c) means
that whites nodes have to be identified with a black
one.
Thus, we have seen that Constraint Program-
ming offers an efficient and relatively natural way
of representing syntactic constraints, as ?all? that
has to be done is to find an adequate node repre-
sentation in terms of sets of nodes, then declare the
constraints associated with these sets, and finally
use a search strategy to compute the solutions.
5 Some features
There are two points worth considering here: (i)
the usability of the formalism to describe a real
scale grammar with a high factorisation, and (ii)
the efficiency of the implementation in terms of
time and memory use.
Concerning the first point, XMG has been used
successfully to compute a TAG having more than
6,000 trees from a description containing 293
15
classes14 . Moreover, this description has been de-
signed relatively quickly as the description lan-
guage is intuitive as advocated in (Crabb e?, 2005a).
Concerning the efficiency of the system, the
compilation of this TAG with more than 6,000 trees
takes about 15 min with a P4 processor 2.6 GHz
and 1 GB RAM. Note that compared with the
compilation time of previous approaches (Candito,
1999; Gaiffe et al, 2002) (with the latter, a TAG of
3,000 trees was compiled in about an hour), these
results are quite encouraging.
Eventually, XMG is released under the terms of
the GPL-like CeCILL license15 and can be freely
downloaded at http://sourcesup.cru.fr/xmg.
6 Conclusion
Unlike previous approaches, the description lan-
guage implemented by XMG is fully declara-
tive, hence allowing to reuse efficient techniques
borrowed to Logic Programming. The system
has been used successfully to produce core TAG
(Crabb e?, 2005b) and Interaction Grammar (Per-
rier, 2003) for French along with a core French
TAG augmented with semantics (Gardent, 2006).
This paper shows that the metagrammar can be
used to put model theoretic syntax at work while
preserving reasonably efficient processing proper-
ties. The strategy used here builds on constraining
offline a TAG whose units are elementary trees The
other option is to formulate constraints applied
on-line, in the course of parsing, applying on the
whole syntactic structure. In a dependency frame-
work, XDG followed this path (Debusmann et al,
2004), however it remains unknown to us whether
this approach remains computationally tractable
for parsing with real scale grammars.
References
A. Abeill e?. 2002. Une grammaire e?lectronique du franais.
CNRS Editions, Paris.
H. Ait-Kaci. 1991. Warren?s abstract machine: A tuto-
rial reconstruction. In K. Furukawa, editor, Proc. of the
Eighth International Conference of Logic Programming.
MIT Press, Cambridge, MA.
T. Becker. 2000. Patterns in metarules. In A. Abeille and
O. Rambow, editors, Tree Adjoining Grammars: formal,
computational and linguistic aspects. CSLI publications,
Stanford.
14I.e. tree fragments or conjunction / disjunction of frag-
ments
15More information about this license at http://www.
cecill.info/index.en.html.
Joan Bresnan and Ronal M. Kaplan. 1982. The Mental Rep-
resentation of Grammatical Relations. The MIT Press,
Cambridge MA.
M.H. Candito. 1999. Repre?sentation modulaire et
parame?trable de grammaires e?lectroniques lexicalise?es :
application au franc? ais et a` l?italien. Ph.D. thesis, Uni-
versit e? Paris 7.
B. Crabb e?. 2005a. Grammatical development with XMG.
Proceedings of the Fifth International Conference on Log-
ical Aspects of Computational Linguistics (LACL05).
B. Crabb e?. 2005b. Repre?sentation informatique de gram-
maires fortement lexicalise?es : Application a` la gram-
maire d?arbres adjoints. Ph.D. thesis, Universit e? Nancy
2.
R. Debusmann, D. Duchier, and G.-J. M. Kruijff. 2004. Ex-
tensible dependency grammar: A new methodology. In
Proceedings of the COLING 2004 Workshop on Recent
Advances in Dependency Grammar, Geneva/SUI.
D. Duchier and J. Niehren. 2000. Dominance constraints
with set operators. In Proceedings of CL2000, volume
1861 of Lecture Notes in Computer Science, pages 326?
341. Springer.
D. Duchier, J. Le Roux, and Y. Parmentier. 2004. The Meta-
grammar Compiler: An NLP Application with a Multi-
paradigm Architecture. In 2nd International Mozart/Oz
Conference (MOZ?2004), Charleroi.
D. Duchier. 1999. Set constraints in computational linguis-
tics - solving tree descriptions. In Workshop on Declara-
tive Programming with Sets (DPS?99), Paris, pp. 91 - 98.
Robert Frank. 2002. Phrase Structure Composition and Syn-
tactic Dependencies. MIT Press, Boston.
B. Gaiffe, B. Crabb e?, and A. Roussanaly. 2002. A new meta-
grammar compiler. In Proceedings of TAG+6, Venice.
C. Gardent. 2006. Int e?gration d?une dimension s e?mantique
dans les grammaires d?arbres adjoints. In Actes de La
13e`me e?dition de la confe?rence sur le TALN (TALN 2006).
F. Pereira and D. Warren. 1980. Definite clause grammars
for language analysis ?a survey of the formalism and a
comparison to augmented transition networks. Artificial
Intelligence, 13:231?278.
David Perlmutter. 1970. Surface structure constraints in syn-
tax. Linguistic Inquiry, 1:187?255.
Guy Perrier. 2003. Les grammaires d?interaction. HDR en
informatique, Universit e? Nancy 2.
J. Rogers and K. Vijay-Shanker. 1994. Obtaining trees from
their descriptions: An application to tree-adjoining gram-
mars. Computational Intelligence, 10:401?421.
The Oz-Mozart Board. 2005. The Oz-Mozart Programming
System. http://www.mozart-oz.org.
Fei Xia. 2001. Automatic Grammar Generation from two
Different Perspectives. Ph.D. thesis, University of Penn-
sylvania.
16
Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 25?32,
New York City, June 2006. c?2006 Association for Computational Linguistics
Increasing the coverage of a domain independent dialogue lexicon with
VERBNET
Benoit Crabbe??, Myroslava O. Dzikovska?, William de Beaumont?, Mary Swift?
? ICCS-HCRC, University of Edinburgh, 2 Buccleuch Place, EH8 9LW, Edinburgh, UK
{bcrabbe,mdzikovs}@inf.ed.ac.uk
?Department of Computer Science, University of Rochester, Rochester NY, USA
{wdebeaum,swift}@cs.rochester.edu
Abstract
This paper investigates how to extend cov-
erage of a domain independent lexicon tai-
lored for natural language understanding.
We introduce two algorithms for adding
lexical entries from VERBNET to the lexi-
con of the TRIPS spoken dialogue system.
We report results on the efficiency of the
method, discussing in particular precision
versus coverage issues and implications
for mapping to other lexical databases.
1 Introduction
This paper explores how different lexicons can be
integrated with the goal of extending coverage of
a deep parser and semantic interpreter. Lexical
semantic databases (Kipper et al, 2000; Johnson
and Fillmore, 2000; Dorr, 1997) use a frame-based
model of lexical semantics. Each database groups
words in classes where predicative words and their
arguments are described. The classes are generally
organised in an inheritance structure. Each such
database can be used, among other things, to per-
form semantic interpretation. However, their actual
structures are quite different, reflecting different un-
derlying methodological approaches to lexical de-
scription, and this results in representation that are
not directly compatible. Since no such database has
full coverage of English, it is worth combining them
in order to get a lexicon with better coverage and a
unified representation for English.
We explore the issues related to merging verb
descriptions from two lexical databases, which
have both syntactic and semantic incompatibilities,
and compare two techniques for aligning semantic
classes and the syntax-semantics mappings between
them. The resulting lexicon is to be used in precise
interpretation tasks, so its consistency and accuracy
are a high priority. Thus, though it is possible to gen-
erate lexical entries automatically (Kwon and Hovy,
2006; Swift, 2005), we use a semi-automatic method
in which an expert hand-checks the automatically
generated entries before adding them to the lexicon.
Therefore, our goal is to maximise the number of
new useful entries added to the lexicon while min-
imising the number of entries that are discarded or
hand-edited.
We take the mapping between the TRIPS lexicon
and the VERBNET lexical database as a case study
for our experiment. The TRIPS lexicon is used to-
gether with a parser to provide a natural language
understanding component for several dialogue ap-
plications in different domains. It outputs highly de-
tailed semantic representations suitable for complex
dialogue tasks such as problem-solving and tutoring
dialogue, inter alia. An essential feature of TRIPS
is the integration of a detailed lexical semantic rep-
resentation, semantic classes and theta role assign-
ments in the parsing process.
Semantic types and role labelling are helpful in
both deep (Tetreault, 2005) and shallow interpreta-
tion tasks (Narayanan and Harabagiu, 2004). TRIPS
provides a convenient test case because its grammar
is already equipped with the formal devices required
to build up a frame-based semantic representation
including this information.1
1While wide coverage grammars such as the English Re-
25
We chose VERBNET to extend the TRIPS lexicon
because it includes a detailed syntax-semantic map-
pings, thus providing a more convenient interface to
the syntactic component of the grammar than lexi-
cons where this connection is left unclear, such as
FRAMENET. However the methods described here
are designed to be reusable for merging other lexi-
cal databases, in particular we intend to experiment
with FRAMENET in the near future.
The plan of the paper is as follows: we first de-
scribe the target lexicon (Section 2) and the source
lexicon (Section 3) for our experiment before de-
scribing the methodology for integration (Section 4).
We finally present an evaluation of the techniques in
Section 5.
2 The TRIPS Lexicon
The TRIPS lexicon (Dzikovska, 2004) is the target
of the mapping procedure we describe in Section
4. It includes syntactic and semantic information
necessary to build semantic representations usable
in dialogue systems. The TRIPS parser is equipped
with a fairly detailed grammar, but a major restric-
tion on coverage in new domains is often lack of
lexical information. The lexicon used in our eval-
uation comprised approximately 700 verb lemmas
with 1010 senses (out of approximately 2500 total
word senses, covering both open- and closed-class
words). The lexicon is designed for incremental
growth, since the lexical representation is domain-
independent and the added words are then re-used
in new domains.
A graphical representation of the information
stored in the TRIPS lexicon and used in parsing is
shown in Figure 1. The lexicon is a list of canon-
ical word entries each of which is made of a set
of sense definitions comprised of a LF type and a
syntax-semantic template.
Semantic classes (LF types) in the TRIPS lexi-
con are organised in a domain-independent ontol-
ogy (the LF ontology). The LF Ontology was orig-
inally based on a simplified version of FRAMENET
source Grammar (Copestake and Flickinger, 2000) build deep
semantic representations which account for scoping and tempo-
ral structure, their lexicons do not provide information related
to word senses and role labels, in part due to the additional dif-
ficulty involved building a wide coverage lexicon with the nec-
essary lexical semantic information.
The tourists admired the paintings
LSUBJ LOBJ
LF::Experiencer-Emotion
LF::Experiencer LF::Theme
Figure 1: Information in the TRIPS word sense def-
inition for mapping between syntactic and semantic
roles.
(Baker et al, 1998; Dzikovska et al, 2004), with
each LF type describing a particular situation, object
or event and its participants. Syntax-Semantics Tem-
plates (or templates) capture the linking between the
syntax and semantics (LF type and semantic roles)
of a word. The semantic properties of an argument
are described by means of a semantic role assigned
to it and selectional restrictions.2
The TRIPS grammar contains a set of indepen-
dently described lexical rules, such as the passive or
dative shift rules, which are designed to create non-
canonical lexical entries automatically, while pre-
serving the linking properties defined in the canoni-
cal entry.
In this context adding an entry to the lexicon re-
quires determining both the list of LF types and
the list of templates for canonical contexts, that is,
the list of mappings between a logical frame and a
canonical subcategorization frame.
3 VERBNET
VERBNET (Kipper et al, 2000) provides an actual
implementation of the descriptive work carried out
by Levin (1993), which has been extended to cover
prepositional constructions and corpus-based sub-
categorization frames (Kipper et al, 2004; Kipper
et al, 2006).
VERBNET is a hierarchical verb lexicon in which
verbs are organised in classes. The fundamental
assumption underlying the classification is that the
members of a given class share a similar syntactic
2The selectional restrictions are domain independent and
specified using features derived from EuroWordNet (Vossen,
1997; Dzikovska et al, to appear).
26
behaviour, that is, they pattern in the same set of al-
ternations, and are further assumed to share common
semantic properties.3
VERBNET classes are organised in an inheritance
hierarchy. Each class includes a set of members
(verbs), a set of (subcategorization) frames and a set
of semantic descriptions. Frames are descriptions of
the linking between syntax and semantics for that
class. Each frame argument contains a syntactic cat-
egory augmented with syntactic features, and a cor-
responding thematic role. Each class also specifies
a set of additional selectional restriction features.
VERBNET further includes for each class a semantic
description stated in terms of event semantics, that
we ignore in this paper.
4 Methodology
The methodology used in the mapping process con-
sists of two steps. First we translate the source,
VERBNET, to an intermediate representation best
suited for parsing purposes. Second this interme-
diate representation is translated to a specific tar-
get, here the TRIPS lexicon. At this stage of our
work, the translation from VERBNET to the inter-
mediate representation mainly concerns normalising
syntactic information coded in VERBNET to make
them easier to handle for parsing purposes, and the
translation from the intermediate representation to
the TRIPS lexicon focuses on translating semantic
information. This architecture is best understood
as a cross compilation scheme: we further expect
to reuse this intermediate representation for produc-
ing outputs for different parsers and to accept inputs
from other lexical databases such as FRAMENET.
4.1 The intermediate representation
The intermediate representation is a lexical repre-
sentation scheme mainly tailored for parsing: in this
context, a lexicon is thus made of a set of words,
each of which consists of a lemma, a syntactic cate-
gory and a list of sense definitions. Each sense def-
inition has a name and a frame. The name of the
sense definition is actually the name of the VERB-
NET class it derives from. The frame of the sense
definition has a list of arguments, each of which con-
3In practice, it turns out that there are exceptions to that hy-
pothesis (Kipper, 2005).
sists of a syntactic category, a syntactic function, a
thematic role and possibly a set of prepositions and
syntactic feature structures.
The content of the intermediate representation
uses the following data categories. Syntactic cate-
gories, thematic roles and features are those used in
VERBNET. We further add the syntactic functions
described in (Carroll et al, 1998). Specifically, two
categories left implicit in VERBNET by the use of
feature structures are made explicit here: preposi-
tional phrases (PP) and sentential arguments (S).
Each argument described in a sense definition
frame is marked with respect to its coreness status.
The coreness status aims to provide the lexicon with
an operational account for common discrepancies
between syntax and semantics descriptions. This
status may be valued as core, non-core or non-sem
and reflects the status of the argument with respect
to the syntax-semantics interface.
Indeed, there is a methodological pitfall concern-
ing the mapping between thematic roles and syntac-
tic arguments: semantic arguments are not defined
following criteria identical to those for syntactic ar-
guments. The main criterion for describing semantic
arguments is their participation in the event, situa-
tion, object described by the frame whereas the cri-
terion for describing syntactic arguments is based on
the obligatoriness or the specificity of the argument
with respect to the verb. The following example il-
lustrates such conflicts:
(1) a. It is raining
b. I am walking to the store
The It in example (1a) plays no role in the seman-
tic representation, but is obligatory in syntax since
it fills a subject position. The locative PP in exam-
ple (1b) is traditionally not treated as an argument
in syntax, rather as a modifier, hence it does not fill
a complement position. Such phrases are, however,
classified in VERBNET as part of the frames. Fol-
lowing this, we distinguish three kinds of arguments:
non-sem as in (1a) are syntactic-only arguments with
no semantic contribution. non-core as in (1b) con-
tribute to the semantics but are not subcategorized.
27
4.2 From VERBNET to the intermediate
representation
Given VERBNET as described in Section 3 and the
intermediate representation we described above, the
translation process requires mainly (1) to turn the
class based representation of VERBNET into a list-
of-word based representation (2) to mark arguments
for coreness (3) to merge some arguments and (4) to
annotate arguments with syntactic functions.
The first step is quite straightforward. Every
member m of every VERBNET class C is associated
with every frame of C yielding a new sense defini-
tion in the intermediate representation for m.
In the second step, each argument receives a core-
ness mark. Arguments marked as non-core are ad-
verbs, and prepositional phrases introduced by a
large class of prepositions (e.g. spatial preposi-
tions). The arguments marked as non-sem are those
with an impersonal it, typically members of the
weather class. All other arguments listed in VERB-
NET frames are marked as core.
In the third step, syntactic arguments are merged
to correspond better to phrase-based syntax.4 For
example, the VERBNET encoding of subcategoriza-
tion frames splits prepositional frames on two slots:
one for the preposition and one for the noun phrase.
We have merged the two arguments, to become a
PP, also merging their syntactic and semantic fea-
tures. Other merges at this stage include merging
possessive arguments such as John?s brother which
are described with three argument slots in VERB-
NET frames. We merged them as a single NP.
The last step in the translation is the inference of
syntactic functions. It is possible to reasonably infer
syntactic functions from positional arguments and
syntactic categories by (a) considering the follow-
ing oblicity order over the set of syntactic functions
used in the intermediate representation:5
(2) NCSUBJ < DOBJ < OBJ2 < {IOBJ, XCOMP,CCOMP}
4We also relabel some categories for convenience without
affecting the process. For instance, VERBNET labels both
clausal arguments and noun phrases with the category NP. The
difference is made with syntactic features. We take advantage
of the features to relabel clausal arguments with the category S.
5This order is partial, such that the 3 last functions are un-
ordered wrt to each other. These functions are the subset of the
functions described in (Carroll et al, 1998) relevant for han-
dling VERBNET data.
and by (b) considering this problem as a transduc-
tion problem over two tapes. One tape being the tape
of syntactic categories and the second the tape of
syntactic functions. Given that, we designed a trans-
ducer that implements a category to function map-
ping. It implements the above oblicity order together
with an additional mapping constraint: nouns can
only map to NCSUBJ, DOBJ, prepositional phrases
can only map to OBJ2, IOBJ, infinitival clauses can
only map to XCOMP and finite clauses to CCOMP.
We further added refinements to account for
frames that do not encode their arguments follow-
ing the canonical oblicity order: for dealing with
dative shift encoded in VERBNET with two differ-
ent frames and for dealing with impersonal contexts,
so that we eventually used the transducer in Figure
2. All states except 0 are meant to be final. The
transduction operates only on core and non-sem ar-
guments, non-core arguments are systematically as-
sociated with an adjunct function. This transducer is
capable of correctly handling the majority of VERB-
NET frames, finding a functional assignment for
more than 99% of the instances.
0 1
2
NP:ncSubj
NP:
? 3
PP: Dobj, Iobj
PP:Iobj
PP:Iobj
S[inf]: Xcomp
S[fin]:Ccomp
S[inf]: Dobj, Xcomp
S[fin]: Dobj, Ccomp
S[fin
]:Cc
omp
S[in
f]:Xc
omp
it[+be]:SUBJ
NP: Iobj,Obj2
4
?:Dobj
Adj:Adj
Adj:Adj
Adj:Adj
Figure 2: A transducer for assigning syntactic func-
tions to ordered subcategorization frames
4.3 From Intermediate representation to TRIPS
Recall that a TRIPS lexical entry is comprised of an
LF type with a set of semantic roles and a template
representing the mappings from syntactic functions
to semantic roles. Converting from our intermedi-
ate representation to the TRIPS format involves two
steps:
28
? For every word sense, determine the appropri-
ate TRIPS LF type
? Establish the correspondence between VERB-
NET and TRIPS syntactic and semantic argu-
ments, and generate the appropriate mapping in
the TRIPS format.
We investigated two strategies to align semantic
classes (VERBNET classes and TRIPS LFs). Both
use a class intersection algorithm as a basis for deci-
sion: two semantic classes are considered a match if
they are associated with the same lexical items.
The intersection algorithm takes advantage of the
fact that both VERBNET and TRIPS contain lexical
sets. A lexical set for VERBNET is a class name
and the set of its members, for TRIPS it is an LF
type and the set of words that are associated with it
in the lexicon. Our intersection algorithm computes
the intersection between every VERBNET lexical set
and every TRIPS lexical set. The sets which intersect
are then considered as candidate mappings from a
VERBNET class to a TRIPS class.
However, this technique produces many 1-word
class intersections, and leads to spurious entries. We
considered two ways of improving precision: first
by requiring a significantly large intersection, sec-
ond by using syntactic structure as a filter. We dis-
cuss them in turn.
4.4 Direct Mapping Between Semantic
Representations
The first technique which we tried for mapping
between TRIPS and VERBNET semantic represen-
tations is to map the classes directly. We con-
sider all candidate mappings between the TRIPS
and VERBNET classes, and take the match with the
largest intersection. We then align the semantic roles
between the two classes and produce all possible
syntax-semantics mappings specified by VERBNET.
This technique has the advantage of providing the
most complete set of syntactic frames and syntax-
semantics mappings which can be retrieved from
VERBNET. However, since VERBNET lists many
possible subcategorization frames for every word,
guessing the class incorrectly is very expensive, re-
sulting in many spurious senses generated. We use a
class intersection threshold to improve reliability.
VERBNET ROLE TRIPS ROLES
Theme LF::THEME, LF::ADDRESSEE,
LF::ALONG, LF::ENTITY
Cause LF::CAUSE, LF::THEME
Experiencer LF::EXPERIENCER, LF::COGNIZER
Source LF::FROM-LOC, LF::SOURCE,
LF::PATH
Destination LF::GOAL, LF::TO-LOC
Recipient LF::RECIPIENT, LF::ADDRESSEE,
LF::GOAL
Instrument LF::INSTRUMENT
Table 1: Sample VERBNET to TRIPS role mappings
At present, we count an LF type match as suc-
cessfully guessed if there is an intersection in lex-
ical entries above the threshold (we determined 3
words as a best value by finding an optimal balance
of precision/recall figures over a small gold-standard
mapping set). Since the classes contain closely re-
lated items, larger intersection means a more reliable
mapping. If the VERBNET class is not successfully
mapped to an LF type then no TRIPS lexical entry is
generated.
Once the correspondence between the LF type
and the VERBNET class has been established, se-
mantic arguments have to be aligned between the
two classes. We established a role mapping table
(a sample is shown in Table 1), which is an extended
version of the mapping from Swift (2005). The role
mapping is one to many (each VERBNET role maps
to 1 to 8 TRIPS roles), however, since the appropriate
LF type has been identified prior to argument map-
ping, we usually have a unique mapping based on
the roles defined by the LF type.6
Once the classes and semantic roles have been
aligned, the mapping of syntactic functions between
the intermediate representation and TRIPS syntax
is quite straightforward. Functional and category
mappings are one to one and do not raise specific
problems. Syntactic features are also translated into
TRIPS representation.
To illustrate the results obtained by the automatic
mapping process, two of the sense definitions gener-
ated for the verb relish are shown in Figure 3. The
TRIPS entries contain references to the class descrip-
tion in the TRIPS LF ontology (line introduced by
6In rare cases where more than 1 correspondence is possible,
we are using the first value in the intersection as the default.
29
;; entries
(relish
(SENSES
((EXAMPLE "The tourists admired the paintings")
(LF-PARENT LF::EXPERIENCER-EMOTION)
(TEMPL VN-EXPERIENCER-THEME-TEMPL-84))
((EXAMPLE "The children liked that the clown had a red nose")
(LF-PARENT LF::EXPERIENCER-EMOTION)
(TEMPL VN-EXPERIENCER-THEME-XP-TEMPL-87))
))
;;Templates
(VN-EXPERIENCER-THEME-TEMPL-84
(ARGUMENTS
(LSUBJ (% NP) LF::EXPERIENCER)
(LOBJ (% NP) LF::THEME)
))
(VN-EXPERIENCER-THEME-XP-TEMPL-87
(ARGUMENTS
(LSUBJ (% NP) LF::EXPERIENCER)
(LCOMP (% CP (vform fin) (ctype s-finite)) LF::THEME)
))
Figure 3: Sample TRIPS generated entries
LF-PARENT) and to a template (line introduced by
TEMPL) generated on the fly by our syntactic con-
version algorithm. The first sense definition and
template in Figure 3 represent the same information
shown graphically in Figure 1. Each argument in a
template is assigned a syntactic function, a feature
structure describing its syntactic properties, and a
mapping to a semantic role defined in the LF type
definition (not depicted here).
4.5 Filtering with syntactic structure
The approach described in the previous section pro-
vides a fairly complete set of subcategorization
frames for each word, provided that the class corre-
spondence has been established successfully. How-
ever, it misses classes with small intersections and
classes for which some but not all members match
(see Section 5 for discussion). To address these is-
sues we tried another approach that automatically
generates all possible class matches between TRIPS
and VERBNET, again using class member intersec-
tion, but using the a TRIPS syntactic template as an
additional filter on the class match. For each poten-
tial match, a human evaluator is presented with the
following:
{confidence score
{verbs in TRIPS-VN class intersection}/
LF-type TRIPS-template
=> VN-class: {VN class members}}
The confidence score is based on the number of
verbs in the intersection, weighted by taking into ac-
count the number of verbs remaining in the respec-
tive TRIPS and VERBNET classes. The template
used for filtering is taken from all templates that oc-
cur with the TRIPS words in this intersection (one
match per template is generated for inspection). For
example:
93.271%
{clutch,grip,clasp,hold,wield,grasp}/
lf::body-manipulation agent-theme-xp-templ
=> hold-15.1-1: {handle}
This gives the evaluator additional syntactic in-
formation to make the judgement on class intersec-
tions. The evaluator can reject entire class matches,
or just individual verbs from the VERBNET class
which don?t quite fit an otherwise good match. We
only used the templates already in TRIPS (those cor-
responding to each of the word senses in the inter-
section) to avoid overwhelming the evaluator with a
large number of possibly spurious template matches
resulting from an incorrect class match. This tech-
nique allows us to pick up class matches based on a
single member intersection, such as:
7.814%
{swallow}/
lf::consume agent-theme-xp-templ
=> gobble-39.3-2: {gulp,guzzle,quaff,swig}
However, the entries obtained are not guaranteed
to cover all frames in VERBNET because if a given
alternation is not already covered in TRIPS, it is not
derived from VERBNET with this method.
5 Evaluation and discussion
Since our goal in this evaluation is to balance the
coverage of VERBNET with precision, we corre-
spondingly evaluate along those two dimensions.
For both techniques, we evaluate how many word
senses were added, and the number of different
words defined and VERBNET classes covered. As a
measure of precision we use, for those entries which
were retrieved, the percentage of those which could
be taken ?as is? (good entries) and the percentage of
entries which could be taken with minor edits (for
example, changing an LF type to a more specific
subclass, or changing a semantic role in a template).
The results of evaluation are shown in Table 2.7
Since for mapping with syntax filtering we con-
sidered all possible TRIPS-VERBNET intersections,
it in effect presents an upper bound the number of
words shared between the two databases. Further
7
?nocos? table rows exclude the other cos VERBNET class,
which is exceptionally broad and skews evaluation results.
30
Class mapping Mapping with syntax filtering
Type Total Good Edit Bad %usable Total Good Edit Bad %usable
Sense 3075 1000 196 1879 0.39 11036 1688 87 9261 0.16
Word 744 274 98 372 0.5 2138 1211 153 714 0.64
Class 15 10 1 4 0.73 198 129 2 67 0.66
Sense-nocos 1136 654 196 286 0.75 7989 1493 87 6409 0.20
Word-nocos 422 218 98 106 0.75 1763 1059 153 491 0.69
Class-nocos 14 9 1 4 0.71 197 128 2 67 0.65
Table 2: Evaluation results for different acquisition techniques. %usable = (good + editable) / bad?.
extension would require extending the TRIPS LF
Ontology with additional types to cover the miss-
ing classes. As can be seen from this table, 65%
of VERBNET classes have an analogous class in
TRIPS. At the same time, there is a very large num-
ber of class intersections possible, so if all possible
intersections are generated, only a very small per-
centage of generated word senses (16%) is usable in
the combined system. Thus developing techniques
to filter out the irrelevant senses and class matches
is important for successful hand-checking.
Our evaluation also shows that while class inter-
section with thresholding provides higher precision,
it does not capture many words and verb senses. One
reason for this is data sparsity. TRIPS is relatively
small, and both TRIPS and VERBNET contain a
number of 1-word classes, which cannot be reliably
mapped without human intervention. This problem
can be alleviated in part as the size of the database
grows. We expect this technique to have better recall
when the combined lexicon is used to merge with a
different lexical database such as FRAMENET.
However, a more difficult issue to resolve is dif-
ferences in class structure. VERBNET was built
around the theory of syntactic alternations, while
TRIPS used FRAMENET structure as a starting point,
simplifying the role structure to make connection
to parsing more straightforward (Dzikovska et al,
2004). Therefore TRIPS does not require that all
words associated with the same LF type share syn-
tactic behaviour, so there are a number of VERB-
NET classes with members which have to be split
between different TRIPS classes based on additional
semantic properties. 70% of all good matches in the
filtering technique were such partial matches. This
significantly disadvantages the thresholding tech-
nique, which provides the mappings on class level,
not allowing for splitting word entries between the
classes.
We believe that the best solution can be found
by combining these two techniques. The thresh-
olding technique could be used to establish reliable
class mappings, providing classes where many en-
tries could be transferred ?as is?. The mapping can
then be examined to determine incorrect class map-
pings as well as the cases where classes should be
split based on individual words. For those entries
judged reliable in the first pass, the syntactic struc-
ture can be transferred fully and quickly, while the
syntactic filtering technique, which requires more
manual checking, can be used to transfer other en-
tries in the intersections where class mapping could
not be established reliably.
Establishing class and member correspondence is
a general problem with merging any two semantic
lexicons. Similar issues have been noted in compar-
ing FRAMENET and VERBNET (Baker and Ruppen-
hofer, 2002). A method recently proposed by Kwon
and Hovy (2006) aligns words in different seman-
tic lexicons to WordNet senses, and then aligns se-
mantic roles based on those matches. Since we are
designing a lexicon for semantic interpretation, it is
important for us that all words should be associated
with frames in a shared hierarchy, to be used in fur-
ther interpretation tasks. We are considering using
this alignment technique to further align semantic
classes, in order to produce a shared database for in-
terpretation covering words from multiple sources.
6 Conclusion
In this paper, we presented a methodology for merg-
ing lexicons including syntactic and lexical semantic
31
information. We developed a model based on cross-
compilation ideas to provide an intermediate repre-
sentation which could be used to generate entries
for different parsing formalisms. Mapping semantic
properties is the most difficult part of the process,
and we evaluated two different techniques for estab-
lishing correspondence between classes and lexical
entries, using TRIPS and VERBNET lexicons as a
case study. We showed that a thresholding technique
has a high precision, but low recall due to inconsis-
tencies in semantic structure, and data sparsity. We
can increase recall by partitioning class intersections
more finely by filtering with syntactic structure. Fur-
ther refining the mapping technique, and using it
to add mappings to other lexical databases such as
FRAMENET is part of our ongoing work.
Acknowledgements
We thank Karin Kipper for providing us useful doc-
umentation on the VERBNET feature system, and
Charles Callaway for technical help with the final
version. This material is based on work supported
by grants from the Office of Naval Research un-
der numbers N000140510048 and N000140510043,
from NSF #IIS-0328811, DARPA #NBCHD030010
via subcontract to SRI #03-000223 and NSF #E1A-
0080124.
References
C. F. Baker and J. Ruppenhofer. 2002. Framenet?s
frames vs. Levin?s verb classes. In Proceedings of the
28th Annual Meeting of the Berkeley Linguistics Soci-
ety, pages 27?38.
C. F. Baker, C. Fillmore, and J. B. Lowe. 1998. The
Berkeley Framenet project. In Proceedings of CoLing-
ACL, Montreal.
J. Carroll, E. Briscoe, and A. Sanfilippo. 1998. Parser
evaluation: A survey and a new proposal. In Proceed-
ings of LREC-98.
A. Copestake and D. Flickinger. 2000. An open
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-2000, Athens, Greece.
B. Dorr. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?375.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. 2004.
Building a computational lexicon and ontology with
FrameNet. In Proceedings of LREC workshop on
Building Lexical Resources from Semantically Anno-
tated Corpora, Lisbon.
M. O. Dzikovska, M. D. Swift, and J. F. Allen. to ap-
pear. Customizing meaning: Building domain-specific
semantic representations from a generic lexicon. In
H. Bunt, editor, Computing Meaning, Volume 3, Stud-
ies in Linguistics and Philosophy. Kluwer.
M. O. Dzikovska. 2004. A Practical Semantic Repre-
sentation for Natural Language Parsing. Ph.D. thesis,
University of Rochester, Rochester NY.
C. Johnson and C. J. Fillmore. 2000. The FrameNet
tagset for frame-semantic and syntactic coding of
predicate-argument structure. In Proceedings ANLP-
NAACL 2000, Seattle, WA.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-
based construction of a verb lexicon. In Proceedings
of AAAI, Austin.
K. Kipper, B. Snyder, and M. Palmer. 2004. Us-
ing prepositions to extend a verb lexicon. In Pro-
ceedings of HLT-NAACL 2004 Workshop on Compu-
tational Lexical Semantics, pages 23?29, Boston, MA.
K. Kipper, A. Korhonen, N. Ryant, and M. Palmer. 2006.
Extending Verbnet with novel verb classes. In Pro-
ceedings of LREC-2006.
K. Kipper. 2005. Verbnet: A broad coverage, compre-
hensive verb lexicon. Ph.D. thesis, University of Penn-
sylvania.
N. Kwon and E. H. Hovy. 2006. Integrating semantic
frames from multiple sources. In A. F. Gelbukh, edi-
tor, CICLing, volume 3878 of Lecture Notes in Com-
puter Science, pages 1?12. Springer.
B. Levin. 1993. English Verb Classes and Alternations.
The University of Chicago Press.
S. Narayanan and S. Harabagiu. 2004. Question answer-
ing based on semantic structures. In Proceedings of
International Conference on Computational Linguis-
tics (COLING 2004), Geneva, Switzerland.
M. Swift. 2005. Towards automatic verb acquisition
from Verbnet for spoken dialog processing. In Pro-
ceedings of Interdisciplinary Workshop on the Identi-
fication and Representation of Verb Features and Verb
Classes, Saarbruecken, Germany.
J. Tetreault. 2005. Empirical Evaluations of Pronoun
Resolution. Ph.D. thesis, University of Rochester.
P. Vossen. 1997. Eurowordnet: A multilingual database
for information retrieval. In Proceedings of the Delos
workshop on Cross-language Information Retrieval.
32
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
On statistical parsing of French with supervised and semi-supervised
strategies
Marie Candito*, Beno?t Crabb?* and Djam? Seddah?
* Universit? Paris 7
UFRL et INRIA (Alpage)
30 rue du Ch?teau des Rentiers
F-75013 Paris ? France
? Universit? Paris 4
LALIC et INRIA (Alpage)
28 rue Serpente
F-75006 Paris ? France
Abstract
This paper reports results on grammati-
cal induction for French. We investigate
how to best train a parser on the French
Treebank (Abeill? et al, 2003), viewing
the task as a trade-off between generaliz-
ability and interpretability. We compare,
for French, a supervised lexicalized pars-
ing algorithm with a semi-supervised un-
lexicalized algorithm (Petrov et al, 2006)
along the lines of (Crabb? and Candito,
2008). We report the best results known
to us on French statistical parsing, that we
obtained with the semi-supervised learn-
ing algorithm. The reported experiments
can give insights for the task of grammat-
ical learning for a morphologically-rich
language, with a relatively limited amount
of training data, annotated with a rather
flat structure.
1 Natural language parsing
Despite the availability of annotated data, there
have been relatively few works on French statis-
tical parsing. Together with a treebank, the avail-
ability of several supervised or semi-supervised
grammatical learning algorithms, primarily set up
on English data, allows us to figure out how they
behave on French.
Before that, it is important to describe the char-
acteristics of the parsing task. In the case of sta-
tistical parsing, two different aspects of syntactic
structures are to be considered : their capacity to
capture regularities and their interpretability for
further processing.
Generalizability Learning for statistical parsing
requires structures that capture best the underlying
regularities of the language, in order to apply these
patterns to unseen data.
Since capturing underlying linguistic rules is
also an objective for linguists, it makes sense
to use supervised learning from linguistically-
defined generalizations. One generalization is
typically the use of phrases, and phrase-structure
rules that govern the way words are grouped to-
gether. It has to be stressed that these syntactic
rules exist at least in part independently of seman-
tic interpretation.
Interpretability But the main reason to use su-
pervised learning for parsing, is that we want
structures that are as interpretable as possible, in
order to extract some knowledge from the anal-
ysis (such as deriving a semantic analysis from
a parse). Typically, we need a syntactic analysis
to reflect how words relate to each other. This
is our main motivation to use supervised learn-
ing : the learnt parser will output structures as
defined by linguists-annotators, and thus inter-
pretable within the linguistic theory underlying the
annotation scheme of the treebank. It is important
to stress that this is more than capturing syntactic
regularities : it has to do with the meaning of the
words.
It is not certain though that both requirements
(generalizability / interpretability) are best met in
the same structures. In the case of supervised
learning, this leads to investigate different instan-
tiations of the training trees, to help the learning,
while keeping the maximum interpretability of the
trees. As we will see with some of our experi-
ments, it may be necessary to find a trade-off be-
tween generalizability and interpretability.
Further, it is not guaranteed that syntactic rules
infered from a manually annotated treebank pro-
duce the best language model. This leads to
49
methods that use semi-supervised techniques on
a treebank-infered grammar backbone, such as
(Matsuzaki et al, 2005; Petrov et al, 2006).
The plan of the paper is as follows : in the
next section, we describe the available treebank
for French, and how its structures can be inter-
preted. In section 3, we describe the typical prob-
lems encountered when parsing using a plain prob-
abilistic context-free grammar, and existing algo-
rithmic solutions that try to circumvent these prob-
lems. Next we describe experiments and results
when training parsers on the French data. Finally,
we discuss related work and conclude.
2 Interpreting the French trees
The French Treebank (Abeill? et al, 2003) is a
publicly available sample from the newspaper Le
Monde, syntactically annotated and manually cor-
rected for French.
<SENT>
<NP fct="SUJ">
<w cat="D" lemma="le" mph="ms" subcat="def">le</w>
<w cat="N" lemma="bilan" mph="ms" subcat="C">bilan</w>
</NP>
<VN>
<w cat="ADV" lemma="ne" subcat="neg">n?</w>
<w cat="V" lemma="?tre" mph="P3s" subcat="">est</w>
</VN>
<AdP fct="MOD">
<w compound="yes" cat="ADV" lemma="peut-?tre">
<w catint="V">peut</w>
<w catint="PONCT">-</w>
<w catint="V">?tre</w>
</w>
<w cat="ADV" lemma="pas" subcat="neg">pas</w>
</AdP>
<AP fct="ATS">
<w cat="ADV" lemma="aussi">aussi</w>
<w cat="A" lemma="sombre" mph="ms" subcat="qual">sombre</w>
</AP>
<w cat="PONCT" lemma="." subcat="S">.</w>
</SENT>
Figure 1: Simplified example of the FTB
To encode syntactic information, it uses a com-
bination of labeled constituents, morphological
annotations and functional annotation for verbal
dependents as illustrated in Figure 1. This con-
stituent and functional annotation was performed
in two successive steps : though the original re-
lease (Abeill? et al, 2000) consists of 20,648 sen-
tences (hereafter FTB-V0), the functional annota-
tion was performed later on a subset of 12351 sen-
tences (hereafter FTB). This subset has also been
revised, and is known to be more consistently an-
notated. This is the release we use in our experi-
ments. Its key properties, compared with the Penn
Treebank, (hereafter PTB) are the following :
Size : The FTB is made of 385 458 tokens and
12351 sentences, that is the third of the PTB. The
average length of a sentence is 31 tokens in the
FTB, versus 24 tokens in the PTB.
Inflection : French morphology is richer than En-
glish and leads to increased data sparseness for
statistical parsing. There are 24098 types in the
FTB, entailing an average of 16 tokens occurring
for each type (versus 12 for the PTB).
Flat structure : The annotation scheme is flatter
in the FTB than in the PTB. For instance, there
are no VPs for finite verbs, and only one sentential
level for sentences whether introduced by comple-
mentizer or not. We can measure the corpus flat-
ness using the ratio between tokens and non ter-
minal symbols, excluding preterminals. We obtain
0.69 NT symbol per token for FTB and 1.01 for the
PTB.
Compounds : Compounds are explicitly annotated
(see the compound peut-?tre in Figure 1 ) and very
frequent : 14,52% of tokens are part of a com-
pound. They include digital numbers (written with
spaces in French 10 000), very frozen compounds
pomme de terre (potato) but also named entities
or sequences whose meaning is compositional but
where insertion is rare or difficult (garde d?enfant
(child care)).
Now let us focus on what is expressed in the
French annotation scheme, and why syntactic in-
formation is split between constituency and func-
tional annotation.
Syntactic categories and constituents capture dis-
tributional generalizations. A syntactic category
groups forms that share distributional properties.
Nonterminal symbols that label the constituents
are a further generalizations over sequences of cat-
egories or constituents. For instance about any-
where it is grammatical to have a given NP, it is
implicitly assumed that it will also be grammati-
cal - though maybe nonsensical - to have instead
any other NPs. Of course this is known to be false
in many cases : for instance NPs with or with-
out determiners have very different distributions in
French (that may justify a different label) but they
also share a lot. Moreover, if words are taken into
account, and not just sequences of categories, then
constituent labels are a very coarse generalization.
Constituents also encode dependencies : for in-
stance the different PP-attachment for the sen-
tences I ate a cake with cream / with a fork re-
flects that with cream depends on cake, whereas
with a fork depends on ate. More precisely, a
syntagmatic tree can be interpreted as a depen-
dency structure using the following conventions :
50
for each constituent, given the dominating symbol
and the internal sequence of symbols, (i) a head
symbol can be isolated and (ii) the siblings of that
head can be interpreted as containing dependents
of that head. Given these constraints, the syntag-
matic structure may exhibit various degree of flat-
ness for internal structures.
Functional annotation Dependencies are en-
coded in constituents. While X-bar inspired con-
stituents are supposed to contain all the syntac-
tic information, in the FTB the shape of the con-
stituents does not necessarily express unambigu-
ously the type of dependency existing between a
head and a dependent appearing in the same con-
stituent. Yet this is crucial for example to ex-
tract the underlying predicate-argument structures.
This has led to a ?flat? annotation scheme, com-
pleted with functional annotations that inform on
the type of dependency existing between a verb
and its dependents. This was chosen for French
to reflect, for instance, the possibility to mix post-
verbal modifiers and complements (Figure 2), or
to mix post-verbal subject and post-verbal indi-
rect complements : a post verbal NP in the FTB
can correspond to a temporal modifier, (most of-
ten) a direct object, or an inverted subject, and in
the three cases other subcategorized complements
may appear.
SENT
NP-SUJ
D
une
N
lettre
VN
V
avait
V
?t?
V
envoy?e
NP-MOD
D
la
N
semaine
A
derni?re
PP-AOBJ
P
aux
NP
N
salari?s
SENT
NP-SUJ
D
Le
N
Conseil
VN
V
a
V
notifi?
NP-OBJ
D
sa
N
d?cision
PP-AOBJ
P
?
NP
D
la
N
banque
Figure 2: Two examples of post-verbal NPs : a
direct object and a temporal modifier
3 Algorithms for probabilistic grammar
learning
We propose here to investigate how to apply statis-
tical parsing techniques mainly tested on English,
to another language ? French ?. In this section we
briefly introduce the algorithms investigated.
Though Probabilistic Context Free Grammars
(PCFG) is a baseline formalism for probabilistic
parsing, it suffers a fundamental problem for the
purpose of natural language parsing : the inde-
pendence assumptions made by the model are too
strong. In other words all decisions are local to a
grammar rule.
However as clearly pointed out by (Johnson,
1998) decisions have to take into account non lo-
cal grammatical properties: for instance a noun
phrase realized in subject position is more likely to
be realized by a pronoun than a noun phrase real-
ized in object position. Solving this first method-
ological issue, has led to solutions dubbed here-
after as unlexicalized statistical parsing (Johnson,
1998; Klein and Manning, 2003a; Matsuzaki et
al., 2005; Petrov et al, 2006).
A second class of non local decisions to be
taken into account while parsing natural languages
are related to handling lexical constraints. As
shown above the subcategorization properties of
a predicative word may have an impact on the de-
cisions concerning the tree structures to be asso-
ciated to a given sentence. Solving this second
methodological issue has led to solutions dubbed
hereafter as lexicalized parsing (Charniak, 2000;
Collins, 1999).
In a supervised setting, a third and practical
problem turns out to be critical: that of data
sparseness since available treebanks are generally
too small to get reasonable probability estimates.
Three class of solutions are possible to reduce data
sparseness: (1) enlarging the data manually or au-
tomatically (e.g. (McClosky et al, 2006) uses self-
training to perform this step) (2) smoothing, usu-
ally this is performed using a markovization pro-
cedure (Collins, 1999; Klein and Manning, 2003a)
and (3) make the data more coarse (i.e. clustering).
3.1 Lexicalized algorithm
The first algorithm we use is the lexicalized parser
of (Collins, 1999). It is called lexicalized, as it
annotates non terminal nodes with an additional
latent symbol: the head word of the subtree. This
additional information attached to the categories
aims at capturing bilexical dependencies in order
to perform informed attachment choices.
The addition of these numerous latent sym-
bols to non terminals naturally entails an over-
specialization of the resulting models. To en-
sure generalization, it therefore requires to add
additional simplifying assumptions formulated as
a variant of usual na?ve Bayesian-style simplify-
ing assumptions: the probability of emitting a non
51
head node is assumed to depend on the head and
the mother node only, and not on other sibling
nodes1.
Since Collins demonstrated his models to sig-
nificantly improve parsing accuracy over bare
PCFG, lexicalization has been thought as a ma-
jor feature for probabilistic parsing. However two
problems are worth stressing here: (1) the reason
why these models improve over bare PCFGs is not
guaranteed to be tied to the fact that they capture
bilexical dependencies and (2) there is no guar-
antee that capturing non local lexical constraints
yields an optimal language model.
Concerning (1) (Gildea, 2001) showed that full
lexicalization has indeed small impact on results :
he reimplemented an emulation of Collins? Model
1 and found that removing all references to bilex-
ical dependencies in the statistical model2, re-
sulted in a very small parsing performance de-
crease (PARSEVAL recall on WSJ decreased from
86.1 to 85.6). Further studies conducted by (Bikel,
2004a) proved indeed that bilexical information
were used by the most probable parses. The idea
is that most bilexical parameters are very similar
to their back-off distribution and have therefore a
minor impact. In the case of French, this fact can
only be more true, with one third of training data
compared to English, and with a much richer in-
flection that worsens lexical data sparseness.
Concerning (2) the addition of head word an-
notations is tied to the use of manually defined
heuristics highly dependent on the annotation
scheme of the PTB. For instance, Collins? mod-
els integrate a treatment of coordination that is not
adequate for the FTB-like coordination annotation.
3.2 Unlexicalized algorithms
Another class of algorithms arising from (John-
son, 1998; Klein and Manning, 2003a) attempts
to attach additional latent symbols to treebank cat-
egories without focusing exclusively on lexical
head words. For instance the additional annota-
tions will try to capture non local preferences like
1This short description cannot do justice to (Collins,
1999) proposal which indeed includes more fine grained in-
formations and a backoff model. We only keep here the key
aspects of his work relevant for the current discussion.
2Let us consider a dependent constituent C with head
word Chw and head tag Cht, and let C be governed by a con-
stituent H, with head word Hhw and head tag Hht. Gildea
compares Collins model, where the emission of Chw is con-
ditioned on Hhw, and a ?mono-lexical? model, where the
emission of Chw is not conditioned on Hhw.
the fact that an NP in subject position is more
likely realized as a pronoun.
The first unlexicalized algorithms set up in this
trend (Johnson, 1998; Klein and Manning, 2003a)
also use language dependent and manually de-
fined heuristics to add the latent annotations. The
specialization induced by this additional annota-
tion is counterbalanced by simplifying assump-
tions, dubbed markovization (Klein and Manning,
2003a).
Using hand-defined heuristics remains prob-
lematic since we have no guarantee that the latent
annotations added in this way will allow to extract
an optimal language model.
A further development has been first introduced
by (Matsuzaki et al, 2005) who recasts the prob-
lem of adding latent annotations as an unsuper-
vised learning problem: given an observed PCFG
induced from the treebank, the latent grammar is
generated by combining every non terminal of the
observed grammar to a predefined set H of latent
symbols. The parameters of the latent grammar
are estimated from the observed trees using a spe-
cific instantiation of EM.
This first procedure however entails a combi-
natorial explosion in the size of the latent gram-
mar as |H| increases. (Petrov et al, 2006) (here-
after BKY) overcomes this problem by using the
following algorithm: given a PCFG G0 induced
from the treebank, iteratively create n grammars
G1 . . . Gn (with n = 5 in practice), where each
iterative step is as follows :
? SPLIT Create a new grammar Gi from Gi?1
by splitting every non terminal of Gi in
two new symbols. Estimate Gi?s parameters
on the observed treebank using a variant of
inside-outside. This step adds the latent an-
notation to the grammar.
? MERGE For each pair of symbols obtained
by a previous split, try to merge them back.
If the likelihood of the treebank does not
get significantly lower (fixed threshold) then
keep the symbol merged, otherwise keep the
split.
? SMOOTH This step consists in smoothing the
probabilities of the grammar rules sharing the
same left hand side.
This algorithm yields state-of-the-art results on
52
English3. Its key interest is that it directly aims
at finding an optimal language model without (1)
making additional assumptions on the annotation
scheme and (2) without relying on hand-defined
heuristics. This may be viewed as a case of semi-
supervised learning algorithm since the initial su-
pervised learning step is augmented with a second
step of unsupervised learning dedicated to assign
the latent symbols.
4 Experiments and Results
We investigate how some treebank features impact
learning. We describe first the experimental pro-
tocol, next we compare results of lexicalized and
unlexicalized parsers trained on various ?instan-
tiations? of the xml source files of the FTB, and
the impact of training set size for both algorithms.
Then we focus on studying how words impact the
results of the BKYalgorithm.
4.1 Protocol
Treebank setting For all experiments, the tree-
bank is divided into 3 sections : training (80%),
development (10%) and test (10%), made of
respectively 9881, 1235 and 1235 sentences.
We systematically report the results with the
compounds merged. Namely, we preprocess the
treebank in order to turn each compound into a
single token both for training and test.
Software and adaptation to French For the
Collins algorithm, we use Bikel?s implementation
(Bikel, 2004b) (hereafter BIKEL), and we report
results using Collins model 1 and model 2, with
internal tagging. Adapting model 1 to French
requires to design French specific head propaga-
tion rules. To this end, we adapted those de-
scribed by (Dybro-Johansen, 2004) for extracting
a Stochastic Tree Adjoining Grammar parser on
French. And to adapt model 2, we have further
designed French specific argument/adjunct identi-
fication rules.
For the BKY approach, we use the Berkeley
implementation, with an horizontal markovization
h=0, and 5 split/merge cycles. All the required
knowledge is contained in the treebank used for
training, except for the treatment of unknown or
rare words. It clusters unknown words using ty-
pographical and morphological information. We
3(Petrov et al, 2006) obtain an F-score=90.1 for sentences
of less than 40 words.
adapted these clues to French, following (Arun
and Keller, 2005).
Finally we use as a baseline a standard PCFG
algorithm, coupled with a trigram tagger (we refer
to this setup as TNT/LNCKY algorithm4).
Metrics For evaluation, we use the standard PAR-
SEVAL metric of labeled precision/recall, along
with unlabeled dependency evaluation, which is
known as a more annotation-neutral metric. Unla-
beled dependencies are computed using the (Lin,
1995) algorithm, and the Dybro-Johansen?s head
propagation rules cited above5. The unlabeled
dependency F-score gives the percentage of in-
put words (excluding punctuation) that receive the
correct head.
As usual for probabilistic parsing results, the re-
sults are given for sentences of the test set of less
than 40 words (which is true for 992 sentences of
the test set), and punctuation is ignored for F-score
computation with both metrics.
4.2 Comparison using minimal tagsets
We first derive from the FTB a minimally-
informed treebank, TREEBANKMIN, instantiated
from the xml source by using only the major syn-
tactic categories and no other feature. In each ex-
periment (Table 1) we observe that the BKY al-
gorithm significantly outperforms Collins models,
for both metrics.
parser BKY BIKEL BIKEL TNT/
metric M1 M2 LNCKY
PARSEVAL LP 85.25 78.86 80.68 68.74
PARSEVAL LR 84.46 78.84 80.58 67.93
PARSEVAL F1 84.85 78.85 80.63 68.33
Unlab. dep. Prec. 90.23 85.74 87.60 79.50
Unlab. dep. Rec. 89.95 85.72 86.90 79.37
Unlab. dep. F1 90.09 85.73 87.25 79.44
Table 1: Results for parsers trained on FTB with
minimal tagset
4The tagger is TNT (Brants, 2000), and the parser
is LNCKY, that is distributed by Mark Johnson
(http://www.cog.brown.edu/?mj/Software.htm).
Formally because of the tagger, this is not a strict PCFG
setup. Rather, it gives a practical trade-off, in which the
tagger includes the lexical smoothing for unknown and rare
words.
5For this evaluation, the gold constituent trees are con-
verted into pseudo-gold dependency trees (that may con-
tain errors). Then parsed constituent trees are converted
into parsed dependency trees, that are matched against the
pseudo-gold trees.
53
4.3 Impact of training data size
How do the unlexicalized and lexicalized ap-
proaches perform with respect to size? We com-
pare in figure 3 the parsing performance BKY and
COLLINSM1, on increasingly large subsets of the
FTB, in perfect tagging mode6 and using a more
detailed tagset (CC tagset, described in the next
experiment). The same 1235-sentences test set
is used for all subsets, and the development set?s
size varies along with the training set?s size. BKY
outperforms the lexicalized model even with small
amount of data (around 3000 training sentences).
Further, the parsing improvement that would re-
sult from more training data seems higher for BKY
than for Bikel.
2000 4000 6000 8000 10000
76
78
80
82
84
86
88
Number of training sentences
F?
sc
or
e
Bikel
Berkeley
Figure 3: Parsing Learning curve on FTB with CC-
tagset, in perfect-tagging
This potential increase for BKY results if we
had more French annotated data is somehow con-
firmed by the higher results reported for BKY
training on the Penn Treebank (Petrov et al, 2006)
: F1=90.2. We can show though that the 4 points
increase when training on English data is not only
due to size : we extracted from the Penn Treebank
a subset comparable to the FTB, with respect to
number of tokens and average length of sentences.
We obtain F1=88.61 with BKY training.
4.4 Symbol refinements
It is well-known that certain treebank transfor-
mations involving symbol refinements improve
6For BKY, we simulate perfect tagging by changing
words into word+tag in training, dev and test sets. We ob-
tain around 99.8 tagging accuracy, errors are due to unknown
words.
PCFGs (see for instance parent-transformation of
(Johnson, 1998), or various symbol refinements in
(Klein and Manning., 2003b)). Lexicalization it-
self can be seen as symbol refinements (with back-
off though). For BKY, though the key point is to
automatize symbol splits, it is interesting to study
whether manual splits still help.
We have thus experimented BKY training with
various tagsets. The FTB contains rich mor-
phological information, that can be used to split
preterminal symbols : main coarse category (there
are 13), subcategory (subcat feature refining the
main cat), and inflectional information (mph fea-
ture).
We report in Table 2 results for the four tagsets,
where terminals are made of : MIN: main cat,
SUBCAT: main cat + subcat feature, MAX: cat +
subcat + all inflectional information, CC: cat + ver-
bal mood + wh feature.
Tagset Nb of tags Parseval Unlab. dep Tagging
F1 F1 Acc
MIN 13 84.85 90.09 97.35
SUBCAT 34 85.74 ? 96.63
MAX 250 84.13 ? 92.20
CC 28 86.41 90.99 96.83
Table 2: Tagset impact on learning with BKY (own
tagging)
The corpus instantiation with CC tagset is our
best trade-off between tagset informativeness and
obtained parsing performance7 . It is also the best
result obtained for French probabilistic parsing.
This demonstrates though that the BKY learning
is not optimal since manual a priori symbol refine-
ments significantly impact the results.
We also tried to learn structures with functional
annotation attached to the labels : we obtain PAR-
SEVAL F1=78.73 with tags from the CC tagset +
grammatical function. This degradation, due to
data sparseness and/or non local constraints badly
captured by the model, currently constrains us to
use a language model without functional informa-
tions. As stressed in the introduction, this limits
the interpretability of the parses and it is a trade-
off between generalization and interpretability.
4.5 Lexicon and Inflection impact
French has a rich morphology that allows some
degree of word order variation, with respect to
7The differences are statistically significant : using a stan-
dard t-test, we obtain p-value=0.015 between MIN and SUB-
CAT, and p-value=0.002 between CC and SUBCAT.
54
English. For probabilistic parsing, this can have
contradictory effects : (i) on the one hand, this
induces more data sparseness : the occurrences
of a French regular verb are potentially split into
more than 60 forms, versus 5 for an English
verb; (ii) on the other hand, inflection encodes
agreements, that can serve as clues for syntactic
attachments.
Experiment In order to measure the impact
of inflection, we have tested to cluster word
forms on a morphological basis, namely to partly
cancel inflection. Using lemmas as word form
classes seems too coarse : it would not allow to
distinguish for instance between a finite verb and
a participle, though they exhibit different distri-
butional properties. Instead we use as word form
classes, the couple lemma + syntactic category.
For example for verbs, given the CC tagset, this
amounts to keeping 6 different forms (for the 6
moods).
To test this grouping, we derive a treebank where
words are replaced by the concatenation of lemma
+ category for training and testing the parser.
Since it entails a perfect tagging, it has to be
compared to results in perfect tagging mode :
more precisely, we simulate perfect tagging
by replacing word forms by the concatenation
form+tag.
Moreover, it is tempting to study the impact of
a more drastic clustering of word forms : that of
using the sole syntactic category to group word
forms (we replace each word by its tag). This
amounts to test a pure unlexicalized learning.
Discussion Results are shown in Figure 4.
We make three observations : First, comparing
the terminal=tag curves with the other two, it
appears that the parser does take advantage of
lexical information to rank parses, even for this
?unlexicalized? algorithm. Yet the relatively small
increase clearly shows that lexical information
remains underused, probably because of lexical
data sparseness.
Further, comparing terminal=lemma+tag and ter-
minal=form+tag curves, we observe that grouping
words into lemmas helps reducing this sparseness.
And third, the lexicon impact evolution (i.e.
the increment between terminal=tag and termi-
nal=form+tag curves) is stable, once the training
size is superior to approx. 3000 sentences8 .
This suggests that only very frequent words
matter, otherwise words? impact should be more
and more important as training material augments.
0 2000 4000 6000 8000 10000
76
78
80
82
84
86
88
Number of training sentences
Pa
rs
ev
al
 F
?s
co
re
Bky terminal=form+tag
Bky terminal=lemma+tag
Bky terminal=tag
Figure 4: Impact of clustering word forms (train-
ing on FTB with CC-tagset, in perfect-tagging)
5 Related Work
Previous works on French probabilistic parsing are
those of (Arun and Keller, 2005), (Schluter and
van Genabith, 2007), (Schluter and van Genabith,
2008). One major difficulty for comparison is that
all three works use a different version of the train-
ing corpus. Arun reports results on probabilistic
parsing, using an older version of the FTB and us-
ing lexicalized models (Collins M1 and M2 mod-
els, and the bigram model). It is difficult to com-
pare our results with Arun?s work, since the tree-
bank he has used is obsolete (FTB-V0). He obtains
for Model 1 : LR=80.35 / LP=79.99, and for the
bigram model : LR=81.15 / LP=80.84, with min-
imal tagset and internal tagging. The results with
FTB (revised subset of FTB-V0) with minimal
8 This is true for all points in the curves, except for
the last step, i.e. when full training set is used. We per-
formed a 10-fold cross validation to limit sample effects. For
the BKYtraining with CC tagset, and own tagging, we ob-
tain an average F-score of 85.44 (with a rather high stan-
dard deviation ?=1.14). For the clustering word forms ex-
periment, using the full training set, we obtain : 86.64 for
terminal=form+tag (?=1.15), 87.33 for terminal=lemma+tag
(?=0.43), and 85.72 for terminal=tag (?=0.43). Hence our
conclusions (words help even with unlexicalized algorithm,
and further grouping words into lemmas helps) hold indepen-
dently of sampling.
55
tagset (Table 1) are comparable for COLLINSM1,
and nearly 5 points higher for BKY.
It is also interesting to review (Arun and Keller,
2005) conclusion, built on a comparison with the
German situation : at that time lexicalization was
thought (Dubey and Keller, 2003) to have no siz-
able improvement on German parsing, trained on
the Negra treebank, that uses a flat structures. So
(Arun and Keller, 2005) conclude that since lex-
icalization helps much more for parsing French,
with a flat annotation, then word-order flexibility
is the key-factor that makes lexicalization useful
(if word order is fixed, cf. French and English)
and useless (if word order is flexible, cf. German).
This conclusion does not hold today. First, it can
be noted that as far as word order flexibility is con-
cerned, French stands in between English and Ger-
man. Second, it has been proven that lexicalization
helps German probabilistic parsing (K?bler et al,
2006). Finally, these authors show that markoviza-
tion of the unlexicalized Stanford parser gives al-
most the same increase in performance than lex-
icalization, both for the Negra treebank and the
T?ba-D/Z treebank. This conclusion is reinforced
by the results we have obtained : the unlexicalized,
markovized, PCFG-LA algorithm outperforms the
Collins? lexicalized model.
(Schluter and van Genabith, 2007) aim at learn-
ing LFG structures for French. To do so, and in
order to learn first a Collins parser, N. Schluter
created a modified treebank, the MFT, in order (i)
to fit her underlying theoretical requirements, (ii)
to increase the treebank coherence by error min-
ing and (iii) to improve the performance of the
learnt parser. The MFT contains 4739 sentences
taken from the FTB, with semi-automatic trans-
formations. These include increased rule stratifi-
cation, symbol refinements (for information prop-
agation), coordination raising with some manual
re-annotation, and the addition of functional tags.
MFT has also undergone a phase of error min-
ing, using the (Dickinson and Meurers, 2005) soft-
ware, and following manual correction. She re-
ports a 79.95% F-score on a 400 sentence test
set, which compares almost equally with Arun?s
results on the original 20000 sentence treebank.
So she attributes her results to the increased co-
herence of her smaller treebank. Indeed, we ran
the BKY training on the MFT, and we get F-
score=84.31. While this is less in absolute than
the BKY results obtained with FTB (cf. results in
table 2), it is indeed very high if training data size
is taken into account (cf. the BKY learning curve
in figure 3). This good result raises the open ques-
tion of identifying which modifications in the MFT
(error mining and correction, tree transformation,
symbol refinements) have the major impact.
6 Conclusion
This paper reports results in statistical parsing
for French with both unlexicalized (Petrov et al,
2006) and lexicalized parsers. To our knowledge,
both results are state of the art on French for each
paradigm.
Both algorithms try to overcome PCFG?s sim-
plifying assumptions by some specialization of the
grammatical labels. For the lexicalized approach,
the annotation of symbols with lexical head is
known to be rarely fully used in practice (Gildea,
2001), what is really used being the category of
the lexical head.
We observe that the second approach (BKY)
constantly outperforms the lexicalist strategy ? la
(Collins, 1999). We observe however that (Petrov
et al, 2006)?s semi-supervised learning procedure
is not fully optimal since a manual refinement of
the treebank labelling turns out to improve the
parsing results.
Finally we observe that the semi-supervised
BKY algorithm does take advantage of lexical in-
formation : removing words degrades results. The
preterminal symbol splits percolates lexical dis-
tinctions. Further, grouping words into lemmas
helps for a morphologically rich language such as
French. So, an intermediate clustering standing
between syntactic category and lemma is thought
to yield better results in the future.
7 Acknowledgments
We thank N. Schluter and J. van Genabith for
kindly letting us run BKY on the MFT, and A.
Arun for answering our questions. We also thank
the reviewers for valuable comments and refer-
ences. The work of the second author was partly
funded by the ?Prix Diderot Innovation 2007?,
from University Paris 7.
56
References
Anne Abeill?, Lionel Cl?ment, and Alexandra Kinyon.
2000. Building a treebank for french. In Proceed-
ings of the 2nd International Conference Language
Resources and Evaluation (LREC?00).
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a treebank for French. Kluwer, Dor-
drecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 306?313, Ann Arbor, MI.
Daniel M. Bikel. 2004a. A distributional analysis
of a lexicalized statistical parsing model. In Proc.
of Empirical Methods in Natural Language Pro-
cessing (EMNLP 2004), volume 4, pages 182?189,
Barcelona, Spain.
Daniel M. Bikel. 2004b. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
NLP Conference (ANLP), Seattle-WA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the Annual Meet-
ing of the North American Association for Com-
putational Linguistics (NAACL-00), pages 132?139,
Seattle, Washington.
Michael Collins. 1999. Head driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
Actes de la 15?me Conf?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN?08), pages
45?54, Avignon.
Markus Dickinson and W. Detmar Meurers. 2005.
Prune diseased branches to get healthy trees! how
to find erroneous local trees in treebank and why
it matters. In Proceedings of the 4th Workshop
on Treebanks and Linguistic Theories (TLT 2005),
Barcelona, Spain.
Amit Dubey and Frank Keller. 2003. Probabilis-
tic parsing for german using sister-head dependen-
cies. In In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 96?103.
Ane Dybro-Johansen. 2004. Extraction automatique
de grammaires ? partir d?un corpus fran?ais. Mas-
ter?s thesis, Universit? Paris 7.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the First Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 167?202.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang
Maier. 2006. Is it really that difficult to parse ger-
man? In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 111?119, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
75?82.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Natalie Schluter and Josef van Genabith. 2007.
Preparing, restructuring, and augmenting a french
treebank: Lexicalised parsers or coherent treebanks?
In Proceedings of PACLING 07.
Natalie Schluter and Josef van Genabith. 2008.
Treebank-based acquisition of lfg parsing resources
for french. In European Language Resources As-
sociation (ELRA), editor, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
57
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 138?141,
Paris, October 2009. c?2009 Association for Computational Linguistics
Improving generative statistical parsing with semi-supervised word
clustering
Marie Candito and Beno?t Crabb?
Universit? Paris 7/INRIA (Alpage), 30 rue du Ch?teau des Rentiers, 75013 Paris
Abstract
We present a semi-supervised method to
improve statistical parsing performance.
We focus on the well-known problem of
lexical data sparseness and present exper-
iments of word clustering prior to pars-
ing. We use a combination of lexicon-
aided morphological clustering that pre-
serves tagging ambiguity, and unsuper-
vised word clustering, trained on a large
unannotated corpus. We apply these clus-
terings to the French Treebank, and we
train a parser with the PCFG-LA unlex-
icalized algorithm of (Petrov et al, 2006).
We find a gain in French parsing perfor-
mance: from a baseline of F1=86.76% to
F1=87.37% using morphological cluster-
ing, and up to F1=88.29% using further
unsupervised clustering. This is the best
known score for French probabilistic pars-
ing. These preliminary results are encour-
aging for statistically parsing morpholog-
ically rich languages, and languages with
small amount of annotated data.
1 Introduction
Lexical information is known crucial in natural
language parsing. For probabilistic parsing, one
main drawback of the plain PCFG approach is to
lack sensitivity to the lexicon. The symbols acces-
sible to context-free rules are part-of-speech tags,
which encode generalizations that are too coarse
for many parsing decisions (for instance subcat-
egorization information is generally absent from
tagsets). The lexicalized models first proposed
by Collins reintroduced words at every depth of a
parse tree, insuring that attachments receive prob-
abilities that take lexical information into account.
On the other hand, (Matsuzaki et al, 2005) have
proposed probabilistic CFG learning with latent
annotation (hereafter PCFG-LA), as a way to au-
tomate symbol splitting in unlexicalized proba-
bilistic parsing (cf. adding latent annotations to
a symbol is comparable to splitting this symbol).
(Petrov et al, 2006) rendered the method usable in
practice, with a tractable technique to retain only
the beneficial splits.
We know that both lexicalized parsing algo-
rithm and PCFG-LA algorithm suffer from lex-
ical data sparseness. For lexicalized parsers,
(Gildea, 2001) shows that bilexical dependencies
parameters are almost useless in the probabilistic
scoring of parser because they are too scarce.
For PCFG-LA, we have previously studied the
lexicon impact on this so-called ?unlexicalized?
algorithm, for French parsing (Crabb? and Can-
dito, 2008), (Candito et al, 2009). We have tested
a totally unlexicalized parser, trained on a treebank
where words are replaced by their POS tags. It ob-
tains a parseval F1=86.28 (note that it induces per-
fect tagging). We compared it to a parser trained
with word+tag as terminal symbols (to simulate a
perfect tagging), achieving F1=87.79. This proves
that lexical information is indeed used by the ?un-
lexicalized? PCFG-LA algorithm: some lexical
information percolates through parse trees via the
latent annotations.
We have also reported a slight improvement
(F1=88.18) when word forms are clustered on a
morphological basis, into lemma+tag clusters. So
PCFG-LA uses lexical information, but it is too
sparse, hence it benefits from word clustering. Yet
the use of lemma+tag terminals supposes tagging
prior to parsing. We propose here to apply rather
a deterministic supervised morphological cluster-
ing that preserves tagging ambiguities, leaving it
to the parser to disambiguate POS tags.
We also investigate the use of unsupervised
word clustering, obtained from unannotated text.
It has been proved useful for parsing by (Koo et
al., 2008) and their work directly inspired ours.
They have shown that parsing improves when
cluster information is used as features in a discrim-
inative training method that learns dependency
parsers. We investigate in this paper the use of
such clusters in a generative approach to proba-
bilistic phrase-structure parsing, simply by replac-
ing each token by its cluster.
138
We present in section 2 the treebank instanti-
ation we use for our experiments, the morpho-
logical clustering in section 3, and the Brown al-
gorithm for unsupervised clustering in section 4.
Section 5 presents our experiments, results and
discussion. Section 6 discusses related work. Sec-
tion 7 concludes with some ideas for future work.
2 French Treebank
For our experiments, we use the French Treebank
(hereafter FTB) (Abeill? et al, 2003), containing
12531 sentences of the newspaper Le Monde. We
started with the treebank instantiation defined in
(Crabb? and Candito, 2008), where the rich origi-
nal annotation containing morphological and func-
tional information is mapped to a plain phrase-
structure treebank with a tagset of 28 POS tags.
In the original treebank, 17% of the tokens be-
long to a compound, and compounds range from
very frozen multi word expressions like y com-
pris (literally there included, meaning including)
to syntactically regular entities like loi agraire
(land law). In most of the experiments with the
FTB, each compound is merged into a single to-
ken: (P (CL y) (A compris)) is merged as (P
y_compris). But because our experiments aim at
reducing lexical sparseness but also at augmenting
lexical coverage using an unannotated corpus, we
found it necessary to make the unannotated cor-
pus tokenisation and the FTB tokenisation consis-
tent. To set up a robust parser, we chose to avoid
recognizing compounds that exhibit syntactically
regular patterns. We create a new instance of the
treebank (hereafter FTB-UC), where syntactically
regular patterns are ?undone? (Figure 1). This re-
duces the number of distinct compounds in the
whole treebank from 6125 to 3053.
NP
D
l?
N
N
Union
A
?conomique
C
et
A
mon?taire
NP
D
l?
N
Union
AP
A
?conomique
COORD
C
et
AP
A
mon?taire
Figure 1: A NP with a compound (left) changed
into a regular structure with simple words (right)
3 Morphological clustering
The aim of this step is to reduce lexical sparseness
caused by inflection, without hurting parsability,
and without committing ourselves as far as ambi-
guity is concerned. Hence, a morphological clus-
tering using lemmas is not possible, since lemma
assignment supposes POS disambiguation. Fur-
ther, information such as mood on verbs is nec-
essary to capture for instance that infinitive verbs
have no overt subject, that participial clauses are
sentence modifiers, etc... This is encoded in the
FTB with different projections for finite verbs
(projecting sentences) versus non finite verbs (pro-
jecting VPpart or VPinf).
We had the intuition that the other inflection
marks in French (gender and number for determin-
ers, adjectives, pronouns and nouns, tense and per-
son for verbs) are not crucial to infer the correct
phrase-structure projected by a given word1.
So to achieve morphological clustering, we de-
signed a process of desinflection, namely of re-
moving some inflection marks. It makes use of
the Lefff, a freely available rich morphological and
syntactic French lexicon (Sagot et al, 2006), con-
taining around 116000 lemmas (simple and com-
pounds) and 535000 inflected forms. The desin-
flection is as follows: for a token t to desin-
flect, if it is known in the lexicon, for all the in-
flected lexical entries le of t, try to get corre-
sponding singular entries. If for all the le, cor-
responding singular entries exist and all have the
same form, then replace t by the corresponding
singular. For instance for wt=entr?es (ambigu-
ous between entrances and entered, fem, plural),
the two lexical entries are [entr?es/N/fem/plu] and
[entr?es/V/fem/plu/part/past]2 , each have a corre-
sponding singular lexical entry, with form entr?e.
Then the same process applies to map feminine
forms to corresponding masculine forms. This
allows to change mang?e (eaten, fem, sing) into
mang? (eaten, masc, sing). But for the form en-
tr?e, ambiguous between N and Vpastpart entries,
only the participle has a corresponding masculine
entry (with form entr?). In that case, in order
to preserve the original ambiguity, entr?e is not
replaced by entr?. Finite verb forms, when un-
ambiguous with other POS, are mapped to sec-
ond person plural present indicative corresponding
forms. This choice was made in order to avoid cre-
ating ambiguity: the second person plural forms
end with a very typical -ez suffix, and the result-
ing form is very unlikely ambiguous. For the first
1For instance, French oral comprehension does not seem
to need plural marks very much, since a majority of French
singular forms have their corresponding plural form pro-
nounced in the same way.
2This is just an example and not the real Lefff format.
139
token of a sentence, if unknown in the lexicon,
the algorithm tries to desinflect the low case cor-
responding form.
This desinflection reduces the number of dis-
tinct tokens in the FTB-UC from 27143 to 20268.
4 Unsupervised word clustering
We chose to use the (Brown et al, 1992) hard clus-
tering algorithm, which has proven useful for var-
ious NLP tasks, such as dependency parsing (Koo
et al, 2008) or named entity recognition (Liang,
2005). The algorithm to obtain C clusters is as
follows: each of the C most frequent tokens of
the corpus is assigned its own distinct cluster. For
the (C+1)th most frequent token, create a (C+1)th
cluster. Then for each pair among the C+1 result-
ing clusters, merge the pair that minimizes the loss
in the likelihood of the corpus, according to a bi-
gram language model defined on the clusters. Re-
peat this operation for the (C+2)th most frequent
token, etc... This results in a hard clustering into
C clusters. The process can be continued to fur-
ther merge pairs of clusters among the C clusters,
ending with a unique cluster for the whole vocab-
ulary. This can be traced to obtain a binary tree
representing the merges of the C clusters. A clus-
ter can be identified by its path within this binary
tree. Hence, clusters can be used at various levels
of granularity.
5 Experiments and discussion
For the Brown clustering algorithm, we used Percy
Liang?s code3, run on the L?Est R?publicain cor-
pus, a 125 million word journalistic corpus, freely
available at CNRTL4. The corpus was tokenised5 ,
segmented into sentences and desinflected using
the process described in section 3. We ran the clus-
tering into 1000 clusters for the desinflected forms
appearing at least 20 times.
We tested the use of word clusters for parsing
with the Berkeley algorithm (Petrov et al, 2006).
Clustering words in this case has a double advan-
tage. First, it augments the known vocabulary,
which is made of all the forms of all the clus-
ters appearing in the treebank. Second, it reduces
sparseness for the latent annotations learning on
the lexical rules of the PCFG-LA grammar.
3http://www.eecs.berkeley.edu/ pliang/software
4http://www.cnrtl.fr/corpus/estrepublicain
5The 200 most frequent compounds of the FTB-UC were
systematically recognized as one token.
We used Petrov?s code, adapted to French by
(Crabb? and Candito, 2008), for the suffixes used
to classify unknown words, and we used the same
training(80%)/dev(10%)/test(10%) partition. We
used the FTB-UC treebank to train a baseline
parser, and three other parsers by changing the ter-
minal symbols used in training data:
desinflected forms: as described in section 3
clusters + cap: each desinflected form is re-
placed by its cluster bit string. If the desinflected
form has no corresponding cluster (it did not ap-
pear 20 times in the unannotated corpus), a spe-
cial cluster UNKC is used. Further, a _C suffix is
added if the form starts with a capital.
clusters + cap + suffixes: same as before, ex-
cept that 9 additional features are used as suffixes
to the cluster: if form is all digits, ends with ant,
or r, or ez (cf. this is how end desinflected forms
of unambiguous finite verbs), ...
We give in table 1 parsing performance in terms
of labeled precision/recall/Fscore, and also the
more neutral unlabeled attachment score (UAS)6.
The desinflection process does help: benefits
from reducing data sparseness exceed the loss
of agreement markers. Yet tagging decreases a
little, and this directly impacts the dependency
score, because the dependency extraction uses
head propagation rules that are sensitive to tag-
ging. In the same way, the use of bare clusters
increases labeled recall/precision, but the tagging
accuracy decreases, and thus the UAS. This can
be due to the coarseness of the clustering method,
which sometimes groups words that have differ-
ent POS (for instance among a cluster of infinite
verbs, one may find a present participle). The
quality of the clusters is more crucial in our case
than when clusters are features, whose informativ-
ity is discriminatively learnt. This observation led
us to append a restricted set of suffixes to the clus-
ters, which gives us the best results for now.
6 Related work
We already mentioned that we were inspired by
the success of (Koo et al, 2008) in using word
clusters as features for the discriminative learning
of dependency parsers. Another approach to aug-
ment the known vocabulary for a generative prob-
6In all metrics punctuation tokens are ignored and all re-
sults are for sentences of less than 40 words. Note that we
used the FTB-UC treebank. There are mors tokens in sen-
tences than in the FTB with all compounds merged, and base-
line F1 scores are a little higher (86.79 versus 86.41).
140
terminal symbols LP LR F1 UAS Vocab. size Tagging Acc.
inflected forms (baseline) 86.94 86.65 86.79 91.00 27143 96.90
desinflected forms 87.42 87.32 87.37 91.14 20268 96.81
clusters + cap 88.08 87.50 87.79 91.12 1201 96.37
clusters + cap + suffixes 88.43 88.14 88.29 91.68 1987 97.04
Table 1: Parsing performance when training and parsing use clustered terminal symbols
abilistic parser is the one pursued in (Goldberg et
al., 2009). Within a plain PCFG, the lexical proba-
bilities for words that are rare or absent in the tree-
bank are taken from an external lexical probabil-
ity distribution, estimated using a lexicon and the
Baulm-Welch training of an HMM tagger. This is
proved useful to better parse Hebrew.
7 Conclusion and future work
We have tested the very simple method of replac-
ing inflected forms by clusters of forms in a gener-
ative probabilistic parser. This crude technique has
surprisingly good results and offers a very cheap
and simple way to augment the vocabulary seen at
training time. It seems interesting to try the tech-
nique on other generative approaches such as lex-
icalized probabilistic parsing.
We plan to optimize the exact shape of termi-
nal symbols to use. Bare unsupervised clusters are
unsatisfactory, and we have seen that adding sim-
ple suffixes to the clusters improved performance.
Learning such suffixes is a path to explore. Also,
the hierarchical organization of the clusters could
be used, in the generative approach adopted here,
by modulating the granularity of the clusters de-
pending on their frequency in the treebank.
We also need to check to what extent the desin-
flection step helps for taking advantage of the very
local information captured by the Brown cluster-
ing.Finally, we could try using other kinds of clus-
tering, such as the approach of (Lin, 1998), which
captures similarity between syntactic dependen-
cies beared by nouns and verbs.
8 Acknowledgements
The authors truly thank Percy Liang and Slav
Petrov for providing their code for respec-
tively Brown clustering and PCFG-LA. This
work was supported by the French National
Research Agency (SEQUOIA project ANR-08-
EMER-013).
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer,
Dordrecht.
Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jen-
nifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Compu-
tational linguistics, 18(4):467?479.
Marie Candito, Benoit Crabb?, and Djam? Seddah.
2009. On statistical parsing of french with super-
vised and semi-supervised strategies. In EACL 2009
Workshop Grammatical inference for Computa-
tional Linguistics, Athens, Greece.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
Actes de la 15?me Conf?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN?08), pages
45?54, Avignon, France.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proc. of EMNLP?01, pages 167?202,
Pittsburgh, USA.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and
Michael Elhadad. 2009. Enhancing unlexicalized
parsing performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based lexical
probabilities. In Proc. of EACL-09, pages 327?335,
Athens, Greece.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. of ACL-08, Columbus, USA.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. In MIT Master?s thesis, Cambridge,
USA.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of ACL-98, pages 768?
774, Montreal, Canada.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proc. of ACL-05, pages 75?82, Ann Arbor, USA.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of ACL-06, Syd-
ney, Australia.
Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for french: architecture, acquisi-
tion, use. In Proc. of LREC?06, Genova, Italy.
141
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 150?161,
Paris, October 2009. c?2009 Association for Computational Linguistics
Cross Parser Evaluation and Tagset Variation : a French Treebank Study
Djam? Seddah?, Marie Candito? and Beno?t Crabb??
? Universit? Paris-Sorbonne
LALIC & INRIA (ALPAGE)
28 rue Serpente
F-75006 Paris ? France
? Universit? Paris 7
INRIA (ALPAGE)
30 rue du Ch?teau des Rentiers
F-75013 Paris ? France
Abstract
This paper presents preliminary investiga-
tions on the statistical parsing of French by
bringing a complete evaluation on French
data of the main probabilistic lexicalized
and unlexicalized parsers first designed
on the Penn Treebank. We adapted the
parsers on the two existing treebanks of
French (Abeill? et al, 2003; Schluter and
van Genabith, 2007). To our knowledge,
mostly all of the results reported here are
state-of-the-art for the constituent parsing
of French on every available treebank. Re-
garding the algorithms, the comparisons
show that lexicalized parsing models are
outperformed by the unlexicalized Berke-
ley parser. Regarding the treebanks, we
observe that, depending on the parsing
model, a tag set with specific features
has direct influence over evaluation re-
sults. We show that the adapted lexical-
ized parsers do not share the same sensi-
tivity towards the amount of lexical ma-
terial used for training, thus questioning
the relevance of using only one lexicalized
model to study the usefulness of lexical-
ization for the parsing of French.
1 Introduction
The development of large scale symbolic gram-
mars has long been a lively topic in the French
NLP community. Surprisingly, the acquisition of
probabilistic grammars aiming at stochastic pars-
ing, using either supervised or unsupervised meth-
ods, has not attracted much attention despite the
availability of large manually syntactic annotated
data for French. Nevertheless, the availability
of the Paris 7 French Treebank (Abeill? et al,
2003), allowed (Dybro-Johansen, 2004) to carry
out the extraction of a Tree Adjoining Grammar
(Joshi, 1987) and led (Arun and Keller, 2005)
to induce the first effective lexicalized parser for
French. Yet, as noted by (Schluter and van Gen-
abith, 2007), the use of the treebank was ?chal-
lenging?. Indeed, before carrying out successfully
any experiment, the authors had to perform a deep
restructuring of the data to remove errors and in-
consistencies. For the purpose of building a sta-
tistical LFG parser, (Schluter and van Genabith,
2007; Schluter and van Genabith, 2008) have re-
annotated a significant subset of the treebank with
two underlying goals: (1) designing an annota-
tion scheme that matches as closely as possible
the LFG theory (Kaplan and Bresnan, 1982) and
(2) ensuring a more consistent annotation. On the
other hand, (Crabb? and Candito, 2008) showed
that with a new released and corrected version of
the treebank1 it was possible to train statistical
parsers from the original set of trees. This path
has the advantage of an easier reproducibility and
eases verification of reported results.
With the problem of the usability of the data
source being solved, the question of finding one
or many accurate language models for parsing
French raises. Thus, to answer this question,
this paper reports a set of experiments where
five algorithms, first designed for the purpose of
parsing English, have been adapted to French:
a PCFG parser with latent annotation (Petrov et
al., 2006), a Stochastic Tree Adjoining Grammar
parser (Chiang, 2003), the Charniak?s lexicalized
parser (Charniak, 2000) and the Bikel?s implemen-
tation of Collins? Model 1 and 2 (Collins, 1999)
described in (Bikel, 2002). To ease further com-
parisons, we report results on two versions of the
treebank: (1) the last version made available in
December 2007, hereafter FTB , and described
in (Abeill? and Barrier, 2004) and the (2) LFG
inspired version of (Schluter and van Genabith,
2007).
The paper is structured as follows : After a brief
presentation of the treebanks, we discuss the use-
1This has been made available in December 2007.
150
fulness of testing different parsing frameworks
over two parsing paradigms before introducing
our experimental protocol and presenting our re-
sults. Finally, we discuss and compare with re-
lated works on cross-language parser adaptation,
then we conclude.
2 Treebanks for French
This section provides a brief overview to the cor-
pora on which we report results: the French Tree-
bank (FTB) and the Modified French Treebank
(MFT).
2.1 The French Treebank
THE FRENCH TREEBANK is the first treebank
annotated and manually corrected for French. It
is the result of a supervised annotation project of
newspaper articles from Le Monde (Abeill? and
Barrier, 2004). The corpus is annotated with la-
belled constituent trees augmented with morpho-
logical annotations and functional annotations of
verbal dependents as shown below :
<SENT>
<NP fct="SUJ">
<w cat="D" lemma="le" mph="ms" subcat="def">le</w>
<w cat="N" lemma="bilan" mph="ms" subcat="C">bilan</w>
</NP>
<VN>
<w cat="ADV" lemma="ne" subcat="neg">n?</w>
<w cat="V" lemma="?tre" mph="P3s" subcat="">est</w>
</VN>
<AdP fct="MOD">
<w compound="yes" cat="ADV" lemma="peut-?tre">
<w catint="V">peut</w>
<w catint="PONCT">-</w>
<w catint="V">?tre</w>
</w>
<w cat="ADV" lemma="pas" subcat="neg">pas</w>
</AdP>
<AP fct="ATS">
<w cat="ADV" lemma="aussi">aussi</w>
<w cat="A" lemma="sombre" mph="ms" subcat="qual">sombre</w>
</AP>
<w cat="PONCT" lemma="." subcat="S">.</w>
</SENT>
Figure 1: Simplified example of the FTB: ?Le bi-
lan n?est peut-?tre pas aussi sombre.? (i.e. The
result is perhaps not as bleak)
Though the original release (in 2000) consists
of 20,648 sentences, the subset of 12351 function-
ally annotated sentences is known to be more con-
sistently annotated and therefore is the one used
in this work. Its key properties, compared with
the Penn Treebank (hereafter PTB, (Marcus et al,
1994)), are the following :
Size: The FTB consists of 385,458 tokens and
12,351 sentences, that is the third of the PTB. It
also entails that the average length of a sentence
is 27.48 tokens. By contrast the average sentence
length in the PTB is 24 tokens.
Inflection: French morphology is richer than
English and leads to increased data sparseness is-
sues for the purpose of statistical parsing. There
are 24,098 types in the FTB, entailing an average
of 16 tokens occurring for each type.
A Flat Annotation Scheme: Both the FTB
and the PTB are annotated with constituent trees.
However, the annotation scheme is flatter in the
FTB. For instance, there are no VPs for finite verbs
and only one sentential level for clauses or sen-
tences whether or not introduced by a complemen-
tizer. Only verbal nucleus (VN) is annotated and
comprises the verb, its clitics, auxiliaries, adverbs
and surrounding negation.
While X-bar inspired constituents are supposed
to contain all the syntactic information, in the FTB
the shape of the constituents does not necessar-
ily express unambiguously the type of dependency
existing between a head and a dependent appear-
ing in the same constituent. Yet, this is crucial to
extract the underlying predicate-argument struc-
tures. This has led to a ?flat? annotation scheme,
completed with functional annotations that inform
on the type of dependency existing between a verb
and its dependents. This was chosen for French
to reflect, for instance, the possibility to mix post-
verbal modifiers and complements (Figure 2), or
to mix post-verbal subject and post-verbal indirect
complements : a post verbal NP in the FTB can
correspond to a temporal modifier, (most often) a
direct object, or an inverted subject, and all cases,
other subcategorized complements may appear.
SENT
NP-SUJ
D
une
N
lettre
VN
V
avait
V
?t?
V
envoy?e
NP-MOD
D
la
N
semaine
A
derni?re
PP-AOBJ
P
aux
NP
N
salari?s
(a) A letter had been sent last week to the employees
SENT
NP-SUJ
D
Le
N
Conseil
VN
V
a
V
notifi?
NP-OBJ
D
sa
N
d?cision
PP-AOBJ
P
?
NP
D
la
N
banque
(b) The Council has notified his decision to the bank
Figure 2: Two examples of post-verbal NPs : a
temporal modifier (a) and a direct object (b)
Compounds: Compounds are explicitly anno-
tated and very frequent in the treebank: 14.52% of
tokens are part of a compound (see the compound
peut-?tre ?perhaps? in Figure 1 ). They include
151
digit numbers (written with spaces in French) (e.g.
10 000), frozen compounds (eg. pomme de terre
?potato?) but also named entities or sequences
whose meaning is compositional but where inser-
tion is rare or difficult (e.g. garde d?enfant ?child
care?). As noted by (Arun and Keller, 2005), com-
pounds in French may exhibit ungrammatical se-
quences of tags as in ? la va vite ?in a hurry?
: Prep+ Det+ finite verb + adverb or can in-
clude ?words? which do not exist outside a com-
pound (e.g hui in aujourd?hui ?today?). Therefore,
compounds receive a two-level annotation : con-
stituent parts are described in a subordinate level
using the same POS tagset as the genuine com-
pound POS. This makes it more difficult to extract
a proper grammar from the FTB without merged
compounds2. This is why, following (Arun and
Keller, 2005) and (Schluter and van Genabith,
2007), all the treebanks used in this work contain
compounds.
2.2 The Modified French Treebank
THE MODIFIED FRENCH TREEBANK (MFT) has
been derived from the FTB by (Schluter and van
Genabith, 2008) as a basis for a PCFG-based Lexi-
cal Functional Grammar induction process (Cahill
et al, 2004) for French. The corpus is a subset of
4739 sentences extracted from the original FTB.
The MFT further introduces formal differences of
two kinds with respect to the original FTB: struc-
tural and labeling modifications.
Regarding structural changes, the main transfor-
mations include increased rule stratification (Fig.
3), coordination raising (Fig. 5).
Moreover, the MFT?s authors introduced new
treatments of linguistic phenomena that were not
covered by their initial source treebank. Those
include, for example, analysis for ?It?-cleft con-
structions.3 Since the MFT was designed for the
purpose of improving the task of grammar induc-
tion, the MFT?s authors also refined its tag set by
propagating information (such as mood features
added to VN node labels), and added functional
paths4 to the original function labels. The modifi-
cations introduced in the MFT meet better the for-
mal requirements of the LFG architecture set up
2Consider the case of the compound peut-?tre ?perhaps?
whose POS is ADV, its internal structure (Fig. 1) would lead
to a CFG rule of the form ADV ?? V V.
3See pages 2-3 of (Schluter and van Genabith, 2007) for
details.
4Inspired by the LFG framework (Dalrymple, 2001).
AdP
ADV
encore
ADV
pas
ADV
tr?s
ADV
bien
AdP
ADV
encore
AdP
ADV
pas
AdP ADV
tr?s
AdP ADV
bien
FTB initial analysis MFT modification
Figure 3: Increased stratification in the MFT : ?en-
core pas tr?s bien? (?still not very well?)
XP1
..Y.. X1 ..Z.. COORD
C XP2
XP1
COORD-XP
XP
..Y.. X1 ..Z..
C XP2
Figure 5: Coordinated structures in the general
case, for FTB (up) and MFT (down)
by (Cahill et al, 2004) and reduce the size of the
grammars extracted from the treebank. MFT has
also undergone a phase of error mining and an ex-
tensive manual correction.
2.3 Coordination in French Treebanks
One of the key differences between the two French
treebanks is the way they treat coordinate struc-
tures. Whereas the FTB represents them with an
adjunction of a COORD phrase as a sister or a
daughter of the coordinated element, the MFT in-
troduces a treatment closer to the one used in the
PTB to describe such structures. As opposed to
(Arun and Keller, 2005) who decided to transform
the FTB?s coordinations to match the PTB?s analy-
sis, the COORD label is not removed but extended
to include the coordinated label (Fig. 5).
In Figure 5, we show the general coordination
structure in the FTB, and the corresponding mod-
ified structure in the MFT. A more complicated
modification concerns the case of VP coordina-
tions. (Abeill? et al, 2003) argue for a flat repre-
sentation with no VP-node for French, and this is
152
SENT
VN
CL
Elle
V
ajoute
Ssub
que ...
COORD
CC
et
VN
V
pr?sente
NP
douze points de d?saccord
SENT
NP
CL
Elle
COORD-VP
VP
VN-finite
V-finite
ajoute
Ssub
que ...
C-C
et
VP
VN-finite
V-finite
pr?sente
NP
douze points de d?saccord
Figure 4: Two representations of ?VP coordinations? for the sentence She adds that ... and presents
twelve sticking points: in the FTB (left) and in the MFT (right)
particularly justified in some cases of subject-verb
inversion. Nevertheless, VP phrases are used in
the FTB for non-finite VPs only (nodes VPinf and
VPpart). In the MFT, finite VPs were introduced
to handle VP coordinations. In those cases, the
FTB annotation scheme keeps a flat structure (Fig-
ure 4, left), where the COORD phrase has to be in-
terpreted as a coordinate of the VN node; whereas
finite VP nodes are inserted in the MFT (Figure 4,
right).
2.4 Summary
In Table 2, we describe the annotation schemes of
the treebanks and we provide in Table 1 a numeric
summary of some relevant different features be-
tween these two treebanks. The reported numbers
take into account the base syntactic category labels
without functions, part-of-speech tags without any
morpho-syntactic information (ie. no ?gender? or
number?).
properties FTB MFT
# of sentences 12351 4739
Average sent. length 27.48 28.38
Average node branching 2.60 2.11
PCFG size (without term. prod.) 14874 6944
# of NT symbols 13 39
# of POS tags 15 27
Table 1: Treebanks Properties
3 Parsing Algorithms
Although Probabilistic Context Free Grammars
(PCFG) are a baseline formalism for probabilis-
tic parsing, it is well known that they suffer from
two problems: (a) The independence assumptions
made by the model are too strong, and (b) For Nat-
ural Language Parsing, they do not take into ac-
count lexical probabilities. To date, most of the
results on statistical parsing have been reported
for English. Here we propose to investigate how
to apply these techniques to another language ?
French ? by testing two distinct enhancements
FTB MFT
POS tags A ADV C CL D ET
I N P P+D P+PRO
PONCT PREF PRO
V
A A_card ADV
ADV_int AD-
Vne A_int CC CL
C_S D D_card
ET I N N_card
P P+D PONCT
P+PRO_rel PREF
PRO PRO_card
PRO_int PRO_rel
V_finite V_inf
V_part
NT labels AP AdP COORD NP
PP SENT Sint Srel
Ssub VN VPinf VP-
part
AdP AdP_int AP
AP_int COORD_XP
COORD_UC CO-
ORD_unary NC
NP NP_int NP_rel
PP PP_int PP_rel
SENT Sint Srel Ssub
VN_finite VN_inf
VN_part VP VPinf
VPpart VPpart_rel
Table 2: FTB?s and MFT?s annotation schemes
over the bare PCFG model carried out by two class
of parser models: an unlexicalized model attempt-
ing to overcome problem (a) and 3 different lex-
icalized models attempting to overcome PCFG?s
problems (a) and (b)5.
3.1 Lexicalized algorithms
The first class of algorithms used are lexicalized
parsers of (Collins, 1999; Charniak, 2000; Chi-
ang, 2003). The insight underlying the lexical-
ized algorithms is to model lexical dependencies
between a governor and its dependants in order to
improve attachment choices.
Even though it has been proven numerous times
that lexicalization was useful for parsing the Wall
Street Journal corpus (Collins, 1999; Charniak,
2000), the question of its relevance for other lan-
guages has been raised for German (Dubey and
Keller, 2003; K?bler et al, 2006) and for French
5Except (Chiang, 2003) which is indeed a TREE IN-
SERTION GRAMMAR (Schabes and Waters, 1995) parser but
which must extract a lexicalized grammar from the set of con-
text free rules underlying a treebank.
153
(Arun and Keller, 2005) where the authors ar-
gue that French parsing benefits from lexicaliza-
tion but the treebank flatness reduces its impact
whereas (Schluter and van Genabith, 2007) argue
that an improved annotation scheme and an im-
proved treebank consistency should help to reach
a reasonable state of the art. As only Collins? mod-
els 1 & 2 have been used for French as instances
of lexicalised parsers, we also report results from
the history-based generative parser of (Charniak,
2000) and the Stochastic Tree Insertion Grammar
parser of (Chiang, 2003) as well as (Bikel, 2002)?s
implementation of the Collins? models 1 & 2
(Collins, 1999). Most of the lexicalized parsers
we use in this work are well known and since their
releases, almost ten years ago, their core parsing
models still provide state-of-the-art performance
on the standard test set for English.6 We insist on
the fact that one of the goals of this work was to
evaluate raw performance of well known parsing
models on French annotated data. Thus, we have
not considered using more complex parsing archi-
tectures that makes use of reranking (Charniak and
Johnson, 2005) or self-training (McClosky et al,
2006) in order to improve the performance of a
raw parsing model. Furthermore, studying and de-
signing a set of features for a reranking parser was
beyond the scope of this work. However, we did
use some of these models in a non classical way,
leading us to explore a Collins? model 2 variation,
named model X, and a Stochastic Tree Adjoining
Grammar (Schabes, 1992; Resnik, 1992) variant7 ,
named Spinal Stochastic Tree Insertion Grammars
(hereafter SPINAL STIG), which was first used to
validate the heuristics used by our adaptation of
the Bikel?s parser to French. The next two subsec-
tions introduce these variations.
Collins? Model 2 variation During the ex-
ploratory phase of this work, we found out that a
specific instance of the Collins? model 2 leads to
significantly better performance than the canoni-
cal model when applied to any of the French Tree-
banks. The difference between those two models
relies on the way probabilities associated to so-
called ?modifier non terminals? nodes are handled
by the generative model.
To explain the difference, let us recall that
6Section 23 of the Wall Street Journal section of the PTB.
7The formalism actually used in this parser is a con-
text free variant of Tree Adjoining Grammar, Tree Insertion
Grammars (TIG), first introduced in (Schabes and Waters,
1995).
a lexicalized PCFG can roughly be described
as a set of stochastic rules of the form:
P ? Ln Ln?1 ..L1 H R1 .. Rm?1 Rm
where Li, H , Ri and P are all lexicalized non
terminals; P inherits its head from H (Bikel,
2004). The Collins? model 2 deterministically
labels some nodes of a rule to be arguments of
a given Head and the remaining nodes are con-
sidered to be modifier non terminals (hereafter
MNT).
In this model, given a left-hand side symbol, the
head and its arguments are first generated and then
the MNT are generated from the head outward.
In Bikel?s implementation of Collins?s model 2
(Bikel, 2004), the MNT parameter class is the fol-
lowing (for clarity, we omit the verb intervening,
subcat and side features which are the same in
both classes) :
? model 2 (canonical) :
p(M(t)i|P,H,wh, th,map(Mi?1))
Where M(t)i is the POS tag of the ith MNT,
P the parent node label, H the head node
label, wh the head word and th its POS
tag. map(Mi?1) is a mapped version of
the previously-generated modifier added to
the conditioning context (see below for its
definition).
map(Mi) =
8
>><
>>:
+START+ if i = 0
CC if Mi = CC
+PUNC+ if Mi =,
or Mi =:
+OTHER+ otherwise
9
>>=
>>;
Whereas in the model we call X 8, the mapping
version of the previously generated non terminal is
replaced by a complete list of all previously gen-
erated non terminals.
? Model X :
p(M(t)i|P,H,wh, th, (Mi?1, ...,Mi?k))
The FTB being flatter than the PTB, one can con-
jecture that giving more context to generate MNT
will improve parsing accuracy, whereas clustering
MNT in a X-bar scheme must help to reduce data
sparseness. Note that the Model X, to the best of
our knowledge, is not documented but included in
Bikel?s parser.
8See file NonTerminalModelStructure1.java in Bikel?s
parser source code at http://www.cis.upenn.edu/
~dbikel/download/dbparser/1.2/install.sh.
154
The spinal STIG model In the case of the STIG
parser implementation, having no access to an
argument adjunct table leads it to extract a gram-
mar where almost all elementary trees consist of
a suite of unary productions from a lexical anchor
to its maximal projection (i.e. spine9). Therefore
extracted trees have no substitution node.
Moreover, the probability model, being split
between lexical anchors and tree templates,
allows a very coarse grammar that contains, for
example, only 83 tree templates for one treebank
instantiation, namely the FTB-CC (cf. section 5).
This behavior, although not documented10, is
close to Collins? model 1, which does not use any
argument adjunct distinction information, and led
to results interesting enough to be integrated as
the ?Chiang Spinal? model in our parser set. It
should be noted that, recently, the use of similar
models has been independently proposed in
(Carreras et al, 2008) with the purpose of getting
a richer parsing model that can use non local
features and in (Sangati and Zuidema, 2009) as a
mean of extracting a Lexicalized Tree Substitution
Grammar. In their process, the first extracted
grammar is actually a spinal STIG.
3.2 Unlexicalized Parser
As an instance of an unlexicalized parser, the last
algorithm we use is the Berkeley unlexicalized
parser (BKY) of (Petrov et al, 2006). This algo-
rithm is an evolution of treebank transformation
principles aimed at reducing PCFG independence
assumptions (Johnson, 1998; Klein and Manning,
2003).
Treebank transformations may be of two kinds
(1) structure transformation and (2) labelling
transformations. The Berkeley parser concentrates
on (2) by recasting the problem of acquiring an
optimal set of non terminal symbols as an semi-
supervised learning problem by learning a PCFG
with Latent annotations (PCFG-LA): given an ob-
served PCFG induced from the treebank, the latent
grammar is generated by combining every non ter-
minal of the observed grammar to a predefined set
H of latent symbols. The parameters of the latent
grammar are estimated from the actual treebank
9Not to be confused with the ?spine? in the Tree Adjunct
Grammar (Joshi, 1987) framework which is the path from a
foot node to the root node.
10We mistakenly ?discovered? this obvious property dur-
ing the preliminary porting phase.
trees (or observed trees) using a specific instanci-
ation of EM.
4 Experimental protocol
In this section, we specify the settings of the
parsers for French, the evaluation protocol and the
different instantiations of the treebanks we used
for conducting the experiments.
4.1 Parsers settings
Head Propagation table All lexicalized parsers
reported in this paper use head propagation tables.
Adapting them to the French language requires
to design French specific head propagation
rules. To this end, we used those described by
(Dybro-Johansen, 2004) for training a Stochastic
Tree Adjoining Grammar parser on French. From
this set, we built a set of meta-rules that were
automatically derived to match each treebank
annotation scheme.
As the Collins Model 2 and the STIG model need
to distinguish between argument and adjunct
nodes to acquire subcategorization frames prob-
abilities, we implemented an argument-adjunct
distinction table that takes advantage of the
function labels annotated in the treebank. This is
one of the main differences with the experiments
described in (Arun and Keller, 2005) and (Dybro-
Johansen, 2004) where the authors had to rely
only on the very flat treebank structure without
function labels, to annotate the arguments of a
head.
Morphology and typography adaptation Fol-
lowing (Arun and Keller, 2005), we adapted
the morphological treatment of unknown words
proposed for French when needed (BKY?s and
BIKEL?s parser). This process clusters unknown
words using typographical and morphological in-
formation. Since all lexicalized parsers contain
specific treatments for the PTB typographical con-
vention, we automatically converted the original
punctuation parts of speech to the PTB?s punctua-
tion tag set.
4.2 Experimental details
For the BKY parser, we use the Berkeley imple-
mentation, with an initial horizontal markoviza-
tion h=0, and 5 split/merge cycles. For the
COLLINS? MODEL, we use the standard param-
eters set for the model 2, without any argu-
155
ment adjunct distinction table, as a rough emu-
lation of the COLLINS MODEL 1. The same set
of parameters used for COLLINS? MODEL 2 is
used for the MODEL X except for the parameters
?Mod{Nonterminal,Word}ModelStructureNumber? set to
1 instead of 2.
4.3 Protocol
For all parsers, we report parsing results with the
following experimental protocol: a treebank is di-
vided in 3 sections : test (first 10%), development
(second 10%) and training (remaining 80%). The
MFT partition set is the canonical one (3800 sen-
tences for training, 509 for the dev set and the last
430 for the test set). We systematically report the
results with compounds merged. Namely, we pre-
process the treebank in order to turn each com-
pound into a single token both for training and test.
4.4 Evaluation metrics
Constituency Evaluation: we use the standard
labeled bracketed PARSEVAL metric for evalua-
tion (Black et al, 1991), along with unlabeled
dependency evaluation, which is described as a
more annotation-neutral metric in (Rehbein and
van Genabith, 2007). In the remainder of this pa-
per, we use PARSEVAL as a shortcut for Labeled
Brackets results on sentence of length 40 or less.
Dependency Evaluation: unlabeled dependencies
are computed using the (Lin, 1995) algorithm,
and the Dybro Johansens?s head propagation rules
cited above11. The unlabeled dependency accu-
racy gives the percentage of input words (exclud-
ing punctuation) that receive the correct head. All
reported evaluations in this paper are calculated on
sentences of length less than 40 words.
4.5 Baseline : Comparison using minimal
tagsets
We compared all parsers on three different in-
stances, but still comparable versions, of both the
FTB and the MFT. In order to establish a base-
line, the treebanks are converted to a minimal tag
set (only the major syntactic categories.) without
any other information (no mode propagation as in
the MFT) except for the BIKEL?s parser in Collins?
model 2 (resp. model X) and the STIG parser (i.e.
11For this evaluation, the gold constituent trees are con-
verted into pseudo-gold dependency trees (that may con-
tain errors). Then parsed constituent trees are converted
into parsed dependency trees, that are matched against the
pseudo-gold trees.
STIG-pure) whose models needs function labels to
perform.
Note that by stripping all information from the
node labels in the treebanks, we do not mean
to compare the shape of the treebanks or their
parsability but rather to present an overview of
parser performance on each treebank regardless of
tagset optimizations. However, in each experiment
we observe that the BKY parser significantly out-
performs the other parsers in all metrics.
As the STIG parser presents non statistically sig-
nificant PARSEVAL results differences between its
two modes (PURE & SPINAL) with a f-score p-
value of 0.32, for the remaining of the paper we
will only present results for the STIG?s parser in
?spinal? mode.
FTB-min MFT-min
COLLINS MX PARSEVAL 81.65 79.19
UNLAB. DEP 88.48 84.96
COLLINS M2 PARSEVAL 80.1 78.38
UNLAB. DEP 87.45 84.57
COLLINS M1 PARSEVAL 77.98 76.09
UNLAB. DEP 85.67 82.83
CHARNIAK PARSEVAL 82,44 81.34
UNLAB. DEP 88.42 84.90
CHIANG-SPINAL PARSEVAL 80.66 80.74
UNLAB. DEP 87.92 85,14
BKY PARSEVAL 84,93 83.16
UNLAB. DEP 90.06 87.29
CHIANG-PURE PARSEVAL 80.52 79.56
UNLAB. DEP 87,95 85.02
Table 3: Labeled F1 scores for unlexicalised
and lexicalised parsers on treebanks with minimal
tagsets
5 Cross parser evaluation of tagset
variation
In (Crabb? and Candito, 2008), the authors
showed that it was possible to accurately train the
Petrov?s parser (Petrov et al, 2006) on the FTB us-
ing a more fine grained tag set. This tagset, named
CC12 annotates the basic non-terminal labels with
verbal mood information, and wh-features. Re-
sults were shown to be state of the art with a F1
parseval score of 86.42% on less than 40 words
sentences.
To summarize, the authors tested the impact of
tagset variations over the FTB using constituency
measures as performance indicators.
Knowing that the MFT has been built with PCFG-
based LFG parsing performance in mind (Schluter
12TREEBANKS+ in (Crabb? and Candito, 2008).
156
and van Genabith, 2008) but suffers from a small
training size and yet alows surprisingly high pars-
ing results (PARSEVAL F-score (<=40) of 79.95
% on the MFT gold standard), one would have
wished to verify its
performance with more annotated data.
However, some semi-automatic modifications
brought to the global structure of this treebank
cannot be applied, in an automatic and reversible
way, to the FTB. Anyway, even if we cannot evalu-
ate the influence of a treebank structure to another,
we can evaluate the influence of one tagset to an-
other treebank using handwritten conversion tools.
In order to evaluate the relations between tagsets
and parsing accuracy on a given treebank, we ex-
tract the optimal tagsets13 from the FTB, the CC
tagset and we convert the MFT POS tags to this
tagset. We then do the same for the FTB on which
we apply the MFT?s optimal tagset (ie. SCHLU).
Before introducing the results of our experiments,
we briefly describe these tagsets.
1. min : Preterminals are simply the main cate-
gories, and non terminals are the plain labels
2. cc : (Crabb? and Candito, 2008) best tagset.
Preterminals are the main categories, con-
catenated with a wh- boolean for A, ADV,
PRO, and with the mood for verbs (there are 6
moods). No information is propagated to non
terminal symbols. This tagset is shown in Ta-
ble 4, and described in (Crabb? and Candito,
2008).
ADJ ADJWH ADV ADVWH CC CLO CLR
CLS CS DET DETWH ET I NC NPP P P+D
P+PRO PONCT PREF PRO PROREL PROWH
V VIMP VINF VPP VPR VS
Table 4: CC tagset
3. schlu : N. Schluter?s tagset (Table 2.
Preterminals are the main categories, plus
an inf/finite/part verbal distinction, and
int/card/rel distinction on N, PRO, ADV, A.
These distinctions propagate to non terminal
nodes projected by the lexical head. Non ter-
minals for coordinating structures are split
according to the type of the coordinated
phrases.
Results of these experiments, presented in Table
5, show that BKY displays higher performances
13W.r.t constituent parsing accuracy
in every aspects (constituency and dependency,
except for the MFT-SCHLU). Regardless of the
parser type, we note that unlabeled dependency
scores are higher with the SCHLU tagset than with
the CC tagset. That can be explained by the finest
granularity of the SCHLU based rule set compared
to the other tagset?s rules. As these rules have all
been generated from meta description (a general
COORD label rewrites into COORD_vfinite, CO-
ORD_Sint, etc..) their coverage and global accu-
racy is higher. For example the FTB-CC contains
18 head rules whereas the FTB-SCHLU contains
43 rules.
Interestingly, the ranking of lexicalized parsers
w.r.t PARSEVAL metrics shows that CHARNIAK
has the highest performance over both treebank
tagsets variation even though the MFT?s table (ta-
ble 5) exhibits a non statistically significant vari-
ation between CHARNIAK and STIG-spinal on
PARSEVAL evaluation of the MFT-CC.14
One the other hand, unlabeled dependency evalu-
ations over lexicalized parsers are different among
treebanks. In the case of the FTB, CHARNIAK
exhibits the highest F-score ( FTB-CC: 89.7,
FTB-SCHLU: 89.67) whereas SPINAL STIG per-
forms slightly better on the MFT-SCHLU (MFT-
CC: 86,7, MFT-SCHLU: 87.16). Note that both
tested variations of the Collins? model 2 display
very high unlabeled dependency scores with the
SCHLU tagset.
6 Related Works
As we said in the introduction, the initial work
on the FTB has been carried by (Dybro-Johansen,
2004) in order to extract Tree Adjunct Grammars
from the treebank. Although parsing results were
not reported, she experienced the same argument
adjunct distinction problem than (Arun and Keller,
2005) due to the treebank flatness and the lack of
functional labels in this version. This led Arun
to modify some node annotations (VNG to distin-
guish nodes dominating subcategorized subject cl-
itics and so on) and to add bigrams probabilities to
the language model in order to enhance the over-
all COLLINS? MODEL? performance. Although
our treebanks cannot be compared (20.000 sen-
tences for Arun?s one vs 12351 for the FTB), we
report his best PARSEVAL results (<=40): 80.65
LP, 80.25 LR, 80.45 F1.
However, our results are directly comparable with
14Precision P-value = 0.1272 and Recall = 0.06.
157
Parser
Collins (MX)
Collins (M2)
Collins (M1)
Charniak
Chiang (Sp)
Bky
Parseval Dependency
MFTCC MFTSCH. MFTCC MFTSCH.
80.2 80.96 85.97 87.98
78.56 79.91 84.84 87.43
74 78.49 81.31 85.94
82.5 82.66 86.45 86.94
82.6 81.97 86.7 87.16
83.96 82.86 87.41 86.87
Parseval Dependency
FTBCC FTBSCH. FTBCC FTBSCH.
82.52 82.65 88.96 89.12
80.8 79.56 87.94 87.87
79.16 78.51 86.66 86.93
84.27 83.27 89.7 89.67
81.73 81.54 88.85 89.02
86.02 84.95 90.48 90.73
Table 5: Evaluation Results: MFT-CC vs MFT-SCHLU and FTB-CC vs FTB-SCHLU
(Schluter and van Genabith, 2007) whose best
PARSEVAL F-score on raw text is 79.95 and our
best 82.86 on the MFT-SCHLU.
PARSER FTBARUN MFTSCHLU
Arun (acl05) 80.45 -
Arun (this paper) 81.08 -
Schluter (pacling07) - 79.95
Collins (Mx) 81.5 80,96
Collins (M2) 79.36 79,91
Collins (M1) 77.82 -
Charniak 82.35 82,66
Chiang (Sp) 80.94 81,86
Bky 84.03 82.86
Table 6: Labeled bracket scores on Arun?s FTB
version and on the MFT
In order to favour a ?fair? comparison between
our work and (Arun and Keller, 2005), we also
ran their best adaptation of the COLLINS MODEL
2 on their treebank version using our own head
rules set15 and obtained 81.08% of F1 score (Ta-
ble 6). This shows the important influence of a
fine grained head rules set and argues in favor
of data driven induction of this kind of heuris-
tics. Even though it was established, in (Chiang
and Bikel, 2002), that unsupervised induction of
head rules did not lead to improvement over an
extremely hand crafted head rules set, we believe
that for resource poor languages, such methods
could lead toward significant improvements over
parsing accuracy. Thus, the new unsupervised
head rules induction method presented in (Sangati
and Zuidema, 2009) seems very promising for this
topic.
However, it would be of interest to see how the
Arun?s model would perform using the MODEL X
parameter variations.
7 Discussion
Regarding the apparent lack of success of a gen-
uine COLLINS? MODEL 2 (in most cases, its per-
15Due to the lack of function annotation labels in this tree-
bank, (Arun and Keller, 2005)?s argument distinction table
was used for this experiment.
formance is worse than the other parsers w.r.t to
constituent parsing accuracy) when trained on a
treebank with annotated function labels, we sus-
pect that this is caused by the increased data
sparseness added by these annotations. The same
can be said about the pure STIG model, whose re-
sults are only presented on the FTB-MIN because
the differences between the spinal model and itself
were too small and most of the time not statisti-
cally significant. In our opinion, there might be
simply not enough data to accurately train a pure
COLLINS? MODEL 2 on the FTB with function
labels used for clues to discriminate between argu-
ment and adjuncts. Nevertheless, we do not share
the commonly accepted opinion about the poten-
tial lack of success of lexicalized parsers.
To the best of our knowledge, most adaptations of
a lexicalized model to a western language have
been made with Dan Bikel?s implementation of
COLLINS? MODELS.16
In fact, the adaptations of the CHARNIAK and
BKY?s models exhibit similar magnitudes of per-
formances for French as for English. Evidence of
lexicalization usefulness is shown through a learn-
ing curve (Figure 6) obtained by running some of
our parsers in perfect tagging mode. This experi-
ment was done in the early stages of this work, the
goal was to see how well the parsers would behave
with the same head rules and the same set of pa-
rameters. We only compared the parsers that could
be used without argument adjunct distinction table
(ie. COLLIN?S MODEL 1, SPINAL STIG, CHAR-
NIAK and BKY).
For this earlier experiment, our implementation
of the COLLINS MODEL 1 actually corresponds to
the MODEL X without an argument adjunct dis-
tinction table. More precisely, the absence of ar-
gument nodes, used for the acquisition of subcat-
egorization frames features, makes the MODEL X
parsing model consider all the nodes of a rule, ex-
16Note that the CHARNIAK?s parser has been adapted for
Danish (Zeman and Resnik, 2008) ; the authors report a 80.20
F1 score for a specific instance of the Danish Treebank.
158
2000 4000 6000 8000 10000
76
78
80
82
84
86
88
Number of training sentences
La
be
le
d 
br
a
ck
e
ts
 F
?s
co
re
 (<
=4
0)
Berkeley
Charniak
SpinalTig
Model 1 (emulated)
Figure 6: Learning Curve experiment results for
parsers in perfect tagging mode
cept the head, as Modifier Non Terminal nodes
(MNTs). Hence, because of the impossibility to
extract subcategorization frames, the generation
of a MNT depends mainly on the parent head
word and on the whole list of previously gener-
ated MNTs. One can suppose that training on
small treebanks would lead this distribution to be
sparse, therefore most of the discriminant infor-
mation would come from less specific distribu-
tions. Namely the ones conditioned on the head
pos tag and on the last previously generated MNT
as shown in this model back-off structure (Table
7).
Back-off level p(M(t)i| ? ? ? )
0 P,H,wh, th, ?Mi?1, ...,Mi?k?
1 P,H, th,Mi?1
2 P,H, f
Table 7: MODEL X simplified parameter class for
MNTs
M(t)i is the POS tag of the ith MNT, P the parent node
label, H the head node label, wh the head word, th its POS
tag, ?Mi?1, ...,Mi?k? the list of previously generated MNTs
and f a flag stating if the current node is the first MNT to be
generated.
Interestingly, in the SPINAL STIG model,
almost all the extracted trees are spinal and conse-
quently are handled by an operation called Sister
Adjunction whose probability model for a given
root node of an elementary tree, also conditions
its generation upon the label of the previously
generated tree (Chiang, 2003). Furthermore,
the second component of the Sister Adjunction?s
back-off structure (Table 8) is made coarser by the
removing of the lexical anchor of the tree where a
sister-adjunction is to occur.
Studying in depth the respective impact of these
features on the performance of both models is
outside the scope of this paper, nevertheless we
note that their back-off structures are based on
similar principles: a deletion of the main lexical
information and a context limited to the root label
of the previously generated tree (resp. MNT node
label for the MODEL X). This can explain why
these formally different parsers display almost the
same learning curves (Fig. 6) and more over why
they surprisingly exhibit few sensitivity to the
amount of lexical material used for training.
Back-off level Psa(?| ? ? ? )
0 ??, ??, ??, i,X
1 ??, ??, i,X
2 ??, ??, i
Table 8: SPINAL STIG parameter class for Sister-
adjoining tree templates (Chiang, 2003)
? is the tree to be generated on the sister adjunction site
(??, i) of the tree template ?? , ?? is the lexical anchor of ?? ,
?? is ?? stripped from its anchor POS tag and X is the root
label of the previous tree to sister-adjoin at the site (??, i).
However, the learning curve also shows that the
CHARNIAK?s17 and BKY?s parsers have almost
parallel curves whereas this specific COLLIN?S
MODEL 1 parser and the SPINAL STIG model have
very similar shape and seem to reach an upper
limit very quickly.18 The last two parsers having
also very similar back-off models (Chiang, 2003),
we wonder (1) if we are not actually comparing
them because of data sparseness issues and (2) if
the small size of commonly used treebanks does
not lead the community to consider lexicalized
models, via the lone COLLINS? MODELS, as inap-
propriate to parse other languages than Wall Street
Journal English.
17As opposed to the other parsers, the Charniak?s parser
tagging accuracy did not reach the 100% limit, 98.32% for the
last split. So the comparison is not really fair but we believe
that the visible tendency still stands.
18We are of course aware that the curve?s values are also
function of the amount of new productions brought by the
increased treebank size. That should be of course taken into
account.
159
Regarding the remarkable performance of the
BKY algorithm, it remains unclear why exactly
it systematically outperforms the other lexicalized
algorithms. We can only make a few remarks
about that. First, the algorithm is totally dis-
joint from the linguistic knowledge, that is entirely
taken from the treebank, except for the suffixes
used for handling unknown words. This is not true
of the Collins? or Charniak?s models, that were
set up with the PTB annotation scheme in mind.
Another point concerns the amount of data nec-
essary for an accurate learning. We had the intu-
ition that lexicalized algorithms would have ben-
efited more than BKY from the training data size
increase. Yet the BKY?s learning curve displays a
somewhat faster progression than lexicalized algo-
rithms such as the SPINAL STIG and our specific
instance of the COLLINS? MODEL 1.
In our future work, we plan to conduct
self-training experiments using discriminative
rerankers on very large French corpora to study
the exact impact of the lexicon on this unlexical-
ized algorithm.
8 Conclusion
By adapting those parsers to French and carry-
ing out extensive evaluation over the main char-
acteristics of the treebank at our disposal, we
prove indeed that probabilistic parsing was effi-
cient enough to provide accurate parsing results
for French. We showed that the BKY model estab-
lishes a high performance level on parsing results.
Maybe more importantly we emphasized the im-
portance of tag set model to get distinct state of
the art evaluation metrics for FTB parsing, namely
the SCHLU tagset to get more accurate unlabeled
dependencies and the CC tagset to get better con-
stituency parses. Finally, we showed that the lexi-
calization debate could benefit from the inclusion
of more lexicalized parsing models.
9 Acknowledgments
This work was supported by the ANR Sequoia
(ANR-08-EMER-013). We heartily thank A.
Arun, J. van Genabith an N. Schluter for kindly
letting us use our parsers on their treebanks.
Thanks to the anonymous reviewers for their com-
ments. All remaining errors are ours. We thank J.
Wagner for his help and we would like to acknowl-
edge the Centre for Next Generation Localization
(www.cngl.ie) for providing access to one of its
high-memory nodes.
References
Anne Abeill? and Nicolas Barrier. 2004. Enrich-
ing a french treebank. In Proceedings of Language
Ressources and Evaluation Conference (LREC), Lis-
bon.
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,
2003. Building a Treebank for French. Kluwer,
Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
french. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 306?313, Ann Arbor, MI.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In
Proceedings of the second international conference
on Human Language Technology Research, pages
178?182. Morgan Kaufmann Publishers Inc. San
Francisco, CA, USA.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cov-
erage of english grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop,
pages 306?311, San Mateo (CA). Morgan Kaufman.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics, pages
320?327, Barcelona, Spain.
Xavier Carreras, Mickael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning (CoNLL), pages 9?16.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2005), Ann Arbor (MI).
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Annual
Meeting of the North American Chapter of the ACL
(NAACL), Seattle.
160
David Chiang and Daniel M. Bikel. 2002. Recover-
ing latent information in treebanks. In Proceedings
of COLING?02, 19th International Conference on
Computational Linguistics, Taipei, Taiwan, August.
David Chiang, 2003. Statistical Parsing with an Auto-
matically Extracted Tree Adjoining Grammar, chap-
ter 16, pages 299?316. CSLI Publications.
Michael Collins. 1999. Head Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
Actes de la 15?me Conf?rence sur le Traitement Au-
tomatique des Langues Naturelles (TALN?08), pages
45?54, Avignon, France.
Mary Dalrymple. 2001. Lexical-Functional Grammar,
volume 34 of Syntax and Semantics. San Diego,
CA; London. Academic Press.
Amit Dubey and Frank Keller. 2003. Probabilis-
tic parsing for german using sister-head dependen-
cies. In In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 96?103.
Ane Dybro-Johansen. 2004. Extraction automatique
de grammaires ? partir d?un corpus fran?ais. Mas-
ter?s thesis, Universit? Paris 7.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Aravind K. Joshi. 1987. Introduction to tree adjoining
grammar. In A. Manaster-Ramer, editor, The Math-
ematics of Language. J. Benjamins.
R. Kaplan and J. Bresnan. 1982. Lexical-functional
grammar: A formal system for grammarical repre-
sentation. In J. Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
Mass.: MIT Press.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics Morristown,
NJ, USA.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang
Maier. 2006. Is it really that difficult to parse ger-
man? In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 111?119, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Ines Rehbein and Josef van Genabith. 2007. Tree-
bank annotation schemes and parser evaluation for
german. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), Prague.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammars as a framework for statistic natural lan-
guage processing. COLING?92, Nantes, France.
F. Sangati and W. Zuidema. 2009. Unsupervised
methods for head assignments. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 701?709, Athens, Greece.
Association for Computational Linguistics.
Y. Schabes and R.C. Waters. 1995. Tree Insertion
Grammar: Cubic-Time, Parsable Formalism that
Lexicalizes Context-Free Grammar without Chang-
ing the Trees Produced. Computational Linguistics,
21(4):479?513.
Yves Schabes. 1992. Stochastic Lexicalized Tree Ad-
joining Grammars. In Proceedings of the 14th con-
ference on Computational linguistics, pages 425?
432, Nantes, France. Association for Computational
Linguistics.
Natalie Schluter and Josef van Genabith. 2007.
Preparing, restructuring, and augmenting a french
treebank: Lexicalised parsers or coherent treebanks?
In Proceedings of PACLING 07.
Natalie Schluter and Josef van Genabith. 2008.
Treebank-based acquisition of lfg parsing resources
for french. In European Language Resources As-
sociation (ELRA), editor, Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08), Marrakech, Morocco, may.
Daniel Zeman and Philip Resnik. 2008. Cross-
language parser adaptation between related lan-
guages. In Proceedings of IJCNLP 2008 Work-
shop on NLP for Less Privileged Languages, Haj-
dar?b?du, India.
161
XMG: eXtensible MetaGrammar
Beno??t Crabbe??
INRIA - Universite? Paris 7
Denys Duchier??
LIFO - Universite? d?Orle?ans
Claire Gardent?
CNRS - LORIA, Nancy
Joseph Le Roux?
LIPN - Universite? Paris Nord
Yannick Parmentier?
LIFO - Universite? d?Orle?ans
In this article, we introduce eXtensible MetaGrammar (XMG), a framework for specifying
tree-based grammars such as Feature-Based Lexicalized Tree-Adjoining Grammars (FB-LTAG)
and Interaction Grammars (IG). We argue that XMG displays three features that facilitate
both grammar writing and a fast prototyping of tree-based grammars. Firstly, XMG is fully
declarative. For instance, it permits a declarative treatment of diathesis that markedly departs
from the procedural lexical rules often used to specify tree-based grammars. Secondly, the XMG
language has a high notational expressivity in that it supports multiple linguistic dimensions,
inheritance, and a sophisticated treatment of identifiers. Thirdly, XMG is extensible in that its
computational architecture facilitates the extension to other linguistic formalisms. We explain
how this architecture naturally supports the design of three linguistic formalisms, namely,
FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show
how it permits a straightforward integration of additional mechanisms such as linguistic and
formal principles. To further illustrate the declarativity, notational expressivity, and extensibility
of XMG, we describe the methodology used to specify an FB-LTAG for French augmented with a
? UFR de Linguistique, Universite? Paris Diderot-Paris 7, Case 7003, 2, F-75205 Paris Cedex 13, France.
E-mail: bcrabbe@linguist.jussieu.fr.
?? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: denys.duchier@univ-orleans.fr.
? Laboratoire LORIA - CNRS, Projet Synalp, Ba?timent B, BP 239, Campus Scientifique, F-54506
Vand?uvre-Le`s-Nancy Cedex, France. E-mail: gardent@loria.fr.
? Laboratoire d?Informatique de Paris Nord, UMR CNRS 7030, Institut Galile?e - Universite? Paris-Nord, 99,
avenue Jean-Baptiste Cle?ment, F-93430 Villetaneuse, E-mail: leroux@univ-paris13.fr.
? Laboratoire d?Informatique Fondamentale d?Orle?ans, Ba?timent IIIA, Rue Le?onard de Vinci, B.P. 6759,
F-45067 Orle?ans Cedex 2, France. E-mail: yannick.parmentier@univ-orleans.fr.
Submission received: 27 March 2009; revised version received: 2 July 2012; accepted for publication:
11 August 2012.
doi:10.1162/COLI a 00144
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
unification-based compositional semantics. This illustrates both how XMG facilitates the
modeling of the tree fragment hierarchies required to specify tree-based grammars and of a
syntax/semantics interface between semantic representations and syntactic trees. Finally, we
briefly report on several grammars for French, English, and German that were implemented
using XMG and compare XMG with other existing grammar specification frameworks for
tree-based grammars.
1. Introduction
In the late 1980s and early 1990s, many grammar engineering environments were
developed to support the specification of large computational grammars for natural
language. One may, for instance, cite XLE (Kaplan and Newman 1997) for specifying
Lexical-Functional Grammars (LFG), LKB (Copestake and Flickinger 2000) for speci-
fying Head-driven Phrase Structure Grammars (HPSG), and DOTCCG (Baldridge
et al 2007) for specifying Combinatory Categorial Grammars (CCG). Concretely, such
environments usually rely on (i) a formal language used to describe a target com-
putational grammar, and (ii) a processor for this language, which aims at generating
the actual described grammar (and potentially at checking it, e.g., by feeding it to
a parser).
Although these environments were tailored for specific grammar formalisms, they
share a number of features. Firstly, they are expressive enough to characterize subsets
of natural language. Following Shieber (1984), we call this feature weak completeness.
Secondly, they are notationally expressive enough to relatively easily formalize important
theoretical notions. Thirdly, they are rigorous, that is, the semantics of their underlying
language is well defined and understood. Additionally, for an environment to be useful
in practice, it should be simple to use (by a linguist), and make it possible to detect errors
in the described target grammar.
If we consider a particular type of computational grammar, namely, tree-based
grammars?that is, grammars where the basic units are trees (or tree descriptions) of
arbitrary depth, such as Tree-Adjoining Grammar (TAG; Joshi, Levy, and Takahashi
1975), D-Tree Grammar (DTG; Rambow, Vijay-Shanker, and Weir 1995), Tree Description
Grammars (TDG; Kallmeyer 1999) or Interaction Grammars (IG; Perrier 2000)?
environments sharing all of the listed features are lacking. As we shall see in Section 7
of this article, there have been some proposals for grammar engineering environments
for tree-based grammar (e.g., Candito 1996; Xia, Palmer, and Vijay-Shanker 1999,
but these lack notational expressivity. This is partly due to the fact that tree-based
formalisms offer an extended domain of locality where one can encode constraints
between remote syntactic constituents. If one wants to define such constraints while
giving a modular and incremental specification of the grammar, one needs a high level
of notational expressivity, as we shall see throughout the article (and especially in
Section 4).
In this article, we present XMG (eXtensible MetaGrammar), a framework for
specifying tree-based grammars. Focusing mostly on Feature-Based Lexicalized Tree-
Adjoining Grammars (FB-LTAG) (but using Interaction Grammars [IG] and Multi-
Component Tree-Adjoining Grammars [MC-TAG] to illustrate flexibility), we argue that
XMG departs from other existing computational frameworks for designing tree-based
grammars in three main ways:
 First, XMG is a declarative language. In other words, grammaticality is
defined in an order-independent fashion by a set of well-formedness
592
Crabbe? et al XMG: eXtensible MetaGrammar
constraints rather than by procedures. In particular, XMG permits a
fully declarative treatment of diathesis that markedly departs from the
procedural rules (called meta-rules or lexical rules) previously used to
specify tree-based grammars.
 Second, XMG is notationally expressive. The XMG language supports full
disjunction and conjunction of grammatical units, a modular treatment
of multiple linguistic dimensions, multiple inheritance of units, and a
sophisticated treatment of identifiers. We illustrate XMG?s notational
expressivity by showing (i) how it facilitates the modeling of the tree
fragment hierarchies required to specify tree-based grammars and (ii) how
it permits a natural modeling of the syntax/semantics interface between
semantic representations and syntactic trees as can be used in FB-LTAG.
 Third, XMG is extensible in that its computational architecture facilitates
(i) the integration of an arbitrary number of linguistic dimensions (syntax,
semantics, etc.), (ii) the modeling of different grammar formalisms
(FB-LTAG, MC-TAG, IG), and (iii) the specification of general linguistic
principles (e.g., clitic ordering in French).
The article is structured as follows. Section 2 starts by giving a brief introduction
to FB-LTAG, the grammar formalism we used to illustrate most of XMG?s features. The
next three sections then go on to discuss and illustrate XMG?s three main features?
namely, declarativity, notational expressivity, and flexibility. In Section 3, we focus
on declarativity and show how XMG?s generalized disjunction permits a declarative
encoding of diathesis. We then contrast the XMG approach with the procedural methods
previously resorted to for specifying FB-LTAG. Section 4 addresses notational expressiv-
ity. We present the syntax of XMG and show how the sophisticated identifier handling
it supports or permits a natural treatment (i) of identifiers in tree based hierarchies
and (ii) of the unification-based syntax/semantics interface often used in FB-LTAG. In
Section 5, we concentrate on extensibility. We first describe the operational semantics
of XMG and the architecture of the XMG compiler. We then show how these facilitate
the adaptation of the basic XMG language to (i) different grammar formalisms (IG,
MC-TAG, FB-LTAG), (ii) the integration of specific linguistic principles such as clitic
ordering constraints, and (iii) the specification of an arbitrary number of linguistic
dimensions. In Section 6, we illustrate the usage of XMG by presenting an XMG
specification for the verbal fragment of a large scale FB-LTAG for French augmented
with a unification-based semantics. We also briefly describe the various other tree-
based grammars implemented using XMG. Section 7 discusses the limitations of other
approaches to the formal specification of tree-based grammars, and Section 8 concludes
with pointers for further research.
2. Tree-Adjoining Grammar
A Tree-Adjoining Grammar (TAG) consists of a set of auxiliary or initial elementary
trees and of two tree composition operations, namely, substitution and adjunction.
Initial trees are trees whose leaves are either substitution nodes (marked with ?) or
terminal symbols (words). Auxiliary trees are distinguished by a foot node (marked
with ) whose category must be the same as that of the root node. Substitution inserts a
tree onto a substitution node of some other tree and adjunction inserts an auxiliary tree
593
Computational Linguistics Volume 39, Number 3
N
Marie
Mary
V
V
a
has
V
S
N? V
vu
seen
N?
N
Jean
John
??
S
N
Marie
Mary
V
V
a
has
V
vu
seen
N
Jean
John
Figure 1
Sample derivation of Marie a vu Jean ?Mary has seen John? in a TAG.
into a tree. Figure 1 shows a toy TAG generating the sentence Marie a vu Jean ?Mary has
seen John? and sketches its derivation.1
Among existing variants of TAG, one commonly used in practice is Lexical-
ized FB-LTAG (Vijay-Shanker and Joshi 1988). A lexicalized TAG is such that each
elementary tree has at least one leaf labeled with a lexical item (word), whereas in
an FB-LTAG, tree nodes are additionally decorated with two feature structures (called
top and bottom). These feature structures are unified during derivation as follows. On
substitution, the top features of the substitution node are unified with the top features of
the root node of the tree being substituted in. On adjunction, the top features of the root
of the auxiliary tree are unified with the top features of the node where adjunction takes
place; and the bottom features of the foot node of the auxiliary tree are unified with the
bottom features of the node where adjunction takes place. At the end of a derivation,
the top and bottom feature structures of all nodes in the derived tree are unified.
Implementation of Tree-Adjoining Grammars. Most existing implementations of TAGs fol-
low the three-layer architecture adopted for the XTAG grammar (XTAG Research Group
2001), a feature-based lexicalized TAG for English. Thus the grammar consists of (i) a
set of so-called tree schemas (i.e., elementary trees having a leaf node labeled with a
 referring to where to anchor lexical items2), (ii) a morphological lexicon associating
words with lemmas, and (iii) a syntactic lexicon associating lemmas with tree schemas
(these are gathered into families according to syntactic properties, such as the sub-
categorization frame for verbs). Figure 2 shows some of the tree schemas associated
with transitive verbs in the XTAG grammar. The tree corresponds (a) to a declarative
sentence, (b) to a WH-question on the subject, (c) to a passive clause with a BY-agent,
and (d) to a passive clause with a WH-object. As can be seen, each tree schema contains
an anchor node (marked with ). During parsing this anchor node can be replaced by
any word morphologically related to a lemma listed in the syntactic lexicon as anchor-
ing the transitive tree family.
This concept of tree family allows us to share structural information (tree schemas)
between words having common syntactic properties (e.g., sub-categorization frames).
There still remains a large redundancy within the grammar because many elementary
tree schemas share common subtrees (large coverage TAGs usually consist of hun-
dreds, sometimes thousands, of tree schemas). An important issue when specifying
1 The elementary trees displayed in this article conform to Abeille? (2002), that is, we reject the use of a VP
constituent in French.
2 As mentioned earlier, we describe lexicalized TAG, thus every tree schema has to contain at least one
anchor (node labeled ).
594
Crabbe? et al XMG: eXtensible MetaGrammar
(a) (b)
Sr
NP0 ? VP
V NP1 ?
Sq
NP0 ?
[
wh +
]
[]
Sr
NPNA

VP
V NP1 ?
(c) (d)
Sr [][
mode 3
]
NP1 ? VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Sq
NP1 ?
[
wh +
]
[]
Sr [][
mode 3
]
NPNA

VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
NP0 ?
Figure 2
Some tree schemas for English transitive verbs.
such grammars is thus structure sharing. Being able to share structural information is
necessary not only for a faster grammar development, but also for an easier grammar
maintenance (modifications to be applied to the tree schemas would be restricted to
shared structures). In the next section, we will see how XMG declarativity can be
efficiently used to factorize TAGs. In addition, Section 4 will show how XMG notational
expressivity facilitates the specification of another commonly used tree sharing device,
namely, inheritance hierarchies of tree fragments.
Extending TAG with a Unification-Based Semantics. To extend FB-LTAG with a compo-
sitional semantics, Gardent and Kallmeyer (2003) propose to associate each elementary
tree with a flat semantic representation. For instance, in Figure 3, the trees3 for John, runs,
and often are associated with the semantics l0:name(j,john), l1:run(e,s), and l2:often(x),
respectively. Importantly, the arguments of semantic functors are represented by uni-
fication variables which occur both in the semantic representation of this functor and
on some nodes of the associated syntactic tree. Thus in Figure 3, the semantic index s
occurring in the semantic representation of runs also occurs on the subject substitution
node of the associated elementary tree. The value of semantic arguments is then deter-
mined by the unifications resulting from adjunction and substitution. For instance, the
semantic index s in the tree for runs is unified during substitution with the semantic
index j labeling the root node of the tree for John. As a result, the semantics of John often
runs is {l0:name(j,john), l1:run(e,j), l2:often(e)}.
Gardent and Kallmeyer?s (2003) proposal was applied to various semantic phe-
nomena (Kallmeyer and Romero 2004a, 2004b, 2008). Its implementation, however,
3 Cx/Cx abbreviate a node with category C and a top/bottom feature structure including the feature-value
pair { index : x}.
595
Computational Linguistics Volume 39, Number 3
NPj
John
l0:name(j,john)
Sg
NP?s VPgf
Vfe
runs
l1:run(e,s)
VPx
often VP*x
l2:often(x)
? l0:name(j,john), l1:run(e,j), l2:often(e)
Figure 3
A toy lexicalized FTAG with unification-based semantics (l0, l1, l2, e, and j are constants and
s, f, g, x are unification variables).
relies on having a computational framework that associates syntactic trees with flat
semantic formulae while allowing for shared variables between trees and formulae. In
the following sections, we will show how XMG notational expressivity makes it pos-
sible to specify an FB-LTAG equipped with a unification-based semantics.
3. Declarativity
In this section, we show how a phenomenon which is often handled in a procedural
way by existing approaches can be provided with a declarative specification in XMG.
Concretely, we show how XMG supports a declarative account of diathesis that avoids
the drawbacks of lexical rules (e.g., information erasing). We start by presenting the
lexical rule approach. We then contrast it with the XMG account.
3.1 Capturing Diathesis Using Lexical Rules
Following Flickinger (1987), redundancy among grammatical descriptions is often han-
dled using two devices: an inheritance hierarchy and a set of lexical rules. Whereas
the inheritance hierarchy permits us to encode the sharing of common substructures,
lexical rules (sometimes called meta-rules) permit us to capture relationships between
trees by deriving new trees from already specified ones. For instance, passive trees will
be derived from active ones.
Although Flickinger?s (1987) approach was developed for HPSGs, several similar
approaches have been put forward for FB-LTAG (Vijay-Shanker and Schabes 1992;
Becker 1993; Evans, Gazdar, and Weir 1995; XTAG Research Group 2001). One important
drawback of these approaches, however, is that they are procedural in that the order in
which lexical rules apply matters. For instance, consider again the set of trees given
in Figure 2. In the meta-rule representation scheme adopted by Becker (1993), the base
tree (a) would be specified in the inheritance hierarchy grouping all base trees, and
the derived trees (b, c, d) would be generated by applying one or more meta-rules on
this base tree. Figure 4 sketches these meta-rules. The left-hand side of the meta-rule
is a matching pattern replaced with the right-hand side of the meta-rule in the newly
generated tree. Symbol ??? denotes a meta-variable whose matching subtree in the input
is substituted in place of the variable in the output tree. Given these, the tree family in
Figure 2 is generated as follows: (b) and (c) are generated by application to the base
tree (a) of the Wh-Subject and Passive meta-rules, respectively. Further, (d) is generated
by applying first, the Wh-Subject meta-rule and second, the Passive meta-rule to the
base tree.
596
Crabbe? et al XMG: eXtensible MetaGrammar
Passive meta-rule Wh-Subject meta-rule
Sr
?1 NP? VP
V ?2 NP?
? Sr [][
mode 3
]
?2 NP? VP
[
mode 3
]
?
?
mode 2
passive 1
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
PP
P
by
?1 NP?
Sr
?2NP? ? ?1
? Sq
?2NP? ?
[
wh +
]
[]
Sr
NP?NA

?1
Figure 4
Simplified meta-rules for passive and wh-subject extraction.
More generally a meta-rule is a procedural device that, given a tree instance,
generates a new tree instance by adding, suppressing (hence possibly substituting)
information in grammatical units. Prolo (2002) defines a set of meta-rules that can
be used to specify a large FB-LTAG for English. Given an ordered set of meta-rules,
however, there is no guarantee that the trees they derive are linguistically appropriate
and that the derivation process terminates. Thus, to ensure termination and consistency,
Prolo needs to additionally provide rule ordering schemes (expressed as automata).
3.2 XMG: Capturing Diathesis Using Disjunction
XMG provides an alternative account for describing tree sets such as that of Figure 2
without lexical rules and without the related ordering constraints. In essence, the
approach consists of enumerating trees by combining tree fragments using conjunction
and disjunction.
More specifically, the tree set given in Figure 2 can be generated by combining
some of the tree fragments sketched in Figure 5 using the following conjunctions and
disjunctions:4
Subject ? CanonicalSubject ? Wh-NP-Subject (1)
ActiveTransitiveVerb ? Subject ? ActiveVerb ? CanonicalObject (2)
PassiveTransitiveVerb ? Subject ? PassiveVerb ? CanonicalByObject (3)
TransitiveVerb ? ActiveTransitiveVerb ? PassiveTransitiveVerb (4)
The first clause (Subject) groups together two subtrees representing the possi-
ble realizations of a subject (canonical and wh). The next two clauses define a tree
set for active and passive transitive verbs, respectively. The last clause defines the
TransitiveVerb family as a disjunction of the two verb forms (passive or active). In sum,
the TransitiveVerb clause defines the tree set sketched in Figure 2 as a disjunction of
conjunctions of tree fragments.
One of the issues of meta-rules reported by Prolo (2002) is the handling of feature
equations. For a number of cases (including subject relativization in passive trees),
4 For now, let us consider that the tree fragments are combined in order to produce minimal trees by
merging nodes whose categories (and features) unify. In the next section, we will see how to precisely
control node identification using either node variables or node constraints.
597
Computational Linguistics Volume 39, Number 3
Canonical Subject ? Wh-NP-Subject ? Canonical Object ? Wh-NP-Object ?
Sr
NP? VP
Sq
NP?
[
wh +
]
[]
Sr
NPNA

VP
VP
V NP?
Sq
NP?
[
wh +
]
[]
Sr
VP NPNA

Canonical By Object ? Wh By Object ? Active Verb ? Passive Verb ?
VP
V PP
P
by
NP?
VP
PP
P
by
NP?
V
Sr
VP
V
Sr[]
[
mode 3
]
VP
[
mode 3
]
?
?
passive 1
mode 2
?
?
V
?
?
passive 1 +
mode 2 ppart
?
?
[]
Figure 5
Tree fragments.
ad hoc meta-rules are needed, for a unified tree transformation cannot be defined. In
a declarative approach such as the one here, dealing with feature equations can be
done relatively easily. Let us imagine that we now want to extend the trees of Figure 2
with feature equations for subject?number agreement. We can for instance do so by
defining the following tree fragment (the dashed line indicates that the VP node can be
a descendant, not only a daughter, of the S node):5
SubjAgreement ? S
NP?
[
num 1
]
[
num 1
]
VP
[
num 1
]
[
num 1
]
Then we extend the definition of Subject as follows:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (5)
If we want to get further with the description of transitive verbs, for instance by
taking into account wh-objects and by-objects, this can be done as follows. We first
define the elementary fragments Wh-NP-Object and Wh-By-Object (see Figure 5), and
then define the following additional combinations:6
ActiveTransitiveVerb ? CanonicalSubject ? ActiveVerb ? Wh-Np-Object (6)
PassiveTransitiveVerb ? CanonicalSubject ? PassiveVerb ? Wh-By-Object (7)
5 Note that in XMG, it is not mandatory to define any tree structure inside SubjAgreement. We could define
independent NP and VP nodes, and associate them with variables, say n1 and n2. n1 and n2 would then
be exported and reused directly in the classes CanonicalSubject and Wh-NP-Subject, respectively.
6 Note that these clauses only consider canonical subjects to avoid having both a Wh-subject and a
Wh-object. This is not entirely satisfactory, as we would prefer to define a single abstraction over objects
(as was done for subjects) and use it wherever possible. There would then be another mechanism to
capture this exception and cause the invalid combination to fail (that is, the resulting tree description not
to have any model). Such a mechanism exists in XMG, and is called linguistic principle (see Section 5).
598
Crabbe? et al XMG: eXtensible MetaGrammar
Evans, Gazdar, and Weir (1995) argue for the necessity of using lexical rules for
grammatical description based on two arguments: (i) morphology is irregular and has
to be handled by a non-monotonic device and (ii) erasing rules such as the agentless
passive (John eats an apple / An apple is eaten ) are needed to erase an argument from
the canonical base tree. Neither of these arguments holds here, however: The first
argument because we describe tree schema hence lexical and morphological issues are
ruled out; the second because agentless passive and, more generally, argument erasing
constructions can simply be defined by an additional clause such as:
AgentlessPassiveTransitiveVerb ? Subject ? PassiveVerb (8)
To summarize, using a declarative language to specify a tree-based grammar offers
an adequate level of control on the structures being described while avoiding having
to deal with ordering and termination issues. It facilitates grammar design and mainte-
nance, by providing an abstract view on grammar trees, uniquely made of monotonic
(no information removal) combinations of tree fragments.
4. Notational Expressivity
We now focus on notational expressivity and show how XMG supports a direct
encoding of (i) distinct linguistic dimensions (here syntax, semantics and the syntax/
semantics interface) and (ii) the various types of coreferences7 that arise in the devel-
opment of tree-based grammars.
The syntax of the XMG language can be formally defined as follows.
Class ::= NameC1,...,Ckx1,...,xn ? Content (9)
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
(10)
SYN ::=
n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
(11)
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM (12)
DYN ::= ? f1 : v1,...,fn : vn ? (13)
Here and in what follows, we use the following notational conventions. Ci denote
variables over class names; xi, x, and y are variables ranging over tree nodes or feature
values; ni refer to node variables; f, fi are features and v, vi and feature values (constants
or variables); li, hj, p, and Ei are variables over semantic labels, semantic holes, predi-
cates, and predicate arguments in flat semantic formulae, respectively.8 [ ] are used to
associate a node variable with some feature constraint. ( ) are used to associate a node
variable with some property constraint (e.g., node colors, see Section 5). ci and cvi denote
7 By coreference, we mean the sharing of information between distinct elementary fragments of the
grammar specification.
8 See Gardent and Kallmeyer (2003) for a detailed introduction to flat semantics.
599
Computational Linguistics Volume 39, Number 3
a property constraint and a property constraint value, respectively. Ci.y denotes the y
variable declared in class Ci and = is unification; ? and ? denote linear precedence and
immediate dominance relations between nodes. Finally, +, ? represent the transitive and
transitive-reflexive closure of a relation, respectively.
The first two clauses of the formal definition here specify XMG classes and how they
combine. The next three clauses define the languages supported for describing three lin-
guistic dimensions, namely, syntax (SYN), semantics (SEM), and the syntax/semantics
interface (called DYN for dynamic interface). We now discuss each of these in more
detail starting bottom?up with the three linguistic dimensions and ending with the
control language that permits us to combine basic linguistic units into bigger ones.
SYN. The XMG formalism for syntax (copied here for convenience) is a tree description
logic similar to that proposed by Vijay-Shanker and Schabes (1992) and Rogers and
Vijay-Shanker (1994) to describe tree-based grammars.
SYN ::= n1 ? n2 | n1 ?+ n2 | n1 ?? n2 | n1 ? n2 | n1 ?+ n2 | n1 ?? n2 |
n1[f1 : v1,..., fk : vk] | n1(c1 : cv1,..., cl : cvl) | n1 = n2 | x = Ci.y |
n1 (c1 : cv1,..., cl : cvl) [f1 : v1,..., fk : vk] | SYN ? SYN
It includes tree node variables, feature names, feature values, and feature variables.
Tree node variables can be related by equality (node identification), precedence (imme-
diate or non-immediate), and dominance (immediate or non-immediate). Tree nodes
can also be labeled with feature structures of depth 2, that is, sets of feature/value
pairs where feature values are either variables, constants (e.g., syntactic category), or
non-recursive feature structure (e.g., top and bottom feature structures).
Here is a graphical illustration of how tree logic formulae can be used to describe
tree fragments: The depicted tree fragment is a model satisfying the given formula.
n1 ? n2 ? n1 ? n3 ? n2 ? n3
? n1[cat : S] ? n2(mark : subst) [cat : NP] ? n3[cat : VP]
S
NP? VP
One distinguishing feature of the XMG tree language is the introduction of node
constraints (n1(c : cv)) that generalize Muskens and Krahmer?s (1998) use of positive
and negative node markings. Concretely, node constraints are attribute-value matri-
ces, which contain information to be used when solving tree descriptions to produce
grammar trees. In other words, node constraints are used to further restrict the set
of models satisfying a tree description. As an example of node constraint, consider
node annotations in FB-LTAG (foot node, substitution node, null-adjunction, etc.). Such
annotations can be used as node constraints to allow the description solver to apply
well-formedness constraints (e.g., there is at most one foot node).
Another interesting feature of XMG concerns the inclusion of the dot operator,
which permits us to identify variables across classes in cases where name sharing cannot
be resorted to. When a variable y is declared in a class C, the latter being instantiated
within a class D, y can be accessed from D by C.y (the identifier y still being available
in D?s namespace).
600
Crabbe? et al XMG: eXtensible MetaGrammar
SEM. The semantic dimension supports a direct encoding of the flat semantic formulae
used by Gardent and Kallmeyer (2003):
SEM ::= li : p(E1,...,En) | li ? hj | SEM ? SEM
where li : p(E1,..., En) represents a predicate p with label li and arguments E1,..., En and
li ? hj is a scope constraint between label li and scope hj. Expressions (predicate argu-
ments Ei) can refer to semantic holes, constants (atomic values), or unification variables
(written x, y hereafter).
For instance, the following flat semantic formula can be used to underspecify the
meaning of the sentence ?Every dog chases a cat?:
l0 : ?(x, h1, h2) ? l1 ? h1 ? l1 : Dog(x) ? l2 ? h2 ? l2 : Chase(x, y)
? l3 : ?(y, h3, h4) ? l4 ? h3 ? l4 : Cat(y) ? l2 ? h4
(14)
This formula denotes the following two first-order logic formulae, thereby describing
the two possibles readings of this sentence.9
l0 : ?(x, l1, l3) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l2) ? l4 : Cat(y) (15)
l0 : ?(x, l1, l2) ? l1 : Dog(x) ? l2 : Chase(x, y) ? l3 : ?(y, l4, l0) ? l4 : Cat(y) (16)
DYN. The DYN dimension generalizes Kinyon?s hypertag (Kinyon 2000) which is
unified whenever two tree fragments are combined. Similarly, in XMG the DYN
dimension is a feature structure that is unified whenever two XMG classes are com-
bined through inheritance or through conjunction (see the discussion on XMG control
language, subsequently).
For instance, the following constraints ensure a coreference between the index I
occurring in the syntactic dimension and the argument X occurring in the semantic
dimension (indexsubject and arg1 are feature names, and E, I, X, and V local unification
variables).
C1 ? Node [idx : I] ? ?indexsubject : I? (17)
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X? (18)
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V? (19)
More generally, the DYN dimension permits us to unify nodes and feature values
that belong to distinct classes and dimensions, and are thus often not related within
the inheritance hierarchy. As we shall see in Section 6, the DYN dimension permits
a modular account of the syntax/semantics interface in which linking constraints can
be stipulated separately and reused to specify the various diatheses.
In other words, the DYN feature structure allows us to extend the scope of some
specific variables so that they can be unified with variables (or values) introduced
in some other classes of the metagrammar. This concept of scope extension can be
compared with that of hook in Copestake, Lascarides, and Flickinger (2001).
9 For more details on the interpretation of flat semantics and on its association with a grammar of natural
language, see Gardent (2008).
601
Computational Linguistics Volume 39, Number 3
Control language. The linguistic units (named Content here) defined by the linguist can
be abstracted and combined as follows:
Class ::= NameC1,...,Ckx1,...,xn ? Content
Content ::= ?SYN, SEM, DYN? | Name | Content ? Content | Content ? Content
The first clause states that the linguistic information encoded in Content is abstracted in
a class named Name and that this class inherits classes C1,..., Ck and exports variables
x1,..., xn. That is, XMG allows for abstraction, inheritance, and variable exports. By
default, variables (referring to nodes and feature values) are local to a class. Export
statements extend the scope of a variable to all sub-classes, however. An exported
variable can also be accessed from outside its class in case of class instantiation (using
the dot operator introduced earlier in this section). The second clause states that an
XMG class consists of a syntactic, a semantic, and a dynamic description (each of them
possibly empty), and that XMG classes can be combined by conjunction and disjunc-
tion and reused through class instantiation. The notation ?SYN, SEM, DYN? represents
simultaneous contributions (possibly empty) to all three dimensions.10
The XMG control language differs from other frameworks used to specify tree-
based grammars (Vijay-Shanker and Schabes 1992; Xia et al 1998; Candito 1999)
in two main ways. First, it supports generalized conjunctions and disjunctions of
classes. As shown in Section 3, this permits us, inter alia, a declarative treatment of
diathesis.
Second, it allows for both local and exported variables. As mentioned in Section 3, a
common way to share structure within a tree-based grammar is to define an inheritance
hierarchy of either tree fragments (Evans, Gazdar, and Weir 1995) or tree descriptions
(Vijay-Shanker and Schabes 1992; Candito 1996; Xia 2001). When considering an FB-
LTAG augmented with unification semantics, the hierarchy will additionally contain
semantic representations and/or tuples made of tree fragments and semantic represen-
tations. In all cases, the question arises of how to handle identifiers across classes and,
more specifically, how to share them.
In Candito?s (1996) approach, tree nodes are referred to using constants so that
multiple occurrences of the same node constant refer to the same node. As pointed out
in Gardent and Parmentier (2006), global names have several non-trivial shortcomings.
First, they complicate grammar writing in that the grammar writer must remember the
names used and their intended interpretation. Second, they fail to support multiple uses
of the same class within one class. For instance, in French, some verbs sub-categorize
for two prepositional phrases (PP). A natural way of deriving the tree for such verbs
would be to combine a verbal tree fragment with two instances of a PP fragment. If,
however, the nodes in the PP fragment are labeled with global names, then the two
occurrences of these nodes will be identified thereby blocking the production of the
appropriate tree.11
A less restrictive treatment of identifiers is proposed by Vijay-Shanker and Schabes
(1992), where each tree description can be associated with a set of declared node
variables and subsets of these node variables can be referred to by descriptions in the
10 Although formally precise, this notation can be cumbersome. In the interest of legibility we adopt
throughout the convention that SYN stands for ?SYN, , ?, SEM for ? , SEM, ?, and DYN for ? , , DYN?.
11 An analogous situation may arise in English with ditransitive verbs requiring two direct objects.
602
Crabbe? et al XMG: eXtensible MetaGrammar
hierarchy that inherit from the description in which these node variables were declared.
For instance, if entity A in the hierarchy declares such a special node variable X and B
inherits from A, then X can be referred to in B using the notation A.X.12
XMG generalizes Vijay-Shanker and Schabes?s (1992) approach by integrating an
export mechanism that can be used to extend the scope of a given identifier (node
or feature value variable) to classes that inherit from the exporting class. Thus if
class B inherits from class A and class A exports variable X, then X is visible in B
and its reuse forces identity. If B inherits from several classes and two (or more) of
these inherited classes export the same variable name X, then X is not directly visible
from B. It can be accessed though using the dot operator. First A is identified with a
local variable (e.g., T = A), then T.X can be used to refer to the variable X exported
by A.
To summarize, XMG allows for local variables to be exported to sub-classes as well
as for prefixed variables?that is, variables that are prefixed (using the dot operator)
with a reference to the class in which they are declared. In this way, the pitfalls in-
troduced by global names are avoided while providing enough expressivity to handle
variable coreference (via the definition of variable namespaces). Section 6 will further
illustrate the use of the various coreference devices made available by XMG showing
how they concretely facilitate grammar writing.
Let us finally illustrate variable handling with XMG in the example of Figure 2.
Recall that we define the trees of Figure 2 as the conjunctions and disjunctions of some
tree fragments of Figure 5, such as:
Subject ? SubjAgreement ? ( CanonicalSubject ? Wh-NP-Subject ) (20)
CanonicalSubject can be defined as a tree description formula as follows (only variables
n2 and n3 are exported):
CanonicalSubjectn2,n3 ?
n1 ? n2 ? n1[cat : S] ? n2(mark : subst) [cat : NP]?
n1 ? n3 ? n3[cat : VP] ? n2 ? n3
(21)
The class Wh-NP-Subject is defined accordingly (i.e., by means of a slightly more
complex tree description formula using the n2 and n3 variable identifiers to refer to
the nodes involved in subject agreement). The class SubjAgreement is defined slightly
differently (we do not impose any tree relation between the node concerned with
number agreement):
SubjAgreementn1,n2 ?
n1 [[top : [num : x]] [bot : [num : x]]]?
n2 [[top : [num : x]] [bot : [num : x]]]
(22)
12 In fact, the notation used by Vijay-Shanker and Schabes (1992) is attr:X with attr an attribute variable
ranging over a finite set of attributes, to indicate special node variables that scope outside their class; and
attr(A) to refer to such variables from outside the entity in which they were declared. We use a different
notation here to enforce consistency with the XMG notation.
603
Computational Linguistics Volume 39, Number 3
We can then explicitly control the way the fragments combine as follows:
Subject ?
C1 = SubjAgreementn1,n2 ?
C2 = ( CanonicalSubjectn2,n3 ? Wh-NP-Subjectn2,n3 ) ?
C1.n1 = C2.n2 ? C1.n2 = C2.n3
(23)
In this example, we see how to constrain, via variable export and unification, some
given syntactic nodes to be labeled with feature structures defined somewhere else in
the metagrammar. We use XMG?s flexible management of variable scope to deal with
node coreference. Compared with previous approaches on metagrammars such as those
of Candito (1996), Xia (2001), having the possibility of handling neither only global nor
only local variables, offers a high level of expressivity along with a precise control on
the structures being described.
5. Extensibility
A third distinguishing feature of XMG is extensibility. XMG is extensible in that
(i) dimensions can be added and (ii) each dimension can be associated with its own
interpreter. In order to support an arbitrary number of dimensions, XMG relies on a
device permitting the accumulation of an arbitrary number of types of literals, namely,
Extensible Definite Clause Grammar (EDCG) (Van Roy 1990). Once literals are accumu-
lated according to their type (i.e., each type of literals is accumulated separately), they
can be fed to dedicated interpreters. Because each of these sets of literals represents
formulas of a description language, these interpreters are solvers whose role is to
compute models satisfying the accumulated formulas.
Via this concept of separated dimensions, XMG allows us (i) to describe different
levels of language (not only syntax, but also semantics and potentially morphology,13
etc.), and (ii) to define linguistic principles (well-formedness constraints to be applied on
the structures being described). These principles depend either on the dimension (e.g.,
scope constraints in flat semantics), the target formalism (e.g. cooccurrence predicate-
arguments in FB-LTAG), or the natural language (e.g., clitic ordering in Romance lan-
guages) being described.
In what follows, we start by showing how XMG handles dimensions independently
from each other introducing EDCG (Section 5.1). We then summarize the architecture
of the XMG system (Section 5.2). We finally show how different solvers can be used
to implement various constraints on each of these dimensions (Section 5.3). In partic-
ular, we discuss three kinds of extensions implemented in XMG: extension to several
grammar formalisms, integration of explicit linguistic generalizations, and inclusion of
color-based node marking to facilitate grammar writing.
5.1 XMG: Accumulating and Interpreting an Arbitrary Number of Descriptions
Accumulating (tree) descriptions. First, let us notice that XMG is nothing other than a logic
language a` la Prolog (Duchier, Parmentier, and Petitjean 2012). More precisely, an XMG
13 Recently, XMG has been used to describe the morphology of verbs in Ikota, a Bantu language spoken in
Gabon (Duchier, Parmentier, and Petitjean 2012).
604
Crabbe? et al XMG: eXtensible MetaGrammar
specification is a collection of Horn clauses, which contribute a declarative description
of what a computational tree grammar is.
Logic Program XMG Metagrammar
Clause ::= Head ? Body
Body ::= Fact | Head |
Body ? Body |
Body ? Body
Query ::= Head
Class ::= Name ? Content
Content ::= Description | Name |
Content ? Content |
Content ? Content
Axiom ::= Name
Recall that the descriptions handled by XMG are in fact tuples of the form
?SYN, SEM, DYN?. An XMG class can thus describe, in a non-exclusive way, any of these
three levels of description. If one wants to add another level of description (i.e., another
dimension), one needs to extend the arity of this tuple. Before discussing this, let us first
see how such tuples are processed by XMG.
As mentioned earlier, XMG?s control language is comparable to Horn clauses.
A common way to represent Horn clauses is by using Definite Clause Grammar
(DCG) (Pereira and Warren 1980). Concretely, a DCG is a rewriting system (namely, a
context-free grammar), where the symbols of the rewriting rules are equipped with
pairs of unification variables (these are usually called difference list or accumulator)
(Blackburn, Bos, and Striegnitz 2006, page 100). As an illustration, consider the follow-
ing toy example.
s --> np,vp. np --> det,n.
vp --> v,np. vp --> v.
det --> [the]. det --> [a].
n --> [cat]. n --> [mouse].
v --> [eats].
The string language described by this DCG can be obtained by submitting the query
s(X,[]) where X is a unification variable to be bound with lists of facts (these being the
sentences belonging to the string language). As we can easily see, this language contains
the sentences ?a cat eats,? ?the cat eats,? ?a mouse eats,? ?the mouse eats,? ?a cat eats a
mouse,? ?a mouse eats a cat,? and so on.
Similarly, we can represent XMG classes as DCG clauses. For instance, the combina-
tions of syntactic fragments given in relations (1)?(4) can be rewritten as DCG clauses
as follows:
subject --> canonicalSubject.
subject --> whNpSubject.
activeTransitiveVerb --> subject, activeVerb, canonicalObject.
passiveTransitiveVerb --> subject, passiveVerb, canonicalByObject.
transitiveVerb --> activeTransitiveVerb.
transitiveVerb --> passiveTransitiveVerb.
Disjunctions (e.g., the subject specification) translate to multiple clauses with iden-
tical heads and conjunctions (e.g., activeTransitiveVerb) to a clause body.
In our case, the terminal symbols of the underlying DCG are not just facts, but
tuples of descriptions. In other words, the DCG clause whose head is canonicalSubject
is associated with a tuple of the following form (the dots have to be replaced with
605
Computational Linguistics Volume 39, Number 3
adequate descriptions, these can contain unification variables, whose scope is by default
local to the clause):
canonicalSubject --> [desc(syn(...),sem(...),dyn(...))].
In order to allow for an extension of XMG to an arbitrary number of dimensions,
instead of compiling XMG classes into a DCG whose accumulator stores tuples with
a fixed arity, these classes are compiled into an EDCG (Van Roy 1990). EDCG are DCG
with multiple accumulators. In XMG, each dimension is thus allocated a dedicated
accumulator in the underlying EDCG.
Note that although the content of the various dimensions is accumulated separately,
dimensions may nevertheless share information either via local unification variables
(if the XMG class defines several dimensions locally), via exported unification vari-
ables (in case of class instantiation or inheritance), or via the shared unification variables
supported by the DYN dimension.
At the end of the EDCG execution, we obtain, for each axiom of the metagrammar
(i.e., for each class name to be valuated), a list of description formulas per accumulator.
These lists are grouped together into a tuple of lists of the following form (N is the
number of dimensions, and consequently of accumulators):
desc(accu1(L1),accu2(L2), ... ,accuN(LN))
Each element (i.e., list Li) of such a tuple is a complete description of a given dimension,
where shared variables have been unified (via unification with backtracking).
Solving (tree) descriptions. As illustrated earlier, interpreting XMG?s control language in
terms of an EDCG yields tuples whose arity is the number of dimensions defined by
the linguist, that is, triples of the form ?SYN, SEM, DYN? if syntax, semantics, and the
dynamic interface are described.
For each dimension D, XMG includes a constraint solver SD that computes the set of
minimal models MD = SD(dD) satisfying the description (dD) of that dimension. In other
words, each dimension is interpreted separately by a specific solver. For instance, the
syntactic dimension is handled by a tree description solver that produces, for a given
tree description, the set of trees satisfying that description, whereas the solver for the
semantic dimension simply outputs the flat semantic representation (list of semantic
literals) built by the EDCG through accumulation.
Note that, although solvers are distinct, the models computed in each dimension
may nonetheless be coupled through shared variables. In that case, these variables can
constrain the models computed by the respective solvers. For instance, shared variables
can be used for the syntactic tree description solver to be parametrized by some value
coming from the semantic input description. Note that the output of the solving process
is a Cartesian product of the sets of minimal models of each solver. As a consequence,
the worst case complexity of metagrammar compilation is that of the various solvers
associated with relevant dimensions.
In addition to having separate solvers for each dimension, the constraint-solving
approach used in XMG permits us to modularize a given solver by combining different
principles. Each such principle enforces specific constraints on the models satisfying
the description of a given dimension. For instance, for the syntactic dimension of an
FB-LTAG, a set of principles is used to enforce that the structures produced by the
compiler are trees, and that these conform to the FB-LTAG formalism (e.g., there is no
tree having two foot nodes).
606
Crabbe? et al XMG: eXtensible MetaGrammar
5.2 Architecture
The XMG compiler14 consists of the following three modules:
 A compiler that parses XMG?s concrete syntax and compiles XMG classes
into clauses of an EDCG.
 A virtual machine (VM), which interprets EDCG. This VM performs
the accumulation of dimensions along with scope management and
identifiers resolution. This VM is basically a unification engine equipped
with backtracking, and which is extended to support EDCG. Although its
architecture is inspired by the Warren Abstract Machine (A??t-Kaci 1991),
it uses structure-sharing to represent and unify prolog terms, and, given
a query on a class, processes the conjunctions, disjunctions, inheritance,
and export statements related to that class to produce its full definition,
namely, a tree description for the SYN dimension, a flat semantic formula
for the SEM dimension, and a feature structure for the DYN dimension.
 A constraint-solving phase that produces for each dimension the minimal
models satisfying the input description as unfolded by the preceding
two steps.
As already mentioned, the first part is extensible in that new linguistic dimensions
can be added by specifying additional dedicated accumulators to the underlying EDCG.
The second part is a unification engine that interprets EDCG while performing both term
unification and polarized unification (i.e., unification of polarized feature structures, as
defined by Perrier [2000], and discussed in Section 5.3.1). This extended unification is
the reason why XMG does not merely recourse to an existing Prolog engine to process
EDCG, but relies on a specific VM instead.
The third part is completely modular in that various constraint solvers can be
plugged in depending on the requirements set by the dimensions used, and the chosen
grammatical framework. For instance, the SYN dimension is solved in terms of tree
models, and the SEM dimension is solved in terms of underspecified flat semantic
formulae (i.e., the input semantics remains untouched modulo the unification of its
shared variables).
Importantly, these additional solvers can be ?turned on/off? (via a primitive of the
XMG language) so that, for instance, the same processor can be used to compile an
XMG specification for an FB-LTAG using linguistic principles such as those defined in
the next section (i.e., clitic ordering principle) or not.
5.3 Three Extensions of XMG
We now show (i) how the modular architecture of the XMG compiler permits us
to specify grammars for several tree-based linguistic formalisms; (ii) how it can be
extended to enforce language specific constraints on the syntactic trees; and (iii) how
additional formal constraints (namely node marking) can be integrated to simplify node
identifications (and consequently grammar writing).
14 The XMG compiler is open source software released under the terms of the CeCILL GPL-compliant
licence. See http://sourcesup.renater.fr/xmg.
607
Computational Linguistics Volume 39, Number 3
Eq
Up
Down
Left
Right
Figure 6
Partition of the nodes of tree models.
5.3.1 TAG, MC-TAG, and IG: Producing Trees, Tree Sets, or Tree Descriptions. XMG in-
tegrates a generic tree solver that computes minimal tree models from tree descrip-
tion logic formulae built on the language SYN introduced in Section 4. This solver
integrates the dominance solving technique proposed by Duchier and Niehren (2000)
and can be summarized as follows. A minimal tree model is described in terms of
the relative positions of its nodes. For each node n in a minimal tree model T, the
set of all the nodes of T can be partitioned in five subsets, depending on their po-
sition relative to n. Hence, for each node variable n appearing in a tree description,
it is first associated with an integer (called node id). We then define the five sets
of node ids (i.e., sets of integers) Downn, Upn, Leftn, Rightn, and Eqn referring to the
ids of the nodes located below, above, on the left, on the right, or identified with n,
respectively (see Figure 6). Note that we require that these sets are a partition of all
node ids.
Using this set-based representation of a model, we translate each node relation
from the input formula (built on the tree description language introduced in Section 4)
into constraints on the sets of node ids that must hold in a valid model. For instance,
the sub-formula n1 ?+ n2, which states that node n1 strictly precedes node n2, is
translated into:
n1 ?+ n2 ? EqDownn1 ? Leftn2 ? EqDownn2 ? Rightn1?
Rightn2 ? Rightn1 ? Leftn1 ? Leftn2
(24)
where15 EqDownx = Eqx unionmulti Downx for x ? {n1, n2}. In other words, in a valid minimal
tree model, the set of nodes below or equal to n1 is included in the set of nodes (strictly)
on the left of n2, the set of nodes below or equal to n2 is included in the set of nodes
(strictly) on the right of n1, the set of nodes on the right of n2 is included in the set of
nodes on the right of n1, and finally the set of nodes on the left of n1 is included in the
set of nodes on the left of n2.
Once all input relations are translated into set constraints, the solver uses standard
Constraint Satisfaction techniques (e.g., a first-fail exploration of the search tree) to find a
set of consistent partitions. Finally, the nodes of the models are obtained by considering
nodes with distinct Eqn.
15 unionmulti represents disjoint union.
608
Crabbe? et al XMG: eXtensible MetaGrammar
FB-LTAG trees. To support the specification of FB-LTAG trees, the XMG compiler extends
the generic tree solver described here with a set of constraints ensuring that the trees are
well-formed TAG trees. In effect, these constraints require the trees to be linear ordered
trees with appropriate decorations. Each node must be labeled with a syntactic category.
Leaf nodes are either terminal, foot, or substitution nodes. There is at most one foot
node per tree and the category of the foot node must be identical to that of the root
node. Finally, each tree must have at least one leaf node that is an anchor.
MCTAG tree sets. Where FB-LTAG consists of trees, MC-TAG (Weir 1988) consists of sets
of trees. To support the specification of MC-TAG, the sole extension needed concerns
node variables that are not dominated by any other node variable in the tree description.
Whereas for FB-LTAG, these are taken to denote either the same root node or nodes that
are connected to some other node (i.e., uniqueness of the root), for MC-TAG they can
be treated as distinct nodes, thereby allowing for models that are sets of trees rather
than trees (Parmentier et al 2007). In other words, the only modification brought to the
tree description solver is that, in MC-TAG mode, it does not enforce the uniqueness of
a root node in a model.
IG polarized tree descriptions. IG (Perrier 2000) consist of tree descriptions whose node
variables are labeled with polarized feature structures. A polarized feature structure is
a set of polarized feature triples (f, p, v) where f and v are standard features and feature
values, respectively, and p is a polarity value in {?,?,=,?}. Polarities are used to
guide parsing in that a valid derivation structure must neutralize polarities.
To support an XMG encoding of IG, two extensions are introduced, namely, (i) the
ability to output tree descriptions rather than trees, and (ii) the ability to write polarized
feature structures. The first extension is trivially realized by specifying a description
solver that ensures that any output description has at least one tree model. For the
second point, the SYN language is extended to define polarized feature structures and
the unification engine to support unification of polarized features (for instance, a ?
feature will unify with a neutral (=) feature to yield a ? polarized feature value triple).
5.3.2 Adding Specific Linguistic Constraints: The Case of Clitics. XMG can be extended
to support specific constraints on tree descriptions (e.g., constraints on node linear
order), which make it possible to describe linguistic-dependent phenomena, such as,
for instance, clitic ordering in French, at a meta-level (i.e., within the metagrammar).
According to Perlmutter (1970), clitics are subject to two hard constraints. First,
they appear in front of the verb in a fixed order according to their rank (Exam-
ples 25a and 25b).16 Second, two different clitics in front of the verb cannot have the
same rank (Example 25c).
(25) a. Jean le3 lui4 donne.
?John gives it to him.?
b. *Jean lui4 le3 donne.
*?John gives to him it.?
c. *Jean le3 la3 donne.
*?John gives it it.?
16 In (Examples 25a?c), the numbers on the clitics indicate their rank.
609
Computational Linguistics Volume 39, Number 3
S
N? ?+ V?
?
V?
Cl?3 ?+ V
?
V?
Cl?4 ?+ V
?
S
V?
V
?
S
N? V?
Cl?3 Cl?4 V
S
N? V?
Cl?4 Cl?3 V
Figure 7
Clitic ordering in French.
To support a direct encoding of Perlmutter?s observation, XMG includes both a
node uniqueness principle and a node ordering principle. The latter allows us to label
nodes with some property (let us call it rank) whose value is an integer (for instance,
one can define a node as n1(rank : 2)[cat : Cl]). When solving tree descriptions, XMG
further requires that in a valid tree model, (i) there are no two nodes with the same
rank and (ii) sibling nodes labeled with a rank are linearly ordered according to their
rank.
Accordingly, in the French grammar of Crabbe? (2005), each node labeled with a clitic
category is also labeled with a numerical node property representing its rank.17 XMG
ordering principle then ensures that the ill-formed tree crossed out in Figure 7 is not
produced. Note that in Figure 7, every type of clitic is defined locally (i.e., in a separate
class), and that the interactions between these local definitions are handled by XMG
using this rank principle, to produce only one valid description (pictured to the right of
the arrow).
That is, XMG ordering constraints permit a simple, declarative encoding of the
interaction between clitics. This again contrasts with systems based on lexical rules. As
noted by Perlmutter (1970), if clitics are assumed to be moved by transformations, then
the order in which lexical rules apply this movement must be specified.
To implement the uniqueness principle, one needs to express the fact that in a valid
model ?, there is only one node having a given property p (i.e., a parameter of the
constraint, here the value of the rank node property). This can be done by introducing,
for each node n of the description, a Boolean variable pn indicating whether the node
denoting n in the model has this property or not (i.e., are there two nodes of identical
rank?). Then, if we call V?p the set of integers referring to nodes having the property p in
a model, we have: pn ? (Eqn ? V
?
p ) = ?. Finally, if we represent pn being true with 1 and
pn being false with 0,18 and we sum pn for each n in the model, we have that in a valid
model this sum is strictly lower than 2:
?
n?? pn < 2.
To implement the ordering principle, one needs to express the fact that in a valid
model ?, two sibling nodes n1 and n2 having a given property p of type integer and
of values p1 and p2, respectively, are such that the linear precedence between these
nodes conform to the natural order between p1 and p2. This can be done by first
introducing, for each pair of nodes n, m of the description, a Boolean variable bn,m
indicating whether they have the same ancestors: bn,m ? (Upn ? Upm) = (Upn ? Upm).
For each pair of nodes that do so, we check whether they both have the property p,
17 Recall that node properties are features whose values are used by the tree description solver in order to
restrict the set of valid models. These properties may not appear in the trees produced from the input
metagrammar. For instance, the rank property is not part of the FB-LTAG formalism, and thus does not
appear in the FB-LTAG elementary trees produced by XMG.
18 These integer representations are usually called reified constraints.
610
Crabbe? et al XMG: eXtensible MetaGrammar
and if this is the case, we add to the input description a strict precedence constraint on
these nodes according to their respective values of the property p:19
bn,m ? (pn < pm) ? n ?+ m (26)
bn,m ? (pm < pn) ? m ?+ n (27)
5.3.3 Adding Color Constraints to Facilitate Grammar Writing. To further ease grammar
development, XMG supports a node coloring mechanism that permits nameless node
identification (Crabbe? and Duchier 2004), reminiscent of the polarity-based node iden-
tification first proposed by Muskens and Krahmer (1998) and later used by Duchier
and Thater (1999) and Perrier (2000). Such a mechanism offers an alternative to explicit
node identification using equations between node variables. The idea is to label node
variables with a color property, whose value (either red, black, or white) can trigger
node identifications.
This mechanism is another parameter of the tree solver. When in use, the valid
tree models must satisfy some color constraints, namely, they must only have red or
black nodes (no remaining white nodes; these have to be identified with some black
nodes). As shown in the following table, node identification must observe the following
constraints: A white node must be identified with a black node; a red node cannot be
identified with any other node; and a black node may be identified with one or more
white nodes.20
?B ?R ?W ?
?B ? ? ?B ?
?R ? ? ? ?
?W ?B ? ?W ?
? ? ? ? ?
We now briefly describe how the constraint solver sketched in Section 5.3.1 was
extended to support colors. As mentioned previously, in valid models all white nodes
are identified with a black node (at most one black node per white node). Consequently,
there is a bijection from the red and black nodes of the tree description to the nodes of
the model. In order to take this bijection into account, we add a node variable RBn to
the five sets already associated with a node variable n from Section 5.1. RBn denotes
either n if n is a black or red node, or the black node identified with n if n is a white
node. Note that all the node variables must be colored: the set of node variables in a
tree description can then be partitioned into three sets: Red, Black, and White. Basically,
we know that, for all nodes n, RBn ? Eqn (this is what the bijection is about). Again
we translate color information into constraints on node sets (these constraints help the
generic tree solver by reducing the ambiguity for the Eqn sets):
n ? Red ? (n = RBn) ? (Eqn = {n}) (28)
n ? Black ? (n = RBn) ? (Eqn\{n} ? White) (29)
n ? White ? (RBn ? Black) ? (Eqn ? Black = {RBn}) (30)
19 In fact, rather than adding strict precedence constraints to the tree description, we directly add to the
solver their equivalent set constraints on Eq, Up, Left, Right, Down, introduced earlier.
20 In other words, node colors can be seen as information on node saturation.
611
Computational Linguistics Volume 39, Number 3
Node coloring offers an alternative to complex namespace management. The main
advantage of this particular identification mechanism is its economy: Not only is there
no longer any need to remember node identifiers, there is in fact no need to choose a
name for node variables.
It is worth stressing that the XMG node identification process is reduced to a
constraint-solving problem and so it is not a sequential process. Thus the criticisms
leveled by Cohen-Sygal and Wintner (2007, 2009) against non-associative constraints
on node unification do not apply.
Briefly, in their work, Cohen-Sygal and Wintner (2007, 2009) showed that any
polarity-based tree description formalism is not associative. In other words, when
describing trees in terms of combinations of polarized structures, the order in which
the structures are combined matters (i.e., the output structures depend on the combi-
nation order). This feature makes such formalisms not appropriate for a modular and
collaborative grammar engineering, such as that of Cohen-Sygal and Wintner (2011) for
Unification Grammar.
In the XMG case, when using node colors, the tree description solver does not
rely on any specific fragment combination order. It computes all possible combination
orders. In this context, the grammar designer cannot think in terms of sequences of node
identifications. This would lead to tree overgeneration.
Again, it is important to remember that tree solving computes any valid tree model,
independently of any specific sequence of node identifications (all valid node identifica-
tions are computed). In this context, non-associativity of color-based node identification
is not an issue, but rather a feature, as it allows for a compact description of a large
number of node identifications (and thus of tree structures).
6. Writing Grammars with XMG
In this section, we first provide a detailed example showing how XMG can be used to
specify the verbal trees of a large FB-LTAG for French extended with unification-based
semantics. We then give a brief description of several large- and middle-scale grammars
that were implemented using XMG.
6.1 SEMTAG: A large FB-LTAG for French Covering Syntax and Semantics
We now outline the XMG specification for the verbal trees of SEMTAG, a large FB-LTAG
for French. This specification further illustrates how the various features of XMG (e.g.,
combined use of disjunction and conjunction, node colors) permit us to specify compact
and declarative grammar descriptions. We first discuss the syntactic dimension (SYN).
We then go on to show how the semantic dimension (SEM) and the syntax/semantic
interface (DYN) are specified.
6.1.1 The Syntactic Dimension. The methodology used to implement the verbal fragment
of SEMTAG can be summarized as follows. First, tree fragments are defined that rep-
resent either a possible realization of a verb argument or a possible realization of the
verb. The verbal elementary TAG trees of SEMTAG are then defined by appropriately
combining these tree fragments.
To maximize structure sharing, we work with four levels of abstraction. First, basic
tree fragments describing verb or verb argument realizations are defined. Second, gram-
matical functions are defined as disjunctions of argument realizations. Third, verbal
diathesis alternatives are defined as conjunctions of verb realizations and grammatical
612
Crabbe? et al XMG: eXtensible MetaGrammar
CanonSubj ?
S?W
N??R V?W CanonObj ?
S?W
V?W N??R
CanonIndirObj ?
S?W
V?W PP?R
P?R
a`?R
N??R
CanonByObj ?
S?W
V?W PP?R
P?R
par?R
N??R
RelatSubj ?
N?R
N?R S?W
N??R V?W WhObj ?
S?R
N??R S?W
V?W
WhByObj ?
S?R
PP?R
P?R
par?R
N??R
S?W
WhIndirObj ?
S?R
PP?R
P?R
a`?R
N??R
S?W
ActiveVerbForm?
S?B
V?B PassiveVerbForm?
S?B
V?B
V??B V?B
Figure 8
Elementary tree fragments used as building blocks of the grammar (nodes are colored to control
their identification when blocks are combined).
functions. Fourth, diathesis alternatives are gathered into tree families. In the next
paragraphs, we explain each of these levels in more detail.
Tree fragments. Tree fragments are the basic building blocks used to define SEMTAG.
These are the units that are shared and reused in the definition of many elementary
trees. For instance, the fragment for a canonical subject will be used by all FB-LTAG
elementary trees involving a canonical subject.
As mentioned earlier, to specify the verbal elementary trees of SEMTAG, we begin
by defining tree fragments which describe the possible syntactic realizations of the verb
arguments and of the verb itself. Figure 8 provides some illustrative examples of these
fragments. Here and in the following, we omit the feature structures decorating the trees
to facilitate reading.21
To further factorize information and facilitate grammar maintenance, the basic tree
fragments are organized in an inheritance hierarchy.22 Figure 9 shows a partial view of
21 See Crabbe? (2005) for a complete description of SEMTAG tree fragments, including feature structures.
22 Recall from Section 4 that inheritance is used to share namespaces. Thus, (node or feature) variables
introduced in a given class C can be directly reused in the sub-classes of C.
613
Computational Linguistics Volume 39, Number 3
VerbalArgument
CanonSubj CanonCompl
CanonObj CanPP
CanonIndirObj CanonByObj
Wh
WhObj WhPP
WhIndirObj WhByObj
RelatSubj
Figure 9
Organization of elementary fragments in an inheritance hierarchy.
this hierarchy illustrating how the tree fragments for argument realization depicted in
Figure 8 are organized to maximize the sharing of common information. The hierarchy
classifies the verbal arguments depicted in Figure 8 into four categories:
1. The canonical subject is a noun realized in front of the verb.
2. Canonical complements occur after the verb. The canonical object is a
noun phrase whereas prepositional complements are introduced by
specific prepositions, namely, a` for the canonical indirect object and
par for the canonical by object.
3. Wh-arguments (or questioned arguments) occur in front of a sentence
headed by a verb. A Wh-object is an extracted noun whereas questioned
prepositional objects are extracted prepositional phrases that are
introduced by a specific preposition.
4. Finally, the relativized subject is a relative pronoun realized in front
of the sentence. Extracted subjects in French cannot be realized at an
unbounded distance from the predicate.
Syntactic functions. The second level of abstraction uses syntactic function names such
as Subject and Object to group together alternative ways in which a given syntactic
function can be realized. For instance, if we make the simplifying assumption that the
possible argument realizations are limited to those given in Figure 8, the Subject, Object,
ByObject, and IndirectObject classes would be defined as follows.23
Subject ? CanonSubj ? RelatSubj (31)
Object ? CanonObj ? WhObj (32)
ByObject ? CanonByObj ? WhByObj (33)
IndirectObject ? CanonIndirObj ? WhIndirObj (34)
That is, we define the Subject class as an abstraction for talking about the set of tree
fragments that represent the possible realizations of a subject argument?namely, in
23 Note that, when these abstractions will be combined to describe for instance transitive verbs, the
combination of WhObj with WhByObj will be ruled out by using a uniqueness principle such as
introduced in Section 5.
614
Crabbe? et al XMG: eXtensible MetaGrammar
our restricted example, canonical and relativized subject. Thus, the simplified Subject
class defined in Equation (31) characterizes contexts such as the following:
(35) a. Jean mange. (canonical subject)
?John eats.?
b. Le garc?on qui mange (relativized subject)
?The boy who eats?
Similarly, the IndirectObject class abstracts over the realization of an argument intro-
duced by the preposition a` to the right of the verb (CanonIndirObj) or realized in
extracted position (possibly realized at an unbounded distance from the predicate) as
illustrated by the following examples:
(36) a. Jean parle a` Marie. (canonical indirect object)
?John talks to Mary.?
b. A` qui Jean parle-t-il ? (wh indirect object)
?To whom is John talking ??
c. A` qui Pierre croit-il que Jean parle ? (wh indirect object)
?To whom Peter thinks that John talks ??
This way of grouping tree fragments is reminiscent of the informal classification of
French syntactic functions presented by Iordanskaja and Mel?c?uk (2009) whereby each
syntactic function is associated with a set of possible syntactic constructions.
Diathesis alternations. In this third level, we take advantage of the abstractions defined
in the previous level to represent diathesis alternations. Again, we are interested here
in describing alternatives. Diathesis alternations are those alternations of mapping
between arguments and syntactic functions such as for instance the active/passive
alternation. In a diathesis alternation, the actual form of the verb constrains the way
predicate arguments are realized in syntax. Thus, in the following example, it is con-
sidered that both Examples (37a) and (37b) are alternative realizations of a predicate
argument structure such as send(John, a letter).
(37) a. Jean envoie une lettre.
?John sends a letter.?
b. Une lettre est envoye?e par Jean.
?A letter is sent by John.?
The active/passive diathesis alternation captures the fact that if the verb is in the
active form, its two arguments are realized by a subject and an object whereas if the
verb is in the passive form, then the arguments consist of a subject and a by-object.
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
(38)
Finally a traditional case of ?erasing,?24 such as the agentless passive (or passive
without agent) can be expressed in our language by adding an additional alternative
24 It is often argued that a language of grammatical representation must be equipped with an ?erasing
device? like lexical rules because of phenomena such as the passive without agent. In this framework it
turns out that this kind of device is not needed because we do not grant any special status to base trees.
615
Computational Linguistics Volume 39, Number 3
where the by-object or agentive complement is not expressed. Thus Equation (39) is an
augmentation of (38) where we have added the agentless passive alternative (indicated
in boldface).
TransitiveDiathesis ? (Subject ? ActiveVerbForm ? Object)
? (Subject ? PassiveVerbForm ? ByObject)
? (Subject ? PassiveVerbForm)
(39)
This methodology can be further augmented to implement an actual linking in the
manner of Bresnan and Zaenen (1990). For the so-called erasing cases, one can map the
?erased? predicative argument to an empty realization in syntax. We refer the reader to
Crabbe? (2005) for further details.
Tree families. Finally, tree families are defined?that is, sets of trees capturing alternative
realizations of a given verb type (i.e., sub-categorization frame). Continuing with the
simplified example presented so far, we can for instance define the tree family for
verbs taking a nominal subject, a nominal object, and an indirect nominal object (i.e.,
ditransitive verbs) as follows:
DitransitiveFamily ? TransitiveDiathesis ? IndirectObject (40)
The trees generated for such a family will, among others, handle the following
contexts:25
(41) a. Jean offre des fleurs a` Marie.
?John offers flowers to Mary.?
b. A` quelle fille Jean offre-t-il des fleurs ?
?To which girl does John offer flowers ??
c. Le garc?on qui offre des fleurs a` Marie.
?The boy who offers flowers to Mary.?
d. Quelles fleurs le garc?on offre-t-il a` Marie ?
?Which flowers does the boy offer to Mary ??
e. Les fleurs sont offertes par Jean a` Marie.
?The flowers are offered by John to Mary.?
f. Par quel garc?on les fleurs sont-elles offertes a` Marie ?
?By which boy are the flowers offered to Mary ??
It is straightforward to extend the grammar with new families. Thus, for instance,
Equation (42) shows how to define the transitive family (for verbs taking a nominal
subject and a nominal object) and Equation (43), the intransitive one (alternatives of a
verb sub-categorizing for a nominal subject).
TransitiveFamily ? TransitiveDiathesis (42)
IntransitiveFamily ? Subject ? ActiveVerbForm (43)
25 Note that number and gender agreements are dealt with using coreferences between features labeling
syntactic nodes, see Crabbe? (2005).
616
Crabbe? et al XMG: eXtensible MetaGrammar
Similarly, tree families for non-verbal predicates (adjectives, nouns) can be defined
using the abstraction over grammatical functions defined for verbs. For instance, the ex-
amples in (44a?44b) can be captured using the adjectival trees defined in Equations (46)
and (47), respectively, where Subject extends the definition of subject given above with a
Wh-subject, PredAdj combines a subject tree fragment with a tree fragment describing a
predicative adjective, and PredAdjAObj extends a PredAdj tree fragment with a canonical
a`-object.
(44) a. Jean est attentif. Qui est attentif ? L?homme qui est attentif
?John is mindful. Who is mindful ? The man who is mindful?
b. Jean est attentif a` Marie. Qui est attentif a` Marie ? L?homme qui est attentif a`
Marie
?John is mindful of Mary. Who is mindful of Mary ? The man who is mindful
of Mary?
Subject ? CanonSubj ? RelatSubj ? WhSubj (45)
PredAdj ? Subject ? AdjectivalForm (46)
PredAdjAObj ? PredAdj ? CanonAObj (47)
6.1.2 The Semantic Dimension and the Syntax/Semantic Interface. We now show how to
extend the XMG specification presented in the previous section to integrate a
unification-based compositional semantics. Three main changes need to be carried out:
1. Each elementary tree must be associated with a semantic formula. This is
done using the SEM dimension.
2. The nodes of elementary trees must be labeled with the appropriate
semantic indices. This involves introducing the correct attribute-value pair
in the correct feature structure (top or bottom) on the appropriate node.
3. Syntax and semantics need to be synchronized?that is, variable sharing
between semantic formulae and tree indices need to be enforced. To this
end we use the DYN dimension.
Informing the semantic dimension. To associate each elementary tree with a formula rep-
resenting the meaning of the words potentially anchoring that tree, we use the SEM
dimension to specify a semantic schema. For instance, the TransitiveFamily class defined
in Equation (42) for verbs taking two nominal arguments is extended as follows:
TransitiveFamily ? TransitiveDiathesis ? BinaryRel (48)
where TransitiveDiathesis is the XMG class defined in Equation (39) to describe the set of
trees associated with transitive verbs and BinaryRel the class describing the following
semantic schema:
L : P(E) ? L : Theta1(E, X) ? L : Theta2(E, Y) (49)
In this semantic schema, P, Theta1, and Theta2 are unification variables that become
ground when the tree is anchored with a specific word. For instance, P, Theta1, and
Theta2 are instantiated to eat, agent, and patient, respectively, when the anchor is ate (these
617
Computational Linguistics Volume 39, Number 3
pieces of information?predicate, thematic roles?are associated with lemmas, located
in the syntactic lexicon, and unified with adequate semantic variables via anchoring
equations). Further, X, Y, E, L are unification variables representing semantic arguments.
As illustrated in Figure 3, these become ground during (or after) derivation as a side
effect of the substitutions and adjunctions taking place when trees are combined. It
is worth noting that by combining semantic schemas with diathesis classes, one such
specification assigns the specified semantic schema to many trees, namely, all the trees
described by the corresponding diathesis class. In this way, the assignment of semantic
formulae to trees is relatively economical. Indeed in SEMTAG, roughly 6,000 trees are
assigned a semantic schema using a total of 75 schema calls.
Co-indexing trees and formulae indices. Assuming that tree nodes are appropriately deco-
rated with semantic indices by the specification scheme described in the next paragraph,
we now show how to enforce the correct mapping between syntactic and semantic
arguments. This is done in two steps.
First, we define a set of interface constraints of the form ?indexF : V, argi : V? which
are used to enforce the identification of the semantic index (indexF) labeling a given tree
node with grammatical function F (e.g., F := subject) with the index (argi) representing
the i-th argument in a semantic schema. For instance, the following constraints ensure
a subject/arg1 mapping, that is, a coreference between the index labeling a subject node
and the index representing the first argument of a semantic schema:
C1 ? Node [idx : I] ? ?indexsubject : I?
C2 ? L : P(E) ? L : Theta1(E, X) ? ?arg1 : X?
SubjectArg1 ? C1 ? C2 ? ?indexsubject : V, arg1 : V?
(50)
Given such interface constraints, we refine the diathesis definitions so as to ensure the
correct bindings. For instance, the specification in Equation (38) is modified to:
TransitiveDiathesis ? TransitiveActive ? TransitivePassive
TransitiveActive ? (SubjectArg1 ? ObjectArg2?
Subject ? ActiveVerbForm ? Object)
(51)
and the passive diathesis is specified as:
TransitivePassive ? (SubjectArg2 ? ByObjectArg1?
Subject ? PassiveVerbForm ? ByObject)
(52)
Labeling tree nodes with semantic indices. This scheme relies on the assumption that tree
nodes are appropriately labeled with semantic indices (e.g., the subject node must be
labeled with a semantic index) and that these indices are appropriately named (arg1
must denote the parameter representing the first argument of a binary relation and
indexsubject the value of the index feature on a subject node). As suggested by Gardent
(2007), a complete semantic labeling of a TAG with the semantic features necessary
618
Crabbe? et al XMG: eXtensible MetaGrammar
to enrich this TAG with the unification-based compositional semantics sketched in the
previous section can be obtained by applying the following labeling principles:26
Argument labeling: In trees associated with semantic functors, each argument node
is labeled with a semantic index27 named after the grammatical function of the
argument node (e.g., indexsubject for a subject node).
Controller/Controllee: In trees associated with control verbs, the semantic index of the
controller is identified with the value of the controlled index occurring on the
sentential argument node.
Anchor projection: The anchor node projects its index up to its maximal projection.
Foot projection: A foot node projects its index up to the root.28
As we shall now see, XMG permits a fairly direct encoding of these principles.
The Argument Labeling principle states that, in the tree associated with a syntactic
functor (e.g., a verb), each node representing a syntactic argument (e.g., the subject
node) should be labeled with a semantic index named after the grammatical function of
that node (e.g., indexsubject).29
To specify this labeling, we define for each grammatical function Function ?
{Subject, Object, ByObject, IndirectObject, . . . }, a semantic class FunctionSem which as-
sociates with an (exported) node variable called FunctionNode the feature value pair
[index : I] and a DYN constraint of the form ?indexFunction : I?. For instance, the class
SubjectSem associates the node SubjectNode with the feature value pair [index : I] and
the DYN constraint ?indexsubject : I?.
SubjectSem ? SubjectNode [index : I] ? ?indexsubject : I? (53)
Additionally, in the tree fragments describing the possible realizations of the grammat-
ical functions, the (exported) variable denoting the argument node is systematically
named ArgNode.
Finally, we modify the specification of the realizations of the grammatical functions
to import the appropriate semantic class and identify ArgNode and FunctionNode. For
instance, the Subject specification given above is changed to:
Subject ? SubjectSem ? ArgNode = SubjectNode ?
(CanonSubj ? RelatSubj ? WhSubj)
(54)
26 The principles required to handle quantification are omitted. We refer the reader to Gardent (2007) for a
more extensive presentation of how semantics is implemented using XMG.
27 For simplicity, we only mention indices. To be complete, however, labels should also be used.
28 The foot projection principle only applies to foot nodes that are not argument nodes (i.e., to modifiee
nodes).
29 In other words, this argument labeling principle defines an explicit and normalized reference to any
realization of a semantic argument. Following FB-LTAG predicate?argument co-occurrence principle
(Abeille?, Candito, and Kinyon 1999), we know that any elementary tree includes a leaf node for each
realized semantic argument of its anchor. This principle thus holds in any FB-LTAG. Its implementation,
however, is closely related to the architecture of the metagrammar; here we benefit from the fact that
verbal arguments are described in dedicated classes to reach a high degree of factorization.
619
Computational Linguistics Volume 39, Number 3
E3
E2
E2
E1
E2
E1
E1
E
E1
E
E1
E
? E? ? E? ? E?
Depth 3 Depth 2 Depth 1
SE2E1
VPE1E
?VE?
ActiveVerbForm
Figure 10
Anchor/Foot projection.
As a result, all ArgNode nodes in the tree descriptions associated with a subject realiza-
tion are labeled with an index feature I whose global name is indexsubject.
Value sharing between the semantic index of the controller (e.g., the subject of
the control verb) and that of the controllee (e.g., the empty subject of the infinitival
complement) is enforced using linking constraints between the semantic index labeling
the controller node and that labeling the sentential argument node of the control verb.
Control verb definitions then import the appropriate (object or subject control) linking
constraint.
The anchor (respectively, foot) projection principle stipulates the projection of
semantic indices from the anchor (respectively, foot) node up to the maximal projection
(respectively, root). Concretely, this means that the top and bottom features of the nodes
located on this path between the anchor (respectively, foot) and the maximal projection
(respectively, root) all include an index feature whose value is shared between adjacent
nodes (see variables Ei in Figure 10).30 Once the top and bottom structures are unified,
so are the semantic indices along this path (modulo expected adjunctions realized on
the projection).
To implement these principles, we define a set of anchor projection classes
{Depth1, Depth2, Depth3} as illustrated in Figure 10. We then ?glue? these projection
skeletons onto the relevant syntactic trees by importing the skeletons in the syntactic
tree description and explicitly identifying the anchor node of the semantic projection
classes with the anchor or foot node of these syntactic tree descriptions. Because the
models must be trees, the nodes dominating the anchor node of the projection class
will deterministically be identified with those dominating the anchor or foot node of
the trees being combined with. For instance, for verbs, the class specifying the verbal
spine (e.g., ActiveVerbForm, see Figure 10) equates the anchor node of the verbal spine
with that of the projection skeleton. As a result, the verb projects its index up to
the root.
6.1.3 Some Figures About SEMTAG. As mentioned previously, SEMTAG is a large FB-LTAG
for French equipped with semantics (Gardent 2008); it extends the purely syntactic
FTAG of Crabbe? (2005) with a unification based compositional semantics as described
by Gardent and Kallmeyer (2003).31 The syntactic FTAG in essence implements Abeille??s
(2002) proposal for an FB-LTAG-based modeling of French syntax. FTAG contains
around 6,000 elementary trees built from 293 XMG classes and covers some 40 basic
30 For sake of brevity, we write E2E1 for [bot : [index : E1] top : [index : E2]]. ? ? refers to the anchor / foot.31 FTAG and SEMTAG are freely available under the terms of the GPL-compliant CeCILL license, the former
at https://sourcesup.renater.fr/scm/viewvc.php/trunk/METAGRAMMARS/FrenchTAG/?root=xmg, and
the latter on request.
620
Crabbe? et al XMG: eXtensible MetaGrammar
verbal sub-categorization frames. For each of these frames, FTAG defines a set of
argument alternations (active, passive, middle, neuter, reflexivization, impersonal,
passive impersonal) and of argument realizations (cliticization, extraction, omission,
permutations, etc.) possible for this frame. Predicative (adjectival, nominal, and
prepositional) and light verb constructions are also covered as well as some common
sub-categorizing noun and adjective constructions. Basic descriptions are provided for
the remaining constructions namely, adverbs, determiners, and prepositions.
FTAG and SEMTAG were both evaluated on the Test Suite for Natural Language Pro-
cessing (TSNLP) (Lehmann et al 1996), using a lexicon designed specifically on the test
suite, hence reducing lexical ambiguity (Crabbe? 2005; Parmentier 2007). This test suite
focuses on difficult syntactical phenomena, providing grammatical and ungrammatical
sentences. These competence grammars accept 76% of the grammatical items, reject 83%
of the ungrammatical items, and have an average ambiguity of 1.64 parses per sentence.
To give an idea of the compilation time, under architectures made of a 2-Ghz processor
with 1 Gb of RAM, it takes XMG 10 minutes to compile the whole SEMTAG (recall that
there is no semantic description solving, hence the compilation times between FTAG
and SEMTAG do not differ).32
Note that SEMTAG can be used for assigning semantic representations to sentences
when combined with an FB-LTAG parser and a semantic construction module as de-
scribed by Gardent and Parmentier (2005, 2007).33 Conversely, it can be used to verbalize
the meaning denoted by a given semantic representation when coupled with the GenI
surface realizer described by Gardent and Kow (2007).
6.2 Other Grammars Designed with XMG
XMG has been used mainly to design FB-LTAG and IG for French or English. More
recently, it has also been used to design a FB-LTAG for Vietnamese and a TreeTuple
MC-TAG for German. We now briefly describe each of these resources.
SemXTAG. The English grammar, SEMXTAG (Alahverdzhieva 2008), reimplements the
FB-LTAG developed for English at the University of Pennsylvania (XTAG Research
Group 2001) and extends it with a unification-based semantics. It contains 1,017 trees
and covers the syntactic fragment of XTAG, namely, auxiliaries, copula, raising and
small clause constructions, topicalization, relative clauses, infinitives, gerunds, pas-
sives, adjuncts, ditransitives (and datives), ergatives, it-clefts, wh-clefts, PRO con-
structions, noun?noun modification, extraposition, determiner sequences, genitives,
negation, noun?verb contractions, sentential adjuncts, imperatives, and resultatives.
The grammar was tested on a handbuilt test-suite of 998 sentences illustrating the
various syntactic constructions meant to be covered by the grammar. All sentences in
the test suite can be parsed using the grammar.
FrenchIG. The extended XMG framework was used to design a core IG for French
consisting of 2,059 tree descriptions compiled out of 448 classes (Perrier 2007). The
resulting grammar is lexicalized, and its coverage was evaluated using the previously
mentioned TSNLP. The French IG accepts 88% of the grammatical sentences and rejects
32 As a comparison, about one hour was needed by Candito?s (1999) compiler to produce a French FB-LTAG
containing about 1,000 tree schemas.
33 As an alternative way to parse FB-LTAG grammars equipped with flat semantics such as those produced
by XMG, one can use the Tu?bingen Linguistic Parsing Architecture (TuLiPA) (Kallmeyer et al 2010).
621
Computational Linguistics Volume 39, Number 3
85% of the ungrammatical sentences, although the current version of the French IG
does not yet cover all the syntactic phenomena presented in the test suite (for example,
causative and superlative constructions).
Vietnamese TAG. The XMG language was used by Le Hong, N?Guyen, and Roussanaly
(2008) to produce a core FB-LTAG for Vietnamese. Their work is rather a proof of con-
cept than a large-scale implementation. They focused on Vietnamese?s categorization
frames, and were able to produce a TAG covering the following frames: intransitive
(tree family N0V), transitive with a nominal complement (N0VN1), transitive with a
clausal complement (N0VS1), transitive with modal complement (N0V0V1), ditransi-
tive (N0VN1N2), ditransitive with a preposition (N0VN1ON2), ditransitive with a ver-
bal complement (N0V0N1V1), ditransitive with an adjectival complement (N0VN1A),
movement verbs with a nominal complement (N0V0V1N1), movement verbs with an
adjectival complement (N0V0AV1), and movement ditransitive (N0V0N1V1N2).
GerTT. Another XMG-based grammar corresponds to the German MC-TAG of
Kallmeyer et al (2008). This grammar, called GerTT, is in fact an MC-TAG with
Tree Tuples (Lichte 2007). This variant of MCTAG has been designed to model free
word order phenomena. This is done by imposing node sharing constraints on MCTAG
derivations (Kallmeyer 2005). GerTT covers phenomena such as scrambling, coherent
constructions, relative clauses, embedded questions, copula verbs, complementized
sentences, verbs with various sub-categorization frames, nouns, prepositions, determin-
ers, adjectives, and partly includes semantics. It is made of 103 tree tuples, compiled
from 109 classes.
7. Related Work
We now compare XMG with existing environments for designing tree-based grammars
and briefly report on the grammars designed with these systems.
7.1 Environments for Designing Tree-Based Grammars
Candito?s Metagrammar Compiler. The concept of metagrammar was introduced by
Candito (1996). In her paper, Candito presented a compiler for abstract specifications
of FB-LTAG trees (the so-called metagrammars). Such specifications are based on three
dimensions, each of them being encoded in a separate inheritance hierarchy of linguistic
descriptions. Dimension 1 describes canonical sub-categorization frames (e.g., transitive),
the Dimension 2 describes redistributions of syntactic functions (e.g., active to passive),
and Dimension 3 the tree descriptions corresponding to the realizations of the syntactic
functions defined in Dimension 2. This three-dimensional metagrammatical description
is then processed by a compiler to compute FB-LTAG tree schemas. In essence, these
tree schemas are produced by associating a canonical sub-categorization frame (Dimen-
sion 1) with a compatible redistribution schema (Dimension 2), and with exactly one
function realization (Dimension 3) for each function required by the sub-categorization
frame.
Candito?s (1996, 1999) approach improves on previous proposals by Vijay-Shanker
and Schabes (1992) and Evans, Gazdar, and Weir (1995) in that it provides a linguistically
principled basis for structuring the inheritance hierarchy. As shown in Section 6.1,
622
Crabbe? et al XMG: eXtensible MetaGrammar
the XMG definition of SEMTAG uses similar principles. Candito?s approach differs,
however, from the XMG account in several important ways:
 Much of the linguistic knowledge used to determine which classes to
combine is hard-coded in the compiler (unlike in XMG, there is no explicit
control on class combinations). In other words, there is no clear separation
between the linguistic knowledge needed to specify a high-level FB-LTAG
description and the algorithm used to compile an actual FB-LTAG from
this description. This makes grammar extension and maintenance by
linguists extremely difficult.
 As in Vijay-Shanker and Schabes (1992) Evans, Gazdar, and Weir (1995),
the linguistic description is non-monotonic in that some erasing classes
are used to remove information introduced by other dimensions
(e.g., agentless passive).
 The approach fails to provide an easy means to state exceptions. These
are usually encoded in the compiling algorithm.
 The tree description language used to specify classes in Dimension 3
relies on global node variables. Thus, two variables with identical names
introduced in different classes are expected to refer to the same tree node.
As argued in Section 4, this makes it hard to design large-scale
metagrammars.
The LexOrg system. An approach similar to Candito?s was presented by Xia et al
(1998), Xia (2001), and Xia, Palmer, and Vijay-Shanker (2005, 2010). As in Candito?s
approach, a TAG abstract specification relies on a three-dimensional description made
of, namely, sub-categorization frames, blocks, and lexical redistribution rules. To com-
pile this specification into a TAG, the system selects a canonical sub-categorization
frame, and applies some lexical redistribution rules to derive new frames and finally
select blocks corresponding to the resulting frames. These blocks contain tree descrip-
tions using the logic of Rogers and Vijay-Shanker (1994).
LexOrg suffers from similar limitations as Candito?s compiler. Much of the lin-
guistic knowledge is embedded in the compiling algorithm, making it difficult for
linguists to extend the grammar description and to handle exceptions. Unlike in Can-
dito?s framework, the tree description language uses local node variables and lets the
tree description solver determine node identifications. Although this avoids having to
memorize node names, this requires that the descriptions be constrained enough to
impose the required node identifications and prevent the unwanted ones. In practice,
this again complicates grammar writing. In contrast, XMG provides an intermediate
solution which, by combining local variables with export declarations, avoids having to
memorize too many node variable names (only those local to the relevant sub-hierarchy
need memorizing) while allowing for explicit node identification.
The Metagrammar Compiler of Gaiffe, Crabbe?, and Roussanaly. Gaiffe, Crabbe?, and
Roussanaly (2002) proposed a compiler for FB-LTAG that aims to remedy both the lack
of a clear separation between linguistic information and compilation algorithm, and
the lack of explicit control on the class combinations prevalent in Candito (1996), Xia
et al (1998), and Xia (2001). In their approach, the linguistic specification consists of
a single inheritance hierarchy of classes, each class containing a tree description. The
623
Computational Linguistics Volume 39, Number 3
description logic used is similar to Candito?s. That is, global node names are used. To
trigger class combinations, classes are labeled with two types of information: needs and
resources. The compiler selects all final classes of the hierarchy, performs all possible
combinations, and only keeps those combinations that neutralize the stated needs
and resources. The tree descriptions contained in these neutral combinations are then
solved to produce the expected trees.
Although this approach implements a clear separation between linguistic informa-
tion and compilation algorithm, the fully automatic derivation of FB-LTAG trees from
the inheritance hierarchy makes it difficult in practice to control overgeneration. In
contrast, XMG?s explicit definitions of class combinations by conjunction, disjunction,
and inheritance makes it easier to control the tree set that will be generated by the
compiler from the grammar specification. Additionally, the issues raised by global
variables remain (no way to instantiate twice a given class, and cumbersome definition
of variables in large metagrammars).
The MGCOMP System. More recently, Villemonte de la Clergerie (2005, 2010) proposed a
compiler for FB-LTAG that aims at preserving a high degree of factorization in both the
abstract grammar specification and the grammar which is compiled from it. Thus, the
MGCOMP system does not compute FB-LTAG elementary trees, but factorized trees.
In MGCOMP, like in Gaiffe, Crabbe?, and Roussanaly?s (2002) approach, a meta-
grammar consists of a single hierarchy of classes. The classes are labeled with needs and
resources, and final classes of the hierarchy are combined to compute tree descriptions.
The main differences with Gaiffe, Crabbe?, and Roussanaly (2002), lies in the fact that
(i) a description can include new factorizing operators, such as repetition (Kleene-star
operator), shuffling (interleaving of nodes), optionality, and disjunctions; and (ii) it offers
namespaces to specify the scope of variables. MGCOMP?s extended tree descriptions
are not completely solved by the compiler. Rather, it compiles underspecified trees (also
called factorized trees). With this approach, a large grammar is much smaller in terms of
number of grammatical structures than a classical FB-LTAG. As a result, the grammars it
compiles are only compatible with the DyALog parsing environment (Villemonte de La
Clergerie 2005). And, because the linguist designs factorized trees and not actual TAG
trees, debugging the metagrammar becomes harder.
7.2 Resources Built Using Candito, Xia, and De La Clergerie?s Systems
Candito?s system has been used by Candito (1999) herself to design a core FB-LTAG
for French and Italian, and later by Barrier (2006) to design a FB-LTAG for adjectives
in French. Xia?s system (LexOrg) has been used to semi-automatically generate XTAG
(Xia 2001). De La Clergerie?s system (MGCOMP) has been used to design a grammar
for French named FRMG (FRench MetaGrammar) (Villemonte de la Clergerie 2010).
FRMG makes use of MGCOMP?s factorizing operators (e.g., shuffling operator), thus
producing not sensu stricto a FB-LTAG, but a factorized FB-LTAG. FRMG is freely
available, contains 207 factorized trees (having optional branches, etc.) built from 279
metagrammatical classes, and covers 95% of the TSNLP.
8. Conclusion
In this article, we presented the eXtensible MetaGrammar framework and argued that,
contrary to other existing grammar writing environments for tree-based grammar,
624
Crabbe? et al XMG: eXtensible MetaGrammar
XMG is declarative, extensible, and notationally expressive. We believe that these fea-
tures make XMG particularly appropriate for a fast prototyping of the kind of deep
tree-based grammars that are used in applications requiring high precision in gram-
mar modeling (e.g., language teaching, man/machine dialogue systems, data-to-text
generation).
The XMG language is documented on-line, and its compiler is open source soft-
ware, freely available under the terms of the GPL-compliant CeCILL license.34 Many
grammars designed with XMG (FB-LTAG and IG for French and English, TT-MCTAG
for German) are also open-source and available on-line.35
Future research will focus on extensibility. So far, XMG has been used to design tree-
based grammars for different languages. We plan to extend XMG to handle other types
of formalisms36 such as dependency grammars, and to support dimensions other than
syntax and semantics such as for instance, phonology or morphology. As mentioned
here, XMG offers a modular architecture, making it possible to extend it relatively easily.
Nonetheless, in its current state, such extensions imply modifying XMG?s code. We are
exploring new extensions of the formalism, which would allow the linguist to dynam-
ically define her/his metagrammar formalism (e.g., which principles or descriptions to
use) depending on the target formalism.
Another interesting question concerns cross-language grammar engineering. So far,
the metagrammar allows for dealing with structural redundancy. As pointed out by
Kinyon et al (2006), a metagrammar can be used to capture generalizations across
languages and is surely worth further investigating.
Finally, we plan to extend XMG with features borrowed from Integrated De-
velopment Environments (IDE) for programming languages. Designing a grammar
is, in some respect, similar to programming an application. Grammar environments
should benefit from the same tools as those used for the development of applications
(incremental compilation, debugger, etc.).
Acknowledgments
We are grateful to the three anonymous
reviewers for their valuable comments.
Any remaining errors are ours.
References
Abeille?, A. 2002. Une grammaire e?lectronique
du franc?ais. CNRS Editions.
Abeille?, A., M. Candito, and A. Kinyon. 1999.
Ftag: current status and parsing scheme.
In Proceedings of Vextal ?99, pages 283?292,
Venice.
A??t-Kaci, Hassan. 1991. Warren?s Abstract
Machine: A Tutorial Reconstruction. MIT
Press, Cambridge, MA.
Alahverdzhieva, Katya. 2008. XTAG using
XMG. Masters thesis, Nancy Universite?.
Baldridge, Jason, Sudipta Chatterjee,
Alexis Palmer, and Ben Wing. 2007.
DotCCG and VisCCG: Wiki and
programming paradigms for improved
grammar engineering with OpenCCG.
In Tracy Holloway King and Emily M.
Bender, editors, Proceedings of the Grammar
Engineering Across Framework Workshop
(GEAF 07). CSLI, Stanford, CA, pages 5?25.
Barrier, Se?bastien. 2006. Une me?tagrammaire
pour les noms pre?dicatifs du franc?ais :
de?veloppement et expe?rimentations pour les
grammaires TAG. Ph.D. thesis, Universite?
Paris 7.
Becker, Tilman. 1993. HyTAG: A New Type
of Tree Adjoining Grammars for Hybrid
Syntactic Representation of Free Word Order
Language. Ph.D. thesis, Universita?t des
Saarlandes.
34 See https://sourcesup.renater.fr/xmg.
35 The French TAG and French and English IG are available on XMG?s website, and the German TreeTuple
MC-TAG is available at http://www.sfs.uni-tuebingen.de/emmy/res.html.
36 Preliminary work on cross-framework grammar engineering has been realized by Cle?ment and Kinyon
(2003), who used Gaiffe et al?s compiler to produce both a TAG and a LFG from a given metagrammar.
625
Computational Linguistics Volume 39, Number 3
Blackburn, Patrick, Johan Bos, and Kristina
Striegnitz. 2006. Learn Prolog Now!,
volume 7 of Texts in Computing. College
Publications, London.
Bresnan, Joan and Annie Zaenen. 1990. Deep
unaccusitivity in LFG. In K. Dziwirek,
P. Farell, and E. Mejias-Bikandi, editors,
Grammatical Relations: A Cross-Theoretical
Perspective. CSLI publications, Stanford,
CA, pages 45?57.
Candito, Marie. 1996. A principle-based
hierarchical representation of LTAGs.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 194?199, Copenhagen.
Candito, Marie. 1999. Repre?sentation modulaire
et parame?trable de grammaires e?lectroniques
lexicalise?es : application au franc?ais et a`
l?italien. Ph.D. thesis, Universite? Paris 7.
Cle?ment, Lionel and Alexandra Kinyon.
2003. Generating parallel multilingual
lfg-tag grammars from a metagrammar.
In Proceedings of the 41st Annual Meeting of
the Association for Computational Linguistics,
pages 184?191, Sapporo.
Cohen-Sygal, Yael and Shuly Wintner. 2007.
The Non-Associativity of Polarized
Tree-Based Grammars. In Proceedings of the
Eighth International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLing-2007), pages 208?217, Mexico City.
Cohen-Sygal, Yael and Shuly Wintner. 2009.
Associative grammar combination operators
for tree-based grammars. Journal of Logic,
Language and Information, 18(3):293?316.
Cohen-Sygal, Yael and Shuly Wintner. 2011.
Towards modular development of typed
unification grammars. Computational
Linguistics, 37(1):29?74.
Copestake, Ann and Dan Flickinger. 2000.
An open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings of the
Second Conference on Language Resources and
Evaluation (LREC-2000), Athens.
Copestake, Ann, Alex Lascarides, and Dan
Flickinger. 2001. An algebra for semantic
construction in constraint-based
grammars. In Proceedings of 39th Annual
Meeting of the Association for Computational
Linguistics, pages 140?147, Toulouse.
Crabbe?, Benoit. 2005. Repre?sentation
informatique de grammaires fortement
lexicalise?es : Application a` la grammaire
d?arbres adjoints. Ph.D. thesis, Universite?
Nancy 2.
Crabbe?, Beno??t and Denys Duchier. 2004.
Metagrammar redux. In Proceedings of the
Workshop on Constraint Solving for Language
Processing (CSLP 2004), pages 32?47,
Copenhagen.
Duchier, Denys, Brunelle Magnana Ekoukou,
Yannick Parmentier, Simon Petitjean, and
Emmanuel Schang. 2012. Describing
morphologically-rich languages using
metagrammars: A look at verbs in Ikota.
In Workshop on ?Language Technology for
Normalisation of Less-resourced Languages,?
8th SALTMIL Workshop on Minority
Languages and 4th Workshop on African
Language Technology, International
Conference on Language Resources and
Evaluation, LREC 2012, pages 55?60,
Istanbul.
Duchier, Denys and Joachim Niehren. 2000.
Dominance constraints with set operators.
In John W. Lloyd, Vero?nica Dahl, Ulrich
Furbach, Manfred Kerber, Kung-Kiu Lau,
Catuscia Palamidessi, Lu??s Moniz Pereira,
Yehoshua Sagiv, and Peter J. Stuckey,
editors, Proceedings of the First International
Conference on Computational Logic,
volume 1861 of Lecture Notes in Computer
Science. Springer, Berlin, pages 326?341.
Duchier, Denys, Yannick Parmentier, and
Simon Petitjean. 2012. Metagrammars as
logic programs. In International Conference
on Logical Aspects of Computational
Linguistics (LACL 2012). Proceedings of
the Demo Session, pages 1?4, Nantes.
Duchier, Denys and Stefan Thater. 1999.
Parsing with tree descriptions: A
constraint-based approach. In Proceedings
of the Sixth International Workshop on
Natural Language Understanding and Logic
Programming (NLULP?99), pages 17?32,
Las Cruces, NM.
Evans, Roger, Gerald Gazdar, and David
Weir. 1995. Encoding lexicalized tree
adjoining grammars with a nonmonotonic
inheritance hierarchy. In Proceedings of the
33rd Annual Meeting of the Association for
Computational Linguistics, pages 77?84,
Cambridge, MA.
Flickinger, Daniel. 1987. Lexical Rules in the
Hierarchical Lexicon. Ph.D. thesis, Stanford
University.
Gaiffe, Bertrand, Beno??t Crabbe?, and Azim
Roussanaly. 2002. A new metagrammar
compiler. In Proceedings of the Sixth
International Workshop on Tree Adjoining
Grammars and Related Frameworks (TAG+6),
pages 101?108, Venice.
Gardent, Claire. 2007. Tree adjoining
grammar, semantic calculi and labelling
invariants. In Proceedings of the International
Workshop on Computational Semantics
(IWCS), Tilburg.
626
Crabbe? et al XMG: eXtensible MetaGrammar
Gardent, Claire. 2008. Integrating a
unification-based semantics in a large
scale lexicalised tree adjoininig grammar
for French. In Proceedings of the 22nd
International Conference on Computational
Linguistics (COLING?08), pages 249?256,
Manchester.
Gardent, Claire and Laura Kallmeyer. 2003.
Semantic construction in feature-based
tree adjoining grammar. In Proceedings of
the 10th Conference of the European Chapter of
the Association for Computational Linguistics,
pages 123?130, Budapest.
Gardent, Claire and Eric Kow. 2007. A
symbolic approach to near-deterministic
surface realisation using tree adjoining
grammar. In 45th Annual Meeting of the
Association for Computational Linguistics,
pages 328?335, Prague.
Gardent, Claire and Yannick Parmentier.
2005. Large scale semantic construction for
tree adjoining grammars. In Proceedings
of the Fifth International Conference
on Logical Aspects of Computational
Linguistics (LACL?05), pages 131?146,
Bordeaux.
Gardent, Claire and Yannick Parmentier.
2006. Coreference Handling in XMG.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006) Main Conference Poster Sessions,
pages 247?254, Sydney.
Gardent, Claire and Yannick Parmentier.
2007. SemTAG: A platform for specifying
tree adjoining grammars and performing
TAG-based semantic construction. In
Proceedings of the 45th Annual Meeting
of the Association for Computational
Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions,
pages 13?16, Prague.
Iordanskaja, Lidija and Igor Mel?c?uk, 2009.
Establishing an inventory of surface?
syntactic relations: valence-controlled
surface-dependents of the verb in French.
In A. Polgue`re and I. A. Mel?duk, editors,
Dependency in Linguistic Description.
John Benjamins, Amsterdam,
pages 151?234.
Joshi, Aravind K., Leon S. Levy, and Masako
Takahashi. 1975. Tree adjunct grammars.
Journal of Computer and System Sciences,
10(1):136?163.
Kallmeyer, Laura. 1999. Tree Description
Grammars and Underspecified
Representations. Ph.D. thesis,
Universita?t Tu?bingen.
Kallmeyer, Laura. 2005. Tree-local
multicomponent tree-adjoining grammars
with shared nodes. Computational
Linguistics, 31(2):187?226.
Kallmeyer, Laura, Timm Lichte, Wolfgang
Maier, Yannick Parmentier, and Johannes
Dellert. 2008. Developing a TT-MCTAG for
German with an RCG-based parser. In
Proceedings of the Sixth Language Resources
and Evaluation Conference (LREC),
pages 782?789, Marrakech.
Kallmeyer, Laura, Wolfgang Maier, Yannick
Parmentier, and Johannes Dellert. 2010.
TuLiPA?Parsing extensions of TAG with
range concatenation grammars. Bulletin of
the Polish Academy of Sciences: Technical
Sciences, 58(3):377?392.
Kallmeyer, Laura and Maribel Romero.
2004a. LTAG semantics for questions.
In Proceedings of 7th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+7), pages 186?193,
Vancouver.
Kallmeyer, Laura and Maribel Romero.
2004b. LTAG semantics with semantic
unification. In Proceedings of 7th
International Workshop on Tree-Adjoining
Grammar and Related Formalisms (TAG+7),
page 155?162, Vancouver.
Kallmeyer, Laura and Maribel Romero.
2008. Scope and situation binding in
LTAG using semantic unification.
Research on Language and Computation,
6(1):3?52.
Kaplan, Ronald and Paula Newman. 1997.
Lexical resource reconciliation in the Xerox
linguistic environment. In Proceedings
of the ACL Workshop on Computational
Environments for Grammar Development
and Linguistic Engineering, pages 54?61,
Madrid.
Kinyon, Alexandra. 2000. Hypertags.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING?00), pages 446?452, Saarbru?cken.
Kinyon, Alexandra, Owen Rambow, Tatjana
Scheffler, SinWon Yoon, and Aravind K.
Joshi. 2006. The metagrammar goes
multilingual: A cross-linguistic look at
the v2-phenomenon. In Proceedings of
the Eighth International Workshop on Tree
Adjoining Grammar and Related Formalisms,
pages 17?24, Sydney.
Le Hong, Phuong, Thi-Min-Huyen
N?Guyen, and Azim Roussanaly. 2008.
A metagrammar for Vietnamese. In
Proceedings of the 9th International Workshop
on Tree-Adjoining Grammar and Related
Formalisms (TAG+9), Tu?bingen.
627
Computational Linguistics Volume 39, Number 3
Lehmann, Sabine, Stephan Oepen, Sylvie
Regnier-Prost, Klaus Netter, Veronika Lux,
Judith Klein, Kirsten Falkedal, Frederik
Fouvry, Dominique Estival, Eva Dauphin,
Herve? Compagnion, Judith Baur, Lorna
Balkan, and Doug Arnold. 1996. TSNLP?
Test suites for natural language processing.
In Proceedings of the 16th International
Conference on Computational Linguistics
(COLING?96), pages 711?716, Copenhagen.
Lichte, Timm. 2007. An MCTAG with tuples
for coherent constructions in German.
In Proceedings of the 12th Conference on
Formal Grammar (FG 2007), 12 pages,
Dublin.
Muskens, Reinhard and Emiel Krahmer.
1998. Description theory, LTAGs and
Underspecified Semantics. In Fourth
International Workshop on Tree Adjoining
Grammars and Related Frameworks,
pages 112?115, Philadelphia, PA.
Parmentier, Yannick. 2007. SemTAG: une
plate-forme pour le calcul se?mantique a` partir
de Grammaires d?Arbres Adjoints. Ph.D.
thesis, Universite? Henri Poincare? - Nancy.
Parmentier, Yannick, Laura Kallmeyer, Timm
Lichte, and Wolfgang Maier. 2007. XMG:
eXtending MetaGrammars to MCTAG. In
Proceedings of the Workshop on High-Level
Syntactic Formalisms, 14th Conference on
Natural Language Processing (TALN?2007),
pages 473?482, Toulouse.
Pereira, Fernando and David Warren. 1980.
Definite clause grammars for language
analysis?A survey of the formalism
and a comparison to augmented
transition networks. Artificial
Intelligence, 13:231?278.
Perlmutter, David. 1970. Surface structure
constraints in syntax. Linguistic Inquiry,
1:187?255.
Perrier, Guy. 2000. Interaction grammars.
In Proceedings of the 18th International
Conference on Computational Linguistics
(COLING 2000), pages 600?606,
Saarbru?cken.
Perrier, Guy. 2007. A French interaction
grammar. In Proceedings of the 6th
Conference on Recent Advances in Natural
Language Processing (RANLP 2007),
pages 463?467, Borovets.
Prolo, Carlos A. 2002. Generating the
XTAG English grammar using metarules.
In Proceedings of the 19th International
Conference on Computational Linguistics
(COLING?2002), pages 814?820, Taipei.
Rambow, Owen, K. Vijay-Shanker, and
David Weir. 1995. D-tree grammars.
In Proceedings of the 33th Meeting of the
Association for Computational Linguistics,
pages 151?158, Cambridge, MA.
Rogers, James and K. Vijay-Shanker. 1994.
Obtaining trees from their descriptions:
An application to tree-adjoining
grammars. Computational Intelligence,
10:401?421.
Shieber, Stuart M. 1984. The design of a
computer language for linguistic
information. In Proceedings of the Tenth
International Conference on Computational
Linguistics, pages 362?366, Stanford, CA.
Van Roy, Peter. 1990. Extended DCG
notation: A tool for applicative
programming in prolog. Technical
Report UCB/CSD 90/583, University
of California, Berkeley.
Vijay-Shanker, K. and Aravind K. Joshi.
1988. Feature structures based tree
adjoining grammars. In Proceedings
of the 12th Conference on Computational
Linguistics (COLING?88), pages 714?719,
Budapest.
Vijay-Shanker, K. and Yves Schabes. 1992.
Structure sharing in lexicalized tree
adjoining grammars. In Proceedings
of the 14th International Conference on
Computational Linguistics (COLING?92),
pages 205?212, Nantes.
Villemonte de La Clergerie, E?ric. 2005.
DyALog: a tabular logic programming
based environment for NLP. In Proceedings
of 2nd International Workshop on Constraint
Solving and Language Processing (CSLP?05),
pages 18?33, Barcelona.
Villemonte de la Clergerie, E?ric. 2010.
Building factorized TAGs with
meta-grammars. In Proceedings of
the 10th International Workshop on
Tree-Adjoining Grammar and Related
Formalisms (TAG+10), pages 111?118,
New Haven, CT.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Xia, Fei. 2001. Automatic Grammar Generation
from Two Different Perspectives. Ph.D. thesis,
University of Pennsylvania.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 1999. Toward semi-automating
grammar development. In Proceedings of
the 5th Natural Language Processing Pacific
Rim Symposium (NLPRS-99), pages 96?101,
Beijing.
Xia, Fei, Martha Palmer, and K. Vijay-
Shanker. 2005. Automatically generating
tree adjoining grammars from abstract
specifications. Journal of Computational
Intelligence, 21(3):246?287.
628
Crabbe? et al XMG: eXtensible MetaGrammar
Xia, Fei, Martha Palmer, and
K. Vijay-Shanker. 2010. Developing
tree-adjoining grammars with lexical
descriptions. In Srinivas Bangalore and
Aravind Joshi, editors, Supertagging:
Using Complex Lexical Descriptions in
Natural Language Processing. MIT Press,
Cambridge, MA, pages 73?110.
Xia, Fei, Martha Palmer, K. Vijay-Shanker,
and Joseph Rosenzweig. 1998. Consistent
grammar development using partial-tree
descriptions for LTAGs. In Proceedings
of the 4th International Workshop on
Tree Adjoining Grammar and Related
Formalisms (TAG+ 1998), pages 180?183,
Philadelphia, PA.
XTAG Research Group. 2001. A lexicalized
tree adjoining grammar for English.
Technical Report IRCS-01-03, IRCS,
University of Pennsylvania.
629

Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 1?9,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Testing the Robustness of Online Word Segmentation:
Effects of Linguistic Diversity and Phonetic Variation
Luc Boruta1,2, Sharon Peperkamp2, Beno??t Crabbe?1, and Emmanuel Dupoux2
1 Univ. Paris Diderot, Sorbonne Paris Cite?, ALPAGE, UMR-I 001 INRIA, F-75205, Paris, France
2 LSCP?DEC, E?cole des Hautes E?tudes en Sciences Sociales, E?cole Normale Supe?rieure,
Centre National de la Recherche Scientifique, F-75005, Paris, France
luc.boruta@inria.fr, peperkamp@ens.fr, benoit.crabbe@inria.fr, emmanuel.dupoux@gmail.com
Abstract
Models of the acquisition of word segmen-
tation are typically evaluated using phonem-
ically transcribed corpora. Accordingly, they
implicitly assume that children know how to
undo phonetic variation when they learn to ex-
tract words from speech. Moreover, whereas
models of language acquisition should per-
form similarly across languages, evaluation
is often limited to English samples. Us-
ing child-directed corpora of English, French
and Japanese, we evaluate the performance
of state-of-the-art statistical models given in-
puts where phonetic variation has not been re-
duced. To do so, we measure segmentation
robustness across different levels of segmen-
tal variation, simulating systematic allophonic
variation or errors in phoneme recognition.
We show that these models do not resist an in-
crease in such variations and do not generalize
to typologically different languages. From the
perspective of early language acquisition, the
results strengthen the hypothesis according to
which phonological knowledge is acquired in
large part before the construction of a lexicon.
1 Introduction
Speech contains very few explicit boundaries be-
tween linguistic units: silent pauses often mark ut-
terance boundaries, but boundaries between smaller
units (e.g. words) are absent most of the time. Pro-
cedures by which infants could develop word seg-
mentation strategies have been discussed at length,
from both a psycholinguistic and a computational
point of view. Many models relying on statistical
information have been proposed, and some of them
exhibit satisfactory performance: MBDP-1 (Brent,
1999), NGS-u (Venkataraman, 2001) and DP (Gold-
water, Griffiths and Johnson, 2009) can be consid-
ered state-of-the-art. Though there is evidence that
prosodic, phonotactic and coarticulation cues may
count more than statistics (Johnson and Jusczyk,
2001), it is still a matter of interest to know how
much can be learned without linguistic cues. To use
Venkataraman?s words, we are interested in ?the per-
formance of bare-bones statistical models.?
The aforementioned computational simulations
have two major downsides. First, all models of
language acquisition should generalize to typolog-
ically different languages; however, the word seg-
mentation experiments mentioned above have never
been carried out on phonemically transcribed, child-
directed speech in languages other than English.
Second, these experiments use phonemically tran-
scribed corpora as the input and, as such, make the
implicit simplifying assumption that, when children
learn to segment speech into words, they have al-
ready learned phonological rules and know how to
reduce the inherent variability in speech to a finite
(and rather small) number of abstract categories: the
phonemes. Rytting, Brew and Fosler-Lussier (2010)
addressed this issue and replaced the usual phone-
mic input with probability vectors over a finite set
of symbols. Still, this set of symbols is limited to
the phonemic inventory of the language: the reduc-
tion of phonetic variation is taken for granted. In
other words, previous simulations evaluated the per-
formance of the models given idealized input but of-
fered no guarantee as to the performance of the mod-
1
els on realistic input.
We present a comparative survey that evaluates
the extent to which state-of-the-art statistical models
of word segmentation resist segmental variation. To
do so, we designed a parametric benchmark where
more and more variation was gradually introduced
into phonemic corpora of child-directed speech.
Phonetic variation was simulated applying context-
dependent allophonic rules to phonemic corpora.
Other corpora in which noise was created by ran-
dom phoneme substitutions were used as controls.
Furthermore, to draw language-independent conclu-
sions, we used corpora from three typologically dif-
ferent languages: English, French and Japanese.
2 Robustness benchmark
2.1 Word segmentation models
The segmentation task can be summarized as fol-
lows: given a corpus of utterances in which word
boundaries have been deleted, the model has to put
them back. Though we did not challenge the usual
idealization that children are able to segment speech
into discrete, phoneme-sized units, modeling lan-
guage acquisition imposes significant constraints on
the models (Brent, 1999; Gambell and Yang, 2004):
they must generalize to different (if not all) lan-
guages, start without any knowledge specific to a
particular language, learn in an unsupervised man-
ner and, most importantly, operate incrementally.
Online learning is a sound desideratum for any
model of language acquisition: indeed, human
language-processors do not wait, in Brent?s words,
?until the corpus of all utterances they will ever
hear becomes available?. Therefore, we favored an
?infant-plausible? setting and only considered on-
line word segmentation models, namely MBDP-1
(Brent, 1999) and NGS-u (Venkataraman, 2001).
Even if DP (Goldwater et al, 2009) was shown to
be more flexible than both MBDP-1 and NGS-u,
we did not include Goldwater et al?s batch model,
nor recent online variants by Pearl et al (in press),
in the benchmark. All aforementioned models rely
on word n-grams statistics and have similar perfor-
mance, but MBDP-1 and NGS-u are minimally suf-
ficient in providing an quantitative evaluation of how
cross-linguistic and/or segmental variation impact
the models? performance. We added two random
segmentation models as baselines. The four models
are described below.
2.1.1 MBDP-1
The first model is Heinz?s implementation of
Brent?s MBDP-1 (Brent, 1999; Heinz, 2006). The
general idea is that the best segmentation of an ut-
terance can be inferred from the best segmentation
of the whole corpus. However, explicitly search-
ing the space of all possible segmentations of the
corpus dramatically increases the model?s computa-
tional complexity. The implementation thus uses an
incremental approach: when the ith utterance is pro-
cessed, the model computes the best segmentation of
the corpus up to the ith utterance included, assuming
the segmentation of the first i?1 utterances is fixed.
2.1.2 NGS-u
This unigram model was described and imple-
mented by Venkataraman (2001). MBDP-1?s prob-
lems of complexity were circumvented using an in-
trinsically incremental n-gram approach. The strat-
egy is to find the most probable word sequence for
each utterance, according to information acquired
while processing previous utterances. In the end,
the segmentation of the entire corpus is the con-
catenation of each utterance?s best segmentation. It
is worth noting that NGS-u satisfies all three con-
straints proposed by Brent: strict incrementality,
non-supervision and universality.
2.1.3 Random
This dummy model rewrites its input, uniformly
choosing after each segment whether to insert a
word boundary or not. It defines a chance line at
and below which models can be considered ineffi-
cient. The only constraint is that no empty word is
allowed, hence no consecutive boundaries.
2.1.4 Random+
The second baseline is weakly supervised: though
each utterance is segmented at uniformly-chosen
random locations, the correct number of word
boundaries is given. This differs from Brent?s base-
line, which was given the correct number of bound-
aries to insert in the entire corpus. As before, con-
secutive boundaries are forbidden.
2
English French Japanese
Tokens Types Tokens Types Tokens Types
U 9,790 5,921 10,000 7,660 10,000 6,315
W 33,399 1,321 51,069 1,893 26,609 4,112
P 95,809 50 121,486 35 102,997 49
Table 1: Elementary corpus statistics, including number
of utterances (U), words (W) and phonemes (P).
2.2 Corpora
The three corpora we used were derived from tran-
scribed adult-child verbal interactions collected in
the CHILDES database (MacWhinney, 2000). For
each sample, elementary textual statistics are pre-
sented in Table 1. The English corpus contains
9790 utterances from the Bernstein?Ratner corpus
that were automatically transcribed and manually
corrected by Brent and Cartwright (1996). It has
been used in many word segmentation experiments
(Brent, 1999; Venkataraman, 2001; Batchelder,
2002; Fleck, 2008; Goldwater et al, 2009; among
others) and can be considered a de facto standard.
The French and the Japanese corpora were both
made by Le Calvez (2007), the former by automati-
cally transcribing the Champaud, Leveille? and Ron-
dal corpora, the latter by automatically transcribing
the Ishii and Noji corpora from ro?maji to phonemes.
To get samples comparable in size to the English
corpus, 10,000 utterances were selected at random
in each of Le Calvez?s corpora. All transcription
choices made by the authors in terms of phonemic
inventory and word segmentation were respected.1
2.3 Variation sources
The main effect of the transformations we applied
to the phonemic corpora was the increase in the av-
erage number of word forms per word. We refer to
this quantity, similar to a type-token ratio, as the cor-
pora?s lexical complexity. As allophonic variation
is context-dependent, the increase in lexical com-
plexity is, in this condition, limited by the phono-
tactic constraints of the language: the fewer con-
texts a phoneme appears in, the fewer contextual al-
lophones it can have. By contrast, the upper limit
is much higher in the control condition, as phoneme
1Some transcription choices made by Brent and Cartwright
are questionable (Blanchard and Heinz, 2008). Yet, we used the
canonical version of the corpus for the sake of comparability.
substitutions are context-free.
From a computational point of view, the applica-
tion of allophonic rules increases both the number of
symbols in the alphabet and, as a byproduct, the lex-
ical complexity. Obviously, when any kind of noise
or variation is added, there is less information in the
data to learn from. We can therefore presume that
the probability mass will be scattered, and that as a
consequence, statistical models relying on word n-
grams statistics will do worse than with phonemic
inputs. Yet, we are interested in quantifying how
such interference impacts the models? performance.
2.3.1 Allophonic variation
In this experiment, we were interested in the per-
formance of online segmentation models given rich
phonetic transcriptions, i.e. the input children pro-
cess before the acquisition of allophonic rules. Con-
sider the following rule that applies in French:
/r/ ?
{
[X] before a voiceless consonant
[K] otherwise
The application of this rule creates two contextual
variants for /kanar/ (canard, ?duck?): [kanaK Zon]
(canard jaune, ?yellow duck?) and [kanaX flotA?] (ca-
nard flottant, ?floating duck?). Before learning the
rule, children have to store both [kanaK] and [kanaX]
in their emerging lexicon as they are not yet able to
undo allophonic variation and construct a single lex-
ical entry: /kanar/.
Daland and Pierrehumbert (2010) compared the
performance of a phonotactic segmentation model
using canonical phonemic transcripts and transcripts
implementing conversational reduction processes.
They found that incorporating pronunciation vari-
ation has a mild negative impact on performance.
However, they used adult-directed speech. Even if,
as they argue, reduced adult-directed speech may
present a worst-case scenario for infants (compared
to hyperarticulated child-direct speech), it offers no
quantitative evaluation of the models? performance
using child-directed speech.
Because of the lack of phonetically transcribed
child-directed speech data, we emulated rich tran-
scriptions applying allophonic rules to the phonemic
corpora. To do so, we represented the internal struc-
ture of the phonemes in terms of articulatory fea-
tures and used the algorithm described by Boruta
3
(2011) to create artificial allophonic grammars of
different sizes containing assimilatory rules whose
application contexts span phonologically similar
contexts of the target phoneme. Compared to Da-
land and Pierrehumbert?s manual inspection of the
transcripts, this automatic approach gives us a finer
control on the degree of pronunciation variation.
The rules were then applied to our phonemic cor-
pora, thus systematizing coarticulation between ad-
jacent segments. We made two simplifying assump-
tions about the nature of the rules. First, all al-
lophonic rules we generated are of the type p ?
a / c where a phoneme p is realized as its allo-
phone a before context c. Thus, we did not model
rules with left-hand or bilateral contexts. Second,
we ensured that no two allophonic rules introduced
the same allophone (as in English flapping, where
both /t/ and /d/ have an allophone [R]), using parent
annotation: each phone is marked by the phoneme
it is derived from (e.g. [R]/t/ and [R]/d/). This was
done to avoid probability mass derived from differ-
ent phonemes merging onto common symbols.
The amount of variation in the corpora is de-
termined by the average number of allophones per
phoneme. We refer to this quantity as the corpora?s
allophonic complexity. Thus, at minimal allophonic
complexity, each phoneme has only one possible re-
alization (i.e. phonemic transcription), whereas at
maximal allophonic complexity, each phoneme has
as many realizations as attested contexts. For each
language, the range of attested lexical and allo-
phonic complexities obtained using Boruta?s (2011)
algorithm are reported in Figure 1.
2.3.2 Phoneme substitutions
Allophonic variation is not the only type of varia-
tion that may interfere with word segmentation. In-
deed, the aforementioned simulations assumed that
all phonemes are recognized with 100% accuracy,
but ?due to factors such as noise or speech rate?
human processors may mishear words. In this con-
trol condition, we examined the models? perfor-
mance on corpora in which some phonemes were
replaced by others. Thus, substitutions increase the
corpus? lexical complexity without increasing the
number of symbols: phoneme misrecognitions give
a straightforward baseline against which to compare
the models? performance when allophonic variation
5 10 15 20
1.0
1.5
2.0
2.5
3.0
3.5
4.0
l
l
l
l
l
l
l
l
l English
French
Japanese
Figure 1: Lexical complexity (the average number of
word forms per word) as a function of allophonic com-
plexity (the average number of allophones per phoneme).
has not been reduced. Such corpora can be consid-
ered the output of a hypothetical imperfect speech-
to-phoneme system or a winner-take-all scalar re-
duction of Rytting et al?s (2010) probability vectors.
We used a straightforward model of phoneme
misrecognition: substitutions are based neither on
a confusion matrix (Nakadai et al, 2007) nor on
phoneme similarity. Starting from the phonemic
corpus, we generated 10 additional corpora con-
trolling the proportion of misrecognized phonemes,
ranging from 0 (perfect recognition) to 1 (constant
error) in increments of 0.1. A noise intensity of n
means that each phoneme has probability n of being
rewritten by another phoneme. The random choice
of the substitution phoneme is weighted by the rela-
tive frequencies of the phonemes in the corpus. The
probability P (p ? x) that a phoneme x rewrites a
phoneme p is defined as
P (p? x) =
?
?
?
1? n if p = x
n
(
f(x) +
f(p)
|P| ? 1
)
otherwise
where n is the noise intensity, f(x) the relative fre-
quency of phoneme x in the corpus andP the phone-
mic inventory of the language.
4
2.4 Evaluation
We used Venkataraman?s (2001) implementation of
the now-standard evaluation protocol proposed by
Brent (1999) and then extended by Goldwater et al
(2009). Obviously, orthographic words are not the
optimal target for a model of language acquisition.
Yet, in line with previously reported experiments,
we used the orthographic segmentation as the stan-
dard of correct segmentation.
2.4.1 Scoring
For each model, we report (as percentages) the
following scores as functions of the lexical complex-
ity of the corpus:
? Ps, Rs, Fs: precision, recall and F -score on
word segmentation as defined by Brent;
? Pl, Rl, Fl: precision, recall and F -score on the
induced lexicon of word types: let L be the
standard lexicon and L? the one discovered by
the algorithm, we define Pl = |L ? L?|/|L?|,
Rl = |L?L?|/|L| and Fl = 2?Pl ?Rl/(Pl+Rl).
The difference between scoring the segmenta-
tion and the lexicon can be exemplified consider-
ing the utterance [@wUd?2kwUd?2kwUd] (a wood-
chuck would chuck wood). If it is segmented as
[@ wUd?2k wUd ?2k wUd], both the segmentation
and the induced lexicon are correct. By contrast, if
it is segmented as [@ wUd ?2k wUd?2k wUd], the
lexicon is still accurate while the word segmentation
is incorrect. A good segmentation inevitably yields a
good lexicon, but the reverse is not necessarily true.
2.4.2 k-shuffle cross-validation
As the segmental variation procedures and the
segmentation baselines are non-deterministic pro-
cesses, all scores were averaged over multiple simu-
lations. Moreover, as MBDP-1 and NGS-u operate
incrementally, their output is conditioned by the or-
der in which utterances are processed. To lessen the
influence of the utterance order, we shuffled the cor-
pora for each simulation. Testing all permutations of
the corpora for each combination of parameter val-
ues is computationally intractable. Thus, scores re-
ported below were averaged over three distinct sim-
ulations with shuffled corpora.
JP
FR
EN
a. Segmentation F?score
0 10 20 30 40 50 60 70 80 90
JP
FR
EN
b. Lexicon F?score
0 10 20 30 40 50 60 70 80 90
MBDP?1
NGS?u
Random+
Random
Figure 2: Cross-linguistic performance of MBDP-1 and
NGS-u on child-directed phonemic corpora in English
(EN), French (FR) and Japanese (JP).
3 Results and discussion
3.1 Cross-linguistic evaluation
Performance of the segmentation models2 on phone-
mic corpora is presented in Figure 1 in terms of Fs-
and Fl-score (upper and lower panel, respectively).
We were able to replicate previous results on En-
glish by Brent and Venkataraman almost exactly; the
small difference, less than one percent, was probably
caused by the use of different implementations.
From a cross-linguistic point of view, the main
observation is that these models do not seem
to generalize to typologically different languages.
Whereas MBDP-1 and NGS-u?s Fs value is 69%
for English, it is only 54% for French and 41% for
Japanese. Similar observations can be made for Fl.
Purely statistical strategies seem to be particularly
ineffective on our Japanese sample: inserting word
boundaries at random yields a better lexicon than us-
ing probabilistic models.
A crude way to determine whether a word seg-
mentation model tends to break words apart (over-
segmentation) or to cluster various words in a single
chunk (under-segmentation) is to compare the aver-
age word length (AWL) in its output to the AWL in
the standard segmentation. If the output?s AWL is
greater than the standard?s, then the output is under-
segmented, and vice versa. Even if NGS-u produces
2The full table of scores for each language, variation source,
and segmentation model was not included due to space limita-
tions. It is available upon request from the first author.
5
shorter words than MBDP-1, both models exhibit,
once again, similar within-language behaviors. En-
glish was slightly under-segmented by MBDP-1 and
over-segmented by NGS-u: ouputs? AWL are re-
spectively 3.1 and 2.7, while the standard is 2.9.
Our results are consistent with what Goldwater et al
(2009) observed for DP: error analysis shows that
both MBDP-1 and NGS-u also break off frequent
English morphological affixes, namely /IN/ (-ing)
and /s,z/ (-s). As for French, AWL values suggest
the corpus was under-segmented: 3.1 for MBDP-1?s
output and 2.9 for NGS-u?s, while the standard is
2.4. On the contrary, Japanese was heavily over-
segmented: many monophonemic words emerged
and, whereas the standard AWL is 3.9, the ouputs?
AWL is 2.7 for both models.
Over-segmentation may be correlated to the num-
ber of syllable types in the language: English
and French phonotactics allow consonantal clusters,
bringing the number of syllable types to a few thou-
sands. By contrast, Japanese has a much simpler
syllabic structure and less syllable types which, as
a consequence, are often repeated and may (incor-
rectly) be considered as words by statistical mod-
els. The fact that the models do worse for French
and Japanese is not especially surprising: both lan-
guages have many more affixal morphemes than En-
glish. Consider French, where the lexical autonomy
of clitics is questionable: whereas /s/ (s? or c?) or
/k/ (qu?) are highly frequent words in our ortho-
graphic standard, many errors are due to the aggluti-
nation of these clitics to the following word. These
are counted as segmentation errors, but should they?
Furthermore, none of the segmentation models
we benchmarked exhibit similar performance across
languages: invariably, they perform better on En-
glish. There may be a correlation between the per-
formance of segmentation models and the percent-
age of word hapaxes, i.e. words which occur only
once in the corpus: the English, French and Japanese
corpora contain 31.7%, 37.1% and 60.7% of word
hapaxes, respectively. The more words tend to occur
only once, the less MBDP-1, NGS-u and DP per-
form on segmentation. This is consistent with the
usual assumption that infants use familiar words to
find new ones. It may also be the case that these
models are not implicitly tuned to English, but that
the contribution of statistical cues to word segmen-
tation differs across languages. In French, for exam-
ple, stress invariably marks the end of a word (al-
though the end of a word is not necessarily marked
by stress). By contrast, there are languages like
English or Spanish where stress is less predictable:
children cannot rely solely on this cue to extract
words and may thus have to give more weight to
statistics.
3.2 Robustness to segmental variation
The performance of MBDP-1, NGS-u and the two
baselines on inputs altered by segmental variation
is presented in Figure 2.3 The first general observa-
tion is that, as predicted, MBDP-1 and NGS-u do not
seem to resist an increase in lexical complexity. In
the case of allophonic variation, their performance
is inversely related to the corpora?s allophonic com-
plexity. However, as suggested by the change in
the graphs? slope, performance for English seems
to stabilize at 2 word forms per word. Similar ob-
servations can be made for French and Japanese on
which the performance of the models is even worse:
Fl values are below chance at 1.7 and 3 variants per
word for Japanese and French, respectively; like-
wise, Fs is below chance at 1.5 for Japanese and
2.5 for French. Phoneme substitutions also impede
the performance of MBDP-1 and NGS-u: the more
phonemes are substituted, the more difficult it be-
comes for the algorithms to learn how to insert word
boundaries. Furthemore, Fl is below chance for
complexities greater than 4 for French, and approx-
imately 2.5 for Japanese. It is worth noting that, in
both conditions, the models exhibit similar within-
language performance as the complexity increases.
The potential lexicon that can be built by com-
bining segments into words may account for the
discrepancy between the two conditions, as it is in
fact the models? search space. In the control con-
dition, substituting phonemes does not increase its
size. However, the likelihood of a given phoneme in
a given word being replaced by the same substitu-
tion phoneme decreases as words get longer. Thus,
the proportion of hapax increases, making statisti-
cal segmentation harder to achieve. By contrast, the
3For the control condition, we did not graph scores for noise
intensities greater than 0.2: 80% accuracy is comparable to the
error rates of state-of-the-art systems in speaker-independent,
continuous speech recognition (Makhoul and Schwartz, 1995).
6
1 2 3 4 5 6 7 8
10
20
30
40
50
60
70
a. English: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l l
l l l
l
l
1 2 3 4 5 6 7 8
10
20
30
40
50
60
70
b. English: lexicon F?score
Le
xic
on
 F?
sco
re
l MBDP?1
NGS?u
Random+
Random
Allophony
Substitutions
l
l
l l l l l l
l
l
1 2 3 4 5 6
10
20
30
40
50
60
c. French: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l l
l
l l l
l
l
1 2 3 4 5 6
10
20
30
40
50
60
d. French: lexicon F?score
Le
xic
on
 F?
sco
re
l
l
l
l l l l l
l
l
l
1.0 1.5 2.0 2.5 3.0
10
20
30
40
50
e. Japanese: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l
l
l
l
l
l
l
1.0 1.5 2.0 2.5 3.0
10
20
30
40
50
f. Japanese: lexicon F?score
Le
xic
on
 F?
sco
re
l
l l l
l
l
l
l
l
l
Figure 3: Fs-score (left column) and Fl-score (right column) as functions of the lexical complexity, i.e. the number of
word forms per word, in the English (top row), French (middle row) and Japanese (bottom row) corpora.
7
application of allophonic rules increases the number
of objects to build words with; as a consequence, the
size of the potential lexicon explodes.
As neither MBDP-1 nor NGS-u is designed to
handle noise, the results are unsurprising. Indeed,
any word form found by these models will be incor-
porated in the lexicon: if [l?NgwI?] and [l?NgwI?]
are both found in the corpus, these variants will be
included as is in the lexicon. There is no mechanism
for ?explaining away? data that appear to have been
generated by systematic variation or random noise.
It is an open issue for future research to create ro-
bust models of word segmentation that can handle
segmental variation.
4 Conclusions
We have shown, first, that online statistical mod-
els of word segmentation that rely on word n-gram
statistics do not generalize to typologically differ-
ent languages. As opposed to French and Japanese,
English seems to be easier to segment using only
statistical information. Such differences in perfor-
mance from one language to another emphasize the
relevance of cross-linguistic studies: any conclusion
drawn from the monolingual evaluation of a model
of language acquisition should be considered with
all proper reservations. Second, our results quan-
tify how imperfect, though realistic, inputs impact
MBDP-1?s and NGS-u?s performance. Indeed, both
models become less and less efficient in discover-
ing words in transcribed child-directed speech as
the number of variants per word increases: though
the performance drop we observed is not surpris-
ing, it is worth noting that both models are less ef-
ficient than random procedures at about twenty al-
lophones per phoneme. However, the number of
context-dependent allophones we introduced is far
less than what is used by state-of-the-art models of
speech recognition (Makhoul and Schwartz, 1995).
To our knowledge, there is no computational
model of word segmentation that both respects the
constraints imposed on a human learner and accom-
modates noise. This highlights the complexity of
early language acquisition: while no accurate lex-
icon can be learned without a good segmentation
strategy, state-of-the-art models fail to deliver good
segmentations in non-idealized settings. Our re-
sults also emphasize the importance of other cues
for word segmentation: statistical learning may be
helpful or necessary for word segmentation, but it is
unlikely that it is sufficient.
The mediocre performance of the models
strengthens the hypotheses that phonological
knowledge is acquired in large part before the
construction of a lexicon (Jusczyk, 1997), or that
allophonic rules and word segmentations could be
acquired jointly (so that neither is a prerequisite
for the other): children cannot extract words from
fluent speech without knowing how to undo at least
part of contextual variation. Thus, the knowledge
of allophonic rules seems to be a prerequisite for
accurate segmentation. Recent simulations of word
segmentation and lexical induction suggest that
using phonological knowledge (Venkataraman,
2001; Blanchard and Heinz, 2008), modeling
morphophonological structure (Johnson, 2008) or
preserving subsegmental variation (Rytting et al,
2010) invariably increases performance. Vice
versa, Martin et al (submitted) have shown that the
algorithm proposed by Peperkamp et al (2006) for
undoing allophonic variation crashes in the face of
realistic input (i.e. many allophones), and that it
can be saved if it has approximate knowledge of
word boundaries. Further research is needed, at
both an experimental and a computational level, to
explore the performance and suitability of an online
model that combines the acquisition of allophonic
variation with that of word segmentation.
References
E. Batchelder. 2002. Bootstrapping the lexicon: a com-
putational model of infant speech segmentation. Cog-
nition, 83:167?206.
D. Blanchard and J. Heinz. 2008. Improving word seg-
mentation by simultaneously learning phonotactics. In
Proceedings of the Conference on Natural Language
Learning, pages 65?72.
L. Boruta. 2011. A note on the generation of allophonic
rules. Technical Report 0401, INRIA.
M. R. Brent and T. A. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61:93?125.
M. R. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34(1?3):71?105.
8
R. Daland and J. B. Pierrehumbert. 2010. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119?155.
M. Fleck. 2008. Lexicalized phonotactic word segmen-
tation. In Proceedings of ACL-2008, pages 130?138.
T. Gambell and C. Yang. 2004. Statistics learning and
universal grammar: Modeling word segmentation. In
Proceedings of the 20th International Conference on
Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2009. A
bayesian framework for word segmentation: exploring
the effects of context. Cognition, 112(1):21?54.
J. Heinz. 2006. MBDP-1, OCaml implementation. Re-
trieved from http://phonology.cogsci.udel.edu/?heinz/
on January 26, 2009.
E. K. Johnson and P. W. Jusczyk. 2001. Word segmenta-
tion by 8-month-olds: When speech cues count more
than statistics. Journal of Memory and Language,
44:548?567.
M. Johnson. 2008. Unsupervised word segmentation for
Sesotho using adaptor grammars. In Proceedings of
the 10th Meeting of ACL SIGMORPHON, pages 20?
27.
P. Jusczyk. 1997. The Discovery of Spoken Language.
MIT Press.
R. Le Calvez. 2007. Approche computationnelle de
l?acquisition pre?coce des phone`mes. Ph.D. thesis,
UPMC.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Elbraum Associates.
J. Makhoul and R. Schwartz. 1995. State of the art in
continuous speech recognition. PNAS, 92:9956?9963.
A. Martin, S. Peperkamp, and E. Dupoux. Submitted.
Learning phonemes with a pseudo-lexicon.
K. Nakadai, R. Sumiya, M. Nakano, K. Ichige, Y. Hi-
rose, and H. Tsujino. 2007. The design of phoneme
grouping for coarse phoneme recognition. In IEA/AIE,
pages 905?914.
L. Pearl, Sh. Goldwater, and M. Steyvers. In press. On-
line learning mechanisms for bayesian models of word
segmentation. Research on Language and Computa-
tion.
S. Peperkamp, R. Le Calvez, J. P. Nadal, and E. Dupoux.
2006. The acquisition of allophonic rules: statisti-
cal learning with linguistic constraints. Cognition,
101(3):B31?B41.
C. A. Rytting, C. Brew, and E. Fosler-Lussier. 2010.
Segmenting words from natural speech: subsegmen-
tal variation in segmental cues. Journal of Child Lan-
guage, 37:513?543.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
9
