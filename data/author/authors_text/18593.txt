Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1765?1775,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Generative Joint, Additive, Sequential Model
of Topics and Speech Acts in Patient-Doctor Communication
Byron C. Wallace?, Thomas A. Trikalinos?, M. Barton Laws?,
Ira B. Wilson? and Eugene Charniak?
?Dept. of Health Services, Policy & Practice, Brown University, Providence, RI
?Dept. of Computer Science, Brown University, Providence, RI
{byron wallace, thomas trikalinos, michael barton laws
ira wilson, eugene charniak}@brown.edu
Abstract
We develop a novel generative model of con-
versation that jointly captures both the top-
ical content and the speech act type asso-
ciated with each utterance. Our model ex-
presses both token emission and state tran-
sition probabilities as log-linear functions of
separate components corresponding to topics
and speech acts (and their interactions). We
apply this model to a dataset comprising anno-
tated patient-physician visits and show that the
proposed joint approach outperforms a base-
line univariate model.
1 Introduction
Communication involves at least two aspects: the
words one says and the acts one performs in saying
them. Examples of the latter include asking ques-
tions, issuing commands, and so on. These are re-
ferred to as speech acts under the sociolinguistic the-
ory of Austin (1955), which was further developed
by Searle (1969; 1985). Recognizing speech acts is
crucial to understanding communication because a
speaker?s meaning is only partially captured by the
words they use; much of their intent is expressed im-
plicitly via speech acts (Searle, 1969).
On this view, conversational utterances can be as-
signed both a topic and a speech act. The former
describes the subject matter of what was said and
the latter captures the ?social act? (e.g., promising)
performed by saying it. For example, the utterance
?Obama won the election? is topically political and
is an example of an information giving speech act.
?Did Obama win the election??, meanwhile, belongs
Role Utterance Topic Speech act
D Let me just write down some
of these issues here so I get
them straight in my mind.
Logistics Commissive
P Doctor you ain?t got to tell me
nuttin?.
Socializing Directive
P I?m in very good hands when
I?m around you.
Socializing Give Info.
P If push comes to a shove, you
open the window and throw
me out.
Socializing Humor/Levity
D I wanted to ask you, too - Biomedical Conv. Mgmt.
D you know you had that
colonic polyp -
Biomedical Ask Q.
D - is it two years from now that
they?re going to be doing the
repeat?
Biomedical Ask Q.
P Yeah. Biomedical Conv. Mgmt.
D We?ll do the repeat coloscopy
in about two years.
Biomedical Give Info.
Table 1: An excerpt from a patient-doctor interaction,
annotated with topic and speech act codes. The D and
P roles denote doctor and patient, respectively. Conv.
Mgmt. abbreviates conversation management; Ask Q. ab-
breviates ask question.
to the same topic but is a question. Both aspects are
necessary to understand conversation.
Previous computational work on speech acts ?
which we review in Section 6 ? has modeled them
in isolation (Perrault and Allen, 1980; Stolcke et al,
1998; Stolcke et al, 2000; Kim et al, 2010), i.e.,
independent of topical content. But a richer model
would account for both speech acts and the contex-
tualizing topic of each utterance. To this end, we de-
velop a novel joint, generative model of topics and
speech acts.
We focus on physician-patient communication as
a motivating domain. This is of interest because
1765
it is widely appreciated that effective communica-
tion is an integral part of clinical practice (Irwin and
Richardson, 2006; Makoul, 2001; Teutsch, 2003).
We provide an excerpt of a conversation between a
patient and their doctor annotated with topics and
speech acts in Table 1. Such annotations can provide
substantive insights into how doctors communicate
with patients (Ong et al, 1995).
A concrete example of this is the use of topic
and speech act codes to assess the efficacy of an
intervention meant to influence physician-patient
communication regarding adherence to antiretrovi-
ral (ARV) medication (Wilson et al, 2010). To
measure the effect of the intervention, investigators
performed a randomized control trial in which they
quantified change in communication patterns by tal-
lying the number of information giving speech acts
that fell under the ARV adherence topic. Without
assigning both topics and speech acts to utterances,
this analysis would not have been possible.
In this work, we develop a novel component-
based generative model for bivariate, sequentially
structured problems. Our approach extends the re-
cently proposed Sparse Additive Generative (SAGE)
model (Eisenstein et al, 2011) and similar recently
developed additive models (Paul and Dredze, 2012;
Paul et al, 2013) to the case of supervised sequen-
tial tasks to capture the joint conditional influence
of topics and speech acts, both with respect to token
generation and state transitions. For brevity, we refer
to this generative Joint, Additive, Sequential model
as JAS. In contrast to previous work on speech acts,
JAS provides a single, coherent generative model of
conversations. And because it is component-based,
this model provides a flexible framework for analyz-
ing communication patterns. We demonstrate that
JAS outperforms a generative univariate baseline in
topic/speech act prediction. Further, we automati-
cally reproduce an analysis of the aforementioned
randomized control trial, and in doing so show that
JAS reproduces the results more faithfully than a
univariate approach.
2 The Markov-Multinomial Model
We begin by considering a baseline generative ap-
proach to modeling topics and speech acts indepen-
dently. This simple approach was used by Stolcke et
al. (2000) to model speech acts. It accounts for only
a single output at each time point yt ? Y , and hence
here we model topics and speech acts independently.
A straight-forward (albeit na??ve) alternative
would be to treat the Cartesian product of topics
and speech acts as a single output space on which
emissions and transitions are conditioned, but this
space is too large and sparse for this approach to be
practicable. We note that the fully coupled HMM
(Brand et al, 1997) suffers from a similar exponen-
tial output state problem. The related factorial HMM
(FHMM) (Ghahramani and Jordan, 1997; Van Gael
et al, 2008), meanwhile, imposes unwarranted (in
our case) independence assumptions with respect to
state transitions along parallel chains, does not obvi-
ously lend itself to discrete observations (typically
Gaussians are assumed), and does not scale well
enough (in terms of training time) to be feasible for
our application.
The Markov-Multinomial (MM) comprises two
components; transitions and emissions. The former
is modeled by making a first-order Markov assump-
tion, specifically:
P (yt|y0, ..., yt?1) = P (yt|yt?1) = ?yt?1,yt (1)
Emissions can be modeled via a multinomial
that captures the conditional probabilities of to-
kens given labels. Denoting an utterance (an utter-
ance comprises the words corresponding to a single
speech act; see Section 4) at time t by ut and its la-
bel by yt, and making the standard na??ve assumption
that words are generated independently conditioned
on a label, we have:
P (ut|yt) =
?
w?ut
P (w|yt) =
?
w?ut
?yt,w (2)
Both sets of parameters (the ??s and the ? ?s) can be
estimated straight-forwardly using maximum like-
lihood (i.e., using observed counts). We can use
Viterbi decoding (Rabiner and Juang, 1986) to make
predictions for new sequences, as usual. To make
both topic and speech act predictions, we simply in-
duce models for each and make predictions indepen-
dently.
3 JAS: A Joint, Additive, Sequential Model
An obvious shortcoming of the simple MM model
outlined above is that it treats topics and speech acts
1766
as statistically independent. They are not (as con-
firmed at statistical significance p < .001 using a
?2 test). One would prefer a more expressive model
that conditions topic and speech act transitions as
well as the production of utterances jointly on both
the current topic and the current speech act.
More specifically, we would like a model that re-
flects the assumption that some latent intent gives
rise to both the topic and the speech act associated
with an utterance. This is consistent with Searle?s
(1969) notion of perlocutionary effects; one per-
forms speech acts with the aim of getting someone
to do something. Intent gives rise to the current
topic and speech act, and the current intent affects
the next; this induces a correlation between adjacent
topics and speech acts. This conceptual model is de-
picted graphically in the left-half of Figure 1.
The latent intent may be, e.g., to encourage a pa-
tient to take their medication more regularly. In our
application the topical content may be ARV adher-
ence and the type of speech act would be selected
by the provider (presumably to maximize the likeli-
hood of patient adherence). For example, she may
opt to urge imperatively (?You really need to take
your medicine?) or to implore with a question (?Will
you please remember to take your medicine??). Be-
cause we have no way of explicitly modeling intent
(it is never observed), we instead rely on variables
for which we have annotations (i.e., the topics and
speech acts; see Figure 1). We next describe the
model in more detail.
We refer to the topic set by Y , the speech act
set by S and the vocabulary as W . We denote the
(log of the) background probability of word w by
?w, and we will denote components corresponding
to deviations from ?w due to a specific topic (speech
act) by ?yw (?sw). Further, we include the component
?y,sw to capture interaction effects between topics and
speech acts. We assume that the conditional proba-
bility of word w belonging to an utterance ut with
corresponding topic yt and speech act st is log-linear
with respect to these components, i.e.:
P (w|yt, st) =
1
Zw
exp{?w+?
yt
w +?
st
w +?
st,yt
w } (3)
Where Zw is a normalizing term (implicitly condi-
tioned on yt and st) defined as:
Zw =
?
w??W
exp{?w? + ?
yt
w? + ?
st
w? + ?
st,yt
w? } (4)
We make the standard na??ve assumption that words
are generated independently, given the topic and
speech act of the utterance to which they belong:
P (ut|yt, st) =
?
w?ut
P (w|yt, st) (5)
The per-token emission probability just described
falls under the additive generative family of models
recently proposed by Eisenstein et al (2011). How-
ever, in addition to conditional token emission prob-
abilities, here we need also to model the transition
probabilities such that the likelihood of transition-
ing to topic yt (and to speech act st) reflects both the
previous topic and the previous speech act, captur-
ing the dependencies illustrated in Figure 1. To this
end, we model topic and speech act transition proba-
bilities as log-linear functions of the preceding topic
and speech act.
We denote log of the background topic frequen-
cies by piY , and components capturing the influence
of transitioning to topic yt due to the preceding topic
and speech act by ?yt?1,yt and ?st?1,yt respectively.
We also include a component ?(yt?1,st?1),yt that cor-
responds to the interaction effect on topic transi-
tion probability due to the preceding topic/speech
act pair. We then model the topic transition prob-
ability (given the preceding states) as:
P (yt|yt?1, st?1) =
1
Zy
exp{piYyt +?yt?1,yt +?st?1,yt +?(yt?1,st?1),yt}
(6)
Where Zy is a normalizing term for the topic transi-
tions (implicitly conditioned on st?1, yt?1):
Zy =
?
y??Y
exp{piYy?+?yt?1,y?+?st?1,y?+?(yt?1,st?1),y?}
(7)
Similarly, denoting by piS log-transformed speech
act background frequencies, and including analo-
gous components as above that correspond to the in-
fluence of the preceding topic, speech act and their
interaction on transitioning into speech act st, we
1767
Topic
t-1
Speech Act
t-1
Utterance
t-1
Topic
t
Speech Act
t
Utterance
t
Intent
t-1
Intent
t
Topic
t-1
Speech Act
t-1
Utterance
t-1
Topic
t
Speech Act
t
Utterance
t
Figure 1: The generative story of utterances, depicted graphically. On the left we show our motivating conceptualiza-
tion: a latent intent gives rise to both the topic and speech acts; these, in turn, jointly induce a distribution over words
and transitions. On the right we show our operationalization of this concept. For clarity, we have denoted arrows
capturing influence due to topics with dotted lines.
have:
P (st|st?1, yt?1) =
1
Zs
exp{piSst +?st?1,st +?yt?1,st +?(yt?1,st?1),st}
(8)
Where Zs is a normalizing constant for speech acts
analogous to Equation 7. Putting things together:
P (yt, st|st?1, yt?1, ut) =
P (ut|yt, st) ? P (yt|yt?1, st?1) ? P (st|st?1, yt?1)
(9)
As implied by Figure 1, this model assumes that the
topic and speech act at time t are conditionally in-
dependent given the preceding topic and speech act
(yt?1 and st?1). This is intuitively agreeable be-
cause time intervenes as a blocking factor; condi-
tioning the current topic on the current speech act
(or vice versa) would contradict the fact that these
occur simultaneously. Instead, the correlation is in-
duced by the preceding topic/speech act pair. (That
said, this is still a simplifying assumption, as one
may instead choose to model speech act selection as
conditional on topic (Traum and Larsson, 2003).)
Predictions can again be made via Viterbi de-
coding (Rabiner and Juang, 1986) over a matrix of
pairs of joint topic/speech act states. The strategy of
modeling (additive) components allows JAS to avoid
problems due to sparsity in this large output space.
Model parameters can be estimated using stan-
dard optimization techniques. We fix the ?back-
ground? frequencies ?, piY , piS to the log of the
corresponding observed proportions of words, top-
ics and speech acts, respectively. For the remaining
parameters, one can use descent-based optimization
methods. The partial derivative for the topic-to-topic
transition component ?y,y? with respect to the likeli-
hood, for example, is:
?
??y,y?
=
?
s?S
C(y,s),y? ? P (y
?|y, s)C(y,s),? (10)
Where C(y,s),y? denotes the observed count of tran-
sitions from topic/speech act pair (y, s) to y?, and
C(y,s),? denotes the total number of observed transi-
tions out of this pair. The term P (y?|y, s) is with
respect to the current parameter estimates and is
defined in Equation 6. The partial derivatives for
the other component parameters (both transition and
emission) are analogous. We use a Newton opti-
mization method similar to the approach outlined by
Eisenstein et al (2011).1 We assess convergence
by calculating predictive performance on a held-out
portion (5%) of the training dataset at each step,
halting the descent when this declines.
4 Dataset
We use a corpus of patient-provider visits annotated
with Generalized Medical Interaction Anaylsis Sys-
tem (GMIAS) codes. The GMIAS has been used
to: characterize interaction processes in physician-
patient communication about ARV adherence in the
1With the exception that we do not explicitly model the dis-
tribution over component variances.
1768
Topic; Speech act Count (prevalence)
ARV Adherence; Ask Q 2939 (0.013)
ARV Adherence; Commissive 245 (0.001)
ARV Adherence; Continuation 328 (0.001)
ARV Adherence; Conv. Management 4298 (0.018)
ARV Adherence; Directive 1650 (0.007)
ARV Adherence; Empathy 111 (0.000)
ARV Adherence; Give Information 12796 (0.055)
ARV Adherence; Humor/Levity 46 (0.000)
ARV Adherence; Missing/other 977 (0.004)
ARV Adherence; Social-Ritual 15 (0.000)
Biomedical; Ask Q 13753 (0.059)
Biomedical; Commissive 1049 (0.005)
Biomedical; Continuation 1005 (0.004)
Biomedical; Conv. Management 17611 (0.076)
Biomedical; Directive 4617 (0.020)
Biomedical; Empathy 423 (0.002)
Biomedical; Give Information 54231 (0.233)
Biomedical; Humor/Levity 255 (0.001)
Biomedical; Missing/other 4426 (0.019)
Biomedical; Social-Ritual 119 (0.001)
Logistics; Ask Q 5517 (0.024)
Logistics; Commissive 2308 (0.010)
Logistics; Continuation 435 (0.002)
Logistics; Conv. Management 9672 (0.042)
Logistics; Directive 5148 (0.022)
Logistics; Empathy 100 (0.000)
Logistics; Give Information 23351 (0.101)
Logistics; Humor/Levity 135 (0.001)
Logistics; Missing/other 2732 (0.012)
Logistics; Social-Ritual 285 (0.001)
Missing/other; Ask Q 820 (0.004)
Missing/other; Commissive 70 (0.000)
Missing/other; Continuation 1173 (0.005)
Missing/other; Conv. Management 1605 (0.007)
Missing/other; Directive 523 (0.002)
Missing/other; Empathy 48 (0.000)
Missing/other; Give Information 3994 (0.017)
Missing/other; Humor/Levity 27 (0.000)
Missing/other; Missing/other 12103 (0.052)
Missing/other; Social-Ritual 69 (0.000)
Psycho-Social; Ask Q 2933 (0.013)
Psycho-Social; Commissive 164 (0.001)
Psycho-Social; Continuation 208 (0.001)
Psycho-Social; Conv. Management 4433 (0.019)
Psycho-Social; Directive 787 (0.003)
Psycho-Social; Empathy 262 (0.001)
Psycho-Social; Give Information 15521 (0.067)
Psycho-Social; Humor/Levity 63 (0.000)
Psycho-Social; Missing/other 1199 (0.005)
Psycho-Social; Social-Ritual 36 (0.000)
Socializing; Ask Q 1283 (0.006)
Socializing; Commissive 79 (0.000)
Socializing; Continuation 85 (0.000)
Socializing; Conv. Management 2166 (0.009)
Socializing; Directive 222 (0.001)
Socializing; Empathy 73 (0.000)
Socializing; Give Information 8981 (0.039)
Socializing; Humor/Levity 306 (0.001)
Socializing; Missing/other 849 (0.004)
Socializing; Social-Ritual 1685 (0.007)
Table 2: Topic/speech act pairs and their counts.
context of an intervention trial (Wilson et al, 2010);
analyze communication about sexual risk behavior
(Laws et al, 2011a); elucidate the association of
visit length with constructs of patient-centeredness
(Laws et al, 2011b); and to describe provider-
patient communication regarding ARV adherence
compared with communication about other issues
(Laws et al, 2012). GMIAS annotation is described
at length elsewhere,2 but we summarize it here for
completeness.
GMIAS segments conversation into utterances.
An utterance is here defined as a single completed
speech act. Previous coding systems have simply
defined an utterance as conveying a single thought
(Roter and Larson, 2002) or any independent or un-
restrictive dependent clause of a sentence (Ford and
Ford, 1995). Stolcke et al (2000) followed Meteer
et al (1995) in using ?sentence-level units?. These
definitions provide helpful guidance to coders, but
many speech acts are poorly formed grammatically,
and cannot be described as a ?clause?. Further, some
speech acts cannot be said to convey a ?thought? (or
sentence) at all, but rather are pre-syntactical (e.g.,
interjections and non-lexical utterances like laugh-
ter). In any case, most natural segmentations of con-
versations probably largely agree with intuition, and
are not likely to differ substantially.
The model we develop in this work assumes that
transcripts have been manually segmented. While
this comes at some cost, segmenting is still much
cheaper than annotating transcripts. Manually an-
notating a single visit with GMIAS codes takes 2-
4 hours and must be performed by someone with
substantive domain expertise. By contrast, segment-
ing transcripts into utterances takes at most 1/4th of
the time as annotation and can be done by a less
highly-skilled individual. That said, in future work
we hope to explore incorporating automatic segmen-
tation methods (Galley et al, 2003; Eisenstein and
Barzilay, 2008) into our approach.
Each utterance is assigned a single topic code and
a single speech act code. Inter-rater agreement has
been observed to be relatively high for this task:
Kappa between three trained annotators and a ref-
erence annotation ranged from 0.89 to 1.0 for top-
ics and 0.81 to 0.95 for speech acts. We next de-
2
https://sites.google.com/a/brown.edu/m-barton-laws/home/gmias
1769
scribe the topics and speech acts we consider in
more detail; Table 2 enumerates all pairs of these
and their respective counts in the corpus. We note
that GMIAS defines a hierarchy of both topic and
speech act codes, but here we only attempt to cap-
ture the highest level codes in these hierarchies.
Topics comprise six major categories: ARV
adherence, biomedical, logistics, missing/other,
psycho-social and socializing. Antiretroviral (ARV)
adherence applies to utterances that address ARV
medication usage. Biomedical utterances subsume
clinical observations and diagnostic conclusions.
Utterances that concern the business of conducting a
physical examination fall under logistics. The miss-
ing/other topic covers a few cases, including utter-
ances that are effectively outside of the GMIAS uni-
verse and inaudible utterances; however we note that
missing/other is a topic explicitly assigned by hu-
man annotators. The psycho-social topic includes
such issues as substance abuse, recovery, employ-
ment and relationships. Finally, socializing refers to
casual conversation unrelated to the business of the
medical visit, and to social rituals such as greetings.
There are 10 speech acts:3 ask question, commis-
sive, continuation, conversation management, direc-
tive, empathy, give information, humor/levity, miss-
ing/other, and social-ritual. Ask question is self-
explanatory. Utterances in which the speaker makes
a promise or resolves to take action are commissives.
A continuation refers to the completion of a previ-
ously interrupted speech act (these are rare). Con-
versation management describes utterances that fa-
cilitate turn-taking or guide discussion (?talk about
talk?). Directives refer to statements that look to
control or influence the behavior of the interlocutor.
Utterances that express responses to emotions, con-
cerns or feelings are coded under empathy. Com-
munication of (purported) facts falls under give in-
formation. Humor/levity captures jokes and jovial
conversation. Missing/other is the same as for top-
ics. Finally, social-ritual utterances represent for-
malities (e.g., ?thank you?).
The corpus we use includes 360 GMIAS anno-
tated patient-provider interactions (median length:
605 utterances). This data originated as part of
3These are high-level speech acts; technically each consti-
tutes a category of speech act types.
a study designed to assess the role of the patient-
provider relationship in explaining racial/ethnic dis-
parities in HIV care. Study subjects were HIV care
providers and their patients at four US care sites.
The group responsible for the data are awaiting a
decision from the institutional review board (IRB)
regarding whether we can make this data publicly
available in some form.
5 Experimental Results
Markov-Multinomial Joint Additive Sequential0.19
0.20
0.21
0.22
0.23
0.24
0.25
0.26
av
er
ag
e F
-sc
or
e
Figure 2: Mean F-scores across all topic/speech act pairs
for the Markov-Multinomial (MM; left) and the proposed
Joint Additive Sequential (JAS; right) models. The thick
black line shows the mean difference over ten different
folds; the thin grey lines describe per-fold differences.
The proposed JAS model outperforms the baseline MM
model for all folds
Our evaluation includes two parts.4 First, we
perform standard cross-validation over the afore-
mentioned 360 annotated interactions, evaluating F-
measure for each topic/speech act pair. Second,
we look to automatically reproduce an analysis of
4Source code at: https://github.com/bwallace/JAS; unfortu-
nately we do not yet have permission to post the data.
1770
a randomized control trial that assessed the efficacy
of an intervention meant to alter physician-patient
communication. We show that JAS outperforms the
baseline approach with respect to both tasks.
We emphasize that while we are here compar-
ing predictive performance, we are specifically in-
terested in fully generative models of conversations
due to the longer-term applications we have in mind.
We would like, e.g., to use this model to assess the
variation in communicative approaches across dif-
ferent doctors, and generative models are more nat-
urally amenable to answering such exploratory ques-
tions. Indeed, perhaps the main strength of the ad-
ditive component based sequential model we have
proposed here is that it will allow us to easily in-
corporate physician-specific parameters that capture
deviations in provider speech act and/or topic tran-
sition patterns. Further, we may soon have access
to many unannotated transcripts, and we would like
to learn from these; generative approaches allow
straight-forward exploitation of unlabeled data. For
these reasons, we did not experiment with discrim-
inative models, e.g., Dynamic Conditional Random
Fields (DCRFs) (Sutton et al, 2007) for this work.
5.1 Cross-fold Validation
Our aim is to measure model performance in terms
of correctly identifying both the topic and speech act
corresponding to each utterance. We quantify this
via the F-score calculated for each topic/speech act
pair that is observed at least once. One can see in
Table 2 that many such pairs have low prevalence;
this can result in undefined F-scores (e.g., when no
utterances are assigned to a given pair). In this case,
it is reasonable to treat these as zero values, as is
commonly done (Forman and Scholz, 2010). This
penalizes models when they completely fail to iden-
tify an entire class of utterances.
We first report macro-averages, that is, averages
of the individual topic/speech act pair F-scores.
Figure 2 displays the macro-averaged F-score for
each of the 10 folds (grey lines connect folds)
and the average of these (thick black line). The
JAS model achieves an average macro-averaged F-
score of .234 versus the .207 achieved by base-
line Markov-Multinomial (MM) model; JAS outper-
forms MM on every fold.
For a more granular picture, Figure 3 displays av-
erage F-score differences with respect to every indi-
vidual topic and speech act pair for which this differ-
ence was non-zero. This is the (signed) difference of
the F-score achieved using JAS minus that achieved
using the MM model; black lines thus correspond
to pairs for which JAS outperformed MM, and red
lines to pairs for which MM outperformed JAS. The
latter achieves an improvement of >= .05 for 10
pairs, and results in an F-score of > .02 below that
attained by MM only once.
The relatively low F-scores for the metrics quanti-
fying performance with respect to the cross of topic
and speech act codes belie relatively good over-
all (marginal) predictive performance. That is, we
achieve much better performance with respect to
metrics that measure topic and speech act predic-
tions independently of one another. This is due to the
very large output space under consideration (see Ta-
ble 2). Specifically, averaged over ten runs, the MM
model achieves a marginal mean topic F-score of
.667 and marginal mean speech act F-score of .516.
JAS begets a marginal mean topic F-score of .661
and a marginal mean speech act F-score of .544;
hence the JAS model incurs an F-score loss of .006
(a 0.9% decrease) with respect to marginal topic
code prediction, but improves the marginal speech
act F-score by .028 (a 5.4% increase).
5.2 (Re-)Analysis of Randomized Control Trial
We also evaluated performance by tallying model
predictions over 116 held-out cases collected from
a randomized, cross-over study of an intervention
aimed at improving physicians knowledge of pa-
tients anti-retroviral (ARV) adherence (Wilson et al,
2010). The intervention was a report given to the
physician before a routine office visit that contained
information regarding the patients ARV usage and
their beliefs about ARV therapy. To explore the ef-
ficacy of this intervention, 58 paired (116 total) au-
dio recorded visits were annotated with GMIAS; 58
correspond to visits before which the provider was
not provided with the report (control cases), while
the other 58 correspond to visits before which they
were (intervention cases).
Wilson et al (2010) demonstrated that the in-
tervention indeed increased adherence-related dia-
logue, and specifically the number of information
giving speech acts performed by the physician un-
1771
Figure 3: Average difference in F-scores corresponding to specific topic/speech act pairs, sorted by magnitude. Black
lines (extending rightward) represent pairs for which JAS outperforms the baseline model; red lines (leftward) are
pairs for which baseline performs better.
True MM JAS
control intervention control intervention control intervention
10 (4, 28) 23 (11, 39) 13 (5, 33) 27 (16, 44) 12 (5, 28) 23 (14, 40)
Table 3: Utterance counts {Median (25th, 75th per-
centile)} for the ARV/information giving topic and speech
act pair. We show the ?gold standard? (True) tallies,
which were assigned by humans, and the counts taken
using the two models, MM and JAS. The JAS model pre-
dictions are closer to the true numbers.
derneath this topic. We attempted to reproduce this
finding using automated rather manual annotations.
To this end, we trained MM and JAS models over
the aforementioned 360 annotated visits and then
used this model to generate topic and speech act
code predictions for the utterances comprising the
116 held-out visits used for the analysis (these were
not part of the training set). We then assessed the
direction and magnitude of the change in the num-
ber of ARV adherence/information giving utterances
in the paired control versus intervention cases. We
compared the results for this analysis calculated us-
ing the true (manually assigned) codes to the results
calculated using the predicted codes.
Following the original analysis (Wilson et
al., 2010), we report the median number of
ARV/information giving utterances and correspond-
ing 25th and 75th percentiles over the 58 control and
intervention visits, as counted using the true (hu-
man) annotations and using the codes predicted by
the MM and JAS models. These are reported in Ta-
ble 3. The JAS model predictions better match the
true labels in all except one case (the lower 25th for
the controls, for which it predicts the same number
as the MM model).
6 Related work
There is a relatively long history of research into
modeling conversational speech acts in computa-
tional linguistics. Perrault and Allen (1980) con-
ducted pioneering work on computationally formal-
izing speech acts, though their work pre-dates statis-
tical NLP and is therefore not directly relevant to the
present work.
Stolcke et al (2000; 1998) proposed a probabilis-
tic approach to modeling conversational speech acts
based on the Hidden Markov Model (HMM) (Ra-
biner and Juang, 1986). They were interested in
modeling an unrestricted set of conversations, and
did not impose a hierarchy on the speech acts; they
1772
therefore enumerated many more speech acts (42)
than we do in the present work (recall that we use 10
?high-level? speech acts).5 Their model has served
as the baseline approach in the present work. Stol-
cke et al also considered jointly performing speech
recognition and speech act classification.
Others have investigated visual structures of
patient-provider interactions to qualitatively assess
communication in care. Specifically, (Cretchley et
al., 2010) leveraged concept maps to explore conver-
sations between people with schizophrenia and their
carers. Briefly, this approach allowed them to (qual-
itatively) identify two distinct conversational strate-
gies used by care-takers and their patients. Angus et
al. (Angus et al, 2012) presented a similar approach
in which they used text visualization software to ex-
plore patterns of (inferred) topics in consultations.
Another thread of research has investigated classi-
fying speech acts in emails into one of a small set of
?email speech acts?, e.g., request, propose, commit
(Cohen et al, 2004; Goldstein et al, 2006). Cohen et
al. (2004) demonstrated that good performance can
be achieved for this task via existing text classifica-
tion technologies. Elsewhere, researchers have ex-
plored automatically inferring ?speech acts? in vari-
ous other online social mediums, including message
board posts (Qadir and Riloff, 2011), Wikipedia talk
pages (Ferschke et al, 2012) and Twitter (Zhang et
al., 2012).
A separate line of inquiry concerns classifying di-
alogue acts in chat. Researchers have attempted di-
alogue act classification both for 1-on-1 (Kim et al,
2010) and multi-party (Kim et al, 2012; Clark and
Popescu-Belis, 2004) online chats. Ang et al (2005)
considered the task of jointly segmenting and clas-
sifying utterances comprising multiparty meetings,
while Hsueh and Moore (2006) proposed analogous
methods for topic segmentation and labeling (other
works on topic segmentation include (Galley et al,
2003) and (Eisenstein and Barzilay, 2008)). Incor-
porating such segmentation methods into the pro-
posed model (rather than relying on inputs to be
manually segmented beforehand) would be a natu-
ral extension of this work.
Additive component models of text have recently
5We note that only 8 of the 42 speech acts appeared with
greater than 1% frequency in Stolcke et al?s corpus.
gained traction (Eisenstein et al, 2011; Paul, 2012;
Paul and Dredze, 2012; Paul et al, 2013). To our
knowledge, this is the first extension of supervised
additive component models to a sequential task.6
7 Conclusions and Future Directions
We have proposed a novel Joint, Additive, Sequen-
tial (JAS) model of conversational topics and speech
acts. In contrast to previous approaches to mod-
eling conversational exchanges, this model factors
both the current topic and the current speech act into
token emission and state transition probabilities. We
demonstrated that this model consistently outper-
forms a univariate generative baseline that treats
speech acts and topics independently. Furthermore,
we showed JAS can automatically re-produce the
analysis of a randomized control trial designed to as-
sess the efficacy of an intervention to alter physician
communication habits with high-fidelity.
The generative component-based framework we
have introduced in this work provides a means of
exploring factors in patient-physician communica-
tion. One limitation of the model we have pre-
sented is that it makes several simplifying assump-
tions around dialogue. For example, we have ig-
nored non-linearities and ?back-channels? in con-
versation, and we have ignored differences across
physicians with respect to communication styles.
Going forward, we hope to address these limita-
tions. We also plan on extending this model to in-
vestigate qualitative questions surrounding patient-
physician communication quantitatively. For exam-
ple, we are interested in investigating how communi-
cation varies across hospitals and physicians. To ex-
plore this, we can add additional components to the
transition probability terms corresponding to differ-
ent hospitals and doctors. Ultimately, we would like
to correlate patterns in physician communication (as
gleaned from the model) with objective, measured
health outcomes (e.g., patient satisfaction and adher-
ence to ARVs).
6Though Paul (2012) recently proposed ?mixed-
membership? Markov models for unsupervised conversation
modeling.
1773
8 Acknowledgements
The authors thank members of the Brown Labora-
tory for Linguistic Information Processing (BLLIP)
and Kevin Small for providing helpful feedback on
earlier versions of this work. We also thank the three
anonymous EMNLP reviewers for insightful com-
ments. This work was partially supported by the Na-
tional Institute of Mental Health (2 K24MH092242,
R34MH089279 and R01MH083595) and by NIDA
(R01DA015679).
References
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classification
in multiparty meetings. In Proc. ICASSP, volume 1,
pages 1061?1064.
Daniel Angus, Bernadette Watson, Andrew Smith, Cindy
Gallois, and Janet Wiles. 2012. Visualising con-
versation structure across time: Insights into effective
doctor-patient consultations. PloS one, 7(6).
John Langshaw Austin. 1955. How to do things with
words, volume 88. Harvard University Press.
Matthew Brand, Nuria Oliver, and Alex Pentland. 1997.
Coupled hidden Markov models for complex action
recognition. In Computer Vision and Pattern Recog-
nition, 1997. Proceedings., 1997 IEEE Computer So-
ciety Conference on, pages 994?999. IEEE.
Alexander Clark and Andrei Popescu-Belis. 2004.
Multi-level dialogue act tags. In Proc. SIGdial, pages
163?170.
William W Cohen, Vitor R Carvalho, and Tom M
Mitchell. 2004. Learning to classify email into speech
acts. In Proceedings of EMNLP, volume 4. sn.
Julia Cretchley, Cindy Gallois, Helen Chenery, and An-
drew Smith. 2010. Conversations between carers
and people with schizophrenia: a qualitative analy-
sis using leximancer. Qualitative Health Research,
20(12):1611?1628.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 334?343. Association for
Computational Linguistics.
J. Eisenstein, A. Ahmed, and E.P. Xing. 2011. Sparse
additive generative models of text. In Proceedings of
ICML, pages 1041?1048.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebotar.
2012. Behind the article: Recognizing dialog acts in
wikipedia talk pages. In Proceedings of the 13th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL 2012), pages 777?
786. Citeseer.
Jeffrey D Ford and Laurie W Ford. 1995. The role of
conversations in producing intentional change in or-
ganizations. Academy of Management Review, pages
541?570.
George Forman and Martin Scholz. 2010. Apples-to-
apples in cross-validation studies: pitfalls in classifier
performance measurement. volume 12, pages 49?57.
ACM.
Michel Galley, Kathleen McKeown, Eric Fosler-Lussier,
and Hongyan Jing. 2003. Discourse segmentation of
multi-party conversation. In Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics, pages 562?569. Association for Compu-
tational Linguistics.
Zoubin Ghahramani and Michael I Jordan. 1997. Facto-
rial hidden Markov models. Machine learning, 29(2-
3):245?273.
Jade Goldstein, Andrew Kwasinski, Paul Kingsbury,
R Sabin, and Albert McDowell. 2006. Annotating
subsets of the enron email corpus. In Proceedings of
the Third Conference on Email and Anti-Spam. Cite-
seer.
P-Y Hsueh and Johanna D Moore. 2006. Automatic
topic segmentation and labeling in multiparty dia-
logue. In Spoken Language Technology Workshop,
2006. IEEE, pages 98?101. IEEE.
Richard S Irwin and Naomi D Richardson. 2006.
Patient-focused careusing the right tools. CHEST
Journal, 130(1 suppl):73S?82S.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-one
live chats. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 862?871. Association for Computational Lin-
guistics.
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2012. Classifying dialogue acts in multi-party
live chats.
Michael Barton Laws, Ylisabyth S Bradshaw, Steven A
Safren, Mary Catherine Beach, Yoojin Lee, William
Rogers, and Ira B Wilson. 2011a. Discussion of sex-
ual risk behavior in HIV care is infrequent and appears
ineffectual: a mixed methods study. AIDS and Behav-
ior, 15(4):812?822.
Michael Barton Laws, Lauren Epstein, Yoojin Lee,
William Rogers, Mary Catherine Beach, and Ira B
Wilson. 2011b. The association of visit length and
measures of patient-centered communication in HIV
care: A mixed methods study. Patient Education and
Counseling, 85(3):e183?e188.
1774
Michael Barton Laws, Mary Catherine Beach, Yoojin
Lee, William H Rogers, Somnath Saha, P Todd Ko-
rthuis, Victoria Sharp, and Ira B Wilson. 2012.
Provider-patient adherence dialogue in HIV care: re-
sults of a multisite study. AIDS and Behavior, pages
1?12.
Gregory Makoul. 2001. Essential elements of communi-
cation in medical encounters: the kalamazoo consen-
sus statement. Academic Medicine, 76(4):390?393.
Marie W Meteer, Ann A Taylor, Robert MacIntyre, and
Rukmini Iyer. 1995. Dysfluency annotation stylebook
for the switchboard corpus. University of Pennsylva-
nia.
Lucille ML Ong, Johanna CJM De Haes, Alaysia M
Hoos, and Frits B Lammes. 1995. Doctor-patient
communication: a review of the literature. Social sci-
ence & medicine, 40(7):903?918.
Michael Paul and Mark Dredze. 2012. Factorial lda:
Sparse multi-dimensional text models. In Advances
in Neural Information Processing Systems 25, pages
2591?2599.
Michael J. Paul, Byron C. Wallace, and Mark Dredze.
2013. What affects patient (dis)satisfaction? analyz-
ing online doctor ratings with a joint topic-sentiment
model. In AAAI Workshop on Expanding the Bound-
aries of Health Informatics Using AI (HIAI).
Michael J Paul. 2012. Mixed membership Markov mod-
els for unsupervised conversation modeling. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 94?104.
Association for Computational Linguistics.
C Raymond Perrault and James F Allen. 1980. A plan-
based analysis of indirect speech acts. Computational
Linguistics, 6(3-4):167?182.
Ashequl Qadir and Ellen Riloff. 2011. Classifying sen-
tences as speech acts in message board posts. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 748?758. Asso-
ciation for Computational Linguistics.
Lawrence Rabiner and B Juang. 1986. An introduction
to hidden Markov models. ASSP Magazine, IEEE,
3(1):4?16.
Debra Roter and Susan Larson. 2002. The roter in-
teraction analysis system (rias): utility and flexibility
for analysis of medical interactions. Patient education
and counseling, 46(4):243?251.
John R Searle. 1969. Speech acts: An essay in the phi-
losophy of language. Cambridge university press.
John R Searle. 1985. Expression and meaning: Studies
in the theory of speech acts. Cambridge University
Press.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Martin, Marie
Meteer, Klaus Ries, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Dialog act modeling for conversa-
tional speech. In AAAI Spring Symposium on Apply-
ing Machine Learning to Discourse Processing, pages
98?105.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational linguistics, 26(3):339?373.
Charles Sutton, Andrew McCallum, and Khashayar Ro-
hanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. The Journal of Machine
Learning Research, 8:693?723.
Carol Teutsch. 2003. Patient-doctor communication.
The medical clinics of North America, 87(5):1115.
David R Traum and Staffan Larsson. 2003. The infor-
mation state approach to dialogue management. In
Current and new directions in discourse and dialogue,
pages 325?353. Springer.
Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahra-
mani. 2008. The infinite factorial hidden Markov
model. In Neural Information Processing Systems,
volume 21.
Ira B Wilson, M Barton Laws, Steven A Safren, Yoo-
jin Lee, Minyi Lu, William Coady, Paul R Skolnik,
and William H Rogers. 2010. Provider-focused inter-
vention increases adherence-related dialogue, but does
not improve antiretroviral therapy adherence in per-
sons with HIV. Journal of acquired immune deficiency
syndromes, 53(3):338.
Renxian Zhang, Dehong Gao, and Wenjie Li. 2012. To-
wards scalable speech act recognition in twitter: Tack-
ling insufficient training data. EACL 2012, page 18.
1775
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512?516,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Humans Require Context to Infer Ironic Intent
(so Computers Probably do, too)
Byron C. Wallace, Do Kook Choe, Laura Kertz and Eugene Charniak
Brown University
{byron wallace, do kook choe, laura kertz, eugene charniak}@brown.edu
Abstract
Automatically detecting verbal irony
(roughly, sarcasm) is a challenging task
because ironists say something other
than ? and often opposite to ? what they
actually mean. Discerning ironic intent
exclusively from the words and syntax
comprising texts (e.g., tweets, forum
posts) is therefore not always possible:
additional contextual information about
the speaker and/or the topic at hand is
often necessary. We introduce a new
corpus that provides empirical evidence
for this claim. We show that annota-
tors frequently require context to make
judgements concerning ironic intent, and
that machine learning approaches tend
to misclassify those same comments for
which annotators required additional
context.
1 Introduction & Motivation
This work concerns the task of detecting verbal
irony online. Our principal argument is that sim-
ple bag-of-words based text classification models
? which, when coupled with sufficient data, have
proven to be extremely successful for many natu-
ral language processing tasks (Halevy et al, 2009)
? are inadequate for irony detection. In this paper
we provide empirical evidence that context is often
necessary to recognize ironic intent.
This is consistent with the large body of prag-
matics/linguistics literature on irony and its us-
age, which has emphasized the role that context
plays in recognizing and decoding ironic utter-
ances (Grice, 1975; Clark and Gerrig, 1984; Sper-
ber and Wilson, 1981). But existing work on au-
tomatic irony detection ? reviewed in Section 2
? has not explicitly attempted to operationalize
such theories, and has instead relied on features
(mostly word counts) intrinsic to the texts that are
to be classified as ironic. These approaches have
achieved some success, but necessarily face an
upper-bound: the exact same sentence can be both
intended ironically and unironically, depending on
the context (including the speaker and the topic at
hand). Only obvious verbal ironies will be recog-
nizable from intrinsic features alone.
Here we provide empirical evidence for the
above claims. We also introduce a new annotated
corpus that will allow researchers to build models
that augment existing approaches to irony detec-
tion with contextual information regarding the text
(utterance) to be classified and its author. Briefly,
our contributions are summarized as follows.
? We introduce the first version of the reddit
irony corpus, composed of annotated com-
ments from the social news website reddit.
Each sentence in every comment in this cor-
pus has been labeled by three independent an-
notators as having been intended by the au-
thor ironically or not. This dataset is publicly
available.
1
? We provide empirical evidence that human
annotators consistently rely on contextual in-
formation to make ironic/unironic sentence
judgements.
? We show that the standard ?bag-of-words? ap-
proach to text classification fails to accurately
judge ironic intent on those cases for which
humans required additional context. This
suggests that, as humans require context to
make their judgements for this task, so too do
computers.
Our hope is that these observations and this
dataset will spur innovative new research on meth-
ods for verbal irony detection.
1
https://github.com/bwallace/
ACL-2014-irony
512
2 Previous Work
There has recently been a flurry of interesting
work on automatic irony detection (Tepperman
et al, 2006; Davidov et al, 2010; Carvalho et
al., 2009; Burfoot and Baldwin, 2009; Tsur et
al., 2010; Gonz?alez-Ib?a?nez et al, 2011; Filatova,
2012; Reyes et al, 2012; Lukin and Walker, 2013;
Riloff et al, 2013). In these works, verbal irony
detection has mostly been treated as a standard
text classification task, though with some innova-
tive approaches specific to detecting irony.
The most common data source used to experi-
ment with irony detection systems has been Twit-
ter (Reyes et al, 2012; Gonz?alez-Ib?a?nez et al,
2011; Davidov et al, 2010), though Amazon prod-
uct reviews have been used experimentally as well
(Tsur et al, 2010; Davidov et al, 2010; Reyes et
al., 2012; Filatova, 2012). Walker et al (2012)
also recently introduced the Internet Argument
Corpus (IAC), which includes a sarcasm label
(among others).
Some of the findings from these previous ef-
forts have squared with intuition: e.g., overzealous
punctuation (as in ?great idea!!!!?) is indicative of
ironic intent (Carvalho et al, 2009). Other works
have proposed novel approaches specifically for
irony detection: Davidov et al (2010), for ex-
ample, proposed a semi-supervised approach in
which they look for sentence templates indicative
of irony. Elsewhere, Riloff et al (2013) proposed
a method that exploits contrasting sentiment in the
same utterance to detect irony.
To our knowledge, however, no previous work
on irony detection has attempted to leverage
contextual information regarding the author or
speaker (external to the utterance). But this is nec-
essary in some cases, however. For example, in
the case of Amazon product reviews, knowing the
kinds of books that an individual typically likes
might inform our judgement: someone who tends
to read and review Dostoevsky is probably be-
ing ironic if she writes a glowing review of Twi-
light. Of course, many people genuinely do enjoy
Twilight and so if the review is written subtly it
will likely be difficult to discern the author?s in-
tent without this background. In the case of Twit-
ter, it is likely to be difficult to classify utterances
without considering the contextualizing exchange
of tweets (i.e., the conversation) to which they be-
long.
1
2
3
4
Figure 1: The web-based tool used by our annotators to la-
bel reddit comments. Enumerated interface elements are de-
scribed as follows: 1 the text of the comment to be anno-
tated ? sentences marked as ironic are highlighted; 2 buttons
to label sentences as ironic or unironic; 3 buttons to request
additional context (the embedding discussion thread or asso-
ciated webpage ? see Section 3.2); 4 radio button to provide
confidence in comment labels (low, medium or high).
3 Introducing the reddit Irony Dataset
Here we introduce the first version (? 1.0) of
our irony corpus. Reddit (http://reddit.
com) is a social-news website to which news
stories (and other links) are posted, voted on
and commented upon. The forum compo-
nent of reddit is extremely active: popular
posts often have well into 1000?s of user com-
ments. Reddit comprises ?sub-reddits?, which fo-
cus on specific topics. For example, http://
reddit.com/r/politics features articles
(and hence comments) centered around political
news. The current version of the corpus is avail-
able at: https://github.com/bwallace/
ACL-2014-irony. Data collection and annota-
tion is ongoing, so we will continue to release new
(larger) versions of the corpus in the future. The
present version comprises 3,020 annotated com-
ments scraped from the six subreddits enumerated
in Table 1. These comments in turn comprise a
total of 10,401 labeled sentences.
2
3.1 Annotation Process
Three university undergraduates independently
annotated each sentence in the corpus. More
specifically, annotators have provided binary ?la-
bels? for each sentence indicating whether or not
they (the annotator) believe it was intended by the
author ironically (or not). This annotation was
provided via a custom-built browser-based anno-
tation tool, shown in Figure 1.
We intentionally did not provide much guid-
ance to annotators regarding the criteria for what
2
We performed na??ve ?segmentation? of comments based
on punctuation.
513
sub-reddit (URL) description number of labeled comments
politics (r/politics) Political news and editorials; focus on the US. 873
conservative (r/conservative) A community for political conservatives. 573
progressive (r/progressive) A community for political progressives (liberals). 543
atheism (r/atheism) A community for non-believers. 442
Christianity (r/Christianity) News and viewpoints on the Christian faith. 312
technology (r/technology) Technology news and commentary. 277
Table 1: The six sub-reddits that we have downloaded comments from and the corresponding number of comments for which
we have acquired annotations in this ? version of the corpus. Note that we acquired labels at the sentence level, whereas the
counts above reflect comments, all of which contain at least one sentence.
constitutes an ?ironic? statement, for two reasons.
First, verbal irony is a notoriously slippery concept
(Gibbs and Colston, 2007) and coming up with an
operational definition to be consistently applied is
non-trivial. Second, we were interested in assess-
ing the extent of natural agreement between an-
notators for this task. The raw average agreement
between all annotators on all sentences is 0.844.
Average pairwise Cohen?s Kappa (Cohen, 1960)
is 0.341, suggesting fair to moderate agreement
(Viera and Garrett, 2005), as we might expect for
a subjective task like this one.
3.2 Context
Reddit is a good corpus for the irony detection
task in part because it provides a natural prac-
tical realization of the otherwise ill-defined con-
text for comments. In particular, each comment is
associated with a specific user (the author), and
we can view their previous comments. More-
over, comments are embedded within discussion
threads that pertain to the (usually external) con-
tent linked to in the corresponding submission (see
Figure 2). These pieces of information (previous
comments by the same user, the external link of
the embedding reddit thread, and the other com-
ments in this thread) constitute our context. All
of this is readily accessible. Labelers can opt to
request these pieces of context via the annotation
tool, and we record when they do so.
Consider the following example comment taken
from our dataset: ?Great idea on the talkathon
Cruz. Really made the republicans look like the
sane ones.? Did the author intend this statement
ironically, or was this a subtle dig on Senator
Ted Cruz? Without additional context it is diffi-
cult to know. And indeed, all three annotators re-
quested additional context for this comment. This
context at first suggests that the comment may
have been intended literally: it was posted in the
r/conservative subreddit (Ted Cruz is a conserva-
tive senator). But if we peruse the author?s com-
Figure 2: An illustrative reddit comment (highlighted). The
title (?Virginia Republican ...?) links to an article, providing
one example of contextualizing content. The conversational
thread in which this comment is embedded provides addi-
tional context. The comment in question was presumably in-
tended ironically, though without the aforementioned context
this would be difficult to conclude with any certainty.
ment history, we see that he or she repeatedly de-
rides Senator Cruz (e.g., writing ?Ted Cruz is no
Ronald Reagan. They aren?t even close.?). From
this contextual information, then, we can reason-
ably assume that the comment was intended iron-
ically (and all three annotators did so after assess-
ing the available contextual information).
4 Humans Need Context to Infer Irony
We explore the extent to which human annotators
rely on contextual information to decide whether
or not sentences were intended ironically. Recall
that our annotation tool allows labelers to request
additional context if they cannot make a decision
based on the comment text alone (Figure 1). On
average, annotators requested additional context
for 30% of comments (range across annotators of
12% to 56%). As shown in Figure 3, annotators
are consistently more confident once they have
consulted this information.
We tested for a correlation between these re-
quests for context and the final decisions regard-
ing whether comments contain at least one ironic
sentence. We denote the probability of at least one
annotator requesting additional context for com-
ment i by P (C
i
). We then model the probability
of this event as a linear function of whether or not
514
64
86
174
forced decision30 final decision
152
90
529
forced decision51 final decision
176
207
364
forced decision25 final decision
ironic ?ironic
ironic ?unironic
unironic ?unironic
unironic ?ironic
annotator 1
annotator 2 annotator 3
Figure 3: This plot illustrates the effect of viewing contextual information for three annotators (one table for each annotator).
For all comments for which these annotators requested context, we show forced (before viewing the requested contextual
content) and final (after) decisions regarding perceived ironic intent on behalf of the author. Each row shows one of four
possible decision sequences (e.g., a judgement of ironic prior to seeing context and unironic after). Numbers correspond to
counts of these sequences for each annotator (e.g., the first annotator changed their mind from ironic to unironic 86 times).
Cases that involve the annotator changing his or her mind are shown in red; those in which the annotator stuck with their initial
judgement are shown in blue. Color intensity is proportional to the average confidence judgements the annotator provided:
these are uniformly stronger after they have consulted contextualizing information. Note also that the context frequently results
in annotators changing their judgement.
any annotator labeled any sentence in comment i
as ironic. We code this via the indicator variable
I
i
which is 1 when comment i has been deemed
to contain an ironic sentence (by any of the three
annotators) and 0 otherwise.
logit{P (C
i
)} = ?
0
+ ?
1
I
i
(1)
We used the regression model shown in Equa-
tion 1, where ?
0
is an intercept and ?
1
captures
the correlation between requests for context for a
given comment and its ultimately being deemed
to contain at least one ironic sentence. We fit this
model to the annotated corpus, and found a signif-
icant correlation:
?
?
1
= 1.508 with a 95% confi-
dence interval of (1.326, 1.690); p < 0.001.
In other words, annotators request context sig-
nificantly more frequently for those comments
that (are ultimately deemed to) contain an ironic
sentence. This would suggest that the words
and punctuation comprising online comments
alone are not sufficient to distinguish ironic from
unironic comments. Despite this, most machine
learning based approaches to irony detection have
relied nearly exclusively on such intrinsic features.
5 Machines Probably do, too
We show that the misclassifications (with respect
to whether comments contain irony or not) made
by a standard text classification model signifi-
cantly correlate with those comments for which
human annotators requested additional context.
This provides evidence that bag-of-words ap-
proaches are insufficient for the general task of
irony detection: more context is necessary.
We implemented a baseline classification ap-
proach using vanilla token count features (binary
bag-of-words). We removed stop-words and lim-
ited the vocabulary to the 50,000 most frequently
occurring unigrams and bigrams. We added ad-
ditional binary features coding for the presence
of punctuational features, such as exclamation
points, emoticons (for example, ?;)?) and question
marks: previous work (Davidov et al, 2010; Car-
valho et al, 2009) has found that these are good
indicators of ironic intent.
For our predictive model, we used a linear-
kernel SVM (tuning the C parameter via grid-
search over the training dataset to maximize F1
score). We performed five-fold cross-validation,
recording the predictions y?
i
for each (held-out)
comment i. Average F1 score over the five-folds
was 0.383 with range (0.330, 0.412); mean recall
was 0.496 (0.446, 0.548) and average precision
was 0.315 (0.261, 0.380). The five most predictive
tokens were: !, yeah, guys, oh and shocked. This
represents reasonable performance (with intuitive
predictive tokens); but obviously there is quite a
bit of room for improvement.
3
We now explore empirically whether these mis-
classifications are made on the same comments for
which annotators requested context. To this end,
we introduce a variable M
i
for each comment i
such that M
i
= 1 if y?
i
6= y
i
, i.e., M
i
is an in-
3
Some of the recently proposed strategies mentioned in
Section 2 may improve performance here, but none of these
address the fundamental issue of context.
515
dicator variable that encodes whether or not the
classifier misclassified comment i. We then ran
a second regression in which the output variable
was the logit-transformed probability of the model
misclassifying comment i, i.e., P (M
i
). Here we
are interested in the correlation of the event that
one or more annotators requested additional con-
text for comment i (denoted by C
i
) and model mis-
classifications (adjusting for the comment?s true
label). Formally:
logit{P (M
i
)} = ?
0
+ ?
1
I
i
+ ?
2
C
i
(2)
Fitting this to the data, we estimated
?
?
2
= 0.971
with a 95% CI of (0.810, 1.133); p < 0.001. Put
another way, the model makes mistakes on those
comments for which annotators requested addi-
tional context (even after accounting for the an-
notator designation of comments).
6 Conclusions and Future Directions
We have described a new (publicly available) cor-
pus for the task of verbal irony detection. The
data comprises comments scraped from the so-
cial news website reddit. We recorded confidence
judgements and requests for contextualizing infor-
mation for each comment during annotation. We
analyzed this corpus to provide empirical evidence
that annotators quite often require context beyond
the comment under consideration to discern irony;
especially for those comments ultimately deemed
as being intended ironically. We demonstrated
that a standard token-based machine learning ap-
proach misclassified many of the same comments
for which annotators tend to request context.
We have shown that annotators rely on contex-
tual cues (in addition to word and grammatical fea-
tures) to discern irony and argued that this implies
computers should, too. The obvious next step is to
develop new machine learning models that exploit
the contextual information available in the corpus
we have curated (e.g., previous comments by the
same user, the thread topic).
7 Acknowledgement
This work was made possible by the Army Re-
search Office (ARO), grant #64481-MA.
References
C Burfoot and T Baldwin. 2009. Automatic satire de-
tection: are you having a laugh? In ACL-IJCNLP,
pages 161?164. ACL.
P Carvalho, L Sarmento, MJ Silva, and E de Oliveira.
2009. Clues for detecting irony in user-generated
contents: oh...!! it?s so easy;-). In CIKM workshop
on Topic-sentiment analysis for mass opinion, pages
53?56. ACM.
HH Clark and RJ Gerrig. 1984. On the pretense the-
ory of irony. Journal of Experimental Psychology,
113:121?126.
J Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
D Davidov, O Tsur, and A Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twit-
ter and amazon. pages 107?116.
E Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In LREC,
volume 12, pages 392?398.
RW Gibbs and HL Colston. 2007. Irony in language
and thought: a cognitive science reader. Lawrence
Erlbaum.
R Gonz?alez-Ib?a?nez, S Muresan, and N Wacholder.
2011. Identifying sarcasm in twitter: a closer look.
In ACL, volume 2, pages 581?586. Citeseer.
HP Grice. 1975. Logic and conversation. 1975, pages
41?58.
A Halevy, P Norvig, and F Pereira. 2009. The unrea-
sonable effectiveness of data. Intelligent Systems,
IEEE, 24(2):8?12.
S Lukin and M Walker. 2013. Really? well. ap-
parently bootstrapping improves the performance of
sarcasm and nastiness classifiers for online dialogue.
NAACL, pages 30?40.
A Reyes, P Rosso, and T Veale. 2012. A multidimen-
sional approach for detecting irony in twitter. LREC,
pages 1?30.
E Riloff, A Qadir, P Surve, LD Silva, N Gilbert, and
R Huang. 2013. Sarcasm as contrast between a pos-
itive sentiment and negative situation. In EMNLP,
pages 704?714.
D Sperber and D Wilson. 1981. Irony and the use-
mention distinction. 1981.
J Tepperman, D Traum, and S Narayanan. 2006.
?Yeah Right?: Sarcasm Recognition for Spoken Di-
alogue Systems.
O Tsur, D Davidov, and A Rappoport. 2010. ICWSM-
a great catchy name: Semi-supervised recognition
of sarcastic sentences in online product reviews. In
AAAI Conference on Weblogs and Social Media.
AJ Viera and JM Garrett. 2005. Understanding in-
terobserver agreement: the kappa statistic. Family
Medicine, 37(5):360?363.
MA Walker, JEF Tree, P Anand, R Abbott, and J King.
2012. A corpus for research on deliberation and de-
bate. In LREC, pages 812?817.
516
