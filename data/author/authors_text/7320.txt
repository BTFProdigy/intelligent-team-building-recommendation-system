Antonymy and Conceptual Vectors
Didier Schwab, Mathieu Lafourcade and Violaine Prince
LIRMM
Laboratoire d?informatique, de Robotique
et de Microe?lectronique de Montpellier
MONTPELLIER - FRANCE.
{schwab,lafourca,prince}@lirmm.fr
http://www.lirmm.fr/ ?{schwab, lafourca, prince}
Abstract
For meaning representations in NLP, we focus
our attention on thematic aspects and concep-
tual vectors. The learning strategy of concep-
tual vectors relies on a morphosyntaxic analy-
sis of human usage dictionary definitions linked
to vector propagation. This analysis currently
doesn?t take into account negation phenomena.
This work aims at studying the antonymy as-
pects of negation, in the larger goal of its inte-
gration into the thematic analysis. We present a
model based on the idea of symmetry compat-
ible with conceptual vectors. Then, we define
antonymy functions which allows the construc-
tion of an antonymous vector and the enumer-
ation of its potentially antinomic lexical items.
Finally, we introduce a measure which evaluates
how a given word is an acceptable antonym for
a term.
1 Introduction
Research in meaning representation in NLP is
an important problem still addressed through
several approaches. The NLP team at LIRMM
currently works on thematic and lexical disam-
biguation text analysis (Laf01). Therefore we
built a system, with automated learning capa-
bilities, based on conceptual vectors for mean-
ing representation. Vectors are supposed to en-
code ?ideas? associated to words or to expres-
sions. The conceptual vectors learning system
automatically defines or revises its vectors ac-
cording to the following procedure. It takes, as
an input, definitions in natural language con-
tained in electronic dictionaries for human us-
age. These definitions are then fed to a morpho-
syntactic parser that provides tagging and anal-
ysis trees. Trees are then used as an input
to a procedure that computes vectors using
tree geometry and syntactic functions. Thus,
a kernel of manually indexed terms is necessary
for bootstrapping the analysis. The transver-
sal relationships1, such as synonymy (LP01),
antonymy and hyperonymy, that are more or
less explicitly mentioned in definitions can be
used as a way to globally increase the coher-
ence of vectors. In this paper, we describe a
vectorial function of antonymy. This can help
to improve the learning system by dealing with
negation and antonym tags, as they are often
present in definition texts. The antonymy func-
tion can also help to find an opposite thema to
be used in all generative text applications: op-
posite ideas research, paraphrase (by negation
of the antonym), summary, etc.
2 Conceptual Vectors
We represent thematic aspects of textual seg-
ments (documents, paragraph, syntagms, etc)
by conceptual vectors. Vectors have been used
in information retrieval for long (SM83) and
for meaning representation by the LSI model
(DDL+90) from latent semantic analysis (LSA)
studies in psycholinguistics. In computational
linguistics, (Cha90) proposes a formalism for
the projection of the linguistic notion of se-
mantic field in a vectorial space, from which
our model is inspired. From a set of elemen-
tary concepts, it is possible to build vectors
(conceptual vectors) and to associate them to
lexical items2. The hypothesis3 that considers
a set of concepts as a generator to language
has been long described in (Rog52). Polysemic
words combine different vectors corresponding
1well known as lexical functions (MCP95)
2Lexical items are words or expressions which consti-
tute lexical entries. For instance, ?car ? or ?white ant ? are
lexical items. In the following we will (some what) use
sometimes word or term to speak about a lexical item.
3that we call thesaurus hypothesis.
to different meanings. This vector approach
is based on known mathematical properties, it
is thus possible to undertake well founded for-
mal manipulations attached to reasonable lin-
guistic interpretations. Concepts are defined
from a thesaurus (in our prototype applied to
French, we have chosen (Lar92) where 873 con-
cepts are identified). To be consistent with the
thesaurus hypothesis, we consider that this set
constitutes a generator family for the words and
their meanings. This familly is probably not
free (no proper vectorial base) and as such, any
word would project its meaning on it according
to the following principle. Let be C a finite set
of n concepts, a conceptual vector V is a linear
combinaison of elements ci of C. For a meaning
A, a vector V (A) is the description (in exten-
sion) of activations of all concepts of C. For ex-
ample, the different meanings of ?door ? could be
projected on the following concepts (the CON-
CEPT [intensity] are ordered by decreasing val-
ues): V(?door ?) = (OPENING[0.8], BARRIER[0.7],
LIMIT [0.65], PROXIMITY [0.6], EXTERIOR[0.4], IN-
TERIOR[0.39], . . .
In practice, the larger C is, the finer the mean-
ing descriptions are. In return, the computing
is less easy: for dense vectors4, the enumera-
tion of activated concepts is long and difficult
to evaluate. We prefer to select the themati-
cally closest terms, i.e., the neighbourhood. For
instance, the closest terms ordered by increas-
ing distance to ?door ? are: V(?door ?)=?portal ?,
?portiere?, ?opening?, ?gate?, ?barrier ?,. . .
2.1 Angular Distance
Let us define Sim(A,B) as one of the similar-
ity measures between two vectors A et B, of-
ten used in information retrieval (Mor99). We
can express this function as: Sim(A,B) =
cos(A?, B) = A?B?A???B? with ??? as the scalar
product. We suppose here that vector com-
ponents are positive or null. Then, we define
an angular distance DA between two vectors A
and B as DA(A,B) = arccos(Sim(A,B)). In-
tuitively, this function constitutes an evaluation
of the thematic proximity and measures the an-
gle between the two vectors. We would gener-
ally consider that, for a distance DA(A,B) ? pi4
4Dense vectors are those which have very few null
coordinates. In practice, by construction, all vectors are
dense.
(45 degrees) A and B are thematically close and
share many concepts. For DA(A,B) ? pi4 , the
thematic proximity between A and B would be
considered as loose. Around pi2 , they have no
relation. DA is a real distance function. It ver-
ifies the properties of reflexivity, symmetry and
triangular inequality. We have, for example,
the following angles(values are in radian and de-
grees).
DA(V(?tit ?), V(?tit ?))=0 (0)
DA(V(?tit ?), V(?bird ?))=0.55 (31)
DA(V(?tit ?), V(?sparrow ?))=0.35 (20)
DA(V(?tit ?), V(?train ?))=1.28 (73)
DA(V(?tit ?), V(?insect ?))=0.57 (32)
The first one has a straightforward interpreta-
tion, as a ?tit ? cannot be closer to anything else
than itself. The second and the third are not
very surprising since a ?tit ? is a kind of ?sparrow ?
which is a kind of ?bird ?. A ?tit ? has not much
in common with a ?train?, which explains a large
angle between them. One can wonder why there
is 32 degrees angle between ?tit ? and ?insect ?,
which makes them rather close. If we scruti-
nise the definition of ?tit ? from which its vector
is computed (Insectivourous passerine bird with
colorful feather.) perhaps the interpretation of
these values seems clearer. In effect, the the-
matic is by no way an ontological distance.
2.2 Conceptual Vectors Construction.
The conceptual vector construction is based on
definitions from different sources (dictionaries,
synonym lists, manual indexations, etc). Defini-
tions are parsed and the corresponding concep-
tual vector is computed. This analysis method
shapes, from existing conceptual vectors and
definitions, new vectors. It requires a bootstrap
with a kernel composed of pre-computed vec-
tors. This reduced set of initial vectors is man-
ually indexed for the most frequent or difficult
terms. It constitutes a relevant lexical items
basis on which the learning can start and rely.
One way to build an coherent learning system
is to take care of the semantic relations between
items. Then, after some fine and cyclic compu-
tation, we obtain a relevant conceptual vector
basis. At the moment of writing this article,
our system counts more than 71000 items for
French and more than 288000 vectors, in which
2000 items are concerned by antonymy. These
items are either defined through negative sen-
tences, or because antonyms are directly in the
dictionnary. Example of a negative definition:
?non-existence?: property of what does not exist.
Example of a definition stating antonym: ?love?:
antonyms: ?disgust ?, ?aversion?.
3 Definition and Characterisation of
Antonymy
We propose a definition of antonymy compat-
ible with the vectorial model used. Two lexi-
cal items are in antonymy relation if there is
a symmetry between their semantic components
relatively to an axis. For us, antonym construc-
tion depends on the type of the medium that
supports symmetry. For a term, either we can
have several kinds of antonyms if several possi-
bilities for symmetry exist, or we cannot have
an obvious one if a medium for symmetry is not
to be found. We can distinguish different sorts
of media: (i) a property that shows scalar val-
ues (hot and cold which are symmetrical values
of temperature), (ii) the true-false relevance or
application of a property (e.g. existence/non-
existence) (iii) cultural symmetry or opposition
(e.g. sun/moon).From the point of view of lex-
ical functions, if we compare synonymy and
antonymy, we can say that synonymy is the
research of resemblance with the test of sub-
stitution (x is synonym of y if x may replace
y), antonymy is the research of the symmetry,
that comes down to investigating the existence
and nature of the symmetry medium. We have
identified three types of symmetry by relying
on (Lyo77), (Pal76) and (Mue97). Each sym-
metry type characterises one particular type of
antonymy. In this paper, for the sake of clarity
and precision, we expose only the complemen-
tary antonymy. The same method is used for
the other types of antonymy, only the list of
antonymous concepts are different.
3.1 Complementary Antonymy
The complementary antonyms are couples like
event/unevent, presence/absence.
he is present ? he is not absent
he is absent ? he is not present
he is not absent ? he is present
he is not present ? he is absent
In logical terms, we would have:
?x P (x)? ?Q(x) ?x ?P (x)? Q(x)
?x Q(x)? ?P (x) ?x ?Q(x)? P (x)
This corresponds to the exclusive disjunction
relation. In this frame, the assertion of one
of the terms implies the negation of the other.
Complementary antonymy presents two kinds
of symmetry, (i) a value symmetry in a boolean
system, as in the examples above and (ii) a sym-
metry about the application of a property (black
is the absence of color, so it is ?opposed? to all
other colors or color combinaisons).
4 Antonymy Functions
4.1 Principles and Definitions.
The aim of our work is to create a function
that would improve the learning system by sim-
ulating antonymy. In the following, we will be
mainly interested in antonym generation, which
gives a good satisfaction clue for these functions.
We present a function which, for a given lex-
ical item, gives the n closest antonyms as the
neighbourhood function V provides the n clos-
est items of a vector. In order to know which
particular meaning of the word we want to op-
pose, we have to assess by what context mean-
ing has to be constrained. However, context is
not always sufficient to give a symmetry axis
for antonymy. Let us consider the item ?father ?.
In the ?family? context, it can be opposite to
?mother ? or to ?children? being therefore ambigu-
ous because ?mother ? and ?children? are by no way
similar items. It should be useful, when context
cannot be used as a symmetry axis, to refine
the context with a conceptual vector which is
considered as the referent. In our example, we
should take as referent ?filiation?, and thus the
antonym would be ?children? or the specialised
similar terms (e.g. ?sons? , ?daughters?) ?marriage?
or ?masculine? and thus the antonym would be
?mother ?.
The function AntiLexS returns the n closest
antonyms of the word A in the context defined
by C and in reference to the word R.
AntiLexS(A,C,R, n)
AntiLexR(A,C, n) = AntiLexS(A,C,C, n)
AntiLexB(A,R, n) = AntiLexS(A,R,R, n)
AntiLexA(A, n) = AntiLexS(A,A,A, n)
The partial function AntiLexR has been de-
fined to take care of the fact that in most cases,
context is enough to determine a symmetry axis.
AntiLexB is defined to determine a symmetry
axis rather than a context. In practice, we have
AntiLexB = AntiLexR. The last function is
the absolute antonymy function. For polysemic
words, its usage is delicate because only one
word defines at the same time three things: the
word we oppose, the context and the referent.
This increases the probability to get unsatis-
factory results. However, given usage habits,
we should admit that, practically, this function
will be the most used. It?s sequence process is
presented in picture 1. We note Anti(A,C) the
ITEMS
ANTONYMOUS
CONCEPTUAL VECTOR
CALCULATION
IDENTIFICATION
OF THE CLOSEST 
ITEMS
neighbourhood
CONCEPTUAL VECTORS
strong contextualisation
CALCULATION
X, C, R
X1, X2, ..., Xn
ITEMS
VAnti
VECTOR
ANTONYMOUS
OF THEanti
Vcx, Vcr
VECTORS
CORRESPONDING
OF THE
Figure 1: run of the functions AntiLex
antonymy function at the vector level. Here,
A is the vector we want to oppose and C the
context vector.
Items without antonyms: it is the case
of material objects like car, bottle, boat, etc.
The question that raises is about the continu-
ity the antonymy functions in the vector space.
When symmetry is at stake, then fixed points
or plans are always present. We consider the
case of these objects, and in general, non op-
posable terms, as belonging to the fixed space
of the symmetry. This allows to redirect the
question of antonymy to the opposable proper-
ties of the concerned object. For instance, if we
want to compute the antonym of a ?motorcycle?,
which is a ROAD TRANSPORT, its opposable prop-
erties being NOISY and FAST, we consider its cat-
egory (i.e. ROAD TRANSPORT) as a fixed point,
and we will look for a road transport (SILEN-
CIOUS and SLOW ), something like a ?bicycle? or
an ?electric car ?. With this method, thanks to
the fixed points of symmetry, opposed ?ideas?
or antonyms, not obvious to the reader, could
be discovered.
4.2 Antonym vectors of concept lists
Anti functions are context-dependent and can-
not be free of concepts organisation. They
need to identify for every concept and for ev-
ery kind of antonymy, a vector considered as
the opposite. We had to build a list of triples
?concept, context, vector?. This list is called
antonym vectors of concept list (AVC).
4.2.1 AVC construction.
The Antonym Vectors of Concepts list is manu-
ally built only for the conceptual vectors of the
generating set. For any concept we can have the
antonym vectors such as:
AntiC(EXISTENCE, V ) = V (NON-EXISTENCE)
AntiC(NON-EXISTENCE, V ) = V (EXISTENCE)
AntiC(AGITATION, V ) = V (INERTIA)? V (REST)
AntiC(PLAY, V ) = V (PLAY)
?V
AntiC(ORDER, V (order) ? V (disorder)) =
V (DISORDER)
AntiC(ORDER, V (classification) ? V (order)) =
V (CLASSIFICATION)
As items, concepts can have, according to
the context, a different opposite vector even
if they are not polysemic. For instance, DE-
STRUCTION can have for antonyms PRESERVA-
TION, CONSTRUCTION, REPARATION or PROTEC-
TION. So, we have defined for each concept, one
conceptual vector which allows the selection of
the best antonym according to the situation.
For example, the concept EXISTENCE has the
vector NON-EXISTENCE for antonym for any con-
text. The concept DISORDER has the vector of
ORDER for antonym in a context constituted by
the vectors of ORDER ?DISORDER5 and has CLAS-
SIFICATION in a context constituted by CLASSI-
FICATION and ORDER.
The function AntiC(Ci, Vcontext) returns for
a given concept Ci and the context defined by
Vcontext , the complementary antonym vector in
the list.
4.3 Construction of the antonym
vector: the Anti Function
4.3.1 Definitions
We define the relative antonymy function
AntiR(A,C) which returns the opposite vec-
tor of A in the context C and the absolute
antonymy function AntiA(A) = AntiR(A,A).
The usage of AntiA is delicate because the lexi-
cal item is considered as being its own context.
We will see in 4.4.1 that this may cause real
problems because of sense selection. We should
stress now on the construction of the antonym
vector from two conceptual vectors: Vitem, for
5? is the normalised sum V = A?B | vi = xi+yi?V ?
the item we want to oppose and the other, Vc,
for the context (referent).
4.3.2 Construction of the Antonym
Vector
The method is to focus on the salient notions in
Vitem and Vc. If these notions can be opposed
then the antonym should have the inverse ideas
in the same proportion. That leads us to define
this function as follows:
AntiR(Vitem, Vc) =
?N
i=1 Pi ?AntiC(Ci, Vc)
with Pi = V 1+CV (Vitem)itemi ?max(Vitemi , Vci)
We crafted the definition of the weight P after
several experiments. We noticed that the func-
tion couldn?t be symmetric (we cannot reason-
ably have AntiR(V(?hot ?),V(?temperature?)) =
AntiR(V(?temperature?),V(?hot ?))). That is why
we introduce this power, to stress more on the
ideas present in the vector we want to oppose.
We note also that the more conceptual6 the vec-
tor is, the more important this power should be.
That is why the power is the variation coeffi-
cient7 which is a good clue for ?conceptuality?.
To finish, we introduce this function max be-
cause an idea presents in the item, even if this
idea is not present in the referent, has to be op-
posed in the antonym. For example, if we want
the antonym of ?cold ? in the ?temperature? con-
text, the weight of ?cold ? has to be important
even if it is not present in ?temperature?.
4.4 Lexical Items and Vectors:
Problem and Solutions
The goal of the functions AntiLex is to return
antonym of a lexical item. They are defined
with the Anti function. So, we have to use tools
which allow the passage between lexical items
and vectors. This transition is difficult because
of polysemy, i.e. how to choose the right relation
between an item and a vector. In other words,
how to choose the good meaning of the word.
4.4.1 Transition lexical items ?
Conceptual Vectors
As said before, antonymy is relative to a con-
text. In some cases, this context cannot be suf-
ficient to select a symmetry axis for antonymy.
6In this paragraph, conceptual means: closeness of a
vector to a concept
7The variation coefficient is SD(V )?(V ) with SD as the
standart deviation and ? as the arithmetic mean.
To catch the searched meaning of the item and,
if it is different from the context, to catch the
selection of the meaning of the referent, we use
the strong contextualisation method. It com-
putes, for a given item, a vector. In this vector,
some meanings are favoured against others ac-
cording to the context. Like this, the context
vector is also contextualised.
This contextualisation shows the problem
caused by the absolute antonymy function
Anti?R . In this case, the method will compute
the vector of the word item in the context item.
This is not a problem if item has only one defini-
tion because, in this case, the strong contextu-
alisation has no effect. Otherwise, the returned
conceptual vector will stress on the main idea it
contains which one is not necessary the appro-
priate one.
4.4.2 Transition Conceptual Vectors ?
Lexical Items
This transition is easier. We just have to com-
pute the neighbourhood of the antonym vector
Vant to obtain the items which are in thematic
antonymy with Vitem. With this method, we
have, for instance:
V(AnticR(death, ?death ? & ?life?))=(LIFE 0.4)
(?killer ? 0.449) (?murderer ? 0.467) (?blood sucker ?
0.471) (?strige? 0.471) (?to die? 0.484) (?to live? 0.486)
V(AnticR(life, ?death ? & ?life?))=(?death ? 0.336)
(DEATH 0.357) (?murdered ? 0.367) (?killer ? 0.377)
(C3:AGE OF LIFE 0.481) (?tyrannicide? 0.516) (?to kill ?
0.579) (?dead ? 0.582)
V(AntiCcA(LIFE))=(DEATH 0.034) (?death ? 0.427)
(C3:AGE OF LIFE 0.551) (?killer ? 0.568) (?mudered ?
0.588) (?tyrannicide? 0.699) (C2:HUMAN 0.737) (?to
kill ? 0.748) (?dead ? 0.77)
It is not important to contextualise the con-
cept LIFE because we can consider that, for ev-
ery context, the opposite vector is the same.
In complementary antonymy, the closest item
is DEATH. This result looks satisfactory. We can
see that the distance between the antonymy vec-
tor and DEATH is not null. It is because our
method is not and cannot be an exact method.
The goal of our function is to build the best
(closest) antonymy vector it is possible to have.
The construction of the generative vectors is the
second explanation. Generative vectors are in-
terdependent. Their construction is based on an
ontology. To take care of this fact, we don?t have
boolean vectors, with which, we would have ex-
actly the same vector. The more polysemic the
term is, the farthest the closest item is, as we
can see it in the first two examples.
We cannot consider, even if the potential of
antonymy measure is correct, the closest lexical
item from Vanti as the antonym. We have to
consider morphological features. Simply speak-
ing, if the antonym of a verb is wanted, the re-
sult would be better if a verb is caught.
4.5 Antonymy Evaluation Measure
Besides computing an antonym vector, it seems
relevant to assess wether two lexical items can
be antonyms. To give an answer to this ques-
tion, we have created a measure of antonymy
evaluation. Let A and B be two vectors.
The question is precisely to know if they can
reasonably be antonyms in the context of C.
The antonymy measure MantiEval is the an-
gle between the sum of A and B and the sum
of AnticR(A,C) and AnticR(B,C). Thus, we
have:
MantiEval = DA(A?B,AntiR(A,C)?AntiR(B,C))
A+B
A
B
Anti(A,C)
Anti(B,C)
Anti(A,C)+Anti(B,C)
Figure 2: 2D geometric representation of the antonymy
evaluation measure MantiEval
The antonymy measure is a pseudo-distance.
It verifies the properties of reflexivity, symme-
try and triangular inequality only for the subset
of items which doesn?t accept antonyms. In this
case, notwithstanding the noise level, the mea-
sure is equal to the angular distance. In the
general case, it doesn?t verify reflexivity. The
conceptual vector components are positive and
we have the property: Distanti ? [0, pi2 ]. The
smaller the measure, the more ?antonyms? the
two lexical items are. However, it would be a
mistake to consider that two synonyms would be
at a distance of about pi2 . Two lexical items atpi
2 have not much in common8. We would rather
see here the illustration that two antonyms
share some ideas, specifically those which are
not opposable or those which are opposable with
a strong activation. Only specific activated con-
cepts would participate in the opposition. A
distance of pi2 between two items should rather
be interpreted as these two items do not share
much idea, a kind of anti-synonymy. This re-
sult confirms the fact that antonymy is not the
exact inverse of synonymy but looks more like a
?negative synonymy? where items remains quite
related. To sum up, the antonym of w is not
a word that doesn?t share ideas with w, but a
word that opposes some features of w.
4.5.1 Examples
In the following examples, the context has been
ommited for clarity sake. In these cases, the
context is the sum of the vectors of the two
items.
MantiEval(EXISTENCE,NON-EXISTENCE) = 0.03
MantiEvalC(?existence?, ?non-existence?) = 0.44
MantiEvalC(EXISTENCE, CAR) = 1.45
MantiEvalC(?existence?, ?car ?) = 1.06
MantiEvalC(CAR, CAR) = 0.006
MantiEvalC(?car ?, ?car ?) = 0.407
The above examples confirm what pre-
sented. Concepts EXISTENCE and NON-
EXISTENCE are very strong antonyms in comple-
mentary antonymy. The effects of the polysemy
may explain that the lexical items ?existence? and
?non-existence? are less antonyms than their re-
lated concepts. In complementary antonymy,
CAR is its own antonym. The antonymy mea-
sure between CAR and EXISTENCE is an exam-
ple of our previous remark about vectors shar-
ing few ideas and that around pi/2 this mea-
sure is close to the angular distance (we have
DA(existence, car) = 1.464.). We could con-
sider of using this function to look in a concep-
tual lexicon for the best antonyms. However,
the computation cost (around a minute on a P4
at 1.3 GHz) would be prohibitive.
8This case is mostly theorical, as there is no language
where two lexical items are without any possible relation.
5 Action on learning and method
evaluation
The function is now used in the learning process.
We can use the evaluation measure to show the
increase of coherence between terms:
MantiEvalC new old
?existence?, ?non-existence? 0.33 0.44
?existence?, ?car ? 1.1 1.06
?car ?, ?car ? 0.3 0, 407
There is no change in concepts because they are
not learned. In the opposite, the antonymy eval-
uation measure is better on items. The exemple
shows that ?existence? and ?non-existence? have
been largely modified. Now, the two items are
stronger antonyms than before and the vector
basis is more coherent. Of course, we can test
these results on the 71000 lexical items which
have been modified more or less directly by the
antonymy function. We have run the test on
about 10% of the concerned items and found an
improvement of the angular distance through
MantiEvalC ranking to 0.1 radian.
6 Conclusion
This paper has presented a model of antonymy
using the formalism of conceptual vectors. Our
aim was to be able: (1) to spot antonymy if
it was not given in definition and thus provide
an antonym as a result, (2) to use antonyms
(discovered or given) to control or to ensure the
coherence of an item vector, build by learning,
which could be corrupted. In NLP, antonymy is
a pivotal aspect, its major applications are the-
matic analysis of texts, construction of large lex-
ical databases and word sense disambiguation.
We grounded our research on a computable lin-
guisitic theory being tractable with vectors for
computational sake. This preliminary work on
antonymy has also been conducted under the
spotlight of symmetry, and allowed us to express
antonymy in terms of conceptual vectors. These
functions allow, from a vector and some contex-
tual information, to compute an antonym vec-
tor. Some extensions have also been proposed so
that these functions may be defined and usable
from lexical items. A measure has been identi-
fied to assess the level of antonymy between two
items. The antonym vector construction is nec-
essary for the selection of opposed lexical items
in text generation. It also determines opposite
ideas in some negation cases in analysis.
Many improvements are still possible, the
first of them being revision of the VAC lists.
These lists have been manually constructed by
a reduced group of persons and should widely be
validated and expanded especially by linguists.
We are currently working on possible improve-
ments of results through learning on a corpora.
References
Jacques Chauche?. De?termination se?mantique
en analyse structurelle : une expe?rience base?e
sur une de?finition de distance. TAL Informa-
tion, 1990.
Scott C. Deerwester, Susan T. Dumais,
Thomas K. Landauer, George W. Furnas, and
Richard A. Harshman. Indexing by latent se-
mantic analysis. Journal of the American So-
ciety of Information Science, 41(6):391?407,
1990.
Mathieu Lafourcade. Lexical sorting and lexical
transfer by conceptual vectors. In Proceeding
of the First International Workshop on Mul-
tiMedia Annotation, Tokyo, January 2001.
Larousse. The?saurus Larousse - des ide?es aux
mots, des mots aux ide?es. Larousse, 1992.
Mathieu Lafourcade and Violaine Prince. Syn-
onymies et vecteurs conceptuels. In actes de
TALN?2001, Tours, France, July 2001.
John Lyons. Semantics. Cambridge University
Press, 1977.
Igor Mel?c?uk, Andre? Clas, and Alain Polgue`re.
Introduction a` la lexicologie explicative et
combinatoire. Duculot, 1995.
Emmanuel Morin. Extraction de liens
se?mantiques entre termes a` partir de
corpus techniques. PhD thesis, Universite? de
Nantes, 1999.
Victoria Lynn Muehleisen. Antonymy and se-
mantic range in english. PhD thesis, North-
western university, 1997.
F.R. Palmer. Semantics : a new introduction.
Cambridge University Press, 1976.
P. Roget. Roget?s Thesaurus of English Words
and Phrases. Longman, London, 1852.
Gerard Salton and Michael McGill. Introduc-
tion to Modern Information Retrieval. Mc-
GrawHill, 1983.
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 232?240, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
GETALP: Propagation of a Lesk Measure through an Ant Colony Algorithm
Didier Schwab, Andon Tchechmedjiev, J?r?me Goulian,
Mohammad Nasiruddin, Gilles S?rasset, Herv? Blanchon
LIG-GETALP
Univ. Grenoble Alpes
http://getalp.imag.fr/WSD
firstname.lastname@imag.fr
Abstract
This article presents the GETALP system for
the participation to SemEval-2013 Task 12,
based on an adaptation of the Lesk measure
propagated through an Ant Colony Algorithm,
that yielded good results on the corpus of Se-
meval 2007 Task 7 (WordNet 2.1) as well as
the trial data for Task 12 SemEval 2013 (Ba-
belNet 1.0). We approach the parameter es-
timation to our algorithm from two perspec-
tives: edogenous estimation where we max-
imised the sum the local Lesk scores; exoge-
nous estimation where we maximised the F1
score on trial data. We proposed three runs
of out system, exogenous estimation with Ba-
belNet 1.1.1 synset id annotations, endoge-
nous estimation with BabelNet 1.1.1 synset id
annotations and endogenous estimation with
WordNet 3.1 sense keys. A bug in our imple-
mentation led to incorrect results and here, we
present an amended version thereof. Our sys-
tem arrived third on this task and a more fine
grained analysis of our results reveals that the
algorithms performs best on general domain
texts with as little named entities as possible.
The presence of many named entities leads the
performance of the system to plummet greatly.
1 Introduction
Out team is mainly interested in Word Sense Disam-
biguation (WSD) based on semantic similarity mea-
sures. This approach to WSD is based on a local
algorithm and a global algorithm. The local algo-
rithm corresponds to a semantic similarity measure
(for example (Wu and Palmer, 1994), (Resnik, 1995)
or (Lesk, 1986)), while the global algorithm propa-
gates the values resulting from these measures at the
level of a text, in order to disambiguate the words
that compose it. For two years, now, our team has
focussed on researching global algorithms. The lo-
cal algorithm we use, a variant of the Lesk algo-
rithm that we have evaluated with several global al-
gorithms (Simulated Annealing (SA), Genetic Al-
gorithms (GA) and Ant Colony Algorithms (ACA))
(Schwab et al, 2012; Schwab et al, 2013), has
shown its robustness with WordNet 3.0. For the
present campaign, we chose to work with an ant
colony based global algorithms that has proven its
efficiency (Schwab et al, 2012; Tchechmedjiev et
al., 2012).
Presently, for this SemEval 2013 Task 12 (Nav-
igli et al, 2013), the objective is to disambiguate a
set of target words (nouns) in a corpus of 13 texts
in 5 Languages (English, French, German, Italian,
Spanish) by providing, for each sense the appropri-
ate sense labels. The evaluation of the answers is
performed by comparing them to a gold standard
annotation of the corpus in all 5 languages using
three possible sense inventories and thus sense tags:
BabelNet 1.1.1 Synset ids (Navigli and Pozetto,
2012), Wikipedia page names and Wordnet sense
keys (Miller, 1995).
Our ant colony algorithm is a stochastic algorithm
that has several parameters that need to be selected
and tuned. Choosing the values of the parameters
based on linguistic criteria remains an open and dif-
ficult problem, which is why we wanted to autom-
atize the parameter search process. There are two
ways to go about this process: exogenous estima-
232
tion, when the parameter values are selected so as
to maximise the F-score on a small training anno-
tated corpus and then used to disambiguate another
corpus (weakly supervised); endogenous estimation,
when the parameters are chosen so as to maximise
the global similarity score on a text or corpus (unsu-
pervised). Our first experiment and system run con-
sists in tuning the parameters on the trial corpus of
the campaign and running the system with the Ba-
belNet sense inventory. Our second and third exper-
iments consist in endogenous parameter estimation,
the first using BabelNet as a sense inventory and the
second using WordNet. Unfortunately, the presence
of an implementation issue prevented us from ob-
taining scores up to par with the potential of our sys-
tem and thus we will present indicative results of the
performance of the system after the implementation
issue was fixed.
2 The GETALP System: Propagation of a
Lesk Measure through an Ant Colony
Algorithm
In this section we will first describe the local al-
gorithm we used, followed by a quick overview of
global algorithms and our own Ant Colony Algo-
rithm.
2.1 The Local Algorithm: a Lesk Measure
Our local algorithm is a variant of the Lesk Algo-
rithm (Lesk, 1986). Proposed more than 25 years
ago, it is simple, only requires a dictionary and no
training. The score given to a sense pair is the num-
ber of common words (space separated strings) in
the definition of the senses, without taking into ac-
count neither the word order in the definitions (bag-
of-words approach), nor any syntactic or morpho-
logical information. Variants of this algorithm are
still today among the best on English-language texts
(Ponzetto and Navigli, 2010).
Our local algorithm exploits the links provided by
WordNet: it considers not only the definition of a
sense but also the definitions of the linked senses
(using all the semantic relations for WordNet, most
of them for BabelNet) following (Banerjee and Ped-
ersen, 2002), henceforth referred asExtLesk1 Con-
1All dictionaries and Java implementations of all algorithms
of our team can be found on our WSD page
trarily to Banerjee, however, we do not consider
the sum of squared sub-string overlaps, but merely
a bag-of-words overlap that allows us to generate
a dictionary from WordNet, where each word con-
tained in any of the word sense definitions is indexed
by a unique integer and where each resulting defini-
tion is sorted. Thus we are able to lower the compu-
tational complexity fromO(mn) toO(m), wherem
and n are the respective length of two definitions and
m ? n. For example for the definition: "Some kind
of evergreen tree", if we say that Some is indexed by
123, kind by 14, evergreen by 34, and tree by 90,
then the indexed representation is {14, 34, 90, 123}.
2.2 Global Algorithm : Ant Colony Algorithm
We will first review the principles pertaining to
global algorithms and then a more detailed account
of our Ant Colony algorithm.
2.2.1 Global algorithms, Global scores and
Configurations
A global algorithm is a method that allows to
propagate a local measure to a whole text in or-
der to assign a sense label to each word. In the
similarity-based WSD perspective, the algorithms
require some fitness measure to evaluate how good
a configuration is. With this in mind, the score
of the selected sense of a word can be expressed
as the sum of the local scores between that sense
and the selected senses of all the other words of a
context. Hence, in order to obtain a fitness value
(global score) for the whole configuration, it is
possible to simply sum the scores for all selected
senses of the words of the context: Score(C) =
?m
i=1
?m
j=iExtLesk(wi,C[i], wj,C[j]).
For a given text, the chosen configuration is
the one which maximizes the global score among
the evaluated ones. The simplest approach is the
exhaustive evaluation of sense combinations (BF),
used for example in (Banerjee and Pedersen, 2002),
that assigns a score to each word sense combination
in a given context (window or whole text) and se-
lects the one with the highest score. The main is-
sue with this approach is that it leads to a combi-
http://getalp.imag.fr/WSD and more specifically for
SemEval 2013 Task 12 on the following page
http://getalp.imag.fr/static/wsd/
GETALP-WSD-ACA/
233
natorial explosion in the length of the context win-
dow or text. The number of combinations is indeed
?|T |
i=1(|s(wi)|), where s(wi) is the set of possible
senses of word i of a text T . For this reason it is
very difficult to use the BF approach on an analy-
sis window larger than a few words. In our work,
we consider the whole text as context. In this per-
spective, we studied several methods to overcome
the combinatorial explosion problem.
2.2.2 Complete and Incomplete Approaches
Several approximation methods can be used in or-
der to overcome the combinatorial explosion issue.
On the one hand, complete approaches try to reduce
dimensionality using pruning techniques and sense
selection heuristics. Some examples include: (Hirst
and St-Onge, 1998), based on lexical chains that re-
strict the possible sense combinations by imposing
constraints on the succession of relations in a taxon-
omy (e.g. WordNet); or (Gelbukh et al, 2005) that
review general pruning techniques for Lesk-based
algorithms; or yet (Brody and Lapata, 2008) who
exploit distributional similarity measures extracted
from corpora (information content).
On the other hand, incomplete approaches gen-
erally use stochastic sampling techniques to reach a
local maximum by exploring as little as necessary
of the search space. Our present work focuses on
such approaches. Furthermore, we can distinguish
two possible variants:
? local neighbourhood-based approaches (new
configurations are created from existing con-
figurations) among which are some approaches
from artificial intelligence such as genetic al-
gorithms or optimization methods such as sim-
ulated annealing;
? constructive approaches (new configurations
are generated by iteratively adding new ele-
ments of solutions to the configuration under
construction), among which are for example
ant colony algorithms.
2.2.3 Principle of our Ant Colony Algorithm
In this section, we briefly describe out Ant Colony
Algorithm so as to give a general idea of how it op-
erates. However, readers are strongly encouraged
to read the detailed papers (Schwab et al, 2012;
Schwab et al, 2013) for a more detailed description
of the system, including examples of how the graph
is built, of how the algorithm operates step by step
as well all pseudo code listing.
Ant colony algorithms (ACA) are inspired from
nature through observations of ant social behavior.
Indeed, these insects have the ability to collectively
find the shortest path between their nest and a source
of food (energy). It has been demonstrated that
cooperation inside an ant colony is self-organised
and allows the colony to solve complex problems.
The environment is usually represented by a graph,
in which virtual ants exploit pheromone trails de-
posited by others, or pseudo-randomly explore the
graph. ACAs are a good alternative for the resolu-
tion of optimization problems that can be encoded
as graphs and allow for a fast and efficient explo-
ration on par with other search heuristics. The main
advantage of ACAs lies in their high adaptivity to
dynamically changing environments. Readers can
refer to (Dorigo and St?tzle, 2004) or (Monmarch?,
2010) for a state of the art.
In this article we use a simple hierarchical graph
(text, sentence, word) that matches the structure of
the text and that exploits no external linguistic infor-
mation. In this graph we distinguish two types of
nodes: nests and plain nodes. Following (Schwab et
al., 2012), each possible word sense is associated to
a nest. Nests produce ants that move in the graph in
order to find energy and bring it back to their mother
nest: the more energy is brought back by ants, the
more ants can be produced by the nest in turn. Ants
carry an odour (a vector) that contains the words of
the definition of the sense of its mother nest. From
the point of view of an ant, a node can be: (1) its
mother nest, where it was born; (2) an enemy nest
that corresponds to another sense of the same word;
(3) a potential friend nest: any other nest; (4) a plain
node: any node that is not a nest. Furthermore, to
each plain node is also associated an odour vector of
a fixed length that is initially empty.
Ant movement is function of the scores given by
the local algorithm, of the presence of energy, of the
passage of other ants (when passing on an edge ants
leave a pheromone trail that evaporates over time)
and of the nodes? odour vectors (ants deposit a part
of their odour on the nodes they go through). When
an ant arrives onto the nest of another word (that cor-
responds to a sense thereof), it can either continue its
234
exploration or, depending on the score between this
nest and its mother nest, decide to build a bridge be-
tween them and to follow it home. Bridges behave
like normal edges except that if at any given time the
concentration of pheromone reaches 0, the bridge
collapses. Depending on the lexical information
present and the structure of the graph, ants will fa-
vor following bridges between more closely related
senses. Thus, the more closely related the senses of
the nests are, the more bridges between them will
contribute to their mutual reinforcement and to the
sharing of resources between them (thus forming
meta-nests); while the bridges between more dis-
tant senses will tend to fade away. We are thus able
to build interpretative paths (possible interpretations
of the text) through emergent behaviour and to sup-
press the need to use a complete graph that includes
all the links between the senses from the start (as is
usually the case with classical graph-based optimi-
sation approaches).
Through the emergence of interpretative paths,
sense pairs that are closer semantically benefit from
an increased ant traffic and thus tend to capture most
of the energy of the system at a faster pace, thus
favouring a faster convergence over an algorithm
that uses a local neighbourhood graph (nodes are
senses interconnected so as to represent all sense
combinations in a context window) without sacrific-
ing the quality of the results.
The selected answers correspond, for each word
to the nest node with the highest energy value. The
reason for this choice over using the pheromone con-
centration is that empirically, the energy level bet-
ter correlates with the actual F1 scores. In turn, the
global Lesk score of a selected sense combination
correlates even better with the F1 score, which is
why, we keep the sense combinations resulting from
each iteration of the algorithm (highest energy nests
at each iteration) and select the one with the highest
global Lesk score as the final solution.
2.3 Parameters
This version of our ant algorithm has seven param-
eters (?, Ea, Emax, E0, ?v, ?, LV ) which have an
influence on the emergent phenomena in the system:
? The maximum amount of energy an ant can
carry, Emax and Ea the amount of energy an
ant can take on a node, influences how much
an ant explores the environment. Ants cannot
go back through an edge they just crossed and
have to make circuits to come back to their nest
(if the ant does not die before that). The size
of the circuits depend on the moment the ants
switch to return mode, hence on Emax.
? The evaporation rate of the pheromone between
cycles (?) is one of the memories of the sys-
tem. The higher the rate is, the least the trails
from previous ants are given importance and
the faster interpretative paths have to be con-
firmed (passed on) by new ants in order not to
be forgotten by the system.
? The initial amount of energy per node (E0)
and the ant life-span (?) influence the number
of ants that can be produced and therefore the
probability of reinforcing less likely paths.
? The odour vector length (Lv) and the propor-
tion of odour components deposited by an ant
on a plain node (?V ) are two dependent param-
eters that influence the global system memory.
The higher the length of the vector, the longer
the memory of the passage of an ant is kept. On
the other hand, the proportion of odour compo-
nents deposited has the opposite effect.
Given the lack of an analytical way of determin-
ing the optimal parameters of the ant colony al-
gorithm, they have to be estimated experimentally,
which is detailed in the following section.
3 Acquisition of Parameter Values
The algorithms we are interested in have a certain
number of parameters that need tuning in order to
obtain the best possible score on the evaluation cor-
pus. There are three possible approaches:
? Make an educated guess about the value ranges
based on a priori knowledge about the dynam-
ics of the algorithm;
? Test manually (or semi-manually) several com-
binations of parameters that appear promising
and determine the influence of making small
adjustments to the values ;
? Use a learning algorithm to automate acquisi-
tion of parameters values. We present that ap-
proach in the following part.
235
3.1 Automated Parameter Estimation
Two methods can be used to automatically acquire
parameters. The first one consists in maximizing
the F-score on an sense-annotated corpus (weak ap-
proach) while the second one consist in maximizing
the global Lesk score (unsupervised approach).
3.1.1 Generalities
Both approaches are based on the same principle
(Tchechmedjiev et al, 2012). We use a simulated
annealing algorithm (Laarhoven and Aarts, 1987)
combined with a non-parametric statistical (Mann-
Whitney-U test (Mann and Whitney, 1947)) test with
a p-value adapted for multiple comparisons through
False Discovery Rate control (FDR) (Benjamini and
Hochberg, 1995). The estimation algorithm oper-
ates on all the parameters of the ant colony algo-
rithm described above and attempts to maximise the
objective function (Global score, F1). The reason
why we need to use a statistical test and FDR rather
than using the standard SA algorithm, is that the
Ant Colony Algorithm is stochastic in nature and
requires tuning to be performed over the distribu-
tion of possible answers for a given set of param-
eter values. Indeed, there is no guarantee that the
value resulting from one execution is representative
at all of the distribution. The exact nature of the dis-
tribution of answers is unknown and thus we take
a sampling of the distribution as precise as can be
afforded. Thus, we require the statistical test to as-
certain the significance between the scores for two
parameter configurations.
3.1.2 Exogenous parameter tuning
If we have a sense-annotated corpus at our dis-
posal, it is possible to directly use the F1 value ob-
tained by the system on this reference to tune the
parameters of the systems so as to maximise said F1
score. The main issues that arise from such meth-
ods are the fact that gold standards are expensive to
produce and that there is no guarantee on the gen-
erality of the contents of the gold standard. Thus,
in languages with little resources we may be un-
able to obtain a gold standard and in the case one
is available, there is a potentially strong risk of over
fitting. Furthermore due to the nature of the train-
ing, taking training samples in a random order for
cross-validation becomes tricky. This is why we also
want to test another method that can tune the pa-
rameters without using labelled examples. For the
evaluation, we estimated parameters on the F1 score
on the test corpus for English and French (the only
ones available). We used the parameters estimated
for English for our English results for our first sys-
tem run GETALP-BN1 and the French parameters
for the results on French, German, Italian, Spanish.
For English we found: ? = 26, Ea =
14, Emax = 3, E0 = 34, ?v = 0.9775, ? =
0.3577, LV = 25.
For French: ? = 19, Ea = 9, Emax = 3, E0 =
32, ?v = 0.9775, ? = 0.3577, LV = 25.
3.1.3 Endogenous parameter tuning
In the context of the evaluation campaign, the ab-
sence of an example gold standard on the same ver-
sion of the resource (synset id mismatch between
BabelNet 1.0 and 1.1.1 2) made dubious the prospect
of using parameters estimated from a gold standard.
Consequently, we set out to investigate the relation
between the F1 score of the gold standard and the
Global Lesk Score of successive solutions through-
out the execution of the algorithm.
We observed that the Lesk score is highly corre-
lated to the F1 score and can be used as an estimator
thereof. The main quality criterion being the dis-
criminativeness of the Lesk score compared to the
F1 score (average ratio between the number of pos-
sible F1 score values for a single Lesk score value),
for which the correlation is a possible indicator. We
make the hypothesis based on the correlation that for
a given specific local measure, the global score will
be an adequate estimator of the F1 score. Our sec-
ond system run GETALP-WSD-BN2 is based on the
endogenous parameter estimation. We will not list
all the parameters here, as there is a different set of
parameters for each text and each language.
3.2 Voting
In previous experiment, as can be expected, we have
observed a consistent rise the F1 score when apply-
ing a majority vote method on the output of several
executions (Schwab et al, 2012). Consequently we
followed the same process here, and for all the runs
of our system we performed 100 executions and ap-
plied a majority vote (For each word, our of all se-
2http://lcl.uniroma1.it/babelnet/
236
lected senses, take the one that has been selected the
most over all the executions) on all 100 answer files.
The result of this process is a single answer file and
comes with the advantage of greatly reducing the
variability of the answers. Say this voting process
is repeated over and over again 100 times, then the
standard deviation of F1 scores around the mean is
much smaller. Thus, we also have a good solutions
to the problem of selecting the answer that yields the
highest score, without actually having access to the
gold standard.
4 Runs for SemEval 2013 task 12
In this section we will describe the various runs we
performed in the context of Task 12. We will first
present our methodologies relating to the BabelNet
tagged gold standard followed by the methodologies
relating to the WordNet tagged gold standard.
4.1 BabelNet Gold Standard Evaluation
In the context of the BabelNet gold standard evalu-
ation, we need to tag the words of the corpus with
BabelNet synset ids. Due to the slow speed of re-
trieving Babel synsets and extracting glosses, espe-
cially in the context of our extended Lesk Approach,
we pre-generate a dictionary for each language that
contains entries for each word of the corpus and then
for each possible sense (as per BabelNet). In the
short time allotted for the competition, we restrict
ourselves to building dictionaries only for the words
of the corpus, but the process described can be ap-
plied to pre-generate a dictionary for the whole of
BabelNet.
Each BabelNet synset for a word is considered as
a possible sense in the dictionary. For each synset
we retrieve the Babel senses and retain the ones that
are in the appropriate language. Then, we retrieve
the Glosses corresponding to each selected sense
and combine them in as the definition correspond-
ing to that particular BabelNet synset. Furthermore,
we also retrieve certain of the related synsets and
repeat the same process so as to add the related def-
initions to the BabelNet synset being considered. In
our experiments on the test corpus, we determined
that what worked best (i.e. English and French)
was to use only relations coming from WordNet, all
the while excluding the r, gdis, gmono relation
added by BabelNet. We observed a similar increase
in disambiguation quality with the Degree (Navigli
and Lapata, 2010) algorithm implementation that
comes with BabelNet. The r relation correspond to
the relations in BabelNet extracted from Wikipedia,
whereas gdis and gmono corresponds to relation
created using a disambiguation algorithm (respec-
tively for monosemous and polysemous words).
4.2 WordNet Gold Standard Evaluation
In the context of the WordNet gold standard evalua-
tion, we initially thought the purpose would be to an-
notate the corpus in all five languages with WordNet
sense keys through alignments extracted from Ba-
belNet. As a consequence, we exploited BabelNet
as a resource, merely obtaining WordNet sense keys
through the main senses expressed in BabelNet, that
correspond to WordNet synsets. Although we were
able to produce annotations for all languages, as it
turns out, the WordNet evaluation was merely aimed
at evaluating monolingual systems that do not sup-
port BabelNet at all. For reference, we subsequently
generated a dictionary from WordNet only, to gauge
the performance of our system on the evaluation as
intended by the organisers.
5 Results
We will first present the general results pertaining
to Task 12, followed by a more detailed analysis on
a text by text basis, as well as the comparison with
results obtained on the Semeval 2007 WSD task in
terms of specific parts of speech.
5.1 General Results for Semeval-2013 Task 12
Important: implementation issue during the
evaluation period During the evaluation period,
we had an implementation issue, where a parameter
that limited the size of definition was not disabled
properly. As a consequence, when we experimented
to determine the appropriate relations to consider
for the context expansion of the glosses, we arrived
at the experimental conclusion that using all rela-
tions worked best. However, since it was already the
case with WordNet (Schwab et al, 2011), we read-
ily accepted that our experimental conclusion was
indeed correct. The issue was indirectly resolved
as an unforeseen side effect of another hot-fix ap-
plied shortly before the start of the evaluation period.
237
Given that we were not aware of the presence of a
limitation on the definition length before the hot-fix,
we performed all the experiments under an incorrect
hypothesis which led us to an incorrect conclusion,
that itself led to the results we obtained for the cam-
paign. Indeed, with no restrictions on the size of the
definition, our official results for this task were con-
sistently inferior to the random baseline across the
board. After a thorough analysis of our runs we ob-
served that the sum of local measures (global lesk
score) that correlated inversely with the gold stan-
dard F1 score, the opposite of what it should have
been. We immediately located and corrected this
bug when we realized what had caused these bad
results that did not correspond at all with what we
obtained on the test corpus. After the fix, we strictly
ran the same experiment without exploiting the gold
standard, so as to obtain the results we would have
obtained had the bug not been present in the first
place.
Run Lang. P R F1 MFS
BN1 EN 58.3 58.3 58.3 65.6
FR 48.3 48.2 48.3 50.1
DE 52.3 52.3 52.3 68.6
ES 57.6 57.6 57.6 64.4
IT 52.6 52.5 52.6 57.2
BN2 EN 56.8 56.8 56.8 65.6
FR 48.3 48.2 48.3 50.1
DE 51.9 51.9 51.9 68.6
ES 57.8 57.8 57.8 64.4
IT 52.8 52.8 52.8 57.2
WN1 EN 51.4 51.4 51.4 63.0
Table 1: Results after fixing the implementation is-
sue for all three of our runs, compared to the Most
Frequent Sense baseline (MFS).
We can see in Table 1, that after the removal of
the implementation issues, the scores become more
competitive and meaningful compared to the other
system, although we remain third of the evalua-
tion campaign. We can observe that there is no
large difference between the exogenous results (us-
ing a small annotated corpus) and endogenous re-
sults. Except for the English corpus where there is
a 2% increase. The endogenous estimation, since it
is performed on a text by text basis is much slower
and resource consuming. Given that the exogenous
estimation offers slightly better results and that it re-
quires very little annotated data, we can conclude
that in most cases the exogenous estimation will be
much faster to obtain.
5.2 A more detailed analysis
In this section we will first make a more detailed
analysis for each text on the English corpus, by look-
ing where our algorithm performed best. We restrict
ourselves on one language for this analysis for the
sake of brevity. As we can see in Table 2, the re-
sults can vary greatly depending on the text (within
a twofold range). The system consistently performs
better on texts from the general domain (T 4, 6, 10),
often beating the first sense baseline. For more spe-
cialized texts, however, (T 2, 7, 8, 11, 12, 13) the
algorithm performs notably lower than the baseline.
The one instance where the algorithm truly fails, is
when the text in question contains many ambigu-
ous entities. Indeed for text 7, which is about foot-
ball, many of the instance words to disambiguate are
the names of players and of clubs. Intuitively, this
behaviour is understandable and can be mainly at-
tributed to the local Lesk algorithm. Since we use
glosses from the resource, that mostly remain in the
general domain, a better performance in matching
texts is likely. As for named entities, the Lesk algo-
rithm is mainly meant to capture the similarity be-
tween concepts and it is much more difficult to dif-
ferentiate two football players from a definition over
concepts (often more general).
To further outline the strength of our approach, we
need to look back further at a setting with all parts
of speech being considered, namely Task 7 from Se-
mEval 2007. As can be seen in Table 3, even though
for adjectives and adverbs the system is slightly be-
low the MFS (respectively), it has a good perfor-
mance compared to graph based WSD approaches
that would be hindered by the lack of taxonomical
relations. For verbs the performance is lower as is
consistently observed with WSD algorithms due to
the high degree of polysemy of verbs. For example,
in the case of Degree (Navigli and Pozetto, 2012),
nouns are the part of speech for which the system
performs the best, while the scores for other parts of
speech are somewhat lower. Thus, we can hypoth-
238
Text Descr. Len. F1 MFS Diff.
1 Gen. Env. 228 61.4 68.9 -7.5
2 T. Polit. 84 51.2 66.7 -15.5
3 T. Econ. 84 52.4 56.0 - 3.6
4 News. Gen. 119 58.8 58.0 0.8
5 T. Econ. 74 39.2 36.5 2.7
6 Web Gen. 210 67.1 64.3 2.8
7 T. Sport. 190 34.2 60.5 -26.3
8 Sci. 153 63.4 67.3 -3.9
9 Geo. Econ. 190 63.2 74.2 -11
10 Gen. Law. 160 61.9 61.9 0
11 T. Sport. 125 56.8 64.0 -7.2
12 T. Polit. 185 64.3 73.0 -8.7
13 T. Econ. 130 68.5 72.6 -4.1
Table 2: Text by text F1 scores compared to the
MFS baseline for the English corpus (T.= Trans-
lated, Gen.= General, Env.= Environment, Polit.=
Politics, Econ.= Economics, Web= Internet, Sport.=
Sports, Geo.= Geopolitics, Sci.= Science).
A P.O.S. F1 MFS F1 Diff
1108 Noun 79.42 77.4 +1.99
591 Verb 74.78 75.3 -0.51
362 Adj. 82.66 84.3 -1.59
208 Adv. 86.95 87.5 -0.55
2269 All 79.42 78.9 +0.53
Table 3: Detailed breakdown of F1 score per part
of speech category for Semeval-2007 Task 7, over
results resulting from a vote over 100 executions
esise that using a different local measure depending
on the part of speech may constitute an interesting
development while allowing a return to a more gen-
eral all-words WSD task where all parts of speech
are considered, even when the resource does not of-
fer taxonomical relation for the said parts of speech.
6 Conclusions & Perspectives
In this paper, we present a method based on a
Lesk inspired local algorithm and a global algorithm
based on ant colony optimisation. An endogenous
version (parameter estimation based on the maximi-
sation of the F-score on an annotated corpus) and
an exogenous version (parameter estimation based
on the maximisation of the global Lesk score on
the corpus) of the latter algorithm do not exhibit a
significant difference in terms of the F-score of the
result. After a more detailed analysis on a text by
text basis, we found that the algorithm performs best
on general domain texts with as little named enti-
ties as possible (around or above the MFS baseline).
For texts of more specialized domain the algorithm
consistently performs below the MFS baseline, and
for texts with many named entities, the performance
plummets greatly slightly above the level of a ran-
dom selection. We also show that with our Lesk
measure the system is best suited for WSD in a more
general setting with all parts of speech, however in
the context of just nouns, it is not the most suitable
local measure. As we have seen from the other sys-
tems, graph based local measures may be the appro-
priate answer to reach the level of the best systems
on this task, however it is important not to dismiss
the potential of other approaches. The quality of the
results depend on the global algorithm, however they
are also strongly bounded by the local measure con-
sidered. Our team, is headed towards investigating
local semantic similarity measures and towards ex-
ploiting multilingual features so as to improve the
disambiguation quality.
7 Acknowledgements
The work presented in this paper was conducted in
the context of the Formicae project funded by the
University Grenoble 2 (Universit? Pierre Mend?s
France) and the Videosense project, funded by the
French National Research Agency (ANR) under
its CONTINT 2009 programme (grant ANR-09-
CORD-026).
References
[Banerjee and Pedersen2002] Satanjee Banerjee and Ted
Pedersen. 2002. An adapted lesk algorithm for word
sense disambiguation using wordnet. In CICLing
2002, Mexico City, February.
[Benjamini and Hochberg1995] Yoav Benjamini and
Yosef Hochberg. 1995. Controlling the False Dis-
covery Rate: A Practical and Powerful Approach to
Multiple Testing. Journal of the Royal Statistical
Society. Series B (Methodological), 57(1):289?300.
[Brody and Lapata2008] Samuel Brody and Mirella La-
pata. 2008. Good neighbors make good senses:
Exploiting distributional similarity for unsupervised
239
WSD. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 65?72, Manchester, UK.
[Dorigo and St?tzle2004] Dorigo and St?tzle. 2004. Ant
Colony Optimization. MIT-Press.
[Gelbukh et al2005] Alexander Gelbukh, Grigori
Sidorov, and Sang-Yong Han. 2005. On some opti-
mization heuristics for Lesk-like WSD algorithms. In
International Conference on Applications of Natural
Language to Information Systems ? NLDB?05, pages
402?405, Alicante, Spain.
[Hirst and St-Onge1998] G. Hirst and David D. St-Onge.
1998. Lexical chains as representations of context for
the detection and correction of malapropisms. Word-
Net: An electronic Lexical Database. C. Fellbaum. Ed.
MIT Press. Cambridge. MA, pages 305?332. Ed. MIT
Press.
[Laarhoven and Aarts1987] P.J.M. Laarhoven and E.H.L.
Aarts. 1987. Simulated annealing: theory and appli-
cations. Mathematics and its applications. D. Reidel.
[Lesk1986] Michael Lesk. 1986. Automatic sense dis-
ambiguation using mrd: how to tell a pine cone from
an ice cream cone. In Proceedings of SIGDOC ?86,
pages 24?26, New York, NY, USA. ACM.
[Mann and Whitney1947] H. B. Mann and D. R. Whitney.
1947. On a Test of Whether one of Two Random Vari-
ables is Stochastically Larger than the Other. The An-
nals of Mathematical Statistics, 18(1):50?60.
[Miller1995] George A. Miller. 1995. Wordnet: A lexical
database. ACM, Vol. 38(No. 11):p. 1?41.
[Monmarch?2010] N. Monmarch?. 2010. Artificial Ants.
Iste Series. John Wiley & Sons.
[Navigli and Lapata2010] Roberto Navigli and Mirella
Lapata. 2010. An experimental study of graph con-
nectivity for unsupervised word sense disambiguation.
IEEE Trans. Pattern Anal. Mach. Intell., 32:678?692,
April.
[Navigli and Pozetto2012] Roberto Navigli and Si-
mone Paolo Pozetto. 2012. Babelnet: The
automatic construction, evaluation and applica-
tion of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217?250.
http://dx.doi.org/10.1016/j.artint.2012.07.004.
[Navigli et al2013] Roberto Navigli, David Jurgens, and
Daniele Vannella. 2013. Semeval-2013 task 12: Mul-
tilingual word sense disambiguation. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM 2013), Atlanta, Georgia, 14-15 June.
[Ponzetto and Navigli2010] Simone Paolo Ponzetto and
Roberto Navigli. 2010. Knowledge-rich word sense
disambiguation rivaling supervised systems. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1522?1531.
[Resnik1995] Philip Resnik. 1995. Using information
content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI?95,
pages 448?453, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
[Schwab et al2011] Didier Schwab, J?r?me Goulian, and
Nathan Guillaume. 2011. D?sambigu?sation lexicale
par propagation de mesures semantiques locales par al-
gorithmes a colonies de fourmis. In TALN, Montpel-
lier (France), Juillet.
[Schwab et al2012] Didier Schwab, J?r?me Goulian, An-
don Tchechmedjiev, and Herv? Blanchon. 2012. Ant
colony algorithm for the unsupervised word sense dis-
ambiguation of texts: Comparison and evaluation. In
Proceedings of COLING?2012, Mumbai (India), De-
cember. To be published.
[Schwab et al2013] Didier Schwab, Jer?me Goulian, and
Andon Tchechmedjiev. 2013. Theoretical and empir-
ical comparison of artificial intelligence methods for
unsupervised word sense disambiguation. Int. J. of
Web Engineering and Technology. In Press.
[Tchechmedjiev et al2012] Andon Tchechmedjiev,
J?r?me Goulian, Didier Schwab, and Gilles S?rasset.
2012. Parameter estimation under uncertainty with
simulated annealing applied to an ant colony based
probabilistic wsd algorithm. In Proceedings of
the First International Workshop on Optimization
Techniques for Human Language Technology, pages
109?124, Mumbai, India, December. The COLING
2012 Organizing Committee.
[Wu and Palmer1994] Zhibiao Wu and Martha Palmer.
1994. Verbs semantics and lexical selection. In Pro-
ceedings of the 32nd annual meeting of Association for
Computational Linguistics, ACL ?94, pages 133?138,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
240
Coling 2008: Proceedings of the workshop on Cognitive Aspects of the Lexicon (COGALEX 2008), pages 9?17
Manchester, August 2008
Lexical Access Based on Underspecified Input
Michael ZOCK
LIF-CNRS
?
Equipe TALEP
163, Avenue de Luminy
F-13288 Marseille Cedex 9
michael.zock@lif.univ-mrs.fr
Didier SCHWAB
Groupe GETALP
Laboratoire d?Informatique de Grenoble
385 avenue de la Bibliothque - BP 53
F-38041 Grenoble Cedex 9
didier.schwab@imag.fr
Abstract
Words play a major role in language pro-
duction, hence finding them is of vital im-
portance, be it for speaking or writing.
Words are stored in a dictionary, and the
general belief holds, the bigger the bet-
ter. Yet, to be truly useful the resource
should contain not only many entries and a
lot of information concerning each one of
them, but also adequate means to reveal the
stored information. Information access de-
pends crucially on the organization of the
data (words) and on the navigational tools.
It also depends on the grouping, ranking
and indexing of the data, a factor too often
overlooked.
We will present here some preliminary re-
sults, showing how an existing electronic
dictionary could be enhanced to support
language producers to find the word they
are looking for. To this end we have started
to build a corpus-based association ma-
trix, composed of target words and ac-
cess keys (meaning elements, related con-
cepts/words), the two being connected at
their intersection in terms of weight and
type of link, information used subsequently
for grouping, ranking and navigation.
1 Context and problem
When speaking or writing we encounter basi-
cally either of the following two situations: one
where everything works automatically, somehow
like magic, words popping up one after another
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
like spring water, and another where we look de-
liberately and often painstakingly for a specific,
possibly known word. We will be concerned here
with this latter situation: a speaker/ writer using
an electronic dictionary to look for such a word.
Unfortunately, alphabetically organized dictionar-
ies are not well suited for this kind of reverse
lookup where the inputs are meanings (elements of
the word?s definition) or conceptually related ele-
ments (collocations, associations), and the outputs
the target words.
Without any doubt, lexicographers have made
considerable efforts to assist language users, build-
ing huge resources, composed of many words and
lots of information associated with each one of
them. Still, it is not unfair to say most dictionar-
ies have been conceived from the reader?s point of
view. The lexicographers have hardly taken into
account the language producer?s perspective,
1
con-
sidering conceptual input, incomplete as it may be,
as starting point. While readers start with words,
looking generally for their corresponding mean-
ings, speakers or writers usually start with the op-
posite, meanings or concepts,
2
which should be the
entry points of a dictionary, which ideally is neu-
tral in terms of access direction.
3
The problem is that we still don?t know very
well what concepts are, whether they are compo-
sitional and if so, how many primitives there are
(Wilks, 1977; Wierzbicka, 1996; Goddard, 1998).
1
Roget?s thesaurus (Roget, 1852), Miller and Fellbaum?s
WordNet (Fellbaum, 1998) and Longman?s Language Activa-
tor (Summers, 1993), being notable exceptions (For more de-
tails, see next section).
2
Of course, this does not preclude, that we may have to
use words to refer to them in a concept-based query.
3
While we agree with Polgu`ere theoretically when he
pleads for dictionary neutrality with regard to lexical access
(Polgu`ere, 2006), from a practical point of view the situation
is obviously quite different for the speaker and listener, even
if both of them draw on the same resource.
9
Neither do we know how to represent them. Yet,
there are ways around this problem as we will
show. Whether concepts and words are organized
and accessed differently is a question we cannot
answer here. We can agree though on the fact
that getting information concerning words is fairly
unproblematic when reading, at least in the case
of most western languages. Words can gener-
ally be found easily in a dictionary, provided the
user knows the spelling, the alphabet and how to
build lemma starting from an inflected form. Un-
like words, which are organized alphabetically (in
western languages) or by form (stroke counts in
Chinese), concepts are organized topically: they
are clustered into functional groups according to
their role in real world, or our perception of it.
Psychologist have studied the difficulties peo-
ple have when trying to produce or access words
(Aitchinson, 2003). In particular, they have stud-
ied the tip-of-the-tongue phenomenon (Brown and
McNeill, 1996) and the effects an input can have
on the quality of an output (error analysis (Cutler,
1982)) and on the ease of its production: positive
or negative priming effect (activation/inhibition).
Obviously, these findings allow certain conclu-
sions, and they might guide us when developing
tools to help people find the needed word. In par-
ticular, they reveal two facts highly relevant for our
goal:
1. even if people fail to access a given word, they
might know a lot about it: origin, meaning
(word definition, role played in a given sit-
uation), part of speech, number of syllables,
similar sounding words, etc. Yet, despite all
this knowledge, they seem to lack some cru-
cial information to be able to produce the pho-
netic form. The word gets blocked at the very
last moment, even though it has reached the
tip-of-the-tongue. This kind of nuisance is all
the more likely as the target word is rare and
primed by a similar sounding word.
2. unlike words in printed or electronic dictio-
naries, words in our mind may be inexis-
tent as tokens. What we seem to have in
our minds are decomposed, abstract entities
which need to be synthesized over time.
4
Ac-
4
This may be very surprising, yet, this need not be the case
if we consider the fact that speech errors are nearly always
due to competing elements from the same level or an adja-
cent one, unless they are the result of a surrounding concept
which has been activated, or which is about to be translated
cording to Levelt (Levelt, 1996) the genera-
tion of words (synthesis) involves the follow-
ing stages: conceptual preparation, lexical se-
lection, phonological- and phonetic encoding,
articulation. Bear in mind that having per-
formed ?lexical selection? does not imply ac-
cess to the phonetic form (see the experiments
on the tip-of-the-tongue phenomenon).
What can be concluded from these observa-
tions? It seems that underspecified input is suffi-
ciently frequent to be considered as normal. Hence
we should accept it, and make the best out of it by
using whatever information is available (accessi-
ble), no matter how incomplete, since it may still
contribute to find the wanted information, be it by
reducing the search space. Obviously, the more in-
formation we have the better, as this reduces the
number of words among which to choose.
2 Related work and goal
While more dictionaries have been built for the
reader than for the writer, there have been some
onomasiological attempts as early as in the mid-
dle of the 19th century. For example, Roget?s
Thesaurus (Roget, 1852), T?ong?s Chinese and
English instructor (T?ong, 1862), or Boissiere?s
analogical dictionary (Boissi`ere, 1862).
5
Newer
work includes Mel??cuk?s ECD (Mel??cuk et al,
1999), Miller and Fellbaum?s WordNet (Fellbaum,
1998), Richardson and Dolan?s MindNet (Richard-
son et al, 1998), Dong?s HowNet (Dong and
Dong, 2006) and Longman?s Language Activa-
tor (Summers, 1993). There is also the work of
into words. Put differently, we do not store words at all in
our mind, at least not in the layman?s or lexicographer?s sense
who consider word-forms and their meanings as one. If we
are right, than rather continue to consider the human mind as
a word store we could consider it as a word factory. Indeed,
by looking at some of the work done by psychologists who try
to emulate the mental lexicon (for a good survey see (Harley,
2004), pages 359-374) one gets the impression that words are
synthesized rather than located and read out. Taking a look at
all this work, generally connectionist models, one may con-
clude that, rather than having words in our mind we have a
set of more or less abstract features (concepts, syntactic infor-
mation, phonemes), distributed across various layers, which
need to be synthesized over time. To do so we proceed from
abstract meanings to concrete sounds, which at some point
were also just abstract features. By propagating energy rather
than data (as there is no message passing, transformation or
cumulation of information, there is only activation spreading,
that is, changes of energy levels, call it weights, electronic
impulses, or whatever), that we propagate signals, activating
ultimately certain peripheral organs (larynx, tongue, mouth,
lips, hands) in such a way as to produce movements or sounds,
that, not knowing better, we call words.
5
For a more recent proposal see (Robert et al, 1993).
10
(Fontenelle, 1997; Sierra, 2000; Moerdijk, 2008),
various collocation dictionaries (BBI, OECD) and
Bernstein?s Reverse Dictionary.
6
Finally, there is
M. Rundell?s MEDAL, a thesaurus produced with
the help of Kilgariff?s Sketch Engine (Kilgarriff et
al., 2004).
As one can see, a lot of progress has been ac-
complished over the last few years, yet more can be
done, especially with regard to unifying linguistic
and encyclopedic knowledge. Let?s take an exam-
ple to illustrate our point.
Suppose, you were looking for a word express-
ing the following ideas: ?superior dark coffee made
from beans from Arabia?, and that you knew that
the target word was neither espresso nor cappuc-
cino. While none of this would lead you directly
to the intended word, mocha, the information at
hand, i.e. the word?s definition or some of its ele-
ments, could certainly be used. In addition, people
draw on knowledge concerning the role a concept
(or word) plays in language and in real world, i.e.
the associations it evokes. For example, they may
know that they are looking for a noun standing for
a beverage that people take under certain circum-
stances, that the liquid has certain properties, etc.
In sum, people have in their mind an encyclope-
dia: all words, concepts or ideas being highly con-
nected. Hence, any one of them has the potential to
evoke the others. The likelihood for this to happen
depends, of course, on factors such as frequency
(associative strength), distance (direct vs. indirect
access), prominence (saliency), etc.
How is this supposed to work for a dictionary
user? Suppose you were looking for the word
mocha (target word: t
w
), yet the only token com-
ing to your mind were computer (source word:
s
w
). Taking this latter as starting point, the system
would show all the connected words, for example,
Java, Perl, Prolog (programing languages), mouse,
printer (hardware), Mac, PC (type of machines),
etc. querying the user to decide on the direction of
search by choosing one of these words. After all,
s/he knows best which of them comes closest to the
t
w
. Having started from the s
w
?computer?, and
knowing that the t
w
is neither some kind of soft-
ware nor a type of computer, s/he would probably
choose Java, which is not only a programming lan-
guage but also an island. Taking this latter as the
6
There is also at least one electronic incarnation
of a dictionary with reverse access, combining a dic-
tionary (WordNet) and an encyclopedia (Wikipedia)
(http://www.onelook.com/reverse-dictionary.shtml).
new starting point s/he might choose coffee (since
s/he is looking for some kind of beverage, possibly
made from an ingredient produced in Java, coffee),
and finally mocha, a type of beverage made from
these beans. Of course, the word Java might just
as well trigger Kawa which not only rhymes with
the s
w
, but also evokes Kawa Igen, a javanese vol-
cano, or familiar word of coffee in French.
As one can see, this approach allows word ac-
cess via multiple routes: there are many ways lead-
ing to Rome. Also, while the distance covered
in our example is quite unusual, it is possible to
reach the goal quickly. It took us actually very
few moves, four, to find an indirect link, between
two, fairly remotely related terms: computer and
mocha. Of course, cyber-coffee fans might be even
quicker in reaching their goal.
3 The lexical matrix revisited
The main question that we are interested in here
is how, or in what terms, to index the dictionary
in order to allow for quick and intuitive access to
words. Access should be possible on the basis
of meaning (or meaning elements), various kinds
of associations (most prominently ?syntagmatic?
ones) and, more generally speaking, underspeci-
fied input. To this end we have started to build an
association matrix (henceforth AM), akin to, yet
different from G. Miller?s initial proposal of WN
(Miller et al, 1990). He suggested to build a lex-
ical matrix by putting on one axis all the forms,
i.e. words of the language, and on the other, their
corresponding meanings. The latter being defined
in terms of synsets. The corresponding meaning-
form relations are signaled via a boolean (pres-
ence/absence). Hence, looking at the intersection
of meanings and forms, one can see which mean-
ings are expressed by, or converge toward what
forms, or conversely, what form expresses which
meanings. Whether this is the way WN is actually
implemented is not clear to us, though we believe
that it is not. Anyhow, our approach is different,
and we hope the reader will understand in a mo-
ment the reasons why.
We will also put on one axis all the form ele-
ments, i.e. the lemmata or expressions of a given
language (we refer to them as target words, hence-
forth t
w
). On the other axis we will place the trig-
gers or access-words (henceforth a
w
), that is, the
words or concepts capable and likely to evoke the
t
w
. These are typically the kind of words psy-
11
chologists have gathered in their association ex-
periments (Jung and Riklin, 1906; Deese, 1965;
Schvaneveldt, 1989). Note, that instead of putting
a boolean value at the intersection of the t
w
and the
a
w
, we will put weights and the type of link hold-
ing between the co-occurring terms. This gives us
quadruplets. For example, an utterance like ?this
is the key of the door? might yield the a
w
(key),
the t
w
(door), the link type l
t
(part of), and a weight
(let?s say 15).
The fact that we have these two kinds of in-
formation is very important later on, as it allows
the search engine to cluster by type the possible
answers to be given in response to a user query
(word(s) provided as input) and to rank them.
Since the number of hits, i.e. words from which
the user must choose, may be substantial (depend-
ing on the degree of specification of the input), it is
important to group and rank them to ease naviga-
tion, allowing the user to find directly and quickly
the desired word, or at least the word with which
to continue search.
Obviously, different word senses (homographs),
require different entries (bank-money vs bank-
river), but so will synonyms, as every word-form,
synonym or not, is likely to be evoked by a differ-
ent key- or access-word (similarity of sound).
7
Also, we will need a new line for every different
relation between a a
w
and a t
w
. Whether more than
one line is needed in the case of identical links be-
ing expressed by different linguistic resources (the
lock of the door vs. the door?s lock vs. the door
has a lock) remains an open empirical question.
Let us see quickly how our AM is supposed
to work. Imagine you wanted to find the word
for the following concept: hat of a bishop. In
such a case, any of the following concepts or
words might come to your mind: church, Vati-
can, abbot, monk, monastery, ceremony, ribbon,
and of course rhyming words like: brighter, fighter,
lighter, righter, tighter, writer,
8
as, indeed, any of
them could remind us of the t
w
: mitre. Hence, all
of them are possible a
w
.
Once this resource is built, access is quite
straightforward. The user gives as input all the
words coming to his mind when thinking of a given
7
Take, for example, the nouns rubbish and garbage which
can be considered as synonyms. Yet, while the former may
remind you of a rabbit or (horse)-radish, the latter may evoke
the word cabbage.
8
The question, whether rhyming words should be com-
puted is not crucial at this stage.
idea or concept,
9
and the system will display all
connected words. If the user can find the item he
is looking for in this list, search stops, otherwise
it will continue, the user giving other words of the
list, or words evoked by them.
Of course, remains the question of how to build
this resource, in particular, how to populate the
axis devoted to the trigger words, i.e. access-
keys. At present we consider three approaches:
one, where we use the words occurring in word
definitions (see also, (Dutoit and Nugues, 2002;
Bilac et al, 2004)), the other is to mine a well-
balanced corpus, to find co-occurrences within a
given window (Ferret and Zock, 2006), the size
depending a bit on the text type (encyclopedia) or
type of corpus. Still another solution would be
to draw on the association lists produced by psy-
chologists, see for example http://www.usf.edu/, or
http://www.eat.rl.ac.uk.
Of course, the idea of using matrices in linguis-
tics is not new. There are at least two authors who
have proposed its use: M. Gross (Gross, 1984)
used it for coding the syntactic behavior of lex-
ical items, hence the term lexicon-grammar, and
G. Miller, the father of WN (Miller et al, 1990)
suggested it to support lexical access. While the
former work is not relevant for us here, Miller?s
proposal is. What are the differences between his
proposal and ours? There are basically four main
differences:
1. we use, collocations or access-words, i.e a
ws
rather than synsets; Hence, any of the follow-
ing a
ws
(cat, grey, computer device, cheese,
Speedy Gonzales) could point toward the t
w
?mouse?, none of them are part of the mean-
ing, leave alone synonyms.
2. we mark explicitly the weight and the type of
link between the t
w
and the a
w
(isa, part of,
etc.),
10
whereas WN uses only a binary value.
Both the weight and link are necessary infor-
mation for ranking and grouping, i.e. naviga-
tion.
3. our AM is corpus-sensitive (see below),
hence, we can, at least in principle, accommo-
9
The quantifier all shouldn?t be taken too literally. What
we have in mind are ?salient? words available in the speaker?s
mind at a given moment
10
Hence, if several links are possible between the t
w
and
the a
w
, several cells will be used. Think of the many possible
relations between a city and a country, example: Paris and
France (part of, biggest city of, located in, etc.)
12
date the fact that a speaker is changing topics,
adapting the weight of a given word or find a
more adequate a
w
in this new context. Think
of ?piano? in the contexts of a concert or mov-
ing your household. Only the latter would
evoke the notion of weight.
4. relying on a corpus, we can take advantage of
syntagmatic associations (often encyclopedic
knowledge), something which is difficult to
obtain for WN.
4 Keep the set of lexical candidates small
Here and in the next section we describe how the
idea of the AM has been computationally dealt
with. The goal is to reduce the number of hits,
i.e. possible t
ws
(output), as a function of the in-
put, i.e, the number of relevant a
ws
given by the
speaker/writer. To achieve this goal we apply lex-
ical functions to the a
ws
, considering the intersec-
tion of the obtained sets to be the relevant t
ws
.
4.1 Lexical Functions
The usefulness of lexical functions for linguistics
in general and for language production in particu-
lar has been shown by Mel??cuk (Mel??cuk, 1996).
We will use them here, as they seem to fit also our
needs of information extraction or lexical access.
Mel??cuk has coined the term lexical functions to
refer to the fact that two terms are systematically
related. For example, the lexical function Gener
refers to the fact that some term (let?s say ?cat?)
can be replaced by a more general term (let?s say
?animal?).
Lexical functions encode the combinability of
words. While ?big? and ?strong? express the same
idea (intensity, magnitude), they cannot be com-
bined freely with any noun: strong can be as-
sociated with fever, whereas big cannot. Of
course, this kind of combinability between lexical
terms is language specific, because unlike in En-
glish, in French one can say grosse fi`evre or forte
fi`evre, both being correct (Schwab and Lafourcade,
2007). Our AM handles, of course these kind of
functions. Here is a list of some of them:
- paradigmatic associations: hypernymy
(?cat? - ?animal?), hyponymy, synonymy, or
antonymy,. . . ;
- syntagmatic associations: collocations (?fear?
being associated with ?strong? or ?little?);
- morphological relations ie. terms being de-
rived from another part of speech: applying
the change-part-of-speech lexical function
f
cpos
to ?garden? will yield: f
cpos
(?garden?) =
{?to garden?, ?gardener?, . . .}
- sound-related items: homophones, rhymes.
4.2 Assumptions concerning search
The purpose of using lexical functions is to reduce
the number of possible outcomes from which the
user must choose. The list contains either the t
w
or another promising a
w
the user may want use to
continue search. Hence, lexical functions are use-
ful for search provided that:
1. the speaker/writer is able to specify the kind
of relations s/he wants to use. The problem
here lies in the nature and number of the func-
tions, some of them being very well specified,
while others are not.
2. the larger the number of trigger words the
smaller the list of words from which to
choose: the speaker/writer can add or delete
words to broaden or narrow the scope of
his/her query.
These hypotheses are being modeled by using
set properties of lexical functions. The idea is to
apply all functions, or a selection of them, to the
a
ws
and to give the speaker/writer the intersection
as result (see section 5.3.5 for an example)
5 Experiment
We have started with a simple, preliminary exper-
iment. Only one lexical function was used: neigh-
borhood (henceforth f
neig
). Let f
neig
be the func-
tion producing the set of co-occurring terms within
a given window (sentence or a paragraph).
11
The
result produced by the system and returned to the
user is the intersection of the application of f
neig
to the a
ws
. In the next section we explain how this
function is applied to two corpora (Wordnet and
Wikipedia), to show their respective qualities and
shortcomings for this specific task.
5.1 WordNet
5.1.1 Description
WordNet (henceforth WN) is a lexical database
for English developed under the guidance of G.
11
The scope or window size will vary with the text type
(normal text vs. encyclopedia). The optimal size is at this
point still an empirical question.
13
Miller (Miller et al, 1990). One of his goals was
to support lexical access akin to the human mind,
association-based. Knowledge is stored in a net-
work composed of nodes and links (nodes being
words or concepts and the links are the means of
connecting them) and access to knowledge, i.e.
search, takes place by entering the network at some
point and follow the links until one has reached the
goal (unless one has given up before). This kind
of navigation in a huge conceptual/lexical network
can be considered equivalent to spreading activa-
tion taking place in our brain.
Of course, such a network has to be built, and
navigational support must be provided to find the
location where knowledge or words are stored.
This is what Miller and his coworkers did by build-
ing WN. The resource has been built manually, and
it contains at present about 150.000 entries.
The structure of the dictionary is different from
conventional, alphabetical resources. Words are
organized in WN in two ways. Semantically sim-
ilar words, i.e. synonyms, are grouped as clus-
ters. These sets of synonyms, called synsets, are
then linked in various ways, depending on the
kind of relationship they entertain with the ad-
jacent synset. For example, their neighbors can
be more general or specific (hyperonymy vs. hy-
ponymy), they can be part of some reference ob-
ject (meronymy: car-motor), they can be the op-
posite (antonymy: hot-cold), etc. While WN is a
resource it can also be seen as a corpus.
5.1.2 Using WN as a corpus
There are many good reasons to use WN for
learning f
n
. For one, there are many extensions,
and second, the one we are using, eXtended WN
(Mihalcea and Moldovan, 2001) spares us the trou-
ble of having to address issues like: (a) seg-
mentation: we do not need to identify sentence
boundaries ; (b) semantic ambiguity: words being
tagged, we get good precision; (c) lemmatization:
since only verbs, nouns, adjectives and adverbs are
tagged, we need neither a stoplist nor a lemmatizer.
Despite all these qualities, two important prob-
lems remain nevertheless for this kind of corpus:
(a) size: though, all words are tagged, the cor-
pus remains small as it contains only 63.941 dif-
ferent words; (b) in consequence, the corpus lacks
many syntagmatic associations encoding encyclo-
pedic knowledge.
5.2 Using Wikipedia as corpus
Wikipedia is a free, multilingual encyclopedia, ac-
cessible on the Web.
12
For our experiment we have
chosen the English version which of this day (12th
of may 2008) contains 2,369,180 entries.
Wikipedia has exactly the opposite properties of
WN. While it covers well encyclopedic relations, it
is only raw text. Hence problems like text segmen-
tation, lemmatisation and stoplist definition need
to be addressed.
Our experiments with Wikipedia were very rudi-
mentary, given that we considered only 1000 doc-
uments. These latter were obtained in response to
the term ?wine?, by following the links obtained for
about 72.000 words.
5.3 Prototype
5.3.1 Building the resource and using it.
Building the resource requires processing a cor-
pus and building the database. Given a corpus
we apply our neighborhood function to a prede-
termined window (a paragraph in the case of ency-
clopedias).
13
The result, i.e. the co-occurrences,
will be stored in the database, together with their
weight, i.e. number of times two terms appear to-
gether, and the type of link. As mentionned above,
both kinds of information are needed later on for
ranking and navigation.
14
At present, cooccurences are stored as triplets
(t
w
, a
w
, times), where times represents the number
of times the two terms cooccur in the corpus, the
scope of coccurence being here the paragraph.
5.3.2 Processing of the Wikipedia page
For each Wikipedia page, a preprocessor
converts HTML pages into plain text. Next,
a part-of-speech tagger (http://www.ims.uni-
stuttgart.de/projekte/corplex/TreeTagger/) is used
to annotate all the words of the paragraph under
consideration. This allows the filtering of all
irrelevant words, to keep but a bag of words,
that is, the nouns, adjectives, verbs and adverbs
occuring in the paragraph. These words will be
used to fill the triplets of our database.
12
http://www.wikipedia.org
13
The optimal window-size depends probably on the text
type (encyclopedia vs. unformatted text). Yet, in the absence
of clear criteria, we consider the optimal window-size as an
open, empirical question.
14
This latter aspect is not implemented yet, but will be
added in the future, as it is a necessary component for easy
navigation (Zock and Bilac, 2004; Zock, 2006; Zock, 2007).
14
5.3.3 Corpus Building
We start arbitrarily from some page (for our ex-
periment, we have chosen ?wine? as input), apply
the algorithm outlined here above and pick then
randomly a noun within this page to fetch with this
input a new page on Wikipedia. This process is re-
peated until a given sample size is obtained (in our
case 1000 pages). Of course, instead of picking
randomly a noun, we could have decided to pro-
cess all the nouns of a given page, and to add then
incrementally the nouns of the next pages. Yet,
doing this would have led us to privilege a specific
topic (in our case ?wine?) instead of a more general
one.
5.3.4 Usage
We have developed a website in Java as a
servlet. Interactions with humans are simple: peo-
ple can add or delete a word from the current list
(see Input in the figure on top of the next page).
The example presented shows that with very few
words, hence very quickly, we can obtain the de-
sired word.
Given some input, the system provides the user
with a list of words cooccuring with the a
ws
. The
output is an ordered list of words, the order de-
pending on the overall score, i.e. number of cooc-
currences between the a
w
and the t
w
. For exam-
ple, if the a
ws
?wine? and ?harvest? co-occur with
the t
w
?bunch? respectively 5 and 8 times, then
the overall score of cooccurence of ?bunch? is 13:
((wine, harvest), bunch, 13). Hence, all words with
a higher score will precede it, while those with a
lower score will follow it.
5.3.5 Examples and Comparison of the
results of the two corpora
Here below are the examples extracted from the
WN corpus (see figure-1). Our goal was to find
the word ?vintage?. Trigger words are ?wine? and
?harvest?, yielding respectively 488 and 30 hits, i.e.
words. As one can see ?harvest? is a better ac-
cess term than ?wine?. Combining the two will re-
duce the list to 6 items. Please note that the t
w
?vintage? is not among them, eventhough it exists
in WordNet, which illustrates nicely the fact that
storage does not guarantee accessibility (Sinopal-
nikova and Smrz, 2006).
Looking at figure-1 you will see that the results
have improved considerably with Wikipedia. The
same input, ?wine? evokes many more words (1845
as opposed to 488). For ?harvest? we get 983 hits in-
Input WordNet Wikipedia
488 words 1845
words
grape sweet aloholic country
serve france god characteristics
wine small fruit regulation grape
dry bottle appellation system
produce red bottled like
bread hold christian track
. . . . . . . . . . . .
30 words 983 words
month fish produce grain
grape revolutionary autumn farms
calendar festival energy cut
harvest butterfish dollar combine ground
person make balance rain
wine first amount rich
. . . . . . . . . . . .
6 words 45 words
make grape grape vintage
wine fish someone bottle produce
+harvest commemorate person fermentation juice
. . . . . . Beaujolais taste
viticulture France
Bordeaux vineyard
. . . . . .
Figure 1: Comparing two corpora (eXtended
WordNet and Wikipedia) with various inputs
stead of 30 (the intersection containing 62 words).
Combining the two reduces the set to 45 items
among which we will find, of course, the target
word.
We hope that this example is clear enough to
convince the reader that it makes sense to use real
text as corpus to extract from it the kind of in-
formation (associations) people are likely to give
when looking for a word.
6 Conclusion and perspectives
We have addressed in this paper the problem of
word finding for speakers or writers. Conclud-
ing that most dictionaries are not well suited to al-
low for this kind of reverse access based on mean-
ings (or meaning related elements, associations),
we looked at work done by psychologists to get
some inspiration. Next we tried to clarify which
of these findings could help us build the dictionary
of tomorrow, that is, a tool integrating linguistic
and encyclopedic knowledge, allowing navigation
by taking either or as starting point. While lin-
guistic knowledge is more prominent for analysis
(reading), encyclopedic facts are more relevant for
production. We?ve presented then our ideas of how
to build a resource, allowing lexical access based
15
on underspecified, i.e. imperfect input. To achieve
this goal we?ve started building an AM composed
of form elements (the words and expressions of
a given language) and a
ws
. The role of the lat-
ter being to lead to or to evoke the t
w
. In the last
part we?ve described briefly the results obtained by
comparing two resources (WN and Wikipedia) and
various inputs. Given the fact that the project is
still quite young, only preliminary results can be
shown at this point.
Our next steps will be to take a closer look at the
following work: clustering of similar words (Lin,
1998), topic signatures (Lin and Hovy, 2000) and
Kilgariff?s sketch engine (Kilgarriff et al, 2004).
We plan also to add other lexical functions to en-
rich our database with a
ws
. We plan to experiment
with corpora, trying to find out which ones are best
for our purpose
15
and we will certainly experiment
with the window size
16
to see which size is best
for which text type. Finally, we plan to insert in
our AM the relations holding between the a
w
and
the t
w
. As these links are contained in our corpus,
we should be able to identify and type them. The
question is, to what extent this can be done auto-
matically.
Obviously, the success of our resource will de-
pend on the quality of the corpus, the quality of
the a
ws
, weights and links, and the representativ-
ity of all this for a given population. While we do
believe in the justification of our intuitions, more
work is needed to reveal the true potential of the
approach. The ultimate judge being, of course, the
future user.
15
For example, we could consider a resource like Con-
ceptNet of the Open Mind Common-Sense project (Liuh and
Singh, 2004).
16
For example, it would have been interesting to consider
coocurrences beyond the scope of the paragraph, by consider-
ing the logical structure of the Wikipedia document. Anyhow,
our experiment needs to be redone with more data than just
1000 pages, the size chosen here for lack of time. Indeed one
could consider using the entire corpus of Wikipedia or mixed
corpora
References
Aitchinson, Jean. 2003. Words in the Mind: an Intro-
duction to the Mental Lexicon (3d edition). Black-
well, Oxford.
Bilac, S., W. Watanabe, T. Hashimoto, T. Tokunaga,
and H. Tanaka. 2004. Dictionary search based
on the target word description. In Proc. of the
Tenth Annual Meeting of The Association for NLP
(NLP2004), pages 556?559, Tokyo, Japan.
Boissi`ere, P. 1862. Dictionnaire analogique de la
langue franc?aise : r?epertoire complet des mots par
les id?ees et des id?ees par les mots. Larousse et A.
Boyer, Paris.
Brown, R. and D. McNeill. 1996. The tip of the tounge
phenomenon. Journal of Verbal Learning and Ver-
bal Behaviour, 5:325?337.
Cutler, A, editor, 1982. Slips of the Tongue and Lan-
guage Production. Mouton, Amsterdam.
Deese, James. 1965. The structure of associations in
language and thought. Johns Hopkins Press.
Dong, Zhendong and Qiang Dong. 2006. HOWNET
and the computation of meaning. World Scientific,
London.
Dutoit, Dominique and P. Nugues. 2002. A lexical
network and an algorithm to find words from defini-
tions. In van Harmelen, F., editor, ECAI2002, Proc.
of the 15th European Conference on Artificial Intel-
ligence, pages 450?454, Lyon. IOS Press, Amster-
dam.
Fellbaum, Christiane, editor, 1998. WordNet: An Elec-
tronic Lexical Database and some of its Applica-
tions. MIT Press.
Ferret, Olivier and Michael Zock. 2006. Enhancing
electronic dictionaries with an index based on associ-
ations. In ACL ?06: Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the ACL, pages 281?288.
Fontenelle, Thierry. 1997. Using a bilingual dictionary
to create semantic networks. International Journal
of Lexicography, 10(4):275?303.
Goddard, Cliff. 1998. Bad arguments against seman-
tic primitives. Theoretical Linguistics, 24(2-3):129?
156.
16
Gross, Maurice. 1984. Lexicon-grammar and the anal-
ysis of french. In Proc. of the 11th COLING, pages
275?282, Stanford, CA.
Harley, Trevor. 2004. The psychology of language.
Psychology Press, Taylor and Francis, Hove and
New York.
Jung, Carl and F. Riklin. 1906. Experimentelle
Untersuchungen ?uber Assoziationen Gesunder. In
Jung, C. G., editor, Diagnostische Assoziationsstu-
dien, pages 7?145. Barth, Leipzig, Germany.
Kilgarriff, Adam, Pavel Rychl?y, Pavel Smr?z, and David
Tugwell. 2004. The Sketch Engine. In Proceedings
of the Eleventh EURALEX International Congress,
pages 105?116, Lorient, France.
Levelt, Willem. 1996. A theory of lexical access
in speech production. In Proc. of the 16th Con-
ference on Computational Linguistics, Copenhagen,
Denmark.
Lin, Chin-Yew and Eduard H. Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In COLING, pages 495?501. Morgan Kauf-
mann.
Lin, Dekang. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?774,
Montreal.
Liuh, H. and P. Singh. 2004. ConceptNet: a practi-
cal commonsense reasoning toolkit. BT Technology
Journal.
Mel??cuk, I., N. Arbatchewsky-Jumarie, L. Iordanskaja,
S. Mantha, and A. Polgu`ere. 1999. Dictionnaire
explicatif et combinatoire du franc?ais contemporain
Recherches lexico-s?emantiques IV. Les Presses de
l?Universit?e de Montr?eal, Montr?eal.
Mel??cuk, Igor. 1996. Lexical functions: A tool for
the description of lexical relations in the lexicon. In
Wanner, L., editor, Lexical Functions in Lexicogra-
phy and Natural Language Processing, pages 37?
102. Benjamins, Amsterdam/Philadelphia.
Mihalcea, Rada and Dan Moldovan. 2001. Extended
WordNet: progress report. In NAACL 2001 - Work-
shop on WordNet and Other Lexical Resources, Pitts-
burgh, USA.
Miller, G. A., R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-
line lexical database. International Journal of Lexi-
cography, 3(4), pages 235?244.
Moerdijk, Fons. 2008. Frames and semagrams; Mean-
ing description in the general dutch dictionary. In
Proceedings of the Thirteenth Euralex International
Congress, EURALEX, Barcelona.
Polgu`ere, Alain. 2006. Structural properties of lexi-
cal systems: Monolingual and multilingual perspec-
tives. Sidney. Coling workshop ?Multilingual Lan-
guage Resources and Interoperability?.
Richardson, S., W. Dolan, and L. Vanderwende. 1998.
Mindnet: Acquiring and structuring semantic infor-
mation from text. In ACL-COLING?98, pages 1098?
1102.
Robert, Paul, Alain Rey, and J. Rey-Debove. 1993.
Dictionnaire alphabetique et analogique de la
Langue Franc?aise. Le Robert, Paris.
Roget, P. 1852. Thesaurus of English Words and
Phrases. Longman, London.
Schvaneveldt, R., editor, 1989. Pathfinder Associa-
tive Networks: studies in knowledge organization.
Ablex, Norwood, New Jersey, US.
Schwab, Didier and Mathieu Lafourcade. 2007. Mod-
elling, detection and exploitation of lexical functions
for analysis. ECTI Transactions Journal on Com-
puter and Information Technology, 2(2):97?108.
Sierra, Gerardo. 2000. The onomasiological dictio-
nary: a gap in lexicography. In Proceedings of the
Ninth Euralex International Congress, pages 223?
235, IMS, Universit?at Stuttgart.
Sinopalnikova, Anna and Pavel Smrz. 2006. Knowing
a word vs. accessing a word: Wordnet and word as-
sociation norms as interfaces to electronic dictionar-
ies. In Proceedings of the Third International Word-
Net Conference, pages 265?272, Korea.
Summers, Della. 1993. Language Activator: the
world?s first production dictionary. Longman, Lon-
don.
T?ong, Ting-K?u. 1862. Ying ?u tsap ts??un (The Chinese
and English Instructor). Canton.
Wierzbicka, Anna. 1996. Semantics: Primes and Uni-
versals. Oxford University Press, Oxford.
Wilks, Yorick. 1977. Good and bad arguments about
semantic primitives. Communication and Cognition,
10(3?4):181?221.
Zock, Michael and Slaven Bilac. 2004. Word lookup
on the basis of associations : from an idea to a
roadmap. In Workshop on ?Enhancing and using
electronic dictionaries?, pages 29?35, Geneva. COL-
ING.
Zock, Michael. 2006. Navigational aids, a critical
factor for the success of electronic dictionaries. In
Rapp, Reinhard, P. Sedlmeier, and G. Zunker-Rapp,
editors, Perspectives on Cognition: A Festschrift for
Manfred Wettler, pages 397?414. Pabst Science Pub-
lishers, Lengerich.
Zock, Michael. 2007. If you care to find what you
are looking for, make an index: the case of lexical
access. ECTI, Transaction on Computer and Infor-
mation Technology, 2(2):71?80.
17
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 75?84,
Beijing, August 2010
Lexical Access, a Search-Problem
Michael Zock (1), Didier Schwab (2) and Nirina Rakotonanahary (2)
(1) LIF-CNRS, TALEP, 163, Avenue de Luminy
(2) LIG-GETALP, University of Grenoble
zock@free.fr, didier.schwab@imag.fr, damanidaddy@msn.com
Abstract
Our work is confined to word access,
that is, we present here our ideas of how to
improve electronic dictionaries in order to
help language producers (speaker/writer)
to find the word they are looking for. Our
approach is based on psychological find-
ings (representation, storage and access of
information in the human mind), observed
search strategies and typical navigational
behavior.
If one agrees with the idea that lex-
ical access (word finding) is basically a
search problem, then one may still want
to find out where and how to search.
While the space, i.e. the semantic map
in which search takes place is a resource
problem,? any of the following could be
used: dictionary, corpus, thesauraus, etc.
or a mix of them,? its exploration is typ-
ically a search problem. Important as it
may be, the building of a high quality re-
source is not the focus of this work, we
rely on an existing one, and while we
are concerned with its quality, we will be
mostly concerned here with search meth-
ods, in order to determine the best.
1 Problem: find the needle in a haystack
One of the most vexing problems in speaking or
writing is that one knows a given word, yet one
fails to access it when needed. This kind of search
failure, often referred to as dysnomia or Tip of the
Tongue-problem, occurs not only in communica-
tion, but also in other activities of everyday life.
Being basically a search problem it is likely to oc-
cur whenever we look for something that exists
in real world (objects) or our mind: dates, phone
numbers, past events, peoples? names, or you just
name it.
As one can see, we are concerned here with the
problem of words, or rather, how to find them in
the place where they are stored: the human brain,
or an external resource, a dictionary. Our work
being confined to lexical access, we would like
to develp a semantic map and a compass to help
language producers to find the word they are look-
ing for. More precisely, we try to build an index
and a navigational tool allowing people to access
words no matter how incomplete their conceptual
input may be. Our approach is based on psy-
chological findings concerning the mental lexicon
(Aitchison, 2003; Levelt et al, 1999), i.e. storage
and access of information in the human mind, ob-
served search strategies and typical navigational
behavior.
2 Consider the following elements before
attempting an engineering solution
Before conceiving a roadmap leading to an en-
gineering solution it may be useful to consider
certain points. The list here below is by no
means complete, neither is the following discus-
sion. Nevertheless we believe that the following
points are worth consideration: features of the
mental lexicon, how to build and use the resource,
searching, ranking and weights, interface prob-
lems. For reasons of space constraints we will
touch briefly only upon some of these points.
Our main goal is the enhancement of electronic
dictionaries to help speakers or writers to find
75
quickly and intuitively the word they are looking.
To achieve this target we take inspiration in the
findings concerning the human brain (structure,
process) when it tries access words in the mental
lexicon.
2.1 The mental lexicon, a small-world
network?
While forms (lemma) and meanings (lexical con-
cepts, definitions) are stored side by side in pa-
per dictionaries (holistic presentation), the hu-
man brain stores them differently. The informa-
tion concerning meaning, forms and sound is dis-
tributed across various layers. Lexical fragmen-
tation or information distribution is supported by
many empirical findings,1 and while this fact is
arguably the reason accounting for word access
problems, it is probably also the explanation of
the power and the flexibility of the human mind
which generally manages to find in no time the
right term after having searched for it in a huge
store of words.
While it is still not entirely clear what is stored,
or whether anything is stored at all 2 coming close
to the kind of information generally found in dic-
tionaries, it does seem clear though that the struc-
ture of mental lexicon is a multidimensional net-
work in which the user navigates. ?Entries in the
lexicon are not islands; the lexicon has an inter-
nal structure. Items are connected or related in
various ways...There are item relations within and
between entries.? (Levelt, 1989). While the for-
mer relate meanings and forms: syntactic (part
of speech), morphological, phonological informa-
tion, the latter connect lexical entries.3 In sum,
1Speech errors (Fromkin, 1980), studies on aphasia (Dell
et al, 1997; Blanken et al, 2004) or response times i.e.
chronometric studies (Levelt et al, 1999), neuroimaging
(Shafto et al, 2007; Kikyo et al, 2001), eye movements,
(Roelofs, 2004), experiments on priming (Schvaneveldt et
al., 1976) or the tip of the tongue problem (TOT) (Brown
and McNeill, 1996).
2An important feature of the mental lexicon lies in the
fact that the entries are not accessed but activated (Marslen-
Wilson, 1990; Altmann, 1997). Of course, such a detail can
have far reaching consequences concerning knowledge rep-
resentation and use, i.e. structure and process.
3These are typically the kind of relations we can find in
WordNet (Fellbaum, 1998), which happens to be quite rich
in this respect, but relatively poor with regard to intrinsic, i.e.
intralexical information.
lexical networks store or encode the information
people typically have with regard to words, and
finding the needed information, amounts to enter
the graph at some point,? in the case of writing or
speaking, usually a node dedicated to meaning,?
and to follow the links until one has reached the
goal (target word). While computer scientists
call this kind of search ?navigation?, psychologists
prefer the term ?activation spreading. While not
being exactly the same, functionally speaking they
are equivalent though.
As every day language experience shows,
things may go wrong, we lack information, hence
we get blocked. Yet when trying to complete the
puzzle we do not start from scratch, we rely on
existing information, which, in terms of the net-
work metaphor means that we start from (infor-
mation underlying) a word being close to the tar-
get word.4
It is interesting to note, that our lexical graphs
seem to have similar characteristics as small-
world networks. These latter are a type of graph
in which most nodes, eventhough not being direct
neighbors, can be reached via a small number of
clicks, about 6, regardless of the starting point.
This property of networks, where objects, or the
nodes standing for them, are highly connected has
first been described by Frigyes Karinthy (1929)
a Hungarian writer, to be tested then many years
later by a social psychologist (Milgram, 1961).
Nodes can be anything, people, words, etc. If they
represent people, than edges specify their relation-
ship, i.e. the very fact that they know each other,
that they are friends, etc. Given this high connec-
tivity, anything seems to be at the distance of a few
mouse clicks. Hence, it is easy to connect peo-
ple or to find out who entertains with whom what
kind of relationship. Obviously, there is a strik-
ing similarity to our lexical graphs, and the small-
world feature has been tested by mathematicians,
who concluded that the distance for words is even
smaller than in the original Milgram experiments,
namely 4 rather than 6. Indeed, (Motter et al,
2002) and colleagues could show that more than
4As TOT experiments have shown (Brown and McNeill,
1996), people always know something concerning the target
word (meaning, form, relation to other words), hence finding
a word in such a situation amounts to puzzle-completion.
76
99 percent of the word pairs of their corpus could
be connected in 4 steps at the most.
2.2 Building the resource
There are two elements we need to get a clearer
picture of: the nature of the resource (semantic
map), and the search method i.e. the way to ex-
plore it. Concerning the resource, there are many
possible sources (dictionary, thesaurus, corpora,
or a mix of all this) and many ways of build-
ing it. Since our main goal is the building of
an index based on the notion of word relations
(triples composed of two terms and a link), the
two prime candidates are of course corpora and
association lists like the ones collected by psy-
chologists. While the former are raw data, con-
taining the information in a more or less hidden
form, the latter (often) contain the data explicitely,
but they are scarce, subject to change, and some of
the links are questionable.5
Corpora: Concerning the resource the follow-
ing points deserve consideration: size, representa-
tivity and topic sensitivity.
? Size or coverage: While size or coverage are
critical variables, they should not be overem-
phasized though, trading quantity against
quality. We need to define the meaning of
quality here, and whether, when or how lack
of quality can be (partially) compensated by
quantity. In other words, we need to define
thresholds. In the absence of clear guidelines
it is probably wise to strive for a good bal-
ance between the two, which again assumes
that we know what quality means.
? Representativity: Obviously, the system we
have in mind is only as good as the data we
use, i.e. the purity/accuracy and represen-
tativity of the word/feature-association lists.
5This flaw is due to the experimental protocol. Subjects
are asked to give the first word coming to their mind right
after a stimulas. Not having been asked to specify the link it
is the experimenter who does so. Yet, many word pairs,? say,
cat and dog,? allow for various links (love, tease, chase, etc.),
and it is not obvious at all which is the one intended by the
user. This problem could have been avoided to a large extent
if the instruction had been, ?build a sentence containing the
following word?. Another potential problem may be due to
the distance between the source and the target word: the link
may be mediated.
No single set of data (dictionary, corpus, the-
saurus) will ever suffice to capture the knowl-
edge people have. While it would be unreal-
istic to try to model the semantic map of ev-
eryone, it is not unreasonable to try to reach
an average user, say someone who has been
to school and is a computer literate. If we
want to capture the world-knowledge of this
kind of user (target), than we must beware
that it is contained in the material we use,
since our resource will be based on this data.
Hence, taking as corpus only the newspapers
read by an elite (say, Le Monde, in France),
will surely not suffice to capture the informa-
tion we need, as it will not relate information
ordinary citizens, say sport fans, are famil-
iar with or interested in. In sum, we need to
take a wide variety of sources to extract then
the needed information. While there is short-
age of some document types needed, there
are nevertheless quite a few sources one may
consider to begin with: Wikipedia, domain
taxonomies, topic signatures, (Lin and Hovy,
2000), a database like (http://openrdf.org),
etc.
? Topic sensitivity
Weights are important, but they tend to
change dynamically with time and the topic.
Think of the word ?piano? uttered in the con-
texts of a ?concert? or ?household moving?. It
is only in this latter case that this term evokes
ideas like size or weight. The dynamic re-
compution of weights as a function of topic
changes requires that the system be able to
recognize the topic changes, as otherwise it
might mislead the user by providing of in-
adequate weights. For some initial work see
(Ferret and Zock, 2006).
Association lists: Psychologists have built such
lists already decades ago (Deese, 1965; Schvan-
eveldt, 1989). Similar lists are nowadays freely
available on the web. For example, for English
there is the Edinburgh Associative Thesaurus 6
and the compilation done by Nelson and his col-
leagues in Florida 7. There are also some re-
6http://www.eat.rl.ac.uk/
7http://cyber.acomp.usf.edu/FreeAssociation/
77
sources for German (see 8 or 9), for Japanese,10
and probably many other languages.
While association lists are generally built man-
ually, one can also try to do so automatically or
with the help of people (see section 5 in (Zock
and Bilac, 2004)). JeuxdeMot (JdM), a collec-
tively built resource focusing on French being an
example in case.11
2.3 Searching
The goal of searching is more complex than one
might think. Of course, ultimately one should find
the object one is looking for,12 but the very pro-
cess should also be carried out quickly and natu-
rally. In addition we want to allow for recovery in
case of having taken the wrong turn, and we want
to avoid looping, that is, walking in circles, with-
out ever getting closer to the goal. Last, but not
least we want to make sure that stored informa-
tion can also be accessed.
That this is less obvious than it might seem at
first sight has been shown by (Zock and Schwab,
2008). Taking two resources (WN and Wikipedia)
that contain both a given target word, we wanted
to see whether we could access it or not. The
target word was ?vintage?. In order to find it we
provided two access keys, i.e. trigger words:
?wine? and ?harvest?. Combining the two produced
a list of 6 items in the case of WN and 45 in the
case of Wikipedia, yet, while the latter displayed
the target word, it was absent from the list pro-
duced by WN. This example illustrates the fact
that our claim concerning storage and acess is well
founded. Having stored something does by no
means guarantee its access.
In the next sections we will present a small ex-
periment concerning search.
3 System architecture
To allow for word access, we need at least two
components: an index, i.e. a resource, repre-
senting or encoding the way words are connected
8http://www.schulteimwalde.de/resource.html
9http://www.coli.uni-saarland.de/projects/nag/
10http://www.valdes.titech.ac.jp/ terry/jwad.html
11http://www.lirmm.fr/jeuxdemots/rezo.php
12This poses special requirements concerning the organi-
zation, indexing and ranking of the data, i.e. words. We will
not get into these issues here.
(database or semantic network encoding associa-
tive relations between words) and an efficient
search algorithm to find the needed information,
in our case, words.
In other words, since search requires a map or a
resource in which to search and a good algorithm
to perform the search, we are keen in finding out
how different resources (for example, Wikipedia,
WordNet or JeuxdeMots) and various search al-
gorithms might affect efficiency of word access.
While there is a link between (the quality of) the
resource and the searching, we will separate the
two, focusing here mainly on the search algo-
rithms and possible ways to evaluate them.

	








	






Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 52?60,
Beijing, August 2010
Ontology driven content extraction using interlingual annotation of
texts in the OMNIA project
Achille Falaise, David Rouquet, Didier Schwab, Herve? Blanchon, Christian Boitet
LIG-GETALP, University of Grenoble
{Firstname}.{Lastname}@imag.fr
Abstract
OMNIA is an on-going project that aims
to retrieve images accompanied with
multilingual texts. In this paper, we pro-
pose a generic method (language and do-
main independent) to extract conceptual
information from such texts and sponta-
neous user requests. First, texts are la-
belled with interlingual annotation, then
a generic extractor taking a domain on-
tology as a parameter extract relevant
conceptual information. Implementation
is also presented with a first experiment
and preliminary results.
1 Introduction
The OMNIA project (Luca Marchesotti et al,
2010) aims to retrieve images that are described
with multilingual free companion texts (cap-
tions, comments, etc.) in large Web datasets.
Images are first classified with formal descrip-
tors in a lightweight ontology using automatic
textual and visual analysis. Then, users may ex-
press spontaneous queries in their mother tongue
to retrieve images. In order to build both formal
descriptors and queries for the ontology, a con-
tent extraction in multilingual texts is required.
Multilingual content extraction does not im-
ply translation. It has been shown in (Daoud,
2006) that annotating words or chunks with in-
terlingual lexemes is a valid approach to initiate
a content extraction. We thus skip syntactical
analysis, an expensive and low quality process,
and get language-independent data early in our
flow, allowing further treatments to be language-
independent. We use the lightweight ontology
for image classifications as the formal knowl-
edge representation tha determines relevant in-
formation to extract. This ontology is considered
as a domain parameter for the content extractor.
We are testing this method on a database pro-
vided for the image retrieval challenge CLEF09
by the Belgium press agency Belga. The
database contains 500K images with free com-
panion texts of about 50 words (about 25M
words in total). The texts in the database are in
English only, and we ?simulate? multilinguism
with partially post-edited machine translation.
The rest of the paper is organized as fol-
low. We first depict our general architecture de-
ployed for CLIA and then detail the various pro-
cesses involved : interlingual annotation, con-
ceptual vector based disambiguation and ontol-
ogy driven content extraction. We conclude
with the first results of experimentations on the
CLEF09 data.
2 General architecture
2.1 General process
In our scenario, there are two types of tex-
tual data to deal with : companion texts in the
database (captions), but also user requests. The
two are processed in a very similar way.
The general architecture is depicted in figure
1. The main components, that will be described
in detail, may be summarized as follows:
? Texts (both companions and requests) are
first lemmatised with a language-dependent
piece of software. Ambiguities are pre-
served in a Q-graph structure presented in
section 3.1.2.
52
ConceptsQ-Graph Conceptextraction Lemmatisation Disamb
Companiontexts
NL Requests
NL-UWdictionnary UW-ConceptMap Ontology
Interlingualannotation
Figure 1: General architecture of CLIA in the OMNIA project
? Then, the lemmatised texts are annotated
with interlingual (ideally unambiguous)
lexemes, namely Universal Words (UW)
presented in section 3.1.1. This adds a lot
of ambiguities to the structure, as an ac-
tual lemma may refer to several semanti-
cally different lexemes.
? The possible meanings for lemmas are then
weighted in the Q-graph through a disam-
biguation process.
? Finally, relevant conceptual information is
extracted using an alignment between a do-
main ontology and the interlingual lexemes.
The conceptual information in the output may
adopt different shapes, such as a weighted con-
ceptual vector, statements in the A-Box of the
ontology or annotations in the original text, etc.
In the case of OMNIA, conceptual informa-
tion extracted from companion texts is stored
in a database, while conceptual information ex-
tracted from users requests are transformed into
formal requests for the database (such as SQL,
SPARQL, etc.).
2.2 Implementation
The general process is implemented following a
Service Oriented Architecture (SOA). Each part
of the process corresponds to a service.
This allowed us to reuse part of existing re-
sources developed on heterogeneous platforms
using web interfaces (in the best case REST in-
terfaces (Fielding, 2000), but frequently only
HTML form-based interfaces). A service su-
pervisor has been built to deal with such an
heterogeneity and address normalization issues
(e.g. line-breaks, encoding, identification, cook-
ies, page forwarding, etc.).
This architecture is able to process multiple
tasks concurrently, allowing to deal with users
requests in real time while processing compan-
ion texts in the background.
3 Interlingual annotation
We present in this section the preliminary treat-
ments of multilingual texts (image companion
texts or user requests) that are required for
our content extraction process (Rouquet and
Nguyen, 2009a).
In order to allow a content extraction in multi-
lingual texts, we propose to represent texts with
the internal formalism of the Q-Systems and
to annotate chunks with UNL interlingual lex-
emes (UW) . Roughly, we are making an inter-
lingual lemmatisation, containing more informa-
tion than simple tagging, that is not currently
proposed by any lemmatisation software.
3.1 Resources and data structures
3.1.1 The Universal Network Language
UNL (Boitet et al, 2009; Uchida Hiroshi et
al., 2009) is a pivot language that represents the
meaning of a sentence with a semantic abstract
structure (an hyper-graph) of an equivalent En-
glish sentence.
The vocabulary of UNL consists in a set of
Universal Words (UW). An UW consists of:
1. a headword, if possible derived from En-
glish, that can be a word, initials, an expres-
sion or even an entire sentence. It is a label
for the concepts it represents in its original
language ;
2. a list of restrictions that aims to precisely
specify the concept the UW refers to. Re-
strictions are semantic relations with other
53
UW. The most used is the ?icl? relation that
points to a more general UW.
Examples :
? book(icl>do, agt>human, obj>thing)
and book(icl>thing).
Here, the sense of the headword is focused
by the attributes.
? ikebana(icl>flower arrangement).
Here, the headword comes from Japanese.
? go down.
Here, the headword does not need any re-
finement.
Ideally, an UW refers unambiguously to a con-
cept, shared among several languages. However,
UW are designed to represent acceptions in a
language ; we therefore find distinct UW that
refer to the same concept as for ?affection? and
?disease?.
We are mainly using the 207k UW built by the
U++ Consortium (Jesus Carden?osa et al, 2009)
from the synsets of the Princeton WordNet, that
are linked to natural languages via bilingual dic-
tionaries. The storage of these dictionaries can
be supported by a suitable platform like PIVAX
(Nguyen et al, 2007) or a dedicated database.
The gain of a pivot language is illustrated in fig-
ure 2. If we want to add a new language in the
multilingual system, we just need to create the
links with the pivot but not with all the other lan-
guages.
3.1.2 The Q-Systems
We can think of inserting the UW annotations
with tags (e.g. XML) directly along the source
text as in table 1. However, this naive approach is
not adequate to represent the segmentation am-
biguities that can occur in the text interpretation
(in the example of table 1, we list the different
possible meanings for ?in?, but cannot represent
?waiting?, ?room? and ?waiting room? as three
possible lexical units).
In order to allow the representation of segmen-
tation and other ambiguities, that can occur in
a text interpretation, we propose to use the Q-
Systems. They represent texts in an adequate
Interlingual
UW volume
French 
volume
English 
volume
Chinese
 volume
Figure 2: Multilingual architecture with a pivot
in a waiting room
<tag uw=?in(icl-sup-how),
in(icl-sup-adj),
in(icl-sup-linear unit,
equ-sup-inch)?>in</tag>
<tag uw=?unk?>a</tag> <tag
uw=?waiting room(icl-sup-room,
equ-sup-lounge)?>waiting
room</tag>
Table 1: Naive annotation of a text fragment
graph structure decorated with bracketed expres-
sions (trees) and, moreover, allow processing on
this structure via graph rewriting rules (a set of
such rewriting rules is a so called Q-System).
An example of the Q-System formalism is
given in figure 3 of section 3.2.3. It presents
successively : the textual input representing a Q-
graph, a rewriting rule and a graphical view of
the Q-graph obtained after the application of the
rule (and others).
The Q-Systems were proposed by Alain
Colmeraurer at Montreal University (Colmer-
auer, 1970). For our goal, they have three main
advantages :
? they provide the formalized internal struc-
ture for linguistic portability that we men-
tioned in the introduction (Hajlaoui and
Boitet, 2007) ;
? they unify text processing with powerful
graph rewriting systems ;
54
? they allow the creation or the edition of
a process by non-programmers (e.g. lin-
guists) using SLLP (Specialized Language
for Linguistic Programming).
We are actually using a reimplementation of
the Q-Systems made in 2007 by Hong-Thai
Nguyen during his PhD in the LIG-GETALP
team (Nguyen, 2009).
3.2 Framework of the annotation process
3.2.1 Overview
The annotation process is composed by the
following steps :
1. splitting the text in fragments if too long ;
2. lemmatisation with a specialized software ;
3. transcription to the Q-Systems format ;
4. creation of local bilingual dictionaries
(source language - UW) for each fragment
with PIVAX ;
5. execution of those dictionaries on the frag-
ments ;
3.2.2 Lemmatisation
As we want to use dictionaries where entries
are lemmas, the first step is to lemmatise the in-
put text (i.e. to annotate occurrences with possi-
ble lemmas). This step is very important because
it although gives the possible segmentations of
the text in lexical units. It brings two kinds of
ambiguities into play : on one hand, an occur-
rence can be interpreted as different lemmas, on
the other, there can be several possible segmen-
tations (eventually overlapping) to determine the
lexical units.
For content extraction or information retrieval
purpose, it is better to preserve an ambiguity than
to badly resolve it. Therefore we expect from a
lemmatiser to keep all ambiguities and to repre-
sent them in a confusion network (a simple tag-
ger is not suitable). Several lemmatiser can be
used to cover different languages. For each of
them, we propose to use a dedicated ANTLR
grammar (Terence Parr et al, 2009) in order to
soundly transform the output in a Q-graph.
To process the Belga corpus, we developed a
lemmatiser that produce natively Q-graphs. It
is based on the morphologic dictionary DELA1
available under LGPL licence.
3.2.3 Local dictionaries as Q-Systems
Having the input text annotated with lemmas,
with the Q-System formalism, we want to use the
graph rewriting possibilities to annotate it with
UW. To do so, we use PIVAX export features to
produce rules that rewrite a lemma in an UW (see
figure 3). Each rule correspond to an entry in
the bilingual dictionary. To obtain a tractable Q-
Systems (sets of rules), we built local dictionar-
ies that contain the entries for fragments of the
text (about 250 words in the first experiment).
Figure 3: Creation and execution of a Q-System
Considering the significant quantity of ambi-
guities generated by this approach (up to a dozen
UW for a single word), we need to include a
disambiguation process. This process, based on
conceptual vectors, is presented in the next sec-
tion.
4 Conceptual vector based
disambiguation
Vectors have been used in NLP for over 40 years.
For information retrieval, the standard vector
model (SVM) was invented by Salton (Salton,
1991) during the late 60?s, while for meaning
representation, latent semantic analysis (LSA)
1http://infolingu.univ-mlv.fr/DonneesLinguistiques/
Dictionnaires/telechargement.html
55
was developed during the late 80?s (Deerwester
et al, 1990). These approaches are inspired
by distributional semantics (Harris et al, 1989)
which hypothesises that a word meaning can be
defined by its co-text. For example, the mean-
ing of ?milk? could be described by {?cow?, ?cat?,
?white?, ?cheese?, ?mammal?, . . . }. Hence, distribu-
tional vector elements correspond directly (for
SVM) or indirectly (for LSA) to lexical items
from utterances.
The conceptual vector model is different as it
is inspired by componential linguistics (Hjelm-
lev, 1968) which holds that the meaning of words
can be described with semantic components.
These can be considered as atoms of meaning
(known as primitives (Wierzbicka, 1996)), or
also only as constituents of the meaning (known
as semes, features (Greimas, 1984), concepts,
ideas). For example, the meaning of ?milk?
could be described by {LIQUID, DAIRY PRODUCT, WHITE,
FOOD, . . .}. Conceptual vectors model a formal-
ism for the projection of this notion in a vectorial
space. Hence, conceptual vector elements corre-
spond to concepts indirectly, as we will see later.
For textual purposes2, conceptual vectors can
be associated to all levels of a text (word, phrase,
sentence, paragraph, whole texts, etc.). As they
represent ideas, they correspond to the notion of
semantic field3 at the lexical level, and to the
overall thematic aspects at the level of the entire
text.
Conceptual vectors can also be applied to
lexical meanings. They have been studied in
word sense disambiguation (WSD) using iso-
topic properties in a text, i.e. redundancy of ideas
(Greimas, 1984). The basic idea is to maximise
the overlap of shared ideas between senses of
lexical items. This can be done by computing the
angular distance between two conceptual vectors
(Schwab and Lafourcade, 2007).
In our case, conceptual vectors are used for
automatic disambiguation of texts. Using this
method, we calculate confidence score for each
UW hypothesis appearing in the Q-Graph.
2Conceptual vectors can be associated with any content,
not only text: images, videos, multimedia, Web pages, etc.
3The semantic field is the set of ideas conveyed by a
term.
5 Ontology driven content extraction
The content extraction has to be leaded by a
?knowledge base? containing the informations
we want to retrieve.
5.1 Previous works in content extraction
This approach has its roots in machine trans-
lation projects such as C-Star II (1993-1999)
(Blanchon and Boitet, 2000) and Nespole!
(2000-2002) (Metze et al, 2002), for on the fly
translation of oral speech acts in the domain of
tourism. In these projects, semantic transfer was
achieved through an IF (Inter-exchange Format),
that is a semantic pivot dedicated to the domain.
This IF allows to store information extracted
from texts but is although used to lead the con-
tent extraction process by giving a formal repre-
sentation of the relevant informations to extract,
according to the domain.
The Nespole! IF consists of 123 concepts
from the tourism domain, associated with sev-
eral arguments and associable with speech acts
markers. The extraction process is based on pat-
terns. As an example, the statement ?I wish a
single room from September 10th to 15th? may
be represented as follows:
{ c:give-information+disposition+room
( disposition=(desire, who=i),
room-spec=
( identifiability=no,single_room ),
time=
( start-time=(md=10),
end-time(md=15, month=9)
)
)
}
5.2 Ontologies as parameter for the domain
In the project OMNIA, the knowledge base has
the form of a lightweight ontology for image
classification 4. This ontology contains 732 con-
cepts in the following domains : animals, pol-
itics, religion, army, sports, monuments, trans-
ports, games, entertainment, emotions, etc. To
us, using an ontology has the following advan-
tages :
? Ontologies give an axiomatic description
of a domain, based on formal logics (usu-
4http://kaiko.getalp.org/kaiko/ontology/OMNIA/OMNIA current.owl
56
ally description logics (Baader et al, 2003))
with an explicit semantic. Thus, the knowl-
edge stored in them can be used soundly by
software agents;
? Ontological structures are close to the or-
ganisation of ideas as semantic networks in
human mind (Aitchenson, 2003) and are la-
beled with strings derived from natural lan-
guages. Thus humans can use them (brows-
ing or contributing) in a pretty natural way;
? Finally, with the advent of the Semantic
Web and normative initiatives such as the
W3C5, ontologies come with a lot of shared
tools for editing, querying, merging, etc.
As the content extractor might only process
UW annotations, it is necessary that the knowl-
edge base is whether expressed using UW or
linked to UW. The ontology is here considered
as a domain parameter of content extraction
and can be changed to improve preformances
on specific data collections. Therefore, given
any OWL ontology6, we must be able to link it
with a volume of UW considering the following
constraints :
Creating manually such correspondences
is costly due to the size of resources so an
automatic process is requiered.
Ontologies and lexicons evolve over the time
so an alignment must be adaptable to incremen-
tal evolutions of resources.
The correspondences must be easily manip-
ulated by users so they can manually improve
the quality of automatically created alignments
with post-edition.
Constructing and maintaining an alignment
between an ontology and an UW lexicon is a
challenging task (Rouquet and Nguyen, 2009b).
Basically, any lexical resource can be repre-
sented in an ontology language as a graph. We
propose to use an OWL version of the UW vol-
ume available on Kaiko website 7. It allows us
5http://www.w3.org/
6http://www.w3.org/2004/OWL/
7http://kaiko.getalp.org
to benefit of classical ontology matching tech-
niques and tools (Euzenat and Shvaiko, 2007)
to represent, compute and manipulate the align-
ment. We implemented two string based match-
ing techniques on top of the alignment API (Eu-
zenat, 2004). Specific disambiguation methods
are in development to improve the alignment
precision. Some of them are based on conceptual
vectors presented in section 4, others will adapt
structural ontology matching techniques. This
approach to match an ontology with a lexical re-
source is detailled in (Rouquet et al, 2010).
5.3 The generic extractor
In the case of the OMNIA project, the system
output format is constraint by the goal of an in-
tegration with visual analysis results, in a larger
multimodal system. The visual analysis systems
are also based on concept extraction, but does not
need an ontology to organise concepts. There-
fore, our results has to remain autonaumous,
which means without references to the ontology
used to extract concepts. So, we use a simple
concept vector as output, with intensity weights;
practically, a simple data-value pairs sequence
formatted in XML.
Concept extraction is achieved through a 3
steps process, has shown in figure 4.
1. Concept matching: each UW in the Q-
Graph, that matches a concept according to
the UW-concept map, is labelled with this
concept.
2. Confidence calculation: each concept la-
bel is given a confidence score, in accor-
dance with the score of the UW carrying the
concept, obtained after disambiguation, and
pondered according to the number of UWs
in the Q-Graph. It is planed to take into ac-
count a few linguistics hints here, such as
negations, and intensity adverbs.
3. Score propagation: because we need au-
tonomous results, we have to perform all
ontology-based calculation before releasing
them. The confidence scores are propagated
in the ontology concept hierarchy: for each
57
labelled concept, its score is added to the
super-concept, and so on.
The ontology and the derivated UW-concept
map are considered as parameters for the treat-
ments, and may be replaced in accordance with
the domain, and the relevance of the concepts
and their hierarchy, according to the task.
ConceptnstsQ-sG-Qreon
ConrtnrtarhQsreons xiQrs enL
msohtxhoxQLQreon
DbNhQx 
ConstxrR
qubConstxrUQx
Wnro-oLd
Figure 4: Detail of concept extraction.
6 Experiments
For a first experiment, we used a small dataset,
containing:
? a sub-corpus of 1046 English companion
texts from CLEF09 corpus (press pictures
and captions of about 50 words),
? a 159 concepts ontology, designed for pic-
ture and emotions depiction,
? a UW-concept map comprising 3099 UW.
It appeared that, with this parameters, con-
cepts where extracted for only 25% of the texts.
This preliminary result stressed the importance
of recall for such short texts. However, there
were many ways to improve recall in the system:
? improve the ontology, in order to better
cover the press domain;
? significantly increase the quantity of UW
linked to concepts (only 3099 obtained for
this experiment), by considering synonyms
during the linking process;
? using UW restrictions during concept
matching for UW that are not directly
linked to a concept, as these restrictions are
a rich source of refined semantic informa-
tion.
A second experiment with an improved on-
tology, including 732 concepts, and the use of
UW restrictions, showed very promising results.
Concepts were retrieved from 77% of texts. The
remaining texts were very short (less than 10
words, sometime just date or name).
For example, we extracted the following con-
cepts from the picture and companion text repro-
duced in figure 5.
CoCncepetnntnstQepe-CGraCahexiC eLexmDbNeRquUWedNyeMWOUmeDelqmymDNyeqgexmDbNeImUdNOUWye?OODuerMddUNWeNWeDeRDNyNW?mqqueNWe-D?ODO?deD??WdMme?dlNyD?tQe?lyUu?metnnt??OODuerMddUNWeNdeOq?UO?elMmdMNW?ey?UeOU??qluUWyeqgeRUDlqWdeqgeuDddeOUdymM?NqWeDWOeRN?eOqe?de?Udyeyqe?NOUey?uegmque?eNWdlU?qmdhey?e-mNyNd???mWuUWye?DNuUOeNWeDeccplD?eOqddNUmeuDOUelM????dye?qMmde?UgqmUeDedlU?D?erqMdUeqge?quuqWdeOU?DyUeqWexmDb?xmDbNe?M?MmUe?NWNdyUmerDuDOe?MddUgerDuuDONe???Oey?e-mNyNd?D?U?yNqWde??DdU?dd??eeeee?CeIr??C?xeCoCaeCoCa
Figure 5: Picture document and companion text
example.
CONCEPT WEIGHT
BUILDING 0.098
HOSPITAL 0.005
HOUSE 0.043
MINISTER 0.016
OTHER BUILDING 0.005
PEOPLE 0.142
PERSON 0.038
POLITICS 0.032
PRESIDENT 0.016
RESIDENTIAL BUILDING 0.043
WOMAN 0.005
As this results were more consistent, we could
have a preliminary survey about precision, on a
30 texts sample. While disambiguation imple-
mentation is still at an early stage, weights were
not yet taken into account. A concept match can
be considered correct following two criterons :
1. Visual relevance considers a concept as
correct if carried by an element of the pic-
ture; for instance, the match of concept
58
?SPORT? is regarded as correct for a pic-
ture containing a minister of sports, even if
not actually performing any sport.
2. Textual relevance considers a concept as
correct if carried by a word of the text,
as parts of texts may involve concepts that
are not actually present in the picture, such
as contextual information, previous events,
etc.
124 concepts were found in 23 texts (7 texts had
no concept match):
1. 99 concepts were correct according to the
visual relevance,
2. 110 were correct according to the textual
relevance,
3. 14 were totally incorrect.
We thus have an overall precision score of 0.798
according to the visual relevance and 0.895 ac-
cording to the textual relevance. Most of the er-
rors where caused by ambiguity problems, and
may be addressed with disambiguation process
that are not fully implemented yet.
7 Conclusion and perspectives
We exposed a generic system designed to extract
content (in the form of concepts) from multi-
lingual texts. Our content extraction process is
generic regarding to two aspects :
? it is language independent, as it process an
interlingual representation of the texts
? the content to be extracted can be specified
using a domain ontology as a parameter
This is an ongoing work, and disambiguation
through conceptual vectors is expected to im-
prove accuracy, giving significant weights to the
hypothetical meanings of words.
In the long run, we will focus on integration
with visual content extractors, speed optimiza-
tion to achieve a real-time demonstrator and de-
tailled evaluation of the method.
References
Aitchenson, J. 2003. Words in the Mind. An Intro-
duction to the Mental Lexicon. Blackwell Publish-
ers.
Baader, De Franz, Diego Calvanese, Deborah
McGuinness, Peter Patel-Schneider, and Daniele
Nardi. 2003. The Description Logic Handbook.
Cambridge University Press.
Blanchon, H. and C. Boitet. 2000. Speech translation
for french within the C-STAR II consortium and
future perspectives. In Proc. ICSLP 2000, pages
412?417, Beijing, China.
Boitet, Christian, Igor Boguslavskij, and Jesus
Carden?osa. 2009. An evaluation of UNL usabil-
ity for high quality multilingualization and projec-
tions for a future UNL++ language. In Computa-
tional Linguistics and Intelligent Text Processing,
pages 361?373.
Colmerauer, A. 1970. Les syste`mes-q ou un for-
malisme pour analyser et synthe?tiser des phrases
sur ordinateur. de?partement d?informatique de
l?Universite? de Montre?al, publication interne, 43,
September.
Daoud, Daoud. 2006. Il faut et on peut constru-
ire des syste`mes de commerce e?lectronique a` inter-
face en langue naturelle restreints (et multilingues)
en utilisant des me?thodes oriente?es vers les sous-
langages et le contenu. Ph.D. thesis, UJF, Septem-
ber.
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A.
Harshman. 1990. Indexing by latent semantic
analysis. Journal of the American Society of In-
formation Science, 41(6).
Euzenat, Je?ro?me and Pavel Shvaiko. 2007. Ontology
matching. Springer, Heidelberg (DE).
Euzenat, Je?ro?me. 2004. An API for ontology align-
ment. In Proceedings of the 3rd International
Semantic Web Conference, pages 698?7112, Hi-
roshima, Japan.
Fielding, Roy T. 2000. Architectural styles and the
design of network-based software architectures.
Ph.D. thesis, University of California.
Greimas, Algirdas Julien. 1984. Structural Seman-
tics: An Attempt at a Method. University of Ne-
braska Press.
Hajlaoui, Najeh and Christian Boitet. 2007. Portage
linguistique d?applications de gestion de contenu.
In TOTh07, Annecy.
59
Harris, Zellig S., Michael Gottfried, Thomas Ryck-
man, Paul Mattick Jr., Anne Daladier, T.N. Har-
ris, and S. Harris. 1989. The form of Information
in Science, Analysis of Immunology Sublanguage,
volume 104 of Boston Studies in the Philosophy of
Science. Kluwer Academic Publisher, Dordrecht.
Hjelmlev, Louis. 1968. Prole?gole`me a` une the?orie
du langage. e?ditions de minuit.
Jesus Carden?osa et al 2009. The U++ con-
sortium (accessed on september 2009).
http://www.unl.fi.upm.es/consorcio/index.php,
September.
Luca Marchesotti et al 2010. The Omnia project
(accessed on may 2010). http://www.omnia-
project.org, May.
Max Silberztein. 2009. NooJ linguistic
software (accessed on september 2009).
http://www.nooj4nlp.net/pages/nooj.html,
September.
Metze, F., J. McDonough, H. Soltau, A. Waibel,
A. Lavie, S. Burger, C. Langley, L. Levin,
T. Schultz, F. Pianesi, R. Cattoni, G. Lazzari,
N. Mana, and E. Pianta. 2002. The Nespole!
speech-to-speech translation system. In Proceed-
ings of HLT-2002 Human Language Technology
Conference, San Diego, USA, march.
Nguyen, H.T., C. Boitet, and G. Se?rasset. 2007. PI-
VAX, an online contributive lexical data base for
heterogeneous MT systems using a lexical pivot.
In SNLP, Bangkok, Thailand.
Nguyen, Hong-Thai. 2009. EMEU w,
a simple interface to test the Q-
Systems (accessed on september 2009).
http://sway.imag.fr/unldeco/SystemsQ.po?localhost=
/home/nguyenht/SYS-Q/MONITEUR/, Septem-
ber.
Rouquet, David and Hong-Thai Nguyen. 2009a.
Interlingual annotation of texts in the OMNIA
project. Poznan, Poland.
Rouquet, David and Hong-Thai Nguyen. 2009b.
Multilingu??sation d?une ontologie par des core-
spondances avec un lexique pivot. In TOTh09, An-
necy, France, May.
Rouquet, David, Cassia Trojahn, Didier Scwab, and
Gilles Se?rasset. 2010. Building correspondences
between ontologies and lexical resources. In to be
published.
Salton, Gerard. 1991. The Smart document re-
trieval project. In Proc. of the 14th Annual Int?l
ACM/SIGIR Conf. on Research and Development
in Information Retrieval, Chicago.
Schwab, Didier and Mathieu Lafourcade. 2007. Lex-
ical functions for ants based semantic analysis. In
ICAI?07- The 2007 International Conference on
Artificial Intelligence, Las Vegas, Nevada, USA,
juin.
Terence Parr et al 2009. ANTLR parser
generator (accessed on september 2009).
http://www.antlr.org/, September.
Uchida Hiroshi et al 2009. The UNDL
foundation (accessed on september 2009).
http://www.undl.org/, September.
Wierzbicka, Anna. 1996. Semantics: Primes and
Universals. Oxford University Press.
60
