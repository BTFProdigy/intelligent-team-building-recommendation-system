Proceedings of the ACL Student Research Workshop, pages 103?109,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Comparison of Techniques to Automatically Identify Complex Words
Matthew Shardlow
School of Computer Science, University of Manchester
IT301, Kilburn Building, Manchester, M13 9PL, England
m.shardlow@cs.man.ac.uk
Abstract
Identifying complex words (CWs) is an
important, yet often overlooked, task
within lexical simplification (The process
of automatically replacing CWs with sim-
pler alternatives). If too many words are
identified then substitutions may be made
erroneously, leading to a loss of mean-
ing. If too few words are identified then
those which impede a user?s understand-
ing may be missed, resulting in a com-
plex final text. This paper addresses the
task of evaluating different methods for
CW identification. A corpus of sentences
with annotated CWs is mined from Sim-
ple Wikipedia edit histories, which is then
used as the basis for several experiments.
Firstly, the corpus design is explained and
the results of the validation experiments
using human judges are reported. Exper-
iments are carried out into the CW identi-
fication techniques of: simplifying every-
thing, frequency thresholding and training
a support vector machine. These are based
upon previous approaches to the task and
show that thresholding does not perform
significantly differently to the more na??ve
technique of simplifying everything. The
support vector machine achieves a slight
increase in precision over the other two
methods, but at the cost of a dramatic trade
off in recall.
1 Introduction
Complex Word (CW) identification is an impor-
tant task at the first stage of lexical simplification
and errors introduced or avoided here will affect
final results. This work looks at the process of au-
tomatically identifying difficult words for a lexi-
cal simplification system. Lexical simplification
is the task of identifying and replacing CWs in a
text to improve the overall understandability and
readability. This is a difficult task which is com-
putationally expensive and often inadequately ac-
curate.
Lexical simplification is just one method of
text simplification and is often deployed alongside
other simplification methods (Carrol et al, 1998;
Alu??sio and Gasperin, 2010). Syntactic simplifi-
cation, statistical machine translation and seman-
tic simplification (or explanation generation) are
all current methods of text simplification. Text
simplification is typically deployed as an assistive
technology (Devlin and Tait, 1998; Alu??sio and
Gasperin, 2010), although this is not always the
case. It may also be used alongside other tech-
nologies such as summarisation to improve their
final results.
Identifying CWs is a task which every lexical
simplification system must perform, either explic-
itly or implicitly, before simplification can take
place. CWs are difficult to define, which makes
them difficult to identify. For example, take the
following sentence:
The four largest islands are Honshu,
Hokkaido, Shikoku, and Kyushu, and
there are approximately 3,000 smaller
islands in the chain.
In the above sentence, we might identify the
proper nouns (Honshu, Hokkaido, etc.) as com-
plex (as they may be unfamiliar) or we may choose
to discount them from our scheme altogether, as
proper nouns are unlikely to have any valid re-
placements. If we discount the proper nouns then
the other valid CW would be ?approximately?. At
13 characters it is more than twice the average of
5.7 characters per word and has more syllables
than any other word. Further, CWs are often iden-
tified by their frequency (see Section 2.1) and here,
103
?approximately? exhibits a much lower frequency
than the other words.
There are many reasons to evaluate the identi-
fication of CWs. This research stems primarily
from the discovery that no previous comparison of
current techniques exists. It is hoped that by pro-
viding this, the community will be able to iden-
tify and evaluate new techniques using the meth-
ods proposed herein. If CW identification is not
performed well, then potential candidates may be
missed, and simple words may be falsely identi-
fied. This is dangerous as simplification will often
result in a minor change in a text?s semantics. For
example, the sentence:
The United Kingdom is a state in
northwest Europe.
May be simplified to give:
The United Kingdom is a country in
northwest Europe.
In this example from the corpus used in this
research, the word ?state? is simplified to give
?country?. Whilst this is a valid synonym in the
given context, state and country are not necessar-
ily semantically identical. Broadly speaking, state
refers to a political entity, whereas country refers
to a physical space within a set of borders. This is
an acceptable change and even necessary for sim-
plification. However, if applied blindly, then too
many modifications may be made, resulting in ma-
jor deviations from the text?s original semantics.
The contributions of this paper are as follows:
? A report on the corpus developed and used in
the evaluation phase. Section 2.2.
? The implementation of a support vector ma-
chine for the classification of CWs. Section
2.6
? A comparison of common techniques on the
same corpus. Section 4.
? An analysis of the features used in the sup-
port vector machine. Section 4.
2 Experimental Design
Several systems for detecting CWs were imple-
mented and evaluated using the CW corpus. The
two main techniques that exist in the literature
are simplifying everything (Devlin and Tait, 1998)
System Score
SUBTLEX 0.3352
Wikipedia Baseline 0.3270
Kucera-Francis 0.3097
Random Baseline 0.0157
Table 1: The results of different exper-
iments on the SemEval lexical simplifi-
cation data. These show that SUBTLEX
was the best word frequency measure for
rating lexical complexity. The other en-
tries correspond to alternative word fre-
quency measures. The Google Web 1T
data (Brants and Franz, 2006) has been
shown to give a higher score, however this
data was not available during the course
of this research.
and frequency based thresholding (Zeng et al,
2005). These were implemented as well as a sup-
port vector machine classifier. This section de-
scribes the design decisions made during imple-
mentation.
2.1 Lexical Complexity
All three of the implementations described in Sec-
tions 2.4, 2.5 and 2.6 require a word frequency
measure as an indicator of lexical complexity. If a
word occurs frequently in common language then
it is more likely to be recognised (Rayner and
Duffy, 1986).
The lexical simplification dataset from Task 1
at SemEval 2012 (De Belder and Moens, 2012)
was used to compare several measures of word
frequency as shown in Table 1. Candidate sub-
stitutions and sample sentences were provided by
the task organisers, together with a gold standard
ranking of the substitutes according to their sim-
plicity. These sentences were ranked according
to their frequency. Although the scores in Table
1 appear to be low, this is the kappa agreement
for several categories and so should be expected.
The inter-annotator agreement on the corpus was
0.488 (De Belder and Moens, 2012). The SUB-
TLEX dataset (Brysbaert and New, 2009) was the
best available for rating word familiarity. This is
a corpus of over 70,000 words collected from the
subtitles of over 8,000 American English films.
104
2.2 CW Corpus
Simple Wikipedia edit histories were mined using
techniques similar to those in Yatskar et al (2010).
This provided aligned pairs of sentences which
had just one word simplified. Whereas Yatskar
et al (2010) used these pairs to learn probabili-
ties of paraphrases, the research in this paper used
them as instances of lexical simplification. The
original simplifications were performed by editors
trying to make documents as simple as possible.
The CW is identified by comparison with the sim-
plified sentence. Further information on the pro-
duction of the corpus will be published in a future
paper.
2.3 Negative Examples
The CW corpus provides a set of CWs in appro-
priate contexts. This is useful for evaluation as
these words need to be identified. However, if
only examples of CWs were available, it would be
very easy for a technique to overfit ? as it could
just classify every single word as complex and
get 100% accuracy. For example, in the case of
thresholding, if only examples of CWs are avail-
able, the threshold could be set artificially high
and still succeed for every case. When this is ap-
plied to genuine data it will classify every word it
encounters as complex, leading to high recall but
low precision.
To alleviate this effect, negative examples are
needed. These are examples of simple words
which do not require any further simplification.
There are several methods for finding these, in-
cluding: selecting words from a reference easy
word list; selecting words with high frequencies
according to some corpus or using the simplified
words from the second sentences in the CW cor-
pus. The chosen strategy picked a word at random
from the sentence in which the CW occurs. Only
one word was edited in this sentence and so the
assumption may be made that none of the other
words in the sentence require further simplifica-
tion. Only one simple word per CW is chosen to
enforce an even amount of positive and negative
data. This gave a set of negative words which were
reflective of the broad language which is expected
when processing free text.
2.4 Simplify Everything
The first implementation involved simplifying ev-
erything, a brute force method, in which a simpli-
fication algorithm is applied to every word. This
assumes that words which are already simple will
not require any further simplification. A com-
mon variation is to limit the simplification to some
combination of all the nouns, verbs and adjectives.
A standard baseline lexical simplification sys-
tem was implemented following Devlin and Tait
(1998). This algorithm generated a set of syn-
onyms from WordNet and then used the SUB-
TLEX frequencies to find the most frequent syn-
onym. If the synonym was more frequent than the
original word then a substitution was made. This
technique was applied to all the words. If a CW
was changed, then it was considered a true posi-
tive; if a simple word was not changed, it was con-
sidered a true negative. Five trials were carried out
and the average accuracy and standard deviation is
reported in Figure 1 and Table 3.
2.5 Frequency Thresholding
The second technique is frequency thresholding.
This relies on each word having an associated fa-
miliarity value provided by the SUBTLEX corpus.
Whilst this corpus is large, it will never cover ev-
ery possible word, and so words which are not en-
countered are considered to have a frequency of 0.
This does not affect comparison as the infrequent
words are likely to be the complex ones.
To distinguish between complex and simple
words a threshold was implemented. This was
learnt from the CW corpus by examining every
possible threshold for a training set. Firstly, the
training data was ordered by frequency, then the
accuracy1 of the algorithm was examined with the
threshold placed in between the frequency of every
adjacent pair of words in the ordered list. This was
repeated by 5-fold cross validation and the mean
threshold determined. The final accuracy of the
algorithm was then determined on a separate set
of testing data.
2.6 Support Vector Machine
Support vector machines (SVM) are statistical
classifiers which use labelled training data to pre-
dict the class of unseen inputs. The training data
consist of several features which the SVM uses
to distinguish between classes. The SVM was
chosen as it has been used elsewhere for similar
tasks (Gasperin et al, 2009; Hancke et al, 2012;
Jauhar and Specia, 2012). The use of many fea-
1The proportion of data that was correctly classified.
105
tures allows factors which may otherwise have
been missed to be taken into account. One fur-
ther advantage is that the features of an SVM can
be analysed to determine their effect on the classi-
fication. This may give some indication for future
feature classification schemes.
The SVM was trained using the LIBSVM pack-
age (Chang and Lin, 2011) in Matlab. the RBF
kernel was selected and a grid search was per-
formed to select values for the 2 parameters C and
?. Training and testing was performed on a held-
out data-set using 5-fold cross validation.
To implement the SVM a set of features was
determined for the classification scheme. Several
external libraries were used to extract these as de-
tailed below:
Frequency The SUBTLEX frequency of each
word was used as previously described in
Section 2.1.
CD Count Also from the SUBTLEX corpus. The
number of films in which a word appeared,
ranging from 0? 8, 388.
Length The word length in number of characters
was taken into account. It is often the case
that longer words are more difficult to pro-
cess and so may be considered ?complex?.
Syllable Count The number of syllables con-
tained in a word is also a good estimate of
its complexity. This was computed using a
library from the morphadorner package2.
Sense Count A count of the number of ways in
which a word can be interpreted - showing
how ambiguous a word is. This measure is
taken from WordNet (Fellbaum, 1998).
Synonym Count Also taken from WordNet, this
is the number of potential synonyms with
which a word could be replaced. This again
may give some indication of a word?s degree
of ambiguity.
3 Results
The results of the experiments in identifying CWs
are shown in Figure 1 and the values are given in
Table 3. The values presented are the mean of 5
trials and the error bars represent the standard de-
viation.
2http://morphadorner.northwestern.edu/
 0
 0.2
 0.4
 0.6
 0.8
 1
Everything Thresholding SVM     
Sc
ore
AccuracyF1PrecisionRecall
Figure 1: A bar chart with error bars
showing the results of the CW identifi-
cation experiments. Accuracy, F1 Score,
Precision and Recall are reported for each
measure.
Feature Coefficient
Frequency 0.3973
CD Count 0.5847
Length ?0.5661
Syllables ?0.4414
Senses ?0.0859
Synonyms ?0.2882
Table 2: The correlation coefficients for
each feature. These show the correlation
against the language?s simplicity and so
a positive correlation indicates that if that
feature is higher then the word will be
simpler.
To analyse the features of the SVM, the corre-
lation coefficient between each feature vector and
the vector of feature labels was calculated. This is
a measure which can be used to show the relation
between two distributions. The adopted labelling
scheme assigned CWs as 0 and simple words as 1
and so the correlation of the features is notionally
against the simplicity of the words.3 The results
are reported in Table 2.
4 Discussion
It is clear from these results that there is a fairly
high accuracy from all the methods. This shows
that they perform well at the task in hand, reflect-
ing the methods which have been previously ap-
plied. These methods all have a higher recall than
3i.e. A positive correlation indicates that if the value of
that feature is higher, the word will be simpler.
106
System Accuracy F1 Precision Recall
Simplify Everything 0.8207? 0.0077 0.8474? 0.0056 0.7375? 0.0084 0.9960? 0
Thresholding 0.7854? 0.0138 0.8189? 0.0098 0.7088? 0.0136 0.9697? 0.0056
SVM 0.8012? 0.0656 0.8130? 0.0658 0.7709? 0.0752 0.8665? 0.0961
Table 3: The results of classification experiments for the three systems.
precision, which indicates that they are good at
identifying the CWs, but also that they often iden-
tify simple words as CWs. This is particularly
noticeable in the ?simplify everything? method,
where the recall is very high, yet the precision is
comparatively low. This indicates that many of the
simple words which are falsely identified as com-
plex are also replaced with an alternate substitu-
tion, which may result in a change in sense.
A paired t-test showed the difference between
the thresholding method and the ?simplify ev-
erything? method was not statistically significant
(p > 0.8). Thresholding takes more data about
the words into account and would appear to be a
less na??ve strategy than blindly simplifying every-
thing. However, this data shows there is little dif-
ference between the results of the two methods.
The thresholding here may be limited by the re-
sources, and a corpus using a larger word count
may yield an improved result.
Whilst the thresholding and simplify everything
methods were not significantly different from each
other, the SVM method was significantly differ-
ent from the other two (p < 0.001). This can be
seen in the slightly lower recall, yet higher preci-
sion attained by the SVM. This indicates that the
SVM was better at distinguishing between com-
plex and simple words, but also wrongly identified
many CWs. The results for the SVM have a wide
standard deviation (shown in the wide error bars in
Figure 1) indicating a higher variability than the
other methods. With more data for training the
model, this variability may be reduced.
One important factor in the increased precision
observed in the SVM is that it used many more
features than the other methods, and so took more
information into account. Table 2 shows that these
features had varying degrees of correlation with
the data label (i.e. whether the word was simple
or not) and hence that they had varying degrees of
effect on the classification scheme.
Frequency and CD count are moderately posi-
tively correlated as may be expected. This indi-
cates that higher frequency words are likely to be
simple. Surprisingly, CD Count has a higher cor-
relation than frequency itself, indicating that this is
a better measure of word familiarity than the fre-
quency measure. However, further investigation is
necessary to confirm this.
Word length and number of syllables are mod-
erately negatively correlated, indicating that the
longer and more polysyllabic a word is, the less
simple it becomes. This is not true in every case.
For example, ?finger? and ?digit? can be used in
the same sense (as a noun meaning an appendage
of the hand). Whilst ?finger? is more commonly
used than ?digit?4, digit is one letter shorter.
The number of senses was very weakly nega-
tively correlated with word simplicity. This in-
dicates that it is not a strong indicative factor in
determining whether a word is simple or not. The
total number of synonyms was a stronger indicator
than the number of senses, but still only exhibited
weak correlation.
One area that has not been explored in this study
is the use of contextual features. Each target word
occurs in a sentence and it may be the case that
those words surrounding the target give extra in-
formation as to its complexity. It has been sug-
gested that language is produced at an even level
of complexity (Specia et al, 2012), and so simple
words will occur in the presence of other simple
words, whereas CWs will occur in the presence
of other CWs. As well as lexical contextual in-
formation, the surrounding syntax may offer some
information on word difficulty. Factors such as
a very long sentence or a complex grammatical
structure can make a word more difficult to under-
stand. These could be used to modify the familiar-
ity score in the thresholding method, or they could
be used as features in the SVM classifier.
5 Related Work
This research will be used for lexical simplifica-
tion. The related work in this field is also generally
4in the SUBTLEX corpus ?finger? has a frequency of
1870, whereas ?digit? has a frequency of 30.
107
used as a precursor to lexical simplification. This
section will explain how these previous methods
have handled the task of identifying CWs and how
these fit into the research presented in this paper.
The simplest way to identify CWs in a sentence
is to blindly assume that every word is complex, as
described earlier in Section 2.4. This was first used
in Devlin?s seminal work on lexical simplification
(Devlin and Tait, 1998). This method is some-
what na??ve as it does not mitigate the possibility
of words being simplified in error. Devlin and Tait
indicate that they believe less frequent words will
not be subject to meaning change. However, fur-
ther work into lexical simplification has refuted
this (Lal and Ru?ger, 2002). This method is still
used, for example Thomas and Anderson (2012)
simplify all nouns and verbs. This corresponds to
the ?Everything? method.
Another method of identifying CWs is to use
frequency based thresholding over word familiar-
ity scores, as described in Section 2.5 and corre-
sponding to the ?Frequency? method in this pa-
per. This has been applied to the medical domain
(Zeng et al, 2005; Elhadad, 2006) for predicting
which words lay readers will find difficult. This
has been correlated with word difficulty via ques-
tionnaires (Zeng et al, 2005; Zeng-Treitler et al,
2008) and via the analysis of low-level readabil-
ity corpora (Elhadad, 2006). In both these cases,
a familiarity score is used to determine how likely
a subject is to understand a term. More recently,
Bott et al (2012) use a threshold of 1% corpus
frequency, along with other checks, to ensure that
simple words are not erroneously simplified.
Support vector machines are powerful statisti-
cal classifiers, as employed in the ?SVM? method
of this paper. A Support Vector Machine is used
to predict the familiarity of CWs in Zeng et al
(2005). It takes features of term frequency and
word length and is correlated against the familiar-
ity scores which are already obtained. This proves
to have very poor performance, something which
the authors attribute to a lack of suitable train-
ing data. An SVM has also been trained for the
ranking of words according to their complexity
(Jauhar and Specia, 2012). This was done for the
SemEval lexical simplification task (Specia et al,
2012). Although this system is designed for syn-
onym ranking, it could also be used for the CW
identification task. Machine learning has also been
applied to the task of determining whether an en-
tire sentence requires simplification (Gasperin et
al., 2009; Hancke et al, 2012). These approaches
use a wide array of morphological features which
are suited to sentence level classification.
6 Future Work
This work is intended as an initial study of meth-
ods for identifying CWs for simplification. The
methods compared, whilst typical of current CW
identification methods, are not an exhaustive set
and variations exist. One further way of expanding
this research would be to take into account word
context. This could be done using thresholding
(Zeng-Treitler et al, 2008) or an SVM (Gasperin
et al, 2009; Jauhar and Specia, 2012).
Another way to increase the accuracy of the fre-
quency count method may be to use a larger cor-
pus. Whilst the corpus used in this paper per-
formed well in the preliminary testing section,
other research has shown the Google Web1T cor-
pus (a n-gram count of over a trillion words) to be
more effective (De Belder and Moens, 2012). The
Web 1T data was not available during the course
of this research.
The large variability in accuracy shown in the
SVM method indicates that there was insufficient
training data. With more data, the SVM would
have more information about the classification
task and would provide more consistent results.
CW identification is the first step in the process
of lexical simplification. This research will be in-
tegrated in a future system which will simplify
natural language for end users. It is also hoped
that other lexical simplification systems will take
account of this work and will use the evaluation
technique proposed herein to improve their identi-
fication of CWs.
7 Conclusion
This paper has provided an insight into the chal-
lenges associated with evaluating the identifica-
tion of CWs. This is a non-obvious task, which
may seem intuitively easy, but in reality is quite
difficult and rarely performed. It is hoped that
new research in this field will evaluate the tech-
niques used, rather than using inadequate tech-
niques blindly and na??vely. This research has also
shown that the current state of the art methods
have much room for improvement. Low precision
is a constant factor in all techniques and future re-
search should aim to address this.
108
Acknowledgment
This work is supported by EPSRC grant
EP/I028099/1. Thanks go to the anonymous
reviewers for their helpful suggestions.
References
Sandra Maria Alu??sio and Caroline Gasperin. 2010.
Fostering digital inclusion and accessibility: the
PorSimples project for simplification of Portuguese
texts. In Proceedings of the NAACL HLT 2010
Young Investigators Workshop on Computational
Approaches to Languages of the Americas, YIW-
CALA ?10, pages 46?53, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefan Bott, Luz Rello, Biljana Drndarevix, and Hora-
cio Saggion. 2012. Can spanish be simpler? lex-
sis: Lexical simplification for spanish. In Coling
2012: The 24th International Conference on Com-
putational Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1.
Marc Brysbaert and Boris New. 2009. Moving beyond
Kucera and Francis : a critical evaluation of current
word frequency norms and the introduction of a new
and improved word frequency measure for Ameri-
can English. Behav Res Methods.
John Carrol, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplifica-
tion of english newspaper text to assist aphasic read-
ers. In AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1?27:27. Software available at http://
www.csie.ntu.edu.tw/?cjlin/libsvm.
Jan De Belder and Marie-Francine Moens. 2012. A
dataset for the evaluation of lexical simplification.
In Computational Linguistics and Intelligent Text
Processing, volume 7182 of Lecture Notes in Com-
puter Science, pages 426?437. Springer Berlin / Hei-
delberg.
Siobhan Devlin and John Tait. 1998. The use of a psy-
cholinguistic database in the simplification of text
for aphasic readers. Linguistic Databases, pages
161?173.
Noe?mie Elhadad. 2006. Comprehending techni-
cal texts: Predicting and defining unfamiliar terms.
In AMIA Annual Symposium proceedings, volume
2006, page 239. American Medical Informatics As-
sociation.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Caroline Gasperin, Lucia Specia, Tiago Pereira, and
Sandra M. Alu??sio. 2009. Learning when to sim-
plify sentences for natural text simplification. In En-
contro Nacional de Intelige?ncia Artificial.
Julia Hancke, Sowmya Vajjala, and Detmar Meurers.
2012. Readability classification for German using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference on
Computational Linguistics (COLING 2012), pages
1063?1080, Mumbai, India.
Sujay Kumar Jauhar and Lucia Specia. 2012. Uow-
shef: Simplex ? lexical simplicity ranking based on
contextual and psycholinguistic features. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceedings
of the main conference and the shared task, and Vol-
ume 2: Proceedings of the Sixth International Work-
shop on Semantic Evaluation (SemEval 2012), pages
477?481, Montre?al, Canada, 7-8 June. Association
for Computational Linguistics.
Partha Lal and Stefan Ru?ger. 2002. Extract-based
summarization with simplification. In Proceedings
of the ACL.
Keith Rayner and Susan Duffy. 1986. Lexical com-
plexity and fixation times in reading: Effects of word
frequency, verb complexity, and lexical ambiguity.
Memory & Cognition, 14:191?201.
Lucia Specia, Sujay Kumar Jauhar, and Rada Mihal-
cea. 2012. Semeval-2012 task 1: English lexical
simplification. In First Joint Conference on Lexical
and Computational Semantics.
S. Rebecca Thomas and Sven Anderson. 2012.
Wordnet-based lexical simplification of a document.
In Jeremy Jancsary, editor, Proceedings of KON-
VENS 2012, pages 80?88. O?GAI, September. Main
track: oral presentations.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: Unsupervised extraction of lexical simpli-
fications from Wikipedia. In Proceedings of the
NAACL.
Qing Zeng, Eunjung Kim, Jon Crowell, and Tony Tse.
2005. A text corpora-based estimation of the famil-
iarity of health terminology. Biological and Medical
Data Analysis, pages 184?192.
Qing Zeng-Treitler, Sergey Goryachev, Tony Tse, Alla
Keselman, and Aziz Boxwala. 2008. Estimat-
ing consumer familiarity with health terminology: a
context-based approach. Journal of the American
Medical Informatics Association, 15(3):349?356.
109
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 69?77,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The CW Corpus: A New Resource for Evaluating
the Identification of Complex Words
Matthew Shardlow
Text Mining Research Group
School of Computer Science, University of Manchester
IT301, Kilburn Building, Manchester, M13 9PL, England
m.shardlow@cs.man.ac.uk
Abstract
The task of identifying complex words
(CWs) is important for lexical simpli-
fication, however it is often carried out
with no evaluation of success. There is
no basis for comparison of current tech-
niques and, prior to this work, there
has been no standard corpus or eval-
uation technique for the CW identi-
fication task. This paper addresses
these shortcomings with a new cor-
pus for evaluating a system?s perfor-
mance in identifying CWs. Simple
Wikipedia edit histories were mined for
instances of single word lexical simpli-
fications. The corpus contains 731 sen-
tences, each with one annotated CW.
This paper describes the method used
to produce the CW corpus and presents
the results of evaluation, showing its
validity.
1 Introduction
CW identification techniques are typically im-
plemented as a preliminary step in a lexical
simplification system. The evaluation of the
identification of CWs is an often forgotten
task. Omitting this can cause a loss of accu-
racy at this stage which will adversely affect
the following processes and hence the user?s
understanding of the resulting text.
Previous approaches to the CW identifica-
tion task (see Section 5) have generally omit-
ted an evaluation of their method. This gap
in the literature highlights the need for evalu-
ation, for which gold standard data is needed.
This research proposes the CW corpus, a
dataset of 731 examples of sentences with ex-
actly one annotated CW per sentence.
A CW is defined as one which causes a sen-
tence to be more difficult for a user to read.
For example, in the following sentence:
?The cat reposed on the mat?
The presence of the word ?reposed? would re-
duce the understandability for some readers.
It would be difficult for some readers to work
out the sentence?s meaning, and if the reader
is unfamiliar with the word ?reposed?, they will
have to infer its meaning from the surrounding
context. Replacing this word with a more fa-
miliar alternative, such as ?sat?, improves the
understandability of the sentence, whilst re-
taining the majority of the original semantics.
Retention of meaning is an important fac-
tor during lexical simplification. If the word
?reposed? is changed to ?sat?, then the specific
meaning of the sentence will be modified (gen-
erally speaking, reposed may indicate a state
of relaxation, whereas sat indicates a body po-
sition) although the broad meaning is still the
same (a cat is on a mat in both scenarios). Se-
mantic shift should be kept to a minimum dur-
ing lexical simplification. Recent work (Biran
et al, 2011; Bott et al, 2012) has employed
distributional semantics to ensure simplifica-
tions are of sufficient semantic similarity.
Word complexity is affected by many fac-
tors such as familiarity, context, morphology
and length. Furthermore, these factors change
from person to person and context to context.
The same word, in a different sentence, may be
perceived as being of a different level of diffi-
culty. The same word in the same sentence,
but read by a different person, may also be
perceived as different in difficulty. For exam-
ple, a person who speaks English as a second
language will struggle with unfamiliar words
depending on their native tongue. Conversely,
the reader who has a low reading ability will
struggle with long and obscure words. Whilst
there will be some crossover in the language
69
these two groups find difficult, this will not be
exactly the same. This subjectivity makes the
automation and evaluation of CW identifica-
tion difficult.
Subjectivity makes the task of natural lan-
guage generation difficult and rules out auto-
matically generating annotated complex sen-
tences. Instead, our CW discovery process
(presented in Section 2) mines simplifications
from Simple Wikipedia1 edit histories. Sim-
ple Wikipedia is well suited to this task as it
is a website where language is collaboratively
and iteratively simplified by a team of editors.
These editors follow a set of strict guidelines
and accountability is enforced by the self polic-
ing community. Simple Wikipedia is aimed
at readers with a low English reading ability
such as children or people with English as a
second language. The type of simplifications
found in Wikipedia and thus mined for use in
our corpus are therefore appropriate for peo-
ple with low English proficiency. By capturing
these simplifications, we produce a set of gen-
uine examples of sentences which can be used
to evaluate the performance of CW identifi-
cation systems. It should be noted that al-
though these simplifications are best suited to
low English proficiency users, the CW identifi-
cation techniques that will be evaluated using
the corpus can be trained and applied for a
variety of user groups.
The contributions of this paper are as fol-
lows:
? A description of the method used to cre-
ate the CW corpus. Section 2.
? An analysis of the corpus combining re-
sults from 6 human annotators. Section
3.
? A discussion on the practicalities sur-
rounding the use of the CW corpus for
the evaluation of a CW identification sys-
tem. Section 4.
Related and future work are also presented in
Sections 5 and 6 respectively.
2 Design
Our corpus contains examples of simplifica-
tions which have been made by human editors
1http://simple.wikipedia.org/
System Score
SUBTLEX 0.3352
Wikipedia Baseline 0.3270
Kuc?era-Francis 0.3097
Random Baseline 0.0157
Table 1: The results of different experi-
ments on the SemEval lexical simplifica-
tion data (de Belder and Moens, 2012),
showing the SUBTLEX data?s superior
performance over several baselines. Each
baseline gave a familiarity value to a set
of words based on their frequency of oc-
currence. These values were used to pro-
duce a ranking over the data which was
compared with a gold standard ranking
using kappa agreement to give the scores
shown here. A baseline using the Google
Web 1T dataset was shown to give a
higher score than SUBTLEX, however
this dataset was not available during the
course of this research.
during their revisions of Simple Wikipedia ar-
ticles. These are in the form of sentences with
one word which has been identified as requir-
ing simplification.2 These examples can be
used to evaluate the output of a CW identi-
fication system (see Section 6). To make the
discovery and evaluation task easier, we limit
the discovered simplifications to one word per
sentence. So, if an edited sentence differs from
its original by more than one word, we do not
include it in our corpus. This also promotes
uniformity in the corpus, reducing the com-
plexity of the evaluation task.
2.1 Preliminaries
SUBTLEX
The SUBTLEX dataset (Brysbaert and New,
2009) is used as a familiarity dictionary. Its
primary function is to associate words with
their frequencies of occurrence, assuming that
words which occur more frequently are sim-
pler. SUBTLEX is also used as a dictionary
for testing word existence: if a word does not
occur in the dataset, it is not considered for
simplification. This may occur in the case of
very infrequent words or proper nouns. The
2We also record the simplification suggested by the
original Simple Wikipedia editor.
70
SUBTLEX data is chosen over the more con-
ventional Kuc?era-Francis frequency (Kuc?era
and Francis, 1967) and over a baseline pro-
duced from Wikipedia frequencies due to a
previous experiment using a lexical simplifica-
tion dataset from task 1 of SemEval 2012 (de
Belder and Moens, 2012). See Table 1.
Word Sense
Homonymy is the phenomenon of a wordform
having 2 distinct meanings as in the clas-
sic case: ?Bank of England? vs. ?River bank ?.
In each case, the word bank is referring to
a different semantic entity. This presents a
problem when calculating word frequency as
the frequencies for homonyms will be com-
bined. Word sense disambiguation is an un-
solved problem and was not addressed whilst
creating the CW corpus. The role of word
sense in lexical simplification will be investi-
gated at a later stage of this research.
Yatskar et al (2010)
The CW corpus was built following the work
of Yatskar et al (2010) in identifying para-
phrases from Simple Wikipedia edit histo-
ries. Their method extracts lexical edits from
aligned sentences in adjacent revisions of a
Simple Wikipedia article. These lexical edits
are then processed to determine their likeli-
hood of being a true simplification. Two meth-
ods for determining this probability are pre-
sented, the first uses conditional probability
to determine whether a lexical edit represents
a simplification and the second uses metadata
from comments to generate a set of trusted
revisions, from which simplifications can be
detected using pointwise mutual information.
Our method (further explained in Section 2.2)
differs from their work in several ways. Firstly,
we seek to discover only single word lexical ed-
its. Secondly, we use both article metadata
and a series of strict checks against a lexicon,
a thesaurus and a simplification dictionary to
ensure that the extracted lexical edits are true
simplifications. Thirdly, we retain the original
context of the simplification as lexical com-
plexity is thought to be influenced by context
(Biran et al, 2011; Bott et al, 2012).
Automatically mining edit histories was
chosen as it provides many instances quickly
and at a low cost. The other method of cre-
ating a similar corpus would have been to
ask several professionally trained annotators
to produce hundreds of sets of sentences, and
to mark up the CWs in these. The use of
professionals would be expensive and annota-
tors may not agree on the way in which words
should be simplified, leading to further prob-
lems when combining annotations.
2.2 Method
In this section, we explain the procedure to
create the corpus. There are many process-
ing stages as represented graphically in Figure
1. The stages in the diagram are further de-
scribed in the sections below. For simplicity,
we view Simple Wikipedia as a set of pages
P, each with an associated set of revisions R.
Every revision of every page is processed iter-
atively until P is exhausted.
Content Articles
The Simple Wikipedia edit histories were ob-
tained.3 The entire database was very large,
so only main content articles were considered.
All user, talk and meta articles were discarded.
Non-content articles are not intended to be
read by typical users and so may not reflect
the same level of simplicity as the rest of the
site.
Revisions which Simplify
When editing a Simple Wikipedia article, the
author has the option to attach a comment to
their revision. Following the work of Yatskar
et al (2010), we only consider those revisions
which have a comment containing some mor-
phological equivalent of the lemma ?simple?,
e.g. simplify, simplifies, simplification, simpler,
etc. This allows us to search for comments
where the author states that they are simpli-
fying the article.
Tf-idf Matrix
Each revision is a set of sentences. As changes
from revision to revision are often small, there
will be many sentences which are the same in
adjacent revisions. Sentences which are likely
to contain a simplification will only have one
word difference and sentences which are un-
related will have many different words. Tf-idf
(Salton and Yang, 1973) vectors are calculated
3Database dump dated 4th February 2012.
71
Simple Wikipedia Edit 
Histories
  For every relevant pair of revisions ri and ri+1
 For every page
Calculate tf-idf matrix for sentences in ri and ri+1
Threshold matrix to give likely candidates
          Sentence Pairs in the form <A,B> Where A 
is a sentence from ri and B is from ri+1
 For every sentence pair
Set of Pages P = p1, ?, pi 
where each pi is the set of 
revisions R = r1, ?, ri and 
each ri  is the set of 
sentences S = s1, ?, si.
Calculate Hamming distance between A and B,
check it is equal to 1
Extract the edited Words: ? from A and ? from B
Check ? and ? are real words
Check ? is simpler than ?
Stem ? and ?, checking the stems are not equal
  If all conditions 
are met
Store pair <A,B> in CW Corpus
Process next pairFalse
True
Verify Candidates
CW corpus
Check ? and ? are synonymous
Extract Likely Candidates
Figure 1: A flow chart showing the process undertaken to extract lexical simplifications.
Each part of this process is further explained in Section 2.2. Every pair of revisions
from every relevant page is processed, although the appropriate recursion is omitted
from the flow chart for simplicity.
72
for each sentence and the matrix containing
the dot product of every pair of sentence vec-
tors from the first and second revision is cal-
culated. This allows us to easily see those vec-
tors which are exactly the same ? as these
will have a score of one.4 It also allows us to
easily see which vectors are so different that
they could not contain a one word edit. We
empirically set a threshold at 0.9 <= X < 1
to capture those sentences which were highly
related, but not exactly the same.
Candidate Pairs
The above process resulted in pairs of sen-
tences which were very similar according to
the tf-idf metric. These pairs were then sub-
jected to a series of checks as detailed below.
These were designed to ensure that as few false
positives as possible would make it to the cor-
pus. This may have meant discarding some
true positives too, however the cautious ap-
proach was adopted to ensure a higher corpus
accuracy.
Hamming Distance
We are only interested in those sentences with
a difference of one word, because sentences
with more than one word difference may con-
tain several simplifications or may be a re-
wording. It is more difficult to distinguish
whether these are true simplifications. We
calculate the Hamming distance between sen-
tences (using wordforms as base units) to en-
sure that only one word differs. Any sentence
pairs which do not have a Hamming distance
of 1 are discarded.
Reality Check
The first check is to ensure that both the words
are a part of our lexicon, ensuring that there
is SUBTLEX frequency data for these words
and also that they are valid words. This stage
may involve removing some valid words, which
are not found in the lexicon, however this is
preferable to allowing words that are the result
of spam or vandalism.
4As tf-idf treats a sentence as a bag of words it is
possible for two sentences to give a score of 1 if they
contain the same words, but in a different order. This
is not a problem as if the sentence order is different,
there is a minimum of 2 lexical edits ? meaning we
still wish to discount this pair.
Inequality Check
It is possible that although a different word
is present, it is a morphological variant of
the original word rather than a simplification.
E.g., due to a change in tense, or a correc-
tion. To identify this, we stem both words
and compare them to make sure they are not
the same. If the word stems are equal then
they are unlikely to be a simplification, so this
pair is discarded. Some valid simplifications
may also be removed at this point, however
these are difficult to distinguish from the non-
simplifications.
Synonymy Check
Typically, lexical simplification involves the se-
lection of a word?s synonym. WordNet (Fell-
baum, 1998) is used as a thesaurus to check if
the second word is listed as a synonym of the
first. As previously discussed (Section 2.1),
we do not take word sense into account at this
point. Some valid simplifications may not be
identified as synonyms in WordNet, however
we choose to take this risk ? discarding all
non-synonym pairs. Improving thesaurus cov-
erage for complex words is left to future work.
Stemming is favoured over lemmatisation
for two reasons. Firstly, because lemmatisa-
tion requires a lot of processing power and
would have terminally slowed the process-
ing of the large revision histories. Secondly,
stemming is a dictionary-independent tech-
nique, meaning it can handle any unknown
words. Lemmatisation requires a large dic-
tionary, which may not contain the rare CWs
which are identified.
Simplicity Check
Finally, we check that the second word is sim-
pler than the first using the SUBTLEX fre-
quencies. All these checks result in a pair of
sentences, with one word difference. The dif-
fering words are synonyms and the change has
been to a word which is simpler than the origi-
nal. Given these conditions have been met, we
store the pair in our CW Corpus as an example
of a lexical simplification.
2.3 Examples
This process was used to mine the following
two examples:
73
Complex word: functions.
Simple word: uses.
A dictionary has been designed to have one
or more that can help the user in a
particular situation.
Complex word: difficult
Simple word: hard
Readability tests give a prediction as to how
readers will find a particular text.
3 Corpus Analysis
3.1 Experimental Design
To determine the validity of the CW corpus, a
set of six mutually exclusive 50-instance ran-
dom samples from the corpus were turned into
questionnaires. One was given to each of 6
volunteer annotators who were asked to deter-
mine, for each sentence, whether it was a true
example of a simplification or not. If so, they
marked the example as correct. This binary
choice was employed to simplify the task for
the annotators. A mixture of native and non-
native English speakers was used, although no
marked difference was observed between these
groups. All the annotators are proficient in
English and currently engaged in further or
higher education. In total, 300 instances of
lexical simplification were evaluated, covering
over 40% of the CW corpus.
A 20 instance sample was also created as
a validation set. The same 20 instances
were randomly interspersed among each of the
6 datasets and used to calculate the inter-
annotator agreement. The validation data
consisted of 10 examples from the CW cor-
pus and 10 examples that were filtered out
during the earlier stages of processing. This
provided sufficient positive and negative data
to show the annotator?s understanding of the
task. These examples were hand picked to rep-
resent positive and negative data and are used
as a gold standard.
Agreement with the gold standard is cal-
culated using Cohen?s kappa (Cohen, 1968).
Inter-annotator agreement is calculated using
Fleiss? kappa (Fleiss, 1971), as in the evalua-
tion of a similar task presented in de Belder
and Moens (2012). In total, each annotator
was presented with 70 examples and asked to
Annotation
Index
Cohen?s
Kappa
Sample
Accuracy
1 1 98%
2 1 96%
3 0.4 70%
4 1 100%
5 0.6 84%
6 1 96%
Table 2: The results of different annota-
tions. The kappa score is given against
the gold standard set of 20 instances. The
sample accuracy is the percentage of the
50 instances seen by that annotator which
were judged to be true examples of a lex-
ical simplification. Note that kappa is
strongly correlated with accuracy (Pear-
son?s correlation: r = 0.980)
label these. A small sample size was used to
reduce the effects of annotator fatigue.
3.2 Results
Of the six annotations, four show the exact
same results on the validation set. These four
identify each of the 10 examples from the CW
corpus as a valid simplification and each of the
10 examples that were filtered out as an invalid
simplification. This is expected as these two
sets of data were selected as examples of posi-
tive and negative data respectively. The agree-
ment of these four annotators further corrob-
orates the validity of the gold standard. An-
notator agreement is shown in Table 2.
The 2 other annotators did not strongly
agree on the validation sets. Calculating Co-
hen?s kappa between each of these annotators
and the gold standard gives scores of 0.6 and
0.4 respectively, indicating a moderate to low
level of agreement. The value for Cohen?s
kappa between the two non-agreeing annota-
tors is 0.2, indicating that they are in low
agreement with each other.
Analysing the errors made by these 2 anno-
tators on the validation set reveals some in-
consistencies. E.g., one sentence marked as
incorrect changes the fragment ?education and
teaching? to ?learning and teaching?. However,
every other annotator marked the enclosing
sentence as correct. This level of inconsistency
and low agreement with the other annotators
74
shows that these annotators had difficulty with
the task. They may not have read the instruc-
tions carefully or may not have understood the
task fully.
Corpus accuracy is defined as the percentage
of instances that were marked as being true
instances of simplification (not counting those
in the validation set). This is out of 50 for
each annotator and can be combined linearly
across all six annotators.
Taking all six annotators into account, the
corpus accuracy is 90.67%. Removing the
worst performing annotator (kappa = 0.4) in-
creases the corpus accuracy to 94.80%. If we
also remove the next worst performing annota-
tor (kappa = 0.6), leaving us with only the four
annotators who were in agreement on the val-
idation set, then the accuracy increases again
to 97.5%.
There is a very strong Pearson?s correlation
(r = 0.980) between an annotator?s agreement
with the gold standard and the accuracy which
they give to the corpus. Given that the lower
accuracy reported by the non-agreeing anno-
tators is in direct proportion to their devia-
tion from the gold standard, this implies that
the reduction is a result of the lower quality
of those annotations. Following this, the two
non-agreeing annotators should be discounted
when evaluating the corpus accuracy ? giving
a final value of 97.5%.
4 Discussion
The necessity of this corpus developed from a
lack of similar resources. CW identification is
a hard task, made even more difficult if blind
to its evaluation. With this new resource, CW
identification becomes much easier to evaluate.
The specific target application for this is lex-
ical simplification systems as previously men-
tioned. By establishing and improving upon
the state of the art in CW identification, lexi-
cal simplification systems will directly benefit
by knowing which wordforms are problematic
to a user.
Methodologically, the corpus is simple to use
and can be applied to evaluate many current
systems (see Section 6). Techniques using dis-
tributional semantics (Bott et al, 2012) may
require more context than is given by just the
sentence. This is a shortcoming of the corpus
in its present form, although not many tech-
niques currently require this level of context.
If necessary, context vectors may be extracted
by processing Simple Wikipedia edit histories
(as presented in Section 2.2) and extracting
the required information at the appropriate
point.
There are 731 lexical edits in the corpus.
Each one of these may be used as an exam-
ple of a complex and a simple word, giving us
1,462 points of data for evaluation. This is
larger than a comparable data set for a simi-
lar task (de Belder and Moens, 2012). Ways
to further increase the number of instances are
discussed in Section 6.
It would appear from the analysis of the val-
idation sets (presented above in Section 3.2)
that two of the annotators struggled with the
task of annotation, attaining a low agreement
against the gold standard. This is most likely
due to the annotators misunderstanding the
task. The annotations were done at the indi-
vidual?s own workstation and the main guid-
ance was in the form of instructions on the
questionnaire. These instructions should be
updated and clarified in further rounds of an-
notation. It may be useful to allow annotators
direct contact with the person administering
the questionnaire. This would allow clarifi-
cation of the instructions where necessary, as
well as helping annotators to stay focussed on
the task.
The corpus accuracy of 97.5% implies that
there is a small error rate in the corpus. This
occurs due to some non-simplifications slip-
ping through the checks. The error rate means
that if a system were to identify CWs perfectly,
it would only attain 97.5% accuracy on the
CW corpus. CW identification is a difficult
task and systems are unlikely to have such a
high accuracy that this will be an issue. If sys-
tems do begin to attain this level of accuracy
then a more rigorous corpus will be warranted
in future.
There is significant interest in lexical sim-
plification for languages which are not English
(Bott et al, 2012; Alu??sio and Gasperin, 2010;
Dell?Orletta et al, 2011; Keskisa?rkka?, 2012).
The technique for discovering lexical simpli-
fications presented here relies heavily on the
existence of Simple English Wikipedia. As no
75
other simplified language Wikipedia exists, it
would be very difficult to create a CW corpus
for any language other than English. However,
the corpus can be used to evaluate CW identi-
fication techniques which will be transferrable
to other languages, given the existence of suf-
ficient resources.
5 Related Work
As previously noted, there is a systemic lack
of evaluation in the literature. Notable excep-
tions come from the medical domain and in-
clude the work of Zeng et al (2005), Zeng-
Treitler et al (2008) and Elhadad (2006).
Zeng et al (2005) first look at word familiarity
scoring correlated against user questionnaires
and predictions made by a support vector ma-
chine. They show that they are able to predict
the complexity of medical terminology with a
relative degree of accuracy. This work is con-
tinued in Zeng-Treitler et al (2008), where a
word?s context is used to predict its familiar-
ity. This is similarly correlated against a user
survey and used to show the importance of
context in predicting word familiarity. The
work of Elhadad (2006) uses frequency and
psycholinguistic features to predict term famil-
iarity. They find that the size of their corpus
greatly affects their accuracy. Whilst these
techniques focus on the medical domain, the
research presented in this paper is concerned
with the more general task of CW identifica-
tion in natural language.
There are two standard ways of identifying
CWs in lexical simplification systems. Firstly,
systems attempt to simplify every word (De-
vlin and Tait, 1998; Thomas and Anderson,
2012; Bott et al, 2012), assuming that CWs
will be modified, but for simple words, no
simpler alternative will exist. The danger is
that too many simple words may be mod-
ified unnecessarily, resulting in a change of
meaning. Secondly, systems use a threshold
over some word familiarity score (Biran et al,
2011; Elhadad, 2006; Zeng et al, 2005). Word
frequency is typically used as the familiarity
score, although it may also be combined with
word length (Biran et al, 2011). The advent
of the CW corpus will allow these techniques
to be evaluated alongside each other on a com-
mon data set.
The CW corpus is similar in conception
to the aforementioned lexical simplification
dataset (de Belder and Moens, 2012) which
was produced for the SemEval 2012 Task 1 on
lexical simplification. This dataset alows syn-
onym ranking systems to be evaluated on the
same platform and was highly useful during
this research (see Table 1).
6 Future Work
The CW corpus is still relatively small at
731 instances. It may be grown by carrying
out the same process with revision histories
from the main English Wikipedia. Whilst the
English Wikipedia revision histories will have
fewer valid simplifications per revision, they
are much more extensive and contain a lot
more data. As well as growing the CW corpus
in size, it would be worthwhile to look at ways
to improve its accuracy. One way would be
to ask a team of annotators to evaluate every
single instance in the corpus and to discard or
keep each according to their recommendation.
Experiments using the corpus are presented
in Shardlow (2013), further details on the use
of the corpus can be found by following this
reference. Three common techniques for iden-
tifying CWs are implemented and statistically
evaluated. The CW Corpus is available from
META-SHARE5 under a CC-BY-SA Licence.
Acknowledgments
This research is supported by EPSRC grant
EP/I028099/1. Thanks go to the annota-
tors and reviewers, who graciously volunteered
their time.
References
Sandra Maria Alu??sio and Caroline Gasperin.
2010. Fostering digital inclusion and accessi-
bility: the PorSimples project for simplifica-
tion of Portuguese texts. In Proceedings of the
NAACL HLT 2010 Young Investigators Work-
shop on Computational Approaches to Lan-
guages of the Americas, YIWCALA ?10, pages
46?53, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Or Biran, Samuel Brody, and Noe?mie Elhadad.
2011. Putting it simply: a context-aware ap-
proach to lexical simplification. In Proceed-
5http://tinyurl.com/cwcorpus
76
ings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human
Language Technologies: short papers - Volume
2, HLT ?11, pages 496?501, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Stefan Bott, Luz Rello, Biljana Drndarevic, and
Horacio Saggion. 2012. Can Spanish be sim-
pler? LexSiS: Lexical simplification for Spanish.
In Coling 2012: The 24th International Confer-
ence on Computational Linguistics., pages 357?
374.
Marc Brysbaert and Boris New. 2009. Moving
beyond Kuc?era and Francis: A critical evalua-
tion of current word frequency norms and the
introduction of a new and improved word fre-
quency measure for American English. Behavior
Research Methods, 41(4):977?990.
Jacob Cohen. 1968. Weighted kappa: nominal
scale agreement with provision for scaled dis-
agreement or partial credit. Psychological Bul-
letin, 70(4):213?220.
Jan de Belder and Marie-Francine Moens. 2012.
A dataset for the evaluation of lexical simpli-
fication. In Computational Linguistics and In-
telligent Text Processing, volume 7182 of Lec-
ture Notes in Computer Science, pages 426?437.
Springer, Berlin Heidelberg.
Felice Dell?Orletta, Simonetta Montemagni, and
Giulia Venturi. 2011. Read-it: assessing read-
ability of Italian texts with a view to text simpli-
fication. In Proceedings of the Second Workshop
on Speech and Language Processing for Assistive
Technologies, SLPAT ?11, pages 73?83, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Siobhan Devlin and John Tait. 1998. The use of a
psycholinguistic database in the simplification of
text for aphasic readers, volume 77. CSLI Lec-
ture Notes, Stanford, CA: Center for the Study
of Language and Information.
Noemie Elhadad. 2006. Comprehending technical
texts: Predicting and defining unfamiliar terms.
In AMIA Annual Symposium proceedings, page
239. American Medical Informatics Association.
Christiane Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological
bulletin, 76:378?382, November.
Robin Keskisa?rkka?. 2012. Automatic text sim-
plification via synonym replacement. Master?s
thesis, Linko?ping University.
Henry Kuc?era and W. Nelson Francis. 1967. Com-
putational analysis of present-day American En-
glish. Brown University Press.
Gerard Salton and Chung-Shu Yang. 1973. On the
specification of term values in automatic index-
ing. Journal of Documentation, 29(4):351?372.
Matthew Shardlow. 2013. A comparison of tech-
niques to automatically identify complex words.
In Proceedings of the Student Research Work-
shop at the 51st Annual Meeting of the Associa-
tion for Computational Linguistics. Association
for Computational Linguistics.
S. Rebecca Thomas and Sven Anderson. 2012.
WordNet-based lexical simplification of a docu-
ment. In Proceedings of KONVENS 2012, pages
80?88. O?GAI, September.
Mark Yatskar, Bo Pang, Cristian Danescu-
Niculescu-Mizil, and Lillian Lee. 2010. For the
sake of simplicity: unsupervised extraction of
lexical simplifications from Wikipedia. In Hu-
man Language Technologies: The 2010 Annual
Conference of the North American Chapter of
the Association for Computational Linguistics,
HLT ?10, pages 365?368, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Qing Zeng, Eunjung Kim, Jon Crowell, and Tony
Tse. 2005. A text corpora-based estimation of
the familiarity of health terminology. In Biolog-
ical and Medical Data Analysis, volume 3745 of
Lecture Notes in Computer Science, pages 184?
192. Springer, Berlin Heidelberg.
Qing Zeng-Treitler, Sergey Goryachev, Tony Tse,
Alla Keselman, and Aziz Boxwala. 2008. Esti-
mating consumer familiarity with health termi-
nology: a context-based approach. Journal of
the American Medical Informatics Association,
15:349?356.
77
