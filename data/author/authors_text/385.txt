Generalizing Dimensionality in Combinatory Categorial Grammar
Geert-Jan M. Kruijff
Computational Linguistics
Saarland University
Saarbru?cken, Germany
gj@coli.uni-sb.de
Jason Baldridge
ICCS, Division of Informatics
University of Edinburgh
Edinburgh, Scotland
jbaldrid@inf.ed.ac.uk
Abstract
We extend Combinatory Categorial Grammar
(CCG) with a generalized notion of multi-
dimensional sign, inspired by the types of rep-
resentations found in constraint-based frame-
works like HPSG or LFG. The generalized
sign allows multiple levels to share information,
but only in a resource-bounded way through
a very restricted indexation mechanism. This
improves representational perspicuity without
increasing parsing complexity, in contrast to
full-blown unification used in HPSG and LFG.
Well-formedness of a linguistic expressions re-
mains entirely determined by the CCG deriva-
tion. We show how the multidimensionality
and perspicuity of the generalized signs lead to
a simplification of previous CCG accounts of
how word order and prosody can realize infor-
mation structure.
1 Introduction
The information conveyed by linguistic utterances
is diverse, detailed, and complex. To properly ana-
lyze what is communicated by an utterance, this in-
formation must be encoded and interpreted at many
levels. The literature contains various proposals for
dealing with many of these levels in the description
of natural language grammar.
Since information flows between different levels
of analysis, it is common for linguistic formalisms
to bundle them together and provide some means
for communication between them. Categorial gram-
mars, for example, normally employ a Saussurian
sign that relates a surface string with its syntactic
category and the meaning it expresses. Syntactic
analysis is entirely driven by the categories, and
when information from other levels is used to affect
the derivational possibilities, it is typically loaded as
extra information on the categories.
Head-driven Phrase Structure Grammar (HPSG)
(Pollard and Sag, 1993) and Lexical Functional
Grammar (LFG) (Kaplan and Bresnan, 1982) also
use complex signs. However, these signs are mono-
lithic structures which permit information to be
freely shared across all dimensions: any given di-
mension can place restrictions on another. For ex-
ample, variables resolved during the construction of
the logical form can block a syntactic analysis. This
provides a clean, unified formal system for dealing
with the different levels, but it also can adversely af-
fect the complexity of parsing grammars written in
these frameworks (Maxwell and Kaplan, 1993).
We thus find two competing perspectives on com-
munication between levels in a sign. In this paper,
we propose a generalization of linguistic signs for
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000b). This generalization enables different
levels of linguistic information to be represented but
limits their interaction in a resource-bounded man-
ner, following White (2004). This provides a clean
separation of the levels and allows them to be de-
signed and utilized in a more modular fashion. Most
importantly, it allows us to retain the parsing com-
plexity of CCG while gaining the representational
advantages of the HPSG and LFG paradigms.
To illustrate the approach, we use it to model var-
ious aspects of the realization of information struc-
ture, an inherent aspect of the (linguistic) meaning
of an utterance. Speakers use information struc-
ture to present some parts of that meaning as de-
pending on the preceding discourse context and oth-
ers as affecting the context by adding new content.
Languages may realize information structure us-
ing different, often interacting means, such as word
order, prosody, (marked) syntactic constructions,
or morphological marking (Vallduv?? and Engdahl,
1996; Kruijff, 2002). The literature presents vari-
ous proposals for how information structure can be
captured in categorial grammar (Steedman, 2000a;
Hoffman, 1995; Kruijff, 2001). Here, we model the
essential aspects of these accounts in a more per-
spicuous manner by using our generalized signs.
The main outcomes of the proposal are three-
fold: (1) CCG gains a more flexible and general
kind of sign; (2) these signs contain multiple levels
that interact in a modular fashion and are built via
CCG derivations without increasing parsing com-
plexity; and (3) we use these signs to simplify pre-
vious CCG?s accounts of the effects of word order
and prosody on information structure.
2 Combinatory Categorial Grammar
In this section, we give an overview of syntactic
combination and semantic construction in CCG. We
use CCG?s multi-modal extension (Baldridge and
Kruijff, 2003), which enriches the inventory of slash
types. This formalization renders constraints on
rules unnecessary and supports a universal set of
rules for all grammars.
2.1 Categories and combination
Nearly all syntactic behavior in CCG is encoded in
categories. They may be atoms, like np, or func-
tions which specify the direction in which they seek
their arguments, like (s\np)/np. The latter is the
category for English transitive verbs; it first seeks
its object to its right and then its subject to its left.
Categories combine through a small set of univer-
sal combinatory rules. The simplest are application
rules which allow a function category to consume
its argument either on its right (>) or on its left (<):
(>) X/?Y Y ? X
(<) Y X\?Y ? X
Four further rules allow functions to compose
with other functions:
(>B) X/Y Y/Z ? X/Z
(<B) Y\Z X\Y ? X\Z
(>B?) X/?Y Y\?Z ? X\?Z
(<B?) Y/?Z X\?Y ? X/?Z
The modalities ?,  and ? on the slashes enforce
different kinds of combinatorial potential on cate-
gories. For a category to serve as input to a rule,
it must contain a slash which is compatible with
that specified by the rule. The modalities work
as follows. ? is the most restricted modality, al-
lowing combination only by the application rules
(> and <).  allows combination with the appli-
cation rules and the order-preserving composition
rules (>B and <B). ? allows limited permutation
via the crossed composition rules (>B? and <B?)
as well as the application rules. Additionally, a per-
missive modality ? allows combination by all rules
in the system. However, we suppress the ? modal-
ity on slashes to avoid clutter. An undecorated slash
may thus combine by all rules.
There are two further rules of type-raising that
turn an argument category into a function over func-
tions that seek that argument:
(>T) X ? Y/i(Y\iX)
(<T) X ? Y\i(Y/iX)
The variable modality i on the output categories
constrains both slashes to have the same modality.
These rules support the following incremental
derivation for Marcel proved completeness:
(1) Marcel proved completeness
np (s\np)/np np
>T
s/(s\np)
>B
s/np
>s
This derivation does not display the effect of us-
ing modalities in CCG; see Baldridge (2002) and
Baldridge and Kruijff (2003) for detailed linguistic
justification for this modalized formulation of CCG.
2.2 Hybrid Logic Dependency Semantics
Many different kinds of semantic representations
and ways of building them with CCG exist. We
use Hybrid Logic Dependency Semantics (HLDS)
(Kruijff, 2001), a framework that utilizes hybrid
logic (Blackburn, 2000) to realize a dependency-
based perspective on meaning.
Hybrid logic provides a language for represent-
ing relational structures that overcomes standard
modal logic?s inability to directly reference states
in a model. This is achieved via nominals, a kind of
basic formula which explicitly names states. Like
propositions, nominals are first-class citizens of the
object language, so formulas can be formed us-
ing propositions, nominals, standard boolean oper-
ators, and the satisfaction operator ?@?. A formula
@i(p? ?F?(j ? q)) indicates that the formulas p and
?F?(j ? q) hold at the state named by i and that the
state j is reachable via the modal relation F.
In HLDS, hybrid logic is used as a language
for describing semantic interpretations as follows.
Each semantic head is associated with a nominal
that identifies its discourse referent and heads are
connected to their dependents via dependency rela-
tions, which are modeled as modal relations. As an
example, the sentence Marcel proved completeness
receives the representation in (2).
(2) @e(prove ? ?TENSE?past
??ACT?(m?Marcel)??PAT?(c?comp.))
In this example, e is a nominal that labels the predi-
cations and relations for the head prove, and m and
c label those for Marcel and completeness, respec-
tively. The relations ACT and PAT represent the de-
pendency roles Actor and Patient, respectively.
By using the @ operator, hierarchical terms such
as (2) can be flattened to an equivalent conjunction
of fixed-size elementary predications (EPs):
(3) @eprove ? @e?TENSE?past ? @e?ACT?m
? @e?PAT?c ? @mMarcel ? @ccomp.
2.3 Semantic Construction
Baldridge and Kruijff (2002) show how HLDS
representations can be built via CCG derivations.
White (2004) improves HLDS construction by op-
erating on flattened representations such as (3) and
using a simple semantic index feature in the syntax.
We adopt this latter approach, described below.
EPs are paired with syntactic categories in the
lexicon as shown in (4)?(6) below. Each atomic cat-
egory has an index feature, shown as a subscript,
which makes a nominal available for capturing syn-
tactically induced dependencies.
(4) prove ` (se\npx)/npy :
@eprove ? @e?TENSE?past
? @e?ACT?x ? @e?PAT?y
(5) Marcel ` npm : @mMarcel
(6) completeness ` npc : @ccompleteness
Applications of the combinatory rules co-index
the appropriate nominals via unification on the cat-
egories. EPs are then conjoined to form the result-
ing interpretation. For example, in derivation (1),
(5) type-raises and composes with (4) to yield (7).
The index x is syntactically unified with m, and this
resolution is reflected in the new conjoined logical
form. (7) can then apply to (6) to yield (8), which
has the same conjunction of predications as (3).
(7) Marcel proved ` se/npy :
@eprove ? @e?TENSE?past
? @e?ACT?m ? @e?PAT?y ? @mMarcel
(8) Marcel proved completeness ` se :
@eprove ? @e?TENSE?past ? @e?ACT?m
?@e?PAT?c?@mMarcel ?@ccompleteness
Since the EPs are always conjoined by the com-
binatory rules, semantic construction is guaranteed
to be monotonic. No semantic information can be
dropped during the course of a derivation. This pro-
vides a clean way of establishing semantic depen-
dencies as informed by the syntactic derivation. In
the next section, we extend this paradigm for use
with any number of representational levels.
3 Generalized dimensionality
To support a more modular and perspicuous encod-
ing of multiple levels of analysis, we generalize the
notion of sign commonly used in CCG. The ap-
proach is inspired on the one hand by earlier work
by Steedman (2000a) and Hoffman (1995), and on
the other by the signs found in constraint-based ap-
proaches to grammar. The principle idea is to ex-
tend White?s (2004) approach to semantic construc-
tion (see ?2.3). There, categories and the mean-
ing they help express are connected through co-
indexation. Here, we allow for information in any
(finite) number of levels to be related in this way.
A sign is an n-tuple of terms that represent in-
formation at n distinct dimensions. Each dimension
represents a level of linguistic information such as
prosody, meaning, or syntactic category. As a repre-
sentation, we assume that we have for each dimen-
sion a language that defines well-formed representa-
tions, and a set of operations which can create new
representations from a set of given representations.1
For example, we have by definition a dimension
for syntactic categories. The language for this di-
mension is defined by the rules for category con-
struction: given a set of atomic categories A, C is a
category iff (i) C ? A or (ii) C is of the form A\mB
or A/mB with A,B categories and m ? {?,  ?, ?}.
The set of combinatory rules defines the possible
operations on categories.
This syntactic category dimension drives the
grammatical analysis, thus guiding the composition
of signs. When two categories are combined via
a rule, the appropriate indices are unified. It is
through this unification of indices that information
can be passed between signs. At a given dimen-
sion, the co-indexed information coming from the
two signs we combine must be unifiable.
With these signs, dimensions interact in a more
limited way than in HPSG or LFG. Constraints (re-
solved through unification) may only be applied
if they are invoked through co-indexation on cat-
egories. This provides a bound on the number of
indices and the number of unifications to be made.
As such, full recursion and complex unification as in
attribute-value matrices with re-entrancy is avoided.
The approach incorporates various ideas from
constraint-based approaches, but remains based on
a derivational perspective on grammatical analysis
and derivational control, unlike e.g Categorial Uni-
fication Grammar. Furthermore, the ability for di-
mensions to interact through shared indices brings
several advantages: (1) ?parallel derivations? (Hoff-
man, 1995) are unnecessary; (2) non-isomorphic,
functional structures across different dimensions
can be employed; and (3) there is no longer a need
to load all the necessary information into syntactic
categories (as with Kruijff (2001)).
1In the context of this paper we assume operations are mul-
tiplicative. Also, note that dimensions may differ in what lan-
guages and operations they use.
4 Examples
In this section, we illustrate our approach on several
examples involving information structure. We use
signs that include the following dimensions.
Phonemic representation: word sequences, composi-
tion of sequences is through concatenation
Prosody: sequences of tunes from the inventory of
(Pierrehumbert and Hirschberg, 1990), composi-
tion through concatenation
Syntactic category: well-formed categories, combina-
tory rules (see ?2)
Information structure: hybrid logic formulas of the
form @d [in]r, with r a discourse referent that has
informativity in (theme ?, or rheme ?) relative to
the current point in the discourse d (Kruijff, 2003).
Predicate-argument structure: hybrid logic formulas
of the form as discussed in ?2.3.
Example (9) illustrates a sign with these dimen-
sions. The word-form Marcel bears an H* accent,
and acts as a type-raised category that seeks a verb
missing its subject. The H* accent indicates that the
discourse referent m introduces new information at
the current point in the discourse d: i.e. the meaning
@mmarcel should end up as part of the rheme (?) of
the utterance, @d [?]m.
(9) Marcel
H*
sh/(sh\npm)
@d [?]m
@mmarcel
If a sign does not specify any information at a
particular dimension, this is indicated by > (or an
empty line if no confusion can arise).
4.1 Topicalization
We start with a simple example of topicalization in
English. In topicalized constructions, a thematic ob-
ject is fronted before the subject. Given the question
Did Marcel prove soundness and completeness?,
(10) is a possible response using topicalization:
(10) Completeness, Marcel proved, and sound-
ness, he conjectured.
We can capture the syntactic and information
structure effects of such sentences by assigning the
following kind of sign to (topicalized) noun phrases:
(11) completeness
>
si/(si/npc)
@d [?]c
@ccompleteness
This category enables the derivation in Figure 1.
The type-raised subject composes with the verb, and
the result is consumed by the topicalizing category.
The information structure specification stated in the
sign in (11) is passed through to the final sign.
The topicalization of the object in (10) only indi-
cates the informativity of the discourse referent re-
alized by the object. It does not yield any indica-
tions about the informativity of other constituents;
hence the informativity for the predicate and the Ac-
tor is left unspecified. In English, the informativity
of these discourse referents can be indicated directly
with the use of prosody, to which we now turn.
4.2 Prosody & information structure
Steedman (2000a) presents a detailed, CCG-based
account of how prosody is used in English as a
means to realize information structure. In the
model, pitch accents and boundary tones have an ef-
fect on both the syntactic category of the expression
they mark, and the meaning of that expression.
Steedman distinguishes pitch accents as markers
of either the theme (?) or of the rheme (?): L+H*
and L*+H are ?-markers; H*, L*, H*+L and H+L*
are ?-markers. Since pitch accents mark individual
words, not (necessarily) larger phrases, Steedman
uses the ?/?-marking to spread informativity over
the domain and the range of function categories.
Identical markings on different parts of a function
category not only act as features, but also as occur-
rences of a singular variable. The value of the mark-
ing on the domain can thus get passed down (?pro-
jected?) to markings on categories in the range.
Constituents bearing no tune have an ?-marking,
which can be unified with either ?, ? or ?. Phrases
with such markings are ?incomplete? until they
combine with a boundary tone. Boundary tones
have the effect of mapping phrasal tones into
intonational phrase boundaries. To make these
boundaries explicit and enforce such ?complete?
prosodic phrases to only combine with other com-
plete prosodic phrases, Steedman introduces two
further types of marking ? ? and ? ? on categories.
The ? markings only unify with other ? or ? mark-
ings on categories, not with ?, ? or ?. These mark-
ings are only introduced to provide derivational con-
trol and are not reflected in the underlying meaning
(which only reflects ?, ? or ?).
Figure 2 recasts the above as an abstract speci-
fication of which different types of prosodic con-
stituents can, or cannot, be combined.2 Steedman?s
2There is one exception we should note: two intermediate
phrases can combine if a second one has a downstepped accent.
We deal with this exception at the end of the section.
completeness Marcel proved
si/(si/npc) sj /(sj \npm) (sp\npx )/npy
@d [?]c
@ccompleteness @mMarcel @pprove ? @p?ACT?x ? @p?PAT?y
>B
sp/npy
@pprove ? @p?ACT?m ? @p?PAT?y ? @mMarcel
>sp
@d [?]c
@pprove ? @p?ACT?m ? @p?PAT?c ? @mMarcel ? @ccompleteness
Figure 1: Derivation for topicalization.
system can be implemented using just one feature
pros which takes the values ip for intermediate
phrases, cp for complete phrases, and up for un-
marked phrases. We write spros=ip , or simply sip if
no confusion can arise.
Figure 2: Abstract specification of derivational con-
trol in prosody
First consider the top half of Figure 2. If a con-
stituent is marked with either a ?- or ?-tune, the
atomic result category of the (possibly complex)
category is marked with ip. Prosodically unmarked
constituents are marked as up. The lexical entries
in (12) illustrates this idea.3
(12) MARCEL proved COMPLETENESS
H* L+H*
sip/(sup\np) (sup\np)/np sip$\(sup$/np)
This can proceed in two ways. Either the marked
MARCEL and the unmarked proved combine to pro-
duce an intermediate phrase (13), or proved and the
marked COMPLETENESS combine (14).
(13) MARCEL proved COMPLETENESS
H* L+H*
sip/(sup\np) (sup\np)/np sip$\(sup$/np)
>
sip/np
3The $?s in the category for COMPLETENESS are standard
CCG schematizations: s$ indicates all functions into s, such as
s\np and (s\np)/np. See Steedman (2000b) for details.
(14) MARCEL proved COMPLETENESS
H* L+H*
sip/(sup\np) (sup\np)/np sip$\(sup$/np)
<
sip\np
For the remainder of this paper, we will suppress up
marking and write sup simply as s.
Examples (13) and (14) show that prosodically
marked and unmarked phrases can combine. How-
ever, both of these partial derivations produce cate-
gories that cannot be combined further. For exam-
ple, in (14), sip/(s\np) cannot combine with sip\np
to yield a larger intermediate phrase. This properly
captures the top half of Figure 2.
To obtain a complete analysis for (12), bound-
ary tones are needed to complete the intermediate
phrases tones. For example, consider (15) (based
on example (70) in Steedman (2000a)):
(15) MARCEL proved COMPLETENESS
H* L L+H* LH%
To capture the bottom-half of Figure 2, the bound-
ary tones L and LH% need categories which cre-
ate complete phrases out of those for MARCEL and
proved COMPLETENESS, and thereafter allow them
to combine. Figure 3 shows the appropriate cate-
gories and complete analysis.
We noted earlier that downstepped phrasal tunes
form an exception to the rule that intermediate
phrases cannot combine. To enable this, we not
only should mark the result category with ip (tune),
but also any leftward argument(s) should have ip
(downstep). Thus, the effect of (lexically) combin-
ing a downstep tune with an unmarked category is
specified by the following template: add marking
xip$\yip to an unmarked category of the form x$\y.
The derivation in Figure 5 illustrates this idea on ex-
ample (64) from (Steedman, 2000a).
To relate prosody to information structure, we ex-
tend the strategy used for constructing logical forms
described in ?2.3, in which a simple index feature
MARCEL proved COMPLETENESS
H* L L+H* LH%
sip/(s\np) (scp/scp$)\?(sip/s$) (s\np)/np sip$\(s$/np) scp$\?sip$
< <
scp/(scp\np) sip\np
<
scp\np
>scp
Figure 3: Derivation including tunes and boundary tones; (70) from (Steedman, 2000a)
Marcel PROVED COMPLETENESS
L+H* LH% H* LL%
np (sip:p\npx )/npy scp$\?sip$ sip\(s/npc) (scp\scp$)\?(sip\s$)
@d [?]p @d [?]c
@mMarcel @pprove ? @p?ACT?x ? @p?PAT?y @ccompleteness
>T <
sip/(sip\np) scp\(scp/npc)
@d [?]c
@mMarcel @ccompleteness
>B
sip/np
@d [?]p
@pprove ? @p?ACT?m ? @p?PAT?y ? @mMarcel
<
scp/npy
@d [?]p
@pprove ? @p?ACT?m ? @p?PAT?y ? @mMarcel
<scp
@d [?]p ? @d [?]c
@pprove ? @p?ACT?m ? @p?PAT?c ? @mMarcel ? @ccompleteness
Figure 4: Information structure for derivation for (67)-(68) from (Steedman, 2000a)
on atomic categories makes a nominal (discourse
referent) available. We represent information struc-
ture as a formula @d [i]r at a dimension separate
from the syntactic category. The nominal r stands
for the discourse referent, which has informativity
i with respect to the current point in the discourse
d (Kruijff, 2003). Following Steedman, we distin-
guish two levels of informativity, namely ? (theme)
and ? (rheme).
We start with a minimal assignment of informa-
tivity: a theme-tune on a constituent sets the infor-
mativity of the discourse referent r realized by the
constituent to ? and a rheme-tune sets it to ?. This
is a minimal assignment in the sense that we do not
project informativity; instead, we only set informa-
tivity for those discourse referents whose realization
shows explicit clues as to their information status.
The derivation in Figure 4 illustrates this idea and
shows the construction of both logical form and in-
formation structure.
Indices can also impose constraints on the infor-
mativity of arguments. For example, in the down-
step example (Figure 5), the discourse referents cor-
responding to ANNA and SAYS are both part of the
theme. We specify this with the constituent that has
received the downstepped tune. The referent of the
subject of SAYS (indexed x) must be in the theme
along with the referent s for SAYS. This is satisfied
in the derivation: a unifies with x, and we can unify
the statements about a?s informativity coming from
ANNA (@d [?]a) and SAYS (@d [?]x with x replaced
by a in the >B step).
5 Conclusions
In this paper, we generalize the traditional Saus-
surian sign in CCG with an n-dimensional linguis-
tic sign. The dimensions in the generalized linguis-
tic sign can be related through indexation. Index-
ation places constraints on signs by requiring that
co-indexed material is unifiable, on a per-dimension
basis. Consequently, we do not need to overload the
syntactic category with information from different
dimensions.
The resulting sign structure resembles the signs
found in constraint-based grammar formalisms.
There is, however, an important difference. Infor-
mation at various dimensions can be related through
co-indexation, but dimensions cannot be directly
ANNA SAYS he proved COMPLETENESS
L+H* !L+H* LH%
npip:a (sip:s\npip:x )/sy s/(s\np) (sp\np)/np
@d [?]a @d [?]s ? @d [?]x @d [?](pron) @d [i]p
>T
sip/(sip\npip)
@d [?]a
>B
sip/s
@d [?]s ? @d [?]a
>B
sip/(s\np)
@d [?]s ? @d [?]a ? @d [?](pron)
>B
sip/np
@d [?]s ? @d [?]a ? @d [?](pron) ? @d [i]p
Figure 5: Information structure for derivation for (64) from (Steedman, 2000a)
referenced. As analysis remains driven only by in-
ference over categories, only those constraints trig-
gered by indexation on the categories are imposed.
We do not allow for re-entrancy.
It is possible to conceive of a scenario in which
the various levels can contribute toward determin-
ing the well-formedness of an expression. For ex-
ample, we may wish to evaluate the current informa-
tion structure against a discourse model, and reject
the analysis if we find it is unsatisfiable. If such a
move is made, then the complexity will be bounded
by the complexity of the dimension for which it is
most difficult to determine satisfiability.
Acknowledgments
Thanks to Ralph Debusmann, Alexander Koller,
Mark Steedman, and Mike White for discussion.
Geert-Jan Kruijff?s work is supported by the DFG
SFB 378 Resource-Sensitive Cognitive Processes,
Project NEGRA EM 6.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. of 40th Annual Meeting of the ACL, pages 319?
326, Philadelphia, Pennsylvania.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proc. of
10th Annual Meeting of the EACL, Budapest.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
Patrick Blackburn. 2000. Representation, reasoning,
and relational structures: a hybrid logic manifesto.
Journal of the Interest Group in Pure Logic, 8(3):339?
365.
Beryl Hoffman. 1995. Integrating ?free? word order
syntax and information structure. In Proc. of 7th An-
nual Meeting of the EACL, Dublin.
Ronald M. Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: A formal system for grammat-
ical representation. In The Mental Representation
of Grammatical Relations, pages 173?281. The MIT
Press, Cambridge Massachusetts.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Logi-
cal Architecture of Informativity: Dependency Gram-
mar Logic & Information Structure. Ph.D. thesis,
Charles University, Prague, Czech Republic.
Geert-Jan M. Kruijff. 2002. Formulating a category of
informativity. In Hilde Hasselgard, Stig Johansson,
Bergljot Behrens, and Cathrine Fabricius-Hansen, ed-
itors, Information Structure in a Cross-Linguistic Per-
spective, pages 129?146. Rodopi, Amsterdam.
Geert-Jan M. Kruijff. 2003. Binding across boundaries.
In Geert-Jan M. Kruijff and Richard T. Oehrle, editors,
Resource Sensitivity, Binding, and Anaphora. Kluwer
Academic Publishers, Dordrecht.
John T. III Maxwell and Ronald M. Kaplan. 1993. The
interface between phrasal and functional constraints.
Computational Linguistics, 19(4):571?590.
Janet Pierrehumbert and Julia Hirschberg. 1990. The
meaning of intonational contours in the interpretation
of discourse. In J. Morgan P. Cohen and M. Pollack,
editors, Intentions in Communication. The MIT Press,
Cambridge Massachusetts.
Carl Pollard and Ivan A. Sag. 1993. Head-Driven
Phrase Structure Grammar. University of Chicago
Press, Chicago IL.
Mark Steedman. 2000a. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
Mark Steedman. 2000b. The Syntactic Process. The
MIT Press, Cambridge, MA.
Enric Vallduv?? and Elisabet Engdahl. 1996. The linguis-
tic realization of information packaging. Linguistics,
34:459?519.
Michael White. 2004. Efficient realization of coordinate
structures in Combinatory Categorial Grammar. Re-
search on Language and Computation. To appear.
211
212
213
214
215
216
217
218
Ensemble-based Active Learning for Parse Selection
Miles Osborne and Jason Baldridge
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
 
miles,jbaldrid  @inf.ed.ac.uk
Abstract
Supervised estimation methods are widely seen
as being superior to semi and fully unsuper-
vised methods. However, supervised methods
crucially rely upon training sets that need to
be manually annotated. This can be very ex-
pensive, especially when skilled annotators are
required. Active learning (AL) promises to
help reduce this annotation cost. Within the
complex domain of HPSG parse selection, we
show that ideas from ensemble learning can
help further reduce the cost of annotation. Our
main results show that at times, an ensemble
model trained with randomly sampled exam-
ples can outperform a single model trained us-
ing AL. However, converting the single-model
AL method into an ensemble-based AL method
shows that even this much stronger baseline
model can be improved upon. Our best results
show a  reduction in annotation cost com-
pared with single-model random sampling.
1 Introduction
Active learning (AL) methods, such as uncertainty sam-
pling (Cohn et al, 1995) or query by committee (Seung
et al, 1992), can dramatically reduce the cost of creat-
ing an annotated dataset. In particular, they enable rapid
creation of labeled datasets which can then be used for
trainable speech and language technologies. Progress in
AL will therefore translate into even greater savings in
annotation costs and hence faster creation of speech and
language systems.
In this paper, we:
 Present a novel way of improving uncertainty sam-
pling by generalizing it from using a single model to
using an ensemble model. This generalization easily
outperforms single-model uncertainty sampling.
 Introduce a new, extremely simple AL method
(called lowest best probability selection) which is
competitive with uncertainty sampling and can also
be improved using ensemble techniques.
 Show that an ensemble of models trained using ran-
domly sampled examples can outperform a single
model trained using (single model) AL methods.
 Demonstrate further reductions in annotation cost
when we train the ensemble parse selection model
using examples selected by an ensemble-based ac-
tive learner. This result shows that ensemble learn-
ing can improve both the underlying model and also
the way we select examples for it.
Our domain is parse selection for Head-Driven Phrase
Structure Grammar (HPSG). Although annotated corpora
exist for HPSG, such corpora do not exist in significant
volumes and are limited to a few small domains (Oepen
et al, 2002). Even if it were possible to bootstrap from
the Penn Treebank, it is still unlikely that there would be
sufficient quantities of high quality material necessary to
improve parse selection for detailed linguistic formalisms
such as HPSG. There is thus a pressing need to efficiently
create significant volumes of annotated material.
AL applied to parse selection is much more challeng-
ing than applying it to simpler tasks such as text classifi-
cation or part-of-speech tagging. Our labels are complex
objects rather than discrete values drawn from a small,
fixed set. Furthermore, the fact that sentences are of vari-
able length and have variable numbers of parses poten-
tially adds to the complexity of the task.
Our results specific to parse selection show that:
 An ensemble of three parse selection models is able
to achieve a 10.8% reduction in error rate over the
best single model.
 Annotation cost should not assume a unit expendi-
ture per example. Using a more refined cost met-
ric based upon efficiently selecting the correct parse
from a set of possible parses, we are able to show
that some AL methods are more effective than oth-
ers, even though they perform similarly when mak-
ing the unit cost per example assumption.
 Ad-hoc selection methods based upon superficial
characteristics of the data, such as sentence length
or ambiguity rate, are typically worse than random
sampling. This motivates using AL methods.
 Labeling sentences in the order they appear in the
corpus ? as is typically done in annotation ? per-
forms much worse than using random selection.
Throughout this paper, we shall treat the terms sen-
tences and examples as interchangeable; we shall also
consider parses and labels as equivalent. Also, we shall
use the term method whenever we are talking about AL,
and model whenever we are talking about parse selection.
2 Parse selection
2.1 The Redwoods treebank
Many broad coverage grammars providing detailed syn-
tactic and semantic analyses of sentences exist for a va-
riety of computational grammar frameworks, but their
purely symbolic nature means that when ordering li-
censed analyses, parse selection models are necessary. To
overcome this limitation for the HPSG English Resource
Grammar (ERG, Flickinger (2000)), the Redwoods tree-
bank has been created to provide annotated training ma-
terial (Oepen et al, 2002).
For each utterance in Redwoods, analyses licensed by
the ERG are enumerated and the correct one, if present,
is indicated. Each analysis is represented as a tree that
records the grammar rules which were used to derive it.
For example, Figure 1a shows the preferred derivation
tree, out of three analyses, for what can I do for you?.
Using these trees and the ERG, several different views
of analyses can be recovered: phrase structures, semantic
interpretations, and elementary dependency graphs. The
phrase structures contain detailed HPSG non-terminals
but are otherwise of the variety familiar from context-free
grammar, as can be seen in Figure 1b.
Unlike most treebanks, Redwoods also provides se-
mantic information for utterances. The semantic interpre-
tations are expressed using Minimal Recursion Seman-
tics (MRS) (Copestake et al, 2001), which provides the
means to represent interpretations with a flat, underspec-
ified semantics using terms of the predicate calculus and
generalized quantifiers. An example MRS structure is
given in Figure 2.
An elementary dependency graph is a simplified ab-
straction on a full MRS structure which uses no under-
specification and retains only the major semantic predi-
cates and their relations to one another.
In this paper, we report results using the third growth
of Redwoods, which contains 5302 sentences for which
there are at least two parses and for which a unique pre-
ferred parse is identified. These sentences have 9.3 words
and 58.0 parses on average. Due to the small size of Red-
woods and the underlying complexity of the system, ex-
ploring the effect of AL techniques for this domain is of
practical, as well as theoretical, interest.
2.2 Modeling parse selection
As is now standard for feature-based grammars, we use
log-linear models for parse selection (Johnson et al,
1999). Log-linear models are popular for their ability to
incorporate a wide variety of features without making as-
sumptions about their independence.1
For log-linear models, the conditional probability of
an analysis   given a sentence with a set of analyses 

 
	 is given as:

  
Coupling CCG and Hybrid Logic Dependency Semantics
Jason Baldridge
ICCS
Division of Informatics
2 Buccleuch Place
University of Edinburgh
Edinburgh, UK, EH8 9LW
jmb@cogsci.ed.ac.uk
Geert-Jan M. Kruijff
Universita?t des Saarlandes
Computational Linguistics
Lehrstuhl Uszkoreit
Building 17, Postfach 15 11 50
66041 Saarbru?cken, Germany
gj@CoLi.Uni-SB.DE
Abstract
Categorial grammar has traditionally used
the ?-calculus to represent meaning. We
present an alternative, dependency-based
perspective on linguistic meaning and sit-
uate it in the computational setting. This
perspective is formalized in terms of hy-
brid logic and has a rich yet perspicuous
propositional ontology that enables a wide
variety of semantic phenomena to be rep-
resented in a single meaning formalism.
Finally, we show how we can couple this
formalization to Combinatory Categorial
Grammar to produce interpretations com-
positionally.
1 Introduction
The ?-calculus has enjoyed many years as the stan-
dard semantic encoding for categorial grammars and
other grammatical frameworks, but recent work has
highlighted its inadequacies for both linguistic and
computational concerns of representing natural lan-
guage semantics (Copestake et al, 1999; Kruijff,
2001). The latter couples a resource-sensitive cate-
gorial proof theory (Moortgat, 1997) to hybrid logic
(Blackburn, 2000) to formalize a dependency-based
perspective on meaning, which we call here Hybrid
Logic Dependency Semantics (HLDS). In this pa-
per, we situate HLDS in the computational context
by explicating its properties as a framework for com-
putational semantics and linking it to Combinatory
Categorial Grammar (CCG).
The structure of the paper is as follows. In x2,
we briefly introduce CCG and how it links syntax
and semantics, and then discuss semantic represen-
tations that use indexes to identify subparts of logi-
cal forms. x3 introduces HLDS and evaluates it with
respect to the criteria of other computational seman-
tics frameworks. x4 shows how we can build HLDS
terms using CCG with unification and x5 shows how
intonation and information structure can be incorpo-
rated into the approach.
2 Indexed semantic representations
Traditionally, categorial grammar has captured
meaning using a (simply typed) ?-calculus, build-
ing semantic structure in parallel to the categorial in-
ference (Morrill, 1994; Moortgat, 1997; Steedman,
2000b). For example, a (simplified) CCG lexical en-
try for a verb such as wrote is given in (1).
(1) wrote ` (snn)=n : ?x:?y:write(y;x)
Rules of combination are defined to operate on both
categories and ?-terms simultaneously. For exam-
ple, the rules allow the following derivation for Ed
wrote books.
(2) Ed wrote books
n:Ed (snn)=n:?x:?y:write(y;x) n:books
>
snn : ?y:write(y;books)
<
s : write(Ed;books)
Derivations like (2) give rise to the usual sort
of predicate-argument structure whereby the order
in which the arguments appear (and are bound by
the ??s) is essentially constitutive of their meaning.
Thus, the first argument could be taken to corre-
spond to the writer, whereas the second argument
corresponds to what is being written.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 319-326.
                         Proceedings of the 40th Annual Meeting of the Association for
One deficiency of ?-calculus meaning representa-
tions is that they usually have to be type-raised to
the worst case to fully model quantification, and this
can reverberate and increase the complexity of syn-
tactic categories since a verb like wrote will need to
be able to take arguments with the types of general-
ized quantifiers. The approach we advocate in this
paper does not suffer from this problem.
For CCG, the use of the ?-terms is simply a con-
venient device to bind arguments when presenting
derivations (Steedman, 2000b). In implementations,
a more common strategy is to compute semantic rep-
resentations via unification, a tactic explicitly em-
ployed in Unification Categorial Grammar (UCG)
(Zeevat, 1988). Using a unification paradigm in
which atomic categories are bundles of syntactic and
semantic information, we can use an entry such as
(3) for wrote in place of (1). In the unification set-
ting, (3) permits a derivation analogous to (2).
(3) wrote ` (s : write(y;x)nn:y)=n:x
For creating predicate-argument structures of this
kind, strategies using either ?-terms or unification
to bind arguments are essentially notational vari-
ants. However, UCG goes beyond simple predicate-
argument structures to instead use a semantic repre-
sentation language called Indexed Language (InL).
The idea of using indexes stems from Davidson
(event variables), and are a commonly used mech-
anism in unification-based frameworks and theories
for discourse representation. InL attaches one to ev-
ery formula representing its discourse referent. This
results in a representation such as (4) for the sen-
tence Ed came to the party.
(4) [e][party(x);past(e); to(e;x);come(e;Ed)]
InL thus flattens logical forms to some extent, using
the indexes to spread a given entity or event through
multiple predications. The use of indexes is crucial
for UCG?s account of modifiers, and as we will see
later, we exploit such referents to achieve similar
ends when coupling HLDS and CCG.
Minimal Recursion Semantics (MRS) (Copestake
et al, 1999; Copestake et al, 2001) is a frame-
work for computational semantics that is designed
to simplify the work of algorithms which produce
or use semantic representations. MRS provides the
means to represent interpretations with a flat, un-
derspecified semantics using terms of the predicate
calculus and generalized quantifiers. Flattening is
achieved by using an indexation scheme involving
labels that tag particular groups of elementary pred-
ications (EPs) and handles (here, h1;h2; :::) that ref-
erence those EPs. Underspecification is achieved
by using unresolved handles as the arguments for
scope-bearing elements and declaring constraints
(with the =q operator) on how those handles can be
resolved. Different scopes can be reconstructed by
equating unresolved handles with the labels of the
other EPs obeying the =q constraints. For example,
(5) would be given as the representation for every
dog chases some white cat.
(5) hh0;fh1:every(x;h2;h3);h4:dog(x);
h11:cat(y);h8:some(y;h9;h10);
h11:white(y);h7:chase(x;y)g;
fh0=qh7; h2=qh4; h9=qh11gi
Copestake et al argue that these flat representa-
tions facilitate a number of computational tasks, in-
cluding machine translation and generation, without
sacrificing linguistic expressivity. Also, flatness per-
mits semantic equivalences to be checked more eas-
ily than in structures with deeper embedding, and
underspecification simplifies the work of the parser
since it does not have to compute every possible
reading for scope-bearing elements.
3 Hybrid Logic Dependency Semantics
Kruijff (2001) proposes an alternative way to rep-
resenting linguistically realized meaning: namely,
as terms of hybrid modal logic (Blackburn, 2000)
explicitly encoding the dependency relations be-
tween heads and dependents, spatio-temporal struc-
ture, contextual reference, and information struc-
ture. We call this unified perspective combining
many levels of meaning Hybrid Logic Dependency
Semantics (HLDS). We begin by discussing how hy-
brid logic extends modal logic, then look at the rep-
resentation of linguistic meaning via hybrid logic
terms.
3.1 Hybrid Logic
Though modal logic provides a powerful tool for
encoding relational structures and their properties,
it contains a surprising inherent asymmetry: states
(?worlds?) are at the heart of the model theory for
modal logic, but there are no means to directly
reference specific states using the object language.
This inability to state where exactly a proposition
holds makes modal logic an inadequate representa-
tion framework for practical applications like knowl-
edge representation (Areces, 2000) or temporal rea-
soning (Blackburn, 1994). Because of this, compu-
tational work in knowledge representation has usu-
ally involved re-engineering first-order logic to suit
the task, e.g., the use of metapredicates such as Hold
of Kowalski and Allen. Unfortunately, such logics
are often undecidable.
Hybrid logic extends standard modal logic while
retaining decidability and favorable complexity
(Areces, 2000) (cf. (Areces et al, 1999) for a com-
plexity roadmap). The strategy is to add nominals,
a new sort of basic formula with which we can ex-
plicitly name states in the object language. Next to
propositions, nominals are first-class citizens of the
object language: formulas can be formed using both
sorts, standard boolean operators, and the satisfac-
tion operator ?@?. A formula @i p states that the
formula p holds at the state named by i.1 (There
are more powerful quantifiers ranging over nomi-
nals, such as #, but we do not consider them here.)
With nominals we obtain the possibility to explic-
itly refer to the state at which a proposition holds. As
Blackburn (1994) argues, this is essential for cap-
turing our intuitions about temporal reference. A
standard modal temporal logic with the modalities
F and P (future and past, respectively) cannot cor-
rectly represent an utterance such as Ed finished the
book because it is unable to refer to the specific time
at which the event occurred. The addition of nomi-
nals makes this possible, as shown in (6), where the
nominal i represents the Reichenbachian event time.
(6) hPi(i^Ed-finish-book)
Furthermore, many temporal properties can be de-
fined in terms of pure formulas which use nominals
and contain no propositional variables. For example,
the following term defines the fact that the relations
for F and P are mutually converse:
1A few notes on our conventions: p;q;r are variables over
any hybrid logic formula; i; j;k are variables over nominals; di
and hi denote nominals (for dependent and head, respectively).
(7) @i[F]hPii ^ @i[P]hFii
It is also possible to encode a variety of other rep-
resentations in terms of hybrid logics. For example,
nominals correspond to tags in attribute-value matri-
ces (AVMs), so the hybrid logic formula in (8) cor-
responds to the AVM in (9).
(8) hSUBJi(i^hAGRisingular ^hPREDidog)
^ hCOMPihSUBJii
(9) 2
6
6
6
4
SUBJ 1
"
AGR singular
PRED dog
#
COMP
h
SUBJ 1
i
3
7
7
7
5
A crucial aspect of hybrid logic is that nominals
are at the heart of a sorting strategy. Different sorts
of nominals can be introduced to build up a rich
sortal ontology without losing the perspicuity of a
propositional setting. Additionally, we can reason
about sorts because nominals are part and parcel of
the object language. We can extend the language of
hybrid logic with fSort:Nominalg to facilitate the ex-
plicit statement of what sort a nominal is in the lan-
guage and carry this modification into one of the ex-
isting tableaux methods for hybrid logic to reason ef-
fectively with this information. This makes it possi-
ble to capture the rich ontologies of lexical databases
like WordNet in a clear and concise fashion which
would be onerous to represent in first-order logic.
3.2 Encoding linguistic meaning
Hybrid logic enables us to logically capture two es-
sential aspects of meaning in a clean and compact
way, namely ontological richness and the possibility
to refer. Logically, we can represent an expression?s
linguistically realized meaning as a conjunction of
modalized terms, anchored by the nominal that iden-
tifies the head?s proposition:
(10) @h(proposition
V
h?ii(di ^depi))
Dependency relations are modeled as modal rela-
tions h?ii, and with each dependent we associate
a nominal di, representing its discourse referent.
Technically, (10) states that each nominal di names
the state where a dependent expressed as a proposi-
tion depi should be evaluated and is a ?i successor
of h, the nominal identifying the head. As an exam-
ple, the sentence Ed wrote a long book in London
receives the represention in (11).
(11) @h1(write^hACTi(d0^Ed)
^hPATi(d5^book^hGRi(d7^long))
^hLOCi(d9^London))
The modal relations ACT, PAT, LOC, and GR stand
for the dependency relations Actor, Patient, Loca-
tive, and General Relationship, respectively. See
Kruijff (2001) for the model-theoretic interpretation
of expressions such as (11).
Contextual reference can be modeled as a state-
ment that from the current state (anaphor) there
should be an accessible antecedent state at which
particular conditions hold. Thus, assuming an ac-
cessibility relation XS, we can model the meaning
of the pronoun he as in (12).
(12) @ihXSi( j ^male)
During discourse interpretation, this statement is
evaluated against the discourse model. The pronoun
is resolvable only if a state where male holds is XS-
accessible in the discourse model. Different acces-
sibility relations can be modeled, e.g. to distinguish
a local context (for resolving reflexive anaphors like
himself ) from a global context (Kruijff, 2001).
Finally, the rich temporal ontology underlying
models of tense and aspect such as Moens and
Steedman (1988) can be captured using the sorting
strategy. Earlier work like Blackburn and Lascarides
(1992) already explored such ideas. HLDS employs
hybrid logic to integrate Moens and Steedman?s no-
tion of the event nucleus directly into meaning rep-
resentations. The event nucleus is a tripartite struc-
ture reflecting the underlying semantics of a type of
event. The event is related to a preparation (an ac-
tivity bringing the event about) and a consequent (a
state ensuing to the event), which we encode as the
modal relations PREP and CONS, respectively. Dif-
ferent kinds of states and events are modeled as dif-
ferent sorts of nominals, shown in (13) using the no-
tation introduced above.
(13) @
fActivity:e1ghPREPifAchievement:e2g
^@
fAchievement:e2ghCONSifState:e3g
To tie (13) in with a representation like (11), we
equate the nominal of the head with one of the nom-
inals in the event nucleus (E)a and state its temporal
relation (e.g. hPi). Given the event nucleus in (13),
the representation in (11) becomes (14), where the
event is thus located at a specific time in the past.
(14) @h1(E
(13) ^write^hACTi(d0^Ed)
^hPATi(d5^book^hGRi(d7^long))
^hLOCi(d9^London))
^@h1fAchievement:e2g^hPifAchievement:e2g
Hybrid logic?s flexibility makes it amenable to
representing a wide variety of semantic phenomena
in a propositional setting, and it can furthermore be
used to formulate a discourse theory (Kruijff and
Kruijff-Korbayova?, 2001).
3.3 Comparison to MRS
Here we consider the properties of HLDS with
respect to the four main criteria laid out by
Copestake et al (1999) which a computational se-
mantics framework must meet: expressive adequacy,
grammatical compatibility, computational tractabil-
ity, and underspecifiability.
Expressive adequacy refers to a framework?s abil-
ity to correctly express linguistic meaning. HLDS
was designed not only with this in mind, but as its
central tenet. In addition to providing the means
to represent the usual predicate-valency relations,
it explicitly marks the named dependency relations
between predicates and their arguments and modi-
fiers. These different dependency relations are not
just labels: they all have unique semantic imports
which project new relations in the context of differ-
ent heads. HLDS also tackles the representation of
tense and aspect, contextual reference, and informa-
tion structure, as well as their interaction with dis-
course.
The criterion of grammatical compatibility re-
quires that a framework be linkable to other kinds of
grammatical information. Kruijff (2001) shows that
HLDS can be coupled to a rich grammatical frame-
work, and in x4 we demonstrate that it can be tied to
CCG, a much lower power formalism than that as-
sumed by Kruijff. It should furthermore be straight-
forward to use our approach to hook HLDS up to
other unification-based frameworks.
The definition of computational tractability states
that it must be possible to check semantic equiva-
lence of different formulas straightforwardly. Like
MRS, HLDS provides the means to view linguis-
tic meaning in a flattened format and thereby ease
the checking of equivalence. For example, (15) de-
scribes the same relational structure as (11).
(15) @h1(write^hACTid0 ^hPATid5 ^hLOCid9)
^@d0Ed^@d5book^@d9London
^@d7 long^@d5hGRid7
This example clarifies how the use of nominals is
related to the indexes of UCG?s InL and the labels
of MRS. However, there is an important difference:
nominals are full citizens of the object language with
semantic import and are not simply a device for
spreading meaning across several elementary predi-
cations. They simultaneously represent tags on sub-
parts of a logical form and discourse referents on
which relations are predicated. Because it is possi-
ble to view an HLDS term as a flat conjunction of
the heads and dependents inside it, the benefits de-
scribed by Copestake et al with respect to MRS?s
flatness thus hold for HLDS as well.
Computational tractability also requires that it
is straightforward to express relationships between
representations. This can be done in the object lan-
guage of HLDS as hybrid logic implicational state-
ments which can be used with proof methods to dis-
cover deeper relationships. Kruijff?s model connect-
ing linguistic meaning to a discourse context is one
example of this.
Underspecifiability means that semantic represen-
tations should provide means to leave some semantic
distinctions unresolved whilst allowing partial terms
to be flexibly and monotonically resolved. (5) shows
how MRS leaves quantifier scope underspecified,
and such formulas can be transparently encoded in
HLDS. Consider (16), where the relations RESTR
and BODY represent the restriction and body argu-
ments of the generalized quantifiers, respectively.
(16) @h7(chase^hACTih4 ^hPATih11)
^@h1(every^hRESTRii^hBODYi j)
^@h8(some^hRESTRik^hBODYil)
^@h4dog^@h11cat^@h11hGRi(h12^white)
^@ihQEQih4 ^@khQEQih11
MRS-style underspecification is thus replicated by
declaring new nominals and modeling =q as a modal
relation between nominals. When constructing the
fully-scoped structures generated by an underspeci-
fied one, the =q constraints must be obeyed accord-
ing to the qeq condition of Copestake etal. Because
HLDS is couched directly in terms of hybrid logic,
we can concisely declare the qeq condition as the
following implication:
(17) @ihQEQi j ! @i j_ (@ihBODYik^@khQEQi j)
Alternatively, it would in principle be possible to
adopt a truly modal solution to the representation
of quantifiers. Following Alechina (1995), (general-
ized) quantification can be modeled as modal opera-
tors. The complexity of generalized quantification is
then pushed into the model theory instead of forcing
the representation to carry the burden.
4 CCG Coupled to HLDS
In Dependency Grammar Logic (DGL),
Kruijff (2001) couples HLDS to a resource-
sensitive categorial proof theory (CTL) (Moortgat,
1997). Though DGL demonstrates a procedure for
building HLDS terms from linguistic expressions,
there are several problems we can overcome by
switching to CCG. First, parsing with CCG gram-
mars for substantial fragments is generally more
efficient than with CTL grammars with similar
coverage. Also, a wide-coverage statistical parser
which produces syntactic dependency structures
for English is available for CCG (Clark et al,
2002). Second, syntactic features (modeled by
unary modalities) in CTL have no intuitive semantic
reflection, whereas CCG can relate syntactic and
semantic features perspicuously using unification.
Finally, CCG has a detailed syntactic account of the
realization of information structure in English.
To link syntax and semantics in derivations, ev-
ery logical form in DGL expresses a nominal iden-
tifying its head in the format @i p. This handles de-
pendents in a linguistically motivated way through
a linking theory: given the form of a dependent, its
(possible) role is established, after which its mean-
ing states that it seeks a head that can take such a
role. However, to subsequently bind that dependent
into the verb?s argument slot requires logical axioms
about the nature of various dependents. This not
only requires extra reduction steps to arrive at the
desired logical form, but could also lead to problems
depending on the underlying theory of roles.
We present an alternative approach to binding de-
pendents, which overcomes these problems without
abandoning the linguistic motivation. Because we
work in a lexicalist setting, we can compile the ef-
fects of the linguistic linking theory directly into cat-
egory assignments.
The first difference in our proposal is that argu-
ments express only their own nominal, not the nom-
inal of a head as well. For example, proper nouns
receive categories such as (18).
(18) Ed ` n : @d1 Ed
This entry highlights our relaxation of the strict con-
nection between syntactic and semantic types tradi-
tionally assumed in categorial grammars, a move in
line with the MRS approach.
In contrast with DGL, the semantic portion of a
syntactic argument in our system does not declare
the role it is to take and does not identify the head
it is to be part of. Instead it identifies only its own
referent. Without using additional inference steps,
this is transmuted via unification into a form similar
to DGL?s in the result category. (19) is an example
of the kind of head category needed.
(19) sleeps ` s : @h2(sleep^hACTi(i^p))nn : @ip
To derive Ed sleeps, (18) and (19) combine via back-
ward application to produce (20), the same term as
that built in DGL using one step instead of several.
(20) @h2(sleep^hACTi(d1^Ed))
To produce HLDS terms that are fully compati-
ble with the way that Kruijff and Kruijff-Korbayova?
(2001) model discourse, we need to mark the infor-
mativity of dependents as contextually bound (CB)
and contextually nonbound (NB). In DGL, these ap-
pear as modalities in logical forms that are used to
create a topic-focus articulation that is merged with
the discourse context. For example, the sentence he
wrote a book would receive the following (simpli-
fied) interpretation:
(21) @h1([NB]write^ [NB]hPATi(d5^book)
^ [CB]hACTi(d6 ^hXSi(d3^male)))
DGL uses feature-resolving unary modalities
(Moortgat, 1997) to instantiate the values of in-
formativity. In unification-based approaches such
as CCG, the transferal of feature information into
semantic representations is standard practice. We
thus employ the feature inf and mark informativity
in logical forms with values resolved syntactically.
(22) Ed ` ninf=CB : @d1 Ed
(23) sleeps ` s : @h2([NB]sleep^ [q]hACTi(i^p))nninf=q:@ip
Combining these entries using backward application
gives the following result for Ed sleeps:
(24) s : @h2([NB]sleep^ [CB]hACTi(d1^Ed))
A major benefit of having nominals in our rep-
resentations comes with adjuncts. With HLDS, we
consider the prepositional verbal modifier in the sen-
tence Ed sleeps in the bed as an optional Locative
dependent of sleeps. To implement this, we fol-
low DGL in identifying the discourse referent of the
head with that of the adjunct. However, unlike DGL,
this is compiled into the category for the adjunct.
(25) in ` (s : @i(p^ [r]hLOCi(j^q))ns:@ip)=ninf=r:@jq
To derive the sentence Ed sleeps in the bed (see
Figure 1), we then need the following further entries:
(26) the ` ninf=CB:p=ninf=NB:p
(27) bed ` ninf=NB : @d3 bed
This approach thus allows adjuncts to insert their
semantic import into the meaning of the head, mak-
ing use of nominals in a manner similar to the use of
indexes in Unification Categorial Grammar.
5 Intonation and Information Structure
Information Structure (IS) in English is in part deter-
mined by intonation. For example, given the ques-
tion in (28), an appropriate response would be (29).2
(28) I know what Ed READ. But what did Ed
WRITE?
(29) (Ed WROTE) (A BOOK).
L+H* LH% H* LL%
Steedman (2000a) incorporates intonation into
CCG syntactic analyses to determine the contribu-
tion of different constituents to IS. Steedman calls
segments such as Ed wrote of (29) the theme of the
sentence, and a book the rheme. The former indi-
cates the part of the utterance that connects it with
the preceding discourse, whereas the latter provides
information that moves the discourse forward.
In the context of Discourse Representation The-
ory, Kruijff-Korbayova? (1998) represents IS by
splitting DRT structures into a topic/focus articula-
tion of the form TOPIC ./ FOCUS . We represent
2Following Pierrehumbert?s notation, the intonational con-
tour L+H* indicates a low-rising pitch accent, H* a sharply-
rising pitch accent, and both LH% and LL% are boundary tones.
Ed sleeps (= (24)) in the bed
s : @h2([NB]sleep^ [CB]hACTi(d1^Ed)) s : @i(p^ [r]hLOCi(j^q))ns:@ip)=ninf=r:@jq ninf=CB:s=ninf=NB:s ninf=NB:@d3 bed
>
ninf=CB : @d3 bed
>
s : @i(p^ [CB]hLOCi(d3^bed))ns:@ip
<
s : @h2([NB]sleep^ [CB]hACTi(d1^Ed)^ [CB]hLOCi(d3^bed))
Figure 1: Derivation of Ed sleeps in the bed.
this in HLDS as a term incorporating the ./ opera-
tor. Equating topic and focus with Steedman?s theme
and rheme, we encode the interpretation of (29) as:
(30) @h7([CB]write^ [CB]hACTi(d1^Ed)
./ [NB]hPATi(d4^book))
DGL builds such structures by using a rewriting sys-
tem to produce terms with topic/focus articulation
from the terms produced by the syntax.
Steedman uses the pitch accents to produce lexi-
cal entries with values for the INFORMATION fea-
ture, which we call here sinf . L+H* and H* set
the value of this feature as ? (for theme) or ?
(for rheme), respectively. He also employs cate-
gories for the boundary tones that carry blocking
values for sinf which stop incomplete intonational
phrases from combining with others, thereby avoid-
ing derivations for utterances with nonsensical into-
nation contours.
Our approach is to incorporate the syntactic as-
pects of Steedman?s analysis with DGL?s rewriting
system for using informativity to partition senten-
tial meaning. In addition to using the syntactic fea-
ture sinf , we allow intonation marking to instantiate
the values of the semantic informativity feature inf .
Thus, we have the following sort of entry:
(31) WROTE (L+H*) `
ssinf=?:?nninf=w;sinf=?:@ip=ninf=x;sinf=?:@jq
?=@h2([CB]write^[w]hACTi(i^p)^[x]hPATi( j^q))
We therefore straightforwardly reap the syntactic
benefits of Steedman?s intonation analysis, while IS
itself is determined via DGL?s logical form rewrit-
ing system operating on the modal indications of
informativity produced during the derivation. The
articulation of IS can thus be performed uniformly
across languages, which use a variety of strategies
including intonation, morphology, and word order
variation to mark the informativity of different el-
ements. The resulting logical form plugs directly
into DGL?s architecture for incorporating sentence
meaning with the discourse.
6 Conclusions and Future Work
Since it is couched in hybrid logic, HLDS is ide-
ally suited to be logically engineered to the task at
hand. Hybrid logic can be made to do exactly what
we want, answering to the linguistic intuitions we
want to formalize without yielding its core assets ? a
rich propositional ontology, decidability, and favor-
able computational complexity.
Various aspects of meaning, like dependency re-
lations, contextual reference, tense and aspect, and
information structure can be perspicuously encoded
with HLDS, and the resulting representations can
be built compositionally using CCG. CCG has close
affinities with dependency grammar, and it provides
a competitive and explanatorily adequate basis for
a variety of phenomena ranging from coordination
and unbounded dependencies to information struc-
ture. Nonetheless, the approach we describe could
in principle be fit into other unification-based frame-
works like Head-Driven Phrase Structure Grammar.
Hybrid logic?s utility does not stop with senten-
tial meaning. It can also be used to model dis-
course interpretation and is closely related to log-
ics for knowledge representation. This way we can
cover the track from grammar to discourse with a
single meaning formalism. We do not need to trans-
late or make simplifying assumptions for different
processing modules to communicate, and we can
freely include and use information across different
levels of meaning.
We have implemented a (preliminary) Java pack-
age for creating and manipulating hybrid logic terms
and connected it to Grok, a CCG parsing system.3
The use of HLDS has made it possible to improve
3The software is available at http://opennlp.sf.net
and http://grok.sf.net under an open source license.
the representation of the lexicon. Hybrid logic nom-
inals provide a convenient and intuitive manner of
localizing parts of a semantic structure, which has
made it possible to greatly simplify the use of inher-
itance in the lexicon. Logical forms are created as
an accumulation of different levels in the hierarchy
including morphological information. This is partic-
ularly important since the system does not otherwise
support typed feature structures with inheritance.
Hybrid logics provide a perspicuous logical lan-
guage for representing structures in temporal logic,
description logic, AVMs, and indeed any relational
structure. Terms of HLDS can thus be marshalled
into terms of these other representations with the
potential of taking advantage of tools developed for
them or providing input to modules expecting them.
In future work, we intend to combine techniques
for building wide-coverage statistical parsers for
CCG (Hockenmaier and Steedman, 2002; Clark et
al., 2002) with corpora that have explicitly marked
semantic dependency relations (such as the Prague
Dependency Treebank and NEGRA) to produce
HLDS terms as the parse output.
Acknowledgements
We would like to thank Patrick Blackburn, Johan Bos, Nissim
Francez, Alex Lascarides, Mark Steedman, Bonnie Webber and
the ACL reviewers for helpful comments on earlier versions of
this paper. All errors are, of course, our own. Jason Baldridge?s
work is supported in part by Overseas Research Student Award
ORS/98014014. Geert-Jan Kruijff?s work is supported by the
DFG Sonderforschungsbereich 378 Resource-Sensitive Cogni-
tive Processes, Project NEGRA EM6.
References
Natasha Alechina. 1995. Modal Quantifiers. Ph.D. thesis, Uni-
versity of Amsterdam, Amsterdam, The Netherlands.
Carlos Areces, Patrick Blackburn, and Maarten Marx. 1999. A
road-map on complexity for hybrid logics. In J. Flum and
M. Rodr??guez-Artalejo, editors, Computer Science Logic,
number 1683 in Lecture Notes in Computer Science, pages
307?321. Springer-Verlag.
Carlos Areces. 2000. Logic Engineering. The Case of Descrip-
tion and Hybrid Logics. Ph.D. thesis, University of Amster-
dam, Amsterdam, The Netherlands.
Patrick Blackburn and Alex Lascarides. 1992. Sorts and oper-
ators for temporal semantics. In Proc. of the Fourth Sympo-
sium on Logic and Language, Budapest, Hungary.
Patrick Blackburn. 1994. Tense, temporal reference and tense
logic. Journal of Semantics, 11:83?101.
Patrick Blackburn. 2000. Representation, reasoning, and rela-
tional structures: a hybrid logic manifesto. Logic Journal of
the IGPL, 8(3):339?625.
Stephen Clark, Julia Hockenmaier, and Mark Steedman. 2002.
Building deep dependency structures using a wide-coverage
CCG parser. In Proc. of the 40th Annual Meeting of the As-
sociation of Computational Linguistics, Philadelphia, PA.
Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard.
1999. Minimal recursion semantics: An introduction. ms,
www-csli.stanford.edu/?aac/newmrs.ps.
Ann Copestake, Alex Lascarides, and Dan Flickinger. 2001.
An algebra for semantic construction in constraint-based
grammars. In Proc. of the 39th Annual Meeting of the
Association of Computational Linguistics, pages 132?139,
Toulouse, France.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with combinatory categorial
grammar. In Proc. of the 40th Annual Meeting of the As-
sociation of Computational Linguistics, Philadelphia, PA.
Geert-Jan M. Kruijff and Ivana Kruijff-Korbayova?. 2001. A
hybrid logic formalization of information structure sensitive
discourse interpretation. In Proc. of the Fourth Workshop
on Text, Speech and Dialogue, volume 2166 of LNCS/LNAI,
pages 31?38. Springer-Verlag.
Ivana Kruijff-Korbayova?. 1998. The Dynamic Potential of
Topic and Focus: A Praguian Approach to Discourse Repre-
sentation Theory. Ph.D. thesis, Charles University, Prague,
Czech Republic.
Geert-Jan M. Kruijff. 2001. A Categorial Modal Architec-
ture of Informativity: Dependency Grammar Logic & Infor-
mation Structure. Ph.D. thesis, Charles University, Prague,
Czech Republic.
Marc Moens and Mark Steedman. 1988. Temporal ontology
and temporal reference. Computational Linguistics, 14:15?
28.
Michael Moortgat. 1997. Categorial type logics. In Johan van
Benthem and Alice ter Meulen, editors, Handbook of Logic
and Language. Elsevier Science B.V.
Glyn V. Morrill. 1994. Type Logical Grammar: Categorial
Logic of Signs. Kluwer Academic Publishers, Dordrecht,
Boston, London.
Mark Steedman. 2000a. Information structure and the syntax-
phonology interface. Linguistic Inquiry, 34:649?689.
Mark Steedman. 2000b. The Syntactic Process. The MIT
Press, Cambridge Mass.
Henk Zeevat. 1988. Combining categorial grammar and unifi-
cation. In Uwe Reyle and Christian Rohrer, editors, Natural
Language Parsing and Linguistic Theories, pages 202?229.
Reidel, Dordrecht.
Active learning for HPSG parse selection
Jason Baldridge and Miles Osborne
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
  jmb,osborne  @cogsci.ed.ac.uk
Abstract
We describe new features and algorithms for
HPSG parse selection models and address the
task of creating annotated material to train
them. We evaluate the ability of several sam-
ple selection methods to reduce the number
of annotated sentences necessary to achieve a
given level of performance. Our best method
achieves a 60% reduction in the amount of
training material without any loss in accuracy.
1 Introduction
Even with significant resources such as the Penn Tree-
bank, a major bottleneck for improving statistical parsers
has been the lack of sufficient annotated material from
which to estimate their parameters. Most statistical pars-
ing research, such as Collins (1997), has centered on
training probabilistic context-free grammars using the
Penn Treebank. For richer linguistic frameworks, such as
Head-Driven Phrase Structure Grammar (HPSG), there
is even less annotated material available for training
stochastic parsing models. There is thus a pressing need
to create significant volumes of annotated material in a
logistically efficient manner. Even if it were possible to
bootstrap from the Penn Treebank, it is still unlikely that
there would be sufficient quantities of high quality mate-
rial.
There has been a strong focus in recent years on us-
ing the active learning technique of selective sampling to
reduce the amount of human-annotated training material
needed to train models for various natural language pro-
cessing tasks. The aim of selective sampling is to iden-
tify the most informative examples, according to some se-
lection method, from a large pool of unlabelled material.
Such selected examples are then manually labelled. Se-
lective sampling has typically been applied to classifica-
tion tasks, but has also been shown to reduce the number
of examples needed for inducing Lexicalized Tree Inser-
tion Grammars from the Penn Treebank (Hwa, 2000).
The suitability of active learning for HPSG-type gram-
mars has as yet not been explored. This paper addresses
the problem of minimizing the human effort expended in
creating annotated training material for HPSG parse se-
lection by using selective sampling. We do so in the con-
text of Redwoods (Oepen et al, 2002), a treebank that
contains HPSG analyses for sentences from the Verbmo-
bil appointment scheduling and travel planning domains.
We show that sample selection metrics based on tree en-
tropy (Hwa, 2000) and disagreement between two differ-
ent parse selection models significantly reduce the num-
ber of annotated sentences necessary to match a given
level of performance according to random selection. Fur-
thermore, by combining these two methods as an ensem-
ble selection method, we require even fewer examples ?
achieving a 60% reduction in the amount of annotated
training material needed to outperform a model trained
on randomly selected material. These results suggest
that significant reductions in human effort can be real-
ized through selective sampling when creating annotated
material for linguistically rich grammar formalisms.
As the basis of our active learning approach, we create
both log-linear and perceptron models, the latter of which
has not previously been used for feature-based grammars.
We show that the different biases of the two types of mod-
els is sufficient to create diverse members for a commit-
tee, even when they use exactly the same features. With
respect to the features used to train models, we demon-
strate that a very simple feature selection strategy that ig-
nores the proper structure of trees is competitive with one
that fully respects tree configurations.
The structure of the paper is as follows. In sections 2
and 3, we briefly introduce active learning and the Red-
woods treebank. Section 4 discusses the parse selection
models that we use in the experiments. In sections 5 and
6, we explain the different selection methods that we use
for active learning and explicate the setup in which the
experiments were conducted. Finally, the results of the
experiments are presented and discussed in section 7.
2 Active Learning
Active learning attempts to reduce the number of exam-
ples needed for training statistical models by allowing
the machine learner to directly participate in creating the
corpus it uses. There are a several approaches to active
learning; here, we focus on selective sampling (Cohn et
al., 1994), which involves identifying the most informa-
tive examples from a pool of unlabelled data and pre-
senting only these examples to a human expert for an-
notation. The two main flavors of selective sampling are
certainty-based methods and committee-based methods
(Thompson et al, 1999). For certainty-based selection,
the examples chosen for annotation are those for which
a single learner is least confident, as determined by some
criterion. Committee-based selection involves groups of
learners that each maintain different hypotheses about
the problem; examples on which the learners disagree in
some respect are typically regarded as the most informa-
tive.
Active learning has been successfully applied to a
number of natural language oriented tasks, including text
categorization (Lewis and Gale, 1994) and part-of-speech
tagging (Engelson and Dagan, 1996). Hwa (2000) shows
that certainty-based selective sampling can reduce the
amount of training material needed for inducing Prob-
abilistic Lexicalized Tree Insertion Grammars by 36%
without degrading the quality of the grammars. Like
Hwa, we investigate active learning for parsing and thus
seek informative sentences; however, rather than induc-
ing grammars, our task is to select the best parse from the
output of an existing hand-crafted grammar by using the
Redwoods treebank.
3 The Redwoods Treebank
The English Resource Grammar (ERG, Flickinger
(2000)) is a broad coverage HPSG grammar that provides
deep semantic analyses of sentences but has no means to
prefer some analyses over others because of its purely
symbolic nature. To address this limitation, the Red-
woods treebank has been created to provide annotated
training material to permit statistical models for ambigu-
ity resolution to be combined with the precise interpreta-
tions produced by the ERG (Oepen et al, 2002).
Whereas the Penn Treebank has an implicit grammar
underlying its parse trees, Redwoods uses the ERG ex-
plicitly. For each utterance, Redwoods enumerates the
set of analyses, represented as derivation trees, licensed
by the ERG and identifies which analysis is the preferred
one. For example, Figure 1 shows the preferred deriva-
fillhead wh r
noptcomp
what1
what
hcomp
hcomp
sailr
can aux pos
can
i
i
hadj i uns
extracomp
bse verb infl rule
do2
do
hcomp
for
for
you
you
Figure 1: Redwoods derivation tree for the sentence what
can I do for you? The node labels are the names of the
ERG rules used to build the analysis.
tion tree, out of three ERG analyses, for what can I do
for you?. From such derivation trees, the parse trees and
semantic interpretations can be recovered using an HPSG
parser.
Redwoods is (semi-automatically) updated after
changes have been made to the ERG, and it has thus far
gone through three growths. Some salient characteris-
tics of the first and third growths are given in Table 1 for
utterances for which a unique preferred parse has been
identified and for which there are at least two analyses.1
The ambiguity increased considerably between the first
and third growths, reflecting the increased coverage of
the ERG for more difficult sentences.
corpus sentences length parses
Redwoods-1 3799 7.9 9.7
Redwoods-3 5302 9.3 58.0
Table 1: Characteristics of subsets of Redwoods versions
used for the parse selection task. The columns indi-
cate the number of sentences in the subset, their average
length, and their average number of parses.
The small size of the treebank makes it essential to
explore the possibility of using methods such as active
learning to speed the creation of more annotated material
for training parse selection models.
4 Parse Selection
Committee-based active learning requires multiple learn-
ers which have different biases that cause them to make
different predictions sometimes. As in co-training, one
1There are over 1400 utterances in both versions for which
the ERG produces only one analysis and which therefore are
irrelevant for parse selection. They contain no discriminating
information and are thus not useful for the machine learning
algorithms discussed in the next section.
uni   hcomp 	
bi  


what1 	  



hcomp 	
tri  


noptcomp 	  


what1 	  



hcomp 	
Figure 2: Three example ngram features based on the
derivation tree in Figure 1.
way such diverse learners can be created is by using in-
dependent or partially independent feature sets to reduce
the error correlation between the learners. Another way
is to use different machine learning algorithms trained on
the same feature set. In this section, we discuss two fea-
ture sets and two machine learning algorithms that are
used to produce four distinct models and we give their
overall performance on the parse selection task.
4.1 Features
Our two feature sets are created by using only the deriva-
tion trees made available in Redwoods. The configura-
tional set is loosely based on the derivation tree features
given by Toutanova and Manning (2002), and thus en-
codes standard relations such as grandparent-of and left-
sibling for the nodes in the tree. The ngram set is created
by flattening derivation trees and treating them as strings
of rule names over which ngrams are extracted, taking up
to four rule names at a time and including the number of
intervening parentheses between them. We ignore ortho-
graphic values for both feature sets.
As examples of typical ngram features, the derivation
tree given in Figure 1 generates features such as those de-
picted in Figure 2. Such features provide a reasonable ap-
proximation of trees that implicitly encodes many of the
interesting relationships that are typically gathered from
them, such as grandparent and sibling relations. They
also capture further relationships that cross the brackets
of the actual tree, providing some more long-distance re-
lationships than the configurational features.
4.2 Algorithms
We use both log-linear and perceptron algorithms to cre-
ate parse selection models. Both frameworks use iter-
ative procedures to determine the weights 


of a corresponding set of features Active Learning and the Total Cost of Annotation
Jason Baldridge and Miles Osborne
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
{jbaldrid,miles}@inf.ed.ac.uk
Abstract
Active learning (AL) promises to reduce
the cost of annotating labeled datasets for
trainable human language technologies.
Contrary to expectations, when creating
labeled training material for HPSG parse
selection and later reusing it with other
models, gains from AL may be negligible
or even negative. This has serious impli-
cations for using AL, showing that addi-
tional cost-saving strategies may need to
be adopted. We explore one such strategy:
using a model during annotation to auto-
mate some of the decisions. Our best re-
sults show an 80% reduction in annotation
cost compared with labeling randomly se-
lected data with a single model.
1 Introduction
AL methods such as uncertainty sampling (Cohn
et al, 1995) or query by committee (Seung et al,
1992) have all been shown to dramatically reduce
the cost of creating highly informative labeled sets
for speech and language technologies. However, ex-
periments using AL assume a model that is fixed
ahead of time: the model used in AL is the same
one we are currently developing training material
for. For many complex tasks, we are unlikely to have
a clear idea how best to model the task at the time of
annotation; thus, in practice, we will need to reuse
the labeled training material with other models.
In this paper, we show that AL can be brittle: un-
der a variety of natural reuse scenarios (for example,
allowing the later model to improve in quality, or
else reusing the labeled training material using a dif-
ferent machine learning algorithm) performance of
later models can be significantly undermined when
training upon material created using AL. The key
to knowing how well one model will be able to use
material selected by another is their relatedness ? yet
there may be no means to determine this prior to an-
notation, leading to a chicken-and-egg problem.
Our reusability results thus demonstrate that, ad-
ditionally, other strategies must be adopted to en-
sure we reduce the total cost of annotation. In Os-
borne and Baldridge (2004), we showed that ensem-
ble models can increase model performance and also
produce annotation savings when incorporated into
the AL process. An obvious next step is automating
some decisions. Here, we consider a simple automa-
tion strategy that reduces annotation costs indepen-
dently of AL and examine its effect on reusability.
We find that using both semi-automation and AL
with high-quality models can eliminate the perfor-
mance gap found in many reuse scenarios. However,
for weak models, we show that semi-automation
with random sampling is more effective for improv-
ing reusability than using it with AL ? demonstrat-
ing further cause for caution with AL.
Finally, we show that under the standard assump-
tion of reuse by the selecting model, using a strat-
egy which combines AL, ensembling, and semi-
automated annotation, we are able to achieve our
highest annotation savings to date on the complex
task of parse selection for HPSG: an 80% reduction
in annotation cost compared with labeling randomly
selected data with our best single model.
2 Parse selection for Redwoods
We now briefly describe the Redwoods treebanking
environment (Oepen et al, 2002), our parse selec-
tion models and their performance.
2.1 The Redwoods Treebank
The Redwoods treebank project provides tools and
annotated training material for creating parse se-
lection models for the English Resource Grammar
(ERG, Flickinger (2000)). The ERG is a hand-built
broad-coverage HPSG grammar that provides an ex-
plicit grammar for the treebank. Using this approach
has the advantage that analyses for within-coverage
sentences convey more information than just phrase
structure: they also contain derivations, semantic in-
terpretations, and basic dependencies.
For each sentence, Redwoods records all analyses
licensed by the ERG and indicates which of them,
if any, the annotators selected as being contextually
correct. When selecting such distinguished parses,
rather than simply enumerating all parses and pre-
senting them to the annotator, annotators make use
of discriminants which disambiguate the parse for-
est more rapidly, as described in section 3.
In this paper, we report results using the third
growth of Redwoods, which contains English sen-
tences from appointment scheduling and travel plan-
ning domains of Verbmobil. In all, there are 5302
sentences for which there are at least two parses and
a unique preferred parse is identified. These sen-
tences have 9.3 words and 58.0 parses on average.
2.2 Modeling parse selection
As is now standard for feature-based grammars, we
mainly use log-linear models for parse selection
(Johnson et al, 1999). For log-linear models, the
conditional probability of an analysis ti given a sen-
tence with a set of analyses ? = {t . . .} is given as:
P (ti|s,Mk) =
exp(
?m
j=1 fj(ti)wj)
Z(s)
(1)
where fj(ti) returns the number of times feature
j occurs in analysis t, wj is a weight from model
Mk, and Z(s) is a normalization factor for the sen-
tence. The parse with the highest probability is taken
as the preferred parse for the model. We use the
limited memory variable metric algorithm to deter-
mine the weights. We do not regularize our log-
linear models since labeled data -necessary to set
hyperparameters- is in short supply in AL.
We also make use of simpler perceptron models
for parse selection, which assign scores rather than
probabilities. Scores are computed by taking the in-
ner product of the analysis? feature vector with the
parameter vector:
score(ti, s,Mk) =
m?
j=1
fj(ti)wj (2)
The preferred parse is that with the highest score out
of all analyses. We do not use voted perceptrons
here (which indeed have better performance) as for
the reuse experiments described later in section 6 we
really do wish to use a model that is (potentially)
worse than a log-linear model.
Later for AL , it will be useful to map perceptron
scores into probabilities, which we do by exponenti-
ating and renormalizing the score:
Pp(ti | s,Mk) =
exp(score(ti, s,Mk))
Z(s)
(3)
Z(s) is again a normalizing constant.
The previous parse selection models (equations
1 and 3) use a single model (feature set). It is
possible to improve performance using an ensem-
ble parse selection model. We create our ensemble
model (called a product model) using the product-
of-experts formulation (Hinton, 1999):
P (ti|s,M1 . . .Mn) =
?n
j=1 P (ti|s,Mj)
Z(s)
(4)
Note that each individual model Mi is a well-defined
distribution usually taken from a fixed set of mod-
els. Z(s) is a constant to ensure the product distri-
bution sums to one over the set of possible parses. A
product model effectively averages the contributions
made by each of the individual models. Though sim-
ple, this model is sufficient to show enhanced perfor-
mance when using multiple models.
2.3 Parse selection performance
Osborne and Baldridge (2004) describe three dis-
tinct feature sets ? configurational, ngram, and
conglomerate ? which utilize the various struc-
tures made available in Redwoods: derivation trees,
phrase structures, semantic interpretations, and ele-
mentary dependency graphs. They incorporate dif-
ferent aspects of the parse selection task; this is
crucial for creating diverse models for use in prod-
uct parse selection models as well as for ensemble-
based AL methods. Here, we also use models cre-
ated from a subset of the conglomerate feature set:
the mrs feature set. This only has features from the
semantic interpretations.
The three main feature sets are used to train three
log-linear models ? LL-CONFIG, LL-NGRAM, and
LL-CONGLOM? and a product ensemble of those
three feature sets, LL-PROD, using equation 4. Addi-
tionally, we use a perceptron with the conglomerate
feature set, P-CONGLOM. Finally, we include a log-
linear model that uses the mrs feature set, LL-MRS,
and a perceptron, P-MRS.
Parse selection accuracy is measured using exact
match. A model is awarded a point if it picks some
parse for a sentence and that parse is the correct anal-
ysis indicated by the corpus. To deal with ties, the
accuracy is given as 1/m when a model ranks m
parses highest and the best parse is one of them.
The results for a chance baseline (selecting a
parse at random), the base models and the product
model are given in Table 1. These are 10-fold cross-
validation results, using all the training data for esti-
mation and the test split for evaluation. See section
5 for more details.
Model Perf. Model Perf.
LL-CONFIG 75.05 LL-PROD 77.78
LL-NGRAM 74.01 LL-MRS 64.98
LL-CONGLOM 74.85 P-CONGLOM 73.00
Chance 22.70 P-MRS 62.11
Table 1: Parse selection accuracy.
3 Measuring annotation cost
To aid identification of the best parse out of all those
licensed by the ERG, the Redwoods annotation envi-
ronment provides local discriminants which the an-
notator can mark as true or false properties for the
analysis of a sentence in order to disambiguate large
portions of the parse forest. As such, the annotator
does not need to inspect all parses and so parses are
narrowed down quickly (usually exponentially so)
even for sentences with a large number of parses.
More interestingly, it means that the labeling burden
is relative to the number of possible parses (rather
than the number of constituents in a parse).
Data about how many discriminants were needed
to annotate each sentence is recorded in Redwoods.
Typically, more ambiguous sentences require more
discriminant values to be set, reflecting the extra ef-
fort put into identifying the best parse. We showed
in Osborne and Baldridge (2004) that discriminant
cost does provide a more accurate approximation of
annotation cost than assigning a fixed unit cost for
each sentence. We thus use discriminants as the ba-
sis of calculating annotation cost to evaluate the ef-
fectiveness of different experiment AL conditions.
Specifically, we set the cost of annotating a given
sentence as the number of discriminants whose
value were set by the human annotator plus one to
indicate a final ?eyeball? step where the annotator se-
lects the best parse of the few remaining ones.1 The
discriminant cost of the examples we use averages
3.34 and ranges from 1 to 14.
4 Active learning
Suppose we have a set of examples and labels Dn =
{?x1, y1?, ?x2, y2?, . . .} which is to be extended with
a new labeled example {?xi, yi?}. The information
gain for some model is maximized after selecting,
labeling, and adding a new example xi to Dn such
that the noise level of xi is low and both the bias and
variance of some model using Dn ? {?xi, yi?} are
minimized (Cohn et al, 1995).
In practice, selecting data points for labeling such
that a model?s variance and/or bias is maximally
minimized is computationally intractable, so ap-
proximations are typically used instead. One such
approximation is uncertainty sampling. Uncertainty
sampling (also called tree entropy by Hwa (2000)),
measures the uncertainty of a model over the set of
parses of a given sentence, based on the conditional
1This eyeball step is not always taken, but Redwoods does
not contain information about when this occurred, so we apply
the cost for the step uniformly for all examples.
distribution it assigns to them. Following Hwa, we
use the following measure to quantify uncertainty:
fus(s, ?,Mk) = ?
?
t??
P (t|s,Mk) logP (t|s,Mk) (5)
? denotes the set of analyses produced by the ERG
for the sentence and Mk is some model. Higher val-
ues of fus(s, ?,Mk) indicate examples on which the
learner is most uncertain . Calculating fus is triv-
ial with the conditional log-linear and perceptrons
models described in section 2.2.
Uncertainty sampling as defined above is a single-
model approach. It can be improved by simply re-
placing the probability of a single log-linear (or per-
ceptron) model with a product probability:
fenus (s, ?,M) = ?
?
t??
P (t|s,M) logP (t|s,M) (6)
M is the set of models M1 . . .Mn. As we men-
tioned earlier, AL for parse selection is potentially
problematic as sentences vary both in length and the
number of parses they have. Nonetheless, the above
measures do not use any extra normalization as we
have found no major differences after experimenting
with a variety of normalization strategies.
We use random sampling as a baseline and un-
certainty sampling for AL. Osborne and Baldridge
(2004) show that uncertainty sampling produces
good results compared with other AL methods.
5 Experimental framework
For all experiments, we used a 20-fold cross-
validation strategy by randomly selecting 10%
(roughly 500 sentences) for the test set and select-
ing samples from the remaining 90% (roughly 4500
sentences) as training material. Each run of AL be-
gins with a single randomly chosen annotated seed
sentence. At each round, new examples are selected
for annotation from a randomly chosen, fixed sized
500 sentence subset according to random selection
or uncertainty sampling until models reach certain
desired accuracies. We select 20 examples for anno-
tation at each round, and exclude all examples that
have more than 500 parses.2
2Other parameter settings (such as how many examples to
label at each stage) did not produce substantially different re-
sults to those reported here.
AL results are usually presented in terms of the
amount of labeling necessary to achieve given per-
formance levels. We say that one method is bet-
ter than another method if, for a given performance
level, less annotation is required. The performance
metric used here is parse selection accuracy as de-
scribed in section 2.3.
6 Reusing training material
AL can be considered as selecting some labeled
training set which is ?tuned? to the needs of a particu-
lar model. Typically, we might wish to reuse labeled
training material, so a natural question to ask is how
general are training sets created using AL. So, if we
later improved upon our feature set, or else improved
upon our learner, would the previously created train-
ing set still be useful? If AL selects highly idiosyn-
cratic datasets then we would not be able to reuse our
datasets and thus it might, for example, actually be
better to label datasets using random sampling. This
is a realistic situation since models typically change
and evolve over time ? it would be very problem-
atic if the training set itself inherently limits the ben-
efit of later attempts to improve the model.
We use two baselines to evaluate how well a
model is able to reuse data selected for labeling by
another model: (1) Selecting the data randomly.
This provides the essential baseline; if AL in reuse
situations is going to be useful, it ought to outper-
form this model-free approach. (2) Reuse by the
AL model itself. This is the standard AL scenario;
against this, we can determine if reused data can be
as good as when a model selects data for itself.
We evaluate a variety of reuse scenarios. We re-
fer to the model used with AL as the selector and
the model that is reusing that labeled data as the
reuser. Models can differ in the machine learning al-
gorithm and/or the feature set they use. To measure
relatedness, we use Spearman?s rank correlation on
the rankings that two models assign to the parses of
a sentence. The overall relatedness of two models
is calculated as the average rank correlation on all
examples tested in a 10-fold parse selection experi-
ment using all available training material.
Figure 1 shows complete learning curves for LL-
CONFIG when it reuses material selected by itself,
LL-CONGLOM, P-MRS, and random sampling. The
graph shows that self-reuse is the most effective of
all strategies ? this is the idealized situation com-
monly assumed in active learning studies. However,
the graph reveals that random sampling is actually
more effective than selection both by LL-CONGLOM
until nearly 70% accuracy is reached and by P-MRS
until about 73%. Finally, we see that the material
selected by LL-CONGLOM is always more effective
for LL-CONFIG than that selected by P-MRS. The
reason for this can be explained by the relatedness
of each of these selector models to LL-CONFIG: LL-
CONGLOM and LL-CONFIG have an average rank
correlation of 0.84 whereas P-MRS and LL-CONFIG
have a correlation of 0.65.
 50
 55
 60
 65
 70
 75
 80
 0  1000  2000  3000  4000  5000  6000  7000  8000
A
c
c
u
r
a
c
y
Annotation cost
Selector: LL-CONFIGSelector: LL-CONGLOMSelector: RANDSelector: P-MRS
Figure 1: Learning curves for LL-CONFIG when
reusing material by different selectors.
Table 2 fleshes out the relationship between relat-
edness and reusability more fully. It shows the anno-
tation cost incurred by various reusers to reach 65%,
70%, and 73% accuracy when material is selected
by various models. The list is ordered from top to
bottom according to the rank correlation of the two
models. The first three lines provide the baselines of
when LL-PROD, LL-CONGLOM, and LL-CONFIG se-
lect material for themselves. The last three show the
amount of material needed by these models when
random sampling is used. The rest gives the results
for when the selector differs from the reuser.
For each performance level, the percent increase
in annotation cost over self-reuse is given. For
example, a cost of 2300 discriminants is required
for LL-PROD to reach the 73% performance level
when it reuses material selected by LL-CONGLOM;
this is a 10% increase over the 2100 discriminants
needed when LL-PROD selects for itself. Similarly,
the 5500 discriminants needed by LL-CONGLOM to
reach 73% when reusing material selected by LL-
CONFIG is a 31% increase over the 4200 discrimi-
nants LL-CONGLOM needs with its own selection.
As can be seen from Table 2, reuse always leads
to an increase in cost over self-reuse to reach a given
level of performance. How much that increase will
be is in general inversely related to the rank corre-
lation of the two models. Furthermore, considering
each reusing model individually, this relationship is
almost entirely inversely related at all performance
levels, with the exception of P-CONGLOM and LL-
MRS selecting for LL-CONFIG at the 73% level.
The reason for some models being more related
to others is generally easy to see. For example, LL-
CONFIG and LL-CONGLOM are highly related to LL-
PROD, of which they are both components. In both
of these cases, using AL for use by LL-PROD beats
random sampling by a large amount.
That LL-MRS is more related to LL-CONGLOM
than to LL-CONFIG is explained by the fact the mrs
feature set is actually a subset of the conglom set.
The former contains 15% of the latter?s features.
Accordingly, material selected by LL-MRS is also
generally more reusable by LL-CONGLOM than to
LL-CONFIG. This is encouraging since the case of
LL-CONGLOM reusing material selected by LL-MRS
represents the common situation in which an initial
model ? that was used to develop the corpus ? is
continually improved upon.
A particularly striking aspect revealed by Figure 1
and Table 2 is that random sampling is overwhelm-
ingly a better strategy when there is still little la-
beled material. AL tends to select examples which
are more ambiguous and hence have a higher dis-
criminant cost. So, while these examples may be
highly informative for the selector model, they are
not cheap ? and are far less effective when reused
by another model.
Considering unit cost (i.e., each sentence costs the
same) instead of discriminant cost (which assigns a
variable cost per sentence), AL is generally more
effective than random sampling for reuse through-
out all accuracy levels ? but not always. For exam-
ple, even using unit cost, random sampling is bet-
ter than selection by LL-MRS or P-MRS for reuse by
Rank 65% 70% 73%
Selector Reuser Corr. DC Incr DC Incr DC Incr
LL-PROD LL-PROD 1.00 690 0.0% 1200 0.0% 2050 0.0%
LL-CONGLOM LL-CONGLOM 1.00 1190 0.0% 2330 0.0% 4160 0.0%
LL-CONFIG LL-CONFIG 1.00 1160 0.0% 2530 0.0% 4780 0.0%
LL-CONFIG LL-PROD .92 850 23.2% 1470 22.5% 2430 18.5%
LL-CONGLOM LL-PROD .92 840 21.7% 1560 30.0% 2630 28.3%
LL-CONFIG LL-CONGLOM .84 1340 12.6% 2610 12.0% 4720 13.5%
LL-CONGLOM LL-CONFIG .84 1660 43.1% 3760 48.6% 6840 43.1%
P-CONGLOM LL-CONFIG .79 1960 69.0% 3910 54.5% 7940 66.1%
LL-MRS LL-CONGLOM .77 1600 34.5% 3400 45.9% 6420 54.3%
LL-MRS LL-PROD .76 1080 56.5% 2040 70.0% 3700 80.5%
LL-MRS LL-CONFIG .71 2100 81.0% 4270 68.8% 6870 43.7%
P-MRS LL-CONFIG .65 2650 128.4% 4870 92.5% 8260 72.8%
RAND LL-PROD - 820 18.8% 1950 62.5% 3680 79.5%
RAND LL-CONGLOM - 1400 17.6% 3470 48.9% 7150 71.9%
RAND LL-CONFIG - 1160 0.0% 3890 53.8% 8560 79.1%
Table 2: Comparison of various selection and reuse conditions. Values are given for discriminant cost (DC)
and the percent increase (Incr) in cost over use of material selected by the reuser.
LL-CONFIG until 67% accuracy. Thus, LL-MRS and
P-MRS are so divergent from LL-CONFIG that their
selections are truly sub-optimal for LL-CONFIG, par-
ticularly in the initial stages.
Together, these results shows that AL cannot be
used blindly and always be expected to reduce the
total cost of annotation. The data is tuned to the
models used during AL and how useful that data
will be for other models depends on the degree of
relatedness of the models under consideration.
Given that AL may or may not provide cost reduc-
tions, we consider the effect that semi-automating
annotation has on reducing the total cost of annota-
tion when used with and without AL.
7 Semi-automated labeling
Corpus building, with or without AL, is generally
viewed as selecting examples and then from scratch
labeling such examples. This can be inefficient, es-
pecially when dealing with labels that have complex
internal structures, as a model may be able to rule-
out some of the labeling possibilities.
For our domain, we exploit the fact that we may
already have partial information about an example?s
label by presenting only the top n-best parses to
the annotator, who then navigates to the best parse
within that set using those discriminants relevant to
that set of parses. Rather than using a value for n
that is fixed or proportional to the ambiguity of the
sentence, we simply select all parses for which the
model assigns a probability higher than chance. This
has the advantage of reducing the number of parses
presented to the annotator as the model uses more
training material and reduces its uncertainty.
When the true best parse is within the top n pre-
sented to the annotator, the cost we record is the
number of discriminants needed to identify it from
that subset, plus one ? the same calculation as when
all parses are presented, with the advantage that
fewer discriminants and parses need to be inspected.
When the best parse is not present in the n-best
subset, there is a question as to how to record the
annotation cost. The discriminant decisions made
in reducing the subset are still valid and useful in
identifying the best parse from the entire set, but we
must incur some penalty for the fact that the anno-
tator must confirm that this is the case. To deter-
mine the cost for such situations, we add one to the
usual full cost of annotating the sentence. This en-
codes what we feel is a reasonable reflection of the
penalty since decisions taken in the n-best phase are
still valid in the context of all parses.3
Performance level
65% 70% 73%
1. RAND 820 1950 3680
2. LL-PROD 690 1200 2050
3. RAND (NB) 670 1350 2430
4. LL-PROD (NB) 680 1120 1760
Table 3: Cost for LL-PROD to reach given perfor-
mance levels when using n-best automation (NB).
Table 3 shows the effects of using semi-automated
labeling with LL-PROD. As can be seen, random
selection costs reduce dramatically with n-best au-
tomation (compare rows 1 and 3). It is also an early
winner over basic uncertainty sampling (row 2),
though the latter eventually reaches the higher ac-
curacies more quickly. Nonetheless, the mixture of
AL and semi-automation provides the biggest over-
all gains: to reach 73% accuracy, n-best uncertainty
sampling (row 4) reduces the cost by 17% over n-
best random sampling (row 3) and by 15% over ba-
sic uncertainty sampling (row 2). Similar patterns
hold for n-best automation with LL-CONFIG.
Figure 2 provides an overall view on the accumu-
lative effects of ensembling, n-best automation, and
uncertainty sampling in the ideal situation of reuse
by the AL model itself. Ensemble models and n-best
automation show that massive improvements can be
made without AL. Nonetheless, we see the largest
reductions by using AL, n-best automation, and en-
semble models together: LL-PROD using uncertainty
sampling and n-best automation (row 4 of Table 3)
reaches 73% accuracy with a cost of 1760 compared
to 8560 needed by LL-CONFIG using random sam-
pling without automation. This is our best annota-
tion saving: a cost reduction of 80%.
8 Closing the reuse gap
The previous section?s semi-automated labeling ex-
periments did not involve reuse. If models are ex-
pected to evolve, could n-best automation fill in the
cost gap created by reuse? To test this, we con-
sidered reusing examples with our best model (LL-
3When we do not allow ourselves to benefit from such la-
beling decisions, our annotation savings naturally decrease, but
not below when we do not use n-best labeling.
 50
 55
 60
 65
 70
 75
 80
 0  500  1000  1500  2000  2500  3000  3500  4000  4500  5000
A
c
c
u
r
a
c
y
Annotation cost
LL-PRODUCT, N-best, Uncertainty samplingLL-PRODUCT, N-best, Random samplingLL-PRODUCT, Random samplingLL-CONFIG, Random sampling
Figure 2: Learning curves for accumulative im-
provements to the annotation scenario starting from
random sampling with LL-CONFIG: ensembling, n-
best automation, and uncertainty sampling.
PROD), as selected by different models using both
AL and n-best automation as a combined strategy.
For LL-CONFIG and LL-CONGLOM as selectors, the
gap is entirely closed: costs for reuse were virtually
equal to when LL-PROD selects examples for itself
without n-best (Table 3, row 2).
The gap also closes when n-best automation and
AL are used with the weaker LL-MRS model. Per-
formance (Table 4, row 1) still falls far short of LL-
PROD selecting for itself without n-best (Table 3,
row 2). However, the gap closes even more when n-
best automation and random sampling are used with
LL-MRS (Table 4, line 2).
Performance level
65% 70% 73%
1. NB & US 1040 1920 3320
2. NB & RAND 680 1450 2890
Table 4: Cost for LL-PROD to reach given perfor-
mance levels in reuse situations where n-best au-
tomation (NB) was used with LL-MRS with uncer-
tainty sampling (US) or random sampling (RAND).
Interestingly, when using a weak selector (LL-
MRS), n-best automation combined with random
sampling was more effective than when combined
with uncertainty sampling. The reason for this is
clear. Since AL typically selects more ambiguous
examples, a weak model has more difficulty getting
the best parse within the n-best when AL is used.
Thus, the gains from the more informative examples
selected by AL are surpassed by the gains that come
with the easier labeling with random sampling.
For most situations, n-best automation is benefi-
cial: the gap introduced by reuse can be reduced. n-
best automation never results in an increase in cost.
This is still true even if we do not allow ourselves to
reuse those discriminants which were used to select
the best parse from the n-best subset and the best
parse was not actually present in that subset.
9 Related work
There is a large body of AL work in the machine
learning literature, but less so within natural lan-
guage processing (NLP). Most work in NLP has
primarily focused upon uncertainty sampling (Hwa,
2000; Tang et al, 2002). Hwa (2001) considered
reuse of examples selected for one parser by an-
other with uncertainty sampling. This performed
better than sequential sampling but was only half as
effective as self-selection. Here, we have consid-
ered reuse with respect to many models and their
co-relatedness. Also, we compare reuse perfor-
mance against against random sampling, which we
showed previously to be a much stronger baseline
than sequential sampling for the Redwoods corpus
(Osborne and Baldridge, 2004). Hwa et al (2003)
showed that for parsers, AL outperforms the closely
related co-training, and that some of the labeling
could be automated. However, their approach re-
quires strict independence assumptions.
10 Discussion
AL should only be considered for creating labeled
data when the the task is either well-understood or
else the model is unlikely to substantially change.
Otherwise, it would be prudent to consider improv-
ing either the model itself (using, for example, en-
semble techniques) or else semi-automating the la-
beling task. Naturally, there is a cost associated with
creating the model itself, and this in turn will need
to be factored into the total cost. When there is gen-
uine uncertainty about the model, or else how the
labeled data is going to be eventually used, then the
best strategy may well be to use random selection
rather than AL ? especially when using some form
of automated annotation.
Acknowledgments
We would like to thank Markus Becker, Jeremiah
Crim, Dan Flickinger, Alex Lascarides, Stephan
Oepen, and Andrew Smith. We?d also like to
thank pc-jbaldrid and pc-rosie for their
hard work and 24/7 dedication. This work was sup-
ported by Edinburgh-Stanford Link R36763, ROSIE
project.
References
David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan.
1995. Active learning with statistical models. In G. Tesauro,
D. Touretzky, and T. Leen, editors, Advances in Neural Infor-
mation Processing Systems, volume 7, pages 705?712. The
MIT Press.
Dan Flickinger. 2000. On building a more efficient grammar by
exploiting types. Natural Language Engineering, 6(1):15?
28. Special Issue on Efficient Processing with HPSG.
G. E. Hinton. 1999. Products of experts. In Proc. of the 9th Int.
Conf. on Artificial Neural Networks, pages 1?6.
Rebecca Hwa, Miles Osborne, Anoop Sarkar, and Mark Steed-
man. 2003. Corrected Co-training for Statistical Parsers. In
Proceedings of the ICML Workshop ?The Continuum from
Labeled to Unlabeled Data?, pages 95?102. ICML-03.
Rebecca Hwa. 2000. Sample selection for statistical gram-
mar induction. In Proc. of the 2000 Joint SIGDAT Conf. on
EMNLP and VLC, pages 45?52, Hong Kong, China.
Rebecca Hwa. 2001. On minimizing training corpus for parser
acquisition. In Proc. of the 5th Conference on Natural Lan-
guage Learning, Toulouse.
Mark Johnson, Stuart Geman, Stephen Cannon, Zhiyi Chi,
and Stephan Riezler. 1999. Estimators for Stochastic
?Unification-Based? Grammars. In 37th Annual Meeting of
the ACL.
Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher
Manning, Dan Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods Treebank: Motivation and preliminary ap-
plications. In Proc. of the 19th International Conference on
Computational Linguistics, Taipei, Taiwan.
Miles Osborne and Jason Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proc. of HLT-NAACL,
Boston.
H. S. Seung, Manfred Opper, and Haim Sompolinsky. 1992.
Query by committee. In Computational Learning Theory,
pages 287?294.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Ac-
tive Learning for Statistical Natural Language Parsing. In
Proc. of the 40th Annual Meeting of the ACL, pages 120?
127, Philadelphia, Pennsylvania, USA, July.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 96?103, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Probabilistic Head-Driven Parsing for Discourse Structure
Jason Baldridge and Alex Lascarides
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW
Scotland, UK
{jbaldrid,alex}@inf.ed.ac.uk
Abstract
We describe a data-driven approach to
building interpretable discourse structures
for appointment scheduling dialogues. We
represent discourse structures as headed
trees and model them with probabilis-
tic head-driven parsing techniques. We
show that dialogue-based features regard-
ing turn-taking and domain specific goals
have a large positive impact on perfor-
mance. Our best model achieves an f -
score of 43.2% for labelled discourse rela-
tions and 67.9% for unlabelled ones, sig-
nificantly beating a right-branching base-
line that uses the most frequent relations.
1 Introduction
Achieving a model of discourse interpretation that is
both robust and deep is a major challenge. Consider
the dialogue in Figure 1 (the sentence numbers are
from the Redwoods treebank (Oepen et al, 2002)).
A robust and deep interpretation of it should resolve
the anaphoric temporal description in utterance 154
to the twenty sixth of July in the afternoon. It should
identify that time and before 3pm on the twenty-
seventh as potential times to meet, while ruling out
July thirtieth to August third. It should gracefully
handle incomplete or ungrammatical utterances like
152 and recognise that utterances 151 and 152 have
no overall effect on the time and place to meet.
According to Hobbs et al (1993) and Asher and
Lascarides (2003), a discourse structure consisting
of hierarchical rhetorical connections between utter-
ances is vital for providing a unied model of a wide
149 PAM: maybe we can get together, and, discuss, the
planning, say, two hours, in the next, couple weeks,
150 PAM: let me know what your schedule is like.
151 CAE: okay, let me see.
152 CAE: twenty,
153 CAE: actually, July twenty sixth and twenty seventh looks
good,
154 CAE: the twenty sixth afternoon,
155 CAE: or the twenty seventh, before three p.m., geez.
156 CAE: I am out of town the thirtieth through the,
157 CAE: the third, I am in San Francisco.
Figure 1: A dialogue extract from Redwoods.
range of anaphoric and intentional discourse phe-
nomena, contributing to the interpretations of pro-
nouns, temporal expressions, presuppositions and
ellipses (among others), as well as influencing com-
municative goals. This suggests that a robust model
of discourse structure could complement current ro-
bust interpretation systems, which tend to focus on
only one aspect of the semantically ambiguous ma-
terial, such as pronouns (e.g., Stru?be and Mu?ller
(2003)), definite descriptions (e.g., Vieira and Poe-
sio (2000)), or temporal expressions (e.g., Wiebe
et al (1998)). This specialization makes it hard to
assess how they would perform in the context of a
more comprehensive set of interpretation tasks.
To date, most methods for constructing discourse
structures are not robust. They typically rely on
grammatical input and use symbolic methods which
inevitably lack coverage. One exception is Marcu?s
work (Marcu, 1997, 1999) (see also Soricut and
Marcu (2003) for constructing discourse structures
for individual sentences). Marcu (1999) uses a
decision-tree learner and shallow syntactic features
96
to create classifiers for discourse segmentation and
for identifying rhetorical relations. Together, these
amount to a model of discourse parsing. However,
the results are trees of Rhetorical Structure Theory
(RST) (Mann and Thompson, 1986), and the clas-
sifiers rely on well-formedness constraints on RST
trees which are too restrictive (Moore and Pollack,
1992). Furthermore, RST does not offer an account
of how compositional semantics gets augmented,
nor does it model anaphora. It is also designed for
monologue rather than dialogue, so it does not of-
fer a precise semantics of questions or non-sentential
utterances which convey propositional content (e.g.,
154 and 155 in Figure 1). Another main approach to
robust dialogue processing has been statistical mod-
els for identifying dialogue acts (e.g., Stolcke et al
(2000)). However, dialogue acts are properties of
utterances rather than hierarchically arranged rela-
tions between them, so they do not provide a basis
for resolving semantic underspecification generated
by the grammar (Asher and Lascarides, 2003).
Here, we present the first probabilistic approach
to parsing the discourse structure of dialogue.
We use dialogues from Redwoods? appointment
scheduling domain and adapt head-driven genera-
tive parsing strategies from sentential parsing (e.g.,
Collins (2003)) for discourse parsing. The discourse
structures we build conform to Segmented Dis-
course Representation Theory (SDRT) (Asher and
Lascarides, 2003). SDRT provides a precise dynamic
semantic interpretation for its discourse structures
which augments the conventional semantic repre-
sentations that are built by most grammars. We thus
view the task of learning a model of SDRT-style dis-
course structures as one step towards achieving the
goal of robust and precise semantic interpretations.
We describe SDRT in the context of our domain
in Section 2. Section 3 discusses how we encode
and annotate discourse structures as headed trees
for our domain. Section 4 provides background on
probabilistic head-driven parsing models, and Sec-
tion 5 describes how we adapt the approach for dis-
course and gives four models for discourse parsing.
We report results in Section 6, which show the im-
portance of dialogue-based features on performance.
Our best model performs far better than a baseline
that uses the most frequent rhetorical relations and
right-branching segmentation.
h0 : Request-Elab(149, 150)?
Plan-Elab(150, h1)
h1 : Elaboration(153, h2)?
Continuation(153, 156)?
Continuation(156, 157)
h2 : Alternation(154, 155)
Figure 2: The SDRS for the dialogue in Figure 1.
2 Segmented Discourse Representation
Theory
SDRT extends prior work in dynamic semantics (e.g.,
van Eijk and Kamp (1997)) via logical forms that
feature rhetorical relations. The logical forms con-
sist of speech act discourse referents which la-
bel content (either of a clause or of text seg-
ments). Rhetorical relations such as Explanation
relate these referents. The resulting structures are
called segmented discourse representation struc-
tures or SDRSs. An SDRS for the dialogue in Fig-
ure 1 is given in Figure 2; we have used the numbers
of the elementary utterances from Redwoods as the
speech act discourse referents but have omitted their
labelled logical forms. Note that utterances 151 and
152, which do not contribute to the truth conditions
of the dialogue, are absent ? we return to this shortly.
There are several things to note about this SDRS.
First, SDRT?s dynamic semantics of rhetorical rela-
tions imposes constraints on the contents of its argu-
ments. For example, Plan-Elab(150, h1) (standing
for Plan-Elaboration) means that h1 provides infor-
mation from which the speaker of 150 can elaborate
a plan to achieve their communicative goal (to meet
for two hours in the next couple of weeks). The
relation Plan-Elab contrasts with Plan-Correction,
which would relate the utterances in dialogue (1):
(1) a. A: Can we meet at the weekend?
b. B: I?m afraid I?m busy then.
Plan-Correction holds when the content of the sec-
ond utterance in the relation indicates that its com-
municative goals conflict with those of the first one.
In this case, A indicates he wants to meet next week-
end, and B indicates that he does not (note that then
resolves to the weekend). Utterances (1ab) would
also be related with IQAP (Indirect Question Answer
97
Pair): this means that (1b) provides sufficient infor-
mation for the questioner A to infer a direct answer
to his question (Asher and Lascarides, 2003).
The relation Elaboration(153, h2) in Figure 2
means that the segment 154 to 155 resolves to a
proposition which elaborates part of the content of
the proposition 153. Therefore the twenty sixth in
154 resolves to the twenty sixth of July?any other
interpretation contradicts the truth conditional con-
sequences of Elaboration. Alternation(154, 155)
has truth conditions similar to (dynamic) disjunc-
tion. Continuation(156, 157) means that 156 and
157 have a common topic (here, this amounts to a
proposition about when CAE is unavailable to meet).
The second thing to note about Figure 2 is how
one rhetorical relation can outscope another: this
creates a hierarchical segmentation of the discourse.
For example, the second argument to the Elabo-
ration relation is the label h2 of the Alternation-
segment relating 154 to 155. Due to the semantics
of Elaboration and Alternation, this ensures that the
dialogue entails that one of 154 or 155 is true, but it
does not entail 154, nor 155.
Finally, observe that SDRT allows for a situ-
ation where an utterance connects to more than
one subsequent utterance, as shown here with
Elaboration(153, h2) ? Continuation(153, 156). In
fact, SDRT also allows two utterances to be related
by multiple relations (see (1)) and it allows an utter-
ance to rhetorically connect to multiple utterances in
the context. These three features of SDRT capture the
fact that an utterance can make more than one illo-
cutionary contribution to the discourse. An example
of the latter kind of structure is given in (2):
(2) a. A: Shall we meet on Wednesday?
b. A: How about one pm?
c. B: Would one thirty be OK with you?
The SDRS for this dialogue would feature the re-
lations Plan-Correction(2b, 2c), IQAP(2b, 2c) and
Q-Elab(2a, 2c). Q-Elab, or Question-Elaboration,
always takes a question as its second argument;
any answers to the question must elaborate a plan
to achieve the communicative goal underlying the
first argument to the relation. From a logical per-
spective, recognising Plan-Correction(2b, 2c) and
Q-Elab(2a, 2c) are co-dependent.
Segment/h0
ind
149
Request-Elab
imp
150
Plan-Elab
Segment/h1
Pass
pause
151
irr
152
ind
153
Elaboration/h2
ind
154
Alternation
ind
155
Continuation
ind
156
Continuation
ind
157
Figure 3: The discourse structure for the dialogue
from Figure 1 in tree form.
3 Augmenting the Redwoods treebank
with discourse structures
Our starting point is to create training material for
probabilistic discourse parsers. For this, we have
augmented dialogues from the Redwoods Treebank
(Oepen et al, 2002) with their analyses within a
fragment of SDRT (Baldridge and Lascarides, 2005).
This is a very different effort from that being pur-
sued for the Penn Discourse Treebank (Miltsakaki
et al, 2004), which uses discourse connectives
rather than abstract rhetorical relations like those in
SDRT in order to provide theory neutral annotations.
Our goal is instead to leverage the power of the se-
mantics provided by SDRT?s relations, and in partic-
ular to do so for dialogue as opposed to monologue.
Because the SDRS-representation scheme, as
shown in Figure 2, uses graph structures that do not
conform to tree constraints, it cannot be combined
directly with statistical techniques from sentential
parsing. We have therefore designed a headed tree
encoding of SDRSs, which can be straightforwardly
modeled with standard parsing techniques and from
which SDRSs can be recovered.
For instance, the tree for the dialogue in Figure 1
is given in Figure 3. The SDRS in Figure 2 is recov-
ered automatically from it. In this tree, utterances
are leaves which are immediately dominated by their
tag, indicating either the sentence mood (indicative,
interrogative or imperative) or that it is irrelevant, a
pause or a pleasantry (e.g., hello), annotated as pls.
Each non-terminal node has a unique head daugh-
ter: this is either a Segment node, Pass node, or a
98
leaf utterance tagged with its sentence mood. Non-
terminal nodes may in addition have any number of
daughter irr, pause and pls nodes, and an additional
daughter labelled with a rhetorical relation.
The notion of headedness has no status in the se-
mantics of SDRSs themselves. The heads of these
discourse trees are not like verbal heads with sub-
categorization requirements in syntax; here, they are
nothing more than the left argument of a rhetor-
ical relation, like 154 in Alternation(154, 155).
Nonetheless, defining one of the arguments of
rhetorical relations as a head serves two main pur-
poses. First, it enables a fully deterministic algo-
rithm for recovering SDRSs from these trees. Sec-
ond, it is also crucial for creating probabilistic head-
driven parsing models for discourse structure.
Segment and Pass are non-rhetorical node types.
The former explicitly groups multiple utterances.
The latter allows its head daughter to enter into re-
lations with segments higher in the tree. This allows
us to represent situations where an utterance attaches
to more than one subsequent utterance, such as 153
in dialogue (1). Annotators manually annotate the
rhetorical relation, Segment and Pass nodes and de-
termine their daughters. They also tag the individual
utterances with one of the three sentence moods or
irr, pause or pls. The labels for segments (e.g., h0
and h1 in Figure 3) are added automatically. Non-
veridical relations such as Alternation also introduce
segment labels on their parents; e.g., h2 in Figure 3.
The SDRS is automatically recovered from this
tree representation as follows. First, each rela-
tion node generates a rhetorical connection in the
SDRS: its first argument is the discourse referent
of its parent?s head daughter, and the second is the
discourse referent of the node itself (which unless
stated otherwise is its head daughter?s discourse ref-
erent). For example, the structure in Figure 3 yields
Request-Elab(149, 150), Alternation(154, 155) and
Elaboration(153, h2). The labels for the relations
in the SDRS?which determine segmentation?must
also be recovered. This is easily done: any node
which has a segment label introduces an outscopes
relation between that and the discourse referents
of the node?s daughters. This produces, for ex-
ample, outscopes(h0, 149), outscopes(h1, 153) and
outscopes(h2, 154). It is straightforward to deter-
mine the labels of all the rhetorical relations from
these conditions. Utterances such as 151 and 152,
which are attached with pause and irr to indicate that
they have no overall truth conditional effect on the
dialogue, are ignored when constructing the SDRS,
so SDRT does not assign these terms any semantics.
Overall, this algorithm generates the SDRS in Fig-
ure 2 from the tree in Figure 3.
Thus far, 70 dialogues have been annotated and
reviewed to create our gold-standard corpus. On av-
erage, these dialogues have 237.5 words, 31.5 ut-
terances, and 8.9 speaker turns. In all, there are 30
different rhetorical relations in the inventory for this
annotation task, and 6 types of tags for the utterances
themselves: ind, int, imp, pause, irr and pls.
Finally, we annotated all 6,000 utterances in the
Verbmobil portion of Redwoods with the following:
whether the time mentioned (if there is one) is a
good time to meet (e.g., I?m free then or Shall we
meet at 2pm?) or a bad time to meet (e.g., I?m busy
then or Let?s avoid meeting at the weekend). These
are used as features in our model of discourse struc-
ture (see Section 5). We use these so as to minimise
using directly detailed features from the utterances
themselves (e.g. the fact that the utterance contains
the word free or busy, or that it contains a negation),
which would lead to sparse data problems given the
size of our training corpus. We ultimately aim to
learn good-time and bad-time from sentence-level
features extracted from the 6,000 Redwoods analy-
ses, but we leave this to future work.
4 Generative parsing models
There is a significant body of work on probabilistic
parsing, especially that dealing with the English sen-
tences found in the annotated Penn Treebank. One
of the most important developments in this work is
that of Collins (2003). Collins created several lex-
icalised head-driven generative parsing models that
incorporate varying levels of structural information,
such as distance features, the complement/adjunct
distinction, subcategorization and gaps. These mod-
els are attractive for constructing our discourse trees,
which contain heads that establish non-local depen-
dencies in a manner similar to that in syntactic pars-
ing. Also, the co-dependent tasks of determining
segmentation and choosing the rhetorical connec-
tions are both heavily influenced by the content of
99
the utterances/segments which are being considered,
and lexicalisation allows the model to probabilisti-
cally relate such utterances/segments very directly.
Probabilistic Context Free Grammars (PCFGs)
determine the conditional probability of a right-
hand side of a rule given the left-hand side,
P(RHS|LHS). Collins instead decomposes the
calculation of such probabilities by first generating a
head and then generating its left and right modifiers
independently. In a supervised setting, doing this
gathers a much larger set of rules from a set of la-
belled data than a standard PCFG, which learns only
rules that are directly observed.1
The decomposition of a rule begins by noting that
rules in a lexicalised PCFG have the form:
P (h) ? Ln(ln) . . . L1(l1)H(h)R1(r1) . . . Rm(rm)
where h is the head word, H(h) is the label of the
head constituent, P (h) is its parent, and Li(li) and
Ri(ri) are the n left and m right modifiers, respec-
tively. It is also necessary to include STOP sym-
bols Ln+1 and Rm+1 on either side to allow the
Markov process to properly model the sequences of
modifiers. By assuming these modifiers are gener-
ated independently of each other but are dependent
on the head and its parent, the probability of such
expansions can be calculated as follows (where Ph,
Pl and Pr are the probabilities for the head, left-
modifiers and right-modifiers respectively):
P(Ln(ln) . . . L1(l1)H(h)R1(r1) . . . Rm(rm)|P (h)) =
Ph(H|P (h))
?
Y
i=1...n+1
Pl(Li(li)|P (h), H)
?
Y
i=1...m+1
Pr(Ri(ri)|P (h), H)
This provides the simplest of models. More con-
ditioning information can of course be added from
any structure which has already been generated. For
example, Collins? model 1 adds a distance feature
that indicates whether the head and modifier it is
generating are adjacent and whether a verb is in the
string between the head and the modifier.
1A similar effect can be achieved by converting n-ary trees
to binary form.
5 Discourse parsing models
In Section 3, we outlined how SDRSs can be repre-
sented as headed trees. This allows us to create pars-
ing models for discourse that are directly inspired by
those described in the previous section. These mod-
els are well suited for our discourse parsing task.
They are lexicalised, so there is a clear place in the
discourse model for incorporating features from ut-
terances: simply replace lexical heads with whole
utterances, and exploit features from those utter-
ances in discourse parsing in the same manner as
lexical features are used in sentential parsing.
Discourse trees contain a much wider variety of
kinds of information than syntactic trees. The leaves
of these trees are sentences with full syntactic and
semantic analyses, rather than words. Furthermore,
each dialogue has two speakers, and speaker style
can change dramatically from dialogue to dialogue.
Nonetheless, the task is also more constrained in
that there are fewer overall constituent labels, there
are only a few labels which can act as heads, and
trees are essentially binary branching apart from
constituents containing ignorable utterances.
The basic features we use are very similar to those
for the syntactic parsing model given in the previous
section. The feature P is the parent label that is the
starting point for generating the head and its modi-
fiers. H is the label of the head constituent. The tag
t is also used, except that rather than being a part-of-
speech, it is either a sentence mood label (ind, int, or
imp) or an ignorable label (irr, pls, or pause). The
word feature w in our model is the first discourse cue
phrase present in the utterance.2 In the absence of a
cue phrase, w is the empty string. The distance fea-
ture ? is true if the modifier being generated is adja-
cent to the head and false otherwise. To incorporate
a larger context into the conditioning information,
we also utilize a feature HCR, which encodes the
child relation of a node?s head.
We have two features that are particular to dia-
logue. The first ST , indicates whether the head ut-
terance of a segment starts a turn or not. The other,
TC, encodes the number of turn changes within a
segment with one of the values 0, 1, or ? 2.
Finally, we use the good/bad-time annotations
discussed in Section 3 for a feature TM indicating
2We obtained our list of cue phrases from Oates (2001).
100
Head features Modifier features
P t w HCR ST TC TM P t w H ? HCR ST TC TM
Model 1 X X X X X X X X
Model 2 X X X X X X X X X X X X
Model 3 X X X X X X X X X X X X X X
Model 4 X X X X X X X X X X X X X X X X
Figure 4: The features active for determining the head and modifier probabilities in each of the four models.
one of the following values for the head utterance of
a segment: good time, bad time, neither, or both.
With these features, we create the four models
given in Figure 4. As example feature values, con-
sider the Segment node labelled h1 in Figure 3. Here,
the features have as values: P=Segment, H=Pass,
t=ind (the tag of utterance 153), w=Actually (see
153 in Figure 1), HCR=Elaboration, ST=false,
TC=0, and TM=good time.
As is standard, linear interpolation with back-off
levels of decreasing specificity is used for smooth-
ing. Weights for the levels are determined as in
Collins (2003).
6 Results
For our experiments, we use a standard chart parsing
algorithm with beam search that allows a maximum
of 500 edges per cell. The figure of merit for the
cut-off combines the probability of an edge with the
prior probability of its label, head and head tag. Hy-
pothesized trees that do not conform to some simple
discourse tree constraints are also pruned.3
The parser is given the elementary discourse units
as defined in the corpus. These units correspond di-
rectly to the utterances already defined in Redwoods
and we can thus easily access their complete syntac-
tic analyses directly from the treebank.
The parser is also given the correct utterance
moods to start with. This is akin to getting the cor-
rect part-of-speech tags in syntactic parsing. We
do this since we are using the parser for semi-
automated annotation. Tagging moods for a new
discourse is a very quick and reliable task for the
human. With them the parser can produce the more
complex hierarchical structure more accurately than
if it had to guess them ? with the potential to dra-
matically reduce the time to annotate the discourse
3E.g., nodes can have at most one child with a relation label.
structures of further dialogues. Later, we will create
a sentence mood tagger that presents an n-best list
for the parser to start with, from the tag set ind, int,
imp, irr, pause, and pls.
Models are evaluated by using a leave-one-out
strategy, in which each dialogue is parsed after train-
ing on all the others. We measure labelled and un-
labelled performance with both the standard PAR-
SEVAL metric for comparing spans in trees and a
relation-based metric that compares the SDRS?s pro-
duced by the trees. The latter gives a more direct in-
dication of the accuracy of the actual discourse log-
ical form, but we include the former to show perfor-
mance using a more standard measure. Scores are
globally determined rather than averaged over all in-
dividual dialogues.
For the relations metric, the relations from the
derived discourse tree for the test dialogue are ex-
tracted; then, the overlap with relations from the
corresponding gold standard tree is measured. For
labelled performance, the model is awarded a point
for a span or relation which has the correct discourse
relation label and both arguments are correct. For
unlabelled, only the arguments need to be correct.4
Figure 5 provides the f -scores5 of the various
models and compares them against those of a base-
line model and annotators. All differences between
models are significant, using a pair-wise t-test at
99.5% confidence, except that between the baseline
and Model 2 for unlabelled relations.
The baseline model is based on the most frequent
way of attaching the current utterance to its dia-
4This is a much stricter measure than one which measures
relations between a head and its dependents in syntax because
it requires two segments rather than two heads to be related cor-
rectly. For example, Model 4?s labelled and unlabelled relation
f-scores using segments are 43.2% and 67.9%, respectively; on
a head-to-head basis, they rise to 50.4% and 81.8%.
5The f -score is calculated as 2?precision?recallprecision+recall .
101
PARSEVAL Relations
Model Lab. Unlab. Lab. Unlab.
Baseline 14.7 33.8 7.4 53.3
Model 1 22.7 42.2 23.1 47.0
Model 2 30.1 51.1 31.0 54.3
Model 3 39.4 62.8 39.4 64.4
Model 4 46.3 69.2 43.2 67.9
Inter-annotator 53.7 76.5 50.3 73.0
Annotator-gold 75.9 88.0 75.3 84.0
Figure 5: Model performance.
logue context. The baseline is informed by the gold-
standard utterance moods. For this corpus, this re-
sults in a baseline which is a right-branching struc-
ture, where the relation Plan-Elaboration is used if
the utterance is indicative, Question-Elaboration if
it is interrogative, and Request-Elaboration if it is
imperative. The baseline also appropriately handles
ignorable utterances (i.e, those with the mood labels
irrelevant, pause, or pleasantry).
The baseline performs poorly on labelled rela-
tions (7.4%), but is more competitive on unlabelled
ones (53.3%). The main reason for this is that
it takes no segmentation risks. It simply relates
every non-ignorable utterance to the previous one,
which is indeed a typical configuration with com-
mon content-level relations like Continuation. The
generative models take risks that allow them to cor-
rectly identify more complex segments ? at the cost
of missing some of these easier cases.
Considering instead the PARSEVAL scores for the
baseline, the labelled performance is much higher
(14.7%) and the unlabelled is much lower (33.8%)
than for relations. The difference in labelled per-
formance is due to the fact that the intentional-level
relations used in the baseline often have arguments
that are multi-utterance segments in the gold stan-
dard. These are penalized in the relations compar-
ison, but the spans used in PARSEVAL are blind to
them. On the other hand, the unlabelled score drops
considerably ? this is due to poor performance on
dialogues whose gold standard analyses do not have
a primarily right-branching structure.
Model 1 performs most poorly of all the models.
It is significantly better than the baseline on labelled
relations, but significantly worse on unlabelled rela-
tions. All its features are derived from the structure
of the trees, so it gets no clues from speaker turns or
the semantic content of utterances.
Model 2 brings turns and larger context via the
ST and HCR features, respectively. This improves
segmentation over Model 1 considerably, so that the
model matches the baseline on unlabelled relations
and beats it significantly on labelled relations.
The inclusion of the TC feature in Model 3 brings
large (and significant) improvements over Model 2.
Essentially, this feature has the effect of penalizing
hypothesized content-level segments that span sev-
eral turns. This leads to better overall segmentation.
Finally, Model 4 incorporates the domain-based
TM feature that summarizes some of the semantic
content of utterances. This extra information im-
proves the determination of labelled relations. For
example, it is especially useful in distinguishing a
Plan-Correction from a Plan-Elaboration.
The overall trend of differences between PARSE-
VAL and relations scoring show that PARSEVAL is
tougher on overall segmentation and relations scor-
ing is tougher on whether a model got the right ar-
guments for each labelled relation. It is the latter
that ultimately matters for the discourse structures
produced by the parser to be useful; nonetheless, the
PARSEVAL scores do show that each model progres-
sively improves on capturing the trees themselves,
and that even Model 1 ? as a syntactic model ? is
far superior to the baseline for capturing the overall
form of the trees.
We also compare our best model against two up-
perbounds: (1) inter-annotator agreement on ten
dialogues that were annotated independently and
(2) the best annotator against the gold standard
agreed upon after the independent annotation phase.
For the first, the labelled/unlabelled relations f -
scores are 50.3%/73.0% and for the latter, they are
75.3%/84.0%?this is similar to the performance on
other discourse annotation projects, e.g., Carlson
et al (2001). On the same ten dialogues, Model 4
achieves 42.3%/64.9%.
It is hard to compare these models with Marcu?s
(1999) rhetorical parsing model. Unlike Marcu, we
did not use a variety of corpora, have a smaller train-
ing corpus, are analysing dialogues as opposed to
monologues, have a larger class of rhetorical re-
lations, and obtain the elementary discourse units
102
from the Redwoods annotations rather than estimat-
ing them. Even so, it is interesting that the scores
reported in Marcu (1999) for labelled and unlabelled
relations are similar to our scores for Model 4.
7 Conclusion
In this paper, we have shown how the complex task
of creating structures for SDRT can be adapted to a
standard probabilistic parsing task. This is achieved
via a headed tree representation from which SDRSs
can be recovered. This enables us to directly ap-
ply well-known probabilistic parsing algorithms and
use features inspired by them. Our results show
that using dialogue-based features are a major factor
in improving the performance of the models, both
in terms of determining segmentation appropriately
and choosing the right relations to connect them.
There is clearly a great deal of room for improve-
ment, even with our best model. Even so, that
model performed sufficiently well for use in semi-
automated annotation: when correcting the model?s
output on ten dialogues, one annotator took 30 sec-
onds per utterance, compared to 39 for another an-
notator working on the same dialogues with no aid.
In future work, we intend to exploit an exist-
ing implementation of SDRT?s semantics (Schlangen
and Lascarides, 2002), which adopts theorem prov-
ing to infer resolutions of temporal anaphora and
communicative goals from SDRSs for scheduling di-
alogues. This additional semantic content can in
turn be added (semi-automatically) to a training cor-
pus. This will provide further features for learn-
ing discourse structure and opportunities for learn-
ing anaphora and goal information directly.
Acknowledgments
This work was supported by Edinburgh-Stanford
Link R36763, ROSIE project. Thanks to Mirella La-
pata and Miles Osborne for comments.
References
N. Asher and A. Lascarides. Logics of Conversation. Cam-
bridge University Press, 2003.
J. Baldridge and A. Lascarides. Annotating discourse struc-
tures for robust semantic interpretation. In Proceedings of
the 6th International Workshop on Computational Seman-
tics, Tilburg, The Netherlands, 2005.
L. Carlson, D. Marcu, and M. Okurowski. Building a discourse-
tagged corpus in the framework of rhetorical structure the-
ory. In Proceedings of the 2nd SIGDIAL Workshop on Dis-
course and Dialogue, Eurospeech, 2001.
M. Collins. Head-driven statistical models for natural language
parsing. Computational Linguistics, 29(4):589?638, 2003.
J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. Interpretation
as abduction. Artificial Intelligence, 63(1?2):69?142, 1993.
W. C. Mann and S. A. Thompson. Rhetorical structure theory:
Description and construction of text structures. In G. Kem-
pen, editor, Natural Language Generation: New Results in
Artificial Intelligence, pages 279?300. 1986.
D. Marcu. The rhetorical parsing of unrestricted natural lan-
guage texts. In Proceedings of ACL/EACL, pages 96?103,
Somerset, New Jersey, 1997.
D. Marcu. A decision-based approach to rhetorical parsing.
In Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL99), pages 365?372,
Maryland, 1999.
E. Miltsakaki, R. Prasad, A. Joshi, and B. Webber. The Penn
Discourse TreeBank. In Proceedings of the Language Re-
sources and Evaluation Conference, Lisbon, Portugal, 2004.
J. D. Moore and M. E. Pollack. A problem for RST: The need
for multi-level discourse analysis. Computational Linguis-
tics, 18(4):537?544, 1992.
S. Oates. Generating multiple discourse markers in text. Mas-
ter?s thesis, ITRI, University of Brighton, 2001.
S. Oepen, E. Callahan, C. Manning, and K. Toutanova. LinGO
Redwoods?a rich and dynamic treebank for HPSG. In Pro-
ceedings of the LREC parsing workshop: Beyond PARSEVAL,
towards improved evaluation measures for parsing systems,
pages 17?22, Las Palmas, 2002.
D. Schlangen and A. Lascarides. Resolving fragments using
discourse information. In Proceedings of the 6th Interna-
tional Workshop on the Semantics and Pragmatics of Dia-
logue (Edilog), Edinburgh, 2002.
R. Soricut and D. Marcu. Sentence level discourse parsing using
syntactic and lexical information. In Proceedings of Human
Language Technology and North American Association for
Computational Linguistics, Edmonton, Canada, 2003.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, D. Jurafsky
R. Bates, P. Taylor, R. Martin, C. van Ess-Dykema, and
M. Meteer. Dialogue act modeling for automatic tagging and
recognition of conversational speech. Computational Lin-
guistics, 26(3):339?374, 2000.
M. Stru?be and C. Mu?ller. A machine learning approach to pro-
noun resolution in spoken dialogue. In Proceedings of the
41st Annual Meeting of the Association for Computational
Linguistics (ACL2003), pages 168?175, 2003.
J. van Eijk and H. Kamp. Representing discourse in context.
In J. van Benthem and A. ter Meulen, editors, Handbook of
Logic and Linguistics, pages 179?237. Elsevier, 1997.
R. Vieira and M. Poesio. Processing definite descriptions in
corpora. In Corpus-based and computational approaches to
anaphora. UCL Press, 2000.
J. M. Wiebe, T. P. O?Hara, T. Ohrstrom-Sandgren, and K. J. Mc-
Keever. An empirical approach to temporal reference resolu-
tion. Journal of Artificial Intelligence Research, 9:247?293,
1998.
103
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 57?64
Manchester, August 2008
Weakly supervised supertagging with grammar-informed initialization
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@mail.utexas.edu
Abstract
Much previous work has investigated weak
supervision with HMMs and tag dictionar-
ies for part-of-speech tagging, but there
have been no similar investigations for the
harder problem of supertagging. Here, I
show that weak supervision for supertag-
ging does work, but that it is subject to
severe performance degradation when the
tag dictionary is highly ambiguous. I show
that lexical category complexity and infor-
mation about how supertags may combine
syntactically can be used to initialize the
transition distributions of a first-order Hid-
den Markov Model for weakly supervised
learning. This initialization proves more
effective than starting with uniform tran-
sitions, especially when the tag dictionary
is highly ambiguous.
1 Introduction
Supertagging involves assigning words lexical en-
tries based on a lexicalized grammatical theory,
such as Combinatory Categorial Grammar (CCG)
(Steedman, 2000) Tree-adjoining Grammar (Joshi,
1988), or Head-driven Phrase Structure Grammar
(Pollard and Sag, 1994). Supertag sets are larger
than part-of-speech (POS) tag sets and their ele-
ments are generally far more articulated. For ex-
ample, the English verb join has the POS VB and
the CCG category ((S
b
\NP)/PP)/NP in CCG-
bank (Hockenmaier and Steedman, 2007). This
category indicates that join requires a noun phrase
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
to its left, another to its right, and a prepositional
phrase to the right of that.
Supertags convey such detailed syntactic sub-
categorization information that supertag disam-
biguation is referred to as almost parsing (Banga-
lore and Joshi, 1999). Standard sequence predic-
tion models are highly effective for supertagging,
including Hidden Markov Models (Bangalore and
Joshi, 1999; Nielsen, 2002), Maximum Entropy
Markov Models (Clark, 2002; Hockenmaier et al,
2004; Clark and Curran, 2007), and Conditional
Random Fields (Blunsom and Baldwin, 2006).
The original motivation for supertags?parse pre-
filtering for lexicalized grammars?of Bangalore
and Joshi (1999) has been realized to good effect:
the supertagger of Clark and Curran (2007) pro-
vides staged n-best lists of multi-tags that dramat-
ically improve parsing speed and coverage without
much loss in accuracy. Espinosa et al (2008) have
shown that hypertagging (predicting the supertag
associated with a logical form) can improve both
speed and accuracy of wide-coverage sentence re-
alization with CCG. Supertags have gained fur-
ther relevance as they are increasingly used as fea-
tures for other tasks, including machine translation
(Birch et al, 2007; Hassan et al, 2007).
Supertaggers typically rely on a significant
amount of carefully annotated sentences. As with
many problems, there is pressing need to find
strategies for reducing the amount of supervision
required for producing accurate supertaggers, but
as yet, no one has explored the use of weak super-
vision for the task. In particular, there are many di-
alog systems which rely on hand-crafted lexicons
that both provide a starting point for bootstrapping
a supertagger and which could benefit greatly from
supertag pre-parse filter. For example, the dialog
system used by Kruijff et al (2007) uses a hand-
57
crafted CCG grammar for OpenCCG (White and
Baldridge, 2003). It is important to stress that there
are many such uses of CCG and related frame-
works which do not rely on first annotating (even a
small number of) sentences in a corpus: these de-
fine a lexicon that maps from words to categories
(supertags) for a particular domain/application.
This scenario is a natural fit for learning taggers
from tag dictionaries using hidden Markov mod-
els with Expectation-Maximization (EM). Here,
I investigate such weakly supervised learning for
supertagging and demonstrate the importance of
proper initialization of the tag transition distribu-
tions of the HMM. In particular, such initializa-
tion can be done using inherent properties of the
CCG formalism itself regarding how categories
1
may combine. Informed initialization should help
with supertagging for two reasons. First, cate-
gories have structure?lacking in POS tags?waiting
to be exploited. For example, it is far more likely
a priori to see the category sequence (S\NP)/NP
NP/N than the sequence S/S NP\NP. Given the
categories for a word, this information can be used
to influence our expectations about categories for
adjacent words. Second, this kind of information
truly matters for the task: a key aspect of supertag-
ging that differentiates it from POS tagging is that
the contextual information is much more important
for the former. Lexical probabilities handle most
of the ambiguity for POS tagging, but supertags
are inherently about context and, furthermore, lex-
ical ambiguity is much greater for supertagging,
making lexical probabilities less effective.
I start by defining a distribution over lexical
categories and then use this distribution as part
of creating a CCG-informed transition distribution
that appropriately breaks the symmetry of uniform
HMM initialization. After describing how these
components are included in the HMM, I describe
experiments with CCGbank varying the ambiguity
of the lexicon provided. I show that using knowl-
edge about the formalism consistently improves
performance, and is especially important as cate-
gorial ambiguity increases.
2 Lexical category distribution
The categories of CCG are an inductively defined
set containing elements that are either atomic ele-
ments or (curried) functions specifying the canon-
1
For the rest of the paper, I will refer to categories rather
supertags, but will still refer to the task as supertagging.
ical linear direction in which they seek their argu-
ments. Some example entries from CCGbank are:
the := NP
nb
/N
of := (NP\NP)/NP
of := ((S\NP)\(S\NP)/NP
were := (S
dcl
\NP)/(S
pss
\NP)
buy := (S
dcl
\NP)/NP
buy := ((((S
b
\NP)/PP)/PP)/(S
adj
\NP))/NP
Words can be associated with multiple categories;
the distribution over these categories is typically
quite skewed. For example, the first entry for buy
occurs 33 times in CCGbank, compared with just
once for the second. That the simpler category is
more prevalent is unsurprising: a general strategy
when creating CCG lexicons is to use simpler cate-
gories whenever possible. This points to the possi-
bility of defining distributions over CCG lexicons
based on measures of the complexity of categories.
I use a simple distribution here: given a lexicon L,
the probability of a category i is inversely propor-
tional to its complexity:
?
i
=
1
complexity(c
i
)
?
j?L
1
complexity(c
j
)
(1)
Here, a very simple complexity measure is
assumed: the number of subcategories (to-
kens) contained in a category.
2
For example,
((S\NP)\(S\NP)/NP contains 9: S (twice), NP
(thrice), S\NP (twice), (S\NP)\(S\NP), and
((S\NP)\(S\NP)/NP.
The tag transition distribution defined in the next
section uses ? to bias transitions toward simpler
categories, e.g., preferring the first category for
buy over the second. Performance when using ?
is compared to using a uniform distribution.
Other distributions could be given, e.g., one
which gives more mass to adjunct categories such
as (S\NP)\(S\NP) than to ones which are oth-
erwise similar but do not display such symmetry,
like (S/NP)\(NP\S). However, the most impor-
tant thing for present purposes is that simpler cate-
gories are more likely than more complex ones.
This distribution imposes no internal struc-
ture on the likelihood of a lexicon. As far
as ? is concerned, lexicons can as well have
the category (S\NP)\NP for transitive verbs and
((S/NP)/NP)/NP for ditransitive verbs, even
though this is a highly unlikely pattern since we
2
This worked better than using category arity or number
of unique subcategory types.
58
Vinken will join the board as non?executive director
S/(S\NP) (S\NP)/(S\NP) ((S\NP)/PP)/NP NP/N N PP/NP NP/N N
> >
NP NP
> >
(S\NP)/PP PP
>
S\NP
>
S\NP
>
S
Figure 1: Normal form CCG derivation, using only application rules.
would expect both types of verbs to seek their ar-
guments in the same direction. Languages also
tend to prefer lexicons with one or the other slash
direction predominating (Villavicencio, 2002). In
the future, it would be interesting to consider
Bayesian approaches that could encode more com-
plex structure and assign priors over distributions
over lexicons, building on these observations.
An aspect of CCGbank that relevant for ?
i
is
that some categories actually are not true cate-
gories. For example, many punctuation ?cate-
gories? are given as LRB, ., :, etc. In most
grammars, the category of ?.? is usually assumed
to be S\S. The grammatical behavior of such
pseudo-categories is handled via special rules in
the parsers of Hockenmaier and Steedman (2007)
and Clark and Curran (2007). I relabeled three of
these: , to NP\NP, . to S\S and ; to (S\S)/S. A
single best change was not clear for others such as
LRB and :, so they were left as is.
3 Category transition distribution
CCG analyses of sentences are built up from lex-
ical categories combining to form derived cate-
gories, until an entire sentence is reduced to a sin-
gle derived category with corresponding depen-
dencies. One of CCG?s most interesting linguis-
tic properties is it allows alternative constituents.
Consider the derivations in Figures 1 and 2, which
show a normal form derivation (Eisner, 1996) and
fully incremental derivation, respectively. Both
produce the same dependencies, guaranteed by the
semantic consistency of CCG?s rules (Steedman,
2000). This property of CCG of supporting mul-
tiple derivations of the same analysis has been
termed spurious ambiguity. However, the extra
constituents are anything but spurious: they are
implicated in a range of CCG (along with other
forms of categorial grammar) linguistic analyses,
including coordination, long-distance extraction,
intonation, and incremental processing.
This all boils down to associativity: just as
(1 + (4 + 2)) = ((1 + 4) + 2) = 7, CCG ensures
that (Ed?(saw?Ted)) = ((Ed?saw)?Ted) = S Such
multiple derivations arise when adjacent categories
can combine through either application or compo-
sition. Thus, we would expect that the lexical cat-
egories needed to analyze an entire sentence will
more often than not be able to combine with their
immediate neighbors. For example, six of seven
pairs of adjacent lexical categories in the sentence
in Figure 1 can combine. Only N PP/NP of board
as cannot.
3
This observation can be used in different ways
by different models for CCG supertagging. For
example, discriminative tagging models could in-
clude features that capture whether or not the cur-
rent supertag can combine with the previous one
and possibly via which CCG rule. Here, I show
how it can be used to provide a non-uniform start-
ing point for the transition distributions ?
j|i
in a
first-order Hidden Markov Model. This is similar
to how Grenager et al (2005) use diagonal initial-
ization in an HMM for field segmentation to en-
courage the model to remain in the same state (and
thus predict the same label for adjacent words).
For CCG supertagging, the initialization should
discourage diagonalization and establish a prefer-
ence for some transitions over others.
There are many ways to define such a starting
point. The simplest would be to reserve a small
part of the mass spread uniformly over category
pairs which cannot combine and then spread the
rest of the mass uniformly over those which can.
However, we can provide a more refined distri-
bution, ?
j|i
, by incorporating the lexical category
distribution ?
i
defined in the previous section to
weight these transitions according to this further
information. In a similar manner to Grenager et al
(2005), I define ? as follows:
3
I make the standard assumption that type-raising is per-
formed in the lexicon, so the possibility of combining these
through type-raising plus composition is not available.
59
Vinken will join the board as non?executive director
S/(S\NP) (S\NP)/(S\NP) ((S\NP)/PP)/NP NP/N N PP/NP NP/N N
>B
S/(S\NP)
>B
2
(S/PP)/NP
>B
(S/PP)/N
>
S/PP
>B
S/NP
>B
S/N
>
S
Figure 2: Incremental CCG derivation, using both application and composition (B) rules.
?
j|i
= (1??)?
j
+ ? ? ?(i, j) ?
?
j
?
k?L|?(i,k)
?
k
(2)
where ?(i, j) is an indicator function that returns
1 if categories c
i
and c
j
can combine when c
i
im-
mediately precedes c
j
, ? is a global parameter that
specifying the total probability of transitions that
are combinable from i. Each j receives a propor-
tion of ? according to its lexical prior probability
over the sum of the lexical prior probabilities for
all categories that combine with i. For the experi-
ments in this paper, ? was set to .95. For the mod-
els referred to as ?U and ?U-EM in section 5, the
uniform lexical probability 1/|C| is used for ?
i
.
For ?(i, j), I use the standard rules assumed
for CCGbank parsers: forward and backward
application (>, <), order-preserving com-
position (>B, <B), and backward crossed
composition (<B
?
) for S-rooted categories.
Thus, ?(NP,S\NP)=1, ?(S/NP,NP/N)=1,
?((S\NP)/NP, (S\NP)\(S\NP))=1 and
?(S/NP,NP\NP)=0. For application, left-
ward and rightward arguments are handled
separately by assuming that it would be possi-
ble to consume all preceding arguments of the
first category and all following arguments of
the second. So, ?((S/NP)\S,NP/N)=1 and
?(NP, (S\NP)/NP)=1. Unification on categories
is standard (so ?(NP[nb],S\NP)=1), except that
N unifies with NP only when N is the argument:
?(N,S\NP)=1, but ?(NP/N,NP)=0. This is
to deal with the fact that CCGbank represents
many words with N (e.g., Mr.|N/N Vinken|N
is|(S[dcl]\NP)/NP) and assumes that a parser will
include the unary type changing rule N?NP.
The HMM also has initial and final probabili-
ties; distributions can be defined based on which
categories are likely to start or end a sentence. For
this, I assume only that categories which seek ar-
guments to the left (e.g., S\NP) are less likely
at the beginning of a sentence and those which
seek rightward arguments are less likely at the end.
The initializations for these are defined similarly
to the transition distribution, substituting functions
noLeftArgs(i) and noRightArgs(i) for ?(i, j).
4 Model
A first-order Hidden Markov Model (bitag HMM)
is used for bootstrapping a supertagger from a lex-
icon. See Rabiner (1989) for an extensive intro-
duction to and discussion of HMMs. There are
several reasons why this is an attractive tagging
model here. First, though extra context in the
form of tritag transition distributions or other tech-
niques can improve supervised POS tagging accu-
racy, the accuracy of bitag HMMs is not far behind.
The goal here is to investigate the relative gains
of using CCG-based information in weakly super-
vised HMM learning. Second, the expectation-
maximization algorithm for bitag HMMs is effi-
cient and has been shown to be quite effective for
acquiring accurate POS taggers given only a lex-
icon (tag dictionary) and certain favorable condi-
tions (Banko andMoore, 2004). Third, the model?s
simplicity makes it straightforward to test the idea
of CCG-initialization on tag transitions.
Dirichlet priors can be used to bias HMMs to-
ward more skewed distributions (Goldwater and
Griffiths, 2007; Johnson, 2007), which is espe-
cially useful in the weakly supervised setting con-
sidered here. Following Johnson (2007), I use vari-
ational Bayes EM (Beal, 2003) during the M-step
for the transition distribution:
?
l+1
j|i
=
f(E[n
i,j
] + ?
i
)
f(E[n
i
] + |C | ? ?
i
)
(3)
f(v) = exp(?(v)) (4)
60
?(v) =
{
g(v ?
1
2
) if v > 7
?(v + 1) ?
1
v
o.w.
(5)
g(x) ? log(x) +
1
24x
2
?
7
960x
4
+
31
8064x
6
?
127
30720x
8
(6)
where V is the set of word types, ? is the digamma
function (which is approximated by g), and ?
i
is
the hyperparameter of the Dirichlet priors. In all
experiments, the ?
i
parameters were set symmet-
rically to .005.
For experiments using the transition prior ?
j|i
,
the initial expectations of the model were set as
E[n
i,j
] = |E
i
| ? ?
j|i
and E[n
i
] = |E
i
|, where E
i
is the set of emissions for category c
i
. The uni-
form probability
1
|C |
was used in place of ?
j|i
for
standard HMM initialization.
The emission distributions use standard EM ex-
pectations with more mass reserved for unknowns
for tags with more emissions as follows:
4
?
l+1
k|i
=
E[n
i,k
] + |E
i
| ?
1
|V|
E[n
i
] + |E
i
|
(7)
The Viterbi algorithm is used for decoding.
5 Experiments
CCGbank (Hockenmaier and Steedman, 2007) is a
translation of phrase structure analyses of the Penn
Treebank into CCG analyses. Here, I consider
only the lexical category annotations and ignore
derivations. The standard split used for weakly su-
pervised HMM tagging experiments (Banko and
Moore, 2004; Wang and Schuurmans, 2005) is
used: sections 0-18 for training (train), 19-21 for
development (dev), and 22-24 for testing (test). All
parameters and models were developed using dev.
The test set was used only once to obtain the per-
formance figures reported here.
Counts for word types, word tokens and sen-
tences for each data set are given in Table 1. In
train, there are 1241 distinct categories, the am-
biguity per word type is 1.69, and the maximum
number of categories for a single word type is 126.
This is much greater than for POS tags in CCG-
bank, for which there are 48 POS tags with an av-
4
I also experimented with a Dirichlet prior on the emis-
sions, but it performed worse. Using a symmetric prior
was actually detrimental, while performance within a percent
of those achieved with the above update was achieved with
Dirichlet hyperparameters set relative to |E
i
|/|V|.
Dataset Types Tokens Sentences
train 43063 893k 38,015
dev 14961 128k 5484
test 13898 127k 5435
Table 1: Basic statistics for the datasets.
erage ambiguity of 1.17 per word and a maximum
of 7 tags in train.
5
The set of supertags was not reduced: any cat-
egory found in the data used to initialize a lexi-
con was considered. This is one of the advan-
tages of the HMM over using discriminative mod-
els, where typically only supertags seen at least 10
times in the training material are utilized for effi-
ciency (Clark and Curran, 2007). Ignoring some
supertags makes sense when building supervised
supertaggers for pre-parse filtering, but not for
learning from lexicons, where we cannot assume
we have such frequencies.
For supervised training with the HMM on train,
the performance is 87.6%. This compares to
91.4% for the C&C supertagger. The accuracy of
the HMM, though quite a bit lower than that of
C&C, is still quite good, indicating that it is an ad-
equate model for the task. Note also that it uses
only the words themselves and does not rely on
POS tags. The performance of the C&C tagger
was obtained by training the C&C POS tagger on
the given dataset and tagging the evaluation mate-
rial with it. Finally, the HMM trains in just a few
seconds as opposed to over an hour.
6
Five different weakly supervised scenarios are
evaluated: (1) standard EM with 50 iterations
(EM), (2) ? initialization with uniform lexical
probabilities w/o EM (?U), (3) ? with ? proba-
bilities w/o EM (??), (4) ? with uniform lexical
probabilities and 10 EM iterations, and (5) ? with
? and 10 EM iterations.
7
These scenarios com-
pare the effectiveness of standard EM with the use
of grammar informed transitions; these in turn are
of two varieties ? one using a uniform lexical prior
or one that is biased in favor of less complex cate-
gories according to ?.
As Banko and Moore (2004) discovered when
5
Note that the POS tag information is not used in these
experiments, except for by the C&C tagger.
6
It should be stressed that the goal of this paper is not to
compete on supervised performance with C&C; instead, this
comparison shows that the HMM supervised performance is
reasonable and is thus relevant for bootstrapping.
7
The number of iterations for standard and grammar in-
formed iteration were determined by performance on dev.
61
reimplementing several previous HMMs for POS
tagging, the lexicons had been limited to contain
only tags occurring above a particular frequency.
For POS tagging, this keeps a cleaner lexicon that
avoids errors in annotated data (such as the tagged
as VB) and rare tags (such as a tagged as SYM).
When learning from a lexicon alone, such elements
receive the same weight as their other (correct or
more fundamental) tags in initializing the HMM.
The problem of rare tags turns out to be very im-
portant for weakly supervised CCG supertagging.
8
To consider the effect of the CCG-based initial-
ization for lexicons with differing ambiguity, I use
tag cutoffs that remove any lexical entry containing
a category that appears with a particular word less
than X% of the time (Banko and Moore, 2004),
as well as using no cutoffs at all. Recall that the
goal of these experiments is to investigate the rela-
tive difference in performance between using the
grammar-based initialization or not, given some
(possibly hand-crafted) lexicon. Lexicon cutoffs
actually constitute a strong source of supervision
because they use tag frequencies (which would not
be known for a hand-crafted lexicon), so it should
be stressed that they are used here only so that this
relative performance can be measured for different
ambiguity levels.
Table 2 provides accuracy for ambiguouswords
(and not including punctuation) for the five scenar-
ios, varying the cutoff to measure the effect of pro-
gressively allowing more lexical ambiguity (and
much rarer categories). The number of ambiguous,
non-punctuation tokens is 101,167.
The first thing to note is performance given only
the lexicon and the ?U or ?? initialization of
the transitions. These correspond to taggers which
have only been given the lexicon and have not uti-
lized any data to improve their estimates of the
transition and emission probabilities. Interestingly,
both do quite well with a clean lexicon: see the
columns under ?U and ??. These indicate that
initializing the transitions based on whether cate-
gories can combine does indeed appropriately cap-
ture key aspects of category transitions. Further-
more, using the lexical category distribution (??)
to create the transition initialization provides a bet-
ter starting point than the uniform one (?U), espe-
8
CCGbank actually corrects many errors in the Penn Tree-
bank, and does not suffer as much from mistagged examples.
However, there were two instances of an ill-formed category
((S[b]\NP)/NP)/ in wsj 0595 for the words own and keep.
These were corrected to (S[b]\NP)/NP.
Cutoff EM ?U ?? ?U-EM ??-EM
.1 77.4 73.1 74.7 80.0 79.6
.05 69.1 70.6 72.5 79.2 79.2
.01 60.2 62.2 65.0 75.4 76.7
.005 52.2 57.8 59.0 72.5 73.8
.001 41.3 45.5 48.2 63.0 67.6
None 33.0 33.9 37.8 52.9 56.1
Table 2: Performance on ambiguous word types
of the HMM with standard EM (uniform starting
transitions), just the initial ? transitions (?U and
??), and EM initialized with ?U and ??, for
lexicons with varied cutoffs. Note also that these
scores do not include punctuation.
cially as lexical ambiguity increases.
Next, note that both ?U-EM and ??-EM beat
the randomly initialized EM for all cutoff levels.
For the 10% tag cutoff (the first row), there is an
absolute difference of over 2% for both.
9
As the
ambiguity increases, the grammar-informed ini-
tialization has a much stronger effect. In the ex-
treme case of using no cutoff at all (the None row
of Table 2), ?U-EM and??-EM beat EM by 19.9%
and 23.1%, respectively. Finally, using the lexical
category distribution ? instead of a uniform one
is much more effective when there is more lexi-
cal ambiguity (e.g., compare the .01 through None
rows of the ?U-EM and ??-EM columns), but
has a negligible effect with less ambiguity (rows
.05 and .01). This demonstrates that the grammar-
based initialization can be effectively exploited ? it
is in fact crucial for improving performance when
we are given much more ambiguous lexicons.
The majority of errors with ??-EM involve
marking adjectives (N/N) as nouns (N) or vice
versa, and assigning the wrong prepositional cat-
egory (usually the simpler noun phrase post-
modifier (NP\NP)/NP instead of the verb phrase
modifier ((S\NP)\(S\NP))/NP. Both of these
kinds of errors, and others, could potentially be
corrected if the categories proposed by the tagger
were further filtered by an attempt to parse each
sentence with the categories.
6 Related work
The idea of using knowledge from the formalism
for constraining supertagging originates with Ban-
9
For comparison with the performance of 87.6% for the
fully supervised HMM on all tokens, ?-EM achieves 82.1%
and 58.9% using a cutoff of .1 or no cutoff, respectively.
62
galore and Joshi (1999). They used constraints
based on how elementary trees of Tree-Adjoining
Grammar could or could not combine as filters to
block out tags that do not fit in certain locations
in the string. My approach is different is sev-
eral ways. First, they dealt with fully supervised
supertagging; here I show that using this knowl-
edge is important for weakly supervised supertag-
ging where we are given only a tag dictionary (lex-
icon). Second, my approach encodes grammar-
based cues only as an initial bias, so categories are
never explicitly filtered. Finally, I use CCG rather
than TAG, which makes it possible to exploit a
much higher degree of associativity in derivations.
This in turn makes it easier to utilize prior knowl-
edge about adjacent contexts ? precisely what is
needed for using the grammar to influence the tran-
sition probabilities of a bigram HMM.
On the other hand, Bangalore and Joshi (1999)
use constraints that act at greater distances than
I have considered here. For example, if one
wishes to provide a word with the category
((S\NP)/PP)/NP, then there should be a word
with a category which results in a PP two or more
words to its right ? this is something which the
bigram transitions considered here cannot capture.
An interesting way to extend the present approach
would be to enforce such patterns as posterior con-
straints during EM (Graca et al, 2007).
Recent work considers a damaged tag dictionary
by assuming that tags are known only for words
that occur more than once or twice (Toutanova
and Johnson, 2007). A very interesting aspect of
this work is that they explicitly model ambiguity
classes to exploit commonality in the lexicon be-
tween different word forms, which could be even
more useful for supertagging.
In a grammar development context, it is often
the case that only some of the categories for a word
have been assigned. This is the scenario consid-
ered by Haghighi and Klein (2006) for POS tag-
ging: how to construct an accurate tagger given
a set of tags and a few example words for each
of those tags. They use distributional similarity of
words to define features for tagging that effectively
allow such prototype words to stand in for others.
This idea could be used with my approach as well;
the most obvious way would be to use prototype
words to suggest extra categories (beyond the tag
dictionary) for known words and a reduced set of
categories for unknown words.
Other work aims to do truly unsupervised learn-
ing of taggers, such as Goldwater and Griffiths
(2007) and Johnson (2007). No tag dictionaries
are assumed, and the models are parametrized with
Dirichlet priors. The states of these models implic-
itly represent tags; however, it actually is not clear
what the states in such models truly represent: they
are (probably interesting) clusters that may or may
not correspond to what we normally think of as
parts-of-speech. POS tags are relatively inert, pas-
sive elements in a grammar, whereas CCG cate-
gories are the very drivers of grammatical analy-
sis. That is, syntax is projected, quite locally, by
lexical categories. It would thus be interesting to
consider the induction of categories with grammar-
based priors with such models.
7 Conclusion
I have shown that weakly supervised learning can
indeed be used to induce supertaggers from a lex-
icon mapping words to their possible categories,
but that the extra ambiguity in the supertagging
task over that of POS tagging makes performance
much more sensitive to rare categories that occur
in larger, more ambiguous lexicons. However, I
have also shown that the CCG formalism itself can
provide the basis for useful distributions over lexi-
cal categories and tag transitions in a bitag HMM.
By using these distributions to initialize the HMM,
it is possible to improve performance regardless of
the underlying ambiguity. This is especially im-
portant for reducing error when the lexicon used
for bootstrapping is highly ambiguous and con-
tains very rare categories.
Acknowledgments
Thanks to the UT Austin Natural Language Learn-
ing group and three anonymous reviewers for use-
ful comments and feedback. This work was sup-
ported by NSF grant BCS-0651988 and a Faculty
Research Assignment from UT Austin.
References
Bangalore, Srinivas and Aravind K. Joshi. 1999. Su-
pertagging: an approach to almost parsing. Compu-
tational Linguistics, 25(2):237?265.
Banko, Michele and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COL-
ING.
63
Beal, Matthew. 2003. Variational Algorithms for Ap-
proximate Inference. Ph.D. thesis, University of
Cambridge.
Birch, Alexandra, Miles Osborne, and Philipp Koehn.
2007. CCG supertags in factored statistical machine
translation. In Proceedings of the 2nd Workshop on
Statistical Machine Translation.
Blunsom, Phil and Timothy Baldwin. 2006. Multi-
lingual deep lexical acquisition for HPSGs via su-
pertagging. In Proceedings of EMNLP 06, pages
164?171.
Clark, Stephen and James Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4).
Clark, Stephen. 2002. Supertagging for combina-
tory categorial grammar. In Proceedings of TAG+6,
pages 19?24, Venice, Italy.
Eisner, Jason. 1996. Efficient normal-form parsing for
combinatory categorial grammars. In Proceedings of
the 35th ACL.
Espinosa, Dominic, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183?191, Columbus, Ohio, June.
Goldwater, Sharon and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th ACL.
Graca, Joao, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization, posterior constraints, and
statistical alignment. In Proceedings of NIPS07.
Grenager, Trond, Dan Klein, and Christopher D. Man-
ning. 2005. Unsupervised learning of field segmen-
tation models for information extraction. In Pro-
ceedings of the 43rd ACL, pages 371?378.
Haghighi, Aria and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL 2006.
Hassan, Hany, Khalil Sima?an, and Andy Way. 2007.
Supertagged phrase-based statistical machine trans-
lation. In Proceedings of the 45th ACL.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Hockenmaier, Julia, Gann Bierner, and Jason
Baldridge. 2004. Extending the coverage of a
CCG system. Research in Language and Computa-
tion, 2:165?208.
Johnson, Mark. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the EMNLP-
CoNLL 2007.
Joshi, Aravind. 1988. Tree Adjoining Grammars. In
Dowty, David, Lauri Karttunen, and Arnold Zwicky,
editors, Natural Language Parsing, pages 206?250.
Cambridge University Press, Cambridge.
Kruijff, Geert-Jan M., Hendrik Zender, Patric Jensfelt,
and Henrik I. Christensen. 2007. Situated dialogue
and spacial organization: What, where,...and why?
International Journal of Advanced Robotic Systems,
4(1):125?138.
Nielsen, Leif. 2002. Supertagging with combinatory
categorial grammar. In Proceedings of the Seventh
ESSLLI Student Session, pages 209?220.
Pollard, Carl and Ivan Sag. 1994. Head Driven
Phrase Structure Grammar. CSLI/Chicago Univer-
sity Press, Chicago.
Rabiner, Lawrence. 1989. A tutorial on Hidden
Markov Models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257?
286.
Steedman, Mark. 2000. The Syntactic Process. The
MIT Press, Cambridge Mass.
Toutanova, Kristina and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS 20.
Villavicencio, Aline. 2002. The Acquisition of
a Unification-Based Generalised Categorial Gram-
mar. Ph.D. thesis, University of Cambridge.
Wang, Qin Iris and Dal Schuurmans. 2005. Improved
estimation for unsupervised part-of-speech tagging.
In EEE International Conference on Natural Lan-
guage Processing and Knowledge Engineering.
White, Michael and Jason Baldridge. 2003. Adapting
chart realization to CCG. In Proceedings of ENLG.
64
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 390?399, Prague, June 2007. c?2007 Association for Computational Linguistics
Part-of-speech Tagging for Middle English through Alignment and
Projection of Parallel Diachronic Texts
Taesun Moon and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
tsmoon, jbaldrid@mail.utexas.edu
Abstract
We demonstrate an approach for inducing a
tagger for historical languages based on ex-
isting resources for their modern varieties.
Tags from Present Day English source text
are projected to Middle English text using
alignments on parallel Biblical text. We
explore the use of multiple alignment ap-
proaches and a bigram tagger to reduce the
noise in the projected tags. Finally, we train
a maximum entropy tagger on the output of
the bigram tagger on the target Biblical text
and test it on tagged Middle English text.
This leads to tagging accuracy in the low
80?s on Biblical test material and in the 60?s
on other Middle English material. Our re-
sults suggest that our bootstrapping meth-
ods have considerable potential, and could
be used to semi-automate an approach based
on incremental manual annotation.
1 Introduction
Annotated corpora of historical texts provide an im-
portant resource for studies of syntactic variation
and change in diachronic linguistics. For example,
the Penn-Helsinki Parsed Corpus of Middle English
(PPCME) (Kroch and Taylor, 2000) has been used
to show the existence of syntactic dialectal differ-
ences between northern and southern Middle En-
glish (Kroch et al, 2000) and to examine the syn-
tactic evolution of the English imperative construc-
tion (Han, 2000). However, their utility rests on their
having coverage of a significant amount of annotated
material from which to draw patterns for such stud-
ies, and creating resources such as the PPCME re-
quire significant time and cost to produce. Corpus
linguists interested in diachronic language studies
thus need efficient ways to produce such resources.
One approach to get around the annotation bottle-
neck is to use semi-automation. For example, when
producing part-of-speech tags for the Tycho Brahe
corpus of Historical Portuguese (Britto et al, 2002),
a set of seed sentences was manually tagged, and the
Brill tagger (Brill, 1995) was then trained on those
and consequently used to tag other sentences. The
output was inspected for errors, the tagger was re-
trained and used again to tag new sentences, for sev-
eral iterations.
We also seek to reduce the human effort involved
in producing part-of-speech tags for historical cor-
pora. However, our approach does so by leveraging
existing resources for a language?s modern varieties
along with parallel diachronic texts to produce accu-
rate taggers. This general technique has worked well
for bilingual bootstrapping of language processing
resources for one language based on already avail-
able resources from the other. The first to explore
the idea were Yarowsky and Ngai (2001), who in-
duced a part-of-speech tagger for French and base
noun phrase detectors for French and Chinese via
transfer from English resources. They built a highly
accurate POS tagger by labeling English text with an
existing tagger (trained on English resources), align-
ing that text with parallel French, projecting the au-
tomatically assigned English POS tags across these
alignments, and then using the automatically labeled
French text to train a new French tagger. This tech-
390
nique has since been used for other languages and
tasks, e.g. morphological analysis (Yarowsky et al,
2001), fine-grained POS tagging for Czech (Dra?bek
and Yarowsky, 2005), and tagging and inducing syn-
tactic dependencies for Polish (Ozdowska, 2006).
This methodology holds great promise for pro-
ducing tools and annotated corpora for processing
diachronically related language pairs, such as Mod-
ern English to Middle or Old English. Historical
languages suffer from a paucity of machine readable
text, inconsistencies in orthography, and grammati-
cal diversity (in the broadest sense possible). This
diversity is particularly acute given that diachronic
texts of a given language encompass texts and gen-
res spanning across centuries or millenia with a
plethora of extra-linguistic influences to complicate
the data. Furthermore, even in historically contem-
poraneous texts, possible dialectal variations further
amplify the differences in already idiosyncratic or-
thographies and syntactic structure.
The present study goes further than Britto et al
(2002) by fully automating the alignment, POS tag
induction, and noise elimination process. It is able to
utilize the source language to a greater degree than
the previously mentioned studies that attempted lan-
guage neutrality; that is, it directly exploits the ge-
netic similarity between the source and target lan-
guage. Some amount of surface structural similarity
between a diachronic dialect and its derivatives is to
be expected, and in the case of Middle English and
Modern English, such similarities are not negligible.
The automation process is further aided through
the use of two versions of the Bible, which obviates
the need for sentence alignment. The modern Bible
is tagged using the C&C maximum entropy tagger
(Curran and Clark, 2003), and these tags are trans-
ferred from source to target through high-confidence
alignments aquired from two alignment approaches.
A simple bigram tagger is trained from the resulting
target texts and then used to relabel the same texts as
Middle English training material for the C&C tag-
ger. This tagger utilizes a rich set of features and a
wider context, so it can exploit surface similarities
between the source and target language. By train-
ing it with both the original (Modern English) Penn
Treebank Wall Street Journal (WSJ) material and
our automatically tagged Middle English Wycliffe
material, we achieve an accuracy of 84.8% on pre-
dicting coarse tags, improving upon a 63.4% base-
line of training C&C on the WSJ sentences alone.
Furthermore, we show that the bootstrapped tagger
greatly reduces the error rate on out-of-domain, non-
Biblical Middle English texts.
2 Data
English provides an ideal test case for our study be-
cause of the existence of publically accessible di-
achronic texts of English and their translations in
electronic format and because of the availability of
the large, annotated Penn-Helsinki Parsed Corpus of
Middle English. The former allows us to create a
POS tagger via alignment and projection; the latter
allows us to evaluate the tagger on large quantities
of human-annotated tags.
2.1 The Bible as a parallel corpus
We take two versions of the Bible as our parallel cor-
pus. For modern English, we utilize the NET Bible1.
For Middle English (ME), we utilize John Wycliffe?s
Bible2. The first five lines of Genesis in both Bibles
are shown in Figure 1.
The Bible offers some advantages beyond its
availability. All its translations are numbered, fa-
cilitating assessment of accuracy for sentence align-
ment models. Also, the Bible is quite large for
a single text: approximately 950,000 words for
Wycliffe?s version and 860,000 words for the NET
bible. Finally, Wycliffe?s Bible was released in the
late 14th century, a period when the transition of En-
glish from a synthetic to analytical language was
finalized. Hence, word order was much closer to
Modern English and less flexible than Old English;
also, nominal case distinctions were largely neutral-
ized, though some verbal inflections such as dis-
tinctions for the first and second person singular in
the present tense were still in place (Fennell, 2001).
This places Wycliffe?s Bible as far back as possible
without introducing extreme nominal and verbal in-
flections in word alignment.
The two Bibles were cleaned and processed for
the present task and then examined for levels of
correspondence. The two texts were compared for
1The New English Translation Bible, which may be down-
loaded from http://www.bible.org/page.php?page id=3086.
2Available for download at:
http://wesley.nnu.edu/biblical studies/wycliffe.
391
1 In the beginning God created the heavens and the earth.
2 Now the earth was without shape and empty, and darkness was over the surface of the watery
deep, but the Spirit of God was moving over the surface of the water.
3 God said, ?Let there be light.? And there was light!
4 God saw that the light was good, so God separated the light from the darkness.
5 God called the light day and the darkness night. There was evening, and there was morning,
marking the first day.
1 In the bigynnyng God made of nouyt heuene and erthe.
2 Forsothe the erthe was idel and voide, and derknessis weren on the face of depthe; and the Spiryt
of the Lord was borun on the watris.
3 And God seide, Liyt be maad, and liyt was maad.
4 And God seiy the liyt, that it was good, and he departide the liyt fro derknessis; and he clepide
the liyt,
5 dai, and the derknessis, nyyt. And the euentid and morwetid was maad, o daie.
Figure 1: The first five verses of Genesis the NET Bible (top) and Wycliffe?s Bible (below).
whether there were gaps in the chapters and whether
one version had more chapters over the other. If dis-
crepancies were found, the non-corresponding chap-
ters were removed. Next, because we assume sen-
tences are already aligned in our approach, discrep-
ancies in verses between the two Bibles were culled.
A total of some two hundred lines were removed
from both Bibles. This processing resulted in a total
of 67 books3, with 920,000 words for the Wycliffe
Bible and 840,000 words for the NET Bible.
2.2 The Penn-Helsinki Parsed Corpus of
Middle English
The Penn-Helsinki Parsed Corpus of Middle En-
glish is a collection of text samples derived from
manuscripts dating 1150?1500 and composed dur-
ing the same period or earlier. It is based on and
expands upon the Diachronic Part of the Helsinki
Corpus of English Texts. It contains approximately
1,150,000 words of running text from 55 sources.
The texts are provided in three forms: raw, POS
tagged, and parsed.
Among the texts included are portions of the
Wycliffe Bible. They comprise partial sections of
Genesis and Numbers from the Old Testament and
John I.1?XI.56 from the New Testament. In total,
366 books shared by the churches and one book from the
Apocrypha. A comparison of the two Bibles revealed that
the NET Bible contained the Apocrypha, but only Baruch was
shared between the two versions.
the sections of Wycliffe annotated in PPCME have
some 25,000 words in 1,845 sentences. This was
used as part of the test material. It is important to
note that there are significant spelling differences
from the full Wycliffe text that we use for alignment
? this is a common issue with early writings that
makes building accurate taggers for them more diffi-
cult than for the clean and consistent, edited modern
texts typically used to evaluate taggers.
2.3 Tagsets
The PPCME uses a part-of-speech tag set that has
some differences from that used for the Penn Tree-
bank, on which modern English taggers are gener-
ally trained. It has a total of 84 word tags compared
to the widely used Penn Treebank tag set?s 36 word
tags.4 One of the main reasons for the relative diver-
sity of the PPCME tag set is that it maintains distinc-
tions between the do, have, and be verbs in addition
to non-auxiliary verbs. The tag set is further com-
plicated by the fact that composite POS tags are al-
lowed as in another D+OTHER, midnyght ADJ+N,
or armholes N+NS.
To measure tagging accuracy, we consider two
different tag sets: PTB, and COARSE. A measure-
ment of accuracy is not possible with a direct com-
parison to the PPMCE tags since our approach la-
4In our evaluations, we collapse the many different punctu-
ation tags down to a single tag, PUNC.
392
bels target text in Middle English with tags from
the Penn Treebank. Therefore, with PTB, all non-
corresponding PPCME tags were conflated if neces-
sary and mapped to the Penn Treebank tag set. Be-
tween the two sets, only 8 tags, EX, FW, MD, TO, VB,
VBD, VBN, VBP, were found to be fully identical.
In cases where tags from the two sets denoted the
same category/subcategory, one was simply mapped
to the other. When a PPCME tag made finer dis-
tinctions than a related Penn tag and could be con-
sidered a subcategory of that tag, it was mapped ac-
cordingly. For example, the aforementioned auxil-
iary verb tags in the PPMCE were all mapped to cor-
responding subcategories of the larger VB tag group,
a case in point being the mapping of the perfect par-
ticiple of have HVN to VBN, a plain verbal partici-
ple. For COARSE, the PTB tags were even further
reduced to 15 category tags,5 which is still six more
than the core consensus tag set used in Yarkowsky
and Ngai (2001). Specifically, COARSE was mea-
sured by comparing the first letter of each tag. For
example, NN and NNS are conflated to N.
2.4 Penn Treebank Release 3
The POS tagged Wall Street Journal, sections 2 to
21, from the Penn Treebank Release 3 (Marcus et
al., 1994) was used to train a Modern English tagger
to automatically tag the NET Bible. It was also used
to enhance the maximum likelihood estimates of a
bigram tagger used to label the target text.
3 Approach
Our approach involves three components: (1) pro-
jecting tags from Modern English to Middle English
through alignment; (2) training a bigram tagger; and
(3) bootstrapping the C&C tagger on Middle En-
glish texts tagged by the bigram tagger. This section
describes these components in detail.
3.1 Bootstrapping via alignment
Yarowsky and Ngai (2001) were the first to propose
the use of parallel texts to bootstrap the creation of
taggers. The approach first requires an alignment
to be induced between the words of the two texts;
5Namely, adjective, adverb, cardinal number, complemen-
tizer/preposition, conjunction, determiner, existential there, for-
eign word, interjection, infinitival to, modal, noun, pronoun,
verb, and wh-words.
tags are then projected from words of the source lan-
guage to words of the target language. This natu-
rally leads to the introduction of noise in the target
language tags. Yarowsky and Ngai deal with this
by (a) assuming that each target word can have at
most two tags and interpolating the probability of
tags given a word between the probabilities of the
two most likely tags for that word and (b) interpo-
lating between probabilities for tags projected from
1-to-1 alignments and those from 1-to-n alignments.
Each of these interpolated probabilities is parame-
terized by a single variable; however, Yarowsky and
Ngai do not provide details for how the two param-
eter values were determined/optimized.
Here, we overcome much of the noise by using
two alignment approaches, one of which exploits
word level similarities (present in genetically de-
rived languages such as Middle English and Present
Day English) and builds a bilingual dictionary be-
tween them. We also fill in gaps in the alignment
by using a bigram tagger that is trained on the noisy
tags and then used to relabel the entire target text.
The C&C tagger (Curran and Clark, 2003) was
trained on the Wall Street Journal texts in the Penn
Treebank and then used to tag the NET Bible (the
source text). The POS tags were projected from the
source to the Wycliffe Bible based on two alignment
approaches, the Dice coefficient and Giza++, as de-
scribed below.
3.1.1 Dice alignments
A dictionary file is built using the variation of
the Dice Coefficient (Dice (1945)) used by Kay and
Ro?scheisen (1993):
D(v,w) = 2cNA(v) + NB(w)
? ?
Here, c is the number of cooccurring positions and
NT (x) is the number of occurrences of word x in
corpus T . c is calculated only once for redundant
occurrences in an aligned sentence pair. For exam-
ple, it is a given that the will generally occur more
than once in each aligned sentence. However, even if
the occurs more than once in each of the sentences in
aligned pair sA and sB, c is incremented only once.
v and w are placed in the word alignment table if
they exceed the threshold value ?, which is an em-
pirically determined, heuristic measure.
393
The dictionary was structured to establish a sur-
jective relation from the target language to the
source language. Therefore, no lexeme in the
Wycliffe Bible was matched to more than one lex-
eme in the NET Bible. The Dice Coefficient was
modified so that for a given target word v
Dv = arg max
w
D(v,w)
would be mapped to a corresponding word from the
source text, such that the Dice Coefficient would be
maximized. Dictionary entries were further culled
by removing (v,w) pairs whose maximum Dice Co-
efficient was lower than the ? threshold, for which
we used the value 0.5. Finally, each word which had
a mapping from the target was sequentially mapped
to a majority POS tag. For example, the word like
which had been assigned four different POS tags,
IN, NN, RB, VB, by the C&C tagger in the NET
Bible was only mapped to IN since the pairings of
the two occurred the most frequently. The result is
a mapping from one or more target lexemes to a
source lexeme to a majority POS tag. In the case
of like, two words from the target, as and lijk, were
mapped thereto and to the majority tag IN.
Later, we will refer to the Wycliffe text (partially)
labeled with tags projected using the Dice coeffi-
cient as DICE 1TO1.
3.1.2 GIZA++ alignments
Giza++ (Och and Ney, 2003) was also used to de-
rive 1-to-n word alignments between the NET Bible
and the Wycliffe Bible. This produces a tagged ver-
sion of the Wycliffe text which we will refer to as
GIZA 1TON. In our alignment experiment, we used
a combination of IBM Model 1, Model 3, Model 4,
and an HMM model in configuring Giza++.
GIZA 1TON was further processed to remove
noise from the transferred tag set by creating a 1-to-1
word alignment: each word in the target Middle En-
glish text was given its majority tag based on the as-
signment of tags to GIZA 1TON as a whole. We call
this version of the tagged Wycliffe text GIZA 1TO1.
3.2 Bigram tagger
Note that because the projected tags in the Wycliffe
materials produced from the alignments are incom-
plete, there are words in the target text which have
no tag. Nonetheless, a bigram tagger can be trained
from maximum likelihood estimates for the words
and tag sequences which were successfully pro-
jected. This serves two functions: (1) it creates a
useable bigram tagger and (2) the bigram tagger can
be used to fill in the gaps so that the more powerful
C&C tagger can be trained on the target text.
A bigram tagger selects the most likely tag se-
quence T for a word sequence W by:
arg max
T
P (T |W ) = P (W |T )P (T )
Computing these terms requires knowing the transi-
tion probabilities P (ti|ti?1) and the emission proba-
bilities P (wi|ti). We use straightforward maximum
likelihood estimates from data with projected tags:
P (ti|ti?1) =
f(ti?1, ti)
f(ti?1)
P (wi|ti) =
f(wi, ti)
f(ti)
Estimates for unseen events were obtained
through add-one smoothing.
In order to diversify the maximum likelihood es-
timates and provide robustness against the errors
of any one alignment method, we concatenate sev-
eral tagged versions of the Wycliffe Bible with tags
projected from each of our methods (DICE 1TO1,
GIZA 1TON, and GIZA 1TO1) and the NET Bible
(and its tags from the C&C tagger).
3.3 Training C&C on projected tags
The bigram tagger learned from the aligned text has
very limited context and cannot use rich features
such as prefixes and suffixes of words in making its
predictions. In contrast, the C&C tagger, which is
based on that of Ratnaparkhi (1996), utilizes a wide
range of features and a larger contextual window in-
cluding the previous two tags and the two previous
and two following words. However, the C&C tagger
cannot train on texts which are not fully tagged for
POS, so we use the bigram tagger to produce a com-
pletely labeled version of the Wycliffe text and train
the C&C tagger on this material. The idea is that
even though it is training on imperfect material, it
will actually be able to correct many errors by virtue
of its greater discriminitive power.
394
Evaluate on Evaluate on
PPCME Wycliffe PPCME Test
Model PTB COARSE PTB COARSE
(a) Baseline, tag NN 9.0 17.7 12.6 20.1
(b) C&C, trained on gold WSJ 56.2 63.4 56.2 62.3
(c) Bigram, trained on DICE 1TO1 and GIZA 1TON 68.0 73.1 43.9 49.8
(d) Bigram, trained on DICE 1TO1 and GIZA 1TO1 74.8 80.5 58.0 63.9
(e) C&C, trained on BOOTSTRAP (920k words) 78.8 84.1 61.3 67.8
(f) C&C, trained on BOOTSTRAP and WSJ and NET 79.5 84.8 61.9 68.5
(g) C&C, trained on (gold) PPCME Wycliffe (25k words) n/a n/a 71.0 76.0
(h) C&C, trained on (gold) PPCME training set (327k words) 95.9 96.9 93.7 95.1
Figure 2: Tagging results. See section 4 for discussion.
We will refer to the version of the Wycliffe text
(fully) tagged in this way as BOOTSTRAP.
4 Experiments
The M3 and M34 subsections6 of the Penn Helsinki
corpus were chosen for testing since it is not only
from the same period as the Wycliffe Bible but since
it also includes portions of the Wycliffe Bible. A
training set of 14 texts comprising 330,000 words
was selected to train the C&C tagger and test the
cost necessary to equal or exceed the automatic im-
plementation. The test set consists of 4 texts with
110,000 words. The sample Wycliffe Bible with the
gold standard tags has some 25,000 words.
The results of the various configurations are given
in Figure 2, and are discussed in detail below.
4.1 Baselines
We provide two baselines. The first is the result of
giving every word the common tag NN . The sec-
ond baseline was established by directly applying
the C&C tagger, trained on the Penn Treebank, to
the PPCME data. The results are given in lines (a)
and (b) of Figure 2 for the first and second baselines,
respectively. As can be seen, the use of the Mod-
ern English tagger already provides a strong starting
point for both evaluation sets.
6Composition dates and manuscript dates for M3 are 1350-
1420. The composition dates for M34 are the same but the
manuscripts date 1420-1500
4.2 Bigram taggers
In section 3.1, we discuss three versions of the
Wycliffe target text labeled with tags projected
across alignments from the NET Bible. The
most straightforward of these were DICE 1TO1 and
GIZA 1TON which directly use the alignments from
the methods. Training a bigram tagger on these
two sources leads to a large improvement over the
C&C baseline on the PPCME Wycliffe sentences,
as can be seen by comparing line (c) to line (b)
in Figure 2. However, performance drops on the
PPCME Test sentences, which come from different
domains than the bigram tagger?s automatically pro-
duced Wycliffe training material. This difference is
likely to do good estimates of P (wi|ti), but poor es-
timates of P (ti|ti?1) due to the noise introduced in
GIZA 1TON.
More conservative tags projection is thus likely
to have a large effect on the out-of-domain perfor-
mance of the learned taggers. To test this, we trained
a bigram tagger on DICE 1TO1 and the more con-
servative GIZA 1TO1 projection. This produces fur-
ther gains for the PPCME Wycliffe, and enormous
improvements on the PPCME Test data (see line (d)
of Figure 2). This result confirms that conservativity
beats wild guessing (at the risk of reduced coverage)
for bootstrapping taggers in this way. This is very
much in line with the methodology of Yarowksy and
Ngai (2001), who project a small number of tags out
of all those predicted by alignment. They achieve
this restriction by directly adjusting the probabality
mass assigned to projected tags; we do it by using
two versions of the target text with tags projected in
395
two different 1-to-1 ways.
4.3 Bootstrapping the C&C tagger
As described in section 3.3, a bigram tagger trained
on DICE 1TO1 and GIZA 1TO1 (i.e., the tagger of
line (d)), was used to relabel the entire Wycliffe tar-
get text to produce training material for C&C, which
we call BOOTSTRAP. The intention is to see whether
the more powerful tagger can bootstrap off imper-
fect tags and take advantage of its richer features to
produce a more accurate tagger. As can be seen in
row (e) of Figure 2, it provides a 3-4% gain across
the board over the bigram tagger which produced its
training material (row (d)).
We also considered whether using all available
(non-PPCME) training material would improve tag-
ging accuracy by training C&C on BOOTSTRAP,
the Modern English Wall Street Journal (from the
Penn Treebank), and the automatically tagged NET
text7 It did produce slight gains on both test sets
over C&C trained on BOOTSTRAP alone. This is
likely due to picking up some words that survived
unchanged to the Modern English. Of course, the
utility of modern material used directly in this man-
ner will likely vary a great deal depending on the
distance between the two language variants. What is
perhaps most interesting is that adding the modern
material did not hurt performance.
4.4 Upperbounds
It is apparent from the results that there is a strong
domain effect on the performance of both the bigram
and C&C taggers which have been trained on auto-
matically projected tags. There is thus a question of
how well we could ever hope to perform on PPCME
Test given perfect tags from the Wycliffe texts. To
test this, C&C was trained on the PPCME version of
Wycliffe, which has human annotated standard tags,
and then applied on the PPCME test set. We also
compare this to training on PPCME texts which are
similar to those in PPCME Test.
The results, given in lines (g) and (h) of Figure
2, indicate that there is a likely performance cap on
non-Biblical texts when bootstrapping from parallel
Biblical texts. The results in line (h) also show that
the non-Biblical texts are more difficult, even with
7This essentially is partial self-training since C&C trained
on WSJ was used to produce the NET tags.
gold training material. This is likely due to the wide
variety of authors and genres contained in these texts
? in a sense, everything is slightly out-of-domain.
4.5 Learning curves with manual annotation
The upperbounds raise two questions. One is
whether the performance gap between (g) and (h) in
Figure 2 on PPCME Test is influenced by the signif-
icant difference in the size of their training sets. The
other is how much gold-standard PPCME training
material would be needed to match the performance
of our best bootstrapped tagger (line (f)). This is a
natural question to ask, as it hits at the heart of the
utility of our essentially unsupervised approach ver-
sus annotating target texts manually.
To examine the cost of manually annotating the
target language as compared to our unsupervised
method, the C&C tagger was also trained on ran-
domly selected sets of sentences from PPCME (dis-
joint from PPCME Test). Accuracy was measured
on PPCME Wycliffe and Test for a range of training
set sizes, sampled at exponentially increasing values
(25, 50, 100, . . . , 12800). Though we trained on and
predicted the full tagset used by the PPCME, it was
evaluated on PTB to give an accurate comparison.8
The learning curves on both test sets are shown
in Figure 3. The accuracy of the C&C tagger in-
creases rapidly, and the accuracy exceeds our auto-
mated method on PPCME Test with just 50 labeled
sentences and on the PPCME Wycliffe with 400 ex-
amples. This shows the domain of the target text is
served much better with the projection approach.
To see how much gold-standard PPCME Wycliffe
material is necessary to beat our best bootstrapped
tagger, we trained the tagger as in (g) of Figure 2
with varying amounts of material. Roughly 600 la-
beled sentences were required to beat the perfor-
mance of 61.9%/68.5% (line (f), on both metrics).
These learning curves suggest that when the do-
main for which one wishes to produce a tagger is
significantly different from the aligned text one has
available (in this and in many cases, the Bible), then
labeling a small number of examples by hand is a
quite reasonable approach (provided random sam-
pling is used). However, if one is not careful, con-
siderable effort could be put into labeling sentences
8Evaluation with the full PPCME set produces accuracy fig-
ures about 1% lower.
396
 60
 65
 70
 75
 80
 85
 90
 95
 100
 0  2000  4000  6000  8000  10000  12000  14000
Ac
cu
ra
cy
Number of sentences
PPCME Wycliffe
PPCME Test
Figure 3: Learning curve showing the accuracy for
PTB tags of the C&C tagger on both Bible and Test
as it is given more gold-standard PPCME training
sentences.
that are not optimal overall (imagine getting unlucky
and starting out by manually annotating primarily
Wycliffe sentences). The automated methods we
present here start producing good taggers immedi-
ately, and there is much room for improving them
further. Additionally, they could be used to aid man-
ual annotation by proposing high-confidence labels
even before any annotation has begun.
5 Related work
Despite the fact that the Bible has been translated
into many languages and that it constitutes a solid
source for studies in NLP with a concentration on
machine translation or parallel text processing, the
number of studies involving the Bible is fairly lim-
ited. A near exhaustive list is Chew et al(2006),
Melamed(1998), Resnik et al(1999), and Yarowsky
et al(2001).
Yarowsky and Ngai (2001) is of central rele-
vance to this study. The study describes an unsu-
pervised method for inducing a monolingual POS
tagger, base noun-phrase bracketer, named-entity
tagger and morphological analyzers from training
based on parallel texts, among many of which the
Bible was included. This is particularly useful given
that no manually annotated data is necessary in the
target language and that it works for two languages
from different families such as French and Chinese.
In the case of POS tagging, only the results for
English-French are given and an accuracy of 96% is
achieved. Even though this accuracy figure is based
on a reduced tag set smaller than the COARSE used
in this study, it is still a significant increase over that
achieved here. However, their method had the ad-
vantage of working in a domain that overlaps with
the training data for their POS tagger. Second, the
the French tag set utilized in that study is consider-
ably smaller than the Penn Helsinki tag set, a possi-
ble source of greater noise due to its size.
Dra?bek and Yarowsky (2005) create a fine-
grained tagger for Czech and French by enriching
the tagset for parallel English text with additional
morphological information, which, though not di-
rectly attested by the impoverished English morpho-
logical system (e.g. number on adjectives), typically
does appear in other languages.
6 Conclusion
The purpose of the study was to implement a POS
tagger for diachronic texts of maximal accuracy with
minimal cost in terms of labor, regardless of the
shortcuts taken. Such taggers are the building blocks
in the design of higher level tools which depend
on POS data such as morphological analyzers and
parsers, all of which are certain to contribute to di-
achronic language studies and genetic studies of lan-
guage change.
We showed that using two conservative methods
for projecting tags through alignment significantly
improves bigram POS tagging accuracies over a
baseline of applying a Modern English tagger to
Middle English text. Results were improved further
by training a more powerful maximum entropy tag-
ger on the predictions of the bootstrapped bigram
tagger, and we observed a further, small boost by
using Modern English tagged material in addition to
the projected tags when training the maximum en-
tropy tagger.
Nonetheless, our results show that there is still
much room for improvement. A manually annotated
training set of 400?800 sentences surpassed our best
bootstrapped tagger. However, it should be noted
that the learning curve approach was based on do-
main neutral, fully randomized, incremental texts,
which are not easily replicated in real world appli-
cations. The domain effect is particularly evident in
397
training on the sample Wycliffe and tagging on the
test PPCME set. Of course, our approach can be in-
tegrated with one based on annotation by using our
bootstrapped taggers to perform semi-automated an-
notation, even before the first human-annotated tag
has been labeled.
It is not certain how our method would fare on the
far more numerous parallel diachronic texts which
do not come prealigned. It is also questionable
whether it would still be robust on texts predating
Middle English, which might as well be written in
a foreign language when compared to Modern En-
glish. These are all limitations that need to be ex-
plored in the future.
Immediate improvements can be sought for the al-
gorithms themselves. By restricting the mapping of
words to only one POS tag in the Wycliffe Bible,
this seriously handicapped the utility of a bigram
tagger. It should be relatively straightforward to
transfer the probability mass of multiple POS tags
in a modern text to corresponding words in a di-
achronic text and include this modified probability
in the bigram tagger. When further augmented for
automatic parameter adjustment with the forward-
backward algorithm, accuracy rates might increase
further. Furthermore, different algorithms might be
better able to take advantage of similarities in or-
thography and syntactic structure when constructing
word alignment tables. Minimum Edit Distance al-
gorithms seem particularly promising in this regard.
Finally, it is evident that the utility of the Bible
as a potential resource of parallel texts has largely
gone untapped in NLP research. Considering that
it has probably been translated into more languages
than any other single text, and that this richness
of parallelism holds not only for synchrony but di-
achrony, its usefulness would apply not only to the
most immediate concern of building language tools
for many of the the world?s underdocumented lan-
guages, but also to cross-linguistic studies of un-
precedented scope at the level of language genera.
This study shows that despite the fact that any two
Bibles are rarely in a direct parallel relation, stan-
dard NLP methods can be applied with success.
References
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
Helena Britto, Marcelo Finger, and Charlotte Galves,
2002. Computational and linguistic aspects of the
construction of The Tycho Brahe Parsed Corpus of
Historical Portuguese. Tu?bingen: Narr.
Peter A. Chew, Steve J. Verzi, Travis L. Bauer, and
Jonathan T. McClain. 2006. Evaluation of the bible
as a resource for cross-language information retrieval.
In Proceedings of the Workshop on Multilingual Lan-
guage Resources and Interoperability, Sydney, July
2006, pages 68?74.
James R Curran and Stephen Clark. 2003. Investigat-
ing gis and smoothing for maximum entropy taggers.
In Proceedings of the 11th Meeting of the European
Chapter of the Association for Computational Linguis-
tics (EACL-03).
Lee R. Dice. 1945. Measures of the amount of eco-
logic association between species. Journal of Ecology,
26:297?302.
Elliott Franco Dra?bek and David Yarowsky. 2005. In-
duction of fine-grained part-of-speech taggers via clas-
sifier combination and crosslingual projection. In Pro-
ceedings of the ACL Workshop on Building and Us-
ing Parallel Texts, pages 49?56, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Barbara A. Fennell. 2001. A History of English: A Soci-
olinguistic Approach. Blackwell, Oxford.
Chung-Hye Han, 2000. The Evolution of Do-Support In
English Imperatives, pages 275?295. Oxford Univer-
sity Press.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Anthony Kroch and Ann Taylor. 2000. Penn-helsinki
parsed corpus of middle english, second edition.
Anthony Kroch, Ann Taylor, and Donald Ringe. 2000.
The middle english verb-second constraint: A case
study in language contact and language change. Ams-
terdam Studies in the Theory and History of Linguistic
Science Series, 4:353?392.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
398
Dan I. Melamed. 1998. Manual annotation of transla-
tion equivalence: The blinker project. In Technical
Report 98-07, Institute for Research in Cognitive Sci-
ence, Philadelphia.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Sylwia Ozdowska. 2006. Projecting pos tags and syntac-
tic dependencies from english and french to polish in
aligned corpora. In EACL 2006 Workshop on Cross-
Language Knowledge Induction.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Eric Brill and Ken-
neth Church, editors, Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 133?142. Association for Computational
Linguistics, Somerset, New Jersey.
Philip Resnik, Mari Broman Olsen, and Mona Diab.
1999. The bible as a parallel corpus: Annotating the
?book of 2000 tongues?. Computers and the Humani-
ties, 33(1?2):129?153.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In NAACL ?01: Sec-
ond meeting of the North American Chapter of the As-
sociation for Computational Linguistics on Language
technologies 2001, pages 1?8, Morristown, NJ, USA.
Association for Computational Linguistics.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via
robust projection across aligned corpora. In HLT
?01: Proceedings of the first international conference
on Human language technology research, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Appendix
Figure 4 provides the full mapping from PPCME
tags to the Penn Treebank Tags used in our evalu-
ation.
PPCME?PTB PPCME?PTB
ADJR?JJR N?NN
ADJS?JJS N$?NN
ADV?RB NEG?RB
ADVR?RBR NPR?NNP
ADVS?RBS NPR$?NNP
ALSO?RB NPRS?NNPS
BAG?VBG NPRS$?NNPS
BE?VB NS?NNS
BED?VBD NS$?NNS
BEI?VB NUM?CD
BEN?VBN NUM$?CD
BEP?VBZ ONE?PRP
C?IN ONE$?PRP$
CODE?CODE OTHER?PRP
CONJ?CC OTHER$?PRP
D?DT OTHERS?PRP
DAG?VBG OTHERS$?PRP
DAN?VBN P?IN
DO?VB PRO?PRP
DOD?VBD PRO$?PRP$
DOI?VB Q?JJ
DON?VBN Q$?JJ
DOP?VBP QR?RBR
E S?E S QS?RBS
ELSE?RB RP?RB
EX?EX SUCH?RB
FOR?IN TO?TO
FOR+TO?IN VAG?VBG
FP?CC VAN?VBN
FW?FW VB?VB
HAG?VBG VBD?VBD
HAN?VBN VBI?VB
HV?VB VBN?VBN
HVD?VBD VBP?VBP
HVI?VB WADV?WRB
HVN?VBN WARD?WARD
HVP?VBP WD?WDT
ID?ID WPRO?WP
INTJ?UH WPRO$?WP$
MAN?PRP WQ?IN
MD?MD X?X
MD0?MD
Figure 4: Table of mappings from PPCME tags to
Penn Treebank Tags.
399
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660?669,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Specialized models and ranking for coreference resolution
Pascal Denis
ALPAGE Project Team
INRIA Rocquencourt
F-78153 Le Chesnay, France
pascal.denis@inria.fr
Jason Baldridge
Department of Linguistics
University of Texas at Austin
Austin, TX 78712-0198, USA
jbaldrid@mail.utexas.edu
Abstract
This paper investigates two strategies for im-
proving coreference resolution: (1) training
separate models that specialize in particu-
lar types of mentions (e.g., pronouns versus
proper nouns) and (2) using a ranking loss
function rather than a classification function.
In addition to being conceptually simple, these
modifications of the standard single-model,
classification-based approach also deliver sig-
nificant performance improvements. Specifi-
cally, we show that on the ACE corpus both
strategies produce f -score gains of more than
3% across the three coreference evaluation
metrics (MUC, B3, and CEAF).
1 Introduction
Coreference resolution is the task of partitioning a
set of entity mentions in a text, where each par-
tition corresponds to some entity in an underlying
discourse model. While early machine learning ap-
proaches for the task relied on local, discriminative
classifiers (Soon et al, 2001; Ng and Cardie, 2002b;
Morton, 2000; Kehler et al, 2004), more recent ap-
proaches use joint and/or global models (McCallum
and Wellner, 2004; Ng, 2004; Daume? III and Marcu,
2005; Denis and Baldridge, 2007a). This shift im-
proves performance, but the systems are consider-
ably more complex and often less efficient. Here,
we explore two simple modifications of the first type
of approach that yield performance gains which are
comparable, and sometimes better, to those obtained
with these more complex systems. These modifica-
tions involve: (i) the use of rankers instead of clas-
sifiers, and (ii) the use of linguistically motivated,
specialized models for different types of mentions.
Ranking models provide a theoretically more ad-
equate and empirically better alternative approach
to pronoun resolution than standard classification-
based approaches (Denis and Baldridge, 2007b).
In essence, ranking models directly capture during
training the competition among potential antecedent
candidates, instead of considering them indepen-
dently. This gives the ranker additional discrimina-
tive power and in turn better antecedent selection ac-
curacy. Here, we show that ranking is also effective
for the wider task of coreference resolution.
Coreference resolution involves several different
types of anaphoric expressions: third-person pro-
nouns, speech pronouns (i.e., first and second person
pronouns), proper names, definite descriptions and
other types of nominals (e.g., anaphoric uses of in-
definite, quantified, and bare noun phrases). Differ-
ent anaphoric expressions exhibit different patterns
of resolution and are sensitive to different factors
(Ariel, 1988; van der Sandt, 1992; Gundel et al,
1993), yet most machine learning approaches have
ignored these differences and handle these different
phenomena with a single, monolithic model. A few
exceptions are worth noting. Morton (2000) and Ng
(2005b) propose different classifiers models for dif-
ferent NPs for coreference resolution and pronoun
resolution, respectively. Other partially capture the
differential preferences between different anaphors
via different sample selection strategies during train-
ing (Ng and Cardie, 2002b; Uryupina, 2004). More
recently, Haghighi and Klein (2007) use the distinc-
tion between pronouns, nominals and proper nouns
660
in their unsupervised, generative model for corefer-
ence resolution; for their model, this is absolutely
critical for achieving better accuracy. Here, we show
that using specialized models for different types
of referential expressions improves performance for
supervised models (both classifiers and rankers).
Both these strategies lead to improvements for
all three standard coreference metrics: MUC (Vilain
et al, 1995), B3 (Bagga and Baldwin, 1998), and
CEAF (Luo, 2005). In particular, our specialized
ranker system provides absolute f -score improve-
ments against an otherwise identical standard clas-
sifier system by 3.2%, 3.1%, and 3.6% for MUC, B3,
and CEAF, respectively.
2 Ranking
Numerous approaches to anaphora and coreference
resolution reduce these tasks to a binary classifica-
tion task, whereby pairs of mentions are classified as
coreferential or not (McCarthy and Lehnert, 1995;
Soon et al, 2001; Ng and Cardie, 2002b). Usually
used in combination with a greedy right-to-left clus-
tering, these approaches make very strong indepen-
dence assumptions. Not only do they model each
coreference decision separately, they actually model
each pair of mentions as a separate event. Recast-
ing these tasks as ranking tasks partly addresses this
problem by directly making the comparison between
different candidate antecedents for an anaphor part
of the training criterion. Each candidate is assigned
a conditional probability with respect to the entire
candidate set. (Re)rankers have been successfully
applied to numerous NLP tasks, such as parse se-
lection (Osborne and Baldridge, 2004; Toutanova et
al., 2004), parse reranking (Collins and Duffy, 2002;
Charniak and Johnson, 2005), question-answering
(Ravichandran et al, 2003).
The twin-candidate classification approach pro-
posed by (Yang et al, 2003) shares some similarities
with the ranker in making the comparison between
candidate antecedents part of training. An important
difference however is that under the twin-candidate
approach, candidates are compared in pairwise fash-
ion (and the best overall candidate is the one that has
won the most round robin contests), while the ranker
considers the entire candidate set at once. Another
advantage of the ranking approach is that its com-
plexity is only square in the number of mentions,
while that of the twin-candidate model is cubic (see
Denis and Baldridge (2007b) for a more detailed
comparison in the context of pronoun resolution).
Our ranking models for coreference take the fol-
lowing log-linear form:
Prk(?i|pi) =
exp
m?
j=1
wjfj(pi, ?i)
?
k
exp
m?
j=1
wjfj(pi, ?k)
(1)
where pi stands for the anaphoric expression, ?i for
an antecedent candidate, fj the weighted features of
the model. The denominator consists of a normal-
ization factor over the k candidate mentions. Model
parameters were estimated with the limited memory
variable metric algorithm and Gaussian smoothing
(?2=1000), using TADM (Malouf, 2002).
For the training of the different ranking models,
we use the following procedure. For each model, in-
stances are created by pairing each anaphor of the
proper type (e.g., definite description) with a set of
candidates which contains: (i) a true antecedent, and
(ii) a set of non-antecedents. The selection of the
true antecedent varies depending on the model we
are training: for pronominal forms, the antecedent
is selected as the closest preceding mention in the
chain; for non-pronominal forms, we used the clos-
est preceding non-pronominal mention in the chain
as the antecedent. For the creation of the non-
antecedent set, we collect all the non-antecedents
that appear in a window of two sentences around the
antecedent.1 At test time, we consider all preceding
mentions as potential antecedents.
Not all referential expressions in a given docu-
ment are anaphors: some expressions introduce a
discourse entity, rather than accessing an existing
one. Thus, coreference resolvers must have a way of
identifying such ?discourse-new? expressions. This
is easily handled in the standard classification ap-
proach: a mention will not be resolved if none of its
candidates is classified positively (i.e., as coreferen-
tial). The problem is more troublesome for rankers,
which always pick an antecedent from the candidate
1We suspect that different varying windows might be more
appropriate for different types of expressions, but leaves this for
further investigations.
661
set. A natural solution is to use a model that specifi-
cally predicts the discourse status (discourse-new vs.
discourse-old) of each expression: only expressions
that are classified as ?discourse-old? by this model
are considered by rankers.
Ng and Cardie (Ng and Cardie, 2002a) introduced
the use of an ?anaphoricity? classifier to act as a fil-
ter for coreference resolution in order to correct er-
rors where antecedents are mistakenly identified for
non-anaphoric mentions or antecedents are not de-
termined for mentions which are indeed anaphoric.
Their approach produced significant improvements
in precision, but with consequent larger losses in re-
call. Ng (2004) improves recall by optimizing the
anaphoricity threshold. By using joint inference for
anaphoricity and coreference, Denis and Baldridge
(2007a) avoid cascade-induced errors without the
need to separately optimize the threshold.
We use a similar discourse status classifier to Ng
and Cardie?s as a filter on mentions for our rankers.
We rely on three main types of information sources:
(i) the form of mention (e.g., type of linguistic ex-
pression, number of tokens), (ii) positional features
in the text, (iii) comparisons of the given mention to
the mentions that precede it in the text. Evaluated on
the ACE datasets, training the model on the train
texts, and applying the classifier to the devtest
texts, the model achieves an overall accuracy score
of 80.8%, compared to a baseline of 59.7% when
predicting the majority class (?discourse-old?).
3 Specialized models
Our second strategy is to use different, specialized
models for different referential expressions, simi-
larly to Elwell and Baldridge?s (2008) use of connec-
tive specific models for identifying the arguments of
discourse connectives. For this, one must determine
along which dimension to split such expressions.
For example, Ng (2005b) learns models for each set
of anaphors that are lexically identical (e.g., I, he,
they, etc.). This option is possible for closed sets
like pronouns, but not for other types of anaphors
like proper names and definite descriptions. Another
option is to rely on the particular linguistic form of
the different expressions, as signaled by the head
word category and the determiner (if any). More
concretely, we use separate models for the follow-
ing types: (i) third person pronouns, (ii) speech pro-
nouns, (iii) proper names, (iv) definite descriptions,
and (v) others (i.e., all expressions that don?t fall into
the previous categories).
The correlation between the form of a referen-
tial expression and its anaphoric behavior is actually
central to various linguistic accounts (Prince, 1981;
Ariel, 1988; Gundel et al, 1993). Basically, the idea
is that linguistic form is an indicator of the status of
the corresponding referent in the discourse model.
That is, the use by the speaker of a particular lin-
guistic form corresponds to a particular level of acti-
vation (or familiarity or salience or accessibility) in
(what she thinks is) the addressee?s discourse model.
For many authors, the relation takes the form of a
continuum and is often represented in the form of a
referential hierarchy, such as:
Accessibility Hierarchy (Ariel, 1988)
Zero pronouns >> Pronouns >> Demonstra-
tive pronouns >> Demonstrative NPs >>
Short PNs >> Definite descriptions >> Full
PNs >> Full PNs + appositive
The higher up, the more accessible (or salient) the
entity is. At the extremes are pronouns (these forms
typically require a previous mention in the local con-
text) and proper names (these forms are often used
without previous mentions of the entity). This type
of hierarchy is validated by corpus studies of the
distribution of different types of expressions. For
instance, pronouns find their antecedents very lo-
cally (in a window of 1-2 sentences), while proper
names predominantly find theirs at longer distances
(Ariel, 1988).2 Using discourse structure, Asher et
al. (2006) show that while anaphoric pronouns sys-
tematically obey the right-frontier constraint (i.e.,
their antecedents have to appear on the right edge
of the discourse graph), this is less so for definites,
and even less so for proper names.
From a machine learning perspective, these find-
ings suggest that features encoding some aspect of
salience (e.g., distance, syntactic context) are likely
to receive different sets of parameters depending on
the form of the anaphor. This therefore suggests
that better parameters are likely to be learned in the
2Haghighi and Klein?s (2007) generative coreference model
mirrors this in the posterior distribution which it assigns to men-
tion types given their salience (see their Table 1).
662
Type/Count train test
3rd pron. 4, 389 1, 093
speech pron. 2, 178 610
proper names 7, 868 1, 532
def. NPs 3, 124 796
others 1, 763 568
Total 19, 322 4, 599
Table 1: Distribution of the different anaphors in ACE
context of different models.3 While the above stud-
ies focus primarily on salience, there are of course
other dimensions according to which anaphors differ
in their resolution preferences. Thus, the resolution
of lexical expressions like definite descriptions and
proper names is likely to benefit from the inclusion
of features that compare the strings of the anaphor
and the candidate antecedent (e.g., string matching)
and features that identify particular syntactic config-
urations like appositive structures. This type of in-
formation is however much less likely to help in the
resolution of pronominal forms. The problem is that,
within a single model, such features are likely to re-
ceive strong parameters (due to the fact that they are
good predictors for lexical anaphors) in a way that
might eventually hurt pronominal resolutions.
Note that our split of referential types only
partially cover the referential hierarchies of Ariel
(1988) or Gundel et al (1993). Thus, there is no sep-
arate model for demonstrative noun phrases and pro-
nouns: these are very rare in the corpus we used (i.e.,
the ACE corpus).4 These expressions were therefore
handled through the ?others? model. There is how-
ever a model for first and second person pronouns
(i.e., speech pronouns): this is justified by the fact
that these pronouns behave differently from their
third person counterparts. These forms indeed of-
ten behave like deictics (i.e., they refer to discourse
participants) or they appear within a quote.
The total number of anaphors (i.e., of mentions
that are not chain heads) in the data is 19, 322 and
4, 599 for training and testing, respectively. The dis-
tribution of each anaphoric type is presented in Ta-
ble 1. Roughly, third person pronouns account for
3Another possible approach would consist in introducing
different salience-based features encoding the form of the
anaphor.
4There are only 114 demonstrative NPs and 12 demonstra-
tive pronouns in the entire ACE training.
Linguistic Form
pn ? is a proper name {1,0}
def np ? is a definite description {1,0}
indef np ? is an indefinite description {1,0}
pro ? is a pronoun {1,0}
Context
left pos POS of the token preceding ?
right pos POS of the token following ?
surr pos pair of POS for the tokens surrounding ?
Distance
s dist Binned values for sentence distance between pi and ?
np dist Binned values for mention distance between pi and ?
Morphosyntactic Agreement
gender pairs of attributes {masc, fem, neut, unk} for pi and ?
number pairs of attributes {sg, pl} for pi and ?
person pairs of attributes {1, 2, 3, 4, 5, 6} for pi and ?
Semantic compatibility
wn sense pairs of Wordnet senses for pi and ?
String similarity
str match pi and ? have identical strings {1,0}
left substr one mention is a left substring of the other {1,0}
right substr one mention is a right substring of the other {1,0}
hd match pi and ? have the same head word {1,0}
Apposition
apposition pi and ? are in an appositive structure {1,0}
Acronym
acronym pi is an acronym of ? or vice versa {1,0}
Table 2: Features used by coreference models.
22-24% of all anaphors in the entire corpus, speech
pronouns for 11-13%, proper names for 33-40%,
and definite descriptions for 16-17%. The distribu-
tion is slightly different from one dataset to another,
probably reflecting genre differences. For instance,
BNEWS shows a larger proportion of pronouns in
general (pronominal forms account for 40-44% of
all the anaphoric forms).
We use five broad types of features for all mention
types, plus three others used by specific types, sum-
marized in Table 3. Our feature extraction relies on
limited linguistic processing: we only made use of a
sentence detector, a tokenizer, a POS tagger (as pro-
vided by the OpenNLP Toolkit5) and the WordNet6
database. Since we did not use parser, lexical heads
for the NP mentions were computed using simple
heuristics relying solely on POS sequences. Table 2
describes in detail the entire feature set, and Table 3
shows which features were used for which models.
Linguistic form: the referential form of the an-
tecedent candidate: a proper name, a definite de-
5http://opennlp.sf.net.
6http://wordnet.princeton.edu/
663
Features/Types 3P SP PN Def-NP Oth
Ling. form
? ? ? ? ?
Context
? ? ? ? ?
Distance
? ? ? ? ?
Agreement.
? ? ? ? ?
Sem. compat.
? ? ? ? ?
Str. sim.
? ? ?
Apposition
? ?
Acronym
?
Table 3: Features for each type of referential expression.
scription, an indefinite NP, or a pronoun.
Context: the context of the antecedent candidate:
these features can be seen as approximations of the
grammatical roles, as indicators of the salience of
the potential candidate (Grosz et al, 1995). For
instance, this includes the part of speech tags sur-
rounding the candidate, as well as a feature that
indicates whether the potential antecedent is the
first mention in a sentence (approximating subject-
hood), and a feature indicating whether the candi-
date is embedded inside another mention.
Distance: the distance between the anaphor and
the candidate, measured by the number of sentences
and mentions between them.
Morphosyntactic agreement: indicators of the
gender, number, and person of the two mentions.
These are determined for non-pronominal NPs with
heuristics based on POS tags (e.g., NN vs. NNS for
number) and actual mention strings (e.g., whether
the mention contains a male/female first name or
honorific for gender). These features consist of pairs
of attributes, ensuring that not only strict agreement
(e.g., singular-singular) but also mere compatibility
(e.g., masculine-unknown) is captured.
Semantic compatibility: features designed to as-
sess whether the two mentions are semantically
compatible. For these features, we use the Word-
Net database: in particular, we collected the syn-
onym set (or synset) as well as the synset of their
direct hypernyms associated with each mention. In
the case of common nouns, we used the synset asso-
ciated with the first sense associated with the men-
tion?s head word. In the case of proper names, we
used the synset associated with the name if avail-
able, and the string itself otherwise. For pronouns
(which are not part of Wordnet), we simply used the
pronominal form.
All these features were used in all five models.
While one may question the use of distance for non-
pronominal anaphors,7 their inclusion can be justi-
fied in that they might predict some ?obviation? ef-
fects. Definite descriptions and proper names are
sensitive to distance too, although not in the same
way as pronouns are: they show a preference for an-
tecedents that appear outside a window of one or two
sentences (Ariel, 1988).
Several features are used only for particular men-
tion types:
String similarity: similarity of the anaphor and
the candidate strings. Examples are perfect string
match, substring matches, and head match (i.e., the
two mentions share the same head word).
Appositive: whether the anaphor is an appositive
of the antecedent candidate. Since we do not have
access to syntactic structure, we use heuristics (e.g.,
the presence of a comma between the two mentions)
to extract this feature.
Acronym: whether the anaphor string is an
acronym of the candidate string (or vice versa): e.g.,
NSF and National Science Foundation.
4 Coreference systems
We evaluate several systems to explore the effect of
ranking versus classification and specialized versus
monolithic models. The different systems follow a
generic architecture. Let M be the set of mentions
present in a document. For all models, each mention
m ? M is associated at test time with a set of an-
tecedent candidates Cm, which includes all the men-
tions that linearly precede m. The best candidate is
determined by the model in use. The final output of
each system consists in a list of mention pairs (i.e.,
the coreference links) which in turn defines (through
reflexive, transitive closure) a partition over the set
M. Our models are summarized in Table 4.
The use of the discourse status filter is straightfor-
ward. For each mention m?M, the discourse status
7In fact, Morton (2000) does not use distance in this case.
664
Model Disc.
Model Name Type Specialized? Status
CLASS class No No
CLASS+DS class No Yes
CLASS+SP class Yes No
CLASS+DS+SP class Yes Yes
RANK+DS+SP rank Yes Yes
Table 4: Model names and their properties.
model is first applied to determine whether m intro-
duces a new discourse entity (i.e., it is classified as
?new?) or refers back to an existing entity (i.e., it
is classified as ?old?). If m is classified as ?new?,
the process terminates and goes to the next mention.
If m is classified as ?old?, m along with its set of
antecedent candidates Cm is sent to the model.
For classifiers, we replicate the procedures of Ng
and Cardie (2002b). During training, instances are
formed by pairing each anaphor with each of its pre-
ceding candidates, until the antecedent is reached:
the closest preceding antecedent in the case of a
pronominal anaphor, or the closest non-pronominal
antecedent for other anaphor types. For classifiers,
the use of a discourse status filter at test time is op-
tional. When a filter is not used, then a mention
is left unresolved if none of the pairs created for a
given mention is classified positively. If several pairs
for a given mention are classified positively, then the
pair with the highest score is selected (i.e., ?Best-
First? link selection). If a filter is used, then the can-
didate with the highest score is selected, even if the
probability of coreference is less than one-half.8
The use of specialized models is simple, for both
classifiers and rankers. Specialized models are cre-
ated for: (i) third person pronouns, (ii) speech pro-
nouns, (iii) proper names, (iv) definite descriptions,
(v) other types of phrases. The mention type is de-
8This is very similar to the approach of Ng and Cardie
(2002a). An important difference is that their system does not
necessarily yield an antecedent for each of the anaphors pro-
posed by the discourse status model. In their system, if the
coreference classifier finds that none of the candidates for a
?new? mention are coreferential, it leaves it unresolved. In this
case, the coreference model acts as an additional filter. Not sur-
prisingly, these authors report gains in precision but compar-
atively larger losses in recall. Our development experiments
revealed that forcing a decision on items identified as new pro-
vided performed better across all metrics.
System Accuracy
3rd pron. 82.2
speech pron. 66.9
proper names 83.5
def. NPs 66.5
others 63.6
Table 5: Accuracy of the different ranker models.
termined and the best candidate is chosen by the
appropriate model Following Elwell and Baldridge
(2008), these models could be interpolated with a
monolithic model, or even word specific models, but
we have not explored that option here.
The feature sets for the classifiers in the base-
line systems includes all the features that were used
for the described in Section 3. For the classi-
fiers that do not use specialized models (CLASS and
CLASS+DS), we have also added extra features de-
scribing the linguistic form of the potential anaphor
(whether it is a pronoun, a proper name, and so
on). This is in accordance with standard feature sets
in the pairwise approach. It gives these models a
chance to learn weights more appropriately for the
different types within a single, monolithic model.
5 Experiments
We use the ACE corpus (Phase 2). The corpus has
three parts, each corresponding to a different genre:
newspaper texts (NPAPER), newswire texts (NWIRE),
and broadcast news (BNEWS). Each set is split into
a train part and a devtest part. In our experi-
ments, we consider only true ACE mentions.
5.1 Antecedent selection results
We first evaluate the specialized ranker models
individually on the task of anaphora resolution:
their ability to select a correct antecedent for each
anaphor. Following common practice in this task,
we report results in terms of accuracy, which is sim-
ply the ratio of correctly resolved anaphors. The
candidate set during testing was formed by taking
all the mentions that appear before the anaphor.
Also, we assume that correctly resolving an anaphor
amounts to selecting any of the previous mentions in
the entity as the antecedent. The accuracy scores for
the different models are presented in Table 5.
665
The best accuracy results on the entire ACE cor-
pus are found first for the proper name resolver with
a score of 83.5%, then for the third person pronoun
resolver with 82.2%, then for the definite descrip-
tion and speech pronoun resolvers with 66.9% and
66.5% respectively. The worst scores are obtained
for the ?others? category. The high scores for the
third person pronoun and the proper name rankers
most likely follow from the fact that the resolution
of these expressions relies on simple, reliable pre-
dictors, such as distance and morphosyntactic agree-
ment for pronouns, and string similarity features for
proper names. The resolution of definite descrip-
tions and other types of lexical NPs (which are han-
dled through the ?others? model) are much more
challenging: they rely on lexical semantic and world
knowledge, which is only partially encoded via our
WordNet-based features. Finally, note that the reso-
lution of speech pronouns is also much harder than
that of the other pronominal forms: these expres-
sions are much less (if at all) constrained by re-
cency and agreement. Furthermore, these expres-
sions show a lot of cataphoric uses, which are not
considered by our models. The low scores for the
?others? category is likely due to the fact that it en-
compasses very different referential expressions.
5.2 Coreference Results
For evaluating the coreference performance, we rely
on three primary metrics: (i) the link based MUC
metric (Vilain et al, 1995), the mention based B3
metric (Bagga and Baldwin, 1998), and the entity
based CEAF metric (Luo, 2005). Common to these
metrics is: (i) they operate by comparing the set of
chains S produced by the system against the true
chains T , and (ii) they report performance in terms
of recall and precision. There are however impor-
tant differences in how each metric computes these
scores, each producing a different bias.
MUC scores are based on the number of links
(pairs of mentions) common to S and T . Recall
is the number of common links divided by the to-
tal number of links in T ; precision is the number of
common links divided by the total number of links
in S. This focus gives MUC two main biases. First,
it favors systems that create large chains (and thus
fewer entities). For instance, a system that produces
a single chain achieves 100% recall without severe
degradation in precision. Second, it ignores single
mention entities, which are involved in no links.9
The B3 metric was designed to address the MUC
metric?s shortcomings. It is mention-based: it com-
putes both recall and precision scores for each men-
tion i. Let S be the system chain containing m, T
be the true chain containing m. The set of correct
elements in S is thus |S ? T |. The recall score for
a mention i is |S?T ||T | , while the precision score for i
is |S?T ||S| . Overall recall/precision is obtained by av-
eraging over the individual mention scores. The fact
that this metric is mention-based by definition solves
the problem of single mention entities. Also solved
is the bias favoring larger chains, since this will be
penalized in the precision score of each mention.
The Constrained Entity Aligned F-Measure
(CEAF) (Luo, 2005). aligns each system chain S
with at most one true chain T . It finds the best one-
to-one mapping between the set of chains S and T ,
which is equivalent to finding the optimal alignment
in a bipartite graph. The best mapping maximizes
the similarity over pairs of chains (Si, Ti), where
the similarity between two chains is the number of
common mentions to the two chains. With CEAF,
recall is computed as the total similarity divided by
the number of mentions in all the T (i.e., the self-
similarity), while precision is the total similarity di-
vided by the number of mentions in S.
Table 6 gives scores for all three metrics
for the different models on the entire ACE
corpus. Two main patterns emerge: sig-
nificant improvements are obtained by using
specialized models (CLASS vs CLASS+SP and
CLASS+DS vs CLASS+DS+SP) and by using a
ranker (CLASS+DS+SP vs RANK+DS+SP). Overall,
the RANK+DS+SP system significantly outperforms
the other systems on the three different metrics.10
The f -scores for RANK+DS+SP are 71.6% with
the MUC metric, 72.7% with the B3, and 67.0%
with the CEAF metric. These scores place the
RANK+DS+SP among the best coreference resolu-
tion systems, since most existing systems are typi-
cally under the bar of the 70% in f -score with the
9It is worth noting that the MUC corpus does not annotate
single mention entities.
10Statistical significance was determined with t-tests for both
recall and precision scores, with p < 0.05.
666
System MUC B3 CEAF
R P F R P F F
CLASS 60.8 72.6 66.2 62.4 77.7 69.2 62.3
CLASS+DS 64.9 72.3 68.4 65.6 74.1 69.6 63.4
CLASS+SP 64.8 74.5 69.3 65.3 79.1 71.5 65.0
CLASS+DS+SP 66.8 74.4 70.4 66.4 77.0 71.3 65.3
RANK+DS+SP 67.9 75.7 71.6 66.8 79.8 72.7 67.0
Table 6: Recall (R), Precision (P), and f -score (F) results on the entire ACE corpus using the MUC, B3, and CEAF
metrics. Note that R=P=F for CEAF when using true mentions, as we do here.
MUC and B3 metrics (Ng, 2005a). An interesting
point of comparison is provided by Ng (2007), who
also relies on true mentions and reports MUC f -
scores only slightly superior to ours (73.8%) while
relying on perfect semantic class information. His
best results otherwise are 64.6%. The fact that
our improvements are consistent across the different
evaluation metrics is remarkable, especially given
that these three metrics are quite different in the
way they compute their scores. The gains in f -
score range from 1.2 to 5.4% on the MUC metric
(i.e., error reductions of 4 to 15.9%), from 1.4 to
3.5% on the B3 metric (i.e., error reductions of 4.8
to 11.4%), and from 1.7 to 4.7% on the CEAF met-
ric (i.e., error reductions of 6.9 to 17%). The larger
improvements come from recall, with improvements
ranging from 1.9 to 7.1% with MUC, from 2.4 to
5.6% with B3.11 This suggests that RANK+DS+SP
predicts many more valid coreference links than the
other systems. Smaller but still significant gains are
made in precision: RANK+DS+SP is also able to re-
duce the proportion of invalid links.
The overall improvements found with
RANK+DS+SP suggest that it is able to capi-
talize on the better antecedent selection capabilities
offered by the ranking approach. This is supported
by the error analysis on the development data.
Errors made by a coreference system can be con-
ceptualized as falling into three main classes: (i)
?missed anaphors? (i.e., an anaphoric mention that
fails to be linked to a previous mention), (ii) ?spuri-
ous anaphors? (i.e., an non-anaphoric mention that
is linked to a previous mention), and (iii) ?invalid
resolutions? (i.e., a true anaphor that is linked to a
11Recall that recall and precision scores are identical with
CEAF, due to the fact that we are using true mention boundaries.
incorrect antecedent). The two first types of error
pertain to the determination of the discourse status
of the mention, while the third regards the selection
of an antecedent (i.e., anaphora resolution). Con-
sidering the systems? invalid resolutions, we found
that the RANK+DS+SP had a much lower error rate:
only 17.9% of all true anaphors were incorrectly
resolved by this system, against 23.1% for CLASS,
24.9% for CLASS+DS, 20.4% for CLASS+SP, and
22.1% for CLASS+DS+SP.
Denis (2007) provides multi-metric scores for the
JOINT-ILP model of Denis and Baldridge (2007a),
which uses integer linear programming for joint in-
ference over coreference resolution and discourse
status: f -scores of 73.3%, 68.0%, and 58.9% for
MUC, B3, and CEAF, respectively. Despite the fact
that this MUC score beats RANK+DS+SP?s, it is ac-
tually worse than even the basic model CLASS for
B3 and CEAF. This difference fact that MUC gives
more recall credit for large chains without a conse-
quent precision reduction, and shows the importance
of using B3 and CEAF scores in addition to MUC.
Denis (2007) also extends the JOINT-ILP system
by adding named entity resolution and constraints
on transitivity with respect to coreference links. The
best model reported there (JOINT-DS-NE-AE-ILP)
obtains f -scores of 70.1%, 72.7%, and 66.2% for
MUC, B3, and CEAF, respectively. Interestingly,
RANK+DS+SP actually performs better across all
metrics despite being a simpler model with fewer
sources of information.
5.3 Oracle results
Using specialized rankers with a discourse status
classifier yields coreference performance superior to
that given by various classification-based baseline
systems. Crucially, these improvements have been
667
System MUC B3 CEAF
R P F R P F F
RANK+DS+SP 67.9 75.7 71.6 66.8 79.8 72.7 67.0
RANK+DS-ORACLE+SP 79.1 79.1 79.1 75.4 76.0 75.7 76.9
LINK-ORACLE 78.8 100.0 88.1 74.3 100.0 85.2 79.7
Table 7: Recall (R), Precision (P), and f -score (F) results for RANK+DS-ORACLE+SP and LINK-ORACLE on the
entire ACE corpus.
possible using a discourse status model that has an
accuracy of just 80.8%. Clearly, the performance
of the discourse status module has a direct impact
on the performance of the entire coreference sys-
tem. On the one hand, misclassified anaphors are
simply not resolved by the rankers: this limits the
recall of the coreference system. On the other hand,
misclassified non-anaphors are linked to a previous
mention: this limits precision.
In order to further assess the impact of the er-
rors made by the discourse status classifier, we build
two different oracle systems. The first oracle sys-
tem, RANK+DS-ORACLE+SP, uses the specialized
rankers in combination with a perfect discourse sta-
tus classifier. That is, this system knows for each
mention whether it is anaphoric or not: the only er-
rors made by such a system are invalid resolutions.
RANK+DS-ORACLE+SP thus provides an upper-
bound for the RANK+DS+SP model. The results for
this oracle are given in Table 7: they show substan-
tial improvements over RANK+DS+SP, which sug-
gests that the RANK+DS+SP has also the potential
to be further improved if used in combination with a
more accurate discourse status classifier.
The second oracle system, LINK-ORACLE, uses
the discourse status classifier with a perfect corefer-
ence resolver. That is, this system has perfect knowl-
edge regarding the antecedents of anaphors: the er-
rors made by such a system are only errors in the
discourse status of mentions. The results for LINK-
ORACLE are also reported in Table 7. These figures
show that however accurate our models are at pick-
ing a correct antecedent for a true anaphor, the best
they can achieve in terms of f -scores is 88.1% with
MUC, 85.2% with B3, and 79.7% with CEAF.
6 Conclusion
We present and evaluate two straight-forward tac-
tics for improving coreference resolution: (i) rank-
ing models, and (ii) separate, specialized models
for different types of referring expressions. The
specialized rankers are used in combination with
a discourse status classifier which determines the
mentions that are sent to the rankers. This simple
pipeline architecture produces significant improve-
ments over various implementations of the standard,
classifier-based coreference system. In turn, these
strategies could be integrated with the joint infer-
ence models we have explored elsewhere (Denis and
Baldridge, 2007a; Denis, 2007) and which have ob-
tained performance improvements that are orthogo-
nal to those obtained here.
This paper?s improvements are consistent across
the three main coreference evaluation metrics: MUC,
B3, and CEAF.12 We attribute improvements to: (i)
the better antecedent selection capabilities offered
by the ranking approach, and (ii) the division of la-
bor between specialized models, allowing each one
to better model the corresponding distribution.
Acknowledgments
We would like to thank Nicholas Asher, Andy
Kehler, Ray Mooney, and the three anonymous re-
viewers for their comments. This work was sup-
ported by NSF grant IIS-0535154.
References
M. Ariel. 1988. Referring and accessibility. Journal of
Linguistics, pages 65?87.
N. Asher, P. Denis, and B. Reese. 2006. Names and pops
and discourse structure. In Workshop on Constraints
in Discourse, Maynooth, Ireland.
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of LREC 1998,
pages 563?566.
12We strongly advocate that coreference results should never
be presented in terms of MUC scores alone.
668
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL 2005, Ann Arbor, Michigan.
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete
structures and the voted perceptron. In Proceedings of
ACL 2002, pages 263?270, Philadelphia, PA.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In Proceedings of HLT-
EMNLP 2005, Vancouver, Canada.
P. Denis and J. Baldridge. 2007a. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of HLT-NAACL 2007,
Rochester, NY.
P. Denis and J. Baldridge. 2007b. A ranking approach
to pronoun resolution. In Proceedings of IJCAI 2007,
Hyderabad, India.
Pascal Denis. 2007. New Learning Models for Robust
Reference Resolution. Ph.D. thesis, The University of
Texas at Austin.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In Proceedings of the International Confer-
ence on Semantic Computing, Santa Clara, CA.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
A framework for modelling the local coherence of dis-
course. Computational Linguistics, 2(21).
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, 69:274?307.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric Bayesian model.
In Proceedings ACL 2007, pages 848?855, Prague,
Czech Republic.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proceedings of HLT-
NAACL 2004.
X. Luo. 2005. On coreference resolution performance
metrics. In Proceedings of HLT-NAACL 2005, pages
25?32.
R. Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings
of the Sixth Workshop on Natural Language Learning,
pages 49?55, Taipei, Taiwan.
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In Proceedings of NIPS 2004.
J. F. McCarthy and W. G. Lehnert. 1995. Using deci-
sion trees for coreference resolution. In IJCAI, pages
1050?1055.
T. Morton. 2000. Coreference for NLP applications. In
Proceedings of ACL 2000, Hong Kong.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In Proceedings of COLING 2002.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of ACL 2002, pages 104?111.
V. Ng. 2004. Learning noun phrase anaphoricity to im-
prove coreference resolution: Issues in representation
and optimization. In Proceedings of ACL 2004.
V. Ng. 2005a. Machine learning for coreference reso-
lution: From local classification to global ranking. In
Proceedings of ACL 2005, pages 157?164, Ann Arbor,
MI.
V. Ng. 2005b. Supervised ranking for pronoun resolu-
tion: Some recent improvements. In Proceedings of
AAAI 2005.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of ACL 2007.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proceedings of
HLT-NAACL 2004, pages 89?96, Boston, MA.
E. F. Prince. 1981. Toward a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York.
D. Ravichandran, E. Hovy, and F. J. Och. 2003. Sta-
tistical QA - classifier vs re-ranker: What?s the differ-
ence? In Proceedings of the ACL Workshop on Mul-
tilingual Summarization and Question Answering?
Machine Learning and Beyond.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
K. Toutanova, P. Markova, and C. Manning. 2004. The
leaf projection path view of parse trees: Exploring
string kernels for HPSG parse selection. In Proceed-
ings of EMNLP 2004, pages 166?173, Barcelona.
O. Uryupina. 2004. Linguistically motivated sample se-
lection for coreference resolution. In Proceedings of
DAARC 2004, Furnas.
R. van der Sandt. 1992. Presupposition projection as
anaphora resolution. Journal of Semantics, 9:333?
377.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. In Proceedings fo the 6th Mes-
sage Understanding Conference (MUC-6), pages 45?
52, San Mateo, CA. Morgan Kaufmann.
X. Yang, G. Zhou, J. Su, and C.L. Tan. 2003. Corefer-
ence resolution using competitive learning approach.
In Proceedings of ACL 2003, pages 176?183.
669
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 296?305,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
How well does active learning actually work? Time-based evaluation of
cost-reduction strategies for language documentation.
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@mail.utexas.edu
Alexis Palmer
Computational Linguistics
Saarland University
apalmer@coli.uni-sb.de
Abstract
Machine involvement has the potential to
speed up language documentation. We as-
sess this potential with timed annotation
experiments that consider annotator exper-
tise, example selection methods, and sug-
gestions from a machine classifier. We
find that better example selection and la-
bel suggestions improve efficiency, but ef-
fectiveness depends strongly on annota-
tor expertise. Our expert performed best
with uncertainty selection, but gained lit-
tle from suggestions. Our non-expert per-
formed best with random selection and
suggestions. The results underscore the
importance both of measuring annotation
cost reductions with respect to time and of
the need for cost-sensitive learning meth-
ods that adapt to annotators.
1 Introduction
Data annotated with linguistically interesting la-
bels is used in a wide variety of contexts. Com-
putational linguists generally use annotated data
as training and evaluation material for natural lan-
guage processing systems; corpus linguists use it
to test hypotheses about language; documentary
linguists create interlinear glossed texts to pre-
serve examples of endangered languages and hy-
potheses about the grammars of those languages.
Regardless of the context, creating annotated data
is costly in terms of time and/or money. Since both
time and money are undeniably in limited supply,
there is a widely shared desire to reduce this cost.
Reducing cost involves strategies that do more
with fewer human-annotated labels and/or reduce
the per-label cost. An example of the former is ac-
tive learning, which focuses annotation effort on
data points selected by the learner(s) for their ex-
pected utility in developing a more accurate model
(Settles, 2009). Examples of the latter include
providing suggestions from a machine labeler and
using extremely cheap human labelers, e.g. with
the Amazon Mechanical Turk (Snow et al, 2008).
Different techniques may be more or less appli-
cable depending on the language being annotated,
the kind of labels which are desired (tags, syntac-
tic structures, etc.), and the desired use of the an-
notated data (e.g., for training models, testing lin-
guistic hypotheses, or preserving a language).
This paper discusses experiments that measure
the effectiveness of machine-aided annotation for
language documentation using both active learn-
ing simulation experiments and annotation ex-
periments which involve actual documentary lin-
guists interacting with machine example selec-
tion and label suggestion. Specifically, we deal
with the task of labeling morphemes of the Mayan
language Uspanteko with fine-grained parts-of-
speech. We also run active learning simulation
experiments for part-of-speech tagging for Dan-
ish, Dutch, English, Swedish, and Uspanteko to
show the validity of our models and methods in a
standard setting. For Uspanteko, we provide re-
sults from annotation experiments in which anno-
tation cost is measured in terms of the actual an-
notation time required while varying three factors:
(1) example selection, (2) machine label sugges-
tions, and (3) annotator expertise.
Our findings indicate that there is consider-
able promise for reducing the cost of produc-
ing IGT, but they also demonstrate considerable
variation due to the interaction of these factors.
This suggests different prescriptions for appropri-
ate strategies in different contexts. Most clearly,
the worst performing strategy?by far?is that
used in nearly all documentary work: sequential
annotation without automation. Also, our expert
annotator did best with examples picked by un-
certainty selection, while our non-expert did best
with random selection aided by machine label sug-
296
Language #words-tr #words-dev #tags #sents-tr #sents-dev Avg.sent Avg.tr.sent Avg.dev.sent
Danish 62825 31561 10 3570 1618 18.18 17.60 19.50
Dutch 129586 65483 13 9365 3982 14.61 13.84 16.44
English 167593 131768 45 6945 5527 24.00 24.13 23.84
Swedish 127684 63783 41 7326 3714 17.34 17.43 17.17
Uspanteko 43473 19906 69 7423 3288 5.92 5.86 6.05
Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length.
gestions. This difference confirms the importance
of cost-sensitive active learning strategies that are
not just learner-guided, but also take into account
modeling of the annotators (Settles et al, 2008;
Haertel et al, 2008; Vijayanarasimhan and Grau-
man, 2008). Finally, we confirm the importance
of using actual annotation time to measure annota-
tion cost: a unit-cost assumption?even at a fine-
grained level?can dramatically misrepresent the
actual effectiveness of different strategies.
2 Task and data
Annotation task: language documentation
The amount of money spent on obtaining human
annotations is an extremely important concern in
much language annotation. However, there is a
further urgency for annotation in the case of lan-
guage documentation: languages are dying at the
rate of two each month. By the end of this cen-
tury, half of the approximately 6000 extant spoken
languages will cease to be transmitted effectively
from one generation of speakers to the next (Crys-
tal, 2000). Recorded and transcribed texts anno-
tated with detailed linguistic information create an
important multi-faceted record of these languages,
but there are few trained linguists with adequate
time and appropriate levels of funding relative to
the size of the problem. Annotation cost?in both
time and money?is thus keenly felt in the work
of documenting and describing endangered lan-
guages. Active learning and automated label sug-
gestions could help deal with this language docu-
mentation bottleneck.
We focus on one stage of language documen-
tation, the production of interlinear glossed text
(IGT), a standard form of annotation that in-
volves both morphological and grammatical anal-
ysis. IGT is generally created following transcrip-
tion and translation of recorded speech, with the
annotations often being provided by trained anno-
tators with varying levels of expertise. The result
is generally a small amount of IGT annotated data
and a greater amount of unannotated data.
Data We use a collection of 32 interlinear
glossed texts (IGT) in the Mayan language Uspan-
teko. This corpus was cleaned up and adapted by
Palmer et al (2009) from an original collection of
67 texts that were collected, transcribed, translated
and annotated by the OKMA language documen-
tation project (Pixabaj et al, 2007).
Two core tasks in creating IGT are morpholog-
ical analysis and tagging morphemes with their
glosses (labels indicating part-of-speech and/or
grammatical function). We deal with the latter task
and assume texts are morphologically segmented.
Standard four-line IGT has morphemes on one line
and their glosses on the next. The gloss line in-
cludes labels for grammatical morphemes (e.g. PL
or COM) and translations of stems (e.g. hablar or
idioma). The following is an Uspanteko example:
(1) TEXT: Kita? tinch?ab?ej laj inyolj iin
MORPH:
GLOSS:
POS:
kita?
NEG
PART
t-in-ch?abe-j
INC-E1S-hablar-SC
TAM-PERS-VT-SUF
laj
PREP
PREP
in-yol-j
A1S-idioma-SC
PERS-S-SUF
iin
yo
PRON
TRANS: ?No le hablo en mi idioma.?
We use a single layer that is a combination of the
GLOSS and POS layers (Palmer et al, 2009). For
(1), the morphemes and labels for our task are:
(2) kita?
NEG
t-
INC
in-
E1S
ch?abe
VT
-j
SC
laj
PREP
in-
A1S
yol
S
-j
SC
iin
PRON
We also consider POS-tagging for Danish,
Dutch, English, and Swedish; the English is from
sections 00-05 (as training set) and 19-21 (as de-
velopment set) of the Penn Treebank (Marcus et
al., 1993), and the other languages are from the
CoNLL-X dependency parsing shared task (Buch-
holz and Marsi, 2006).
1
We split the original train-
ing data into training and development sets. Ta-
ble 1 shows the number of words and sentences
in each split of each dataset, as well as the num-
ber of possible labels and the average sentence
length. The Uspanteko data is counted in mor-
phemes rather than words; also, the Uspanteko
texts are divided at the clause rather than sentence
level. This gives the corpus a much lower average
clause length than the other languages (Table 1).
1
The subset of the Penn Treebank was chosen to be of
comparable size to the CoNLL datasets.
297
3 Model and methods
Classification model. We use a standard maxi-
mum entropy classifier for tagging Danish, Dutch,
English, and Swedish words with POS-tags and
tagging Uspanteko morphemes with Gloss/POS
tags. The label for a word/morpheme is pre-
dicted based on the word/morpheme itself plus
a window of two units before and after. Stan-
dard part-of-speech tagging features (Ratnaparkhi,
1998; Curran and Clark, 2003) are extracted from
the morpheme to help with predicting labels for
previously unseen morphemes. This is a strong
but standard model; better, more complex models
could be used, but the gains are likely to be small.
Thus, we opted for simplicity in our model so as to
focus more on the interaction between the annota-
tor and different levels of machine involvement.
The accuracy of the tagger on the datasets when
trained on all available training material is given
in the following table, along with accuracy of a
unigram model (learned from the training set and
constrained by a tag dictionary for known words).
Unigram Model
Danish 91.62% 95.58%
Dutch 90.92% 93.57%
English 87.87% 93.25%
Swedish 84.91% 87.74%
Uspanteko 77.84% 79.39%
Sample selection. We consider three sample
selection methods: sequential, random, and
uncertainty. Sequential selection is important
to consider as it is the default in documentary
projects. It is sub-optimal for corpora with con-
tiguous sub-domains, since it necessitates working
through many similar examples before getting to
possibly more informative examples. Random se-
lection is a model-free method that avoids the sub-
domain trap by sampling freely from the entire
corpus. It generally works better than sequential
selection and provides a strong baseline against
which to compare learner-guided selection.
Uncertainty selection (Cohn et al, 1995) iden-
tifies examples the model is least confident about.
We measure uncertainty as the entropy of the la-
bel distribution predicted by the maximum en-
tropy model for each example. Uncertainty for
a clause is calculated as the average entropy per
morpheme; clauses with the highest average en-
tropy are selected for labeling.
A recent development in active learning is cost-
sensitive selection that is guided not only by the
learner but also by the expected cost of labeling an
example based on its likely complexity and/or the
reliability of the annotator. Settles et al (2008)
provide empirical validation for cost-related in-
tuitions; for example, that cost of annotation is
static neither per example nor per annotator. Also,
they show that taking annotation cost into account
can improve active learning effectiveness, but that
learning to predict annotation cost is not yet well-
understood. A cost-sensitive Return on Investment
heuristic is developed in Haertel et al (2008) and
tested in a simulated POS-tagging context. Our
experiments do not employ cost-sensitive selec-
tion, but our results?from live (non-simulated)
active learning experiments of real-world scale?
empirically support the need to consider cost-
sensitive selection if better cost reductions are to
be achieved.
Annotation setup. We compare results from
two annotators with different levels of exposure to
Uspanteko. Both are documentary linguists with
extensive field experience. Our expert annota-
tor is a native speaker of K?ichee?, a closely re-
lated Mayan language, and has worked extensively
on Uspanteko. Our non-expert annotator had no
prior experience with Uspanteko and only limited
exposure to Mayan languages. During annotation,
he used an Uspanteko-Spanish dictionary.
For each selection method, we consider two
conditions for providing classifier labels: a do-
suggest (ds) condition where the labels predicted
by the machine learner are shown to the annotator,
and a no-suggest (ns) condition where the annota-
tor does not see the predictions. With ds, the anno-
tator is shown the most probable label and a ranked
list of all labels assigned a probability greater than
half that of the best label. For ns, the annotator
sees a frequency-ranked list of labels previously
seen in training data for the given morpheme.
Annotators improve as they see more examples.
To minimize the impact of this learning process,
annotation is done in rounds. Each round con-
sists of sixty clauses?six batches of ten each for
the six experimental cases. The annotator is free
to break between batches. Following annotation,
the newly-labeled clauses are added to the train-
ing data, and a new model is trained and evaluated.
Both annotators completed fifty-six rounds of an-
notation. See Palmer et al (2009) for more details
on the annotation setup.
298
Measuring annotation cost. Active learning
studies usually simulate annotation and use a unit
cost assumption that each word, sentence, con-
stituent, document, etc. takes the same time to an-
notate. This is often the only option since corpora
typically do not retain annotation time, but it is
likely to exaggerate the annotation cost reductions
achieved. This is exacerbated with active learn-
ing: the informative examples it seeks to find are
typically harder to annotate (Hachey et al, 2005).
Baldridge and Osborne (2008) correlate a unit
cost in terms of discriminants (decisions made
by annotators about valid parses) to annotation
time. This is a better approximation than unit costs
where such a relationship cannot be established.
However, it is based on a static measurement of
annotation time, and clearly the time taken to an-
notate an example is not a function of the example
alone. Annotation time is actually dynamic in that
it is dependent on how many and what kinds of
examples have already been annotated. An ?infor-
mative? example is likely to take longer to anno-
tate if selected early than it would after the anno-
tator has seen many other examples.
Thus, it is important to measure annotation time
embedded in the context of a particular annota-
tion experiment with the sample selection/labeling
strategies of interest. In our annotation experi-
ments, we measure the exact time taken to anno-
tate each example by each annotator and use this
as the cost metric, inspired by Ngai and Yarowsky
(2000). In the simulation studies, as we are un-
able to measure time, we measure cost by sen-
tence/clause and word/morpheme.
Learning curve comparison. We are interested
in comparative evaluation of many different exper-
imental settings, across which we vary selection
methods, use of label suggestions, and annotators.
To achieve this, it is useful to have a summary
value for comparing the results from two individ-
ual experiments. One such measure is the percent-
age error reduction (PER), measured over a dis-
crete set of points on the first 20% of the points on
the learning curve (Melville and Mooney, 2004).
2
We use a new related measure, which we call
the overall percentage error reduction (OPER),
that uses the entire area under the curves given by
2
This is justified in standard conditions, sampling from a
finite corpus: active learning runs out of interesting examples
after considering a fraction of the data, so the curve is artifi-
cially pulled down by the remaining, boring examples.
fitted nonlinear regression models rather than av-
eraging over a subset of data points. Specifically,
we fit a modified Michaelis-Menton model:
f(cost, (K,V
m
, A)) =
V
m
(A + cost)
K + cost
The (original) parameters V
m
and K respectively
correspond to the horizontal asymptote and the
cost where accuracy is halfway between 0 and V
m
.
The additional parameter A allows for a better fit
to our data by allowing for less sharp elbows and
letting cost be zero. Model parameters were de-
termined with nls in R (Ritz and Streibig, 2008).
With the fitted regression models, it is straight-
forward to calculate the area under the curve be-
tween a start cost c
i
and end cost c
j
by taking the
integral from c
i
to c
j
. The overall accuracy for
the experiment is given by dividing that area by
100 ? (c
j
? c
i
). Call this the overall curve accu-
racy (OCA). Then, for experiment A compared to
experiment B, OPER(A,B) =
OCA
A
?OCA
B
100?OCA
B
. For
the simulation experiments we calculate OPER for
only the first 20% of cost units, like Melville and
Mooney. For the annotation experiments, we cal-
culate it for the minimum amount of time spent on
any of the experiments (which ended up using less
than 10% of all available morphemes).
4 Simulation experiments
We verify that our tagger and dataset behave as
expected in standard active learning experiments
by running simulations on the Uspanteko data set,
and on POS-tagging for Danish, Dutch, English,
and Swedish. Here, we vary only the selection
method: sequential, random, or uncertainty.
For each language, we randomly select a seed
set of 10 labeled sentences. The number of exam-
ples selected to be labeled in each round begins
at 10 and doubles after every 20 rounds. For rand
and unc, each batch of examples is selected from a
pool (size of 1000) that is itself randomly selected
from the entire set of remaining unlabeled exam-
ples. rand and unc experiments for each language
are replicated 5 times; splines and regressions are
computed over all runs for each condition.
Figure 1 gives learning curves for the Uspan-
teko simulations, with cost measured in terms of
(a) clauses and (b) morphemes. Both graphs show
the usual behavior found in active learning exper-
iments. rand and unc both rise more quickly than
seq, and unc is well above rand. The relation-
ship between the methods is the same regardless
299
0 2000 4000 6000
50
55
60
65
70
75
80
Number of clauses selected
Accu
racy
 on a
ll tok
ens
UncertaintyRandomSequential
0 10000 20000 30000 40000
50
55
60
65
70
75
80
Number of morphemes selected
Accu
racy
 on a
ll tok
ens
UncertaintyRandomSequential
(a) (b)
Figure 1: Learning curves for simulations; (a) clause cost and (b) morphemes cost. The dashed vertical
lines indicate (a) #clauses=1485 and (b) #morphemes=8695 (to compare OPER values).
rand
seq
unc
seq
unc
rand
Uspanteko-Clauses 5.86 13.27 7.86
Uspanteko-Morphs 7.47 11.68 4.55
Table 2: OPER values for Uspanteko simulations,
comparing clause and morpheme cost.
A
B
indi-
cates we compute OPER(A,B).
of the cost metric, but the relative differences in
cost-savings are not, which we see when we look
at OPER values.
The dashed vertical lines in the two graphs cor-
respond to the 20% mark used to calculate OPER
values, which are given in Table 2. Most impor-
tantly, note the much larger OPER for unc over
rand with clause cost (7.86 vs 4.55). Also note
that OPER(rand,seq) is lower with clause cost?
this indicates that the beginning portions of the
corpus contain longer sentences with more mor-
phemes, an accident which overstates how well
seq would likely work in general.
Since rand is unbiased with respect to pick-
ing longer sentences, the large increase of
OPER(unc,rand) from 4.55 to 7.86 is a clear in-
dication of the well-known?but not always at-
tended to?tendency of uncertainty sampling to
select longer sentences. Consequently, one should
at least use sub-sentence cost in order not to over-
state the gains from active learning. The annota-
tion experiments in the next section take this word
rand
seq
unc
seq
unc
rand
Danish 4.58 6.95 2.48
Dutch 21.95 23.68 2.20
English 6.55 8.00 1.56
Swedish 9.56 9.29 -0.30
Uspanteko 7.47 11.68 4.55
Table 3: OPER values for morpheme cost for sim-
ulations.
A
B
indicates we compute OPER(A,B).
of caution one step further: even sub-sentence cost
(morpheme cost, in our setting) can overestimate
gains since the morphemes selected are actually
harder to annotate and thus take more time.
Table 3 gives overall percentage error reduc-
tions (OPER) between different selection methods
based on word/morpheme cost, for each language.
For all languages, rand and unc are better than
seq. Only in the case of Swedish is there no ben-
efit from unc over rand. For Dutch, the large
gains over seq for both rand and unc accurately
reflect the heterogeneity of the underlying Alpino
corpus.
3
Most importantly, for Uspanteko, there
are large reductions from unc to rand to seq, mir-
roring the clear trends in Figure 1b.
These simulations have an unrealistic ?perfect?
annotator, the corpus. Next, we discuss results
with real annotators?who may be fallible or may
(reasonably) beg to differ with the corpus analysis.
3
http://www.let.rug.nl/vannoord/trees/
300
0 1000 2000 3000 4000
20
30
40
50
60
70
Morphemes annotated
Accu
racy
 on a
ll tok
ens
Non?Expert, No Suggest, SequentialNon?Expert, Suggest, RandomExpert, No Suggest, SequentialExpert, No Suggest, Uncertainty
0 5000 10000 15000 20000 25000
20
30
40
50
60
70
Cumulative annotation time
Accu
racy
 on a
ll tok
ens
Non?Expert, No Suggest, SequentialNon?Expert, Suggest, RandomExpert, No Suggest, SequentialExpert, No Suggest, Uncertainty
(a) (b)
Figure 2: A sample of the learning curves with (a) morpheme cost and (b) time cost. Morpheme cost
ranks strategies for a given annotator similarly to time cost, but it gives dramatically different results
from time cost when used to compare different annotators.
5 Annotation experiments
With two annotators (expert, non-expert), three
selection methods (seq, rand, unc), and two ma-
chine labeling settings (ns, ds), we obtain 12 dif-
ferent experiments. Each experiment measures ac-
curacy in terms of all words and unknown words
and cost in terms of clauses, morphemes and time;
this produces six views on every experiment. In
this paper we focus on one view: accuracy over all
words with time-based evaluation of cost.
As with the simulations, clause cost in the an-
notation experiments overestimates the cost reduc-
tions. For morpheme cost, the annotation experi-
ments show that (a) it also overstates cost reduc-
tions compared to time, and (b) it can mis-state
relative effectiveness when comparing annotators.
The big picture. Figure 2 shows curves for four
experiments: seq-ns for both annotators
4
and the
most effective overall condition for each annota-
tor. Figure 2a uses morpheme cost evaluation; on
that metric, both annotators appear to be about
equally effective with seq-ns and much more ef-
fective with machine involvement (unc or ds) than
without. Additionally, the non-expert?s rand-ds
appears to beat the expert?s unc-ns. However, the
time cost evaluation in Figure 2b tells a dramat-
ically different story. Each annotator?s machine-
4
Recall that sequential annotation is the default mode for
producing IGT, so this strategy is of particular interest.
involved experiment is much better than their seq-
ns, but now the expert?s best is clearly better than
the non-expert?s. We see this as clear evidence for
the need for cost-sensitive learning over vanilla ac-
tive learning (as we do here).
5
The non-expert with rand-ds caught up to and
surpassed the unaided expert in about six hours
total annotation time, and he caught up to her
unc-ns curve after 35 hours. This is encourag-
ing since often language documentation projects
have participants with a wide range of expertise
levels, and these results suggest that assistance
from machine learning, if done properly, may in-
crease the effectiveness of participants with less
language-specific expertise. We are also encour-
aged, with respect to the effectiveness of active
learning, that the expert?s best performance is ob-
tained with uncertainty-based selection.
Within annotator comparisons. Figure 3
shows both actual measurements and the fitted
nonlinear regression curves used to compute
OPER. Figure 3a, the expert without suggestions,
exhibits typical active learning behavior similar to
that seen in the simulation experiments. Figure 3b,
5
It is also clear to see that, unsurprisingly, the expert spent
much less time to complete the 56 rounds than the non-expert.
In general, the expert annotator was much quicker, particu-
larly in early rounds, averaging 4.1 seconds per morpheme
annotated against the non-expert?s 8.0 second average. See
Palmer et al (2009) for more details.
301
ll
ll
lll
llll
lllllllll
lllllllllll
lllllllllllllllll
llllll
0 5000 10000 15000 20000 25000 30000
30
40
50
60
70
Cumulative annotation time
Accu
racy
 on a
ll tok
ens
l Expert, No Suggest, UncertaintyExpert, No Suggest, RandomExpert, No Suggest, Sequential
l
l
l
l
llll
ll
lll
ll llllll
l ll
lllllllllll
lllllllllllllll
lllllll
0 5000 10000 15000 20000 25000 30000
30
40
50
60
70
Cumulative annotation time
Accu
racy
 on a
ll tok
ens
l Non?expert, Suggest, UncertaintyNon?expert, Suggest, RandomNon?expert, Suggest, SequentialNon?expert, No Suggest, Sequential
(a) (b)
Figure 3: Sample measurements and fitted nonlinear regression curves for (a) the expert and (b) the
non-expert. Note that the scale is consistent for comparability. The dashed vertical lines indicate 12,500
seconds (about 35 hours), which is the upper limit used in computing OPER values for Table 4.
the non-expert with suggestions, shows that in the
ds conditions the non-expert was less effective
with unc. This is not unexpected: uncertainty
selects harder examples that will either take
longer to annotate or are easier to get wrong,
especially if the annotator trusts the classifier and
especially on examples the classifier is uncertain
about. Nonetheless, in all ds cases, the non-expert
performs better than with seq-ns.
OPER. Table 4 provides OPER values from
time 0 to 12,500 seconds (about 35 hours), the
minimum amount of annotation time logged in any
one of the twelve experiments.
6
The table mixes
three types of comparison: (1) the boxed values
on the diagonal give OPER for the expert versus
the non-expert given the same selection and sug-
gestion conditions; (2) the upper (right) triangle
gives OPER for the expert versus herself for dif-
ferent conditions; and (3) the lower (left) trian-
gle is the non-expert versus himself. For exam-
ple: (1) the expert obtained an 11.52 OPER versus
the non-expert when both used rand-ns; (2) the
expert obtained a 10.52 OPER by using rand-ds
rather than seq-ns; and (3) the non-expert obtained
a 5.93 OPER over rand-ns by using rand-ds.
A number of patterns emerge. Quite unsurpris-
6
Stopping at 12,500 seconds ensures a fair comparison,
for example, between the expert and the non-expert because
it requires no extrapolation of the expert?s performance.
X
X
X
X
X
X
non-exp
exp
seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
seq-ns 15.99 8.85 14.17 6.34 10.52 14.50
rand-ns 13.46 11.52 5.83 -2.76 1.83 6.20
unc-ns 19.20 6.63 10.76 -9.12 -4.25 0.39
seq-ds 10.24 -3.72 -11.09 12.34 4.46 8.72
rand-ds 18.59 5.93 -0.76 9.30 7.67 4.45
unc-ds 11.19 -2.62 -9.91 1.06 -9.09 19.13
Table 4: Overall percentage error reduction
(OPER) comparisons, with timing cost. See ex-
planation of table in the OPER subsection.
ingly, the values on the diagonal show that the ex-
pert is more effective than the non-expert in all
conditions. Also, every other condition is more ef-
fective than seq-ns for both annotators (first row
for the expert, first column for the non-expert).
unc-ns and rand-ds are particularly effective for
the non-expert, giving OPERs of 19.20 and 18.59
over seq-ns, respectively. These reductions, big-
ger than the expert?s reductions of 14.17 and 10.52
for the same conditions, considerably reduce the
large gap in seq-ns effectiveness between the two
annotators (see Figure 2b).
The expert actually gains very little from ds for
both rand and unc: adding suggestions gave OP-
ERs of just 1.83 and .39, respectively. In con-
trast, the non-expert obtains an improvement of
5.93 OPER when suggestions are used with rand,
302
but performs worse when used with unc (-9.91
OPER). Even more striking: the non-expert?s
unc-ds is worse than rand-ns (-2.62 OPER), a
completely model-free setting. These variations
demonstrate the importance of modeling annotator
fallibility and sensitivity to cost, as well as char-
acteristics of the annotation task itself, if learner-
guided selection and suggestion are to be used
(Donmez and Carbonell, 2008; Arora et al, 2009).
Annotator accuracy. Another factor which
must be considered when annotation is done by
human annotators (rather than being simulated)
is the accuracy of the humans? labels. Table 5
shows the overall accuracy of the annotators? la-
bels for each condition (after 56 rounds) as mea-
sured against the original OKMA annotations.
Unsurprisingly, unc selection picks examples that
are more difficult to annotate: accuracy for both
annotators suffers in both unc-ns and unc-ds.
It may seem surprising that the non-expert?s ac-
curacies are generally higher than the expert?s.
The main reason for this is that the non-expert
took nearly twice as long to annotate his examples,
so each one was done with more care. However,
this difference also highlights challenges that arise
when we bring active learning into non-simulated
annotation contexts. The typical assumption is
that gold standard labeled data represents a true,
fixed target, against which annotator or machine-
predicted labels should be measured. In language
documentation, though, the analysis of the lan-
guage is continually evolving, and analysis and
annotation each inform the other. In fact, the ex-
pert recognized (in the morphological segmenta-
tion) several linguistic phenomena for which the
analysis has changed since the original OKMA an-
notations were done. As she changed her analy-
ses, her labels diverged from those of the original
corpus?another reason for her ?lower? accuracy.
This is to say that the ground truth of the current
OKMA annotations we had to work with can be
viewed as one (valid) stage in the iterative reanal-
ysis process that language documentation is.
Error analysis. Preliminary analysis of ?errors?
made by the annotators supports the idea that
the results seen in Table 5 are heavily influenced
by changes in the expert?s analysis of the lan-
guage. Some duplicate clause annotation oc-
curred for each annotator, because each of the
twelve annotator-selection-suggestion conditions
expert non-expert
seq-ns 73.17% 75.09%
rand-ns 69.90% 74.37%
unc-ns 61.23% 60.04%
seq-ds 67.48% 73.13%
rand-ds 68.34% 73.03%
unc-ds 59.79% 60.27%
Table 5: Overall accuracy of annotators? labels,
measured against OKMA annotations.
drew from the same global set of unlabeled ex-
amples. This duplication allows us to measure
the consistency of each annotator on labeling such
duplicate clauses. Table 6 shows the percentage
of morphemes labeled consistently by each anno-
tator. Numbers for the expert appear in the top
(right) triangle, and for the non-expert in the bot-
tom (left) triangle. Overall intra-annotator consis-
tency is much higher for the expert (88.38%) than
for the non-expert (81.64%), suggesting that the
expert maintained a more consistent mental model
of the language, but one which disagrees in some
areas with the original annotations.
Another key error source comes from differ-
ences in use of one individual label: the annota-
tors could assign a label that does not appear in
the original corpus. This is yet another issue that
does not?in fact, cannot?arise in simulated ac-
tive learning. The label ESP was introduced for la-
beling Spanish loans or insertions (such as the dis-
course marker entonces) which do not have a clear
function in Uspanteko grammar. Such tokens are
inconsistently labeled in the original corpus, usu-
ally with catch-all categories like particle or ad-
verb. The annotators felt that the best analysis was
to mark the tokens as of Spanish origin. The expert
annotator used the ESP label for 2086 of 24129 to-
kens (8.65%) versus 221 of 22819 tokens (0.97%)
for the non-expert. Any such token labeled with
ESP is scored as incorrect when compared to the
OKMA standard, so this label alone accounts for
more than 7% of the expert annotator?s total error.
Finally, Table 7 presents inter-annotator agree-
ment measured as percent agreement on mor-
phemes in clauses labeled by both annotators.
Note that in general agreement seems to be low-
est for clauses duplicated in unc conditions, sup-
porting the expected result that uncertainty-based
selection does indeed select clauses that are more
difficult for human annotators to label.
303
PP
P
P
P
P
non
exp
seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
seq-ns ? 95.00% (41) 87.10% (56) 92.39% (60) 91.02% (28) 88.83% (51)
rand-ns 90.11% (49) ? 90.91% (57) 87.57% (35) 90.94% (50) 89.53% (57)
unc-ns 80.80% (44) 81.68% (54) ? 81.35% (41) 89.10% (40) 87.82% (332)
seq-ds 90.00% (54) 87.94% (44) 77.97% (48) ? 86.13% (42) 82.14% (42)
rand-ds 90.15% (52) 86.64% (45) 79.46% (62) 81.43% (44) ? 87.06% (49)
unc-ds 84.15% (47) 78.55% (52) 77.68% (328) 78.81% (35) 77.95% (60) ?
Table 6: Annotation consistency, expert and non-expert, (number of duplicate clauses, of 560 possible)
P
P
P
P
P
P
non
exp
seq-ns rand-ns unc-ns seq-ds rand-ds unc-ds
seq-ns 69.91% (523) 70.82% (42) 62.42% (48) 72.35% (54) 74.25% (28) 67.82% (47)
rand-ns 71.32% (48) 83.94% (39) 66.56% (47) 66.15% (43) 73.75% (42) 67.55% (52)
unc-ns 66.31% (48) 67.87% (53) 62.31% (301) 58.87% (51) 73.31% (40) 61.10% (298)
seq-ds 73.35% (60) 75.56% (34) 56.39% (37) 60.02% (540) 66.00% (44) 61.01% (36)
rand-ds 68.67% (50) 76.40% (63) 66.67% (58) 65.88% (47) 76.33% (42) 66.99% (64)
unc-ds 65.41% (50) 67.98% (55) 60.43% (263) 58.13% (38) 70.74% (57) 60.40% (275)
Table 7: IAA: expert v. non-expert, percentage of morphemes in agreement, (number of duplicate
clauses, of 560 possible)
6 Conclusion
Through actual annotation experiments that con-
trol for several factors, we have evaluated the po-
tential of incorporating active learning and label
suggestions to speed up morpheme glossing in a
realistic language documentation context. Some
configurations of learner-guided example selec-
tion and machine label suggestions perform far
better than the standard strategy of sequential se-
lection without suggestions. However, the effec-
tiveness of any given strategy depends on annota-
tor expertise. The impact of differences between
annotators directly bears on the point made by
Donmez and Carbonell (2008) that if cost reduc-
tions are to be reliably obtained with active learn-
ing techniques, annotators? fallibility, unreliabil-
ity, and sensitivity to cost must be modeled.
Our results suggest some possible prescriptions
for tuning techniques according to annotator ex-
pertise. However, even if we can estimate a rela-
tive level of expertise, following such broad pre-
scriptions is unlikely to be more robust than an ap-
proach which adapts selection and suggestion to
the individual annotator, perhaps working within
an annotation group. Indeed, it seems that dealing
with variation in annotators/oracles may be more
important than devising better selection strategies.
The difference in performance due to expertise
suggests that using multiple annotators to check
relative annotation rate and accuracy of different
annotators could be a key ingredient in any actu-
ally deployed active learning system. This could
provide for better modeling of individual anno-
tators as part of an annotation group they can be
compared against, allowing the system, for exam-
ple, to throttle active selection if an annotator ap-
pears to be too slow or inaccurate.
Another major issue we highlight is the uncer-
tainty around the question of whether active learn-
ing works in practical applications. Respondents
to the survey of Tomanek and Olsson (2009) in-
dicated that this uncertainty?will active learn-
ing work? what methods or techniques will work
best??is one of the reasons active learning is not
widely used in actual annotation. In addition, cre-
ating the necessary software infrastructure to build
an active learning enabled annotation system?
a system which must interface robustly between
data, annotator, and machine classifier, yet still
be easy to use?is a substantial hurdle. It seems
unlikely that there will be much uptake until a)
consistent, large cost reductions can be shown in
actual annotation studies, and b) appropriate, tun-
able, widely-available software exists.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documenta-
tion of Languages using Machine Learning and
Active Learning.? Thanks to Eric Campbell, Ka-
trin Erk, Michel Jacobson, Taesun Moon, Telma
Kaan Pixabaj, and Elias Ponvert.
304
References
Shilpa Arora, Eric Nyberg, and Carolyn P. Ros?e. 2009.
Estimating annotation cost for active learning in a
multi-annotator environment. In Proceedings of the
NAACL HLT Workshop on Active Learning for Nat-
ural Language Processing, pages 18?26, Boulder,
CO.
Jason Baldridge and Miles Osborne. 2008. Active
learning and logarithmic opinion pools for HPSG
parse selection. Natural Language Engineering,
14(2):199?222.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
Shared Task on Multilingual Dependency Parsing.
In Proceedings of the Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
pages 149?164, New York City, June. Association
for Computational Linguistics.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1995. Active learning with statistical mod-
els. In G. Tesauro, D. Touretzky, and T. Leen, ed-
itors, Advances in Neural Information Processing
Systems, volume 7, pages 705?712. The MIT Press.
David Crystal. 2000. Language Death. Cambridge
University Press, Cambridge.
James R. Curran and Stephen Clark. 2003. Investigat-
ing GIS and smoothing for maximum entropy tag-
gers. In Proceedings of the 10th Conference of the
European Association for Computational Linguis-
tics, pages 91?98.
Pinar Donmez and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of
CIKM08, Napa Valley, CA.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Confer-
ence on Computational Natural Language Learning,
Ann Arbor, MI.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger,
and James L. Carroll. 2008. Return on invest-
ment for active learning. In Proceedings of the NIPS
Workshop on Cost-Sensitive Learning.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional linguistics, 19:313?330.
Prem Melville and Raymond J. Mooney. 2004. Di-
verse ensembles for active learning. In Proceed-
ings of the 21st International Conference on Ma-
chine Learning, pages 584?591, Banff, Canada.
Grace Ngai and David Yarowsky. 2000. Rule writ-
ing or annotation: cost-efficient resource usage for
base noun phrase chunking. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117?125, Hong Kong.
Alexis Palmer, Taesun Moon, and Jason Baldridge.
2009. Evaluating automation strategies in language
documentation. In Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 36?44, Boulder, CO.
Telma Can Pixabaj, Miguel Angel Vicente M?endez,
Mar??a Vicente M?endez, and Oswaldo Ajcot Dami?an.
2007. Text Collections in Four Mayan Languages.
Archived in The Archive of the Indigenous Lan-
guages of Latin America.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania, Philadelphia, PA.
Christian Ritz and Jens Carl Streibig. 2008. Nonlinear
Regression with R. Springer.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS Workshop on Cost-Sensitive
Learning.
Burr Settles. 2009. Active learning literature survey.
Technical Report Computer Sciences Technical Re-
port 1648, University of Wisconsin-Madison.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP 2008,
pages 254?263.
Katrin Tomanek and Fredrik Olsson. 2009. A Web
Survey on the Use of Active learning to support an-
notation of text data. In Proceedings of the NAACL
HLT Workshop on Active Learning for Natural Lan-
guage Processing, pages 45?48, Boulder, CO.
Sudheendra Vijayanarasimhan and Kristen Grauman.
2008. Multi-level active prediction of useful im-
age annotations for recognition. In Proceedings of
NIPS08, Vancouver, Canada.
305
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 668?677,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Unsupervised morphological segmentation and clustering with document
boundaries
Taesun Moon, Katrin Erk, and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
Abstract
Many approaches to unsupervised mor-
phology acquisition incorporate the fre-
quency of character sequences with re-
spect to each other to identify word stems
and affixes. This typically involves heuris-
tic search procedures and calibrating mul-
tiple arbitrary thresholds. We present a
simple approach that uses no thresholds
other than those involved in standard ap-
plication of ?2 significance testing. A
key part of our approach is using docu-
ment boundaries to constrain generation of
candidate stems and affixes and clustering
morphological variants of a given word
stem. We evaluate our model on English
and the Mayan language Uspanteko; it
compares favorably to two benchmark sys-
tems which use considerably more com-
plex strategies and rely more on experi-
mentally chosen threshold values.
1 Introduction
Unsupervised morphology acquisition attempts to
learn from raw corpora one or more of the follow-
ing about the written morphology of a language:
(1) the segmentation of the set of word types in a
corpus (Creutz and Lagus, 2007), (2) the cluster-
ing of word types in a corpus based on some notion
of morphological relatedness (Schone and Juraf-
sky, 2000), (3) the generation of out-of-vocabulary
items which are morphologically related to other
word types in the corpus (Yarowsky et al, 2001).
We take a novel approach to segmenting words
and clustering morphologically related words.
The approach uses no parameters that need to
be tuned on data. The two main ideas of the
approach are (a) the filtering of affixes by sig-
nificant co-occurrence, and (b) the integration of
knowledge of document boundaries when gener-
ating candidate stems and affixes and when clus-
tering morphologically related words. The main
application that we envision for our approach is
to produce interlinearized glossed texts for under-
resourced/endangered languages (Palmer et al,
2009). Thus, we strive to eliminate hand-tuned
parameters to enable documentary linguists to use
our model as a preprocessing step for their manual
analysis of stems and affixes. To require a docu-
mentary linguist?who is likely to have little to no
knowledge of NLP methods?to tune parameters is
unfeasible. Additionally, data-driven exploration
of parameter settings is unlikely to be reliable in
language documentation since datasets typically
are quite small. To be relevant in this context, a
model needs to produce useful results out of the
box.
Constraining learning by using document
boundaries has been used quite effectively in un-
supervised word sense disambiguation (Yarowsky,
1995). Many applications in information retrieval
are built on the statistical correlation between doc-
uments and terms. However, we are unaware of
cases where knowledge of document boundaries
has been used for unsupervised learning for mor-
phology. The intuition behind our approach is very
simple: if two words in a single document are
very similar in terms of orthography, then the two
words are likely to be related morphologically. We
measure how integrating these assumptions into
our model at different stages affects performance.
We define a simple pipeline model. After gen-
erating candidate stems and affixes (possibly con-
strained by document boundaries), a ?2 test based
on global corpus counts filters out unlikely affixes.
Mutually consistent affix pairs are then clustered
to form affix groups. These in turn are used to
build morphologically related word clusters, pos-
sibly constrained by evidence from co-occurence
of word forms in documents. Following Schone
and Jurafsky (2000), clusters are evaluated for
668
whether they capture inflectional paradigms using
CELEX (Baayen et al, 1993).
We are unaware of other work on morphology
using ?2 tests despite its wide application across
many disciplines.1 This may be due to the large
degree of noise found in the candidate affix sets
induced through other candidate generation meth-
ods. The ?2 test has two standard thresholds?a
significance threshold and a lower bound on ob-
served counts. These are the only manually set
parameters we require?and we in fact use the
widely accepted standard values for these thresh-
olds without varying them in our experiments.
This is a significant improvement over other ap-
proaches that typically require a number of arbi-
trary thresholds and parameters yet provide little
intuitive justification for them. (We give examples
of these in ?3.)
We evaluate our approach on two languages,
English and Uspanteko, and compare its per-
formance to two benchmark systems, Morfessor
(Creutz and Lagus, 2007) and Linguistica (Gold-
smith, 2001). English is commonly used in other
studies and permits the use of CELEX as a gold
standard for evaluation. Uspanteko is an endan-
gered Mayan language for which we have a set of
interlinearized glossed texts (IGT) (Pixabaj et al,
2007; Palmer et al, 2009). IGT provides word-
by-word morpheme segmenation, which we use
to create a synthetic gold standard. In addition
to evaluation against this standard, Telma Kaan
Pixabaj?a Mayan linguist who helped create the
annotated corpus?reviewed by hand 100 word
clusters produced by our system, Morfessor and
Linguistica. Note that because English is suffixal
and Uspanteko is both prefixal and suffixal, we use
a slightly modified model for Uspanteko.
The approach introduced in this paper compares
favorably to Linguistica and Morfessor, two mod-
els that employ much more complex strategies and
rely on experimentally-tuned language/corpus-
specific parameters. In our evaluation, document
boundary awareness greatly benefits precision for
small datasets, blocking acquisition of spurious af-
fixes. For large datasets, global candidate genera-
tion outperforms document-aware candidate gen-
eration at the task of filtering out spurious stems,
but document-aware clustering improves preci-
sion. These findings are promising for the applica-
tion of this approach to under-resourced languages
1Monson (2004) suggests, but does not actually use, ?2.
like Uspanteko.
2 Unsupervised morphology acquisition
Unsupervised morphology acquisition aims to
model one or more of three properties of writ-
ten morphology: segmentation, clustering around
a common stem, and generation of new word
forms with productive affixes. Intuitively, there are
straightforward, but non-trivial, challenges that
arise when evaluating a model. One large chal-
lenge is distinguishing derivational from inflec-
tional morphology. Most approaches deal with to-
kens without considering context. Since inflec-
tional morphology is virtually always driven by
syntax and word context, such approaches are un-
able to learn only inflectional morphology or only
derivational morphology. Even approaches which
take context into consideration (Schone and Juraf-
sky, 2000; Baroni et al, 2002; Freitag, 2005) can-
not learn specifically for one or the other.
In addition, the evaluation of both segmentation
and clustering involves arbitrary judgment calls.
Concerning segmentation, should altimeter and
altitude be one morpheme or two? (The sam-
ple English gold standard for MorphoChallenge
2009 provides alti+meter but altitude.) Similar is-
sues arise when evaluating clusters of related word
forms if inflection and derivation are not distin-
guished. Does atheism belong to the same cluster
as theism? Where is the frequency cutoff point be-
tween a productive derivational morpheme and an
unproductive one? Yet, many studies have eval-
uated their segmentations and clusters by going
over their results word by word, cluster by cluster
and judging by sight whether some segmentation
or clustering is good (e.g., Goldsmith (2001)).
Like Schone and Jurafsky (2001), we build clus-
ters that will have both inflectionally and deriva-
tionally related stems and evaluate them with re-
spect to a gold standard of only inflectionally re-
lated stems.
3 Related work
There is a diverse body of existing work on unsu-
pervised morphology acquisition. We summarize
previous work, emphasizing some of its more ar-
bitrary and ad hoc aspects.
Letter successor variety. Letter successor va-
riety (LSV) models (Hafer and Weiss, 1974;
Gaussier, 1999; Bernhard, 2005; Bordag, 2005;
669
Keshava and Pitler, 2005; Hammarstro?m, 2006;
Dasgupta and Ng, 2007; Demberg, 2007) use the
hypothesis that there is less certainty when pre-
dicting the next character at morpheme bound-
aries. LSV has several issues that require fine pa-
rameter tuning. For example, Hafer and Weiss
(1974) counts how many types of characters ap-
pear after some initial string (the successor count)
and how many types of characters appear before
some final string (the predecessor count). A suc-
cessful criterion for segmenting a word was if the
predecessor count for the second part was greater
than 17 and the successor count for the first part
was greater than 5. Other studies have similar data
specific parameters and restrictions.
MDL and Bayesian models. Minimum descrip-
tion length (MDL) models (Goldsmith, 2001;
Creutz and Lagus, 2002; Creutz and Lagus, 2004;
Goldsmith, 2006; Creutz and Lagus, 2007) try to
segment words by maximizing the probability of
a training corpus subject to a penalty based on
the size of hypothesized morpheme lexicons they
build on the basis of the segmentations. While the-
oretically elegant, a pure implementation on real
data results in descriptions that do not reflect ac-
tual morphology. Creutz and Lagus (2005) re-
port that, ?frequent word forms remain unsplit,
whereas rare word forms are excessively split.? In
the end, every MDL approach uses probabilisti-
cally motivated refinements that restrict the ten-
dency of raw MDL to generate descriptions that
do not fit linguistic notions of morphology. De-
spite the sophistication of the models in this group,
there are many parameters that need to be set, and
heuristic search procedures are crucial for their
success (Goldwater, 2007). Snover et al (2002)
present a Bayesian model that uses a prior distribu-
tion to refine disjoint clusters of morphologically
related words. It disposes with parameter setting
by selecting the highest ranking hypothesis.
Context aware approaches. A word?s mor-
phology is strongly influenced by its syntactic and
semantic context. Schone and Jurafsky (2000) at-
tempts to cluster morphologically related words
starting with an unrefined trie search (but with a
parameter of minimum possible stem length and
an upper bound on potential affix candidates) that
is constrained by semantic similarity in a word
context vector space. Schone and Jurafsky (2001)
builds on this approach, but adds more ad hoc
parameters to handle circumfixation. Baroni et
al. (2002) takes a similar approach but uses edit
distance to cluster words that are similar but do
not necessarily share a long, contiguous substring.
They remove noise by constraining cluster mem-
bership with mutual information derived semantic
similarity. Freitag (2005) uses a mutual informa-
tion derived measure to learn the syntactic simi-
larity between words and clusters them. Then he
derives finite state machines across words in dif-
ferent clusters and refines them through a graph
walk algorithm. This group is the only one to eval-
uate against CELEX (Schone and Jurafsky, 2000;
Schone and Jurafsky, 2001; Freitag, 2005).
Others. Some other models require input such
as POS tables and lexicons and use a wider range
of information about the corpus (Yarowsky and
Wicentowski, 2000; Yarowsky et al, 2001; Chan,
2006). Because of the knowledge dependence of
these models, they are able to properly induce
inflectional morphology, as opposed to the stud-
ies cited above. Snyder and Barzilay (2008) uses
a set of aligned phrases across related languages
to learn how to segment words with a Bayesian
model and is otherwise fully unsupervised.
4 Model2
Our goal is to generate conflation sets: sets of
word types that are related through either inflec-
tional or derivational morphology (Schone and Ju-
rafsky, 2000). Solving this task requires learning
how individual types are segmented (though the
segmentation itself is not evaluated). For present
purposes, we assume that the affixal pattern of the
language is known: whether it is prefixal, suffixal,
or both. To simplify presentation, we discuss a
model that captures suffixes only. Our approach is
a four stage process:
1. Candidate Generation: generate candidate
stems and affixes using an orthographically
defined data structure (a trie)
2. Candidate Filtering: filter candidate affixes
using the statistical significance for pairs of
affixes based on their co-occurence counts
with shared stems
3. Affix Clustering: cluster significant affix pairs
into affix groups
2The code implementing the model is available from
http://comp.ling.utexas.edu/earl
670
4. Word Clustering: form conflation sets based
on affix clusters
The first and last stages are particularly prone to
noise, which has necessitated many of the thresh-
olds and heuristics employed in previous work.
We hypothesize that naturally occuring document
boundaries provide a strong constraint that should
reduce this noise, and we test that hypothesis by
using it in those stages.
Our intuition comes from an observation by
Yarowsky (1995) regarding multiple tokens of
words in documents. He tabulates the applicabil-
ity of using document boundaries to disambiguate
word senses, which measures how often a given
word occurs more than twice in the same docu-
ment. For ten potentially ambiguous words, he
counts how often they occur more than once in
some document and finds that if the words do oc-
cur, they do so multiple times in 50.1% of these
documents, on average. His counts ignored mor-
phological variation, and it is likely the applica-
bility measure would have increased considerably:
if a content word is used more than once in some
text, it is likely to be repeated in different syntactic
contexts, requiring the word to be inflected or to be
derived for a different part-of-speech category. 3
For stage one, we build separate tries for each
document rather than a trie for the entire corpus.
This should reduce the chance that orthographi-
cally similar but morphologically unrelated word
pairs lead to bad candidates by reducing the search
space for words which share a stem to a local doc-
ument. For example, assuage and assume are both
likely to occur in a large corpus and suggest that
there is a stem assu with affixes -age and -me.
They are less likely to occur together in many dif-
ferent documents that form the corpus, whereas
assume, assumed, and assuming are. We refer to
this document constrained candidate generation as
CandGen-D, and to the unconstrained generation
(a single trie for all documents) as CandGen-G.
For stage four, documents are used to constrain
potential membership of words in clusters: all
pairs of words in a cluster must have occured to-
gether in some document. We refer to document-
constrained clustering as Clust-D and the uncon-
strained global clustering as Clust-G.
3For example, in just this one paragraph we have
{document,documents}, {measure, measures}, {occur, oc-
curs, occuring}, and {word, words}.
4.1 Candidate generation
Given a document or collection of documents, we
use tries (prefix trees) to identify potential stems
and affixes and collect statistics for co-occurrences
between affixes and between affixes and stems.
a
b c
d $
Figure 1
A trie G, like the example
on the right, can be iden-
tified with the set of all
words on paths from the
root to any leaf, in the case
of the example figure the
set G = {abd, ab$, ac}.
(We use $ to denote an
empty affix.) Given a trie
G over alphabet L, we de-
fine the set of trunks of G
as all paths from the root to a branching point:
Tr(G) = {w ? L+ |?a, b ? L, x
1
, x
2
? L
?
:
a 6= b ? wax
1
, wbx
2
? G}
Also, we define the set of branches of a trunk t ?
Tr(G) as the paths from its branching points to the
leaves:
Br(t,G) = {x ? L+ | tx ? G}
In our example, {a, ab} are the trunks, with
Br(a, G) = {bd, b$, c} and Br(ab, G) = {d, $}.
When we use a trie to induce stems and affixes,
all induced stems will be trunks, and all induced
affixes will be branches.
From a given trie, we induce a set of stem can-
didates and affix candidates. A simple criterion is
used: if a trunk is longer than all of its branches,
the trunk is a stem candidate and its branches are
affix candidates. So, the set of stem candidates for
a trie G, CStem(G), is the set of trunks t ? Tr(G)
such that |t| > |b| for all b ? Br(t,G).
Given a stem candidate s ? CStem(G), its set of
affix candidates CAff(s,G) is identical to its set of
branches. (To talk about the sets of stem and affix
candidates for a whole trie G or a set of tries, we
write CAff(G), StC(G), CAff, and CStem.) The
count of an affix candidate b ? CAff is the number
of stem candidates with which it occurs:
count(b) =
?
G
|{s ? CStem(G) | b ? CAff(s,G)}|
For Fig. 1, the set of stem candidates is {ab} (since
some branches of the trunk a are longer than the
671
trunk itself). The matching set of affix candidates
is CAff(ab, G) = {d, $}, each with a count of one.
An affix rule candidate is an unordered pair of
affix candidates {b
1
, b
2
}. It states that any stem
occurring with b
1
can also occur with b
2
. Affix
rules implement the assumption that all produc-
tive affixes will cooccur with other productive af-
fixes and that these will form a coherent group.
The rule candidates for a given stem candidate
s ? CStem(G) are:
CRule(s,G) =
{
{b
1
, b
2
} ? CAff(s,G) | b
1
6= b
2
}
For example, the single stem candidate ab in
Fig. 1 has one rule candidate, {d, $}. We also use
CRule(G) for the rule candidates of a trie G across
all stems, and CRule for the union of rule candi-
dates in a set of tries.
The count of a rule candidate r={b
1
, b
2
} in a
trie is the number of stem candidates it appears
with:
count(r) =
?
G
|{s ? CStem(G) | r ? CRule(s,G)}|
We also use CAff(s) for the set of affix candidates
of stem s across several tries, and CRule(s) for the
set of rule candidates of a stem s across several
tries.
Document-specific versus global candidate gen-
eration. CandGen-D defines separate tries for
every document in the corpus and induces stem,
affix and rule candidates for each document.
CandGen-G instead induces these candidates for
a global trie over all the words in the corpus.
From the perspective of the formalism laid out
above, the only difference is that CandGen-D
has as many tries G
i
as there are documents i
and CandGen-G has only one G. This simple
difference leads to different candidate sets and
counts over their occurrences. For example, say
two documents contain the pair putt/putts and
another contains bogey/bogeys. With CandGen-
D, count($)=3, count(s)=3, and count($, s)=2.
For the same documents, CandGen-G would pro-
duce count($)=2 and count(s)=2 since putt/putts
would have occurred only once in the global trie.
Also, consider a rare pair such as aard-
vark/aardvarks where each word is found in a dif-
ferent document. The pair would be identified
by CandGen-G but not by CandGen-D. The pair
would contribute a count of one to count($, s) in
CandGen-G but not in CandGen-D. So, CandGen-
G can provide better coverage, but it is also more
likely to identify noisy candidates, such as as-
suage/assumed, than CandGen-D.
4.2 Candidate filtering
The sets of candidates CStem,CAff,CRule is ex-
pected to be noisy since the only basis for gener-
ating them was strings that share a large portion of
their substrings. One way of filtering candidates is
to find affix candidates whose co-occurence with
other candidates is not statistically significant.
We measure correlation between candidate af-
fixes b
1
, b
2
in a candidate rule with the paired
?
2 test. By using ?2, we only consider pairwise
correlation between affixes, rather than attempting
global inference. Global consistency of affix sets
is not ensured, and as such the approach is sus-
ceptible to the multiple comparisons problem. We
still opt for this approach for its simplicity and be-
cause global inference is problematic due to data
sparseness.
Correlation between b
1
and b
2
is determined by
the following contingency table:4
b
1
? b
1
b
2
O
11
O
12
? b
2
O
21
O
22
Based on the significance testing, we define the set
of valid rules PairRule as those for which the ?2
test is significant at p < 0.05. Thus, affix can-
didates not significantly correlated with any other
affix in CAff are discarded.
4.3 Affix clustering
The previous stage produces a set of pairs of af-
fixes that are significantly correlated. However,
inflectional paradigms rarely contain just two af-
fixes, so we would like to group together affix
pairs into larger affix sets to improve generaliza-
tion. We use a bottom up, minimum distance clus-
tering for valid affix pairs (rules). We do not as-
sume that cluster membership is exclusive. For
example, it would not make sense to determine
that the null affix -$ can belong to only one cluster.
Therefore, we produce non-disjoint affix clusters.
A valid cluster of affixes is a maximal set of af-
fixes forming pairwise valid rules: Aff ? CAff is a
valid cluster of affixes iff
4where O
11
= count({b
1
, b
2
}), O
12
= count(b
2
) ?
O
11
, O
21
= count(b
1
)?O
11
, O
22
= N?O
11
?O
12
?O
21
and N =
P
b?CAff count(b). See table (1) for examples.
672
ed ?ed
ing 10273 21853
?ing 27120 4119332
(a) ?2 = 352678
le ?le
s 122 132945
?s 936 4044575
(b) ?2 = 239.132
ed ?ed
ing 2651 1310
?ing 1490 150848
(c) ?2 = 65101.6
le ?le
s 20 12073
?s 198 144008
(d) ?2 = 0.631, p = 0.427
Table 1: Affix counts in contingency tables for the valid pair ed/ing and spurious pair le/s according to
CandGen-D in (a) and (b) and according to CandGen-G in (c) and (d). ?2 test values are given under
each table. Data is from NYT. Total affix token counts induced through CandGen-D and CandGen-G
are N=4178578 and N=156299, respectively. A total of 2054 and 3739 affix types were induced for
CandGen-D and CandGen-G, respectively showing that CandGen-G does have better coverage though
it might have more noise.
1. ?b
1
, b
2
? Aff : {b
1
, b
2
} ? PairRule, and
2. If b ? CAff with ?b? ? Aff : {b, b?} ?
PairRule, then b ? Aff.
The set of all valid affix clusters is GroupRule.
This formulation does not rule out the existence
of clusters with affixes in common.
4.4 Word clustering
We next cluster word forms into morphologically
related groups. Our model assumes two word
forms to be morphologically related iff (1) they oc-
curred in the same trie G, (2) they have a trunk s in
common that is a stem in Stem(G), and (3) their af-
fixes under this stem s are members in a common
valid affix cluster in GroupRule. Hence a single
stem s can be involved in at most |GroupRule| con-
flation sets, one for each valid affix cluster. Again,
the only distinction between clustering with a
global trie (Clust-G) and clustering with several
tries from the documents in a corpus (Clust-D) is
that the former has only one trie.
We define the conflation set for a given stem s ?
Stem and valid affix cluster Aff ? GroupRule as
Wd(s,Aff) = {sb
1
, sb
2
| b
1
, b
2
? Aff ?
?G.s ? Stem(G) ? b
1
, b
2
? CAff(s,G)}
One issue that needs clarification is when the
candidate generation and clustering stages use dif-
ferent strategies, i.e. the models CandGen-D
+Clust-G and CandGen-G +Clust-D. This sim-
ply means that the statistics, and thus the valid
GroupRule, are derived from either CandGen-D or
CandGen-G.
4.5 Induction for languages that are both
prefixal and affixal
The above approach would not fit a language that
is prefixal and suffixal. Assuming we have in-
duced separate conflation sets over a prefix trie and
a suffix trie, we merge clusters between the two if
they have at least one word form in common. For-
mally, given a set of prefix conflation sets PCS and
a set of suffix conflation sets SCS, the final set of
conflation sets CS is:
CS = {p ? s |p ? PCS, s ? SCS ? p ? s 6= ?}
5 Data
We apply our method on English and Uspanteko,
an endangered Mayan language.
Learning corpora. For English, we use two
subsets of the NYTimes portion in the Gigaword
corpus which we will call NYT and MINI-NYT.
NYT in the current study is the complete collec-
tion of articles in the New York Times from June,
2002. NYT has 10K articles, 88K types and 9M
tokens. MINI-NYT is a subset of NYT with 190
articles, 15K types and 187K tokens.
The Uspanteko text, USP has 29 distinct texts,
7K types, and 50K tokens. The texts are from
OKMA (Pixabaj et al, 2007) and the segmenta-
tion and labels of the interlinear glossed text anno-
tations were checked for consistency and cleaned
up (Palmer et al, 2009). All counts are for lower-
cased, punctuation-removed word forms.
CELEX. The CELEX lexical database (Baayen
et al, 1993) has been built for Dutch, English and
German and provides detailed entries that list and
analyze the morphological properties of words,
among other information. Using CELEX, we eval-
uate on types rather than tokens. The performance
of the model is based on how many of the words it
judges to be morphologically related overlap with
the entries in CELEX. Following previous work
(Schone and Jurafsky, 2000; Schone and Jurafsky,
673
2001; Freitag, 2005), we evaluate on inflectional
clusters only, using the CELEX file listing clusters
of inflectional variants. 5
6 Experiments and evaluation
We outline our evaluation methodology, baselines,
benchmarks and results, and discuss the results.
6.1 Evaluation metric
Schone and Jurafsky (2000) give definitions for
correct (C), inserted (I), and deleted (D) words
in model-derived conflation sets in relation to a
gold standard. Their formulation does not allow
for multiple cluster membership of words. We ex-
tend the definition to incorporate this fact about the
data. Let w be a word form. We write X
w
for the
clusters induced by the model that contain w, and
Y
w
for gold standard clusters containing w. X
w
and Y
w
only count words which occurred in both
model and gold standard clusters. Then
C =
?
w
?
X
w
?
Y
w
(|X
w
? Y
w
|/|Y
w
|)
I =
?
w
?
X
w
?
Y
w
(|X
w
? (X
w
? Y
w
)|/|Y
w
|)
D =
?
w
?
X
w
?
Y
w
(|Y
w
? (X
w
? Y
w
)|/|Y
w
|)
Based on these definitions, we formulate preci-
sion (P ), recall (R), and the f -score (F ) as: P =
C/(C+I), R = C/(C+D), F = (2PR)/(P+R).
USP evaluation We use two different means to
evaluate the performance on USP. One is the
f -score derived from the above section with re-
spect to a standard that was automatically gen-
erated from the morpheme segment tiers of the
OKMA IGT. We generated the standard by taking
non-hyphenated segments as the stem and cluster-
ing words with shared stems.
We also had an expert in Uspanteko manually
evaluate a random subset (N = 100) of the model
output to compensate for any failings in the stan-
dard. The evaluator determined a dominant stem
for a cluster and identified words which were not
related to that stem. We measured accuracy and
5CELEX does have a second file listing words and their
breakup into constituent morphemes for both derivation and
inflection, but its use would have required additional process-
ing that could introduce errors.
0 10 20 30 40 50 60 70 80 90 100
40
50
60
70
80
90
100
precision
re
ca
ll
 
 
mini?NYT
NYT
Usp?S
Usp?P
Figure 2: Precision/recall graph for baseline ex-
periments on English, prefix USP (Usp-P) and suf-
fix USP (Usp-S).
full cluster accuracy6 for the expert evaluations
(table 4).
We experimented on Uspanteko with three dif-
ferent assumptions: (1) it is only prefixal; (2) it is
only suffixal; (3) it is both prefixal and suffixal.
We applied the assumptions of only prefixal or
only suffixal to LINGUISTICA as well. The rele-
vant results are given row headers in tables with a
corresponding +P(prefix) or +S(suffix).
6.2 Baselines and benchmarks
In a set of baselines, we put words which share
the first k characters into the same cluster. We
do this for NYT, MINI-NYT, and USP in a pre-
fix tree, and for USP in suffix tree (using the last k
characters). We set the values of 0 < k < max,
where max is the length of the longest string, and
plot the results in a precision-recall graph (Fig. 2).
Low k corresponds to high recall and low preci-
sion while high k shows the opposite. The contrast
in morphological patterns for each language can
also be seen. Because Uspanteko is morpholog-
ically complex with suffixes and prefixes, a very
simple strategy cannot achieve high recall as op-
posed to English where it is possible to retrieve all
variants with a simple prefix tree.
We use Linguistica (Goldsmith, 2001) and Mor-
fessor (Creutz and Lagus, 2007) as benchmarks.
We used the default settings for these programs.
Note that comparison with these tools is not com-
6Given a model cluster C
i
and the ?misses? for each clus-
ter M
i
, accuracy is measured as 1/N
P
i
(|C
i
|?|M
i
|)/(|C
i
|)
where N is the sample size. Full cluster accuracy is the num-
ber of clusters that did not have any misses over N .
674
MINI-NYT NYT
P R F P R F
LINGUISTICA 64.30 93.34 76.15 47.50 88.33 61.77
MORFESSOR 45.2 87.8 59.7 63.6 69.2 66.3
CandGen-D + Clust-G 69.41 91.42 78.91 46.00 79.81 58.36
CandGen-D + Clust-D 83.47 80.36 81.89 59.02 74.50 65.86
CandGen-G + Clust-G 73.44 88.72 80.36 61.81 82.98 70.85
CandGen-G + Clust-D 88.34 77.95 82.82 77.71 70.24 73.79
Table 2: Results on English for all models in precision(P), recall(R), f -score(F) for each data set.
pletely fair. Morfessor only generates segmenta-
tions. We therefore processed Morfessor output
by clustering words by assuming that the longest
segment in any segmentation is the stem and eval-
uated this instead. Linguistica produces stems and
associated suffixes so the clusters naturally follow
from this output. However, Linguistica only infers
either prefix or suffix patterns.
6.3 Results and discussion
The results on English are in table 2 with ?2 test
criteria of p<0.05 and each cell in the contingency
table >5. CandGen-G +Clust-D had the best f -
score, and easily beats the benchmarks.
This is different from our expectation that
awareness of document boundaries at all stages
(i.e., CandGen-D +Clust-D) would show the best
results. The discrepancy is especially marked for
the larger NYT. One important reason for this is
the affix criterion itself: trunks must be longer than
branches. Consider again the sample contingency
tables in Table 1 that were derived from NYT
through CandGen-D and CandGen-G. We had as-
sumed at the outset that CandGen-D would be bet-
ter able to filter out noise and would be sparser, but
results show the opposite. The reason is that that
short words in a global lexicon are more likely to
share trunks with longer, unrelated words. This
ensures that short word forms rarely generate can-
didate affixes. Longer words which are less likely
to have spurious long branches generate the bulk
of candidate suffixes and stems. This is born out
by the stems that were associated with the spuri-
ous suffix pair le/s: CandGen-G has cliente, cripp,
crumb, daniel, ender, label, mccord, nag, oval,
sear, stubb, whipp. CandGen-D has crumb, hand,
need, sing, tab, trick, trip. The word forms that
are associated with le/s through the CandGen-D
strategy are crumble/crumbs, handle/hands, . . . .
Compare this with the word forms associated with
the search strategy CandGen-G such as clien-
tele/clientes, cripple/crips, . . . . The majority of
them are not common English words; they are
most probably proper names such as LaBelle and
Searle. Furthermore, there is no item among the
stems from the CandGen-G search where concate-
nating the stems le and s would result in both word
forms being a common noun or verb as is the
case with the stems from the CandGen-D search
where all concatenated word forms are common
English words. Though CandGen-G finds spuri-
ous stems, the counts for the spurious affix pair are
suppressed (see table 1) because it is a type count
rather than a token count. This results in le/s be-
ing properly excluded as a rule. This explains why
CandGen-D has worse precision in general than
CandGen-G.
The affix criterion has other minor issues. One
is that it ignores the few cases where stems are
shorter than affixes, such as the very common
words be, do, go.7 Assuming that the longest
productive inflectional suffix in English is -ing8,
the criterion would correctly find stem candidates
for -ing only when the stem is longer than 3 or
4 letters. Another is that the criterion, when
combined with CandGen-D, generates candidates
from the/them/then/their/these which cooccur fre-
quently in documents. This is not an issue when
the criterion is applied in CandGen-G.
Nonetheless, results show that when data sizes
are small, as with USP (Table 3) and MINI-NYT,
awareness of document boundaries at the candi-
date generation stage is beneficial to precision.
7The exclusion of such words in a token based evaluation
as opposed to a type based evaluation would heavily penalize
our approach. We are not aware, however, of any prior work
in unsupervised morphology that evaluates over tokens.
8with occasional gemination of final consonant such as
occur ? occurring
675
P R F
Ca-D + Cl-D 70.51 44.35 54.45
Ca-G + Cl-G 70.00 46.87 56.15
Ca-D + Cl-D + S 88.58 45.21 59.86
Ca-D + Cl-G + S 85.03 44.75 58.64
Ca-G + Cl-D + S 90.34 45.48 60.50
Ca-G + Cl-G + S 84.54 46.03 59.60
Ca-D + Cl-D + P 93.84 47.90 63.42
Ca-D + Cl-G + P 89.94 47.38 62.06
Ca-G + Cl-D + P 95.42 47.89 63.78
Ca-G + Cl-G + P 92.03 50.01 64.80
LINGUISTICA + S 81.14 47.60 60.00
LINGUISTICA + P 84.15 52.00 64.28
MORFESSOR 28.12 62.28 38.75
Table 3: Performance of models on automatically
generated USP evaluation set. P: Prefix only, S:
Suffix only. If there is no indication of S or P, it
means model attempted to learn both
Acc. FAcc. Avg. Sz.
Ca-G + Cl-G 98.5 79.0 2.94
LINGUISTICA 96.0 85.0 2.64
MORFESSOR 85.3 55.0 4.8
Table 4: Human expert evaluated accuracy (Acc.)
and full cluster accuracy (FAcc.) of models on
USP and average cluster size in words (Avg. Sz.)
However, it seems that CandGen-G has better cov-
erage no matter the size of the corpus, which
explains why coupling it with Clust-D produces
overall better scores. Clust-D does provide a use-
ful added constraint to mere orthographic similar-
ity (i.e. shared trunks in a trie).
A worrisome aspect of the results is that perfor-
mance degrades for large data sets (this is also true
for Linguistica). However, it also hints that this
method might work well for under-resourced lan-
guages. We surmise that since productive suffixes
do not suffer from sparsity, even a small data set
provides sufficient evidence to reach reliable con-
clusions about the productive morphology of some
language. Increasing the size of the data merely
increases the counts of spurious affixes and poses
problems for a relative simple measure such as
the ?2 test. A similar result was shown in Creutz
and Lagus (2005) where f -score performance of
their segmentation method improved as more data
was provided then decreased as the input exceeded
250K tokens in English. Their method showed
continued improvement with increased data for
Finnish. This hints that more data is beneficial
for morphologically complex languages but not
for morphologically impoverished languages.
Finally, it is also encouraging that the manual
evaluation (Table 4) shows very high accuracy, as
judged by a documentary linguist. Both our model
and Linguistica perform very well under this eval-
uation.
7 Conclusion
We have presented a novel approach to unsuper-
vised morphology acquisition that uses a very
simple pipeline and does not use any thresholds
other than standard ones associated with the ?2
test. The model relies on document boundaries
and correlation tests for filtering spurious stems
and affixes. The model compares favorably to
Linguistica and Morfessor, two models that em-
ploy much more complex strategies and rely on
fine-tuned parameters. We found that the use of
document boundaries is especially beneficial with
small datasets, which is promising for the applica-
tion of this model to under-resourced languages.
For large datasets, global candidate generation
outperformed document-aware candidate genera-
tion at the task of filtering out spurious stems,
but document-aware clustering does improve pre-
cision and overall performance.
In this paper we have addressed one aspect of
morphology acquisition, segmentation and clus-
tering. Extending the approach is straightforward,
for example, substituting more sophisticated data
structures or statistical tests for the current ones.
In particular, we will move from the use of doc-
ument boundaries to a flexible notion of textual
distance to estimate likelihood of morphological
relatedness.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documenta-
tion of Languages using Machine Learning and
Active Learning.? Thanks to Alexis Palmer, Telma
Kaan Pixabaj, Elias Ponvert, and the anonymous
reviewers.
676
References
R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX lexical database on CD-ROM. Linguis-
tic Data Consortium, Philadelphia, PA.
M. Baroni, J. Matiasek, and H. Trost. 2002. Unsu-
pervised discovery of morphologically related words
based on orthographic and semantic similarity. In
ACL ?02 workshop on Morphological and phonolog-
ical learning, pages 48?57.
D. Bernhard. 2005. Unsupervised morphological seg-
mentation based on segment predictability and word
segments alignment. In Proceedings of Morpho
Challenge 2005, pages 18?27.
S. Bordag. 2005. Two-step approach to unsupervised
morpheme segmentation. In Proceedings of Morpho
Challenge 2005, pages 23?27.
E. Chan. 2006. Learning Probabilistic Paradigms for
Morphology in a Latent Class Model. In ACL SIG-
PHON ?06, pages 69?78.
M. Creutz and K. Lagus. 2002. Unsupervised dis-
covery of morphemes. In ACL ?02 workshop on
Morphological and phonological learning-Volume
6, pages 21?30.
M. Creutz and K. Lagus. 2004. Induction of a simple
morphology for highly-inflecting languages. In ACL
SIGPHON ?04, pages 43?51.
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unanno-
tated text. In AKRR ?05, pages 106?113.
M. Creutz and K. Lagus. 2007. Unsupervised models
for morpheme segmentation and morphology learn-
ing. ACM Trans. Speech Lang. Process., 4(1):3.
S. Dasgupta and V. Ng. 2007. High-performance,
language-independent morphological segmentation.
In NAACL-HLT, pages 155?163.
V. Demberg. 2007. A language-independent unsu-
pervised model for morphological segmentation. In
ACL ?07, volume 45, page 920.
D. Freitag. 2005. Morphology induction from term
clusters. In CoNLL ?05.
E. Gaussier. 1999. Unsupervised learning of deriva-
tional morphology from inflectional lexicons. In
ACL workshop on Unsupervised Methods in Natu-
ral Language Learning.
J. Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Comp. Ling.,
27(2):153?198.
J. Goldsmith. 2006. An algorithm for the unsupervised
learning of morphology. Natural Language Engi-
neering, 12(04):353?371.
S.J. Goldwater. 2007. Nonparametric Bayesian mod-
els of lexical acquisition. Ph.D. thesis, Brown Uni-
versity.
M.A. Hafer and S.F. Weiss. 1974. Word Segmentation
by Letter Successor Varieties. Information Storage
and Retrieval, 10:371?385.
H. Hammarstro?m. 2006. A naive theory of affixation
and an algorithm for extraction. In ACL SIGPHON
?06, pages 79?88, June.
S. Keshava and E. Pitler. 2005. A simpler, intuitive
approach to morpheme induction. In Proceedings of
Morpho Challenge 2005, pages 28?32.
C. Monson. 2004. A framework for unsupervised nat-
ural language morphology induction. In Proceed-
ings of the Student Workshop at ACL, volume 4.
Alexis Palmer, Taesun Moon, and Jason Baldridge.
2009. Evaluating automation strategies in language
documentation. In Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 36?44, Boulder, CO.
T.C. Pixabaj, M.A. Vicente Me?ndez, M. Vicente
Me?ndez, and O.A. Damia?n. 2007. Text collections
in Four Mayan Languages. Archived in The Archive
of the Indigenous Languages of Latin America.
P. Schone and D. Jurafsky. 2000. Knowledge-free in-
duction of morphology using latent sematic analysis.
In CoNLL-2000 and LLL-2000.
P. Schone and D. Jurafsky. 2001. Knowledge-free
induction of inflectional morphologies. In NAACL
?01, pages 1?9.
M.G. Snover, G.E. Jarosz, and M.R. Brent. 2002. Un-
supervised learning of morphology using a novel di-
rected search algorithm: taking the first step. In ACL
?02 workshop on Morphological and phonological
learning, pages 11?20.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
ACL ?08.
D. Yarowsky and R. Wicentowski. 2000. Minimally
supervised morphological analysis by multimodal
alignment. In ACL ?00, pages 207?216.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In HLT ?01.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL ?95,
pages 189?196.
677
Proceedings of NAACL HLT 2007, pages 236?243,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Joint Determination of Anaphoricity and Coreference Resolution using
Integer Programming
Pascal Denis and Jason Baldridge
Department of Linguistics
University of Texas at Austin
{denis,jbaldrid}@mail.utexas.edu
Abstract
Standard pairwise coreference resolution
systems are subject to errors resulting
from their performing anaphora identifi-
cation as an implicit part of coreference
resolution. In this paper, we propose
an integer linear programming (ILP) for-
mulation for coreference resolution which
models anaphoricity and coreference as a
joint task, such that each local model in-
forms the other for the final assignments.
This joint ILP formulation provides f -
score improvements of 3.7-5.3% over a
base coreference classifier on the ACE
datasets.
1 Introduction
The task of coreference resolution involves impos-
ing a partition on a set of entity mentions in a docu-
ment, where each partition corresponds to some en-
tity in an underlying discourse model. Most work
treats coreference resolution as a binary classifica-
tion task in which each decision is made in a pair-
wise fashion, independently of the others (McCarthy
and Lehnert, 1995; Soon et al, 2001; Ng and Cardie,
2002b; Morton, 2000; Kehler et al, 2004).
There are two major drawbacks with most sys-
tems that make pairwise coreference decisions. The
first is that identification of anaphora is done implic-
itly as part of the coreference resolution. Two com-
mon types of errors with these systems are cases
where: (i) the system mistakenly identifies an an-
tecedent for non-anaphoric mentions, and (ii) the
system does not try to resolve an actual anaphoric
mention. To reduce such errors, Ng and Cardie
(2002a) and Ng (2004) use an anaphoricity classi-
fier ?which has the sole task of saying whether or
not any antecedents should be identified for each
mention? as a filter for their coreference system.
They achieve higher performance by doing so; how-
ever, their setup uses the two classifiers in a cascade.
This requires careful determination of an anaphoric-
ity threshold in order to not remove too many men-
tions from consideration (Ng, 2004). This sensi-
tivity is unsurprising, given that the tasks are co-
dependent.
The second problem is that most coreference sys-
tems make each decision independently of previous
ones in a greedy fashion (McCallum and Wellner,
2004). Clearly, the determination of membership of
a particular mention into a partition should be condi-
tioned on how well it matches the entity as a whole.
Since independence between decisions is an unwar-
ranted assumption for the task, models that consider
a more global context are likely to be more appropri-
ate. Recent work has examined such models; Luo et
al. (2004) using Bell trees, and McCallum and Well-
ner (2004) using conditional random fields, and Ng
(2005) using rerankers.
In this paper, we propose to recast the task of
coreference resolution as an optimization problem,
namely an integer linear programming (ILP) prob-
lem. This framework has several properties that
make it highly suitable for addressing the two afore-
mentioned problems. The first is that it can uti-
lize existing classifiers; ILP performs global infer-
ence based on their output rather than formulating a
236
new inference procedure for solving the basic task.
Second, the ILP approach supports inference over
multiple classifiers, without having to fiddle with
special parameterization. Third, it is much more
efficient than conditional random fields, especially
when long-distance features are utilized (Roth and
Yih, 2005). Finally, it is straightforward to create
categorical global constraints with ILP; this is done
in a declarative manner using inequalities on the as-
signments to indicator variables.
This paper focuses on the first problem, and
proposes to model anaphoricity determination and
coreference resolution as a joint task, wherein the
decisions made by each locally trained model are
mutually constrained. The presentation of the ILP
model proceeds in two steps. In the first, interme-
diary step, we simply use ILP to find a global as-
signment based on decisions made by the corefer-
ence classifier alone. The resulting assignment is
one that maximally agrees with the decisions of the
classifier, that is, where all and only the links pre-
dicted to be coreferential are used for constructing
the chains. This is in contrast with the usual clus-
tering algorithms, in which a unique antecedent is
typically picked for each anaphor (e.g., the most
probable or the most recent). The second step pro-
vides the joint formulation: the coreference classi-
fier is now combined with an anaphoricity classifier
and constraints are added to ensure that the ultimate
coreference and anaphoricity decisions are mutually
consistent. Both of these formulations achieve sig-
nificant performance gains over the base classifier.
Specifically, the joint model achieves f -score im-
provements of 3.7-5.3% on three datasets.
We begin by presenting the basic coreference
classifier and anaphoricity classifier and their per-
formance, including an upperbound that shows the
limitation of using them in a cascade. We then give
the details of our ILP formulations and evaluate their
performance with respect to each other and the base
classifier.
2 Base models: coreference classifier
The classification approach tackles coreference
in two steps by: (i) estimating the probability,
PC(COREF|?i, j?), of having a coreferential out-
come given a pair of mentions ?i, j?, and (ii) apply-
ing a selection algorithm that will single out a unique
candidate out of the subset of candidates i for which
the probability PC(COREF|?i, j?) reaches a particu-
lar value (typically .5).
We use a maximum entropy model for the coref-
erence classifier. Such models are well-suited for
coreference, because they are able to handle many
different, potentially overlapping learning features
without making independence assumptions. Previ-
ous work on coreference using maximum entropy
includes (Kehler, 1997; Morton, 1999; Morton,
2000). The model is defined in a standard fashion
as follows:
PC(COREF|?i, j?) =
exp(
n?
k=1
?kfk(?i, j?, COREF))
Z(?i, j?)
(1)
Z(?i, j?) is a normalization factor over both out-
comes (COREF and ?COREF). Model parameters
are estimated using maximum entropy (Berger et al,
1996). Specifically, we estimate parameters with
the limited memory variable metric algorithm imple-
mented in the Toolkit for Advanced Discriminative
Modeling1 (Malouf, 2002). We use a Gaussian prior
with a variance of 1000 ? no attempt was made to
optimize this value.
Training instances for the coreference classifier
are constructed based on pairs of mentions of the
form ?i, j?, where j and i are the descriptions for
an anaphor and one of its candidate antecedents, re-
spectively. Each such pair is assigned either a label
COREF (i.e. a positive instance) or a label ?COREF
(i.e. a negative instance) depending on whether or
not the two mentions corefer. In generating the train-
ing data, we followed the method of (Soon et al,
2001) creating for each anaphor: (i) a positive in-
stance for the pair ?i, j? where i is the closest an-
tecedent for j, and (ii) a negative instance for each
pair ?i, k? where k intervenes between i and j.
Once trained, the classifier is used to create a set
of coreferential links for each test document; these
links in turn define a partition over the entire set of
mentions. In the system of Soon et. al. (2001) sys-
tem, this is done by pairing each mention j with each
preceding mention i. Each test instance ?i, j? thus
1Available from tadm.sf.net.
237
formed is then evaluated by the classifier, which re-
turns a probability representing the likelihood that
these two mentions are coreferential. Soon et. al.
(2001) use ?Closest-First? selection: that is, the pro-
cess terminates as soon as an antecedent (i.e., a test
instance with probability > .5) is found or the be-
ginning of the text is reached. Another option is to
pick the antecedent with the best overall probability
(Ng and Cardie, 2002b).
Our features for the coreference classifier fall into
three main categories: (i) features of the anaphor, (ii)
features of antecedent mention, and (iii) relational
features (i.e., features that describe properties which
hold between the two mentions, e.g. distance). This
feature set is similar (though not equivalent) to that
used by Ng and Cardie (2002a). We omit details
here for the sake of brevity ? the ILP systems we
employ here could be equally well applied to many
different base classifiers using many different fea-
ture sets.
3 Base models: anaphoricity classifier
As mentioned in the introduction, coreference clas-
sifiers such as that presented in section 2 suf-
fer from errors in which (a) they assign an an-
tecedent to a non-anaphor mention or (b) they as-
sign no antecedents to an anaphoric mention. Ng
and Cardie (2002a) suggest overcoming such fail-
ings by augmenting their coreference classifier with
an anaphoricity classifier which acts as a filter dur-
ing model usage. Only the mentions that are deemed
anaphoric are considered for coreference resolu-
tion. Interestingly, they find a degredation in per-
formance. In particular, they obtain significant im-
provements in precision, but with larger losses in
recall (especially for proper names and common
nouns). To counteract this, they add ad hoc con-
straints based on string matching and extended men-
tion matching which force certain mentions to be
resolved as anaphors regardless of the anaphoric-
ity classifier. This allows them to improve overall
f -scores by 1-3%. Ng (2004) obtains f -score im-
provements of 2.8-4.5% by tuning the anaphoricity
threshold on held-out data.
The task for the anaphoricity determination com-
ponent is the following: one wants to decide for each
mention i in a document whether i is anaphoric or
not. That is, this task can be performed using a sim-
ple binary classifier with two outcomes: ANAPH and
?ANAPH. The classifier estimates the conditional
probabilities P (ANAPH|i) and predicts ANAPH for i
when P (ANAPH|i) > .5.
We use the following model for our anaphoricity
classifier:
PA(ANAPH|i) =
exp(
n?
k=1
?kfk(i, ANAPH))
Z(i)
(2)
This model is trained in the same manner as the
coreference classifier, also with a Gaussian prior
with a variance of 1000.
The features used for the anaphoricity classifier
are quite simple. They include information regard-
ing (1) the mention itself, such as the number of
words and whether it is a pronoun, and (2) properties
of the potential antecedent set, such as the number of
preceding mentions and whether there is a previous
mention with a matching string.
4 Base model results
This section provides the performance of the pair-
wise coreference classifier, both when used alone
(COREF-PAIRWISE) and when used in a cascade
where the anaphoricity classifier acts as a filter on
which mentions should be resolved (AC-CASCADE).
In both systems, antecedents are determined in the
manner described in section 2.
To demonstrate the inherent limitations of cas-
cading, we also give results for an oracle sys-
tem, ORACLE-LINK, which assumes perfect linkage.
That is, it always picks the correct antecedent for
an anaphor. Its only errors are due to being un-
able to resolve mentions which were marked as non-
anaphoric (by the imperfect anaphoricity classifier)
when in fact they were anaphoric.
We evaluate these systems on the datasets from
the ACE corpus (Phase 2). This corpus is di-
vided into three parts, each corresponding to a dif-
ferent genre: newspaper texts (NPAPER), newswire
texts (NWIRE), and broadcasted news transcripts
(BNEWS). Each of these is split into a train
part and a devtest part. Progress during the de-
velopment phase was determined by using cross-
validation on only the training set for the NPAPER
238
System BNEWS NPAPER NWIRE
R P F R P F R P F
COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8
AC-CASCADE 51.1 79.7 62.3 53.7 79.0 63.9 53.0 81.8 64.3
ORACLE-LINK 69.4 100 82.0 71.2 100 83.1 66.7 100 80.0
Table 1: Recall (R), precision (P), and f -score (F) on the three ACE datasets for the basic coreference system
(COREF-PAIRWISE), the anaphoricity-coreference cascade system (AC-CASCADE), and the oracle which
performs perfect linkage (ORACLE-LINK). The first two systems make strictly local pairwise coreference
decisions.
section. No human-annotated linguistic information
is used in the input. The corpus text was prepro-
cessed with the OpenNLP Toolkit2 (i.e., a sentence
detector, a tokenizer, a POS tagger, and a Named
Entity Recognizer).
In our experiments, we consider only the true
ACE mentions. This is because our focus is on eval-
uating pairwise local approaches versus the global
ILP approach rather than on building a full coref-
erence resolution system. It is worth noting that
previous work tends to be vague in both these re-
spects: details on mention filtering or providing
performance figures for markable identification are
rarely given.
Following common practice, results are given in
terms of recall and precision according to the stan-
dard model-theoretic metric (Vilain et al, 1995).
This method operates by comparing the equivalence
classes defined by the resolutions produced by the
system with the gold standard classes: these are the
two ?models?. Roughly, the scores are obtained by
determining the minimal perturbations brought to
one model in order to map it onto the other model.
Recall is computed by trying to map the predicted
chains onto the true chains, while precision is com-
puted the other way around. We test significant dif-
ferences with paired t-tests (p < .05).
The anaphoricity classifier has an average accu-
racy of 80.2% on the three ACE datasets (using a
threshold of .5). This score is slightly lower than
the scores reported by Ng and Cardie (2002a) for
another data set (MUC).
Table 1 summarizes the results, in terms of recall
(R), precision (P), and f -score (F) on the three ACE
data sets. As can be seen, the AC-CASCADE system
2Available from opennlp.sf.net.
generally provides slightly better precision at the ex-
pense of recall than the COREF-PAIRWISE system,
but the performance varies across the three datasets.
The source of this variance is likely due to the fact
that we applied a uniform anaphoricity threshold
of .5 across all datasets; Ng (2004) optimizes this
threshold for each of the datasets: .3 for BNEWS
and NWIRE and .35 for NPAPER. This variance re-
inforces our argument for determining anaphoricity
and coreference jointly.
The limitations of the cascade approach are also
shown by the oracle results. Even if we had a sys-
tem that can pick the correct antecedents for all truly
anaphoric mentions, it would have a maximum re-
call of roughly 70% for the different datasets.
5 Integer programming formulations
The results in the previous section demonstrate the
limitations of a cascading approach for determin-
ing anaphoricity and coreference with separate mod-
els. The other thing to note is that the results in
general provide a lot of room for improvement ?
this is true for other state-of-the-art systems as well.
The integer programming formulation we provide
here has qualities which address both of these is-
sues. In particular, we define two objective func-
tions for coreference resolution to be optimized with
ILP. The first uses only information from the coref-
erence classifier (COREF-ILP) and the second inte-
grates both anaphoricity and coreference in a joint
formulation (JOINT-ILP). Our problem formulation
and use of ILP are based on both (Roth and Yih,
2004) and (Barzilay and Lapata, 2006).
For solving the ILP problem, we use lp solve,
an open-source linear programming solver which
implements the simplex and the Branch-and-Bound
239
methods.3 In practice, each test document is pro-
cessed to define a distinct ILP problem that is then
submitted to the solver.
5.1 COREF-ILP: coreference-only formulation
Barzilay and Lapata (2006) use ILP for the problem
of aggregation in natural language generation: clus-
tering sets of propositions together to create more
concise texts. They cast it as a set partitioning prob-
lem. This is very much like coreference, where
each partition corresponds to an entity in a discourse
model.
COREF-ILP uses an objective function that is
based on only the coreference classifier and the
probabilities it produces. Given that the classifier
produces probabilities pC = PC(COREF|i, j), the
assignment cost of commiting to a coreference link
is cC?i,j? = ?log(pC). A complement assignment
cost cC?i,j? = ?log(1?pC) is associated with choos-
ing not to establish a link. In what follows, M de-
notes the set of mentions in the document, and P the
set of possible coreference links over these mentions
(i.e., P = {?i, j?|?i, j? ? M ? M and i < j}). Fi-
nally, we use indicator variables x?i,j? that are set to
1 if mentions i and j are coreferent, and 0 otherwise.
The objective function takes the following form:
min
?
?i,j??P
cC?i,j? ? x?i,j? + c
C
?i,j? ? (1? x?i,j?) (3)
subject to:
x?i,j? ? {0, 1} ??i, j? ? P (4)
This is essentially identical to Barzilay and Lapata?s
objective function, except that we consider only
pairs in which the i precedes the j (due to the struc-
ture of the problem). Also, we minimize rather than
maximize due to the fact we transform the model
probabilities with ?log (like Roth and Yih (2004)).
This preliminary objective function merely guar-
antees that ILP will find a global assignment that
maximally agrees with the decisions made by the
coreference classifier. Concretely, this amounts to
taking all (and only) those links for which the classi-
fier returns a probability above .5. This formulation
does not yet take advantage of information from a
classifier that specializes in anaphoricity; this is the
subject of the next section.
3Available from http://lpsolve.sourceforge.net/.
5.2 JOINT-ILP: joint anaphoricity-coreference
formulation
Roth and Yih (2004) use ILP to deal with the joint
inference problem of named entity and relation iden-
tification. This requires labeling a set of named enti-
ties in a text with labels such as person and loca-
tion, and identifying relations between them such
as spouse of and work for. In theory, each of these
tasks would likely benefit from utilizing the infor-
mation produced by the other, but if done as a cas-
cade will be subject to propogation of errors. Roth
and Yih thus set this up as problem in which each
task is performed separately; their output is used to
assign costs associated with indicator variables in an
objective function, which is then minimized subject
to constraints that relate the two kinds of outputs.
These constraints express qualities of what a global
assignment of values for these tasks must respect,
such as the fact that the arguments to the spouse of
relation must be entities with person labels. Impor-
tantly, the ILP objective function encodes not only
the best label produced by each classifier for each
decision; it utilizes the probabilities (or scores) as-
signed to each label and attempts to find a global
optimum (subject to the constraints).
The parallels to our anaphoricity/coreference sce-
nario are straightforward. The anaphoricity problem
is like the problem of identifying the type of entity
(where the labels are now ANAPH and ?ANAPH),
and the coreference problem is like that of determin-
ing the relations between mentions (where the labels
are now COREF or ?COREF).
Based on these parallels, the JOINT-ILP system
brings the two decisions of anaphoricity and corefer-
ence together by including both in a single objective
function and including constraints that ensure the
consistency of a solution for both tasks. Let cAj and
cAj be defined analogously to the coreference clas-
sifier costs for pA = PA(ANAPH|j), the probability
the anaphoricity classifier assigns to a mention j be-
ing anaphoric. Also, we have indicator variables yj
that are set to 1 if mention j is anaphoric and 0 oth-
erwise. The objective function takes the following
240
form:
min
?
?i,j??P
cC?i,j? ? x?i,j? + c
C
?i,j? ? (1?x?i,j?)
+
?
j?M
cAj ? yj + c
A
j ? (1?yj) (5)
subject to:
x?i,j? ? {0, 1} ??i, j? ? P (6)
yj ? {0, 1} ?j ? M (7)
The structure of this objective function is very sim-
ilar to Roth and Yih?s, except that we do not uti-
lize constraint costs in the objective function itself.
Roth and Yih use these to make certain combina-
tions impossible (like a location being an argument
to a spouse of relation); we enforce such effects in
the constraint equations instead.
The joint objective function (5) does not constrain
the assignment of the x?i,j? and yj variables to be
consistent with one another. To enforce consistency,
we add further constraints. In what follows, Mj is
the set of all mentions preceding mention j in the
document.
Resolve only anaphors: if a pair of mentions ?i, j?
is coreferent (x?i,j?=1), then mention j must be
anaphoric (yj=1).
x?i,j? ? yj ??i, j? ? P (8)
Resolve anaphors: if a mention is anaphoric
(yj=1), it must be coreferent with at least one an-
tecedent.
yj ?
?
i?Mj
x?i,j? ?j ? M (9)
Do not resolve non-anaphors: if a mention is non-
anaphoric (yj=0), it should have no antecedents.
yj ?
1
|Mj |
?
i?Mj
x?i,j? ?j ? M (10)
These constraints thus directly relate the two
tasks. By formulating the problem this way, the de-
cisions of the anaphoricity classifier are not taken
on faith as they were with AC-CASCADE. Instead,
we optimize over consideration of both possibilities
in the objective function (relative to the probability
output by the classifier) while ensuring that the final
assignments respect the signifance of what it is to be
anaphoric or non-anaphoric.
6 Joint Results
Table 2 summarizes the results for these different
systems. Both ILP systems are significantly better
than the baseline system COREF-PAIRWISE. Despite
having lower precision than COREF-PAIRWISE, the
COREF-ILP system obtains very large gains in recall
to end up with overall f -score gains of 4.3%, 4.2%,
and 3.0% across BNEWS, NPAPER, and NWIRE, re-
spectively. The fundamental reason for the increase
in recall and drop in precision is that COREF-ILP can
posit multiple antecedents for each mention. This
is an extra degree of freedom that allows COREF-
ILP to cast a wider net, with a consequent risk of
capturing incorrect antecedents. Precision is not
completely degraded because the optimization per-
formed by ILP utilizes the pairwise probabilities of
mention pairs as weights in the objective function
to make its assignments. Thus, highly improbable
links are still heavily penalized and are not chosen
as coreferential.
The JOINT-ILP system demonstrates the benefit
ILP?s ability to support joint task formulations. It
produces significantly better f -scores by regaining
some of the ground on precision lost by COREF-
ILP. The most likely source of the improved pre-
cision of JOINT-ILP is that weights corresponding
to the anaphoricity probabilities and constraints (8)
and (10) reduce the number of occurrences of non-
anaphors being assigned antecedents. There are also
improvements in recall over COREF-ILP for NPAPER
and NWIRE. A possible source of this difference is
constraint (9), which ensures that mentions which
are considered anaphoric must have at least one an-
tecedent.
Compared to COREF-PAIRWISE, JOINT-ILP dra-
matically improves recall with relatively small
losses in precision, providing overall f -score gains
of 5.3%, 4.9%, and 3.7% on the three datasets.
7 Related Work
As was just demonstrated, ILP provides a principled
way to model dependencies between anaphoricity
decisions and coreference decisions. In a simi-
lar manner, this framework could also be used to
capture dependencies among coreference decisions
themselves. This option ?which we will leave for
future work? would make such an approach akin to
241
System BNEWS NPAPER NWIRE
R P F R P F R P F
COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8
COREF-ILP 62.2 75.5 68.2 67.1 77.3 71.8 60.1 74.8 66.8
JOINT-ILP 62.1 78.0 69.2 68.0 77.6 72.5 60.8 75.8 67.5
Table 2: Recall (R), precision (P), and f -score (F) on the three ACE datasets for the basic coreference system
(COREF-PAIRWISE), the coreference only ILP system (COREF-ILP), and the joint anaphoricity-coreference
ILP system (JOINT-ILP). All f -score differences are significant (p < .05).
a number of recent global approaches.
Luo et al (2004) use Bell trees to represent the
search space of the coreference resolution problem
(where each leaf is possible partition). The prob-
lem is thus recast as that of finding the ?best? path
through the tree. Given the rapidly growing size of
Bell trees, Luo et al resort to a beam search al-
gorithm and various pruning strategies, potentially
resulting in picking a non-optimal solution. The re-
sults provided by Luo et al are difficult to compare
with ours, since they use a different evaluation met-
ric.
Another global approach to coreference is the
application of Conditional Random Fields (CRFs)
(McCallum and Wellner, 2004). Although both are
global approaches, CRFs and ILP have important
differences. ILP uses separate local classifiers which
are learned without knowledge of the output con-
straints and are then integrated into a larger infer-
ence task. CRFs estimate a global model that di-
rectly uses the constraints of the domain. This in-
volves heavy computations which cause CRFs to
generally be slow and inefficient (even using dy-
namic programming). Again, the results presented
in McCallum and Wellner (2004) are hard to com-
pare with our own results. They only consider
proper names, and they only tackled the task of
identifying the correct antecedent only for mentions
which have a true antecedent.
A third global approach is offered by Ng (2005),
who proposes a global reranking over partitions gen-
erated by different coreference systems. This ap-
proach proceeds by first generating 54 candidate
partitions, which are each generated by a differ-
ent system. These different coreference systems
are obtained as combinations over three different
learners (C4.5, Ripper, and Maxent), three sam-
pling methods, two feature sets (Soon et al, 2001;
Ng and Cardie, 2002b), and three clustering al-
gorithms (Best-First, Closest-First, and aggressive-
merge). The features used by the reranker are of
two types: (i) partition-based features which are
here simple functions of the local features, and (ii)
method-based features which simply identify the
coreference system used for generating the given
partition. Although this approach leads to significant
gains on the both the MUC and the ACE datasets,
it has some weaknesses. Most importantly, the dif-
ferent systems employed for generating the different
partitions are all instances of the local classification
approach, and they all use very similar features. This
renders them likely to make the same types of errors.
The ILP approach could in fact be integrated with
these other approaches, potentially realizing the ad-
vantages of multiple global systems, with ILP con-
ducting their interactions.
8 Conclusions
We have provided two ILP formulations for resolv-
ing coreference and demonstrated their superiority
to a pairwise classifier that makes its coreference as-
signments greedily.
In particular, we have also shown that ILP pro-
vides a natural means to express the use of both
anaphoricity classification and coreference classifi-
cation in a single system, and that doing so provides
even further performance improvements, specifi-
cally f -score improvements of 5.3%, 4.9%, and
3.7% over the base coreference classifier on the ACE
datasets.
With ILP, it is not necessary to carefully control
the anaphoricity threshold. This is in stark contrast
to systems which use the anaphoricity classifier as a
filter for the coreference classifier in a cascade setup.
242
The ILP objective function incorporates the proba-
bilities produced by both classifiers as weights on
variables that indicate the ILP assignments for those
tasks. The indicator variables associated with those
assignments allow several constraints between the
tasks to be straightforwardly stated to ensure consis-
tency to the assignments. We thus achieve large im-
provements with a simple formulation and no fuss.
ILP solutions are also obtained very quickly for the
objective functions and constraints we use.
In future work, we will explore the use of global
constraints, similar to those used by (Barzilay and
Lapata, 2006) to improve both precision and recall.
For example, we expect transitivity constraints over
coreference pairs, as well as constraints on the en-
tire partition (e.g., the number of entities in the doc-
ument), to help considerably. We will also consider
linguistic constraints (e.g., restrictions on pronouns)
in order to improve precision.
Acknowledgments
We would like to thank Ray Mooney, Rohit Kate,
and the three anonymous reviewers for their com-
ments. This work was supported by NSF grant IIS-
0535154.
References
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the HLT/NAACL, pages 359?366, New
York, NY.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
A. Kehler, D. Appelt, L. Taylor, and A. Simma.
2004. The (non)utility of predicate-argument frequen-
cies for pronoun interpretation. In Proceedings of
HLT/NAACL, pages 289?296.
Andrew Kehler. 1997. Probabilistic coreference in infor-
mation extraction. In Proceedings of EMNLP, pages
163?173.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, , and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the ACL.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Language
Learning, pages 49?55, Taipei, Taiwan.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Proceedings of NIPS.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using
decision trees for coreference resolution. In Proceed-
ings of IJCAI, pages 1050?1055.
Thomas Morton. 1999. Using coreference for ques-
tion answering. In Proceedings of ACL Workshop on
Coreference and Its Applications.
Thomas Morton. 2000. Coreference for NLP applica-
tions. In Proceedings of ACL, Hong Kong.
Vincent Ng and Claire Cardie. 2002a. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of COLING.
Vincent Ng and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of ACL, pages 104?111.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings of ACL.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of ACL.
Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of ICML, pages 737?744.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
fo the 6th Message Understanding Conference (MUC-
6), pages 45?52, San Mateo, CA. Morgan Kaufmann.
243
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 896?903,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Sequencing Model for Situation Entity Classification
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith
Department of Linguistics
University of Texas at Austin
{alexispalmer,ponvert,jbaldrid,carlotasmith}@mail.utexas.edu
Abstract
Situation entities (SEs) are the events, states,
generic statements, and embedded facts and
propositions introduced to a discourse by
clauses of text. We report on the first data-
driven models for labeling clauses according
to the type of SE they introduce. SE classifi-
cation is important for discourse mode iden-
tification and for tracking the temporal pro-
gression of a discourse. We show that (a)
linguistically-motivated cooccurrence fea-
tures and grammatical relation information
from deep syntactic analysis improve clas-
sification accuracy and (b) using a sequenc-
ing model provides improvements over as-
signing labels based on the utterance alone.
We report on genre effects which support the
analysis of discourse modes having charac-
teristic distributions and sequences of SEs.
1 Introduction
Understanding discourse requires identifying the
participants in the discourse, the situations they par-
ticipate in, and the various relationships between and
among both participants and situations. Coreference
resolution, for example, is concerned with under-
standing the relationships between references to dis-
course participants. This paper addresses the prob-
lem of identifying and classifying references to situ-
ations expressed in written English texts.
Situation entities (SEs) are the events, states,
generic statements, and embedded facts and propo-
sitions which clauses introduce (Vendler, 1967;
Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher,
1993; Carlson and Pelletier, 1995). Consider the
text passage below, which introduces an event-type
entity in (1), a report-type entity in (2), and a state-
type entity in (3).
(1) Sony Corp. has heavily promoted the VideoWalkman
since the product?s introduction last summer ,
(2) but Bob Gerson , video editor of This Week in Con-
sumer Electronics , says
(3) Sony conceives of 8mm as a ?family of products ,
camcorders and VCR decks , ?
SE classification is a fundamental component in de-
termining the discourse mode of texts (Smith, 2003)
and, along with aspectual classification, for tempo-
ral interpretation (Moens and Steedman, 1988). It
may be useful for discourse relation projection and
discourse parsing.
Though situation entities are well-studied in lin-
guistics, they have received very little computational
treatment. This paper presents the first data-driven
models for SE classification. Our two main strate-
gies are (a) the use of linguistically-motivated fea-
tures and (b) the implementation of SE classification
as a sequencing task. Our results also provide empir-
ical support for the very notion of discourse modes,
as we see clear genre effects in SE classification.
We begin by discussing SEs in more detail. Sec-
tion 3 describes our two annotated data sets and pro-
vides examples of each SE type. Section 4 discusses
feature sets, and sections 5 and 6 present models,
experiments, and results.
896
2 Discourse modes and situation entities
In this section, we discuss some of the linguistic mo-
tivation for SE classification and the relation of SE
classification to discourse mode identification.
2.1 Situation entities
The categorization of SEs into aspectual classes is
motivated by patterns in their linguistic behavior.
We adopt an expanded version of a paradigm relat-
ing SEs to discourse mode (Smith, 2003) and char-
acterize SEs with four broad categories:
1. Eventualities. Events (E), particular states (S),
and reports (R). R is a sub-type of E for SEs
introduced by verbs of speech (e.g., say).
2. General statives. Generics (G) and generaliz-
ing sentences (GS). The former are utterances
predicated of a general class or kind rather than
of any specific individual. The latter are habit-
ual utterances that refer to ongoing actions or
properties predicated of specific individuals.
3. Abstract entities. Facts (F) and proposi-
tions (P).1
4. Speech-act types. Questions (Q) and impera-
tives (IMP).
Examples of each SE type are given in section 3.2.
There are a number of linguistic tests for iden-
tifying situation entities (Smith, 2003). The term
linguistic test refers to a rule which correlates an
SE type to particular linguistic forms. For exam-
ple, event-type verbs in simple present tense are a
linguistic correlate of GS-type SEs.
These linguistic tests vary in their precision and
different tests may predict different SE types for
the same clause. A rule-based implementation us-
ing them to classify SEs would require careful rule
ordering or mediation of rule conflicts. However,
since these rules are exactly the sort of information
extracted as features in data-driven classifiers, they
1In our system these two SE types are identified largely as
complements of factive and propositional verbs as discussed
in Peterson (1997). Fact and propositional complements have
some linguistic as well as some notional differences. Facts may
have causal effects, and facts are in the world. Neither of these
is true for propositions. In addition, the two have somewhat
different semantic consequences of a presuppositional nature.
can be cleanly integrated by assigning them empiri-
cally determined weights. We use maximum entropy
models (Berger et al, 1996), which are particularly
well-suited for tasks (like ours) with many overlap-
ping features, to harness these linguistic insights by
using features in our models which encode, directly
or indirectly, the linguistic correlates to SE types.
The features are described in detail in section 4.
2.2 Basic and derived situation types
Situation entities each have a basic situation type,
determined by the verb plus its arguments, the verb
constellation. The verb itself plays a key role in de-
termining basic situation type but it is not the only
factor. Changes in the arguments or tense of the verb
sometimes change the basic situation types:
(4) Mickey painted the house. (E)
(5) Mickey paints houses. (GS)
If SE type could be determined solely by the verb
constellation, automatic classification of SEs would
be a relatively straightforward task. However, other
parts of the clause often override the basic situation
type, resulting in aspectual coercion and a derived
situation type. For example, a modal adverb can
trigger aspectual coercion:
(6) Mickey probably paints houses. (P)
Serious challenges for SE classification arise from
the aspectual ambiguity and flexibility of many
predicates as well as from aspectual coercion.
2.3 Discourse modes
Much of the motivation of SE classification is
toward the broader goal of identifying discourse
modes, which provide a linguistic characterization
of textual passages according to the situation enti-
ties introduced. They correspond to intuitions as to
the rhetorical or semantic character of a text. Pas-
sages of written text can be classified into modes
of discourse ? Narrative, Description, Argument, In-
formation, and Report ? by examining concrete lin-
guistic cues in the text (Smith, 2003). These cues
are of two forms: the distribution of situation entity
types and the mode of progression (either temporal
or metaphorical) through the text.
897
For example, the Narration and Report modes
both contain mainly events and temporally bounded
states; they differ in their principles of temporal pro-
gression. Report passages progress with respect to
(deictic) speech time, whereas Narrative passages
progress with respect to (anaphoric) reference time.
Passages in the Description mode are predominantly
stative, and Argument mode passages tend to be
characterized by propositions and Information mode
passages by facts and states.
3 Data
This section describes the data sets used in the ex-
periments, the process for creating annotated train-
ing data, and preprocessing steps. Also, we give ex-
amples of the ten SE types.
There are no established data sets for SE classifi-
cation, so we created annotated training data to test
our models. We have annotated two data sets, one
from the Brown corpus and one based on data from
the Message Understanding Conference 6 (MUC6).
3.1 Segmentation
The Brown texts were segmented according to SE-
containing clausal boundaries, and each clause was
labeled with an SE label. Segmentation is itself a
difficult task, and we made some simplifications.
In general, clausal complements of verbs like say
which have clausal direct objects were treated as
separate clauses and given an SE label. Clausal com-
plements of verbs which have an entity as a direct
object and second clausal complement (such as no-
tify) were not treated as separate clauses. In addi-
tion, some modifying and adjunct clauses were not
assigned separate SE labels.
The MUC texts came to us segmented into ele-
mentary discourse units (EDUs), and each EDU was
labeled by the annotators. The two data sets were
segmented according to slightly different conven-
tions, and we did not normalize the segmentation.
The inconsistencies in segmentation introduce some
error to the otherwise gold-standard segmentations.
3.2 Annotation
Each text was independently annotated by two ex-
perts and reviewed by a third. Each clause was as-
signed precisely one SE label from the set of ten
possible labels. For clauses which introduce more
SE Text
S That compares with roughly paperback-book
dimensions for VHS.
G Accordingly, most VHS camcorders are usually
bulky and weigh around eight pounds or more.
S ?Carl is a tenacious fellow,?
R said a source close to USAir.
GS ?He doesn?t give up easily
GS and one should never underestimate what he can
or will do.?
S For Jenks knew
F that Bari?s defenses were made of paper.
E Mr. Icahn then proposed
P that USAir buy TWA,
IMP ?Fermate?!
R Musmanno bellowed to his Italian crewmen.
Q What?s her name?
S Quite seriously, the names mentioned as possibilities
were three male apparatchiks from the Beltway?s
Democratic political machine
N By Andrew B. Cohen Staff Reporter of The WSJ
Table 1: Example clauses and their SE annota-
tion. Horizontal lines separate extracts from differ-
ent texts.
than one SE, the annotators selected the most salient
one. This situation arose primarily when comple-
ment clauses were not treated as distinct clauses, in
which case the SE selected was the one introduced
by the main verb. The label N was used for clauses
which do not introduce any situation entity.
The Brown data set consists of 20 ?popular lore?
texts from section cf of the Brown corpus. Seg-
mentation of these texts resulted in a total of 4390
clauses. Of these, 3604 were used for training and
development, and 786 were held out as final test-
ing data. The MUC data set consists of 50 Wall
Street Journal newspaper articles segmented to a to-
tal of 1675 clauses. 137 MUC clauses were held
out for testing. The Brown texts are longer than
the MUC texts, with an average of 219.5 clauses
per document as compared to MUC?s average of
33.5 clauses. The average clause in the Brown data
contains 12.6 words, slightly longer than the MUC
texts? average of 10.9 words.
Table 1 provides examples of the ten SE types as
well as showing how clauses were segmented. Each
SE-containing example is a sequence of EDUs from
the data sets used in this study.
898
WWORDS words & punctuation
WT
W (see above)
POSONLY POS tag for each word
WORD/POS word/POS pair for each word
WTL
WT (see above)
FORCEPRED T if clause (or preceding clause)
contains force predicate
PROPPRED T if clause (or preceding clause)
contains propositional verb
FACTPRED T if clause (or preceding clause)
contains factive verb
GENPRED T if clause contains generic predicate
HASFIN T if clause contains finite verb
HASMODAL T if clause contains modal verb
FREQADV T if clause contains frequency adverb
MODALADV T if clause contains modal adverb
VOLADV T if clause contains volitional adverb
FIRSTVB lexical item and POS tag for first verb
WTLG
WTL (see above)
VERBS all verbs in clause
VERBTAGS POS tags for all verbs
MAINVB main verb of clause
SUBJ subject of clause (lexical item)
SUPER CCG supertag
Table 2: Feature sets for SE classification
3.3 Preprocessing
The linguistic tests for SE classification appeal to
multiple levels of linguistic information; there are
lexical, morphological, syntactic, categorial, and
structural tests. In order to access categorial and
structural information, we used the C&C2 toolkit
(Clark and Curran, 2004). It provides part-of-speech
tags and Combinatory Categorial Grammar (CCG)
(Steedman, 2000) categories for words and syntac-
tic dependencies across words.
4 Features
One of our goals in undertaking this study was to
explore the use of linguistically-motivated features
and deep syntactic features in probabilistic models
for SE classification. The nature of the task requires
features characterizing the entire clause. Here, we
describe our four feature sets, summarized in table 2.
The feature sets are additive, extending very basic
feature sets first with linguistically-motivated fea-
tures and then with deep syntactic features.
2svn.ask.it.usyd.edu.ap/trac/candc/wiki
4.1 Basic feature sets: W and WT
The WORDS (W) feature set looks only at the words
and punctuation in the clause. These features are
obtained with no linguistic processing.
WORDS/TAGS (WT) incorporates part-of-speech
(POS) tags for each word, number, and punctuation
mark in the clause and the word/tag pairs for each
element of the clause. POS tags provide valuable in-
formation about syntactic category as well as certain
kinds of shallow semantic information (such as verb
tense). The tags are useful for identifying verbs,
nouns, and adverbs, and the words themselves repre-
sent lexico-semantic information in the feature sets.
4.2 Linguistically-motivated feature set: WTL
The WORDS/TAGS/LINGUISTIC CORRELATES
(WTL) feature set introduces linguistically-
motivated features gleaned from the literature
on SEs; each feature encodes a linguistic cue that
may correlate to one or more SE types. These
features are not directly annotated; instead they are
extracted by comparing words and their tags for
the current and immediately preceding clauses to
lists containing appropriate triggers. The lists are
compiled from the literature on SEs.
For example, clauses embedded under predicates
like force generally introduce E-type SEs:
(7) I forced [John to run the race with me].
(8) * I forced [John to know French].
The feature force-PREV is extracted if a member
of the force-type predicate word list occurs in the
previous clause.
Some of the correlations discussed in the litera-
ture rely on a level of syntactic analysis not available
in the WTL feature set. For example, stativity of the
main verb is one feature used to distinguish between
event and state SEs, and particular verbs and verb
tenses have tendencies with respect to stativity. To
approximate the main verb without syntactic analy-
sis, WTL uses the lexical item of the first verb in the
clause and the POS tags of all verbs in the clause.
These linguistic tests are non-absolute, making
them inappropriate for a rule-based model. Our
models handle the defeasibility of these correlations
probabilistically, as is standard for machine learning
for natural language processing.
899
4.3 Addition of deep features: WTLG
The WORDS/TAGS/LINGUISTIC CORRE-
LATES/GRAMMATICAL RELATIONS (WTLG)
feature set uses a deeper level of syntactic analysis
via features extracted from CCG parse representa-
tions for each clause. This feature set requires an
additional step of linguistic processing but provides
a basis for more accurate classification.
WTL approximated the main verb by sloppily tak-
ing the first verb in the clause; in contrast, WTLG
uses the main verb identified by the parser. The
parser also reliably identifies the subject, which is
used as a feature.
Supertags ?CCG categories assigned to words?
provide an interesting class of features in WTLG.
They succinctly encode richer grammatical informa-
tion than simple POS tags, especially subcategoriza-
tion and argument types. For example, the tag S\NP
denotes an intransitive verb, whereas (S\NP)/NP
denotes a transitive verb. As such, they can be seen
as a way of encoding the verbal constellation and its
effect on aspectual classification.
5 Models
We consider two types of models for the automatic
classification of situation entities. The first, a la-
beling model, utilizes a maximum entropy model
to predict SE labels based on clause-level linguistic
features as discussed above. This model ignores the
discourse patterns that link multiple utterances. Be-
cause these patterns recur, a sequencing model may
be better suited to the SE classification task. Our
second model thus extends the first by incorporating
the previous n (0 ? n ? 6) labels as features.
Sequencing is standardly used for tasks like part-
of-speech tagging, which generally assume smaller
units to be both tagged and considered as context
for tagging. We are tagging at the clause level rather
than at the word level, but the structure of the prob-
lem is essentially the same. We thus adapted the
OpenNLP maximum entropy part-of-speech tagger3
(Hockenmaier et al, 2004) to extract features from
utterances and to tag sequences of utterances instead
of words. This allows the use of features of adjacent
clauses as well as previously-predicted labels when
making classification decisions.
3http://opennlp.sourceforge.net.
6 Experiments
In this section we give results for testing on Brown
data. All results are reported in terms of accu-
racy, defined as the percentage of correctly-labeled
clauses. Standard 10-fold cross-validation on the
training data was used to develop models and fea-
ture sets. The optimized models were then tested on
the held-out Brown and MUC data.
The baseline was determined by assigning S
(state), the most frequent label in both training sets,
to each clause. Baseline accuracy was 38.5% and
36.2% for Brown and MUC, respectively.
In general, accuracy figures for MUC are much
higher than for Brown. This is likely due to the fact
that the MUC texts are more consistent: they are all
newswire texts of a fairly consistent tone and genre.
The Brown texts, in contrast, are from the ?popular
lore? section of the corpus and span a wide range
of topics and text types. Nonetheless, the patterns
between the feature sets and use of sequence predic-
tion hold across both data sets; here, we focus our
discussion on the results for the Brown data.
6.1 Labeling results
The results for the labeling model appear in the two
columns labeled ?n=0? in table 3. On Brown, the
simple W feature set beats the baseline by 6.9% with
an accuracy of 45.4%. Adding POS information
(WT) boosts accuracy 4.5% to 49.9%. We did not
see the expected increase in performance from the
linguistically motivated WTL features, but rather a
slight decrease in accuracy to 48.9%. These features
may require a greater amount of training material to
be effective. Addition of deep linguistic information
with WTLG improved performance to 50.6%, a gain
of 5.2% over words alone.
6.2 Oracle results
To determine the potential effectiveness of sequence
prediction, we performed oracle experiments on
Brown by including previous gold-standard labels as
features. Figure 1 illustrates the results from ora-
cle experiments incorporating from zero to six pre-
vious gold-standard SE labels (the lookback). The
increase in performance illustrates the importance of
context in the identification of SEs and motivates the
use of sequence prediction.
900
42
44
46
48
50
52
54
56
58
60
0 1 2 3 4 5 6
Acc
Lookback
WWTWTLWTLG
Figure 1: Oracle results on Brown data.
6.3 Sequencing results
Table 3 gives the results of classification with the se-
quencing model on the Brown data. As with the la-
beling model, accuracy is boosted by WT and WTLG
feature sets. We see an unexpected degradation in
performance in the transition from WT to WTL.
The most interesting results here, though, are the
gains in accuracy from use of previously-predicted
labels as features for classification. When labeling
performance is relatively poor, as with feature set W,
previous labels help very little, but as labeling accu-
racy increases, previous labels begin to effect notice-
able increases in accuracy. For the best two feature
sets, considering the previous two labels raises the
accuracy 2.0% and 2.5%, respectively.
In most cases, though, performance starts to de-
grade as the model incorporates more than two pre-
vious labels. This degradation is illustrated in Fig-
ure 2. The explanation for this is that the model is
still very weak, with an accuracy of less than 54%
for the Brown data. The more previous predicted la-
bels the model conditions on, the greater the likeli-
hood that one or more of the labels is incorrect. With
gold-standard labels, we see a steady increase in ac-
curacy as we look further back, and we would need
a better performing model to fully take advantage of
knowledge of SE patterns in discourse.
The sequencing model plays a crucial role, partic-
ularly with such a small amount of training material,
and our results indicate the importance of local con-
text in discourse analysis.
42
44
46
48
50
52
54
0 1 2 3 4 5 6
WWTWTLWTLG
Figure 2: Sequencing results on Brown data.
BROWN Lookback (n)
0 1 2 3 4 5 6
W 45.4 45.2 46.1 46.6 42.8 43.0 42.4
WT 49.9 52.4 51.9 49.2 47.2 46.2 44.8
WTL 48.9 50.5 50.1 48.9 46.7 44.9 45.0
WTLG 50.6 52.9 53.1 48.1 46.4 45.9 45.7
Baseline 38.5
Table 3: SE classification results with sequencing
on Brown test set. Bold cell indicates accuracy at-
tained by model parameters that performed best on
development data.
6.4 Error analysis
Given that a single one of the ten possible labels
occurs for more than 35% of clauses in both data
sets, it is useful to look at the distribution of er-
rors over the labels. Table 4 is a confusion matrix
for the held-out Brown data using the best feature
set.4 The first column gives the label and number
of occurrences of that label, and the second column
is the accuracy achieved for that label. The next
two columns show the percentage of erroneous la-
bels taken by the labels S and GS. These two labels
are the most common labels in the development set
(38.5% and 32.5%). The final column sums the per-
centages of errors assigned to the remaining seven
labels. As one would expect, the model learns the
predominance of these two labels. There are a few
interesting points to make about this data.
First, 66% of G-type clauses are mistakenly as-
signed the label GS. This is interesting because
these two SE-types constitute the broader SE cat-
4Thanks to the anonymous reviewer who suggested this use-
ful way of looking at the data.
901
% Correct % Incorrect
Label Label S GS Other
S(278) 72.7 n/a 14.0 13.3
E(203) 50.7 37.0 11.8 0.5
GS(203) 44.8 46.3 n/a 8.9
R(26) 38.5 30.8 11.5 19.2
N(47) 23.4 31.9 23.4 21.3
G(12) 0.0 25.0 66.7 8.3
IMP(8) 0.0 75.0 25.0 0.0
P(7) 0.0 71.4 28.6 0.0
F(2) 0.0 100.0 0.0 0.0
Table 4: Confusion matrix for Brown held-out test
data, WTLG feature set, lookback n = 2. Numbers
in parentheses indicate how many clauses have the
associated gold standard label.
egory of generalizing statives. The distribution of
errors for R-type clauses points out another interest-
ing classification difficulty.5 Unlike the other cat-
egories, the percentage of false-other labels for R-
type clauses is higher than that of false-GS labels.
80% of these false-other labels are of type E. The
explanation for this is that R-type clauses are a sub-
type of the event class.
6.5 Genre effects in classification
Different text domains frequently have different
characteristic properties. Discourse modes are one
way of analyzing these differences. It is thus in-
teresting to compare SE classification when training
and testing material come from different domains.
Table 5 shows the performance on Brown when
training on Brown and/or MUC using the WTLG
feature set with simple labeling and with sequence
prediction with a lookback of two. A number of
things are suggested by these figures. First, the la-
beling model (lookback of zero), beats the baseline
even when training on out-of-domain texts (43.1%
vs. 38.5%), but this is unsurprisingly far below
training on in-domain texts (43.1% vs. 50.6%).
Second, while sequence prediction helps with in-
domain training (53.1% vs 50.6%), it makes no
difference with out-of-domain training (42.9% vs
43.1%). This indicates that the patterns of SEs in a
text do indeed correlate with domains and their dis-
course modes, in line with case-studies in the dis-
course modes theory (Smith, 2003). Finally, mix-
5Thanks to an anonymous reviewer for bringing this to our
attention.
lookback Brown test set
WTLG
train:Brown 0 50.6
2 53.1
train:MUC 0 43.1
2 42.9
train:all 0 50.4
2 49.5
Table 5: Cross-domain SE classification
ing out-of-domain training material with in-domain
material does not hurt labelling accuracy (50.4% vs
50.6%), but it does take away the gains from se-
quencing (49.5% vs 53.1%).
These genre effects are suggestive, but inconclu-
sive. A similar setup with much larger training and
testing sets would be necessary to provide a clearer
picture of the effect of mixed domain training.
7 Related work
Though we are aware of no previous work in SE
classification, others have focused on automatic de-
tection of aspectual and temporal data.
Klavans and Chodorow (1992) laid the founda-
tion for probabilistic verb classification with their
interpretation of aspectual properties as gradient and
their use of statistics to model the gradience. They
implement a single linguistic test for stativity, treat-
ing lexical properties of verbs as tendencies rather
than absolute characteristics.
Linguistic indicators for aspectual classification
are also used by Siegel (1999), who evaluates 14 in-
dicators to test verbs for stativity and telicity. Many
of his indicators overlap with our features.
Siegel and McKeown (2001) address classifica-
tion of verbs for stativity (event vs. state) and
for completedness (culminated vs. non-culminated
events). They compare three supervised and one un-
supervised machine learning systems. The systems
obtain relatively high accuracy figures, but they are
domain-specific, require extensive human supervi-
sion, and do not address aspectual coercion.
Merlo and Stevenson (2001) use corpus-based
thematic role information to identify and classify
unergative, unaccusative, and object-drop verbs.
Stevenson and Merlo note that statistical analysis
cannot and should not be separated from deeper lin-
guistic analysis, and our results support that claim.
902
The advantages of our approach are the broadened
conception of the classification task and the use of
sequence prediction to capture a wider context.
8 Conclusions
Situation entity classification is a little-studied but
important classification task for the analysis of dis-
course. We have presented the first data-driven mod-
els for SE classification, motivating the treatment of
SE classification as a sequencing task.
We have shown that linguistic correlations to sit-
uation entity type are useful features for proba-
bilistic models, as are grammatical relations and
CCG supertags derived from syntactic analysis of
clauses. Models for the task perform poorly given
very basic feature sets, but minimal linguistic pro-
cessing in the form of part-of-speech tagging im-
proves performance even on small data sets used for
this study. Performance improves even more when
we move beyond simple feature sets and incorpo-
rate linguistically-motivated features and grammat-
ical relations from deep syntactic analysis. Finally,
using sequence prediction by adapting a POS-tagger
further improves results.
The tagger we adapted uses beam search; this al-
lows tractable use of maximum entropy for each la-
beling decision but forgoes the ability to find the
optimal label sequence using dynamic programming
techniques. In contrast, Conditional Random Fields
(CRFs) (Lafferty et al, 2001) allow the use of max-
imum entropy to set feature weights with efficient
recovery of the optimal sequence. Though CRFs are
more computationally intensive, the small set of SE
labels should make the task tractable for CRFs.
In future, we intend to test the utility of SEs in dis-
course parsing, discourse mode identification, and
discourse relation projection.
Acknowledgments
This work was supported by the Morris Memorial
Trust Grant from the New York Community Trust.
The authors would like to thank Nicholas Asher,
Pascal Denis, Katrin Erk, Garrett Heifrin, Julie
Hunter, Jonas Kuhn, Ray Mooney, Brian Reese, and
the anonymous reviewers.
References
N. Asher. 1993. Reference to Abstract objects in Dis-
course. Kluwer Academic Publishers.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
G. Carlson and F. J. Pelletier, editors. 1995. The Generic
Book. University of Chicago Press, Chicago.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log?linear models. In Proceedings of ACL?
04, pages 104?111, Barcelona, Spain.
D. Dowty. 1979. Word Meaning and Montague Gram-
mar. Reidel, Dordrecht.
J. Hockenmaier, G. Bierner, and J. Baldridge. 2004. Ex-
tending the coverage of a CCG system. Research on
Language and Computation, 2:165?208.
J. L. Klavans and M. S. Chodorow. 1992. Degrees of
stativity: The lexical representation of verb aspect. In
Proceedings of COLING 14, Nantes, France.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labelling sequence data. In Proceedings
of ICML, pages 282?289, Williamstown, USA.
P. Merlo and S. Stevenson. 2001. Automatic verb clas-
sification based on statistical distributions of argument
structure. Computational Linguistics.
M. Moens and M. Steedman. 1988. Temporal ontol-
ogy and temporal reference. Computational Linguis-
tics, 14(2):15?28.
P. Peterson. 1997. Fact Proposition Event. Kluwer.
E. V. Siegel and K. R. McKeown. 2001. Learning meth-
ods to combine linguistic indicators: Improving as-
pectual classification and revealing linguistic insights.
Computational Linguistics, 26(4):595?628.
E. V. Siegel. 1999. Corpus-based linguistic indicators
for aspectual classification. In Proceedings of ACL37,
University of Maryland, College Park.
C. S. Smith. 1991. The Parameter of Aspect. Kluwer.
C. S. Smith. 2003. Modes of Discourse. Cambridge
University Press.
M. Steedman. 2000. The Syntactic Process. MIT
Press/Bradford Books.
Z. Vendler, 1967. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, New York.
H. Verkuyl. 1972. On the Compositional Nature of the
Aspects. Reidel, Dordrecht.
903
Proceedings of ACL-08: HLT, pages 326?334,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Logical Basis for the D Combinator and Normal Form in CCG
Frederick Hoyt and Jason Baldridge
The Department of Linguistics
The University of Texas at Austin
{fmhoyt,jbaldrid}@mail.utexas.edu
Abstract
The standard set of rules defined in Combina-
tory Categorial Grammar (CCG) fails to pro-
vide satisfactory analyses for a number of syn-
tactic structures found in natural languages.
These structures can be analyzed elegantly by
augmenting CCG with a class of rules based
on the combinator D (Curry and Feys, 1958).
We show two ways to derive the D rules:
one based on unary composition and the other
based on a logical characterization of CCG?s
rule base (Baldridge, 2002). We also show
how Eisner?s (1996) normal form constraints
follow from this logic, ensuring that the D
rules do not lead to spurious ambiguities.
1 Introduction
Combinatory Categorial Grammar (CCG, Steedman
(2000)) is a compositional, semantically transparent
formalism that is both linguistically expressive and
computationally tractable. It has been used for a va-
riety of tasks, such as wide-coverage parsing (Hock-
enmaier and Steedman, 2002; Clark and Curran,
2007), sentence realization (White, 2006), learning
semantic parsers (Zettlemoyer and Collins, 2007),
dialog systems (Kruijff et al, 2007), grammar engi-
neering (Beavers, 2004; Baldridge et al, 2007), and
modeling syntactic priming (Reitter et al, 2006).
A distinctive aspect of CCG is that it provides
a very flexible notion of constituency. This sup-
ports elegant analyses of several phenomena (e.g.,
coordination, long-distance extraction, and intona-
tion) and allows incremental parsing with the com-
petence grammar (Steedman, 2000). Here, we argue
that even with its flexibility, CCG as standardly de-
fined is not permissive enough for certain linguistic
constructions and greater incrementality. Following
Wittenburg (1987), we remedy this by adding a set
of rules based on the D combinator of combinatory
logic (Curry and Feys, 1958).
(1) x/(y/z) :f y/w :g ? x/(w/z) :?h.f(?x.ghx)
We show that CCG augmented with this rule im-
proves CCG?s empirical coverage by allowing better
analyses of modal verbs in English and causatives in
Spanish, and certain coordinate constructions.
The D rules are well-behaved; we show this by
deriving them both from unary composition and
from the logic defined by Baldridge (2002). Both
perspectives on D ensure that the new rules are com-
patible with normal form constraints (Eisner, 1996)
for controlling spurious ambiguity. The logic also
ensures that the new rules are subject to modalities
consistent with those defined by Baldridge and Krui-
jff (2003). Furthermore, we define a logic that pro-
duces Eisner?s constraints as grammar internal theo-
rems rather than parsing stipulations.
2 Combinatory Categorial Grammar
CCG uses a universal set of syntactic rules based on
the B, T, and S combinators of combinatory logic
(Curry and Feys, 1958):
(2) B: ((Bf)g)x = f(gx)
T: Txf = fx
S: ((Sf)g)x = fx(gx)
CCG functors are functions over strings of symbols,
so different linearized versions of each of the com-
binators have to be specified (ignoring S here):
326
(3) FA: (>) x/?y y ? x
(<) y x\?y ? x
B: (>B) x/y y/z ? x/z
(<B) y\z x\y ? x\z
(>B?) x/?y y\?z ? x\?z
(<B?) y/?z x\?y ? x/?z
T: (>T) x ? t/i(t\ix)
(<T) x ? t\i(t/ix)
The symbols {?, , ?, ?} are modalities that allow
subtypes of slashes to be defined; this in turn allows
the slashes on categories to be defined in a way that
allows them to be used (or not) with specific subsets
of the above rules. The rules of this multimodal ver-
sion of CCG (Baldridge, 2002; Baldridge and Krui-
jff, 2003) are derived as theorems of a Categorial
Type Logic (CTL, Moortgat (1997)).
This treats CCG as a compilation of CTL proofs,
providing a principled, grammar-internal basis for
restrictions on the CCG rules, transferring language-
particular restrictions on rule application to the lex-
icon, and allowing the CCG rules to be viewed
as grammatical universals (Baldridge and Kruijff,
2003; Steedman and Baldridge, To Appear).
These rules?especially the B rules?allow
derivations to be partially associative: given appro-
priate type assignments, a string ABC can be ana-
lyzed as either A(BC) or (AB)C. This associativity
leads to elegant analyses of phenomena that demand
more effort in less flexible frameworks. One of the
best known is ?odd constituent? coordination:
(4) Bob gave Stan a beer and Max a coke.
(5) I will buy and you will eat a cheeseburger.
The coordinated constituents are challenging be-
cause they are at odds with standardly assumed
phrase structure constituents. In CCG, such con-
stituents simply follow from the associativity added
by the B and T rules. For example, given the cate-
gory assignments in (6) and the abbreviations in (7),
(4) is analyzed as in (8) and (9). Each conjunct is
a pair of type-raised NPs combined by means of the
>B-rule, deriving two composed constituents that
are arguments to the conjunction:1
(6) i. Bob ` s/(s\np)
1We follow (Steedman, 2000) in assuming that type-raising
applies in the lexicon, and therefore that nominals such as Stan
ii. Stan, Max `
((s\np)/np)\(((s\np)/np)/np)
iii. a beer, a coke ` (s\np)\((s\np)/np)
iv. and ` (x\?x)/?x
v. gave ` ((s\np)/np)/np
(7) i. vp = s\np
ii. tv = (s\np)/np
iii. dtv = ((s\np)/np)/np
(8) Stan a beer and Max a coke
tv\dt vp\tv (x\?x)/?x tv\dt vp\tv
<B <B
vp\dt vp\dt
>
(vp\dt)\(vp\dt)
<
vp\dt
(9) Bill gave Stan a beer and Max a coke
s/vp dt vp\dt
<vp
>s
Similarly, I will buy is derived with category s/np
by assuming the category (6i) for I and composing
that with both verbs in turn.
CCG?s approach is appealing because such con-
stituents are not odd at all: they simply follow from
the fact that CCG is a system of type-based gram-
matical inference that allows left associativity.
3 Linguistic Motivation for D
CCG is only partially associative. Here, we discuss
several situations which require greater associativity
and thus cannot be given an adequate analysis with
CCG as standardly defined. These structures have
in common that a category of the form x|(y|z) must
combine with one of the form y|w?exactly the con-
figuration handled by the D schemata in (1).
3.1 Cross-Conjunct Extraction
In the first situation, a question word is distributed
across auxiliary or subordinating verb categories:
(10) . . .what you can and what you must not base
your verdict on.
We call this cross-conjunct extraction. It was noted
by Pickering and Barry (1993) for English, but to the
best of our knowledge it has not been treated in the
have type-raised lexical assignments. We also suppress seman-
tic representations in the derivations for the sake of space.
327
CCG literature, nor noted in other languages. The
problem it presents to CCG is clear in (11), which
shows the necessary derivation of (10) using stan-
dard multimodal category assignments. For the to-
kens of what to form constituents with you can and
you must not, they must must combine directly. The
problem is that these constituents (in bold) cannot be
created with the standard CCG combinators in (3).
(11) s
s/(vp/np)
s/(vp/np)
s/(s/np)
what
s/vp
you can
(s/(vp/np))\(s/(vp/np))
(x\?x)/?x
and
s/(vp/np)
s/(s/np)
what
s/vp
you must not
vp/np
base your verdict on
The category for and is marked for non-associativity
with ?, and thus combines with other expressions
only by function application (Baldridge, 2002). This
ensures that each conjunct is a discrete constituent.
Cross-conjunct extraction occurs in other lan-
guages as well, including Dutch (12), German (13),
Romanian (14), and Spanish (15):
(12) dat
that
ik
I
haar
her
wil
want
en
and
dat
that
ik
I
haar
her
moet
can
helpen.
help
?. . . that I want to and that I can help her.?
(13) Wen
who
kann
can
ich
I
und
and
wen
who
darf
may
ich
I
noch
still
w?hlen?
choose
?Whom can I and whom may I still chose??
(14) Gandeste-te
consider.imper.2s-refl.2s
cui
who.dat
?e
what
vrei,
want.2s
s?i
and
cui
who.dat
?e
what
pot?i,
can.2s
sa?
to
dai.
give.subj.2s
?Consider to whom you want and to whom you
are able to give what.?
(15) Me
me
lo
it
puedes
can.2s
y
and
me
me
lo
it
debes
must.2s
explicar
ask
?You can and should explain it to me.?
It is thus a general phenomenon, not just a quirk
of English. While it could be handled with extra cat-
egories, such as (s/(vp/np))/(s/np) for what, this is
exactly the sort of strong-arm tactic that inclusion of
the standard B, T, and S rules is meant to avoid.
3.2 English Auxiliary Verbs
The standard CCG analysis for English auxiliary
verbs is the type exemplified in (16) (Steedman,
2000, 68), interpreted as a unary operator over sen-
tence meanings (Gamut, 1991; Kratzer, 1991):
(16) can ` (s\np)/(s\np) : ?P et?x.?P (x)
However, this type is empirically underdetermined,
given a widely-noted set of generalizations suggest-
ing that auxiliaries and raising verbs take no subject
argument at all (Jacobson, 1990, a.o.).
(17) i. Lack of syntactic restrictions on the subject;
ii. Lack of semantic restrictions on the subject;
iii. Inheritance of selectional restrictions from the
subordinate predicate.
Two arguments are made for (16). First, it is nec-
essary so that type-raised subjects can compose with
the auxiliary in extraction contexts, as in (18):
(18) what I can eat
s/(s/np) s/vp vp/vp tv
>B
s/vp
>B
s/np
>s
Second, it is claimed to be necessary in order to ac-
count for subject-verb agreement, on the assumption
that agreement features are domain restrictions on
functors of type s\np (Steedman, 1992, 1996).
The first argument is the topic of this paper, and,
as we show below, is refuted by the use of the D-
combinator. The second argument is undermined by
examples like (19):
(19) There appear to have been [ neither [ any catas-
trophic consequences ], nor [ a drastic change in
the average age of retirement ] ] .
In (19), appear agrees with two negative-polarity-
sensitive NPs trapped inside a neither-nor coordi-
nate structure in which they are licensed. Ap-
pear therefore does not combine with them directly,
showing that the agreement relation need not be me-
diated by direct application of a subject argument.
We conclude, therefore, that the assignment of the
vp/vp type to English auxiliaries and modal verbs is
unsupported on both formal and linguistic grounds.
Following Jacobson (1990), a more empirically-
motivated assignment is (20):
328
(20) can ` s/s : ?pt .?p
Combining (20) with a type-raised subject presents
another instance of the structure in (1), where that
question words are represented as variable-binding
operators (Groenendijk and Stokhof, 1997):
(21) what I can
s/(s/np) : ?Qet?yQy s/vp : ?P et .P i? s/s : ?pt .?p
? ? ? >B ? ??
3.3 The Spanish Causative Construction
The schema in (1) is also found in the widely-
studied Romance causative construction (Andrews
and Manning, 1999, a.m.o), illustrated in (22):
(22) Nos
cl.1p
hizo
made.3s
leer
read
El
the
Se?or
Lord
de
of
los
the
Anillos.
Rings
?He made us read The Lord of the Rings.?
The aspect of the construction that is relevant here
is that the causative verb hacer appears to take an
object argument understood as the subject or agent
of the subordinate verb (the causee). However, it has
been argued that Spanish causative verbs do not in
fact take objects (Ackerman and Moore, 1999, and
refs therein). There are two arguments for this.
First, syntactic alternations that apply to object-
taking verbs, such as passivization and periphrasis
with subjunctive complements, do not apply to hacer
(Luj?n, 1980). Second, hacer specifies neither the
case form of the causee, nor any semantic entail-
ments with respect to it. These are instead deter-
mined by syntactic, semantic, and pragmatic factors,
such as transitivity, word order, animacy, gender, so-
cial prestige, and referential specificity (Finnemann,
1982, a.o). Thus, there is neither syntactic nor se-
mantic evidence that hacer takes an object argument.
On this basis, we assign hacer the category (23):
(23) hacer ` (s\np)/s : ?P?x.cause?Px
However, Spanish has examples of cross-conjunct
extraction in which hacer hosts clitics:
(24) No
not
solo
only
le
cl.dat.3ms
ordenaron,
ordered.3p
sino que
but
le
cl.dat.3ms
hicieron
made.3p
barrer
sweep
la
the
verada.
sidewalk
?They not only ordered him to, but also made him
sweep the sidewalk.?
This shows another instance of the schema in (1),
which is undefined for any of the combinators in (3):
(25) le hicieron barrer la verada
(s\np)/((s\np)/np) (s\np)/s (s|np)
? ? ? >B ? ??
3.4 Analyses Based on D
The preceding data motivates adding D rules (we re-
turn to the distribution of the modalities below):
(26) >D x/(y/z) y/w ? x/(w/z)
>D? x/?(y/?z) y\?w ? x\?(w/?z)
>D? x/(y\?z) y/?w ? x/(w\?z)
>D? x/?(y\z) y\?w ? x\?(w\z)
(27) <D y\w x\(y\z) ? x\(w\z)
<D? y/?w x\?(y\?z) ? x/?(w\?z)
<D? y\?w x\(y/?z) ? x\(w/?z)
<D? y/?w x\?(y/z) ? x/?(w/z)
To illustrate with example (10), one application of
>D allows you and can to combine when the auxil-
iary is given the principled type assignment s/s, and
another combines what with the result.
(28) what you can
s/(s/np) s/(s\?np) s/?s
>D?
s/(s\?np)
>D
s/((s\?np)/np)
The derivation then proceeds in the usual way.
Likewise, D handles the Spanish causative con-
structions (29) straightforwardly :
(29) lo hice dormir
(s\np)/((s\np)/np) (s\np)/s s/np
>D
(s\np)/(s/np)
>
s\np
The D-rules thus provide straightforward analy-
ses of such constructions by delivering flexible con-
stituency while maintaining CCG?s committment to
low categorial ambiguity and semantic transparency.
4 Deriving Eisner Normal Form
Adding new rules can have implications for parsing
efficiency. In this section, we show that the D rules
fit naturally within standard normal form constraints
for CCG parsing (Eisner, 1996), by providing both
329
combinatory and logical bases for D. This addition-
ally allows Eisner?s normal form constraints to be
derived as grammar internal theorems.
4.1 The Spurious Ambiguity Problem
CCG?s flexibility is useful for linguistic analy-
ses, but leads to spurious ambiguity (Wittenburg,
1987) due to the associativity introduced by the
B and T rules. This can incur a high compu-
tational cost which parsers must deal with. Sev-
eral techniques have been proposed for the prob-
lem (Wittenburg, 1987; Karttunen, 1989; Hepple
and Morrill, 1989; Eisner, 1996). The most com-
monly used are Karttunnen?s chart subsumption
check (White and Baldridge, 2003; Hockenmaier
and Steedman, 2002) and Eisner?s normal-form con-
straints (Bozsahin, 1998; Clark and Curran, 2007).
Eisner?s normal form, referred to here as Eisner
NF and paraphrased in (30), has the advantage of not
requiring comparisons of logical forms: it functions
purely on the syntactic types being combined.
(30) For a set S of semantically equivalent2 parse trees
for a string ABC, admit the unique parse tree such
that at least one of (i) or (ii) holds:
i. C is not the argument of (AB) resulting from
application of >B1+.
ii. A is not the argument of (BC) resulting from
application of <B1+.
The implication is that outputs of B1+ rules are
inert, using the terminology of Baldridge (2002).
Inert slashes are Baldridge?s (2002) encoding in
OpenCCG3 of his CTL interpretation of Steedman?s
(2000) antecedent-government feature.
Eisner derives (30) from two theorems about the
set of semantically equivalent parses that a CCG
parser will generate for a given string (see (Eisner,
1996) for proofs and discussion of the theorems):
(31) Theorem 1 : For every parse tree ?, there is a se-
mantically equivalent parse-tree NF (?) in which
no node resulting from application of B or S func-
tions as the primary functor in a rule application.
(32) Theorem 2 : If NF (?) and NF (??) are distinct
parse trees, then their model-theoretic interpreta-
tions are distinct.
2Two parse trees are semantically equivalent if: (i) their leaf
nodes have equivalent interpretations, and (ii) equivalent scope
relations hold between their respective leaf-node meanings.
3http://openccg.sourceforge.net
Eisner uses a generalized form Bn (n?0) of compo-
sition that subsumes function application:4
(33) >Bn : x/y y$n ? x$n
(34) <Bn : y$n x\y ? x$n
Based on these theorems, Eisner defines NF as fol-
lows (for R, S, T as Bn or S, and Q=Bn?1 ):
(35) Given a parse tree ?:
i. If ? is a lexical item, then ? is in Eisner-NF.
ii. If ? is a parse tree ?R, ?, ?? and NF (?),
NF (?), then NF (?).
iii. If ? is not in Eisner-NF, then
NF (?) = ?Q, ?1 , ?2 ?, and
NF (?) = ?S, ?1 , NF (?T, ?2 , ??)?.
As a parsing constraint, (30) is a filter on the set
of parses produced for a given string. It preserves all
the unique semantic forms generated for the string
while eliminating all spurious ambiguities: it is both
safe and complete.
Given the utility of Eisner NF for practical CCG
parsing, the D rules we propose should be compati-
ble with (30). This requires that the generalizations
underlying (30) apply to D as well. In the remainder
of this section, we show this in two ways.
4.2 Deriving D from B
The first is to derive the binary B rules from a unary
rule based on the unary combinator B?:5
(36) x/y : fxy ? (x/z)/(y/z) : ?hzy?xz .f(hx)
We then derive D from B? and show that clause (iii)
of (35) holds of Q schematized over both B and D.
Applying D to an argument sequence is equiva-
lent to compound application of binary B:
(37) (((Df)g)h)x = (fg)(hx)
(38) ((((BB)f)g)h)x = ((B(fg))h)x = (fg)(hx)
Syntactically, binary B is equivalent to application
of unary B? to the primary functor ?, followed by
applying the secondary functor ? to the output of B?
by means of function application (Jacobson, 1999):
4We use Steedman?s (Steedman, 1996) ?$?-convention for
representing argument stacks of length n, for n ? 0.
5This is Lambek?s (1958) Division rule, also known as the
?Geach rule? (Jacobson, 1999).
330
(39) ? ?
x/y y/z
>B?
(x/z)/(y/z)
>
x/z
Bn (n ? 1) is derived by applying B? to the primary
functor n times. For example, B2 is derived by 2
applications of B? to the primary functor:
(40) ? ?
x/y (y/w)/z
B?
(x/w)/(y/w)
B?
((x/w)/z)/((y/w)/z)
>
(x/w)/z
The rules for D correspond to application of B? to
both the primary and secondary functors, followed
by function application:
(41) ? ?
x/(y/z) y/w
>B? >B?
(x/(w/z))/((y/z)/(w/z)) (y/z)/(w/z)
>
x/(w/z)
As with Bn , Dn?1 can be derived by iterative appli-
cation of B? to both primary and secondary functors.
Because B can be derived from B?, clause (iii) of
(35) is equivalent to the following:
(42) If ? is not in Eisner-NF, then
NF (?) = ?FA, ?B?, ?1 ?, ?2 ?, such that
NF (?) = ?S, ?1 , NF (?T, ?2 , ??)?
Interpreted in terms of B?, both B and D involve ap-
plication of B? to the primary functor. It follows that
Theorem I applies directly to D simply by virtue of
the equivalence between binary B and unary-B?+FA.
Eisner?s NF constraints can then be reinterpreted
as a constraint on B? requiring its output to be an inert
result category. We represent this in terms of the B?-
rules introducing an inert slash, indicated with ?!?
(adopting the convention from OpenCCG):
(43) x/y : fxy ? (x/!z)/(y/!z) : ?hzy?xzfhx
Hence, both binary B and D return inert functors:
(44) ? ?
x/y y/z
>B?
(x/!z)/(y/!z)
>
x/!z
(45) ? ?
x/(y/z) y/w
>B? >B?
(x/!(w/z))/((y/z)/!(w/z)) (y/!z)/(w/!z)
>
x/!(w/z)
The binary substitution (S) combinator can be
similarly incorporated into the system. Unary sub-
stitution S? is like B? except that it introduces a slash
on only the argument-side of the input functor. We
stipulate that S? returns a category with inert slashes:
(46) (S?) (x/y)/z ? (x/!z)/(y/!z)
T is by definition unary. It follows that all the binary
rules in CCG (including the D-rules) can be reduced
to (iterated) instantiations of the unary combinators
B?, S?, or T plus function application.
This provides a basis for CCG in which all com-
binatory rules are derived from unary B? S?, and T.
4.3 A Logical Basis for Eisner Normal Form
The previous section shows that deriving CCG rules
from unary combinators allows us to derive the D-
rules while preserving Eisner NF. In this section, we
present an alternate formulation of Eisner NF with
Baldridge?s (2002) CTL basis for CCG. This for-
mulation allows us to derive the D-rules as before,
and does so in a way that seamlessly integrates with
Baldridge?s system of modalized functors.
In CTL, B and B? are proofs derived via struc-
tural rules that allow associativity and permutation
of symbols within a sequent, in combination with
the slash introduction and elimination rules of the
base logic. To control application of these rules,
Baldridge keys them to binary modal operators  (for
associativity) and ? (for permutation). Given these,
>B is proven in (47):
(47) ? ` x/y ? ` y/z [a ` z]
[/E]
(? ? ai) ` y
[/E]
(? ? (? ? ai)) ` x
[RA]
((? ? ?) ? ai) ` x
[/I]
(? ? ?) ` x/z
In a CCG ruleset compiled from such logics, a
category must have an appropriately decorated slash
in order to be the input to a rule. This means that
rules apply universally, without language-specific
331
restrictions. Instead, restrictions can only be de-
clared via modalities marked on lexical categories.
Unary B? and the D rules in 4.2 can be derived us-
ing the same logic. For example, >B? can be derived
as in (48):
(48) ? ` x/y [f ` y/z]1 [a ` z]2
[/E]
(f1 ? a2 ) ` y
[/E]
(? ? (f1 ? a2 )) ` x
[RA]
((? ? f1 ) ? a2 ) ` x
[/I]
(? ? f1 ) ` x/z
[/I]
? ` (x/z)/(y/z)
The D rules are also theorems of this system. For
example, the proof for >D applies (48) as a lemma
to each of the primary and secondary functors:
(49) ? ` x/(y/z) ? ` y/w
>B? >B?
? ` (x/(w/z))/((y/z)/(w/z)) ? ` (y/z)/(w/z)
[/E]
(? ? ?) ` x/(w/z)
>D? involves an associative version of B? applied
to the primary functor (50), and a permutative ver-
sion to the secondary functor (51).
(50) ? ` x/(y\?z) [f ` (y\?z)/?(w\?z)]1 [g ` w\?z]2
[/?E]
(f1 ?? g2 ) ` y\?z
[/E]
(? ? (f1 ?. g2 )) ` x
[RA]
((? ? f1 ) ?. g2 ) ` x
[/?I]
(? ? f1 ) ` x/?(w\?z)
[/I]
? ` (x/?(w\?z))/((y\?z)/?(w\?z))
(51) ? ` y/?w [a ` z]1 [f ` w\?z]2
[\?E]
(a1 ?? f2 ) ` w
[/?E]
(? ?? (a1 ?? f2 )) ` y
[LP ]
(a1 ?? (? ?? f2 )) ` y
[\?I]
(? ?? f2 ) ` y\?z
[/?I]
? ` (y\?z)/?(w\?z)
Rules for D with appropriate modalities can there-
fore be incorporated seamlessly into CCG.
In the preceding subsection, we encoded Eisner
NF with inert slashes. In Baldridge?s CTL basis
for CCG, inert slashes are represented as functors
seeking non-lexical arguments, represented as cate-
gories marked with an antecedent-governed feature,
reflecting the intuition that non-lexical arguments
have to be ?bound? by a superordinate functor.
This is based on an interpretation of antecedent-
government as a unary modality ?ant that allows
structures marked by it to permute to the left or right
periphery of a structure:6
(52) ((?a ?? ?ant?b) ?? ?c) ` x
((?a ?? ?c) ?? ?ant?b) ` x
[ARP]
(?a ?? (?ant?b ?? ?c)) ` x
(?ant?b ?? (?a ?? ?c)) ` x
[ALP]
Unlike permutation rules without ?ant , these per-
mutation rules can only be used in a proof when
preceeded by a hypothetical category marked with
the 2?ant modality. The elimination rule for 2
?-
modalities introduces a corresponding ?-marked
object in the resulting structure, feeding the rule:
(53) [a ` 2?antz]
1
[2?E]
?anta1 ` z ? ` y\?z
[\?E]
? ` x/?y (?anta1 ?? ?) ` y
[/?E]
(? ?? (?anta1 ?? ?)) ` x
[ALP ]
[a ` ?ant2
?
antz]
2 (?anta1 ?? (? ?? ?)) ` x
[?E]
(a ?? (? ?? ?)) ` x
[\?I]
2
(? ?? ?) ` x\??ant2
?
antz
Re-introduction of the [a ` ?ant2
?
antz]
k hypothesis
results in a functor the argument of which is marked
with ?ant2
?
ant . Because lexical categories are not
marked as such, the functor cannot take a lexical ar-
gument, and so is effectively an inert functor.
In Baldridge?s (2002) system, only proofs involv-
ing the ARP and ALP rules produce inert categories.
In Eisner NF, all instances of B-rules result in inert
categories. This can be reproduced in Baldridge?s
system simply by keying all structural rules to the
ant-modality, the result being that all proofs involv-
ing structural rules result in inert functors.
As desired, the D-rules result in inert categories as
well. For example, >D is derived as follows (2?ant
and ?ant are abbreviated as 2? and ?):
6Note that the diamond operator used here is a syntactic op-
erator, rather than a semantic operator as used in (16) above.
The unary modalities used in CTL describe accessibility rela-
tionships between subtypes and supertypes of particular cate-
gories: in effect, they define feature hierarchies. See Moortgat
(1997) and Oehrle (To Appear) for further explanation.
332
(54) ? ` y/w [a ` 2?(w/z)]1 [b ` 2?z]2
[2?E] [2?E]
?a ` w/z ?b ` z
[/E]
(?a ? ?b) ` w
[/E]
(? ? (?a ? ?b)) ` y
[RA]
[c ` ?2?z]3 ((? ? ?a) ? ?b) ` y
[?E]2
((? ? ?a) ? c) ` y
[/I]3
(? ? ?a) ` y/?2?z
(55) (54)
...
? ` x/(y/?2?z) (? ? ?a) ` y/?2?z
[/E]
(? ? (? ? ?a)) ` x
[RA]
[d ` ?2?(w/z)]4 ((? ? ?) ? ?a) ` x
[?E]1
((? ? ?) ? d) ` x
[/I]4
(? ? ?) ` x/?2?(w/z)
(54)-(55) can be used as a lemma corresponding to
the CCG rule in (57):
(56) ? ` x/(y/?2?z) ? ` y/w
[D]
(? ? ?) ` x/?2?(w/z)
(57) x/(y/!z) y/w ? x/!(w/z)
This means that all CCG rules compiled from the
logic?which requires ?ant to licence the structural
rules necessary to prove the rules?return inert func-
tors. Eisner NF thus falls out of the logic because all
instances of B, D, and S produce inert categories.
This in turns allows us to view Eisner NF as part of
a theory of grammatical competence, in addition to
being a useful technique for constraining parsing.
5 Conclusion
Including the D-combinator rules in the CCG rule
set lets us capture several linguistic generalizations
that lack satisfactory analyses in standard CCG.
Furthermore, CCG augmented with D is compat-
ible with Eisner NF (Eisner, 1996), a standard
technique for controlling derivational ambiguity in
CCG-parsers, and also with the modalized version
of CCG (Baldridge and Kruijff, 2003). A conse-
quence is that both the D rules and the NF con-
straints can be derived from a grammar-internal per-
spective. This extends CCG?s linguistic applicabil-
ity without sacrificing efficiency.
Wittenburg (1987) originally proposed using rules
based on D as a way to reduce spurious ambiguity,
which he achieved by eliminating B rules entirely
and replacing them with variations on D. Witten-
burg notes that doing so produces as many instances
of D as there are rules in the standard rule set. Our
proposal retains B and S, but, thanks to Eisner NF,
eliminates spurious ambiguity, a result that Witten-
burg was not able to realize at the time.
Our approach can be incorporated into Eisner NF
straightforwardly However, Eisner NF disprefers in-
cremental analyses by forcing right-corner analyses
of long-distance dependencies, such as in (58):
(58) (What (does (Grommet (think (Tottie (said (Victor
(knows (Wallace ate)))))))))?
For applications that call for increased incremental-
ity (e.g., aligning visual and spoken input incremen-
tally (Kruijff et al, 2007)), CCG rules that do not
produce inert categories can be derived a CTL ba-
sis that does not require ?ant for associativity and
permutation. The D-rules derived from this kind of
CTL specification would allow for left-corner analy-
ses of such dependencies with the competence gram-
mar. An extracted element can ?wrap around? the
words intervening between it and its extraction site.
For example, D would allow the following bracket-
ing for the same example (while producing the same
logical form):
(59) (((((((((What does) Grommet) think) Tottie) said)
Victor) knows) Wallace) ate)?
Finally, the unary combinator basis for CCG pro-
vides an interesting additional specification for gen-
erating CCG rules. Like the CTL basis, the unary
combinator basis can produce a much wider range
of possible rules, such as D rules, that may be rel-
evant for linguistic applications. Whichever basis
is used, inclusion of the D-rules increases empirical
coverage, while at the same time preserving CCG?s
computational attractiveness.
Acknowledgments
Thanks Mark Steedman for extensive comments and
suggestions, and particularly for noting the relation-
ship between the D-rules and unary B?. Thanks
also to Emmon Bach, Cem Bozsahin, Jason Eisner,
Geert-Jan Kruijff and the ACL reviewers.
333
References
Farrell Ackerman and John Moore. 1999. Syntagmatic
and Paradigmatic Dimensions of Causee Encodings.
Linguistics and Philosophy, 24:1?44.
Avery D. Andrews and Christopher D. Manning. 1999.
Complex Predicates and Information Spreading in
LFG. CSLI Publications, Palo Alto, California.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proceed-
ings of EACL 10, pages 211?218.
Jason Baldridge, Sudipta Chatterjee, Alexis Palmer, and
Ben Wing. 2007. DotCCG and VisCCG: Wiki and
Programming Paradigms for Improved Grammar En-
gineering with OpenCCG. In Proceedings of GEAF
2007.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
John Beavers. 2004. Type-inheritance Combinatory
Categorial Grammar. In Proceedings of COLING-04,
Geneva, Switzerland.
Robert Borsley and Kersti B?rjars, editors. To Appear.
Non-Transformational Syntax: A Guide to Current
Models. Blackwell.
Cem Bozsahin. 1998. Deriving the Predicate-Argument
Structure for a Free Word Order Language. In Pro-
ceedings of COLING-ACL ?98.
Stephen Clark and James Curran. 2007. Wide-Coverage
Efficient Statistical Parsing with CCG and Log-Linear
Models. Computational Linguistics, 33(4).
Haskell B. Curry and Robert Feys. 1958. Combinatory
Logic, volume 1. North Holland, Amsterdam.
Jason Eisner. 1996. Efficient Normal-Form Parsing for
Combinatory Categorial Grammars. In Proceedings of
the ACL 34.
Michael D Finnemann. 1982. Aspects of the Spanish
Causative Construction. Ph.D. thesis, University of
Minnesota.
L. T. F. Gamut. 1991. Logic, Language, and Meaning,
volume II. Chicago University Press.
Jeroen Groenendijk and Martin Stokhof. 1997. Ques-
tions. In Johan van Benthem and Alice ter Meulen,
editors, Handbook of Logic and Language, chapter 19,
pages 1055?1124. Elsevier Science, Amsterdam.
Mark Hepple and Glyn Morrill. 1989. Parsing and
Derivational Equivalence. In Proceedings of EACL 4.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative Models for Statistical Parsing with Combina-
tory Categorial Grammar. In Proceedings. of ACL 40,
pages 335?342, Philadelpha, PA.
Pauline Jacobson. 1990. Raising as Function Composi-
tion. Linguistics and Philosophy, 13:423?475.
Pauline Jacobson. 1999. Towards a Variable-Free Se-
mantics. Linguistics and Philosophy, 22:117?184.
Lauri Karttunen. 1989. Radical Lexicalism. In Mark
Baltin and Anthony Kroch, editors, Alternative Con-
ceptions of Phrase Structure. University of Chicago
Press, Chicago.
Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Semantic
Research, pages 639?650. Walter de Gruyter, Berlin.
Geert-Jan M. Kruijff, Pierre Lison, Trevor Benjamin,
Henrik Jacobsson, and Nick Hawes. 2007. Incremen-
tal, Multi-Level Processing for Comprehending Situ-
ated Dialogue in Human-Robot Interaction. In Lan-
guage and Robots: Proceedings from the Symposium
(LangRo?2007), Aveiro, Portugal.
Joachim Lambek. 1958. The mathematics of sentence
structure. American Mathematical Monthly, 65:154?
169.
Marta Luj?n. 1980. Clitic Promotion and Mood in Span-
ish Verbal Complements. Linguistics, 18:381?484.
Michael Moortgat. 1997. Categorial Type Logics. In Jo-
han van Benthem and Alice ter Meulen, editors, Hand-
book of Logic and Language, pages 93?177. North
Holland, Amsterdam.
Richard T Oehrle. To Appear. Multi-Modal Type Log-
ical Grammar. In Boersley and B?rjars (Borsley and
B?rjars, To Appear).
Martin Pickering and Guy Barry. 1993. Dependency
Categorial Grammar and Coordination. Linguistics,
31:855?902.
David Reitter, Julia Hockenmaier, and Frank Keller.
2006. Priming Effects in Combinatory Categorial
Grammar. In Proceedings of EMNLP-2006.
Mark Steedman and Jason Baldridge. To Appear. Com-
binatory Categorial Grammar. In Borsley and B?rjars
(Borsley and B?rjars, To Appear).
Mark Steedman. 1996. Surface Structure and Interpre-
tation. MIT Press.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Michael White and Jason Baldridge. 2003. Adapting
Chart Realization to CCG. In Proceedings of ENLG.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
Kent Wittenburg. 1987. Predictive Combinators: A
Method for Efficient Processing of Combinatory Cat-
egorial Grammars. In Proceedings of ACL 25.
Luke Zettlemoyer and Michael Collins. 2007. On-
line Learning of Relaxed CCG Grammars for Parsing
to Logical Form. In Proceedings of EMNLP-CoNLL
2007.
334
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Teaching computational linguistics to a large, diverse student body:
courses, tools, and interdepartmental interaction
Jason Baldridge and Katrin Erk
Department of Linguistics
The University of Texas at Austin
{jbaldrid,erk}@mail.utexas.edu
Abstract
We describe course adaptation and develop-
ment for teaching computational linguistics
for the diverse body of undergraduate and
graduate students the Department of Linguis-
tics at the University of Texas at Austin. We
also discuss classroom tools and teaching aids
we have used and created, and we mention
our efforts to develop a campus-wide compu-
tational linguistics program.
1 Introduction
We teach computational linguistics courses in the
linguistics department of the University of Texas
at Austin, one of the largest American universi-
ties. This presents many challenges and opportu-
nities; in this paper, we discuss issues and strate-
gies for designing courses in our context and build-
ing a campus-wide program.1 The main theme of
our experience is that courses should be targeted to-
ward specific groups of students whenever possible.
This means identifying specific needs and design-
ing the course around them rather than trying to sat-
isfy a diverse set of students in each course. To this
end, we have split general computational linguistics
courses into more specific ones, e.g., working with
corpora, a non-technical overview of language tech-
nology applications, and natural language process-
ing. In section 2, we outline how we have stratified
our courses at both the graduate and undergraduate
levels.
1Links to the courses, tools, and resources described in this
paper can be found on our main website:
http://comp.ling.utexas.edu
As part of this strategy, it is crucial to ensure that
the appropriate student populations are reached and
that the courses fulfill degree requirements. For ex-
ample, our Language and Computers course fulfills
a Liberal Arts science requirement and our Natural
Language Processing is cross-listed with computer
science. This is an excellent way to get students in
the door and ensure that courses meet or exceed min-
imum enrollments. We find that many get hooked
and go on to specialize in computational linguistics.
Even for targeted CL courses, there is still usually
significant diversity of backgrounds among the stu-
dents taking them. Thus, it is still important to care-
fully consider the teaching tools that are used; in sec-
tion 3, we discuss our experience with several stan-
dard tools and two of our own. Finally, we describe
our efforts to build a campus-wide CL program that
provides visibility for CL across the university and
provides coherence in our course offerings.
2 Courses
Our courses are based on those initiated by Jonas
Kuhn between 2002 and 2005. Since 2005, we have
created several spin-off courses for students with
different backgrounds. Our broad goals for these
courses are to communicate both the practical util-
ity of computational linguistics and its promise for
improving our understanding of human languages.
2.1 Graduate
We started with two primary graduate courses, Com-
putational Linguistics I and II. The first introduces
central algorithms and data structures in computa-
tional linguistics, while the second focuses on learn-
1
Figure 1: Flow for non-seminar courses. Left: graduate
courses, right: undergraduate courses.
ing and disambiguation. This served a computation-
ally savvy segment of the student population quite
well. However, we view one of our key teaching
contributions as computational linguists in a linguis-
tics department to be providing non-computational
students with technical and formal skills useful for
their research. We discovered quickly that our first
computational linguistics course did not fill these
needs, and the second is not even accessible to most
students. The graduate linguistics students did put
in the effort to learn Python for Computational Lin-
guistics I, but many would have preferred a (much)
gentler introduction and also more coverage of is-
sues connected to their linguistic concerns. This led
us to create a new course, Working with Corpora.
Still, there is a need for the primary courses,
which receive interest from many students in com-
puter science and also graduate students from other
departments such as German and English. One of
the great surprises for us in our graduate courses has
been the interest from excellent linguistics and com-
puter science undergraduates.
We have sought to encourage our students to be
active in the academic community outside of UT
Austin. One way we do this is to have a final
project for each course (and most seminars) that has
four distinct stages: (i) a project proposal halfway
through the semester, (ii) a progress report three-
quarters of the way through, (iii) a 10-minute pre-
sentation during the last week of class, and (iv) a
final report at the end. We have found that having
course projects done in this staged manner ensures
that students think very thoroughly about what their
topic is early on, receive significant feedback from
us, and then still have enough time to do significant
implementation for their project, rather than rushing
everything in last minute. Also, by having students
do presentations on their work before they hand in
the final report, they can incorporate feedback from
other students. A useful strategy we have found for
scoring these projects is to use standard conference
reviews in Computational Linguistics II. The final
projects have led to several workshops and confer-
ence publications for the students so far, as well
as honors theses. The topics have been quite var-
ied (in line with our varied student body), including
lexicon induction using genetic algorithms (Ponvert,
2007), alignment-and-transfer for bootstrapping tag-
gers (Moon and Baldridge, 2007), lemmatization us-
ing parallel corpora (Moon and Erk, 2008), graphi-
cal visualization of articles using syntactic depen-
dencies (Jeff Rego, CS honors thesis), and feature
extraction for semantic role labeling (Trevor Foun-
tain, CS honors thesis).
Working with corpora. Computational linguis-
tics skills and techniques are tremendously valuable
for linguists using corpora. Ideally, a linguist should
be able to extract the relevant data, count occur-
rences of phenomena, and do statistical analyses.
The intersection of these skills and needs is the core
of this course, which covers corpus formats (XML,
bracket formats for syntax, ?word/POS? formats for
part-of-speech information), query languages and
tools (regular expressions, cqp, tregex), and some
statistical analysis techniques. It also teaches Python
gently for liberal arts students who have never pro-
grammed and have only limited or no knowledge of
text processing. Other main topics are the compi-
lation of corpora and corpus annotation, with issues
like representativity and what meta-data to include.
At the end of this course, students are prepared for
our primary computational courses.
We observed the tremendous teaching potential
of effective visualization in this course with the R
statistics package. It was used for statistical anal-
yses: students loved it because they could produce
meaningful results immediately and visualize them.
The course includes only a very short two-session
introduction to working with R. We were worried
that this would overtax students because R is its own
2
programming language. But interestingly they had
no problems with learning this second programming
language (after Python). This is particularly striking
as most of the students had no programming experi-
ence prior to the class.
We have not yet used the Natural Language
Toolkit (Loper and Bird, 2002) (see Section 3.1) in
this course. But as it, too, offers visualization and
rapid access to meaningful results, we intend to use
it in the future. In particular, the NLTK allows very
easy access to Toolbox data (Robinson et al, 2007),
which we feel will greatly improve the utility and
appeal of the course for the significant number of
documentary linguistics students in the department.
Seminars. We also offer several seminars in
our areas of interest. These include Categorial
Grammar, Computational Syntax, and Lexical Ac-
quisition. These courses have attracted ?non-
computational? linguistics students with related in-
terests, and have served as the launching point for
several qualifying papers and masters theses. It
is important to offer these courses so that these
students gain a view into computational linguistics
from the standpoint of a topic with which they al-
ready have some mastery; it also ensures healthier
enrollments from students in our own department.
We are currently co-teaching a seminar called
Spinning Straw into Gold: Automated Syntax-
Semantics Analysis, that is designed to overlap with
the CoNLL-2008 shared task on joint dependency
parsing and semantic role labeling. The entire class
is participating in the actual competition, and we
have been particularly pleased with how this exter-
nal facet of the course motivates students to consider
the topics we cover very carefully ? the papers truly
matter for the system we are building. It provides an
excellent framework with which to judge the contri-
butions of recent research in both areas and compu-
tational linguistics more generally.
2.2 Undergraduate
Our first undergraduate course was Introduction to
Computational Linguistics in Fall 2006. Our expe-
rience with this course, which had to deal with the
classic divide in computational linguistics courses
between students with liberal arts versus computer
science backgrounds, led us to split it into two
courses. We briefly outline some of the missteps
with this first course (and what worked well) and
how we are addressing them with new courses.
Introduction to Computational Linguistics.
This course is a boiled-down version of the graduate
Computational Linguistics I taught in Fall 2006.
Topics included Python programming, regular
expressions, finite-state transducers, part-of-speech
tagging, context-free grammar, categorial grammar,
meaning representations, and machine translation.
Overall, the course went well, but enrollment
dropped after the mid-term. As many have found
teaching such courses, some students truly struggled
with the course material while others were ready for
it to go much faster. Several students had interpreted
?introduction? to mean that it was going to be about
computational linguistics, but that they would not
actually have to do computational linguistics. Many
stayed with it, but there were still others who could
have gone much further if it had not been necessary
to slow down to cover basic material like for loops.
Note that several linguistics majors were among the
compationally savvy students.
In fairness to the students who struggled, it was
certainly ill-advised to ask students with no previous
background to learn Python and XFST in a single
semester. One of the key points of confusion was
regular expression syntax. The syntax used in the
textbook (Jurafsky and Martin, 2000) transfers eas-
ily to regular expressions in Python, but is radically
different from that of XFST. For students who had
never coded anything in their life, this proved ex-
tremely frustrating. On the other hand, for computa-
tionally savvy students, XFST was great fun, and it
was an interesting new challenge after having to sit
through very basic Python lectures.
On the other hand, the use of NLTK to drive learn-
ing about Python and NLP tasks (like building POS-
taggers) significantly eased the burden for new pro-
grammers. Many of them were highly satisfied that
they could build interesting programs and experi-
ment with their behavior so easily.
Language and Computers. We had fortunately
already planned the first replacement course: Lan-
guage and Computers, based on the course designed
at the Department of Linguistics at the Ohio State
University (Brew et al, 2005). This course intro-
3
duces computational linguistics to a general audi-
ence and is ideal for students who want exposure
to computational methods without having to learn
to program. We designed and taught it jointly, and
added several new aspects to the course. Whereas
OSU?s course fulfills a Mathematical and Logical
Analysis requirement, our course fulfills a Science
requirement for liberal arts majors. These require-
ments were met by course content that requires un-
derstanding and thinking about formal methods.
The topics we added to our course were question
answering, cryptography,2 and world knowledge.
The course provides ample opportunity to discuss
high-level issues in language technology with low-
level aspects such as understanding particular algo-
rithms (e.g., computing edit distance with dynamic
programming) and fundamental concepts (such as
regular languages and frequency distributions).
In addition to its target audience, the course
nonetheless attracts students who are already well-
versed in many of the low-level concepts. The high-
level material plays an important role for such stu-
dents: while they find the low-level problems quite
easy, many find a new challenge in thinking about
and communicating clearly the wider role that such
technologies play. The high-level material is even
more crucial for holding the interest of less formally
minded students. It gives them the motivation to
work through and understand calculations and com-
putations that might otherwise bore them. Finally,
it provides an excellent way to encourage class dis-
cussion. For example, this year?s class became very
animated on the question of ?Can a machine think??
that we discussed with respect to dialogue systems.
Though the course does not require students to
do any programming, we do show them short pro-
grams that accomplish (simplified versions of) some
of the tasks discussed in the course; for example,
short programs for document retrieval and creating
a list of email address from US census data. The
goal is to give students a glimpse into such applica-
tions, demonstrate that they are not hugely compli-
cated magical systems, and hopefully entice some of
them to learn how to do it for themselves.
The 2007 course was quite successful: it filled
2The idea to cover cryptography came from a discussion
with Chris Brew; he now teaches an entire course on it at OSU.
up (40 students) and received very positive feedback
from the students. It filled up again for this year?s
Spring 2008 offering. The major challenge is the
lack of a textbook, which means that students must
rely heavily on lecture slides and notes.
Words in a Haystack: Methods and Tools for
Working with Corpora. This advanced under-
graduate version of Working with corpora was of-
fered because we felt that graduate and undergrad-
uate linguistics students were actually on an equal
footing in their prior knowledge, and could profit
equally from a gentle introduction to programming.
Although the undergraduate students were active
and engaged in the class, they did not benefit as
much from it as the graduate students. This is likely
because graduate students had already experienced
the need for extracting information from corpora for
their research and the consequent frustration when
they did not have the skills to do so.
Natural Language Processing. This is an de-
manding course that will be taught in Fall 2008. It
is cross-listed with computer science and assumes
knowledge of programming and formal methods in
computer science, mathematics, or linguistics. It is
designed for the significant number of students who
wish to carry on further from the courses described
previously. It is also an appropriate course for un-
dergraduates who have ended up taking our graduate
courses for lack of such an option.
Much of the material from Introduction to Com-
putational Linguistics will be covered in this course,
but it will be done at a faster pace and in greater
detail since programming and appropriate thinking
skills are assumed. A significant portion of the grad-
uate course Computational Linguistics II also forms
part of the syllabus, including machine learning
methods for classification tasks, language modeling,
hidden Markov models, and probabilistic parsing.
We see cross-listing the course with computer sci-
ence as key to its success. Though there are many
computationally savvy students in our liberal arts
college, we expect cross-listing to encourage signif-
icantly more computer science students to try out a
course that they would otherwise overlook or be un-
able to use for fulfilling degree requirements.
4
3 Teaching Tools and Tutorials
We have used a range of external tools and have
adapted tools from our own research for various as-
pects of our courses. In this section, we describe our
experience using these as part of our courses.
We have used Python as the common language in
our courses. We are pleased with it: it is straight-
forward for beginning programmers to learn, its in-
teractive prompt facilitates in-class instruction, it is
text-processing friendly, and it is useful for gluing
together other (e.g., Java and C++) applications.
3.1 External tools and resources
NLTK. We use the Natural Language Toolkit
(NLTK) (Loper and Bird, 2002; Bird et al, 2008) in
both undergraduate and graduate courses for in-class
demos, tutorials, and homework assignments. We
use the toolkit and tutorials for several course com-
ponents, including introductory Python program-
ming, text processing, rule-based part-of-speech tag-
ging and chunking, and grammars and parsing.
NLTK is ideal for both novice and advanced pro-
grammers. The tutorials and extensive documenta-
tion provide novices with plenty of support outside
of the classroom, and the toolkit is powerful enough
to give plenty of room for advanced students to play.
The demos are also very useful in classes and serve
to make many of the concepts, e.g. parsing algo-
rithms, much more concrete and apparent. Some
students also use NLTK for course projects. In all,
NLTK has made course development and execution
significantly easier and more effective.
XFST. A core part of several courses is finite-state
transducers. FSTs have unique qualities for courses
about computational linguistics that are taught in
linguistics department. They are an elegant exten-
sion of finite-state automata and are simple enough
that their core aspects and capabilities can be ex-
pressed in just a few lectures. Computer science stu-
dents immediately get excited about being able to
relate string languages rather than just recognizing
them. More importantly, they can be used to ele-
gantly solve problems in phonology and morphol-
ogy that linguistics students can readily appreciate.
We use the Xerox Finite State Toolkit (XFST)
(Beesley and Karttunen, 2003) for in-class demon-
strations and homeworks for FSTs. A great aspect of
using XFST is that it can be used to show that differ-
ent representations (e.g., two-level rules versus cas-
caded rules) can be used to define the same regular
relation. This exercise injects some healthy skepti-
cism into linguistics students who may have to deal
with formalism wars in their own linguistic subfield.
Also, XFST allows one to use lenient composition to
encode Optimality Theory constraints and in so do-
ing show interesting and direct contrasts and com-
parisons between paper-and-pencil linguistics and
rigorous computational implementations.
As with other implementation-oriented activities
in our classes, we created a wiki page for XFST tu-
torials.3 These were adapted and expanded fromXe-
rox PARC materials and Mark Gawron?s examples.
Eisner?s HMM Materials. Simply put: the
spreadsheet designed by Jason Eisner (Eisner, 2002)
for teaching hidden Markov models is fantastic. We
used that plus Eisner?s HMM homework assignment
for Computational Linguistics II in Fall 2007. The
spreadsheet is great for interactive classroom explo-
ration of HMMs?students were very engaged. The
homework allows students to implement an HMM
from scratch, giving enough detail to alleviate much
of the needless frustration that could occur with this
task while ensuring that students need to put in sig-
nificant effort and understand the concepts in order
to make it work. It also helps that the new edition
of Jurafsky and Martin?s textbook discusses Eisner?s
ice cream scenario as part of its much improved
explanation of HMMs. Students had very positive
feedback on the use of all these materials.
Unix command line. We feel it is important to
make sure students are well aware of the mighty
Unix command line and the tools that are available
for it. We usually have at least one homework as-
signment per course that involves doing the same
task with a Python script versus a pipeline using
command line tools like tr, sort, grep and awk.
This gives students students an appreciation for the
power of these tools and for the fact that they are at
times preferable to writing scripts that handle every-
thing, and they can see how scripts they write can
form part of such pipelines. As part of this module,
3http://comp.ling.utexas.edu/wiki/doku.
php/xfst
5
we have students work through the exercises in the
draft chapter on command line tools in Chris Brew
and Marc Moens? Data-Intensive Linguistics course
notes or Ken Church?s Unix for Poets tutorial.4
3.2 Internal tools
Grammar engineering with OpenCCG. The
grammar engineering component of Computational
Syntax in Spring 2006 used OpenCCG,5 a catego-
rial grammar parsing system that Baldridge created
with Gann Bierner and Michael White. The prob-
lem with using OpenCCG is that its native grammar
specification format is XML designed for machines,
not people. Students in the course persevered and
managed to complete the assignments; nonetheless,
it became glaringly apparent that the non-intuitive
XML specification language was a major stumbling
block that held students back from more interesting
aspects of grammar engineering.
One student, Ben Wing, was unhappy enough us-
ing the XML format that he devised a new specifica-
tion language, DotCCG, and a converter to generate
the XML from it. DotCCG is not only simpler?it
also uses several interesting devices, including sup-
port for regular expressions and string expansions.
This expressivity makes it possible to encode a sig-
nificant amount of morphology in the specification
language and reduce redundancy in the grammar.
The DotCCG specification language and con-
verter became the core of a project funded by UT
Austin?s Liberal Arts Instructional Technology Ser-
vices to create a web and graphical user interface,
VisCCG, and develop instructional materials for
grammar engineering. The goal was to provide suit-
able interfaces and a graduated series of activities
and assignments that would allow students to learn
very basic grammar engineering and then grow into
the full capabilities of an established parsing system.
A web interface provided an initial stage that al-
lowed students in the undergraduate Introduction to
Computational Linguistics course (Fall 2006) to test
their grammars in a grammar writing assignment.
This simple interface allows students to first write
out a grammar on paper and then implement it and
test it on a set of sentences. Students grasped the
4http://research.microsoft.com/users/church/
wwwfiles/tutorials/unix for poets.ps
5http://openccg.sf.net
concepts and seemed to enjoy seeing the grammar?s
coverage improve as they added more lexical entries
or added features to constrain them appropriately. A
major advantage of this interface, of course, is that
it was not necessary for students to come to the lab
or install any software on their own computers.
The second major development was VisCCG,
a graphical user interface for writing full-fledged
OpenCCG grammars. It has special support for
DotCCG, including error checking, and it displays
grammatical information at various levels of granu-
larity while still allowing direct source text editing
of the grammar.
The third component was several online
tutorials?written on as publicly available wiki
pages?for writing grammars with VisCCG and
DotCCG. A pleasant discovery was the tremendous
utility of the wiki-based tutorials. It was very easy
to quickly create tutorial drafts and improve them
with the graduate assistant employed for creating
instructional materials for the project, regardless of
where we were. More importantly, it was possible
to fix bugs or add clarifications while students were
following the tutorials in the lab. Furthermore,
students could add their own tips for other students
and share their grammars on the wiki.
These tools and tutorials were used for two grad-
uate courses in Spring 2007, Categorial Grammar
and Computational Linguistics I. Students caught on
quickly to using VisCCG and DotCCG, which was a
huge contrast over the previous year. Students were
able to create and test grammars of reasonable com-
plexity very quickly and with much greater ease. We
are continuing to develop and improve these materi-
als for current courses.
The resources we created have been not only ef-
fective for classroom instruction: they are also be-
ing used by researchers that use OpenCCG for pars-
ing and realization. The work we did produced sev-
eral innovations for grammar engineering that we
reported at the workshop on Grammar Engineering
Across the Frameworks (Baldridge et al, 2007).
3.3 A lexical semantics workbench:
Shalmaneser
In the lexical semantics sections of our classes, word
sense and predicate-argument structure are core top-
ics. Until now, we had only discussed word sense
6
disambiguation and semantic role labeling theoret-
ically. However, it would be preferable to give the
students hands-on experience with the tasks, as well
as a sense of what does and does not work, and why
the tasks are difficult. So, we are now extending
Shalmaneser (Erk and Pado, 2006), a SHALlow se-
MANtic parSER that does word sense and semantic
role assignment using FrameNet frames and roles,
to be a teaching tool. Shalmaneser already offers a
graphical representation of the assigned predicate-
argument structure. Supported by an instructional
technology grant from UT Austin, we are extend-
ing the system with two graphical interfaces that
will allow students to experiment with a variety of
features, settings and machine learning paradigms.
Courses that only do a short segment on lexical se-
mantic analysis will be able to use the web inter-
face, which does not offer the full functionality of
Shalmaneser (in particular, no training of new clas-
sifiers), but does not require any setup. In addition,
there will be a stand-alone graphical user interface
for a more in-depth treatment of lexical semantic
analysis. We plan to have the new platform ready
for use for Fall 2008.
Besides a GUI and tutorial documents, there is
one more component to the new Shalmaneser sys-
tem, an adaptation of the idea of grammar engi-
neering workbenches to predicate-argument struc-
ture. Grammar engineering workbenches allow stu-
dents to specify grammars declaratively. For seman-
tic role labeling, the only possibility that has been
available so far for experimenting with new features
is to program. But, since semantic role labeling fea-
tures typically refer to parts of the syntactic struc-
ture, it should be possible to describe them declar-
atively using a tree description language. We are
now developing such a language and workbench as
part of Shalmaneser. We aim for a system that will
be usable not only in the classroom but also by re-
searchers who develop semantic role labeling sys-
tems or who need an automatic predicate-argument
structure analysis system.
4 University-wide program
The University of Texas at Austin has a long tra-
dition in the field of computational linguistics that
goes back to 1961, when a major machine transla-
tion project was undertaken at the university?s Lin-
guistics Research Center under the direction of Win-
fred Lehman. Lauri Karttunen, Stan Peters, and
Bob Wall were all on the faculty of the linguistics
department in the late 1960?s, and Bob Simmons
was in the computer science department during this
time. Overall activity was quite strong throughout
the 1970?s and 1980?s. After Bob Wall retired in the
mid-1990?s, there was virtually no computational
work in the linguistics department, but Ray Mooney
and his students in computer science remained very
active during this period.6
The linguistics department decided in 2000 to
revive computational linguistics in the department,
and consequently hired Jonas Kuhn in 2002. His
efforts, along with those of Hans Boas in the Ger-
man department, succeeded in producing a com-
putational linguistics curriculum, funding research,
(re)establishing links with computer science, and at-
tracting an enthusiastic group of linguistics students.
Nonetheless, there is still no formal interdepart-
mental program in computational linguistics at UT
Austin. Altogether, we have a sizable group of
faculty and students working on topics related to
computational linguistics, including many other lin-
guists, computer scientists, psychologists and oth-
ers who have interests directly related to issues in
computational linguistics, including our strong arti-
ficial intelligence group. Despite this, it was easy
to overlook if one was considering only an individ-
ual department. We thus set up a site7 to improve
the visibility of our CL-related faculty and research
across the university. There are plans to create an ac-
tual program spanning the various departments and
drawing on the strengths of UT Austin?s language
departments. For now, the web site is a low-cost and
low-effort but effective starting point.
As part of these efforts, we are working to in-
tegrate our course offerings, including the cross-
listing of the undergraduate NLP course. Our stu-
dents regularly take Machine Learning and other
courses from the computer science department. Ray
Mooney will teach a graduate NLP course in Fall
2008 that will offer students a different perspective
and we hope that it will drum up further interest
6For a detailed account, see: http://comp.ling.
utexas.edu/wiki/doku.php/austin compling history
7http://comp.ling.utexas.edu
7
in CL in the computer science department and thus
lead to further interest in our other courses.
As part of the web page, we also created a wiki.8
We have already mentioned its use in teaching and
tutorials. Other uses include lab information, a
repository of programming tips and tricks, list of im-
portant NLP papers, collaboration areas for projects,
and general information about computational lin-
guistics. We see the wiki as an important reposi-
tory of knowledge that will accumulate over time
and continue to benefit us and our students as it
grows. It simplifies our job since we answer many
student questions on the wiki: when questions get
asked again, we just point to the relevant page.
5 Conclusion
Our experience as computational linguists teaching
and doing research in a linguistics department at a
large university has given us ample opportunity to
learn a number of general lessons for teaching com-
putational linguistics to a diverse audience.
The main lesson is to stratify courses according
to the backgrounds different populations of students
have with respect to programming and formal think-
ing. A key component of this is to make expec-
tations about the level of technical difficulty of a
course clear before the start of classes and restate
this information on the first day of class. This is im-
portant not only to ensure students do not take too
challenging a course: other reasons include (a) re-
assuring programming-wary students that a course
will introduce them to programming gently, (b) en-
suring that programming-savvy students know when
there will be little programming involved or formal
problem solving they are likely to have already ac-
quired, and (c) providing awareness of other courses
students may be more interested in right away or af-
ter they have completed the current course.
Another key lesson we have learned is that the for-
mal categorization of a course within a university
course schedule and departmental degree program
are massive factors in enrollment, both at the under-
graduate and graduate level. Computational linguis-
tics is rarely a required course, but when taught in a
liberal arts college it can easily satisify undergradu-
ate math and/or science requirements (as Language
8http://comp.ling.utexas.edu/wiki/doku.php
and Computers does at OSU and UT Austin, respec-
tively). However, for highly technical courses taught
in a liberal arts college (e.g., Natural Language Pro-
cessing) it is useful to cross-list them with computer
science or related areas in order to ensure that the ap-
propriate student population is reached. At the grad-
uate level, it is also important to provide structure
and context for each course. We are now coordinat-
ing with Ray Mooney to define a core set of com-
putational linguistics courses that we offer regularly
and can suggest to incoming graduate students. This
will not be part of a formal degree program per se,
but will provide necessary structure for students to
progress through either the linguistics or computer
science program in a timely fashion while taking
courses relevant to their research interests.
One of the big questions that hovers over nearly
all discussions of teaching computational linguistics
is: how do we teach the computer science to the
linguistics students and teach the linguistics to the
computer science students? Or, rather, the question
is how to teach both groups computational linguis-
tics. This involves getting students to understand the
importance of a strong formal basis, ranging from
understanding what a tight syntax-semantics inter-
face really means to how machine learning mod-
els relate to questions of actual language acquisi-
tion to how corpus data can or should inform lin-
guistic analyses. It also involves revealing the cre-
ativity and complexity of language to students who
think it should be easy to deal with. And it involves
showing linguistics students how familiar concepts
from linguistics translate to technical questions (for
example, addressing agreement using feature log-
ics), and showing computer science students how
familiar friends like finite-state automata and dy-
namic programming are crucial for analyzing nat-
ural language phenomena and managing complexity
and ambiguity. The key is to target the courses so
that the background needs of each type of student
can be met appropriately without needing to skimp
on linguistic or computational complexity for those
who are ready to learn about it.
Acknowledgments. We would like to thank Hans
Boas, Bob Harms, Ray Mooney, Elias Ponvert, Tony
Woodbury, and the anonymous reviewers for their
help and feedback.
8
References
Jason Baldridge, Sudipta Chatterjee, Alexis Palmer, and
Ben Wing. 2007. DotCCG and VisCCG: Wiki and
programming paradigms for improved grammar engi-
neering with OpenCCG. In Proceeings of the GEAF
2007 Workshop.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI Publications.
Steven Bird, Ewan Klein, Edward Loper, and Jason
Baldridge. 2008. Multidisciplinary instruction with
the Natural Language Toolkit. In Proceedings of the
Third Workshop on Issues in Teaching Computational
Linguistics. Association for Computational Linguis-
tics.
C. Brew, M. Dickinson, and W. D. Meurers. 2005. Lan-
guage and computers: Creating an introduction for a
general undergraduate audience. In Proceedings of the
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing And Compu-
tational Linguistics, Ann Arbor, Michigan.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Dragomir
Radev and Chris Brew, editors, Proceedings of the
ACL Workshop on Effective Tools and Methodologies
for Teaching NLP and CL, pages 10?18.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser ? a
flexible toolbox for semantic role assignment. In Pro-
ceedings of LREC-2006, Genoa, Italy.
D. Jurafsky and J. H. Martin. 2000. Speech and language
processing: An Introduction to Natural Language
Processing, Computational Linguistics, and Speech
Recognition. Prentice-Hall, Upper Saddle River, NJ.
Edward Loper and Steven Bird. 2002. NLTK: The natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics, pages 62?69. Somerset, NJ: Association
for Computational Linguistics.
Taesun Moon and Jason Baldridge. 2007. Part-of-speech
tagging for middle English through alignment and pro-
jection of parallel diachronic texts. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
390?399.
Taesun Moon and Katrin Erk. 2008. Minimally super-
vised lemmatization scheme induction through bilin-
gual parallel corpora. In Proceedings of the Interna-
tional Conference on Global Interoperability for Lan-
guage Resources.
Elias Ponvert. 2007. Inducing Combinatory Categorial
Grammars with genetic algorithms. In Proceedings
of the ACL 2007 Student Research Workshop, pages
7?12, Prague, Czech Republic, June. Association for
Computational Linguistics.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolkit. Language Documentation and
Conservation, 1:44?57.
9
Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62?70,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Multidisciplinary Instruction with the Natural Language Toolkit
Steven Bird
Department of Computer Science
University of Melbourne
sb@csse.unimelb.edu.au
Ewan Klein
School of Informatics
University of Edinburgh
ewan@inf.ed.ac.uk
Edward Loper
Computer and Information Science
University of Pennsylvania
edloper@gradient.cis.upenn.edu
Jason Baldridge
Department of Linguistics
University of Texas at Austin
jbaldrid@mail.utexas.edu
Abstract
The Natural Language Toolkit (NLTK) is
widely used for teaching natural language
processing to students majoring in linguistics
or computer science. This paper describes
the design of NLTK, and reports on how
it has been used effectively in classes that
involve different mixes of linguistics and
computer science students. We focus on three
key issues: getting started with a course,
delivering interactive demonstrations in the
classroom, and organizing assignments and
projects. In each case, we report on practical
experience and make recommendations on
how to use NLTK to maximum effect.
1 Introduction
It is relatively easy to teach natural language pro-
cessing (NLP) in a single-disciplinary mode to a uni-
form cohort of students. Linguists can be taught to
program, leading to projects where students manip-
ulate their own linguistic data. Computer scientists
can be taught methods for automatic text processing,
leading to projects on text mining and chatbots. Yet
these approaches have almost nothing in common,
and it is a stretch to call either of these NLP: more
apt titles for such courses might be ?linguistic data
management? and ?text technologies.?
The Natural Language Toolkit, or NLTK, was
developed to give a broad range of students access
to the core knowledge and skills of NLP (Loper
and Bird, 2002). In particular, NLTK makes it
feasible to run a course that covers a substantial
amount of theory and practice with an audience
consisting of both linguists and computer scientists.
NLTK is a suite of Python modules distributed
under the GPL open source license via nltk.org.
NLTK comes with a large collection of corpora,
extensive documentation, and hundreds of exercises,
making NLTK unique in providing a comprehensive
framework for students to develop a computational
understanding of language. NLTK?s code base of
100,000 lines of Python code includes support
for corpus access, tokenizing, stemming, tagging,
chunking, parsing, clustering, classification,
language modeling, semantic interpretation,
unification, and much else besides. As a measure of
its impact, NLTK has been used in over 60 university
courses in 20 countries, listed on the NLTK website.
Since its inception in 2001, NLTK has undergone
considerable evolution, based on the experience
gained by teaching courses at several universities,
and based on feedback from many teachers and
students.1 Over this period, a series of practical
online tutorials about NLTK has grown up into a
comprehensive online book (Bird et al, 2008). The
book has been designed to stay in lock-step with the
NLTK library, and is intended to facilitate ?active
learning? (Bonwell and Eison, 1991).
This paper describes the main features of
NLTK, and reports on how it has been used
effectively in classes that involve a combination
of linguists and computer scientists. First we
discuss aspects of the design of the toolkit that
1(Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst,
2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005;
Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk,
2008)
62
arose from our need to teach computational
linguistics to a multidisciplinary audience (?2). The
following sections cover three distinct challenges:
getting started with a course (?3); interactive
demonstrations (?4); and organizing assignments
and projects (?5).
2 Design Decisions Affecting Teaching
2.1 Python
We chose Python2 as the implementation language
for NLTK because it has a shallow learning curve, its
syntax and semantics are transparent, and it has good
string-handling functionality. As an interpreted
language, Python facilitates interactive exploration.
As an object-oriented language, Python permits
data and methods to be encapsulated and re-used
easily. Python comes with an extensive standard
library, including tools for graphical programming
and numerical processing, which means it can be
used for a wide range of non-trivial applications.
Python is ideal in a context serving newcomers and
experienced programmers (Shannon, 2003).
We have taken the step of incorporating a detailed
introduction to Python programming in the NLTK
book, taking care to motivate programming con-
structs with linguistic examples. Extensive feedback
from students has been humbling, and revealed that
for students with no prior programming experience,
it is almost impossible to over-explain. Despite the
difficulty of providing a self-contained introduction
to Python for linguists, we nevertheless have also
had very positive feedback, and in combination with
the teaching techniques described below, have man-
aged to bring a large group of non-programmer stu-
dents rapidly to a point where they could carry out
interesting and useful exercises in text processing.
In addition to the NLTK book, the code in the
NLTK core is richly documented, using Python doc-
strings and Epydoc3 support for API documenta-
tion.4 Access to the code documentation is available
using the Python help() command at the interac-
tive prompt, and this can be especially useful for
checking the parameters and return type of func-
tions.
2http://www.python.org/
3http://epydoc.sourceforge.net/
4http://nltk.org/doc/api/
Other Python libraries are useful in the NLP con-
text: NumPy provides optimized support for linear
algebra and sparse arrays (NumPy, 2008) and PyLab
provides sophisticated facilities for scientific visual-
ization (Matplotlib, 2008).
2.2 Coding Requirements
As discussed in Loper & Bird (2002), the priorities
for NLTK code focus on its teaching role. When code
is readable, a student who doesn?t understand the
maths of HMMs, smoothing, and so on may benefit
from looking at how an algorithm is implemented.
Thus consistency, simplicity, modularity are all vital
features of NLTK code. A similar importance is
placed on extensibility, since this helps to ensure that
the code grows as a coherent whole, rather than by
unpredictable and haphazard additions.
By contrast, although efficiency cannot be
ignored, it has always taken second place to
simplicity and clarity of coding. In a similar vein,
we have tried to avoid clever programming tricks,
since these typically hinder intelligibility of the
code. Finally, comprehensiveness of coverage has
never been an overriding concern of NLTK; this
leaves open many possibilities for student projects
and community involvement.
2.3 Naming
One issue which has absorbed a considerable
amount of attention is the naming of user-oriented
functions in NLTK. To a large extent, the system of
naming is the user interface to the toolkit, and it is
important that users should be able to guess what
action might be performed by a given function.
Consequently, naming conventions need to be
consistent and semantically transparent. At the same
time, there is a countervailing pressure for relatively
succinct names, since excessive verbosity can also
hinder comprehension and usability. An additional
complication is that adopting an object-oriented
style of programming may be well-motivated for
a number of reasons but nevertheless baffling to
the linguist student. For example, although it is
perfectly respectable to invoke an instance method
WordPunctTokenizer().tokenize(text)
(for some input string text), a simpler version is
also provided: wordpunct tokenize(text).
63
2.4 Corpus Access
The scope of exercises and projects that students
can perform is greatly increased by the inclusion
of a large collection of corpora, along with easy-to-
use corpus readers. This collection, which currently
stands at 45 corpora, includes parsed, POS-tagged,
plain text, categorized text, and lexicons.5
In designing the corpus readers, we emphasized
simplicity, consistency, and efficiency. Corpus
objects, such as nltk.corpus.brown and
nltk.corpus.treebank, define common
methods for reading the corpus contents, abstracting
away from idiosyncratic file formats to provide a
uniform interface. See Figure 1 for an example of
accessing POS-tagged data from different tagged
and parsed corpora.
The corpus objects provide methods for loading
corpus contents in various ways. Common meth-
ods include: raw(), for the raw contents of the
corpus; words(), for a list of tokenized words;
sents(), for the same list grouped into sentences;
tagged words(), for a list of (word, tag) pairs;
tagged sents(), for the same list grouped into
sentences; and parsed sents(), for a list of parse
trees. Optional parameters can be used to restrict
what portion of the corpus is returned, e.g., a partic-
ular section, or an individual corpus file.
Most corpus reader methods return a corpus view
which acts as a list of text objects, but maintains
responsiveness and memory efficiency by only load-
ing items from the file on an as-needed basis. Thus,
when we print a corpus view we only load the first
block of the corpus into memory, but when we pro-
cess this object we load the whole corpus:
>>> nltk.corpus.alpino.words()
[?De?, ?verzekeringsmaatschappijen?,
?verhelen?, ...]
>>> len(nltk.corpus.alpino.words())
139820
2.5 Accessing Shoebox Files
NLTK provides functionality for working with
?Shoebox? (or ?Toolbox?) data (Robinson et
al., 2007). Shoebox is a system used by many
documentary linguists to produce lexicons and
interlinear glossed text. The ability to work
5http://nltk.org/corpora.html
straightforwardly with Shoebox data has created a
new incentive for linguists to learn how to program.
As an example, in the Linguistics Department at
the University of Texas at Austin, a course has been
offered on Python programming and working with
corpora,6 but so far uptake from the target audience
of core linguistics students has been low. They usu-
ally have practical computational needs and many of
them are intimidated by the very idea of program-
ming. We believe that the appeal of this course can
be enhanced by designing a significant component
with the goal of helping documentary linguistics stu-
dents take control of their own Shoebox data. This
will give them skills that are useful for their research
and also transferable to other activities. Although
the NLTK Shoebox functionality was not originally
designed with instruction in mind, its relevance to
students of documentary linguistics is highly fortu-
itous and may prove appealing for similar linguistics
departments.
3 Getting Started
NLP is usually only available as an elective course,
and students will vote with their feet after attending
one or two classes. This initial period is important
for attracting and retaining students. In particular,
students need to get a sense of the richness of lan-
guage in general, and NLP in particular, while gain-
ing a realistic impression of what will be accom-
plished during the course and what skills they will
have by the end. During this time when rapport
needs to be rapidly established, it is easy for instruc-
tors to alienate students through the use of linguistic
or computational concepts and terminology that are
foreign to students, or to bore students by getting
bogged down in defining terms like ?noun phrase?
or ?function? which are basic to one audience and
new for the other. Thus, we believe it is crucial
for instructors to understand and shape the student?s
expectations, and to get off to a good start. The best
overall strategy that we have found is to use succinct
nuggets of NLTK code to stimulate students? interest
in both data and processing techniques.
6http://comp.ling.utexas.edu/courses/
2007/corpora07/
64
>>> nltk.corpus.treebank.tagged_words()
[(?Pierre?, ?NNP?), (?Vinken?, ?NNP?), (?,?, ?,?), ...]
>>> nltk.corpus.brown.tagged_words()
[(?The?, ?AT?), (?Fulton?, ?NP-TL?), ...]
>>> nltk.corpus.floresta.tagged_words()
[(?Um?, ?>N+art?), (?revivalismo?, ?H+n?), ...]
>>> nltk.corpus.cess_esp.tagged_words()
[(?El?, ?da0ms0?), (?grupo?, ?ncms000?), ...]
>>> nltk.corpus.alpino.tagged_words()
[(?De?, ?det?), (?verzekeringsmaatschappijen?, ?noun?), ...]
Figure 1: Accessing Different Corpora via a Uniform Interface
3.1 Student Expectations
Computer science students come to NLP expecting
to learn about NLP algorithms and data structures.
They typically have enough mathematical prepara-
tion to be confident in playing with abstract for-
mal systems (including systems of linguistic rules).
Moreover, they are already proficient in multiple
programming languages, and have little difficulty in
learning NLP algorithms by reading and manipulat-
ing the implementations provided with NLTK. At the
same time, they tend to be unfamiliar with the termi-
nology and concepts that linguists take for granted,
and may struggle to come up with reasonable lin-
guistic analyses of data.
Linguistics students, on the other hand, are
interested in understanding NLP algorithms and
data structures only insofar as it helps them to
use computational tools to perform analytic tasks
from ?core linguistics,? e.g. writing a set of CFG
productions to parse some sentences, or plugging
together NLP components in order to derive the
subcategorization requirements of verbs in a corpus.
They are usually not interested in reading significant
chunks of code; it isn?t what they care about and
they probably lack the confidence to poke around in
source files.
In a nutshell, the computer science students typ-
ically want to analyze the tools and synthesize new
implementations, while the linguists typically want
to use the tools to analyze language and synthe-
size new theories. There is a risk that the former
group never really gets to grips with natural lan-
guage, while the latter group never really gets to
grips with processing. Instead, computer science
students need to learn that NLP is not just an applica-
tion of techniques from formal language theory and
compiler construction, and linguistics students need
to understand that NLP is not just computer-based
housekeeping and a solution to the shortcomings of
office productivity software for managing their data.
In many courses, linguistics students or computer
science students will dominate the class numeri-
cally, simply because the course is only listed in
one department. In such cases it is usually enough
to provide additional support in the form of some
extra readings, tutorials, and exercises in the open-
ing stages of the course. In other cases, e.g. courses
we have taught at the universities of Edinburgh, Mel-
bourne, Pennsylvania, and Texas-Austin or in sum-
mer intensive programs in several countries, there is
more of an even split, and the challenge of serving
both cohorts of students becomes acute. It helps to
address this issue head-on, with an early discussion
of the goals of the course.
3.2 Articulating the Goals
Despite an instructor?s efforts to add a cross-
disciplinary angle, students easily ?revert to
type.? The pressure of assessment encourages
students to emphasize what they do well. Students?
desire to understand what is expected of them
encourages instructors to stick to familiar
assessment instruments. As a consequence,
the path of least resistance is for students to
remain firmly monolingual in their own discipline,
while acquiring a smattering of words from a
foreign language, at a level we might call ?survival
linguistics? or ?survival computer science.? If they
ever get to work in a multidisciplinary team they are
65
likely only to play a type-cast role.
Asking computer science students to write their
first essay in years, or asking linguistics students
to write their first ever program, leads to stressed
students who complain that they don?t know what
is expected of them. Nevertheless, students need
to confront the challenge of becoming bilingual, of
working hard to learn the basics of another disci-
pline. In parallel, instructors need to confront the
challenge of synthesizing material from linguistics
and computer science into a coherent whole, and
devising effective methods for teaching, learning,
and assessment.
3.3 Entry Points
It is possible to identify several distinct pathways
into the field of Computational Linguistics. Bird
(2008) identifies four; each of these are supported
by NLTK, as detailed below:
Text Processing First: NLTK supports variety of
approaches to tokenization, tagging, evaluation, and
language engineering more generally.
Programming First: NLTK is based on Python
and the documentation teaches the language and
provides many examples and exercises to test and
reinforce student learning.
Linguistics First: Here, students come with a
grounding in one or more areas of linguistics, and
focus on computational approaches to that area by
working with the relevant chapter of the NLTK book
in conjunction with learning how to program.
Algorithms First: Here, students come with a
grounding in one or more areas of computer sci-
ence, and can use, test and extend NLTK?S reference
implementations of standard NLP algorithms.
3.4 The First Lecture
It is important that the first lecture is effective at
motivating and exemplifying NLP to an audience
of computer science and linguistics students. They
need to get an accurate sense of the interesting
conceptual and technical challenges awaiting them.
Fortunately, the task is made easier by the simple
fact that language technologies, and language itself,
are intrinsically interesting and appealing to a wide
audience. Several opening topics appear to work
particularly well:
The holy grail: A long term challenge,
mythologized in science fiction movies, is to
build machines that understand human language.
Current technologies that exhibit some basic level
of natural language understanding include spoken
dialogue systems, question answering systems,
summarization systems, and machine translation
systems. These can be demonstrated in class
without too much difficulty. The Turing test is a
linguistic test, easily understood by all students, and
which helps the computer science students to see
NLP in relation to the field of Artificial Intelligence.
The evolution of programming languages has
brought them closer to natural language, helping
students see the essentially linguistic purpose of
this central development in computer science.
The corresponding holy grail in linguistics is full
understanding of the human language faculty;
writing programs and building machines surely
informs this quest too.
The riches of language: It is easy to find
examples of the creative richness of language in its
myriad uses. However, linguists will understand
that language contains hidden riches that can only
be uncovered by careful analysis of large quantities
of linguistically annotated data, work that benefits
from suitable computational tools. Moreover, the
computational needs for exploratory linguistic
research often go beyond the capabilities of the
current tools. Computer scientists will appreciate
the cognate problem of extracting information from
the web, and the economic riches associated with
state-of-the-art text mining technologies.
Formal approaches to language: Computer sci-
ence and linguistics have a shared history in the area
of philosophical logic and formal language theory.
Whether the language is natural or artificial, com-
puter scientists and linguists use similar logical for-
malisms for investigating the formal semantics of
languages, similar grammar formalisms for model-
ing the syntax of languages, and similar finite-state
methods for manipulating text. Both rely on the
recursive, compositional nature of natural and arti-
ficial languages.
3.5 First Assignment
The first coursework assignment can be a significant
step forwards in helping students get to grips with
66
the material, and is best given out early, perhaps
even in week 1. We have found it advisable for
this assignment to include both programming and
linguistics content. One example is to ask students
to carry out NP chunking of some data (e.g. a section
of the Brown Corpus). The nltk.RegexpParser
class is initialized with a set of chunking rules
expressed in a simple, regular expression-oriented
syntax, and the resulting chunk parser can be run
over POS-tagged input text. Given a Gold Standard
test set like the CoNLL-2000 data,7 precision
and recall of the chunk grammar can be easily
determined. Thus, if students are given an existing,
incomplete set of rules as their starting point, they
just have to modify and test their rules.
There are distinctive outcomes for each set of stu-
dents: linguistics students learn to write grammar
fragments that respect the literal-minded needs of
the computer, and also come to appreciate the noisi-
ness of typical NLP corpora (including automatically
annotated corpora like CoNLL-2000). Computer
science students become more familiar with parts
of speech and with typical syntactic structures in
English. Both groups learn the importance of formal
evaluation using precision and recall.
4 Interactive Demonstrations
4.1 Python Demonstrations
Python fosters a highly interactive style of teaching.
It is quite natural to build up moderately complex
programs in front of a class, with the less confi-
dent students transcribing it into a Python session
on their laptop to satisfy themselves it works (but
not necessarily understanding everything they enter
first time), while the stronger students quickly grasp
the theoretical concepts and algorithms. While both
groups can be served by the same presentation, they
tend to ask quite different questions. However, this
is addressed by dividing them into smaller clusters
and having teaching assistants visit them separately
to discuss issues arising from the content.
The NLTK book contains many examples, and
the instructor can present an interactive lecture that
includes running these examples and experiment-
ing with them in response to student questions. In
7http://www.cnts.ua.ac.be/conll2000/
chunking/
early classes, the focus will probably be on learning
Python. In later classes, the driver for such interac-
tive lessons can be an externally-motivated empiri-
cal or theoretical question.
As a practical matter, it is important to consider
low-level issues that may get in the way of students?
ability to capture the material covered in interactive
Python sessions. These include choice of appropri-
ate font size for screen display, avoiding the prob-
lem of output scrolling the command out of view,
and distributing a log of the instructor?s interactive
session for students to study in their own time.
4.2 NLTK Demonstrations
A significant fraction of any NLP syllabus covers
fundamental data structures and algorithms. These
are usually taught with the help of formal notations
and complex diagrams. Large trees and charts are
copied onto the board and edited in tedious slow
motion, or laboriously prepared for presentation
slides. It is more effective to use live demonstrations
in which those diagrams are generated and updated
automatically. NLTK provides interactive graphical
user interfaces, making it possible to view program
state and to study program execution step-by-step.
Most NLTK components have a demonstration
mode, and will perform an interesting task without
requiring any special input from the user. It is
even possible to make minor modifications to
programs in response to ?what if? questions. In this
way, students learn the mechanics of NLP quickly,
gain deeper insights into the data structures and
algorithms, and acquire new problem-solving skills.
An example of a particularly effective set
of demonstrations are those for shift-reduce
and recursive descent parsing. These make
the difference between the algorithms glaringly
obvious. More importantly, students get a concrete
sense of many issues that affect the design of
algorithms for tasks like parsing. The partial
analysis constructed by the recursive descent
parser bobs up and down as it steps forward and
backtracks, and students often go wide-eyed as the
parser retraces its steps and does ?dumb? things
like expanding N to man when it has already
tried the rule unsuccessfully (but is now trying
to match a bare NP rather than an NP with a PP
modifier). Linguistics students who are extremely
67
knowledgeable about context-free grammars and
thus understand the representations gain a new
appreciation for just how naive an algorithm can be.
This helps students grasp the need for techniques
like dynamic programming and motivates them to
learn how they can be used to solve such problems
much more efficiently.
Another highly useful aspect of NLTK is the abil-
ity to define a context-free grammar using a sim-
ple format and to display tree structures graphically.
This can be used to teach context-free grammars
interactively, where the instructor and the students
develop a grammar from scratch and check its cov-
erage against a testbed of grammatical and ungram-
matical sentences. Because it is so easy to modify
the grammar and check its behavior, students readily
participate and suggest various solutions. When the
grammar produces an analysis for an ungrammatical
sentence in the testbed, the tree structure can be dis-
played graphically and inspected to see what went
wrong. Conversely, the parse chart can be inspected
to see where the grammar failed on grammatical sen-
tences.
NLTK?s easy access to many corpora greatly facil-
itates classroom instruction. It is straightforward to
pull in different sections of corpora and build pro-
grams in class for many different tasks. This not
only makes it easier to experiment with ideas on the
fly, but also allows students to replicate the exer-
cises outside of class. Graphical displays that show
the dispersion of terms throughout a text also give
students excellent examples of how a few simple
statistics collected from a corpus can provide useful
and interesting views on a text?including seeing the
frequency with which various characters appear in a
novel. This can in turn be related to other resources
like Google Trends, which shows the frequency with
which a term has been referenced in news reports or
been used in search terms over several years.
5 Exercises, Assignments and Projects
5.1 Exercises
Copious exercises are provided with the NLTK book;
these have been graded for difficulty relative to the
concepts covered in the preceding sections of the
book. Exercises have the tremendous advantage of
building on the NLTK infrastructure, both code and
documentation. The exercises are intended to be
suitable both for self-paced learning and in formally
assigned coursework.
A mixed class of linguistics and computer sci-
ence students will have a diverse range of program-
ming experience, and students with no programming
experience will typically have different aptitudes for
programming (Barker and Unger, 1983; Caspersen
et al, 2007). A course which forces all students
to progress at the same rate will be too difficult for
some, and too dull for others, and will risk alien-
ating many students. Thus, course materials need
to accommodate self-paced learning. An effective
way to do this is to provide students with contexts
in which they can test and extend their knowledge at
their own rate.
One such context is provided by lecture or lab-
oratory sessions in which students have a machine
in front of them (or one between two), and where
there is time to work through a series of exercises to
consolidate what has just been taught from the front,
or read from a chapter of the book. When this can be
done at regular intervals, it is easier for students to
know which part of the materials to re-read. It also
encourages them to get into the habit of checking
their understanding of a concept by writing code.
When exercises are graded for difficulty, it is
easier for students to understand how much effort
is expected, and whether they even have time to
attempt an exercise. Graded exercises are also good
for supporting self-evaluation. If a student takes
20 minutes to write a solution, they also need to
have some idea of whether this was an appropriate
amount of time.
The exercises are also highly adaptable. It is com-
mon for instructors to take them as a starting point
in building homework assignments that are tailored
to their own students. Some instructors prefer to
include exercises that do not allow students to take
advantage of built-in NLTK functionality, e.g. using
a Python dictionary to count word frequencies in the
Brown corpus rather than NLTK?s FreqDist (see
Figure 2). This is an important part of building
facility with general text processing in Python, since
eventually students will have to work outside of
the NLTK sandbox. Nonetheless, students often use
NLTK functionality as part of their solutions, e.g.,
for managing frequencies and distributions. Again,
68
nltk.FreqDist(nltk.corpus.brown.words())
fd = nltk.FreqDist()
for filename in corpus_files:
text = open(filename).read()
for w in nltk.wordpunct_tokenize(text):
fd.inc(w)
counts = {}
for w in nltk.corpus.brown.words():
if w not in counts:
counts[w] = 0
counts[w] += 1
Figure 2: Three Ways to Build up a Frequency Distribu-
tion of Words in the Brown Corpus
this flexibility is a good thing: students learn to
work with resources they know how to use, and can
branch out to new exercises from that basis. When
course content includes discussion of Unix com-
mand line utilities for text processing, students can
furthermore gain a better appreciation of the pros
and cons of writing their own scripts versus using
an appropriate Unix pipeline.
5.2 Assignments
NLTK supports assignments of varying difficulty and
scope: experimenting with existing components to
see what happens for different inputs or parameter
settings; modifying existing components and
creating systems using existing components;
leveraging NLTK?s extensible architecture by
developing entirely new components; or employing
NLTK?s interfaces to other toolkits such as Weka
(Witten and Frank, 2005) and Prover9 (McCune,
2008).
5.3 Projects
Group projects involving a mixture of linguists
and computer science students have an initial
appeal, assuming that each kind of student can
learn from the other. However, there?s a complex
social dynamic in such groups, one effect of which
is that the linguistics students may opt out of the
programming aspects of the task, perhaps with
view that their contribution would only hurt the
chances of achieving a good overall project mark.
It is difficult to mandate significant collaboration
across disciplinary boundaries, with the more
likely outcome being, for example, that a parser is
developed by a computer science team member,
then thrown over the wall to a linguist who will
develop an appropriate grammar.
Instead, we believe that it is generally more pro-
ductive in the context of a single-semester introduc-
tory course to have students work individually on
their own projects. Distinct projects can be devised
for students depending on their background, or stu-
dents can be given a list of project topics,8 and
offered option of self-proposing other projects.
6 Conclusion
We have argued that the distinctive features of
NLTK make it an apt vehicle for teaching NLP
to mixed audiences of linguistic and computer
science students. On the one hand, complete
novices can quickly gain confidence in their ability
to do interesting and useful things with language
processing, while the transparency and consistency
of the implementation also makes it easy for
experienced programmers to learn about natural
language and to explore more challenging tasks.
The success of this recipe is borne out by the
wide uptake of the toolkit, not only within tertiary
education but more broadly by users who just want
try their hand at NLP. We also have encouraging
results in presenting NLTK in classrooms at the
secondary level, thereby trying to inspire the
computational linguists of the future!
Finally, we believe that NLTK has gained much
by participating in the Open Source software move-
ment, specifically from the infrastructure provided
by SourceForge.net and from the invaluable
contributions of a wide range of people, including
many students.
7 Acknowledgments
We are grateful to the members of the NLTK com-
munity for their helpful feedback on the toolkit and
their many contributions. We thank the anonymous
reviewers for their feedback on an earlier version of
this paper.
8http://nltk.org/projects.html
69
References
Jason Baldridge and Katrin Erk. 2008. Teaching com-
putational linguistics to a large, diverse student body:
courses, tools, and interdepartmental interaction. In
Proceedings of the Third Workshop on Issues in Teach-
ing Computational Linguistics. Association for Com-
putational Linguistics.
Ricky Barker and E. A. Unger. 1983. A predictor for
success in an introductory programming class based
upon abstract reasoning development. ACM SIGCSE
Bulletin, 15:154?158.
Steven Bird and Edward Loper. 2004. NLTK: The Nat-
ural Language Toolkit. In Companion Volume to the
Proceedings of 42st Annual Meeting of the Association
for Computational Linguistics, pages 214?217. Asso-
ciation for Computational Linguistics.
Steven Bird, Ewan Klein, and Edward Loper. 2008.
Natural Language Processing in Python. http://
nltk.org/book.html.
Steven Bird. 2005. NLTK-Lite: Efficient scripting
for natural language processing. In 4th International
Conference on Natural Language Processing, Kanpur,
India, pages 1?8.
Steven Bird. 2006. NLTK: The Natural Language
Toolkit. In Proceedings of the COLING/ACL 2006
Interactive Presentation Sessions, pages 69?72, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Steven Bird. 2008. Defining a core body of knowledge
for the introductory computational linguistics curricu-
lum. In Proceedings of the Third Workshop on Issues
in Teaching Computational Linguistics. Association
for Computational Linguistics.
Charles C. Bonwell and James A. Eison. 1991. Active
Learning: Creating Excitement in the Classroom.
Washington, D.C.: Jossey-Bass.
Michael Caspersen, Kasper Larsen, and Jens Benned-
sen. 2007. Mental models and programming aptitude.
SIGCSE Bulletin, 39:206?210.
Marti Hearst. 2005. Teaching applied natural language
processing: Triumphs and tribulations. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 1?8,
Ann Arbor, Michigan, June. Association for Compu-
tational Linguistics.
Ewan Klein. 2006. Computational semantics in the Nat-
ural Language Toolkit. In Proceedings of the Aus-
tralasian Language Technology Workshop, pages 26?
33.
Elizabeth Liddy and Nancy McCracken. 2005. Hands-on
NLP for an interdisciplinary audience. In Proceedings
of the Second ACL Workshop on Effective Tools and
Methodologies for Teaching NLP and CL, pages 62?
68, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Computa-
tional Linguistics, pages 62?69. Association for Com-
putational Linguistics.
Edward Loper. 2004. NLTK: Building a pedagogical
toolkit in Python. In PyCon DC 2004. Python Soft-
ware Foundation.
Nitin Madnani and Bonnie Dorr. 2008. Combining
open-source with research to re-engineer a hands-on
introductory NLP course. In Proceedings of the Third
Workshop on Issues in Teaching Computational Lin-
guistics. Association for Computational Linguistics.
Nitin Madnani. 2007. Getting started on natural lan-
guage processing with Python. ACM Crossroads,
13(4).
Matplotlib. 2008. Matplotlib: Python 2D plotting
library. http://matplotlib.sourceforge.
net/.
William McCune. 2008. Prover9: Automated
theorem prover for first-order and equational logic.
http://www.cs.unm.edu/?mccune/mace4/
manual-examples.html.
NumPy. 2008. NumPy: Scientific computing with
Python. http://numpy.scipy.org/.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolkit. Language Documentation and
Conservation, 1:44?57.
Christine Shannon. 2003. Another breadth-first
approach to CS I using Python. In Proceedings of
the 34th SIGCSE Technical Symposium on Computer
Science Education, pages 248?251. ACM.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann.
70
Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluating Automation Strategies in Language Documentation
Alexis Palmer, Taesun Moon, and Jason Baldridge
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
{alexispalmer,tsmoon,jbaldrid}@mail.utexas.edu
Abstract
This paper presents pilot work integrating ma-
chine labeling and active learning with human
annotation of data for the language documen-
tation task of creating interlinearized gloss
text (IGT) for the Mayan language Uspanteko.
The practical goal is to produce a totally an-
notated corpus that is as accurate as possible
given limited time for manual annotation. We
describe ongoing pilot studies which examine
the influence of three main factors on reduc-
ing the time spent to annotate IGT: sugges-
tions from a machine labeler, sample selection
methods, and annotator expertise.
1 Introduction
Languages are dying at the rate of two each month.
By the end of this century, half of the approxi-
mately 6000 extant spoken languages will cease to
be transmitted effectively from one generation of
speakers to the next (Crystal, 2000). Under this
immense time pressure, documentary linguists seek
to preserve a record of endangered languages while
there are still communities of speakers to work with.
Many language documentation projects target lan-
guages about which our general linguistic knowl-
edge is nonexistent or much less than for more
widely-spoken languages. The vast majority of these
are individual or small-group endeavors on small
budgets with little or no institutional guidance by
the greater documentary linguistic community. The
focus in such projects is often first on collection of
data (documentation), with a following stage of lin-
guistic analysis and description. A key part of the
analysis process, detailed linguistic annotation of the
recorded texts, is a time-consuming and tedious task
usually occurring late in the project, if it occurs at
all.
Text annotation typically involves producing in-
terlinearized glossed text (IGT), labeling for mor-
phology, parts-of-speech, etc., which greatly facil-
itates further exploration and analysis of the lan-
guage. The following is IGT for the phrase xelch
li from the Mayan language Uspanteko:1
(1) x-
COM-
el
salir
-ch
-DIR
li
DEM
Spanish: ?Salio entonces.? English:?Then he left.?
The levels of analysis include morpheme segmenta-
tion, transliteration of stems, and labeling of stems
and morphemes with tags, some corresponding to
parts-of-speech and others to semantic distinctions.
There is no single standard format for IGT. The
IGT systems developed by documentation projects
tend to be idiosyncratic: they may be linguistically
well-motivated and intuitive, but they are unlikely to
be compatible or interchangeable with systems de-
veloped by other projects. They may lack internal
consistency as well. Nonetheless, IGT in a read-
ily accessible format is an important resource that
can be used fruitfully by linguists to examine hy-
potheses on novel data (e.g. Xia and Lewis (2007;
2008), Lewis and Xia (2008)). Furthermore, it can
be used by educators and language activists to create
curriculum material for mother language education
and promote the survival of the language.
Despite the urgent need for such resources, IGT
annotations are time consuming to create entirely by
hand, and both human and financial resources are
extremely limited in this domain. Thus, language
1KEY: COM=completive aspect, DEM=demonstrative,
DIR=directional
36
documentation presents an interesting test case and
an ideal context for use of machine labeling and ac-
tive learning. This paper describes a series of ex-
periments designed to assess this promise in a re-
alistic documentation context: creation of IGT for
the Mayan language Uspanteko. We systematically
compare varying degrees of machine involvement in
the development of IGT, from minimally involved
situations where examples for tagging are selected
sequentially to active learning situations where the
machine learner selects samples for human tagging
and suggests labels. We also discuss the challenges
faced by linguists in having to learn, transcribe, ana-
lyze, and annotate a language almost simultaneously
and discuss whether machine involvement reduces
or compounds those challenges.
In the experiments, two documentary linguists an-
notate IGT for Uspanteko texts using different lev-
els of support from a machine learned classifier. We
consider the interaction of three main conditions: (1)
sequential, random, or uncertainty sampling for re-
questing labels from an annotator, (2) suggestions or
no suggestions from a machine labeler, and (3) ex-
pert versus non-expert annotator. All annotator deci-
sions are timed, enabling the actual time cost of an-
notation to be measured within the context of each
condition. This paper describes the Uspanteko data
set we adapted for the experiments, expands on the
choices described above, and reports on preliminary
results from our ongoing annotation experiments.
2 Data: Uspanteko IGT
This section describes the Uspanteko corpus used
for the experiments, our clean-up of the corpus, and
the specific task?labeling part-of-speech and gloss
tags?addressed by the experiments.
2.1 OKMA Uspanteko corpus
Our primary dataset is a corpus of texts (Pixabaj et
al., 2007) in the Mayan language Uspanteko that
were collected, transcribed, translated (into Span-
ish) and annotated as part of the OKMA language
documentation project.2 Uspanteko, a member of
the K?ichee? branch of the Mayan language family,
is spoken by approximately 1320 people in central
Guatemala (Richards, 2003).
2http://www.okma.org
The corpus contains 67 texts, 32 of them glossed.
Four textual genres are represented in the glossed
portion of the corpus: oral histories (five texts) usu-
ally have to do with the history of the village and the
community, personal experience texts (five texts) re-
count events from the lives of individual people in
the community, and stories (twenty texts) are pri-
marily folk stories and children?s stories. The corpus
also contains one recipe and one advice text in which
a speaker discusses what the community should be
doing to better preserve and protect the environment.
The transcriptions are based on spoken data, with
attendant dysfluencies, repetitions, false starts, and
incomplete sentences. Of the 284,455 words, 74,298
are segmented and glossed. This is a small dataset
by computational linguistics standards but rather
large for a documentation project.
2.2 Interlinearized Glossed Text
Once recordings have been made, the next tasks are
typically to produce translations and transcription of
the audio. Transcription is a complex and difficult
process, often involving the development of an or-
thography for the language in parallel. The product
of the transcription is raw text like the Uspanteko
sample shown below (text 068, clauses 283-287):
Non li in yolow rk?il kita?
tinch?ab?ex laj inyolj iin, si no ke
laj yolj jqaaj tinch?ab?ej i non qe li
xk?am rib? chuwe, non qe li lajori
non li iin yolow rk?ilaq.3
Working with the transcription, the translation, and
any previously-attained knowledge about the lan-
guage, the linguist next makes decisions about the
division of words into morphemes and the contribu-
tions made by individual morphemes to the meaning
of the word or of the sentence. IGT efficiently brings
together and presents all of this information.
In the traditional four-line IGT format, mor-
phemes appear on one line and glosses for those
morphemes on the next. The gloss line includes both
labels for grammatical morphemes (e.g. PL or COM)
and translations of stems (e.g. salir or ropa). See
the following example from Uspanteko:4
3Spanish: Solo asi yo aprendi con e?l. No le hable en el
idioma mio. Si no que en el idioma su papa? le hablo. Y solo asi
me fui acostumbrando. Solo asi ahora yo platico con ellos.
4KEY: E1S=singular first person ergative, INC=incompletive,
PART=particle, PREP=preposition, PRON=pronoun, NEG=negation,
37
(2) Kita? tinch?ab?ej laj inyolj iin
(3) kita?
NEG
PART
t-in-ch?abe-j
INC-E1S-hablar-SC
TAM-PERS-VT-SUF
laj
PREP
PREP
in-yolj
E1S-idioma
PERS-S
iin
yo
PRON
?No le hablo en mi idioma.?
(?I don?t speak to him in my language.?)
Most commonly, IGT is presented in a four-tier
format. The first tier (2) is the raw, unannotated
text. The second (first line of (3)) is the same text
with each word morphologically segmented. The
third tier (second line of (3)), the gloss line, is a
combination of Spanish translations of the Uspan-
teko stems and gloss tags representing the grammat-
ical information encoded by affixes and stand-alone
morphemes. The fourth tier (fourth line of (3)) is a
translation in the target language of documentation.
Some interlinear texts include other project-
defined tiers. OKMA uses a fifth tier (third line of
(3)), described as the word-class line. This line is
a mix of traditional POS tags, positional labels (e.g.
suffix, prefix), and broader linguistic categories like
TAM for tense-aspect-mood.
2.3 Cleaning up the OKMA annotations
The OKMA annotations were created using Shoe-
box,5 a standard tool used by documentary linguists
for lexicon management and IGT creation. To de-
velop a corpus suitable for these studies, it was nec-
essary to put considerable effort into normalizing
the original OKMA source annotations. Varied lev-
els of linguistic training of the original annotators
led to many inconsistencies in the original annota-
tions. Also, Shoebox (first developed in 1987) uses
a custom, pre-XML whitespace delimited data for-
mat, making normalization especially challenging.
Finally, not all of the texts are fully annotated. Al-
most half of the 67 texts are just transcriptions, sev-
eral texts are translated but not further analyzed, and
several others are only partially annotated at text
level, clause level, word level, or morpheme level. It
was thus necessary to identify complete texts for use
in our experiments. Some missing labels in nearly-
complete texts were filled in by the expert annotator.
A challenge for representing IGT in a machine-
readable format is maintaining the links between
S=sustantivo (noun), SC=category suffix, SUF=suffix,
TAM=tense/aspect/mood, VT=transitive verb
5http://www.sil.org/computing/shoebox/
the source text morphemes in the second tier and
the morpheme-by-morpheme glosses in the third
tier. The standard Shoebox output format, for ex-
ample, enforces these links through management of
the number of spaces between items in the output.
To address this, we converted the cleaned annota-
tions into IGT-XML (Palmer and Erk, 2007) with
help from the Shoebox/Toolbox interfaces provided
in the Natural Language Toolkit (Robinson et al,
2007). Automating the transformation from Shoe-
box format to IGT-XML?s hierarchical format re-
quired cleaning up tier-to-tier alignment and check-
ing segmentation in some cases where morphemes
and glosses were misaligned, as in (5) below.6
(4) Non li in yolow rk?il
(5) Non
DEM
DEM
li
DEM
DEM
in
yo
PRON
yolow
platicar
VI
r-k?il
AP
SUF
E3s.-SR
PERS SREL
?Solo asi yo aprendi con e?l.?
Here, the number of elements in the morpheme tier
(first line of (5)) does not match the number of el-
ements in the gloss tier (second line of (5)). The
problem is a misanalysis of yolow: it should be
segmented yol-ow with the gloss platicar-AP.
Automating this transformation has the advantage of
identifying such inconsistencies and errors.
There also were many low-level issues that had
to be handled, such as checking and enforcing con-
sistency of tags. For example, the tag E3s. in the
gloss tier of (5) is a typo; the correct tag is E3S. The
annotation tool used in these studies does not allow
such inconsistencies to occur.
2.4 Target labels
There are two main tasks in producing IGT: word
segmentation (determination of stems and affixes)
and glossing each segment. Stems and affixes each
get a different type of gloss: the gloss of a stem is
typically its translation whereas the gloss of an affix
is a label indicating its grammatical role. The addi-
tional word-class line provides part-of-speech infor-
mation for the stems, such as VT for salir.
Complete prediction of segmentation, gloss trans-
lations and labels is our ultimate goal for aiding IGT
6KEY: AP=antipassive, DEM=demonstrative, E3S=singular third
person ergative, PERS=person marking, SR/SREL=relational noun,
VI=intransitive verb
38
creation with automation. Here, we study the poten-
tial for improving annotation efficiency for the more
limited task of predicting the gloss label for each af-
fix and the part-of-speech label for each stem. Thus,
the experiments aim to produce a single label for
each morpheme. We assume that words have been
pre-segmented and we ignore the gloss translations.
The target representation in these studies is an ad-
ditional tier which combines gloss labels for affixes
and stand-alone morphemes with part-of-speech la-
bels for stems. Example (6) repeats the clause in (4),
adding this new combined tier. Stem labels are given
in bold text, and affix labels in plain text.
(6) Non li in yolow rk?il
(7) Non
DEM
li
DEM
in
PRON
yol-ow
VI-AP
r-k?il
E3S-SR
?Solo asi yo aprendi con e?l.?
A simple procedure was used to create the new tier.
For each morpheme, if a gloss label (such as DEM
or E3S) appears on the gloss line (second line of
(3)), we select that label. If what appears is a stem
translation, we instead select the part-of-speech la-
bel from the next tier down (third line of (3)).
In the entire corpus, sixty-nine different labels
appear in this combined tier. The following table
shows the five most common part-of-speech labels
(left) and the five most common gloss labels (right).
The most common label, S, accounts for 11.3% of
the tokens in the corpus.
S noun 7167 E3S sg.3p. ergative 3433
ADV adverb 6646 INC incompletive 2835
VT trans. verb 5122 COM completive 2586
VI intrans. verb 3638 PL plural 1905
PART particle 3443 SREL relational noun 1881
3 Integrated annotation and automation
The experimental framework described in this sec-
tion is designed to model and evaluate real-time inte-
gration of human annotation, active learning strate-
gies, and output from machine-learned classifiers.
The task is annotation of morpheme-segmented texts
from a language documentation project (sec. 2).
3.1 Tools and resources
Integrating automated support and human annota-
tion in this context requires careful coordination of
three components: 1) presenting examples to the an-
notator and storing the annotations, 2) training and
evaluation of tagging models using data labeled by
the annotator, and 3) selecting new examples for an-
notation. The processes are managed and coordi-
nated using the OpenNLP IGT Editor.7 The anno-
tation component of the tool, and in particular the
user interface, is built on the Interlinear Text Editor
(Lowe et al, 2004).
For tagging we use a strong but simple standard
classifier. There certainly are many other modeling
strategies that could be used, for example a condi-
tional random field (as in Settles and Craven (2008)),
or a model that deals differently with POS labels and
morpheme gloss labels. Nonetheless, a documen-
tary linguistics project would be most likely to use a
straightforward, off-the-shelf labeler, and our focus
is on exploring different annotation approaches in a
realistic documentation setting rather than building
an optimal classifier. To that end, we use a standard
maximum entropy classifier which predicts the label
for a morpheme based on the morpheme itself plus
a window of two morphemes before and after. Stan-
dard features used in part-of-speech taggers are ex-
tracted from the morpheme to help with predicting
labels for previously unseen stems and morphemes.
3.2 Annotators and annotation procedures
A practical goal of these studies is to explore best
practices for using automated support to create fully-
annotated texts of the highest quality possible within
fixed resource limits. For producing IGT, one of the
most valuable resources is the time of a linguist with
language-specific expertise. Documentary projects
may also (or instead) have access to a trained lin-
guist without prior experience in the language. We
compare results from two annotators with different
levels of exposure to the language. Both are trained
linguists who specialize in language documentation
and have extensive field experience.8
The first, henceforth referred to as the expert
annotator, has worked extensively on Uspanteko,
including writing a grammar of the language and
7http://igt.sourceforge.net/
8It should be noted that these are pilot studies. With just
two annotators, the annotation comparisons are suggestive but
not conclusive. Even so, this scenario accurately reflects the
resource limitations encountered in documentation projects.
39
contributing to the publication of an Uspanteko-
Spanish dictionary (A?ngel Vicente Me?ndez, 2007).
She is a native speaker of K?ichee?, a closely-related
Mayan language. The second annotator, the non-
expert annotator, is a doctoral student in language
documentation with no prior experience with Us-
panteko and only limited previous knowledge of
Mayan languages. Throughout the annotation pro-
cess, the non-expert annotator relied heavily on the
Uspanteko-Spanish dictionary. Both annotators are
fluent speakers of Spanish, the target translation and
glossing language for the OKMA texts.
In many annotation projects, labeling of training
data is done with reference to a detailed annotation
manual. In the language documentation context, a
more usual situation is for the annotator(s) to work
from a set of agreed-upon conventions but without
strict annotation guidelines. This is not because doc-
umentary linguists lack motivation or discipline but
simply because many aspects of the language are un-
known and the analysis is constantly changing.
In the absence of explicit written annotation
guidelines, we use an annotation training process for
the annotators to learn the OKMA annotation con-
ventions. Two seed sets of ten clauses each were se-
lected to be used both for human annotation training
and for initial classifier training. The first ten clauses
of the first text in the training data were used to seed
model training for the sequential selection cases (see
3.4). The second set of ten were randomly selected
from the entire corpus and used to seed model train-
ing for both random and uncertainty sampling.
These twenty clauses were used to provide initial
guidance to the annotators. With the aid of a list of
possible labels and the grammatical categories they
correspond to, each annotator was asked to label the
seed clauses, and these labels were compared to the
gold standard labels. Annotators were told which
labels were correct and which were incorrect, and
the process was repeated until all morphemes were
correctly labeled. In some cases during this training
phase, the correct label for a morpheme was sup-
plied to the annotator after several incorrect guesses.
3.3 Suggesting labels
We consider two situations with respect to the con-
tribution of the classifier: a suggest condition in
which the labels predicted by the machine learner
are shown to the annotator as she begins labeling a
selected clause, and a no-suggest condition in which
the annotator does not see the predicted labels.
In the suggest cases, the annotator is shown the la-
bel assigned the greatest likelihood by the tagger as
well as a list of several highly-likely labels, ranked
according to likelihood. To be included on this list,
a label must be assigned a probability greater than
half that of the most-likely label. In the no-suggest
cases, the annotator has access to a list of the la-
bels previously seen in the training data for a given
morpheme, ranked in order of frequency of occur-
rence with the morpheme in question; this is similar
to the input an annotator gets while glossing texts in
Shoebox/Toolbox. Specifically, Shoebox/Toolbox
presents previously seen glosses and labels for a
given morpheme in alphabetic order.
3.4 Sample selection
We consider three methods of selecting examples
for annotation?sequential (seq), random (rand), and
uncertainty sampling (al)?and the performance of
each method in both the suggest and the no-suggest
setups. For uncertainty sampling, we measure un-
certainty of a clause as the average entropy per mor-
pheme (i.e., per labeling decision).
3.5 Measuring annotation cost
Not all examples take the same amount of effort to
annotate. Even so, the bulk of the literature on active
learning assumes some sort of unit cost to determine
the effectiveness of different sample selection strate-
gies. Examples of unit cost measurements include
the number of documents in text classification, the
number of sentences in part-of-speech tagging (Set-
tles and Craven, 2008), or the number of constituents
in parsing (Hwa, 2000). These measures are conve-
nient for performing active learning simulations, but
awareness has grown that they are not truly repre-
sentative measures of the actual cost of annotation
(Haertel et al, 2008a; Settles et al, 2008), with Ngai
and Yarowsky (2000) being an early exception to the
unit-cost approach. Also, Baldridge and Osborne
(2004) use discriminants in parse selection, which
are annotation decisions that they later showed cor-
relate with timing information (Baldridge and Os-
borne, 2008).
The cost of annotation ultimately comes down to
40
money. Since annotator pay may be variable but will
(under standard assumptions) be constant for a given
annotator, the best approximation of likely cost sav-
ings is to measure the time taken to annotate under
different levels of automated support. This is es-
pecially important in sample selection and its inter-
action with automated suggestions: active learning
seeks to find more informative examples, and these
will most likely involve more difficult decisions, de-
creasing annotation quality and/or increasing anno-
tation time (Hachey et al, 2005). Thus, we measure
cost in terms of the time taken by each annotator on
each example. This allows us to measure the actual
time taken to produce a given labeled data set, and
thus compare the effectiveness of different levels of
automated support plus their interaction with anno-
tators of different levels of expertise.
Recent work shows that paying attention to pre-
dicted annotation cost in sample selection itself can
increase the effectiveness of active learning (Settles
et al, 2008; Haertel et al, 2008b). Though we have
not explored cost-sensitive selection here, the sce-
nario described here is an appropriate test ground for
it: in fact, the results of our experiments, reported in
the next section, provide strong evidence for a real
natural language annotation task that active learning
selection with cost-sensitivity is indeed sub-optimal.
4 Discussion
This section presents and discusses preliminary re-
sults from the ongoing annotation experiments. The
Uspanteko corpus was split into training, develop-
ment, and held-out test sets, roughly 50%, 25%,
and 25%. Specifically, the training set of 21 texts
contains 38802 words, the development set of 5
texts contains 16792 words, and the held-out test
set, 6 texts, contains 18704 words. These are small
datasets, but the size is realistic for computational
work on endangered languages.
When measuring the performance of annotators,
factors like fatigue, frustration, and especially the
annotator?s learning process must be considered.
Annotators improve as they see more examples (es-
pecially the non-expert annotator). To minimize the
impact of the annotator?s learning process on the re-
sults, annotation is done in rounds. Each round con-
sists of ten clauses from each of the six experimental
0 10 20 30 40 50
0
10
20
30
40
Number of Annotation Rounds
Seco
nds 
per M
orph
eme
Non?expertExpert
Figure 1: Average annotation time (in seconds per mor-
pheme) over annotation rounds, averaged over all six con-
ditions for each annotator.
cases for each annotator. The newly-labeled clauses
are then added to the labeled training data, and a new
tagging model is trained on the updated training set
and evaluated on the development set. Both annota-
tors have completed fifty-one rounds of annotation
so far, labeling 510 clauses for each of the six ex-
perimental conditions. The average number of mor-
phemes labeled is 3059 per case. Because the anno-
tation experiments are ongoing, we discuss results in
terms of the trends seen thus far.
4.1 Annotator speed
The expert annotator showed a small increase in
speed after an initial familiarization period, and the
non-expert showed a dramatic increase. Figure 1
plots the number of seconds taken per morpheme
over the course of annotation, averaged over all six
conditions for each annotator. The slowest, fastest,
and mean rates, in seconds per morpheme, for the
expert annotator were 12.60, 1.89, and 4.14, respec-
tively. For the non-expert, they were 59.71, 1.90,
and 8.03.
4.2 Accuracy of model on held-out data
Table 1 provides several measures of the current
state of annotation in all 12 conditions after 51
rounds of annotation. The sixth column, labeled
41
Anno Suggest Select Time (sec) #Morphs Model Accuracy Total Accuracy of Annotation
NonExp N Seq 23739.79 3314 63.28 63.92
NonExp N Rand 22721.11 2911 68.36 68.69
NonExp N AL 23755.71 2911 68.26 67.84
NonExp Y Seq 21514.05 2887 66.55 66.89
NonExp Y Rand 22189.68 3002 68.41 68.73
NonExp Y AL 25731.57 2750 67.63 67.30
Exp N Seq 11862.39 3354 61.15 61.88
Exp N Rand 11665.10 3043 64.60 64.91
Exp N AL 13894.14 3379 66.74 66.47
Exp Y Seq 11758.74 2892 61.12 61.48
Exp Y Rand 11426.85 2979 60.13 60.57
Exp Y AL 16253.40 3296 63.30 63.15
Table 1: After 51 rounds of annotation: ModelAcc=accuracy on development set, TotalAnnoAcc=accuracy of fully-labeled corpus
ModelAcc, shows the accuracy of models on the
development data. This represents a unit cost as-
sumption at the clause level: measured this way, the
results would suggest that the non-expert was best
served by random selection, with no effect from ma-
chine suggestions. For the expert, they suggest ac-
tive learning without suggestions is best, and that
suggestions actually hurt effectiveness.
4.3 Accuracy of fully-labeled corpus
We are particularly concerned with the question of
how to develop a fully-labeled corpus with the high-
est level of accuracy, given a finite set of resources.
Thus, we combine the portion of the training set la-
beled by the human annotator with the results of tag-
ging the remainder of the training set with the model
trained on those annotations. The rightmost column
of Table 1, labeled Total Accuracy of Annotation,
shows the accuracy of the fully labeled training set
(part human, part machine labels) after 51 rounds.
These accuracies parallel the model accuracies: ran-
dom selection is best for the non-expert annotator,
and uncertainty selection is best for the expert.
Since this tagging task involves labeling mor-
phemes, a clause cost assumption is not ideal?e.g.,
active learning tends to select longer clauses and
thereby obtains more labels. To reflect this, a sub-
clause cost can help: here we use the number of
morphemes annotated. The column labeled Tokens
in Table 2 shows the total accuracy achieved in each
condition when human annotation ceases at 2750
morphemes. The figure in parentheses is the cumu-
lative annotation time at the morpheme cut-off point.
Here, the non-expert does best: he took great care
with the annotations and was clearly not tempted to
Anno Suggest Select Time Tokens (time)
(11427 sec) (2750 morphs)
NonExp N Seq 55.01 59.80 (21678 secs)
NonExp N Rand 59.95 68.68 (22069 secs)
NonExp N AL 59.86 67.70 (22879 secs)
NonExp Y Seq 60.27 66.79 (21053 secs)
NonExp Y Rand 62.96 68.38 (21194 secs)
NonExp Y AL 59.18 67.30 (25732 secs)
Exp N Seq 61.21 59.18 (10110 secs)
Exp N Rand 64.92 64.42 (10683 secs)
Exp N AL 65.72 65.74 (11826 secs)
Exp Y Seq 61.47 61.47 (11436 secs)
Exp Y Rand 60.57 61.16 (10934 secs)
Exp Y AL 61.54 62.87 (13957 secs)
Table 2: For given cost, accuracy of fully-labeled corpus.
accept erroneous suggestions from the machine la-
beler. In contrast, the expert does seem to have ac-
cepted many bad machine suggestions.
Morpheme unit cost is more fine-grained than
clause-level cost, but it hides the fact that the ex-
pert annotator needed far less time to produce a cor-
pus of higher overall labeled quality than the non-
expert. This can be seen in the Time column of
Table 2, which gives the total annotation accuracy
when 11427 seconds are alloted for human label-
ing. The expert annotator achieved the highest accu-
racy for total labeling of the training set using active
learning without machine label suggestions. Active
learning helps the non-expert as well, but his best
condition is random selection with machine labels.
4.4 Annotator accuracy by round
Active learning clearly selects harder examples that
hurt the non-expert?s performance. To see this
clearly, we measured the accuracy of the annotators?
labels for each round of each experimental setup,
42
0 10 20 30 40 50
50
60
70
80
90
100
Number of Annotation Rounds
Sing
le R
oun
d Ac
cura
cy
Suggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential
0 10 20 30 40 50
50
60
70
80
90
100
Number of Annotation Rounds
Sing
le R
oun
d Ac
cura
cy
Suggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential
(a) (b)
Figure 2: Single round accuracy per round for each experiment type by: (a) non-expert annotator, (b) expert annotator
given in Fig. 2. It is not clear at this stage whether
the tag suggestions by the machine labeler are help-
ful to human annotation. It is useful to compare the
cases where the machine learner is not involved in
example selection (i.e. random and sequential) to
uncertainty sampling, which does involve the ma-
chine learner. One thing that is apparent is that when
active learning is used to select samples for annota-
tion, both the expert and non-expert annotator have
a harder time providing correct tags. A point of con-
trast between the expert and non-expert is that the
non-expert generally outperforms the expert on label
accuracy in the non-active learning scenarios. The
non-expert was very careful with his labeling deci-
sions, but also much slower than the expert. In the
end, speedier annotation rates allowed the expert an-
notator to achieve higher accuracies in less time.
5 Conclusion
We have described a set of ongoing pilot experi-
ments designed to test the utility of machine label-
ing and active learning in the context of documen-
tary linguistics. The production of IGT is a realistic
annotation scenario which desperately needs label-
ing efficiency improvements. Our preliminary re-
sults suggest that both machine labeling and active
learning can increase the effectiveness of annotators,
but they interact quite strongly with the expertise of
the annotators. In particular, though active learn-
ing works well with the expert annotator, for a non-
expert annotator it seems that random selection is
a better choice. However, we stress that our anno-
tation experiments are ongoing. Active learning is
often less effective early in the learning curve, es-
pecially when automated label suggestions are pro-
vided, because the model is not yet accurate enough
to select truly useful examples, nor to suggest labels
for them reliably (Baldridge and Osborne, 2004).
Thus, we expect automation via uncertainty sam-
pling and/or suggestion may gather momentum and
outpace random selection and/or no suggestions by
wider margins as annotation continues.
Acknowledgments
This work is funded by NSF grant BCS 06651988
?Reducing Annotation Effort in the Documentation
of Languages using Machine Learning and Active
Learning.? Thanks to Katrin Erk, Nora England,
Michel Jacobson, and Tony Woodbury; and to anno-
tators Telma Kaan Pixabaj and Eric Campbell. Fi-
nally, thanks to the anonymous reviewers for valu-
able feedback.
43
References
Miguel A?ngel Vicente Me?ndez. 2007. Diccionario bil-
ingu?e Uspanteko-Espan?ol. Cholaj Tzijb?al li Uspan-
teko. Okma y Cholsamaj, Guatemala.
Jason Baldridge and Miles Osborne. 2004. Active learn-
ing and the total cost of annotation. In Proceedings of
Empirical Approaches to Natural Language Process-
ing (EMNLP).
Jason Baldridge and Miles Osborne. 2008. Active learn-
ing and logarithmic opinion pools for HPSG parse se-
lection. Natural Language Engineering, 14(2):199?
222.
David Crystal. 2000. Language Death. Cambridge Uni-
versity Press, Cambridge.
Ben Hachey, Beatrice Alex, and Markus Becker. 2005.
Investigating the effects of selective sampling on the
annotation task. In Proceedings of the 9th Conference
on Computational Natural Language Learning, Ann
Arbor, MI.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and McClanahan Peter. 2008a. Assessing the
costs of sampling methods in active learning for anno-
tation. In Proceedings of ACL-08: HLT, Short Papers,
pages 65?68, Columbus, Ohio, June. Association for
Computational Linguistics.
Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger, and
James L. Carroll. 2008b. Return on investment for
active learning. In Proceedings of the NIPS Workshop
on Cost-Sensitive Learning. ACL Press.
Rebecca Hwa. 2000. Sample selection for statistical
grammar induction. In Proceedings of the 2000 Joint
SIGDAT Conference on EMNLP and VLC, pages 45?
52, Hong Kong, China, October.
William Lewis and Fei Xia. 2008. Automatically iden-
tifying computationally relevant typological features.
In Proceedings of IJCNLP-2008, Hyderabad, India.
John Lowe, Michel Jacobson, and Boyd Michailovsky.
2004. Interlinear text editor demonstration and projet
archivage progress report. In 4th EMELD workshop
on Linguistic Databases and Best Practice, Detroit,
MI.
Grace Ngai and David Yarowsky. 2000. Rule Writing or
Annotation: Cost-efficient Resource Usage for Base
Noun Phrase Chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 117?125, Hong Kong.
Alexis Palmer and Katrin Erk. 2007. IGT-XML: An
XML format for interlinearized glossed text. In Pro-
ceedings of the Linguistic Annotation Workshop (LAW-
07), ACL07, Prague.
Telma Can Pixabaj, Miguel Angel Vicente Me?ndez,
Mar??a Vicente Me?ndez, and Oswaldo Ajcot Damia?n.
2007. Text collections in Four Mayan Languages.
Archived in The Archive of the Indigenous Languages
of Latin America.
Michael Richards. 2003. Atlas lingu???stico de Guatemala.
Servipresna, S.A., Guatemala.
Stuart Robinson, Greg Aumann, and Steven Bird. 2007.
Managing fieldwork data with Toolbox and the Natu-
ral Language Toolki t. Language Documentation and
Conservation, 1:44?57.
Burr Settles and Mark Craven. 2008. An analysis of
active learning strategies for sequence labeling tasks.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1070?1079, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Proceed-
ings of the NIPS Workshop on Cost-Sensitive Learn-
ing, pages 1069?1078. ACL Press.
Fei Xia and William Lewis. 2007. Multilingual struc-
tural projection across interlinear text. In Proceedings
of HLT/NAACL 2007, Rochester, NY.
Fei Xia and William Lewis. 2008. Repurposing theoreti-
cal linguistic data for tool development antd search. In
Proceedings of IJCNLP-2008, Hyderabad, India.
44
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 196?206,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Crouching Dirichlet, Hidden Markov Model:
Unsupervised POS Tagging with Context Local Tag Generation
Taesun Moon, Katrin Erk, and Jason Baldridge
Department of Linguistics
University of Texas at Austin
1 University Station B5100
Austin, TX 78712-0198 USA
{tsmoon,katrin.erk,jbaldrid}@mail.utexas.edu
Abstract
We define the crouching Dirichlet, hidden
Markov model (CDHMM), an HMM for part-
of-speech tagging which draws state prior dis-
tributions for each local document context.
This simple modification of the HMM takes
advantage of the dichotomy in natural lan-
guage between content and function words. In
contrast, a standard HMM draws all prior dis-
tributions once over all states and it is known
to perform poorly in unsupervised and semi-
supervised POS tagging. This modification
significantly improves unsupervised POS tag-
ging performance across several measures on
five data sets for four languages. We also show
that simply using different hyperparameter
values for content and function word states in
a standard HMM (which we call HMM+) is
surprisingly effective.
1 Introduction
Hidden Markov Models (HMMs) are simple, ver-
satile, and widely-used generative sequence models.
They have been applied to part-of-speech (POS) tag-
ging in supervised (Brants, 2000), semi-supervised
(Goldwater and Griffiths, 2007; Ravi and Knight,
2009) and unsupervised (Johnson, 2007) training
scenarios. Though discriminative models achieve
better performance in both semi-supervised (Smith
and Eisner, 2005) and supervised (Toutanova et al,
2003) learning, there has been only limited work on
unsupervised discriminative sequence models (e.g.,
on synthetic data and protein sequences (Xu et al,
2006)), and none to POS tagging.
The tagging accuracy of purely unsupervised
HMMs is far below that of supervised and semi-
supervised HMMs; this is unsurprising as it is still
not well understood what kind of structure is being
found by an unconstrained HMM (Headden III et al,
2008). However, HMMs are fairly simple directed
graphical models, and it is straightforward to ex-
tend them to define alternative generative processes.
This also applies to linguistically motivated HMMs
for recovering states and sequences that correspond
more closely to those implicitly defined by linguists
when they label sentences with parts-of-speech.
One way in which a basic HMM?s structure is a
poor model for POS tagging is that there is no inher-
ent distinction between (open-class) content words
and (closed-class) function words. Here, we propose
two extensions to the HMM. The first, HMM+, is a
very simple modification where two different hyper-
parameters are posited for content states and func-
tion states, respectively. The other is the crouch-
ing Dirichlet, hidden Markov model (CDHMM), an
extended HMM that captures this dichotomy based
on the statistical evidence that comes from context.
Content states display greater variance across lo-
cal context (e.g. sentences, paragraphs, documents),
and we capture this variance by adding a component
to the model for content states that is based on la-
tent Dirichlet alocation (Blei et al, 2003). This ex-
tension is in some ways similar to the LDAHMM
of Griffiths et al (2005). Both models are compos-
ite in that two distributions do not mix with each
other. Unlike the LDAHMM, the generation of con-
tent states is folded into the CDHMM process.
We compare the HMM+ and CDHMM against a
basic HMM and LDAHMM on POS tagging on a
more extensive and diverse set of languages than
previous work in monolingual unsupervised POS
tagging: four languages from three families (Ger-
manic: English and German; Romance: Portuguese;
196
and Mayan: Uspanteko). The CDHMM easily out-
performs all other models, including HMM+, across
three measures (accuracy, F-score, and variation
of information) for unsupervised POS tagging on
most data sets. However, the HMM+ is surpris-
ingly competitive, outperforming the basic HMM
and LDAHMM, and rivaling or even passing the
CDHMM on some measures and data sets.
2 Background
The Bayesian formulation for a basic HMM (Gold-
water and Griffiths, 2007) is:
?t|? ? Dir(?)
?t|? ? Dir(?)
wi|ti = t ? Mult(?t)
ti|ti?1 = t ? Mult(?t)
Dir is the conjugate Dirichlet prior to Mult (a multi-
nomial distribution). The state transitions are gen-
erated by Mult(?t) whose prior ?t is generated by
Dir(?) with a symmetric (i.e. uniform) hyperparam-
eter ?. Emissions are generated by Mult(?t) with
a prior ?t generated by Dir(?) with a symmetric
hyperparameter ?. Hyperparameter values smaller
than one encourage posteriors that are peaked, with
smaller values increasing this concentration. It is
not necessary that the hyperparameters be symmet-
ric, but this is a common approach when one wants
to be na??ve about the data. This is particularly ap-
propriate in unsupervised POS tagging with regard
to novel data since there won?t be a priori grounds
for favoring certain distributions over others.
There is considerable work on extensions to
HMM-based unsupervised POS tagging (see ?6),
but here we concentrate on the LDAHMM (Grif-
fiths et al, 2005), which models topics and state
sequences jointly. The model is a composite of a
probabilistic topic model and an HMM in which a
single state is allocated for words generated from
the topic model. A strength of this model is that it
is able to use less supervision than previous topic
models since it does not require a stopword list.
While the topic model component still uses the bags-
of-words assumption, the joint model infers which
words are more likely to carry topical content and
which words are more likely to contribute to the
local sequence. This model is competitive with a
standard topic model, and its output is also compet-
itive when compared with a standard HMM. How-
ever, Griffiths et al (2005) note that the topic model
component inevitably loses some finer distinctions
with respect to parts-of-speech. Though many con-
tent states such as adjectives, verbs, and nouns can
vary a great deal across documents, the topic state
groups these words together. This leads to assign-
ment of word tokens to clusters that are a poorer fit
for POS tagging. This paper shows that a model that
conflates the LDAHMM topics with content states
can significantly improve POS tagging.
3 Models
We aim to model the fact that in many languages
words can generally be grouped into function words
and content words and that these groups often
have significantly different distributions. There are
few function words and they appear frequently,
while there are many content words appearing infre-
quently. Another difference in distribution is often
implied in information retrieval by the use of stop-
word filters and tf-idf values to remove or reduce the
influence of words which occur frequently but have
low variance (i.e. their global probability is similar
to their local probability in a document).
A difference in distribution is also revealed when
the parts-of-speech are known. When no smoothing
parameters are added, the joint probability of a word
that is not ?the? or ?a? occurring with a DT tag (in
the Penn Treebank) is almost always zero. Similarly
peaked distributions are observed for other function
categories such as MD and CC. On the other hand,
the joint probability of any word occurring with NN
is much less likely to be zero and the distribution is
much less likely to be peaked.
We attempt to account for these two distributional
properties?that certain words have higher variance
across contexts (e.g. a document) and that certain
tags have more peaked emission distributions?in a
sequence model. To do this, we define the crouching
Dirichlet, hidden Markov model1 (CDHMM). This
model, like LDAHMM, captures items of high vari-
ance across contexts, but it does so without losing
1We call our model a ?crouching Dirichlet? model since it
involves a Dirichlet prior that generates distributions for certain
states as if it were ?crouching? on the side.
197
wi
?
?
?
?
ti? ? ? ? ? ?
?
?
?
?
Figure 1: Graphical representation of relevant vari-
ables and dependencies at a given time step i. Ob-
served word wi is dependent on hidden state ti.
Edges to priors ?, ?, ? may or may not be activated
depending on the value of ti. The edge to transition
prior ? is always activated. Hyperparameters to pri-
ors are represented by dots. See ?3.1 for details.
sequence distinctions, namely, a given word?s lo-
cal function via its part-of-speech. We also define
the HMM+, a simple adaptation of a basic HMM
which accounts for the latter property by using dif-
ferent priors for emissions from content and function
states.
3.1 CDHMM
The CDHMM incorporates an LDA-like module to
its graphical structure in order to capture words
and tags which have high variance across contexts.
Such tags correspond to content states. Like the
LDAHMM, the model is composite in that distribu-
tions over a single random variable are composed
of several different distribution functions which de-
pend on the value of the underlying variable.
We posit the following model (see fig. 1 for a dia-
gram of dependencies and all variables involved at a
single time step). We observe a sequence of tokens
w=(w1, . . . , wN ) that we assume is generated by
an underlying state sequence t=(t1, . . . , tN ) over a
state alphabet T with first order Markov dependen-
cies. T is a union of disjoint content states C and
function states F . In this composite model, the pri-
ors for the emission and transition for each step in
the sequence depend on whether state t at step i is
t?C or t?F . If t?C , the word emission is depen-
dent on ? (the content word prior) and the state tran-
sition is dependent on ? (the ?topic? prior) and ? (the
transition prior). If t?F , the word emission proba-
bility is dependent on ? (the function word prior)
and the state transition on ? (again, the transition
prior). Therefore, if t?F , the transition and emis-
sion structure is identical to the standard Bayesian
HMM.
To elaborate, three prior distributions are defined
globally for this model: (1) ?t, the transition prior
such that p(t?|t, ?t) = ?t?|t (2) ?t, the function word
prior such that p(w|t, ?t) = ?w|t (3) ?t, the content
word prior such that p(w|t, ?t) = ?w|t. Locally for
each context d (documents in our case), we define
?d, the topic prior such that p(t|?d) = ?t|d for t?C .
The generative story is as follows:
1. For each state t?T
(a) Draw a distribution over states ?t ?
Dir(?)
(b) If t?C , draw a distribution over words
?t ? Dir(?)
(c) If t?F , draw a distribution over words
?t ? Dir(?)
2. For each context d
(a) Draw a distribution ?d ? Dir(?) over
states t?C
(b) For each word wi in d
i. draw ti from ?ti?1 ? ?d
ii. if ti?C , then draw wi from ?ti , else
draw wi from ?ti
For each context d, we draw a prior distribution
?d?formally identical to the LDA topic prior?that
is defined only for the states t?C . This prior is then
used to weight the draws for states at each word,
from ?ti?1 ? ?d, where we have defined the vector
valued operation ? as follows:
(?ti?1 ? ?d)ti =
{
1
Z ?ti|ti?1 ? ?ti|d ti?C
1
Z ?ti|ti?1 ti?F
where (?ti?1 ? ?d)ti is the element corresponding to
state ti in the vector ?ti?1 ? ?d. Z is a normalization
constant such that the probability mass sums to one.
198
p(ti|t?i,w) ?
?
?
?
?
?
Nwi|ti+?
Nti+W?
Nti|di+?
Ndi+C?
?
Nti|ti?1+?
??
Nti+1|ti+I[ti?1=ti=ti+1]+?
?
Nti+T?+I[ti=ti?1]
ti ? C
Nwi|ti+?
Nti+W?
?
Nti|ti?1+?
??
Nti+1|ti+I[ti?1=ti=ti+1]+?
?
Nti+T?+I[ti=ti?1]
ti ? F
Figure 2: Conditional distribution for ti in the CDHMM.
The important thing to note is that the draw for
states at each word is proportional to a composite
of (a) the product of the individual elements of the
topic and transition priors when ti?C and (b) the
transition priors when ti?F . The draw is propor-
tional to the product of topic and transition priors
when ti?C because we have made a product of ex-
perts (PoE) factorization assumption (Hinton, 2002)
for tractability and to reduce the size of our model.
Without such an assumption, the transition parame-
ters would lie in a partitioned space of size O(|C|4)
as opposed to O(|T |2) for the current model. Fur-
thermore, this combination of a composite hidden
state space with a product of experts assumption al-
lows us to capture high variance for certain states.
To summarize, the CDHMM is a composite
model where both the observed token and the hidden
state variable are composite distributions. For the
hidden state, this means that there is a ?topical? ele-
ment with high variance across contexts that is em-
bedded in the state sequence for a subset of events.
We embed this element through a PoE assumption
where transitions into content states are modeled as
a product of the transition probability and the local
probability of the content state.
Inference. We use a Gibbs sampler (Gao and
Johnson, 2008) to learn the parameters of this and
all other models under consideration. In this infer-
ence regime, two distributions are of particular in-
terest. One is the posterior density and the other is
the conditional distribution, neither of which can be
learned in closed form.
Letting ? = (?, ?, ?, ?) and h = (?, ?, ?, ?), the
posterior density is given as
p(?|w, t;h) ? p(w, t|?)p(?;h)
Note that p(w, t|?) is equal to
D
?
d
Nd
?
i
(
?wi|ti?ti|d?ti|ti?1
)I[ti?C]
(
?wi|ti?ti|ti?1
)I[ti?F ] (1)
where I[?] is the indicator function, D is the number
of documents in the corpus and Nd is the number of
tokens in document d.
Another important measure is the conditional dis-
tribution which is conditioned on all the random
variables except the hidden state variable of interest
and which is derived by integrating out the priors:
p(ti|t?i,w;h) ? p(ti|t?i;h)p(wi|t,w?i;h) (2)
where t?i is the joint random variable t without ti
and w?i is w without wi.
There are two well-known approaches to conduct-
ing Gibbs sampling for HMMs. The default method
is to sample ? based on the posterior, then sample
each ti based on the conditional distribution. An-
other approach is to sample directly from the con-
ditional distribution without sampling from the pos-
terior since the conditional distribution incorporates
the posterior through integration. This is called a
collapsed Gibbs sampler, which is the method em-
ployed for the models in this study.
The full conditional distribution for tag transitions
for the Gibbs sampler is given in Figure 2. At each
time step, we decrement all counts for the current
value of ti, sample a new value for ti from a multino-
mial proportional to the conditional distribution and
assign that value to ti. ?, ? are the hyperparameters
for the word emission priors of the content states and
function states, respectively. ? is the hyperparame-
ter for the state transition priors. ? is the hyperpa-
rameter for the state prior given that it is in some
context d. Note that we have overridden notation so
199
that C and T here refer to the size of the alphabet.
W is the size of the vocabulary. Notation such as
Nti|ti?1 refers to the counts of the events indicated
by the subscript, minus the current token and tag un-
der consideration. Nti|ti?1 is the number of times ti
has occurred after ti?1 minus the tag for wi. Nwi|ti
is the number of times wi has occurred with ti minus
the current value. Nti and Ndi are the counts for the
given tag and document minus the current value.
In its broad outline, the CDHMM is not much
more complicated than an HMM since the decompo-
sition (eqn. 1) is nearly identical to that of an HMM
with the exception that conditional probabilities for
a subset of the states?the content states?are local.
An inference algorithm can be derived that involves
no more than adding a single term to the standard
MCMC algorithm for HMMs (see Figure 2).
3.2 HMM+
The CDHMM explicitly posits two different types
of states: function states and content states. Hav-
ing made this distinction, there is a very simple way
to capture the difference in emission distributions
for function and content states within an otherwise
standard HMM: posit different hyperparameters for
the two types. One type has a small hyperparame-
ter to model a sparse distribution for function words
and the other has a relatively large hyperparameter
to model a distribution with broader support. This
extension, which we refer to as HMM+, provides an
important benchmark to compare with the CDHMM
to see how much is gained by its additional ability to
model the fact that function words occur frequently
but have low variance across contexts.
As with the CDHMM, we use Gibbs sampling to
estimate the model parameters while holding the two
different hyperparameters fixed. The conditional
distribution for tag transitions for this model is iden-
tical to that in fig. 2 except that it does not have the
second term Nti|di+?Ndi+C? in the first case where ti?C .
We are not aware of a published instance of such
an extension to the HMM?which our results show
to be surprisingly effective. Goldwater and Griffiths
(2007) posits different hyperparameters for individ-
ual states, but not for different groups of states.
corpus tokens docs avg. tags
WSJ 974254 1801 541 43
Brown 797328 343 2325 80
Tiger 447079 1090 410 58
Floresta 197422 1956 101 19
Uspanteko 70125 29 2418 83
Table 2: Number of tokens, documents, average to-
kens per document and total tag types for each cor-
pus.
4 Data and Experiments
Data. We use five datasets from four languages
(English, German, Portuguese, Uspanteko) for eval-
uating POS tagging performance.
? English: the Brown corpus (Francis et al, 1982)
and the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1994).
? German: the Tiger corpus (Brants et al, 2002).
? Portuguese: the full Bosque subset of the Floresta
corpus (Afonso et al, 2002).
? Uspanteko (an endangered Mayan language of
Guatemala): morpheme-segmented and POS-
tagged texts collected and annotated by the
OKMA language documentation project (Pixabaj
et al, 2007); we use the cleaned-up version de-
scribed in Palmer et al (2009).
Table 2 provides the statistics for these corpora.
We lowercase all words, do not remove any punc-
tuation or hapax legomena, and we do not replace
numerals with a single identifier. Due to the nature
of the models, document boundaries are retained.
Evaluation We report values for three evaluation
metrics on all five corpora, using their full tagsets.
? Accuracy: We use a greedy search algorithm to
map each unsupervised tag to a gold label such
that accuracy is maximized. We evaluate on a
1-to-1 mapping between unsupervised tags and
gold labels, as well as many-to-1 (M-to-1), cor-
responding to the evaluation mappings used in
Johnson (2007). The 1-to-1 mapping provides a
stricter evaluation. The many-to-one mapping, on
the other hand, may be more adequate as unsu-
pervised tags tend to be more fine-grained than
200
Model Accuracy Pairwise P/R Scores VI1-to-1 M-to-1 P R F
W
SJ
(50
) HMM 0.34 (0.01) 0.49 (0.03) 0.51 (0.03) 0.19 (0.01) 0.28 (0.01) 3.72 (0.08)
LDAHMM 0.30 (0.04) 0.45 (0.04) 0.25 (0.07) 0.27 (0.03) 0.26 (0.04) 3.64 (0.14)
HMM+ 0.42 (0.04) 0.46 (0.05) 0.24 (0.03) 0.49 (0.03) 0.32 (0.03) 2.65 (0.15)
CDHMM 0.44 (0.01) 0.58 (0.02) 0.31 (0.01) 0.43 (0.03) 0.36 (0.02) 2.73 (0.08)
B
ro
w
n
(50
) HMM 0.32 (0.01) 0.50 (0.02) 0.60 (0.02) 0.18 (0.00) 0.28 (0.01) 3.82 (0.05)
LDAHMM 0.28 (0.06) 0.41 (0.08) 0.25 (0.10) 0.28 (0.05) 0.25 (0.05) 3.71 (0.21)
HMM+ 0.43 (0.06) 0.48 (0.07) 0.29 (0.05) 0.50 (0.04) 0.37 (0.05) 2.63 (0.19)
CDHMM 0.48 (0.02) 0.62 (0.02) 0.32 (0.03) 0.54 (0.04) 0.40 (0.03) 2.48 (0.06)
Ti
ge
r
(50
) HMM 0.29 (0.02) 0.49 (0.02) 0.49 (0.04) 0.14 (0.01) 0.22 (0.02) 3.91 (0.06)
LDAHMM 0.31 (0.04) 0.50 (0.04) 0.26 (0.07) 0.24 (0.02) 0.25 (0.04) 3.51 (0.11)
HMM+ 0.41 (0.08) 0.44 (0.05) 0.25 (0.05) 0.58 (0.10) 0.35 (0.06) 2.70 (0.25)
CDHMM 0.47 (0.01) 0.61 (0.02) 0.45 (0.01) 0.58 (0.03) 0.50 (0.02) 2.72 (0.04)
U
sp
.
(50
) HMM 0.36 (0.01) 0.49 (0.02) 0.39 (0.01) 0.18 (0.00) 0.25 (0.00) 3.63 (0.04)
LDAHMM 0.35 (0.02) 0.47 (0.02) 0.26 (0.04) 0.23 (0.03) 0.24 (0.02) 3.52 (0.09)
HMM+ 0.32 (0.02) 0.35 (0.03) 0.12 (0.02) 0.52 (0.05) 0.20 (0.02) 3.13 (0.06)
CDHMM 0.39 (0.02) 0.50 (0.02) 0.16 (0.02) 0.39 (0.03) 0.23 (0.02) 3.00 (0.06)
Fl
o
r.
(50
) HMM 0.30 (0.01) 0.58 (0.03) 0.62 (0.05) 0.18 (0.01) 0.28 (0.01) 3.51 (0.06)
LDAHMM 0.36 (0.06) 0.59 (0.04) 0.55 (0.10) 0.29 (0.07) 0.38 (0.08) 3.22 (0.15)
HMM+ 0.35 (0.04) 0.52 (0.02) 0.28 (0.04) 0.43 (0.06) 0.34 (0.04) 2.58 (0.07)
CDHMM 0.36 (0.01) 0.64 (0.02) 0.37 (0.02) 0.27 (0.01) 0.31 (0.01) 2.73 (0.05)
Table 1: Evaluation on WSJ, Brown, Tiger, Floresta and Uspanteko for models with 50 states. For VI, lower
is better
gold part-of-speech tags. In particular, they tend
to form semantically coherent sub-classes of gold
parts of speech.
? Pairwise Precision and Recall: Viewing tagging
as a clustering task over tokens, we evaluate pair-
wise precision (P ) and recall (R) between the
model tag sequence (M ) and gold tag sequence
(G) by counting the true positives (tp), false pos-
itives (fp) and false negatives (fn) between the
two and setting P = tp/(tp + fp) and R =
tp/(tp+ fn). tp is the number of token pairs that
share a tag in M as well as in G, fp is the number
token pairs that share the same tag in M but have
different tags in G, and fn is the number token
pairs assigned a different tag in M but the same
in G (Meila, 2007). We also provide the f -score
which is the harmonic mean of P and R.
? Variation of Information (VI): The variation of
information is an information theoretic metric
that measures the amount of information lost and
gained in going from tag sequenceM toG (Meila,
2007). It is defined as V I(M,G) = H(M) +
H(G) ? 2I(M,G) where H denotes entropy and
I mutual information. Goldwater and Griffiths
(2007) noted that this measure can point out mod-
els that have more consistent errors in the form
of lower VI, even when accuracy figures are the
same.
We also report learning curves on M-to-1 with ge-
ometrically increasing training set sizes of 8, 16, 32,
64, 128, 256, 512, 1024, and all documents, or as
many as possible given the corpus.
5 Experiments
In this section we discuss our parameter settings and
experimental results.
5.1 Models and Parameters
We compare four different models:
? HMM: a standard HMM
? HMM+: an HMM in which the hyperparameters
for the word emissions are asymmetric, such that
content states have different word emission priors
compared to function states.
? LDAHMM: an HMM with a distinguished state
that generates words from a topic model (Griffiths
et al, 2005)
201
WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
HMM+
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
WSJ Brown Tiger Floresta Uspanteko
LDAHMM
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
WSJ Brown Tiger Floresta Uspanteko
CDHMM
a
cc
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
Figure 3: Averaged many-to-one accuracy on the full tagset for the models HMM+, LDAHMM, CDHMM
when the number of states is set at 20, 30, 40 and 50 states.
? CDHMM: our HMM with context-based emis-
sions, where the context used is the document
We implemented all of these models, ensuring per-
formance differences are due to the models them-
selves rather than implementation details.
For all models, the transition hyperparameters ?
are set to 0.1. For the LDAHMM and HMM all emis-
sion hyperparameters are set to 0.0001. These fig-
ures are the MCMC settings that provided the best
results in Johnson (2007). For the models that distin-
guish content and function states (HMM+, CDHMM),
we fixed the number of content states at 5 and set the
function state emission hyperparameters ? = 0.0001
and the content state emission hyperparameters ? =
0.1. For the models with an LDA or LDA-like com-
ponent (LDAHMM, CDHMM), we set the topic or
content-state hyperparameter ? = 1.
For decoding, we use maximum posterior decod-
ing to obtain a single sample after the required burn-
in, as has been done in other unsupervised HMM
experiments. We use this sample for evaluation.
5.2 Results
Results for all models on the full tagset are provided
in table 1.2 Each number is the mean accuracy of
ten randomly initialized samples after a single chain
burn-in of 1000 iterations. The model with a sta-
tistically significant (p < 0.05) best score for each
measure and data set is given in plain bold. In cases
2Similar results are obtained with reduced tagsets, as is com-
monly done in other work on unsupervised POS-tagging.
where the differences for the best models are not sig-
nificantly different from each other, but are signifi-
cantly better from the others, the top model scores
are given in bold italic.
CDHMM is extremely strong on the accuracy met-
ric: it wins or ties for all datasets for both 1-to-1 and
M-to-1 measures. For pairwise f -score, it obtains
the best score for two datasets (WSJ and Tiger), and
ties with HMM+ on Brown (we return to Uspanteko
and Floresta below in an experiment that varies the
number of states). For VI, HMM+ and CDHMM both
easily outperform the other models, with CDHMM
winning Brown and Uspanteko and HMM+ winning
Floresta.
In the case of Uspanteko, the absolute difference
in mean performance between models is smaller
overall but still significant. This is due to the reduced
variance between samples for all models. This is
striking because the non-CDHMM models have much
higher standard deviation on other corpora but have
sharply reduced standard deviation only for Uspan-
teko. The most likely explanation is that the Uspan-
teko corpus is much smaller than the other corpora.3
Nonetheless, CDHMM comes out strongest on most
measures.
A simple baseline for accuracy is to choose the
most frequent tag for all tokens; this gives accura-
cies of 0.14 (WSJ), 0.14 (Brown), 0.21 (Tiger), 0.20
3which is interesting in itself since the weak law of large
numbers implies that sample standard deviation decreases with
sample size, which in our case is the number of tokens rather
than the 10 samples under discussion
202
Model Accuracy P/R Scores VI1-to-1 M-to-1 P R F
U
sp
.
(10
0) HMM 0.36 (0.01) 0.58 (0.01) 0.56 (0.02) 0.16 (0.00) 0.25 (0.01) 3.53 (0.04)
LDAHMM 0.35 (0.01) 0.58 (0.02) 0.45 (0.04) 0.17 (0.01) 0.24 (0.01) 3.46 (0.06)
HMM+ 0.35 (0.02) 0.41 (0.02) 0.18 (0.01) 0.36 (0.03) 0.24 (0.01) 3.25 (0.08)
CDHMM 0.40 (0.01) 0.59 (0.01) 0.25 (0.02) 0.27 (0.02) 0.26 (0.01) 3.05 (0.03)
Fl
o
r.
(20
) HMM 0.31 (0.02) 0.48 (0.03) 0.40 (0.03) 0.21 (0.01) 0.28 (0.02) 3.54 (0.10)
LDAHMM 0.35 (0.06) 0.46 (0.06) 0.27 (0.07) 0.45 (0.08) 0.33 (0.05) 3.10 (0.10)
HMM+ 0.37 (0.04) 0.50 (0.03) 0.30 (0.02) 0.45 (0.06) 0.36 (0.03) 2.62 (0.06)
CDHMM 0.44 (0.02) 0.55 (0.02) 0.30 (0.01) 0.53 (0.03) 0.39 (0.02) 2.39 (0.07)
Table 3: Evaluation for Uspanteko and Floresta. Experiments in this table use state sizes that correspond
more closely to the size of the tag sets in the respective corpora.
(Floresta), and 0.11 (Uspanteko). Clearly, all of the
models easily outperform this baseline.
Number of states. Figure 3 shows the change in
accuracy for the different models for different cor-
pora when the overall number of states is varied
between 20 and 50. The figure shows results for
M-to-1. All models with the exception of HMM+
show improvements as the number of states is in-
creased. This brings up the valid concern (Clark,
2003; Johnson, 2007) that a model could posit a
very large number of states and obtain high M-to-
1 scores. However, it is neither the case here nor
in any of the studies we cite. Furthermore, as is
strongly suggested with HMM+, it does not seem as
if all models will benefit from assuming a large num-
ber of states.
Looking at the results by number of states on VI
and f -score for CDHMM(Figure 5), it is clear that
Floresta displays the reverse pattern of all other data
sets where performance monotonically deteriorates
as state sizes are increased. Though the exact reason
is unknown, we believe it is partially due to the fact
that Floresta has 19 tags. We therefore wondered
whether positing a state size that more closely ap-
proximated the size of the gold tag set performs bet-
ter. Since the discrepancy is greatest for Uspanteko
and Floresta, we present tabulated results for exper-
iments with state settings of 100 and 20 states re-
spectively (table 3). With the exception of VI (where
lower is better) for Uspanteko, the scores generally
improve when the model state size is closer to the
gold size. M-to-1 goes down for Floresta when 20
states are posited, but this is to be expected since this
score is defined, to a certain extent, to do better with
WSJ Brown Tiger Floresta Uspanteko
20
30
40
50
F?SCORE
f?
sc
or
e
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
WSJ Brown Tiger Floresta Uspanteko
VI
vi
0
1
2
3
4
Figure 5: f -score and VI for CDHMM by number of
states
larger models.
Variance. As we average performance figures
over ten runs for each model, it is also instructive
to consider standard deviation across runs. Standard
deviation is lowest for the CDHMM models and the
vanilla HMM. Standard deviation is high for HMM+
and LDAHMM. This is not surprising for LDAHMM,
since it has fifty topic parameters in addition to the
number of states posited, and random initial condi-
tions would have greater effect on the outcome than
for the other models. It is unexpected, however, that
HMM+ has high variance over different chains. The
model shares the large content emission hyperpa-
rameter ? = 0.1 with CDHMM. At this point, it can
only be assumed that the additional LDA component
acts as a regularization factor for CDHMM and re-
duced the volatility in having a large emission hy-
perparameter.
203
0 1 2 3 4 5 6
0.
3
0.
4
0.
5
0.
6
Brown WSJ Tiger
UspantekoFloresta
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2 3 4 5 6 7 8
0.
3
0.
4
0.
5
0.
6
0 1 2
0.
3
0.
4
0.
5
0.
6
hmm
hmm+
ldahmm
Figure 4: Learning curves on M-to-1 evaluation. The staples at each point represent two standard deviations.
Learning curves We present learning curves on
different sizes of subcorpora in Figure 4. The graphs
are box plots of the full M-1 accuracy figures on
10 randomly initialized training runs for seven sub-
corpora in Brown, nine in WSJ, Tiger, Floresta and
three in Uspanteko.
Comparing the graphs, the performance of HMM+
shows the strongest improvement for English and
German data as the amount of training data in-
creases. Also, it is evident that CDHMM posts con-
sistent performance gains across data sets as it trains
on more data. This stands in opposition to HMM and
LDAHMM which do not seem able to take advantage
of more information for WSJ and Floresta. This
suggests that performance for CDHMM and HMM+
could improve if the training corpora were aug-
mented with out-of-corpus raw data. One exception
to the consistent improvement over increased data is
the performance of the models on Uspanteko, which
uniformly flatline. One reason might be that the tags
are labeled over segmented morphemes instead of
words like the other corpora. Another could be that
Uspanteko has a relatively large number of tags in a
very small corpus.
6 Related work
Unsupervised POS tagging is an active area of re-
search. Most recent work has involved HMMs.
Given that an unconstrained HMM is not well under-
stood in POS tagging, much work has been done on
examining the mechanism and the properties of the
HMM as applied to natural language data (Johnson,
2007; Gao and Johnson, 2008; Headden III et al,
2008). Conversely, there has also been work focused
on improving the HMM as an inference procedure
that looked at POS tagging as an example (Graca et
al., 2009; Liang and Klein, 2009). Nonparametric
HMMs for unsupervised POS tag induction (Snyder
et al, 2008; Van Gael et al, 2009) have seen partic-
ular activity due to the fact that model size assump-
tions are unnecessary and it lets the data ?speak for
itself.?
There is also work on alternative unsupervised
models that are not HMMs (Schu?tze, 1993; Abend
et al, 2010; Reichart et al, 2010b) as well as re-
search on improving evaluation of unsupervised tag-
gers (Frank et al, 2009; Reichart et al, 2010a).
Though they did not concentrate on unsupervised
methods, Haghighi and Klein (2006) conducted an
unsupervised experiment that utilized certain to-
ken features (e.g. character suffixes of 3 or less,
204
has initial capital, etc.; the features themselves are
from Smith and Eisner (2005)) to learn parameters
in an undirected graphical model which was the
equivalent of an HMM in directed models. It was
also the first study to posit the one-to-one evalua-
tion criterion which has been repeated extensively
since (Johnson, 2007; Headden III et al, 2008;
Graca et al, 2009).
Finkel et al (2007) is an interesting variant of un-
supervised POS tagging where a parse tree is as-
sumed and POS tags are induced from this structure
non-parametrically. It is the converse of unsuper-
vised parsing which assumes access to a tagged cor-
pus and induces a parsing model.
Other models more directly influenced or closely
parallel our work. Griffiths et al (2005) is the work
that inspired the current approach where a set of
states is designated to capture variance across con-
texts. The primary goal of that model was to induce
a topic model given data that had not been filtered
of noise in the form of function words. As such,
distinguishing between topic states such that they
model different syntactic states was not attempted,
and we have seen in sec. 3 that such an extension is
not entirely straightforward.4 Boyd-Graber and Blei
(2009) has some parallels to our model in that a hid-
den variable over topics is distributed according to
a normalized product between a context prior and a
syntactic prior. However, it assumes a much greater
amount of information than we do in that a parse tree
as well as (possibly) POS tags are taken as observed.
The model has a very different goal from ours as
well, which is to infer a syntactically informed topic
model. Teichert and Daume? III (2010) is another
study with close similarities to our own. This study
models distinctions between closed class words and
open class words within a modified HMM. It is un-
clear from their formulation how the distinction be-
tween open class and closed class words is learned.
There is also extensive literature on learning se-
quence structure from unlabeled text (Smith and
Eisner, 2005; Goldberg et al, 2008; Ravi and
Knight, 2009) which assume access to a tag dic-
tionary. Goldwater and Griffiths (2007) deserves
mention for examining a semi-supervised model
4We tested a variant of LDAHMM in which more than one
state can generate topics. It did not achieve good results.
that sampled emission hyperparameters for each
state rather than a single symmetric hyperparame-
ter. They showed that this outperformed a symmet-
ric model. An interesting heuristic model is Zhao
and Marcus (2009) that uses a seed set of closed
class words to classify open class words.
7 Conclusion
We have shown that a hidden Markov model that
allocates a subset of the states to have distribu-
tions conditioned on localized domains can signif-
icantly improve performance in unsupervised part-
of-speech tagging. We have also demonstrated that
significant performance gains are possible simply
by setting a different emission hyperparameter for
a subgroup of the states. It is encouraging that these
results hold for both models not just on the WSJ but
across a diverse set of languages and measures.
We believe our proposed extensions to the HMM
are a significant contribution to the general HMM
and unsupervised POS tagging literature in that both
can be implemented with minimum modification
of existing MCMC inferred HMMs, have (nearly)
equivalent run times, produce output that is easy to
interpret since they are based on a generative frame-
work, and bring about considerable performance im-
provements at the same time.
Acknowledgments
The authors would like to thank Elias Ponvert and
the anonymous reviewers. This work was supported
by a grant from the Morris Memorial Trust Fund of
the New York Community Trust.
References
O. Abend, R. Reichart, and A. Rappoport. 2010. Im-
proved unsupervised POS induction through prototype
discovery. In Proceedings of ACL, pages 1298?1307.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. Flo-
resta sinta?(c)tica?: a treebank for Portuguese. In Pro-
ceedings of LREC, pages 1698?1703.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. The Journal of Machine Learning
Research, 3:993?1022.
J. L. Boyd-Graber and D. Blei. 2009. Syntactic topic
models. In Proceedings of NIPS, pages 185?192.
205
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith.
2002. The TIGER treebank. In Proceedings of the
Workshop on Treebanks and Linguistic Theories.
T. Brants. 2000. TnT: a statistical part-of-speech tag-
ger. In Proceedings of conference on Applied natural
language processing, pages 224?231.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proceedings of EACL, pages 59?66.
J. R. Finkel, T. Grenager, and C. D. Manning. 2007. The
infinite tree. In Proceedings of ACL, pages 272?279.
W.N. Francis, H. Kuc?era, and A.W. Mackie. 1982. Fre-
quency analysis of English usage: Lexicon and gram-
mar. Houghton Mifflin Harcourt.
S. Frank, S. Goldwater, and F. Keller. 2009. Evaluating
models of syntactic category acquisition without using
a gold standard. In Proceedings of CogSci.
J. Gao and M. Johnson. 2008. A comparison of Bayesian
estimators for unsupervised Hidden Markov Model
POS taggers. In Proceedings of EMNLP, pages 344?
352.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM
can find pretty good HMM POS-taggers (when given
a good start). In Proceedings of ACL, pages 746?754.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proceedings of ACL, pages 744?751.
J. Graca, K. Ganchev, B. Taskar, and F. Pereira. 2009.
Posterior vs parameter sparsity in latent variable mod-
els. In Proceedings of NIPS, pages 664?672.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In Pro-
ceedings of NIPS, pages 537?544.
A. Haghighi and D. Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL, pages 320?327.
W. P. Headden III, D. McClosky, and E. Charniak.
2008. Evaluating unsupervised part-of-speech tagging
for grammar induction. In Proceedings of COLING,
pages 329?336.
G.E. Hinton. 2002. Training products of experts by min-
imizing contrastive divergence. Neural Computation,
14(8):1771?1800.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of EMNLP-CoNLL,
pages 296?305.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proceedings of HLT/NAACL, pages
611?619.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a large annotated corpus of English:
The Penn Treebank. Comp. ling., 19(2):313?330.
M. Meila. 2007. Comparing clusterings?an informa-
tion based distance. Journal of Multivariate Analysis,
98(5):873?895.
A. Palmer, T. Moon, and J. Baldridge. 2009. Evaluat-
ing automation strategies in language documentation.
In Proceedings of the NAACL-HLT 2009 Workshop
on Active Learning for Natural Language Processing,
pages 36?44.
T. C. Pixabaj, M. A. Vicente Me?ndez, M. Vicente
Me?ndez, and O. A. Damia?n. 2007. Text Collections in
Four Mayan Languages. Archived in The Archive of
the Indigenous Languages of Latin America.
S. Ravi and K. Knight. 2009. Minimized models for
unsupervised part-of-speech tagging. In Proceedings
of ACL and AFNLP, pages 504?512.
R. Reichart, O. Abend, and A. Rappoport. 2010a. Type
level clustering evaluation: New measures and a POS
induction case study. In Proceedings of CoNLL, pages
77?87.
R. Reichart, R. Fattal, and A. Rappoport. 2010b. Im-
proved unsupervised POS induction using intrinsic
clustering quality and a Zipfian constraint. In Proceed-
ings of CoNLL, pages 57?66.
H. Schu?tze. 1993. Part-of-speech induction from scratch.
In Proceedings of ACL, pages 251?258.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Pro-
ceedings of ACL, pages 354?362.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP, pages 1041?
1050.
A.R. Teichert and H. Daume? III. 2010. Unsupervised
Part of Speech Tagging Without a Lexicon. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning 2010.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of
NAACL, pages 173?180.
J. Van Gael, A. Vlachos, and Z. Ghahramani. 2009. The
infinite HMM for unsupervised PoS tagging. In Pro-
ceedings of EMNLP, pages 678?687.
L. Xu, D. Wilkinson, F. Southey, and D. Schuurmans.
2006. Discriminative unsupervised learning of struc-
tured predictors. In Proceedings of ICML, pages
1057?1064.
Q. Zhao and M. Marcus. 2009. A simple unsuper-
vised learner for POS disambiguation rules given only
a minimal lexicon. In Proceedings of EMNLP, pages
688?697.
206
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 821?831, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Type-Supervised Hidden Markov Models for Part-of-Speech Tagging
with Incomplete Tag Dictionaries
Dan Garrette
Department of Computer Science
The University of Texas at Austin
dhg@cs.utexas.edu
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@utexas.edu
Abstract
Past work on learning part-of-speech taggers
from tag dictionaries and raw data has re-
ported good results, but the assumptions made
about those dictionaries are often unrealistic:
due to historical precedents, they assume ac-
cess to information about labels in the raw
and test sets. Here, we demonstrate ways to
learn hidden Markov model taggers from in-
complete tag dictionaries. Taking the MIN-
GREEDY algorithm (Ravi et al2010) as a
starting point, we improve it with several intu-
itive heuristics. We also define a simple HMM
emission initialization that takes advantage of
the tag dictionary and raw data to capture both
the openness of a given tag and its estimated
prevalence in the raw data. Altogether, our
augmentations produce improvements to per-
formance over the original MIN-GREEDY al-
gorithm for both English and Italian data.
1 Introduction
Learning accurate part-of-speech (POS) taggers
based on plentiful labeled training material is gener-
ally considered a solved problem. The best taggers
obtain accuracies of over 97% for English newswire
text in the Penn Treebank, which can be consid-
ered as an upper-bound that matches human perfor-
mance on the same task (Manning, 2011). How-
ever, as Manning notes, this story changes as soon
as one is working with different assumptions and
data, including having less training data, different
kinds of training data, other languages, and other
domains. Such POS tagging work has been plen-
tiful and includes efforts to induce POS tags without
labels (Christodoulopoulos et al2010); learn from
POS-tag dictionaries (Ravi et al2010), incom-
plete dictionaries (Hasan and Ng, 2009) and human-
constructed dictionaries (Goldberg et al2008);
bootstrap taggers for a language based on knowl-
edge about other languages (Das and Petrov, 2011),
and creating supervised taggers for new, challenging
domains such as Twitter (Gimpel et al2011).
Here, we focus on learning from tag dictionar-
ies. This is often characterized as unsupervised or
weakly supervised training. We adopt the terminol-
ogy type-supervised training to distinguish it from
unsupervised training from raw text and supervised
training from word tokens labeled with their parts-
of-speech. Work on type-supervision goes back to
(Merialdo, 1994), who introduced the still standard
procedure of using a bigram Hidden Markov Model
(HMM) trained via Expectation Maximization.
Early research appeared to show that learning
from types works nearly as well as learning from
tokens, with researchers in the 1990s obtaining ac-
curacies up to 96% on English (e.g. Kupiec (1992)).
However, the tag dictionaries in these cases were ob-
tained from labeled tokens. While replicating earlier
experiments, Banko and Moore (2004) discovered
that performance was highly dependent on clean-
ing tag dictionaries using statistics gleaned from the
tokens. This greatly simplifies the job of a type-
supervised HMM: it no longer must entertain entries
for uncommon word-tag pairs (or mistaken pairs
due to annotation errors), which otherwise stand on
equal footing with the common ones. When the
full, noisy tag dictionary was employed, Banko and
Moore found accuracies dropped from 96% to 77%.
Banko and Moore?s observations spurred a new
line of research that sought to improve performance
in the face of full, noisy dictionaries; see Ravi and
821
Knight (2009) for an overview. The highest accu-
racy achieved to date under these assumptions is
91.6% (Ravi et al2010). However, as is often
noted (including by the authors themselves), many
papers that work on learning taggers from tag dic-
tionaries make unrealistic assumptions about the tag
dictionaries they use as input (Toutanova and John-
son, 2008; Ravi and Knight, 2009; Hasan and Ng,
2009). For example, tag dictionaries are typically
constructed with every token-tag pair in the data, in-
cluding those that appear only in the test set. This
means that the evaluation of these taggers does not
measure how they perform on sentences that contain
unseen words or unseen word-tag pairs, a likely oc-
currence in real use of a trained tagger.
We show that it is possible to achieve good tag-
ging accuracy using a noisy and incomplete tag dic-
tionary that has no access to the tags of the raw and
test data and no access to the tag frequency infor-
mation of the labeled training data from which the
dictionary is drawn. We build on Ravi et al (2010)
model minimization approach, which reduces dic-
tionary noise by greedily approximating the mini-
mum set of tag bigrams needed to cover the raw data
and exploits that information as a constraint on the
initialization of the model before running EM. We
extend their method in four distinct ways.
1. Enable the algorithm to be used with incomplete
dictionaries by exploiting the type-based infor-
mation provided by the tag dictionary and raw
text to initialize EM, and by training a standard
supervised HMM on the output of EM.
2. Improve the greedy procedure to find a better
minimized set of tag-tag bigrams.
3. Modify the method to return only the set of bi-
grams required to tag sentences instead of keep-
ing all bigrams chosen by minimization.
4. Exploit the paths found during minimization as a
direct initialization for EM.
Together, these improvements make it possible to
use model minimization in a realistic context, and
obtain higher performance: on English, results go
from 63.5% for a vanilla HMM to 82.1% for an
HMM that uses strategies to deal with unknowns,
then to 85.0% with Ravi and Knight?s minimization
and finally to 88.5% with our enhancements.
2 Supervision for HMMs
Hidden Markov Models (HMMs) are well-known
generative probabilistic sequence models commonly
used for POS-tagging. The probability of a tag se-
quence given a word sequence is determined from
the product of emission and transition probabilities:
P (t|w) ?
N?
i=1
P (wi|ti) ? P (ti|ti?1)
HMMs can be trained directly from labeled data by
calculating maximum likelihood estimates or from
incomplete data using Expectation Maximization
(EM) (Dempster et al1977). We use both strate-
gies in this work: EM is used to estimate models
that can automatically label raw tokens, and then a
new HMM is estimated from that auto-labeled data.
2.1 Token-supervised training
We use a simple but effective smoothing regime to
account for unknown words and unseen tag-tag tran-
sitions. For emissions:
P (wi|ti) =
C(ti, wi) + ?(ti)Puni(wi)
C(ti) + ?(ti)
where Puni(wi) is the unigram probability of wi,
and ?(ti) is a tag specific amount of mass for
smoothing. We use one-count smoothing (Chen and
Goodman, 1996), where ?(ti) is based on the num-
ber of words that occur with ti once:
?(ti) = |wi : C(ti, wi) = 1|
Since open-class tags occur more frequently with
words that appear once, they will reserve more mass
for unknown words than closed-class tags will. The
transition distributions are smoothed in a similar
fashion:
P (ti|ti?1) =
C(ti?1, ti) + ?(ti?1)Puni(ti)
C(ti?1) + ?(ti?1)
?(ti?1) = |ti : C(ti?1, ti) = 1|
This simple scheme is quite effective: an HMM
trained on the Penn Treebank sections 0-18 and eval-
uated on sections 19-21 and smoothed in this way
obtains 96.5% accuracy. We do not use gold stan-
dard labels elsewhere for this paper, but do use this
model on the output of type-supervised HMMs.
822
2.2 Type-supervised training
We are primarily interested in learning taggers from
tag dictionaries combined with unlabeled text. As is
standard, we use EM to iteratively estimate the tran-
sition and emission probability parameters to maxi-
mize the likelihood of unlabeled data. It is known,
however, that EM has particular problems learning a
good HMM for POS tagging (Johnson, 2007; Ravi
and Knight, 2009). One reason is that EM gener-
ally tries to learn probability distributions that are
fairly uniform while POS tag frequencies are quite
skewed. For example, ?a? appears in the training
data with seven different tags, but 99.9% of ?a? to-
kens are determiners. Thus, the accuracy of anything
approaching a uniform distribution for ?a? tags will
suffer greatly. In the context of unsupervised POS
tagging models, modeling this distinction greatly
improves results (Moon et al2010). Here, we can
simply exploit the tag dictionary and raw data.
An initial set of parameters for the transitions
and emissions must be supplied as the input to EM.
Given just a tag dictionary, the simplest initializa-
tion is to set alag transitions to be uniform, rang-
ing over all tag continuations, while for emissions, a
uniform distribution over all words that occur with
the tag is assigned. This may be appropriate when a
complete tag dictionary is available, including com-
plete information for words that appear only in the
test data. This is because there will never be any un-
known words during model estimation or inference.
Likewise, there will never be a situation where the
tag dictionary rules out all possible tag transitions
between two adjacent tokens in training or testing.
As a result, no smoothing is needed in this scenario.
The problem with this is that estimating a model
based on type-supervision requires raw text, and if
we have an incomplete tag dictionary, some of the
words in that text will be missing from the tag dic-
tionary. In a Bayesian setting, priors provide mass
for such tokens; models are estimated using either
Gibbs sampling or variational inference (Johnson,
2007). However, we use vanilla EM here; as a con-
sequence, once a parameter is zero, it is always zero.
We thus need to ensure that mass is reserved for
words outside the tag dictionary at the start of EM.
(For transitions, uniform distributions are sufficient
since the set of tags is closed.)
2.3 Emission probability initialization
The simplest way to initialize the emission distribu-
tions is to assign a count of one to every entry in the
tag dictionary, and one count for unknowns. Then,
during each iteration of EM, the expectation step is
able to estimate new non-zero counts for all possible
emissions encountered in the raw corpus. This basic
strategy allows one to train an HMM with EM us-
ing only an incomplete tag dictionary and raw text.
However, this basic approach for emission proba-
bilities produces bad unknown-word probabilities.
Specifically, if for each tag we simply assume one
count for each entry in the tag dictionary and one
count for unknowns and then normalize, the proba-
bility of an unknown word having a specific tag is
inversely correlated with the number of word types
associated with the tag in the tag dictionary. In other
words, a tag that appears with a smaller number of
distinct words will be seen by the HMM as being a
better candidate tag for an unknown word. Unfor-
tunately this is the opposite behavior we want since
closed-class tags like determiner and preposition are
bad candidates for tagging novel words.
For type-supervised training, we can do much bet-
ter. Note that C(w, t) comes in two varieties: w
is either found in the tag dictionary (known word
types), or it is not (unknown word types). We refer
to the later as td-unknown: these are words that oc-
cur in the raw word sequence used for EM but which
do not occur in the tag dictionary. These are thus
different unknowns from words have not been ob-
served in the dictionary or in the raw set but which
may be encountered at test time. Computing the full
C(w, t) is necessary since we want P (w|t) to cover
known and td-unknown words. We must thus deter-
mine both Cknown(w, t) and Cunktd(w, t).
First, we focus on calculating Cknown(w, t). If a
word w appears C(w) times in the raw corpus, and
is seen with |TD(w)| tags in the tag dictionary, then
assume for each t in TD(w):
Cknown(w, t) = C(w) / |TD(w)|
andCknown(w, t) = 0 for all other t. In other words,
we split C(w), the count of w tokens in the corpus,
evenly among each of w?s possible tags. This pro-
vides us with an estimate of the true C(w, t) by ap-
proximating the portion of the counts of each word
823
type that may be associated with that tag. Note that
while this will give us zeros for any words that don?t
appear in the raw corpus, this is not a problem be-
cause EM training is based only on that corpus.
Second, we look at td-unknown word types: those
in the raw data that are not found in the tag dic-
tionary. Given the value P (unktd|t) for the like-
lihood of an unknown word given a tag t, we can
compute estimated counts Cunktd(w, t) for a td-
unknown word w using
Cunktd(w, t) = C(w) ? P (unktd|t)
where C(w), again, comes from the raw corpus.
This has the effect of spreading C(w), the count of
tokens of that unknown word w, across all of the
possible tags, with each tag receiving a proportion
of the total count as determined by P (unktd|t).
The challenge, then, is to compute P (unktd|t).
For this, we have two potential sources of knowl-
edge, the tag dictionary and the raw token sequence,
each telling us complementary information.
First, the tag dictionary tells us about the openness
of a tag?the likelihood that an unseen word will
have that label?based on our previously-discussed
intuition that we are more likely to see a new word
with a tag that is known to be associated with many
words already. Thus, we can estimate Ptd(unktd|t)
by simply normalizing the |TD(t)| values:
Ptd(unktd|t) =
|TD(t)|2
?
t??Tags |TD(t
?)|2
We exaggerate the differences between tags by
squaring the |TD(t)| terms to draw an even larger
distinction between open and closed class types.
Unfortunately, if we calculate an estimated word
count directly from this using Cunktd(w, t) =
C(w) ? Ptd(unktd|t), the Cunktd(w, t) values would
be taken without any regard to the overall like-
lihood of tag t. Since Cknown(NN) is very
high, Cunktd(NN) will seem very low by compar-
ison. Likewise, since Cknown(RB) is much lower,
Cunktd(RB) will seem very high by comparison.
P (unktd|t) must account for the overall likeli-
hood of t so that the Cunktd(w, t) values will be
scaled appropriately according to the overall likeli-
hood of t. For this, we use our second knowledge
source: the raw data. Based on the Cknown(w, t)
values as given above, the raw data tells us about the
overall expectation of a word having a particular tag.
From this, we can estimate the tag distribution for
known words: Cknown(t) =
?
w??V Cknown(w
?, t)
and then normalize to get Pknown(t).
Finally, we need to combine Ptd(unktd|t) and
Pknown(t) into a single P (unktd|t) that accounts
for both the openness of a tag and its overall preva-
lence. We would like this combination to use the
high Pknown(NN) to boost P (unktd|NN) and the
low Pknown(RB) to dampen P (unktd|RB). So, we
compute and normalize:
P (unktd|t) ?
|TD(t)|2
?
t??Tags |TD(t
?)|2
? Pknown(t)
2.4 Auto-supervised post-EM smoothing
The initialization accounting for td-unknown words
given above allows EM to be run on the raw token
sequence, but it provides no probability for words
that are truly unseen (in either the tag dictionary or
the raw data). Consequently, any novel words in the
test set will have zero emission probabilities, leading
to extremely low unknown-word accuracies.
To overcome this problem, we perform a sim-
ple post-processing step after EM, which we refer
to as auto-supervised training. We take the HMM
trained by EM and use it to label the raw corpus.
This gives us an automatically-labeled corpus that
can be used for standard supervised training (with-
out EM) to produce a new HMM. The effect of this
post-processing step is to smooth the counts learned
from EM onto any new words encountered during
testing. This procedure significantly improves the
ability of the HMM to label unknown words.
As a final note, it would of course be possible to
use other models at this stage, such as a Conditional
Random Field (Lafferty et al2001).
3 Enhancing MIN-GREEDY
As was discussed above, one of the major prob-
lems for type-supervised POS-tagger training with
EM is a tag dictionary with low-frequency entries
such as the word ?a? being associated with the for-
eign word tag when nearly all of its instances are
as a determiner. To avoid the need for manually
pruning the tag dictionary, Ravi and Knight (2009)
824
?b? The boy sees a dog ?\b?
?b?
2
%%
DT
1
&&
DT
1
%%
NN
&&
NN
3

V B
&&
BB
FW
?\b?
Figure 1: MIN-GREEDY graph showing a state in the
first phase. Numbered, solid arrows: order of chosen
bigrams; dotted: potential choices.
?b? The boy sees a dog ?\b?
?b?
%%
DT
&&
DT
%%
NN
&&
NN

V B
&&
BB
FW
BB
?\b?
Figure 2: Start of the second MIN-GREEDY phase.
proposed that low-probability tags might be auto-
matically filtered from the tag dictionary through a
model minimization procedure applied to the raw
text and constrained by the full tag dictionary. Ravi
et al2010) develop a faster approach for model
minimization using a greedy algorithm that they call
MIN-GREEDY. It is this algorithm that we extend.
3.1 The original MIN-GREEDY algorithm
The MIN-GREEDY algorithm starts by initializing a
graph with a vertex for each possible tag of each to-
ken in the raw data. The set of possible tags for each
token is the set of tags associated with that word
in the tag dictionary.Special sentence start and sen-
tence end vertices are added to the graph for each
sentence to mark its beginning and end. Unlike Ravi
et al2010), we allow for an incomplete tag dic-
tionary, meaning that our scenario has the additional
complication that the tag set for some raw-corpus
?b? The boy sees a dog ?\b?
?b?
%%
DT
&&
DT
%%
NN
&&
NN

V B
&&
BB
FW
?\b?
Figure 3: Potential MIN-GREEDY conclusion.
words will not be known. For these words, the full
set of tags is used. Note that this increases the ambi-
guity and overall number of edges in the graph.
The MIN-GREEDY algorithm works in three
phases: Greedy Set Cover, Greedy Path Comple-
tion, and Iterative Model-Fitting. In the first two
phases, the algorithm chooses tag bigrams that form
the edges of the graph. The goal of these phases is to
select a set of edges that is sufficient to allow a path
through every sentence in the raw corpus. The al-
gorithm greedily selects these edges in an attempt to
quickly approximate the minimal set of tag bigrams
needed to accomplish this goal. In the final phase,
the algorithm runs several iterations of EM in order
to fit the bigram set to the raw data.
In the first phase, Greedy Set Cover, the algorithm
selects tag bigrams in an effort to cover all of the
word tokens. A word token is considered covered
if there is at least one tag bigram edge connected
to at least one of its vertices. At each iteration, the
algorithm examines the entire graph, across all sen-
tences, to find the tag bigram that, if added, would
maximize the number of newly covered words.
Consider the graph in Figure 1. Assume, for the
example, that this sentence comprises the entire raw
corpus. At the start of the first phase, no tag bigrams
are selected. On the first iteration, the algorithm
chooses the tag bigram DT?NN because this tag
bigram describes two edges for a total of four words
newly covered: The, boy, a, and dog. On the second
iteration, there are only three word tokens left un-
covered: the start symbol, sees, and the end symbol.
At this point, as the figure shows, there are five tag
bigrams that would each result in covering one addi-
825
tional token. Since there are no tag bigrams whose
choosing would result in covering more than one ad-
ditional token, the algorithm randomly chooses one
of these five. The algorithm iterates like this until all
words are covered, as in, for example, Figure 2.
The second phase of the MIN-GREEDY algorithm,
Greedy Path Completion, seeks to fill holes in the
tag paths found in the graph. A hole is a poten-
tial edge that, if added, would connect two existing
edges. At each iteration, the algorithm finds the tag
bigram that, if selected, would maximize the number
of holes that would be filled across all raw sentences.
The example graph in Figure 2 shows a potential
start of the second phase. At this point, there are
three tag bigrams that each fill one hole if selected,
and the algorithm randomly selects one. Iteration
continues until there is a complete tag path through
each sentence in the raw corpus. One potential reso-
lution for the example is given in Figure 3.
Once a set of tag bigrams has been generated that
allows for a complete tag path through every sen-
tence of the raw corpus, MIN-GREEDY begins its
final phase: Iterative Model-Fitting. In this phase,
the algorithm trains a succession of type-supervised
HMM models. Each iteration trains an HMM and
then uses it to tag the raw corpus, the result of which
is used to prepare inputs for the next iteration.
Iterative Model-Fitting begins with the minimized
set of bigrams returned from the second phase of
MIN-GREEDY. This set is used as a hard constraint
on the allowable tag bigrams during type-supervised
HMM training. While EM is running, the only tag
transitions that are counted are those that fall into the
minimized tag bigram set; all other transition counts
are ignored. Once an HMM has been trained, it is
immediately used to tag the raw corpus, producing a
set of auto-labeled sentences. For the second itera-
tion of the phase, we extract a constrained tag dictio-
nary from the auto-labeled corpus by simply taking
every word/tag pair appearing in the data. This new
tag dictionary is a subset of the original, full, tag
dictionary, and hopefully has fewer low-frequency
entries that would cause problems for EM.
We use this constrained tag dictionary to again
perform type-supervised HMM training, but without
any constraints on the allowable tag bigrams. This
produces our third HMM. Using this HMM, we can,
again, tag the raw corpus, producing another set of
auto-labeled sentences. We can then extract the set
of tag bigrams appearing in this data to produce a
new set of tag transition constraints, similar to what
was returned by the second phase. With this set of
tag transition constraints, and the full tag dictionary,
we can perform another round of type-supervised
HMM training, and repeat the entire process.
The third MIN-GREEDY phase continues iterating,
alternating between training an HMM using a con-
strained set of tag transitions and training one using
a constrained tag dictionary. The size of the set of
constrained tag bigrams produced is tracked on each
iteration, and the algorithm is considered to have
converged when this value changes by less than five
percent. The final result of the MIN-GREEDY algo-
rithm is a trained HMM.
The evaluation of the MIN-GREEDY algorithm, as
described in Ravi et al2010), was performed only
for scenarios with a complete tag dictionary (includ-
ing all raw and test word types). As such, no tech-
niques were described for handling unknown words.
Because we are interested in the more realistic sce-
nario of an incomplete tag dictionary, we augment
the original MIN-GREEDY setup with the smoothing
techniques described above.
3.2 Improving tag bigram selection
One of the major problems with the MIN-GREEDY
algorithm is that its heuristics for choosing the next
tag bigram frequently result in many-way ties. In the
first two phases of MIN-GREEDY, the greedy pro-
cedure looks for the tag bigram that will have the
most positive impact. In the Greedy Set Cover phase
this means choosing the tag bigram that would cover
the most new tokens, and in the Greedy Path Com-
pletion phase this means choosing the tag bigram
that would fill the most holes. However, it is fre-
quently the case that there are many distinct tag bi-
grams that would cover the most new tokens or fill
the most holes, leaving the MIN-GREEDY algorithm
with no choice but to randomly select from these
options. Since there are frequently cases of having
many dozens of options, it is clear that some of those
choices must be better than others, even though MIN-
GREEDY does not make a distinction and considers
them all to be equally good choices.
Consider the example in Figure 1 representing a
possible state of the minimization graph. To have
826
reached this stage, tag bigram DT?NN would have
been chosen since it covered the highest number of
tokens: four. Additionally, ?b??DT and NN??\b?
could have been chosen as the second and third tag
bigrams since they tied for the most new tokens cov-
ered: one. For the state shown in this figure, there
is only one uncovered token, sees, but three tag bi-
grams that cover it. Since each of these tag bigrams
covers exactly one new word, they are all considered
by MIN-GREEDY to be equally good choices as the
next tag bigram for inclusion, and the algorithm will
choose one at random. However, it should be clear
that the VB?FW tag bigram is wrong while the
other two would lead to a correct answer. As such,
we would like for the algorithm to avoid choosing
VB?FW, and to pick one of the others.
In order to push the algorithm into choosing the
right tag bigrams in these otherwise ambiguous sit-
uations, we have added an additional criterion to the
bigram-choosing heuristic: after narrowing down
the set of tag bigrams to those that cover the most
new tokens, we further narrow the choice of bigrams
by minimizing the number of new word-type/tag
pairs that would be added to the result. Consider
our example. If we choose the tag bigram NN?VB
or VB?DT, then exactly one new word-type/tag
pair would be added to our result: sees/VB (since
boy/NN and a/DT would already have been added
by the incorporation of previous selected tag bi-
grams). By contrast if we choose the tag bigram
VB?FW then two new word-type/tag pairs would
be added: sees/VB and a/FW.
Minimizing the number of new word/tag pairs
added by the algorithm has two main advantages.
First, it keeps the selected bigrams focused on the
same vertices, which results in fewer holes that the
Greedy Path Completion phase must deal with. Sec-
ondly, it keeps the selected bigrams focused on more
common tags for each word type, such as a/DT, and
keeps it away from rare tags, such as a/FW.
3.3 Only tag bigrams on minimization paths
As was described above, the output of MIN-
GREEDY?s second stage is a minimized set of tag
bigrams which is used as a constraint on the first
iteration of the third stage, Iterative Model-Fitting.
However, in order to determine when to stop adding
new bigrams during the first two phases, the MIN-
GREEDY algorithm must try to find complete tag
paths through each sentence in the raw corpus, stop-
ping once a tag path has been found for each one.
While the algorithm is trying to select only the tag
bigrams that are necessary for a complete tagging, it
happens frequently that bigrams are selected that are
not actually used on any tag path.
Consider the example shown in Figure 3. The
graph has a complete path through the sentence, but
also contains an extraneous edge, VB?FW, that is
not used on the path. Assuming that this tag bigram
is not used on the tag path of any other sentence, it
can safely be removed from the resultant set to pro-
duce a smaller set of tag bigrams, getting us even
closer to the minimized set that we desire.
To find the set of tag bigrams excluding these ex-
traneous edges, we modify the MIN-GREEDY algo-
rithm. During the first and second phases of the al-
gorithm, we check all raw data sentences for a com-
pleted path after each tag bigram is selected. If a
completed path is found for a sentence, we store that
path immediately. Once a path is found for every
sentence, we extract the set of bigrams used on these
paths, and pass that set, instead of the full set of se-
lected bigrams, to the third phase of the algorithm.
Note that it is important that we store the com-
pleted paths as soon as they are completed. Since
sentences are completed at different stages, and
more tag bigrams are selected after some of these
sentences are complete, it is inevitable that some
sentences will end up with multiple complete tag
paths by the end of the second phase. However, we
seek only the first such path. Tag bigrams are se-
lected in order of their impact, so bigrams selected
earlier are better and should be preferred. Consid-
ering again the example in Figure 3, based on the
frequency of the tags, it is likely that, given the
presence of other sentences in the raw corpus, the
tag path including bigrams VB?DT and DT?NN
would be found before the one including VB?FW
and FW?NN. Since they are more frequent bi-
grams, we would want to keep the first path even
if the second is completed at a later time.
The result of this improvement is a smaller,
cleaner minimized tag bigram set to be delivered to
the third phase of MIN-GREEDY.
827
Scenario Total Known Unk.
0. Random baseline (choose tag randomly from tag dictionary) 63.53 65.49 2.38
1. HMM baseline (simple EM with tag dictionary and raw text) 69.20 71.42 0.27
2. HMM baseline + auto-supervised training 82.33 83.67 40.46
3. HMM baseline + auto-supervised training + emission initialization 82.05 83.27 44.31
4. MIN-GREEDY (Ravi et al2010) with add-one smoothing 74.79 77.17 0.45
5. MIN-GREEDY with add-one smoothing + auto-supervised 86.10 87.59 39.74
6. MIN-GREEDY with add-one smoothing + auto-supervised + emission init 85.02 86.33 44.28
7. 6 + enhanced tag bigram choice heuristic 86.71 88.08 43.93
8. 6 + restrict tag bigrams to tag paths of minimization-tagged output 87.01 88.40 43.74
9. 6 + HMM initialization from minimization-tagged output 88.52 89.92 44.80
10. 6 + 7 + 8 + 9 88.51 89.92 44.80
Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types
are those appearing in the tag dictionary.
3.4 EM initialization with minimization output
As a final improvement to MIN-GREEDY, we took
the set of completed tag paths returned from the sec-
ond phase of the algorithm, as described in the pre-
vious section, and used them as labeled data to ini-
tialize an HMM for EM training.
Since we modified MIN-GREEDY to produce a set
of completed tag paths for sentences, we can take
this to be a complete set of labels for the raw cor-
pus. Furthermore, since we were careful about stor-
ing paths as soon as they become completed by the
minimization process, and the tag bigrams are cho-
sen in order of frequency, there will be more high-
frequency bigrams than low-frequency. As a result,
this labeling will contain good tag transitions and
token labelings. As such, the labeled data produced
by the second phase provides useful information be-
yond a simple set of sufficient bigrams: it contains
legitimate frequency information that can be used
to initialize the HMM. We, therefore, initialize an
HMM directly from this data to start EM.
4 Evaluation1
English data. We evaluate on the Penn Treebank
(Marcus et al1993). In all cases we use the first
47,996 tokens of section 16 as our raw data, sections
19?21 as our development set, and perform the final
evaluation on sections 22?24.
1Source code, scripts, and data to reproduce the results pre-
sented here can be found at github.com/dhgarrette/
type-supervised-tagging-2012emnlp
We evaluate two differently sized tag dictionaries.
The first is extracted directly from sections 00?15
(751,059 tokens) and the second from sections 00?
07 (379,908 tokens). The former contains 39,087
word types, 45,331 word/tag entries, a per-type am-
biguity of 1.16 and yields a per-token ambiguity of
2.21 on the raw corpus (treating unknown words
as having all 45 possible tags). The latter contains
26,652 word types, 30,662 word/tag entries, a per-
type ambiguity of 1.15 and yields a per-token ambi-
guity of 2.03 on the raw corpus. In both cases, every
word/tag pair found in the relevant sections was used
in the tag dictionary: no pruning was performed.
Italian data. As a second evaluation, we use the
TUT corpus (Bosco et al2000). To verify that our
approach is language-independent without the need
for specific tuning, we executed our tests on the Ital-
ian data without any trial runs, parameter modifica-
tions, or other changes. We divided the TUT data,
taking the first half of each of the five sections as in-
put to the tag dictionary, the next quarter as raw data,
and the last quarter as test data. All together, the tag
dictionary was constructed from 41,000 tokens con-
sisting of 7,814 word types, 8,370 word/tag pairs,
per-type ambiguity of 1.07 and a per-token ambigu-
ity of 1.41 on the raw data. The raw data consisted of
18,574 tokens and the test contained 18,763 tokens.
Results We ran eleven experiments for each data
set with results shown in Tables 1 and 2. All scores
are reported as the percentage of tokens for which
the correct tag was assigned. Accuracy is shown as
828
PTB (00-07) TUT
Scenario Total Known Unk. Total Known Unk.
0. Random 64.98 68.04 2.81 62.81 76.10 1.58
1. HMM basic 69.32 72.70 0.56 60.70 73.77 0.51
2. HMM + auto-super 81.50 83.67 37.46 70.03 80.64 21.12
3. HMM + auto-super + init 81.71 83.62 42.89 70.89 80.91 24.74
4. MIN-GREEDY + add-1 68.86 72.20 0.92 53.96 65.49 0.84
5. MIN-GREEDY + add-1 + auto-super 80.78 82.88 38.02 70.85 82.41 17.60
6. MIN-GREEDY + add-1 + auto-super + init 80.92 82.80 42.64 71.52 81.56 25.28
7. 6 + enhanced bigram choice heuristic 86.69 88.83 43.07 71.48 81.57 24.98
8. 6 + restrict tag bigrams to tag paths 80.86 82.73 42.84 72.86 83.45 24.08
9. 6 + HMM init from minimization output 87.61 89.74 44.18 72.00 82.28 24.65
10. 6 + 7 + 8 + 9 87.95 90.12 43.74 71.99 82.50 23.57
Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word
types are those appearing in the tag dictionary. Scenario numbers correspond to Table 1.
the Total (all word types), Known (word types found
in the tag dictionary), and Unknown (word types not
found in the tag dictionary).
Experiments 1?3 evaluate our smoothing tech-
niques applied directly to the task of type-supervised
HMM training with EM, without MIN-GREEDY.
The basic HMM consistently beats the baseline ran-
dom tagger, the auto-supervision technique makes
an enormous improvement for both known and un-
known words, and the the emission initialization
yields a sizable improvement for unknown words.
Experiments 4?6 evaluated our reimplementation
of MIN-GREEDY. We start with the most basic level
of smoothing needed to work in a type-supervised
scenario. For the smaller PTB tag dictionary and
the TUT data, MIN-GREEDY actually has lower per-
formance than the HMM alone. This indicates that
if the tag dictionary has a low degree of ambigu-
ity, then MIN-GREEDY can make the situation worse.
However, with our smoothing techniques, we regain
similar improvements as with the HMM.
Finally we performed experiments evaluat-
ing combinations of our improvements to MIN-
GREEDY. Scenarios 7?9 show each improvement
taken in turn. Scenario 10 shows the results for us-
ing all three improvements. For the English data, the
best results are found when all the improvements are
used. When taken individually, the bigram choice
heuristic and HMM initialization from minimization
output each consistently outperform the improved-
MIN-GREEDY baseline on English. However, re-
stricting the tag bigrams to that in the minimization-
tagged output causes problems in the smaller PTB
scenario, presumably falling to a local maximum
like MIN-GREEDY that the other improvements are
able to help the algorithm avoid.
Though the accuracy improvements are less than
for English, the Italian results show that our MIN-
GREEDY enhancements make an appreciable differ-
ence for a language and dataset for which the ap-
proaches considered were run sight unseen.
Error analysis One of the primary goals of model
minimization is to automatically eliminate low-
probability entries from the tag dictionary that might
confuse the EM algorithm (Ravi et al2010). In or-
der to see how well our techniques are able to iden-
tify and eliminate these unlikely word/tag pairs, we
analyzed the tagging errors from each experiment.
In doing so, we discovered that the two of the most
problematic words for the EM algorithm are ?a? and
?in?. We ran further experiments explore what was
happening with those words. The results, using PTB
sections 00?07 are shown in Table 3.
In PTB sections 00-07 the word ?a? appears 7630
times and with 7 different tags. This includes 7621
occurrences with tag DT, 3 with tag SYM (symbol),
and 1 time with LS (list item marker). As such, we
would want the HMM to lean heavily toward tag DT
when tagging the token ?a?. Unfortunately, the rare
tags confuse the EM procedure and end up with dis-
829
model tokens tagged by scenario
tok output 3 6 7 8 9 10
a DT 32 4 4 4 2424 2425
LS 1531 0 0 0 0 0
SYM 731 2356 2305 2356 0 0
in IN 12 15 2024 4 2042 2047
FW 1922 1910 0 0 0 0
RP 20 27 0 2037 0 0
Table 3: Number of times, for the words ?a? and
?in?, the tagger trained by the particular scenario se-
lected the given tag. Experiments used PTB sections
00-07 for the initial tag dictionary. Scenario num-
bers correspond to Table 1.
proportionately high probabilities. Our experiment
training an HMM without minimization (scenario 3)
resulted in 1531 ?a? tokens being tagged LS, 731 as
SYM, and only 32 tagged as DT.
The situation is similar with the word ?in?, which
appears 6155 times with 5 different tags in the 8
sections. Of these, 6073 occurrences are tagged
IN (preposition), 63 are RP (particle), and 1 is FW
(foreign word). Again, EM without minimization
is confused by the rare tokens, assigning FW 1922
times and IN 12 times.
The minimization procedure attempts to over-
come this problem by removing unlikely tags from
the tag dictionary automatically. As is show in Table
3, MIN-GREEDY without our enhancements is able
to reject the problematic LS as a tag for ?a?, but
unable to do so for SYM, resulting in 2356 tokens
tagged SYM and only 4 tagged DT. Similarly, MIN-
GREEDY is unable to reject FW as a tag for ?in?.
Our enhancements to MIN-GREEDY improve the
situation. More careful choosing of bigrams during
minimization results in the avoidance of LS and FW
(but not SYM) for ?a? as well as FW and RP for
?in?. Restricting the tag bigrams output from MIN-
GREEDY to just those on tag paths avoids LS and FW
for ?a? and FW for ?in?. Finally, using the tagged
sentences from MIN-GREEDY as noisy supervision
for EM initialization eliminates all rare tags, as does
the use of all three enhancements together.
5 Conclusion
Our results show it is possible to create accurate
POS-taggers using type-supervision with incom-
plete tag dictionaries by extending the MIN-GREEDY
algorithm of Ravi et al2010). The most useful
change we made to the MIN-GREEDY procedure was
the implementation of a better heuristic for picking
tag bigrams. An intuitive and straightforward emis-
sion initialization provides the necessary basis to run
EM on a given raw token sequence. Using EM out-
put on this raw sequence as auto-labeled material
to a supervised HMM then proves highly effective
for generalization to new texts containing previously
unseen word types.
Vaswani et al010) explore the use of minimum
description length principles in a Bayesian model as
a way of capturing model minimization, inspired by
the MIN-GREEDY algorithm. The advantage there is
that only a single objective function needs to be opti-
mized, rather than having initialization followed by
an iterative back and forth with pruning of tag-tag
pairs. Our own next steps are to move in a similar
direction to explore the possibilities for encoding the
intuitions we developed for initialization and mini-
mization as a single generative model.
Goldberg et al2008) note that fixing noisy dic-
tionaries by hand is actually quite feasible, and sug-
gest that effort should focus on exploiting human
knowledge rather than just algorithmic improve-
ments. We agree; however, our ultimate motivation
is to use this work to tackle bootstrapping from very
small tag dictionaries or dictionaries obtained from
linguists or resources other than a corpus, and for
tag sets that are more ambiguous (e.g., supertagging
for CCGbank (Hockenmaier and Steedman, 2007)).
Such efforts require automatic expansion of tag dic-
tionaries, which then need be constrained based on
available raw token sequences using methods such
as those explored here. In this respect, the some-
what idiosyncratic noise in the corpus-derived dic-
tionaries used here make a good test.
Acknowledgements
We thank Yoav Goldberg, Sujith Ravi, and the re-
viewers for their feedback. This work was supported
by the U.S. Department of Defense through the U.S.
Army Research Office (grant number W911NF-10-
1-0533) and via a National Defense Science and En-
gineering Graduate Fellowship for the first author.
830
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COLING,
pages 556?561, Geneva, Switzerland.
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, ,
and Leonardo Lesmo. 2000. Building a treebank for
Italian: a data-driven annotation schema. In Proceed-
ings of LREC.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of ACL, pages 310?318, Santa
Cruz, California, USA.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
pos induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT, pages 600?609,
Portland, Oregon, USA.
Arthur P. Dempster, Nan M. Laird, and Donald. B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 39:1?22.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of ACL-HLT, pages 42?47, Portland, Oregon,
USA.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings ACL, pages 746?
754.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
resource-scarce languages. In Proceedings of EACL,
pages 363?371, Athens, Greece.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
A corpus of ccg derivations and dependency structures
extracted from the penn treebank. Computational Lin-
guistics, 33(3):355?396.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings EMNLP-CoNLL, pages
296?305.
Julian Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden markov model. Computer Speech & Lan-
guage, 6(3):225?242.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of ICML, pages 282?289. Morgan Kauf-
mann.
Christopher D. Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics? In
Alexander Gelbukh, editor, Proceedings of CICLing,
volume 6608 of Lecture Notes in Computer Science,
pages 171?189.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Taesun Moon, Katrin Erk, and Jason Baldridge. 2010.
Crouching dirichlet, hidden markov model: Unsu-
pervised POS tagging with context local tag genera-
tion. In Proceedings of EMNLP, pages 196?206, Cam-
bridge, MA.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING,
pages 940?948.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In Proceedings of NIPS.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an mdl-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214, Uppsala, Sweden.
831
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1500?1510, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Supervised Text-based Geolocation
Using Language Models on an Adaptive Grid
Stephen Roller? Michael Speriosu ? Sarat Rallapalli ?
Benjamin Wing ? Jason Baldridge ?
?Department of Computer Science, University of Texas at Austin
?Department of Linguistics, University of Texas at Austin
{roller, sarat}@cs.utexas.edu, {speriosu, jbaldrid}@utexas.edu, ben@benwing.com
Abstract
The geographical properties of words have re-
cently begun to be exploited for geolocating
documents based solely on their text, often in
the context of social media and online content.
One common approach for geolocating texts is
rooted in information retrieval. Given training
documents labeled with latitude/longitude co-
ordinates, a grid is overlaid on the Earth and
pseudo-documents constructed by concatenat-
ing the documents within a given grid cell;
then a location for a test document is chosen
based on the most similar pseudo-document.
Uniform grids are normally used, but they are
sensitive to the dispersion of documents over
the earth. We define an alternative grid con-
struction using k-d trees that more robustly
adapts to data, especially with larger training
sets. We also provide a better way of choosing
the locations for pseudo-documents. We eval-
uate these strategies on existing Wikipedia and
Twitter corpora, as well as a new, larger Twit-
ter corpus. The adaptive grid achieves com-
petitive results with a uniform grid on small
training sets and outperforms it on the large
Twitter corpus. The two grid constructions
can also be combined to produce consistently
strong results across all training sets.
1 Introduction
The growth of the Internet in recent years has
provided unparalleled access to informational re-
sources. It is often desirable to extract summary
metadata from such resources, such as the date of
writing or the location of the author ? yet only a
small portion of available documents are explicitly
annotated in this fashion. With sufficient training
data, however, it is often possible to infer this infor-
mation directly from a document?s text. For exam-
ple, clues to the geographic location of a document
may come from a variety of word features, e.g. to-
ponyms (Toronto), geographic features (mountain),
culturally local features (hockey), and stylistic or di-
alectical differences (cool vs. kewl vs. kool).
This article focuses on text-based document ge-
olocation, the prediction of the latitude and lon-
gitude of a document. Among the uses for this
are region-based search engines; tracing the sources
of historical documents; location attribution while
summarizing large documents; tailoring of ads while
browsing; phishing detection when a user account is
accessed from an unexpected location; and ?activist
mapping? (Cobarrubias, 2009), as in the Ushahidi
project.1 Geolocation has also been used as a fea-
ture in automatic news story identification systems
(Sankaranarayanan et al 2009).
One of the first works on document geolocation is
Ding et al(2000), who attempt to automatically de-
termine the geographic scope of web pages. They
focus on named locations, e.g. cities and states,
found in gazetteers. Locations are predicted based
on toponym detection and heuristic resolution al-
gorithms. A related, recent effort is Cheng et al
(2010), who geolocate Twitter users by resolving
their profile locations against a gazetteer of U.S.
cities and training a classifier to identify geographi-
cally local words.
An alternative to using a discrete set of locations
from a gazetteer is to use information retrieval (IR)
techniques on a set of geolocated training docu-
ments. A new test document is compared with each
1http://ushahidi.com/
1500
training document and a location chosen based on
the location(s) of the most similar training docu-
ment(s). For image geolocation, Chen and Grauman
(2011) perform mean-shift clustering over training
images to discretize locations, then estimate a test
image?s location with weighted voting from the k
most similar documents. For text, both Serdyukov
et al(2009) and Wing and Baldridge (2011) use a
similar approach, but compute document similarity
based on language models rather than image fea-
tures. Additionally, they group documents via a uni-
form geodesic grid rather than a clustered set of lo-
cations. This reduces the number of similarity com-
putations and removes the need to perform location
clustering altogether, but introduces a new param-
eter controlling the granularity of the grid. Kinsella
et al(2011) predict the locations of tweets and users
by comparing text in tweets to language models as-
sociated with zip codes and broader geopolitical en-
closures. Sadilek et al(2012) discretize by simply
clustering data points within a small distance thresh-
old, but only perform geolocation within fixed city
limits.
While the above approaches discretize the contin-
uous surface of the earth, Eisenstein et al(2010)
predict locations based on Gaussian distributions
over the earth?s surface as part of a hierarchical
Bayesian model. This model has many advantages
(e.g. the ability to compute a complete probability
distribution over locations), but we suspect it will be
difficult to scale up to the large document collections
needed for high accuracy.
We build on the IR approach with grids while ad-
dressing some of the shortcomings of a uniform grid.
Uniform grids are problematic in that they ignore the
geographic dispersion of documents and forgo the
possibility of greater-granularity geographic resolu-
tion in document-rich areas. Instead, we construct
a grid using a k-d tree, which adapts to the size of
the training set and the geographic dispersion of the
documents it contains. This can better benefit from
more data, since it enables the training set to support
more pseudo-documents when there is sufficient ev-
idence to do so, while still ensuring that all pseudo-
documents contain comparable amounts of data. It
also has the desirable property of generally requiring
fewer active cells than a uniform grid, drastically re-
ducing the computation time required to label a test
document.
We show that consistently strong results, robust
across both Wikipedia and Twitter datasets, are ob-
tained from the union of the pseudo-documents from
a uniform and adaptive grid. In addition, a sim-
ple difference in the choice of location for a given
grid cell ? the centroid of the training documents
in the cell, rather than the cell midpoint ? results
in across-the-board improvements. We also con-
struct and evaluate on a much larger dataset of ge-
olocated tweets than has been used in previous pa-
pers, demonstrating the scalability and robustness of
our methods and confirming the ability of the adap-
tive grid to more effectively use larger datasets.
2 Data
We work with three datasets: a corpus of geotagged
Wikipedia articles and two corpora of geotagged
tweets.
GEOWIKI is a collection of 1,019,490 geotagged
English articles from Wikipedia. The dump from
Wikimedia requires significant processing to obtain
article text and location, so we rely on the prepro-
cessed data used by Wing and Baldridge (2011).
GEOTEXT is a small dataset consisting of
377,616 messages from 9,475 users tweeting across
48 American states, compiled by Eisenstein et al
(2010). A document in this dataset is the concate-
nation of all tweets by a single user, with a location
derived from the earliest tweet with specific, GPS-
assigned latitude/longitude coordinates.
UTGEO2011 is a new dataset designed to ad-
dress the sparsity problems resulting from the size
of the previous dataset. It is based on 390 mil-
lion tweets collected across the entire globe be-
tween September 4th and November 29th, 2011, us-
ing the publicly available Twitter Spritzer feed and
global search API. Not all collected tweets were
geotagged. To be comparable to GEOTEXT, we
discarded tweets outside of North America (out-
side of the bounding box with latitude/longitude
corners at (25,?126) and (49,?60)). Following
Eisenstein et al(2010), we consider all tweets
of a user concatenated as a single document, and
use the earliest collected GPS-assigned location as
the gold location. Users without a gold location
were discarded. To remove many spammers and
1501
robots, we only kept users following 5 to 1000
people, followed by at least 5 users, and author-
ing no more than 1000 tweets in the three month
period. The resulting dataset contains 38 million
tweets from 449,694 users, or roughly 85 tweets
per user on average. We randomly selected 10,000
users each for development and held-out test eval-
uation. The remaining 429,694 users serve as a
training set termed UTGEO2011-LARGE. We also
randomly selected a 10,000 user training subset
(UTGEO2011-SMALL) to facilitate comparisons
with GEOTEXT and allow us to investigate the rel-
ative improvements for different models with more
training data.
Our code and the UTGEO2011 data set are both
available for download.2
3 Model
Assume we have a collection d of documents and
their associated location labels l. These docu-
ments may be actual texts, or they can be pseudo-
documents comprised of a number of texts grouped
via some algorithm (such as the grids discussed in
the next section).
For a test document di, its similarity to each la-
beled document is computed, and the location of the
most similar document assigned to di. Given an ab-
stract function sim that can be instantiated with an
appropriate similarity function (e.g. cosine distance
or Kullback-Leibler divergence),
loc(di) = loc(argmax
dj?d
sim(di, dj)).
This is a winner-takes-all strategy, which we follow
in this paper. In related work on image geoloca-
tion, Hays and Efros (2008) use the same general
framework, but compute the location based on the
k-nearest neighbors (kNN) rather than the top one.
They compute a distribution from the 120 nearest
neighbors using mean shift clustering (Comaniciu
and Meer, 2002) and choose the cluster with the
most members. This produced slightly better re-
sults than choosing only the closest image. In future
work, we will explore the kNN approach to see if it
is more effective for text geolocation.
2https://github.com/utcompling/
textgrounder/wiki/RollerEtAl_EMNLP2012
Following previous work in document geoloca-
tion, particularly Serdyukov et al(2009) (hence-
forth SMvZ) and Wing and Baldridge (2011)
(henceforth W&B), we geolocate texts using a lan-
guage modeling approach to information retrieval
(Ponte and Croft, 1998; Zhai and Lafferty, 2001).
For each document di, we construct a unigram prob-
ability distribution ?di over the vocabulary.
We smooth documents using the pseudo-Good-
Turing method of W&B, a nonparametric discount-
ing model that backs off from the unsmoothed distri-
bution ??di of the document to the unsmoothed distri-
bution ??D of all documents. A general discounting
model is as follows:
P (w|?di) =
{
(1? ?di)P (w|??di), if P (w|??di) > 0
?di
P (w|??D)
Udi
, otherwise,
where Udi = 1 ?
?
w?di
P (w|??D) is a normaliza-
tion factor that is precomputed when the distribution
for di is constructed. The discount factor ?di indi-
cates how much probability mass to reserve for un-
seen words. For pseudo-Good-Turing, it is
?di =
|w ? di s.t. count(w ? di) = 1|
|w ? di|
,
i.e. the fraction of words seen once in di.
We experimented with other smoothing methods,
including Jelinek-Mercer and Dirichlet smoothing.
A disadvantage of these latter two methods is that
they have an additional tuning parameter to which
their performance is highly sensitive, and even with
optimal parameter settings neither consistently out-
performed pseudo-Good-Turing. We also found no
consistent improvement from using interpolation in
place of backoff.
We also follow W&B in using Kullback-Leibler
(KL) divergence as the similarity metric, since it out-
performed both naive Bayes classification probabil-
ity and cosine similarity:
KL(?di ||?dj ) =
?
k
?di(k) log
?di(k)
?dj (k)
.
The motivation for computing similarity with KL is
that it is a measure of how well each document in
the labeled set explains the word distribution found
in the test document.
1502
4 Collapsing Documents with an Adaptive
Grid
In the previous section, we used the term ?docu-
ment? loosely when speaking of training documents.
A simplistic approach might indeed involve com-
paring a test document to each training document.
However, in the winner-takes-all model described
above, we can rely only on the result of comparing
with the single best training document, which may
not contain enough information to make a good pre-
diction.
A standard strategy to deal with this problem is
to collapse groups of geographically nearby docu-
ments into larger pseudo-documents. This also has
the advantage of reducing the computation time,
as fewer training documents need to be compared
against. Formally, this involves partitioning the
training documents into a set of sets of documents
G = {g1 . . . gn}. A collection d? of pseudo-
documents is formed from this set, such that the
pseudo-document for a particular group gi is simply
the concatenation of the documents in the group:
d?gi =
?
dj?gi
dj .
A location must be associated with each pseudo-
document. This can be chosen based on the parti-
tioning function itself or the locations of the docu-
ments in each group.
Both W&B and SMvZ use uniform grids consist-
ing of cells of equal degree size to partition doc-
uments. We explore an alternative that uses k-d
(k-dimensional) trees to construct a non-uniform
grid that adapts to training sets of different sizes
more gracefully. It ensures a roughly equal num-
ber of documents in each cell, which means that all
pseudo-documents compete on similar footing with
respect to the amount of training data.
W&B define the location for a cell to be its ge-
ographic center, while SMvZ only perform error
analysis in terms of choosing the correct cell. We
obtain consistently improved results using the cen-
troid of the cell?s documents, which takes into ac-
count where the documents are concentrated.
4.1 k-d Trees
A k-d tree is a space-partitioning data structure for
storing points in k-dimensional space, which groups
nearby points into buckets. As one moves down the
tree, the space is split into smaller regions along
chosen dimensions. In this way, it is a generaliza-
tion of a binary search tree to multiple dimensions.
The k-d tree was first introduced by Bentley (1975)
and has since been applied to numerous problems,
e.g. Barnes-Hut simulation (Anderson, 1999) and
nearest-neighbors search (Friedman et al 1977).
Partitioning geolocated documents using a k-d
tree provides finer granularity in dense regions and
coarser granularity elsewhere. For example, doc-
uments from Queens and Brooklyn may show sig-
nificant cultural distinctions, while documents sepa-
rated by the same distance in rural Montana may ap-
pear culturally identical. A uniform grid with large
cells will mash Queens and Brooklyn together, while
small cells will create unnecessarily sparse regions
in Montana.
An important parameter for a k-d tree is its bucket
size, which determines the maximum number of
points (documents in our case) that a cell may con-
tain. By varying the bucket size, the cells can be
made fine- or coarse-grained.
4.2 Partitioning with a k-d Tree
For geolocation, we consider the surface of earth to
be a 2-dimensional space (k=2) over latitude, longi-
tude pairs. We form a k-d tree by a recursive proce-
dure over the training data. Initially, all documents
are placed in the root node of the tree. If the number
of documents in the node exceeds the bucket size,
the node is split into two nodes along a chosen split
dimension and point. This procedure is recursively
called on each of the new child nodes, and repeats
until no node is overflowing. The resulting leaves of
the k-d tree form a patchwork of rectangles which
cover the entire earth.3
When splitting an overflowing node, the choice of
splitting dimension and point can greatly impact the
structure of the resulting k-d tree. Following Fried-
man et al(1977), we choose to always split a node
3We note that the grid ?rectangles? are actually trapezoids
due to the nature of the latitude/longitude coordinate system.
We assume the effect of this is negligible, since most documents
are away from the poles, where distortion is the most extreme.
1503
Figure 1: View of North America showing k-d leaves cre-
ated from GEOWIKI with a bucket size of 600 and the
MIDPOINT method, as visualized in Google Earth.
Figure 2: k-d leaves over the New York City and nearby
areas from the same dataset and parameter settings as in
Figure 1.
along the dimension exhibiting the greatest range of
values. However, there still exist multiple methods
for determining the split point, i.e. the point separat-
ing documents into ?left? and ?right? nodes. In this
paper, we consider two possibilities for selecting this
point: the MIDPOINT method, and the FRIEDMAN
method. The latter splits at the median of all the
points, resulting in an equal number of points in both
the left and right nodes and a perfectly balanced k-d
tree. The former splits at the midpoint between the
two furthest points, allowing for a greater difference
in the number of points in each bin. For geolocation,
the FRIEDMAN splitting method will likely lead to
less sparsity, and therefore more accurate cell selec-
tion. On the other hand, the MIDPOINT method is
likely to draw more geographically desirable bound-
aries.
Figure 1 shows the leaves of the k-d tree formed
over North America using the GEOWIKI dataset,
the MIDPOINT node division method, and a bucket
size of 600. Figure 2 shows the leaves over New
York City and its surrounding area for the same
dataset and settings. More densely populated ar-
eas of the earth (which in turn tend to have more
Wikipedia documents associated with them) contain
smaller and more numerous leaf cells. The cells
over Manhattan are significantly smaller than those
of Queens, the Bronx, and East Jersey, even at such
a coarse bucket size. Though the leaves of the k-d
tree implicitly cover the entire surface of the earth,
our illustrations limit the size of each box by its data,
leaving gaps where no training documents exist.
4.3 Selecting a Representative Location
W&B use the geographic center of a cell as the
geolocation for the pseudo-document it represents.
However, this ignores the fact that many cells will
have imbalances in the dispersion of the documents
they contain ? typically, they will be clumpy, with
documents clustering around areas of high popula-
tion or activity. An alternative is to select the cen-
troid of the locations of all the documents contained
within a cell. Uniform grids with small cells are
not especially sensitive to this choice since the abso-
lute distance between a center or centroid prediction
will not be great, and empty cells are simply dis-
carded. Nonetheless, using the centroid has the ben-
efit of making a uniform grid less sensitive to cell
size, such that larger cells can be used more reliably
? especially important when there are few training
documents.
In contrast, when choosing representative loca-
tions for the leaves of a k-d tree, it is quite important
to use the centroid because the leaves necessarily
span the entire earth and none are discarded (since
all have a roughly similar number of documents in
them). Some areas with low document density are
thus assigned very large cells, such as those over
the oceans, as seen in Figures 1 and 2. Using the
centroid allows these large leaves to be in the mix,
while still predicting the locations in them that have
the greatest document density.
5 Experimental Setup
Configurations. We experiment with several con-
figurations of grids and representative locations.
1504
0 200 400 600 8002
00
250
300
350
Bucket Size
Mean
 Erro
r (km)
ooo
o o
o o
o o
o o
xxx
x
x x
x x x x
xox MidpointFriedman
200 400 600 800 1000 1200
850
900
950
1000
Bucket Size
Mean
 Erro
r (km)
MidpointFriedman
200 400 600 800 1000 1200
1100
1120
1140
1160
1180
Bucket Size
Mean
 Erro
r (km)
MidpointFriedman
(a) (b) (c)
Figure 3: Development set comparisons for (a) GEOWIKI, (b) GEOTEXT, and (c) UTGEO2011-SMALL.
W&B refers to a uniform grid and geographic-
center location selection, UNIFCENTROID to a
uniform grid with centroid location selection,
KDCENTROID to a k-d tree grid with centroid
location selection, and UNIFKDCENTROID to
the union of pseudo-documents constructed by
UNIFCENTROID and KDCENTROID.
We also provide two baselines, both of which are
based on a uniform grid with centroid location selec-
tion. RANDOM predicts a grid cell chosen at random
uniformly; MOSTCOMMONCELL always predicts
the grid cell containing the most training documents.
Note that a most-common k-d leaf baseline does not
make sense, as all k-d leaves contain approximately
the same number of documents.
Evaluation. We use three metrics to measure ge-
olocation performance. The output of each exper-
iment is a predicted coordinate for each test docu-
ment. For each prediction, we compute the error dis-
tance along the surface of the earth to the gold coor-
dinate. We report the mean and median of all such
distances as in W&B and Eisenstein et al(2011).
We also report the fraction of error distances less
than 161 km, corresponding to Cheng et al(2010)?s
measure of predictions within 100 miles of the true
location. This third measure can reveal differences
between models not obvious from just mean and me-
dian.
6 Results
This section provides results for the datasets
described previously: GEOWIKI, GEOTEXT,
UTGEO2011-LARGE and UTGEO2011-SMALL.
We first give details for how we tuned parameters
and algorithmic choices using the development sets,
and then provide performance on the test sets based
on these determinations.
6.1 Tuning
The specific parameters are (1) the partition location
method; (2) the bucket size for k-d partitioning; (3)
the node division method for k-d partitioning; (4) the
degree size for uniform grid partitioning. We tune
with respect to mean error, like W&B.
Partition Location Method. Development set
results show that the centroid always performs bet-
ter than the center for all datasets, typically by a
wide margin (especially for large partition sizes). To
save space, we do not provide details, but point the
reader to the differences in test set results between
W&B and UNIFCENTROID (which are identical ex-
cept that the former uses the center and the latter
uses the centroid) in Tables 1 and 2. All further pa-
rameter tuning is done using the centroid method.
k-d Tree Bucket Size. Bucket size should not be
too large as a proportion of the total number of train-
ing documents. Larger bucket sizes tend to produce
larger leaves, so documents in a partition will have
a higher average distance to the center or centroid
point. This will result in predictions being made at
too coarse a granularity, greatly limiting obtainable
precision even when the correct leaf is chosen.
Conversely, small bucket sizes lead to fewer train-
ing documents per partition. A bucket size of one
reduces to the situation where no pseudo-documents
are used. While this might work well if location pre-
diction is done using the kNNs for a test document, it
1505
Test dataset GEOWIKI GEOTEXT
Method Parameters Mean Med. Acc. Parameters Mean Med. Acc.
RANDOM 0.1? 7056 7145 0.3 5? 2008 1866 1.6
MOSTCOMMONCELL 0.1? 4265 2193 5.0 5? 1158 757 31.3
Eisenstein et al- - - - - 845 501 -
Wing & Baldridge 0.1? 221 11.8 - 5? 967 479 -
UNIFCENTROID 0.1? 181 11.0 90.3 5? 897 432 35.9
KDCENTROID B100, MIDPT. 192 22.5 87.9 B530, FRIED. 958 549 35.3
UNIFKDCENTROID 0.1?, B100, MIDPT. 176 13.4 90.3 5?, B530, FRIED. 890 473 34.1
Table 1: Performance on the held-out test sets of GEOWIKI and GEOTEXT, comparing to the results of Wing and
Baldridge (2011) and Eisenstein et al(2011).
is likely to perform very poorly for the 1NN rule we
adopt. It would also require efficient similarity com-
parisons, using techniques such as locality-sensitive
hashing (Kulis and Grauman, 2009).
The graphs in Figure 3 show development set per-
formance when varying bucket size. For GEOWIKI
and UTGEO2011-LARGE (not shown), increments
of 100 were used, but for the smaller GEOTEXT
and UTGEO2011-SMALL, more fine-grained incre-
ments of 10 were used. In the case of plateaus, as
was common with the FRIEDMAN method, we chose
the middle of the plateau as the bucket size. Overall,
we found optimal bucket sizes of 100 for GEOWIKI,
530 for GEOTEXT, 460 for UTGEO2011-SMALL,
and 1050 for UTGEO2011-LARGE. That the
Wikipedia data requires a smaller bucket size is un-
surprising: the documents themselves are generally
longer and there are many more of them, so a small
bucket size provides good coverage and granularity
without sacrificing the ability to estimate good lan-
guage models for each partition.
Node Division Method. The graphs in Fig-
ure 3 also display the difference between the
two splitting methods. MIDPOINT is clearly bet-
ter for GEOWIKI, while FRIEDMAN is better for
GEOTEXT in the range of bucket sizes produc-
ing the best results. FRIEDMAN is best for
UTGEO2011-LARGE (not shown), but MIDPOINT
is best for UTGEO2011-SMALL.
These results only partly confirm our expecta-
tions. We expected FRIEDMAN to perform bet-
ter on smaller datasets, as it distributes the doc-
uments evenly and avoids many sparsity issues.
We expected MIDPOINT to win on larger datasets,
where all nodes receive plentiful data and the k-d
tree would choose more representative geographical
boundaries.
Cell Size. Following W&B, we choose a
cell degree size of 0.1? for GEOWIKI, and a
cell degree size of 5.0? for GEOTEXT. For
UTGEO2011-LARGE and UTGEO2011-SMALL,
we follow the procedure of W&B, trying sizes
0.1?, 0.5?, 1.0?, 5.0?, and 10.0?, selecting the one
which performed best on the development set. For
UTGEO2011-SMALL, this resulted in coarse cells
of 10.0?, while for UTGEO2011-LARGE, cell sizes
of 0.1? were best.
With these tuned parameters, the average num-
ber of training tokens per k-d leaf was approx-
imately 26k for GEOWIKI, 197k for GEOTEXT,
250k for UTGEO2011-SMALL, and 954k for
UTGEO2011-LARGE.
6.2 Held-out Test Sets
Table 1 shows the performance on the test sets of
GEOWIKI and GEOTEXT of the different configu-
rations, along with that of W&B and Eisenstein et
al. (2011) where possible. The results obtained by
W&B on GEOWIKI are already very strong, but we
do see a clear improvement by changing from the
center-based locations for pseudo-documents they
used to the centroid-based locations we employ:
mean error drops from 221 km to 181 km, and me-
dian error from 11.8 km to 11.0 km. Also, we reduce
the mean error further to 176 km for the configu-
ration that combines the uniform grid and the k-d
partitions, though at the cost of increasing median
error somewhat. The 161 km accuracy is around
90% for all configurations, indicating that the gen-
eral language modeling approach we employ is very
1506
Test dataset UTGEO2011
Training dataset UTGEO2011-SMALL UTGEO2011-LARGE
Method Parameters Mean Med. Acc. Parameters Mean Med. Acc.
RANDOM 10? 1975 1833 2.3 0.1? 1627 1381 2.0
MOSTCOMMONCELL 10? 1522 1186 9.3 0.1? 1525 1185 11.8
Wing & Baldridge 10? 1223 825 3.4 0.1? 956 570 30.9
UNIFCENTROID 10? 1147 782 12.3 0.1? 956 570 30.9
KDCENTROID B460, MIDPT. 1098 733 18.1 B1050, FRIED. 860 463 34.6
UNIFKDCENTROID 10?, B460, MIDPT. 1080 723 18.1 0.1?, B1050, FRIED. 913 532 33.0
Table 2: Performance on the held-out test set of UTGEO2011 for different configurations trained on
UTGEO2011-SMALL (comparable in size to GEOTEXT) and UTGEO2011-LARGE. The numbers given for W&B
were produced from their implementation, and correspond to uniform grid partitioning with locations from centers
rather than centroids.
robust for fact-oriented texts that are rich in explicit
toponyms and geographically relevant named enti-
ties.
For GEOTEXT, the results show that the uniform
grid with centroid locations is the most effective of
our configurations. It improves on Eisenstein et al
(2011) by 69 km with respect to median error, but
has 52 km worse performance than their model with
respect to mean error. This indicates that our model
is generally more accurate, but that it is compara-
tively more wildly off on some documents. Their
model is a sophisticated one that attempts to build
detailed models of the geographic linguistic varia-
tion found in the dataset. Dialectal cues are actually
the most powerful ones in the GEOTEXT dataset,
and it seems our general approach of winner-takes-
all (1NN) hurts performance in this respect, espe-
cially with a very small training set.
Table 2 shows the performance on the test set of
UTGEO2011 with the UTGEO2011-SMALL and
UTGEO2011-LARGE training sets. (Performance
for W&B is obtained from their code.4) With
the small training set, error is worse than with
GEOTEXT, reflecting the wider geographic scope of
UTGEO2011. KDCENTROID is much more effec-
tive than the uniform grids, but combining it with the
uniform grid in UNIFKDCENTROID edges it out by
a small amount. More interestingly, KDCENTROID
is the strongest on all measures when using the large
training set, beating UNIFCENTROID by an even
larger margin for mean and median error than with
4https://bitbucket.org/utcompling/
textgrounder/wiki/WingBaldridge2011
the small training set. The bucket size used with the
large training set is double that for the small one,
but there are many more leaves created since there
are 42 times more training documents. With the ex-
tra data, the model is able to adapt better to the dis-
persion of documents and still have strong language
models for each leaf that work well even with our
greedy winner-takes-all decision method.
Note that the accuracy measurements for all
UTGEO2011 experiments are substantially lower
than those reported by Cheng et al(2010), who
report a best accuracy within 100 miles of 51%.
While UTGEO2011-LARGE contains a substan-
tially larger number of tweets, Cheng et al(2010)
limit themselves to users with at least 1,000
tweets, while we have an average of 85 tweets
per user. Their reported mean error distance of
862 km (versus our best mean of 860 km on
UTGEO2011-LARGE) indicates that their perfor-
mance is hurt by a relatively small number of ex-
tremely incorrect guesses, as ours appears to be.
Figure 4 provides a learning curve on
UTGEO2011?s development set for KDCENTROID.
Performance improves greatly with more data,
indicating that GEOTEXT performance would also
improve with more training data. Parameters, espe-
cially bucket size, need retuning as data increases,
which we hope to estimate automatically in future
work
Finally, we note that the KDCENTROID
method was faster than other methods. While
UNIFCENTROID took nearly 19 hours to com-
plete the test run on GEOWIKI (approximately
1507
0e+00 1e+05 2e+05 3e+05 4e+05
900
950
105
0
Training Set Size (# users)
Mea
n E
rror
 (km
)
l
l
l
l
l
l
l
Figure 4: Learning curve of KDCENTROID on the
UTGEO2011 development set.
1.38 seconds per test document), KDCENTROID
took only 80 minutes (.078 s/doc). Similarly,
UNIFCENTROID took about 67 minutes to
run on UTGEO2011-LARGE (0.34 s/doc), but
KDCENTROID took only 27 minutes (0.014 s/doc).
Generally, the KDCENTROID partitioning results
in fewer cells, and therefore fewer KL-divergence
comparisons. As expected, the UNIFKDCENTROID
model needs as much time as the two together,
taking roughly 21 hours for GEOWIKI (1.52 s/doc)
and 85 minutes for UTGEO2011-LARGE (0.36
s/doc).
7 Discussion
7.1 Error Analysis
We examine some of the greatest error distances
to better understand and improve our models. In
many cases, landmarks in Australia or New Zealand
are predicted in European locations with similarly-
named landmarks, or vice versa ? e.g. the Theatre
Royal, Hobart in Australia is predicted to be in Lon-
don?s theater district, and the Embassy of Australia,
Paris is predicted to be in the capital city of Aus-
tralia. Thus, our model may be inadvertently cap-
turing what Clements et al(2010) call wormholes,
places that are related but not necessarily adjacent.
Some of the other large errors stem from incorrect
gold labels, in particular due to sign errors in latitude
or longitude, which can place documents 10,000 or
more km from their correct locations.
Word Error Word Error
paramus 78 6100 130
ludlow 79 figueroa 133
355 99 dundas 138
ctfuuu 101 120th 139
74th 105 mississauga 140
5701 105 pulaski 144
bloomingdale 122 cucina 146
covina 133 56th 153
lawrenceville 122 403 157
ctfuuuu 124 428 161
Table 3: The 20 words with least average error
(km) in the UTGEO2011 development set, trained
on the UTGEO2011-SMALL training set, using the
KDCENTROID approach with our best parameters. Only
words that occur in at least 10 documents are shown.
Word Error Word Error
seniorpastor 1.1 KS01 2.4
prebendary 1.6 Keio 2.5
Wornham 1.7 Vrah 2.5
Owings 1.9 overspill 2.5
Londoners 2.0 Oriel 2.5
Sandringham 2.1 Holywell 2.6
Sheffield?s 2.2 \?vr&h 2.6
Oxford?s 2.2 operetta 2.6
Belair 2.3 Supertram 2.6
Beckton 2.4 Chanel 2.7
Table 4: Top 20 words with the least average er-
ror (km) in the GEOWIKI development set, using the
UNIFKDCENTROID approach with our best parameters.
Only words occurring in at least 10 documents are shown.
7.2 Most Predictive Words
Our approach relies on the idea that the use of certain
words correlates with a Twitter user or Wikipedia
article?s location. To investigate which words tend
to be good indicators of location, we computed, for
each word in a development set, the average error
distance of documents containing that word. Table 3
gives the 20 words with the least error, among
those that occur in at least 10 documents (users),
for the UTGEO2011 development set, trained on
UTGEO2011-SMALL.
Many of the best words are town names (paramus,
ludlow, bloomingdale), street names (74th, figueroa,
1508
120th), area codes (403), and street numbers (5701,
6100). All are highly locatable terms, as we would
expect. Many of the street addresses are due to
check-ins with the location-based social networking
service Foursquare (e.g. the tweet I?m at Starbucks
(7301 164th Ave NE, Redmond Town Center, Red-
mond)), where the user is literally broadcasting his
or her location. The token ctfuuu(u)?an elongation
of the internet abbreviation ctfu, or cracking the fuck
up?is a dialectal or stylistic feature highly indica-
tive of the Washington, D.C. area.
Similarly, several place names (Wornham, Belair,
Holywell) appear in GEOWIKI. Operettas are a cul-
tural phenomenon largely associated with France,
Germany, and England and particularly with specific
theaters in these countries. However, other highly
specific tokens such as KS01 have a very low aver-
age error because they occur in few documents and
are thus highly unambiguous indicators of location.
Other terms, like seniorpastor and \?vr&h, are due
to extraction errors in the dataset created by W&B,
and are carried along because of a high correlation
with specific documents.
8 Conclusion
We have shown how to construct an adaptive grid
with k-d trees that enables robust text geolocation
and scales well to large training sets. It will be inter-
esting to consider how it interacts with other strate-
gies for improving the IR-based approach. For ex-
ample, the pseudo-document word distributions can
be smoothed based on nearby documents or on the
structure of the k-d tree itself. Integrating our system
with topic models or Bayesian methods would likely
provide more insight with regard to the most dis-
criminative and geolocatable words. We also expect
predicting locations based on multiple most similar
documents (kNN) to be more effective in predict-
ing document location, as the second and third most
similar training documents together may sometimes
be a better estimation of its distribution than just the
first alone. Employing k Nearest Neighbors also al-
lows for more sophisticated methods of location es-
timation than a single leaf?s centroid. Other possi-
bilities include constructing multiple k-d trees using
random subsets of the training data to reduce sensi-
tivity to the bucket size.
In this article, we have considered each user in
isolation. However, Liben-Nowell et al(2005) show
that roughly 70% of social network links can be de-
scribed using geographic information and that the
probability of a social link is inversely proportional
to geographic distance. Backstrom et al(2010) ver-
ify these results on a much larger scale using ge-
olocated Facebook profiles: their algorithm geolo-
cates users with only the social graph and signif-
icantly outperforms IP-based geolocation systems.
Given that both Twitter and Wikipedia have rich,
linked document/user graphs, a natural extension to
our work here will be to combine text and network
prediction for geolocation. Sadilek et al(2012)
also show that a combination of textual and so-
cial data can accurately geolocate individual tweets
when scope is limited to a single city.
Tweets are temporally ordered and the geographic
distance between consecutive tweeting events is
constrained by the author?s movement. For tweet-
level geolocation, it will be useful to build on work
in geolocation that considers the temporal dimen-
sion (Chen and Grauman, 2011; Kalogerakis et al
2009; Sadilek et al 2012) to make better predictions
for documents/images that are surrounded by others
with excellent cues, but which are hard to resolve
themselves.
9 Acknowledgments
We would like to thank Matt Lease and the three
anonymous reviewers for their feedback. This re-
search was supported by a grant from the Morris
Memorial Trust Fund of the New York Community
Trust.
References
Richard J. Anderson. 1999. Tree data structures for
n-body simulation. SIAM Journal on Computing,
28(6):1923?1940.
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
the 19th International Conference on World Wide Web,
pages 61?70.
Jon Louis Bentley. 1975. Multidimensional binary
search trees used for associative searching. Commu-
nications of the ACM, 18(9):509?517.
1509
Chao-Yeh Chen and Kristen Grauman. 2011. Clues from
the beaten path: Location estimation with bursty se-
quences of tourist photos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 1569?1576.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: A content-based approach
to geo-locating twitter users. In Proceedings of the
19th ACM International Conference on Information
and Knowledge Management, pages 759?768.
Martin Clements, Pavel Serdyukov, Arjen P. de Vries, and
Marcel J.T. Reinders. 2010. Finding wormholes with
flickr geotags. In Proceedings of the 32nd European
Conference on Information Retrieval, pages 658?661.
Sebastian Cobarrubias. 2009. Mapping machines: ac-
tivist cartographies of the border and labor lands of
Europe. Ph.D. thesis, University of North Carolina at
Chapel Hill.
Dorin Comaniciu and Peter Meer. 2002. Mean shift: a
robust approach toward feature space analysis. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 24(5):603?619.
Junyan Ding, Luis Gravano, and Narayanan Shivaku-
mar. 2000. Computing geographical scopes of web
resources. In Proceedings of the 26th International
Conference on Very Large Data Bases, pages 545?556.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1277?1287.
Jacon Eisenstein, Ahmed Ahmed, and Eric P. Xing.
2011. Sparse additive generative models of text. In
Proceedings of the 28th International Conference on
Machine Learning, pages 1041?1048.
Jerome H. Friedman, Jon Louis Bentley, and Raphael Ari
Finkel. 1977. An algorithm for finding best matches
in logarithmic expected time. ACM Transactions on
Mathematical Software, 3:209?226.
James Hays and Alexei A. Efros. 2008. im2gps: esti-
mating geographic information from a single image.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1?8.
Evangelos Kalogerakis, Olga Vesselova, James Hays,
Alexei Efros, and Aaron Hertzmann. 2009. Image se-
quence geolocation with human travel priors. In Pro-
ceedings of the IEEE 12th International Conference on
Computer Vision, pages 253?260.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in Glasgow?: Model-
ing locations with tweets. In Proceedings of the 3rd
International Workshop on Search and Mining User-
generated Contents, pages 61?68.
Brian Kulis and Kristen Grauman. 2009. Kernelized
locality-sensitive hashing for scalable image search.
In Proceedings of the 12th International Conference
on Computer Vision, pages 2130?2137.
David Liben-Nowell, Jasmine Novak, Ravi Kumar, Prab-
hakar Raghavan, and Andrew Tomkins. 2005. Geo-
graphic routing in social networks. Proceedings of the
National Academy of Sciences of the United States of
America, 102(33):11623?11628.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proceedings of the 5th ACM Inter-
national Conference on Web Search and Data Mining,
pages 723?732.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings
of the 17th ACM SIGSPATIAL International Confer-
ence on Advances in Geographic Information Systems,
pages 42?51.
Pavel Serdyukov, Vanessa Murdock, and Roelof van
Zwol. 2009. Placing flickr photos on a map. In Pro-
ceedings of the 32nd International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 484?491.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 955?964.
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 334?342.
1510
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 290?300,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Parsing low-resource languages using Gibbs sampling
for PCFGs with latent annotations
Liang Sun
1
Jason Mielens
2
1
Department of Mechanical Engineering
2
Department of Linguistics
The University of Texas at Austin The University of Texas at Austin
sally722@utexas.edu {jmielens,jbaldrid}@utexas.edu
Jason Baldridge
2
Abstract
PCFGs with latent annotations have been
shown to be a very effective model for phrase
structure parsing. We present a Bayesian
model and algorithms based on a Gibbs sam-
pler for parsing with a grammar with latent an-
notations. For PCFG-LA, we present an ad-
ditional Gibbs sampler algorithm to learn an-
notations from training data, which are parse
trees with coarse (unannotated) symbols. We
show that a Gibbs sampling technique is ca-
pable of parsing sentences in a wide variety
of languages and producing results that are
on-par with or surpass previous approaches.
Our results for Kinyarwanda and Malagasy in
particular demonstrate that low-resource lan-
guage parsing can benefit substantially from a
Bayesian approach.
1 Introduction
Despite great progress over the past two decades on
parsing, relatively little work has considered the prob-
lem of creating accurate parsers for low-resource lan-
guages. Existing work in this area focuses primarily on
approaches that use some form of cross-lingual boot-
strapping to improve performance. For instance, Hwa
et al. (2005) use a parallel Chinese/English corpus and
an English dependency grammar to induce an anno-
tated Chinese corpus in order to train a Chinese de-
pendency grammar. Kuhn (2004b) also considers the
benefits of using multiple languages to induce a mono-
lingual grammar, making use of a measure for data re-
liability in order to weight training data based on confi-
dence of annotation. Bootstrapping approaches such as
these achieve markedly improved results, but they are
dependent on the existence of a parallel bilingual cor-
pus. Very few such corpora are readily available, par-
ticularly for low-resource languages, and creating such
corpora obviously presents a challenge for many practi-
cal applications. Kuhn (2004a) shows some of the diffi-
culty in handling low-resource languages by examining
various tasks using Q?anjob?al as an example. Another
approach is that of Bender et al. (2002), who take a
more linguistically-motivated approach by making use
of linguistic universals to seed newly developed gram-
mars. This substantially reduces the effort by making
it unnecessary to learn the basic parameters of a lan-
guage, but it lacks the robustness of grammars learned
from data.
Recent work on Probabilistic Context-Free Gram-
mars with latent annotations (PCFG-LA) (Matsuzaki et
al., 2005; Petrov et al., 2006) have shown them to be
effective models for syntactic parsing, especially when
less training material is available (Liang et al., 2009;
Shindo et al., 2012). The coarse nonterminal symbols
found in vanilla PCFGs are refined by latent variables;
these latent annotations can model subtypes of gram-
mar symbols that result in better grammars and enable
better estimates of grammar productions. In this pa-
per, we provide a Gibbs sampler for learning PCFG-
LA models and show its effectiveness for parsing low-
resource languages such as Malagasy and Kinyawanda.
Previous PCFG-LA work focuses on the prob-
lem of parameter estimation, including expectation-
maximization (EM) (Matsuzaki et al., 2005; Petrov et
al., 2006), spectral learning (Cohen et al., 2012; Co-
hen et al., 2013), and variational inference (Liang et
al., 2009; Wang and Blunsom, 2013). Regardless of
inference method, previous work has used the same
method to parse new sentences: a Viterbi parse un-
der a new sentence-specific PCFG obtained from an
approximation of the original grammar (Matsuzaki et
al., 2005). Here, we provide an alternative approach to
parsing new sentences: an extension of the Gibbs sam-
pling algorithm of Johnson et al. (2007), which learns
rule probabilities in an unsupervised PCFG.
We use a Gibbs sampler to collect sampled trees
theoretically distributed from the true posterior distri-
bution in order to parse. Priors in a Bayesian model
can control the sparsity of grammars (which the inside-
outside algorithm fails to do), while naturally incorpo-
rating smoothing into the model (Johnson et al., 2007;
Liang et al., 2009). We also build a Bayesian model
for parsing with a treebank, and incorporate informa-
tion from training data as a prior. Moreover, we ex-
tend the Gibbs sampler to learn and parse PCFGs with
latent annotations. Learning the latent annotations is
a compute-intensive process. We show how a small
amount of training data can be used to bootstrap: af-
ter running a large number of sampling iterations on a
small set, the resulting parameters are used to seed a
smaller number of iterations on the full training data.
290
This allows us to employ more latent annotations while
maintaining reasonable training times and still making
full use of the available training data.
To determine the cross-linguistic applicability of
these methods, we evaluate on a wide variety of lan-
guages with varying amounts of available training data.
We use English and Chinese as examples of languages
with high data availability, while Italian, Malagasy, and
Kinyarwanda provide examples of languages with little
available data.
We find that our technique comes near state of the
art results on large datasets, such as those for Chinese
and English, and it provides excellent results on limited
datasets ? both artificially limited in the case of En-
glish, and naturally limited in the case of Italian, Mala-
gasy, and Kinyarwanda. This, combined with its abil-
ity to run off-the-shelf on new languages without any
supporting materials such as parallel corpora, make it a
valuable technique for the parsing of low-resource lan-
guages.
2 Gibbs sampling for PCFGs
Our starting point is a Gibbs Sampling algorithm for
vanilla PCFGs introduced by Johnson et al. (2007) for
estimating rule probabilities in an unsupervised PCFG.
We focus instead on using this algorithm for parsing
new sentences and then extending it to learn PCFGs
with latent annotations. We begin by summarizing the
Bayesian PCFG and Gibbs sampler defined by Johnson
et al. (2007).
Bayesian PCFG For a grammarG, each rule r in the
set of rules R has an associated probability ?
r
. The
probabilities for all the rules that expand the same non-
terminal A must sum to one:
?
A???R
?
A??
= 1.
Given an input corpusw=(w
(1)
, ? ? ? ,w
(n)
), we in-
troduce a latent variable t=(t
(1)
, ? ? ? , t
(n)
) for trees
generated by G for each sentence. The joint posterior
distribution of t and ? conditioned on w is:
p(t, ? | w) ? p(?)p(w | t)p(t | ?)
= p(?)(
?
n
i=1
p(w
(i)
| t
(i)
)p(t
(i)
| ?))
= p(?)(
?
n
i=1
p(w
(i)
| t
(i)
)
?
r?R
?
f
r
(t
(i)
r
)) (1)
Here f
r
(t) is the number of occurrences of rule r in the
derivation of t; p(w
(i)
| t
(i)
) = 1 if the yield of t
(i)
is
the sequence w
(i)
, and 0 otherwise.
We use a Dirichlet distribution parametrized by ?
A
:
Dir(?
A
) as the prior of the probability distribution for
all rules expanding non-terminal A (p(?
A
)). The prior
for all ?, p(?), is the product of all Dirichlet distri-
butions over all non-terminals A ? N : p(? | ?) =
?
A?N
p(?
A
| ?
A
).
Since the Dirichlet distribution is conjugate to the
Multinomial distribution, which we use to model the
likelihood of trees, the conditional posterior of ?
A
can
be updated as follows:
p
G
(? | t, ?) ? p
G
(t | ?)p(? | ?)
? (
?
r?R
?
f
r
(t)
r
)(
?
r?R
?
?
r
?1
r
)
=
?
r?R
?
f
r
(t)+?
r
?1
r
(2)
which is still a Dirichlet distribution with updated pa-
rameter f
r
(t) + ?
r
for each rule r ? R.
Gibbs sampler The parameters of the PCFG model
can be learned from an annotated corpus by simply
counting rules. However, parsing cannot be done di-
rectly with standard CKY as with standard PCFGs,
so we use the Gibbs sampling algorithm presented in
Johnson et al. (2007). An additional motivation for us-
ing this algorithm is that Johnson et al. use it for learn-
ing without annotated structures, and in future work we
seek to learn from fewer, and at times partial, annota-
tions.
An advantage of using Gibbs sampling for Bayesian
inference, as opposed to other approximation algo-
rithms such as Variational Bayesian inference (VB) and
Collapsed Variational Bayesian inference (CVB), is
that Markov Chain Monte Carlo (MCMC) algorithms
are guaranteed to converge to a sample from the true
posterior under appropriate conditions (Taddy, 2011).
Both VB and CVB converge to inaccurate and locally
optimal solutions, like EM. In some models, CVB can
achieve more accurate results due to weaker assump-
tions (Wang and Blunsom, 2013). Another advantage
of Gibbs sampling is that the sampler allows for parallel
computation by allowing each sentence to be sampled
entirely independently of the others. After each paral-
lel sampling stage, all model parameters are updated in
a single step, and the process then repeats (see ?2).
To sample the joint posterior p(t, ? | w), we sample
production probabilities ? and then trees t from these
conditional distributions:
p(t | ?,w, ?) =
?
n
i=1
p(t
i
| w
i
, ?) (3)
p(? | t,w, ?) =
?
A?N
Dir(?
A
| f
A
(t) + ?
A
) (4)
Step 1: Sample Rule Probabilities. Given trees t and
prior ?, the production probabilities ?
A
for each non-
terminal A?N are sampled from a Dirichlet distribu-
tion with parameters f
A
(t) + ?
A
. f
A
(t) is a vector,
and each component of f
A
(t), is the number of occur-
rences of one rule expanding nonterminal A.
Step 2: Sample Tree Structures. To sample trees from
p(t
i
| w
i
, ?), we use the efficient sampling scheme
used in previous work (Goodman, 1998; Finkel et al.,
2006; Johnson et al., 2007). There are two parts to this
algorithm. The first constructs an inside table as in the
Inside-Outside algorithm for PCFGs (Lary and Young,
1990). The second selects the tree by recursively sam-
pling productions from top to bottom.
291
Require: A is parent node of binary rule; w
i,k
is a
span of words: i+ 1 < k
function TREESAMPLER(A, i, k)
for i < j < k and pair of child nodes of
A:B,C do
P (j, B,C) =
?
A?BC
?p
B,i,j
?p
C,j,k
?
p
A,i,k
end for
Sample j?, B?, C? from multinomial distribution
for (j, B,C) with probabilities calculated above
return j?, B?, C?
end function
Algorithm 1: Sampling split position and rule to ex-
pand parent node
Consider a sentence w, with sub-spans w
i,k
=
(w
i+1
, ? ? ? , w
k
). Given ?, we construct the inside ta-
ble with entries p
A,i,k
for each nonterminal and each
word span w
i,k
: 0 ? i < k ? l, where p
A,i,k
=
P
G
A
(w
i,k
|?) is the probability that words i through k
were produced by the non-terminal A. The table is
computed recursively by
p
A,k?1,k
= ?
A?w
k
(5)
p
A,i,k
=
?
A?BC?R
?
i<j<k
?
A?BC
? p
B,i,j
? p
C,j,k
(6)
for all A,B,C ? N and 0 ? i < j < k ? l.
The resulting inside probabilities are then used to
generate trees from the distribution of all valid trees of
the sentence. The tree is generated from top to bottom
recursively with the function TreeSampler defined in
Algorithm 1.
In unsupervised PCFG learning, the rule probabil-
ities can be resampled using the sampled trees, then
used to reparse the corpus, and so on. We use this
property to refine latent annotations for the PCFG-LA
model described in the next section.
3 PCFG with latent annotations
When labeled trees are available, rule frequencies can
be directly extracted and used as priors for a PCFG.
However, when learning PCFG-LAs, we must learn the
fine-grained rules from the coarse trees, so we extend
the Gibbs sampler to assign latent annotations to unan-
notated trees. The resulting learned PCFG-LA parser
outputs samples of annotated trees so that we can ob-
tain unannotated trees after marginalizing.
3.1 Model
With the PCFG-LA model (Matsuzaki et al., 2005;
Petrov et al., 2006) fine-grained CFG rules are auto-
matically induced from training, effectively providing
a form of feature engineering without human interven-
tion. GivenH = {1, ? ? ? ,K}, a set of latent annotation
symbols, and x ? H:
? ?
A[x]?U
is the probability of rule A[x] ? U ,
where U ? N ?N ? T . The probabilities for all
rules that expand the same annotated non-terminal
must sum to one.
? ?
A[x],B,C?y,z
is the probability of assigning la-
tent annotation y, z to child nodes B,C of A[x].
?
y,z?H?H
?
A[x],B,C?y,z
= 1.
The inputs to the PCFG-LA are a CFG G with finite
number of latent annotations for each non-terminal, an
initial guess of probabilities of grammar rule ?
0
, and a
prior ?
?
is learned from training.
The joint posterior distribution of t and ?, ? condi-
tioned on w is:
p(t, ?, ? | w) ? p(?, ?)p(w | t)p(t | ?, ?)
= p(?)p(?)(
?
n
i=1
p(w
i
| t
i
)p(t
i
| ?, ?)) (7)
We assume that ? and ? are independent to get
P (?, ?) = P (?)P (?).
To learn parameters ?, ?, we use a Dirichlet distribu-
tion as a prior for both ? and ?. The distribution for all
rules expanding A[x] is:
P (? | ?
?
) =
?
A?N,x?H
P (?
A[x]
| ?
?
A[x]
) (8)
The distribution for latent annotations associated
with child nodes of A[x]? BC is:
P (? | ?
?
) =
?
y,z?H?H
P (?
A[x],B,C
| ?
?
A[x],B,C
).
(9)
With this setting, the conditional posterior of ?
A[x]
and ?
A[x],B,C
can be updated, as in ?2. For all unary
and binary rules r expanding A[x]:
?
A[x]
| t, ?
?
? Dir(f
r
(t) + ?
?
r
) (10)
Here, f
r
(t) is the number of occurrence of annotated
rule r in t. Also, for combination of latent annotations
y, z ? H ?H assigned to B,C in rule A[x]? B,C:
?
A[x],B,C
| t, ?
?
? Dir(f
d
(t) + ?
?
d
) (11)
Here, f
d
(t) is the number of occurrences of combina-
tion d in t.
3.2 Learning PCFG-LAs from raw text
To learn from raw text, we extend the sampler in ?2
to PCFG-LA. Given priors ?
?
, ?
?
and raw text, the al-
gorithm alternates between two steps. The first sam-
ples trees for the entire corpus; the second samples ?
and ? from Dirichlet distributions with updated param-
eters, combining priors and counts from sampled trees.
The algorithm then alternates between these steps un-
til convergence. The outputs are samples of ?, ? and
annotated trees.
The parsing process is specified in Algorithm 2. The
first step assigns a tree to a sentence, say w
0,l
. We first
292
Require: w
1
, ? ? ? , w
n
are raw sentences; ?
0
, ?
0
are
initial values; ?
?
, ?
?
are priors; M is the number
of iterations
function PARSE(w
1
, .., w
n
, ?
0
, ?
0
, ?
?
, ?
?
,M )
for iteration i = 1 to M do
for sentence s = 1 to n do
Calculate Inside Table
Sample tree nodes and associated latent
annotations, get tree structure t
(i)
s
end for
Sample ?
(i)
, ?
(i)
end for
for sentence s = 1 to n do
Marginalize the latent annotations to get
unannotated trees T
(1)
s
, ? ? ? , T
(M)
s
Find the mode of T
(1)
s
, ? ? ? , T
(M)
s
: T
s
end for
return T
1
, ? ? ? , T
n
end function
Algorithm 2: Parsing new sentences
construct an inside table (see ?2). Each entry in the ta-
ble stores the probability that a word span is produced
by a given annotated nonterminal. For root node S,
with ?, ? and inside table p
A[x],i,k
, we sample one an-
notation based on all p
S[x],0,l
, x ? H . Assume that
we sampled x for S, we further sample a rule to ex-
pand S[x] and possible splits of the span w
0,l
jointly.
Assume that we sampled nonterminals B,C to expand
S[x], where B is responsible for w
0,j
and C is respon-
sible for w
j,l
. We further sample annotations for B,C
together, say y, z. Then we sample rules and split po-
sitions to expand B[y] and C[z], and continue until
reaching the terminals.
This algorithm alone could be used for unsupervised
learning of PCFG-LA if we input a non-informed or
weakly-informed prior ?
?
and ?
?
. With access to
unannotated trees for training, we only need to assign
latent annotations to them and then use the frequen-
cies of these annotated rules as the prior when parsing.
The details of training when trees are available are il-
lustrated in ?3.3.
Once we have trees (with latent annotations), the
step of sampling ? and ? from a Dirichlet distribution
is direct. We need to count the number of occurrences
f
r
(t) for each rule r like A[x] ? U,U ? N ?N ? T
in updated annotated trees t, and draw ?
A[x]
from the
updated Dirichlet distribution Dir(f
A[x]
(t) + ?
?
A[x]
).
We also need to count the number of occurrences of
f
d
(t) for each combination of yz ? H?H assigned to
B,C givenA[x]? B,C in t, and draw ?
A[x],B,C
from
the updated Dirichlet distribution Dir(f
A[x],B,C
(t) +
?
?
A[x],B,C
) similarly.
To parse a sentence, we first calculate the inside table
and then sample the tree.
Calculate the inside table. Given ?,? and a string
w=w
0,l
, we construct a table with entries p
A[x],i,k
for
each A?N , x ? H and 0 ? i < k ? l, where
p
A[x],i,k
= P
G
A[x]
(w
i,k
|?, ?) is the probability that
words i through k were produced by the annotated non-
terminal A[x]. The table can be computed recursively,
for all A ? N , x ? H , by
p
A[x],k?1,k
= ?
A[x]?w
k
(12)
p
A[x],i,k
=
?
A[x]?BC:BC?N?N
?
j:i<j<k
?
yz?H?H
?
A[x]?BC
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
(13)
Sample the tree, top to bottom. First, from start sym-
bol S, sample latent annotation from multinomial with
probability pi
S[x]
p
S[x],0,l
for each x ? H . Next, given
annotated non-terminal A[x] and i, k, sample possible
child nodes and split positions from multinomial with
probability:
p(B,C, j) =
1
p
A[x],i,k
?
?
y,z?H
?
A[x]?BC
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
(14)
Here the probability is calculated by marginaliz-
ing all possible latent annotations for B,C, and
?
A[x]?BC
?
A[x]BC?yz
is the probability of choosing
B[y], C[z] to expandA[x], and p
B[y],i,j
p
C[z],j,k
are the
probabilities for B[y] and C[z] to be responsible for
word span w
i,j
and w
j,k
respectively. And p
A[x],i,k
is
the normalizing term.
Third, given A[x], B,C, i, j, k, sample annotations
for B,C from multinomial with probability:
p(y, z) =
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
?
y,z
?
A[x]BC?yz
p
B[y],i,j
p
C[z],j,k
(15)
A crucial aspect of this procedure is that all trees can
be sampled independently. This parallel process pro-
duces a substantial speed gain that is important partic-
ularly when using more latent annotations. After all
trees have been sampled (independently), the counts
from each individual tree are combined prior to the next
sampling iteration.
3.3 Learning from coarse training trees
In training, we need to learn the probabilities of fine-
grained rules given coarsely-labeled trees. We perform
Gibbs sampling on the training data by first iteratively
sampling probabilities and then assigning annotations
to tree nodes. We use the average counts of anno-
tated production rules from sampled trees to produce
the prior ?
?
and ?
?
incorporated into parsing raw sen-
tences.
We first index the non-terminal nodes of each tree T
by 1, 2, ? ? ? from top to bottom, and left to right. Then
the sampler iterates between two steps. The first sam-
ples ?, ? given annotated trees (as in ?3.2). The sec-
ond samples latent annotations for nonterminal nodes
293
Require: T
1
, ? ? ? , T
n
are fully parsed trees; ?
0
, ?
0
are initial values; ?
?
0
, ?
?
0
are priors; M is the
number of iterations
function ANNO(T
1
, ? ? ? , T
n
, ?
0
, ?
0
, ?
?
0
, ?
?
0
,M )
for iteration i = 1 to M do
for sentence s = 1 to n do
Calculate inside probability
Sample latent annotations for each node
in the tree, get tree with latent annotations t
(i)
s
end for
Sample ?
(i)
, ?
(i)
end for
return Mean of number of occurrences of
production rules and associated latent annotations
from all sampled annotated trees
end function
Algorithm 3: Learning prior from training
in parsed trees, which also takes two steps. The first
step is to, for each node in the tree, calculate and store
the probability that the node is annotated by x. The
second step is to jointly sample latent annotations for
child nodes of root nodes, and then continue this pro-
cess from top to bottom until reaching the pre-terminal
nodes.
Step one: inside probabilities. Given tree T , com-
pute b
i
T
[x] for each non-terminal i recursively:
1. If node N
i
is a pre-terminal node above terminal
symbol w, then for x?H
b
i
T
[x] = ?
N
i
[x]?w
(16)
2. Otherwise, let j, k be two child nodes of i, then
for x ? H
b
i
T
[x] =
?
y,z?H
?
N
i
[x]?N
j
N
k
?
N
i
[x]N
j
N
k
?y,z
b
j
T
[y]b
k
T
[z] (17)
Step two: outside sampling. Given inside probabil-
ity b
i
T
[x] for every non-terminal i and all latent annota-
tions x?H , we sample the latent annotations from top
to bottom:
1. If node i is the root node (i = 1), then sample x ?
H from a multinomial distribution with f
i
T
[x] =
pi(N
i
[x]).
2. For a parent node with sampled latent annotation
N
i
[x] with childrenN
j
, N
k
, sample latent annota-
tions for these two nodes from a multinomial dis-
tribution with
f
i
T
[y, z] =
1
b
i
T
[x]
?
?
N
i
[x]?N
j
N
k
?
N
i
[x]N
j
N
k
?y,z
b
j
T
[y]b
k
T
[z] (18)
After training, we take the average counts of sampled
annotated rules and combinations of latent annotations
as priors to parse raw sentences.
4 Experiments
1
Our goal is to understand parsing efficacy using sam-
pling and latent annotations for low-resource lan-
guages, so we perform experiments on five languages
with varying amount of training data. We compare
our results to a number of previously established base-
lines. First, for all languages, we use both a stan-
dard unsmoothed PCFG and the Bikel parser, trained
on the training corpus. Additionally, we compare to
state-of-the-art results for both English and Chinese,
which have an existing body of work in PCFGs using
a Bayesian framework. For Chinese, we compare to
Huang & Harper (2009), using their results that only
use the Chinese Treebank (CTB). For English, we com-
pare to Liang et al. (2009). Prior results for parsing
the constituency version of the Italian data are avail-
able from Alicante et al. (2012), but as they make use
of a different version of the treebank including extra
sentences, and additionally use the extensive functional
tags present in the corpus, we do not directly compare
our results to theirs.
2
4.1 Data
English (ENG) and Chinese (CHI) are the two main
languages used for this work; they are commonly used
in parser evaluation and have previous examples of sta-
tistical parsers using a Bayesian framework. And since
we primarily are interested in parsing low-resource lan-
guages, we include results for Kinyarwanda (KIN) and
Malagasy (MLG) as examples of languages without
substantial existing treebanks. Finally, as a middle-
ground language, we use Italian (ITL).
For English, we use the Wall-Street Journal section
of the Penn Treebank (WSJ) (Marcus et al., 1993). The
data split is sections 02-21 for training, section 22 for
development, and section 23 for testing. For Chinese,
the Chinese Treebank (CTB5) (Xue et al., 2005) was
used. The data split is files 81-899 for training, files 41-
80 for development, and files 1-40/900-931 for testing.
The ITL data is from the Turin University Treebank
(TUT) (Bosco et al., 2000) and consists of 2,860 Italian
sentences from a variety of domains. It was split into
training, development, and test sets with a 70/15/15
percentage split.
The KIN texts are transcripts of testimonies by sur-
vivors of the Rwandan genocide provided by the Ki-
gali Genocide Memorial Center, along with a few BBC
news articles. The MLG texts are articles from the
websites Lakroa and La Gazette and Malagasy Global
Voices. Both datasets are described in Garrette and
Baldridge (2013). The KIN and MLG data is very
small compared to ENG and CHI: the KIN dataset con-
1
Code available at github.com/jmielens/gibbs-pcfg-2014,
along with instructions for replicating experiments when pos-
sible
2
As part of a standardized pre-processing step, we strip
functional tags, which makes a direct comparison to their re-
sults inappropriate.
294
tains 677 sentences, while the MLG dataset has only
113. Also, we simulated a small training set for ENG
data by using only section 02 of the WSJ for training.
4.2 Experimental Setup
As a preprocessing step, all trees are converted into
Chomsky Normal-Form such that all non-terminal pro-
ductions are binary and all unary chains are removed.
Additional standard normalization is performed.
Functional tags (e.g. the SBJ part of NP-SBJ), empty
nodes (traces), and indices are removed. Our binariza-
tion is simple: given a parent, select the rightmost child
as the head and add a stand-in node that contains the
remainder of the original children; the process then re-
curses. This simple technique uses no explicit head-
finding rules, which eases cross-linguistic applicability.
From this normalized data, we train latent PCFGs
with K=1,2,4,8,16,32 (where K=1 is equivalent to the
plain PCFG described in section 2).
4.3 Practical refinements
Unknown word handling. We use a similar unknown
word handling procedure to Liang et al. (2009). From
our raw corpus we extract features associated with ev-
ery word, these features include surrounding context
words as well as substring suffix/prefix features. Using
these features we produce fifty clusters using k-means.
Then, as a pre-parsing step, we replace all words oc-
curring less than five times with their cluster label -
this simulates unknown words for training. Finally,
during evaluation, any word not seen in training was
also replaced with its corresponding cluster label. This
final step is simple because there are no ?unknown un-
knowns? in our corpus, as the clustering has been per-
formed over the entire corpus prior to training. This
approach is similar to methods for unsupervised POS-
tag induction that also utilize clusters in this manner
(Dasgupta & Ng, 2007).
We compare this unknown word handling method to
one in which the clustering and a classifier is trained
not on the corpus under consideration, but rather on a
separate corpus of unrelated data. This comparison was
made to understand the effects of including the eval-
uation set in the training data (without labels) versus
training on out-of-domain texts. This is a more real-
istic measurement of out-of-the-box performance of a
trained model.
Jump-starting sampling. In the basic setup, train-
ing high K-value models takes a prohibitively long
time, so we also consider a jump-start technique that
allows larger annotation values (such as K=16) to be
run in less time. We train these high-K value models
first on a highly reduced training set (5% of the full
training set) for a large number of iterations, and then
use the found ? values as the starting point for training
on the full training set for a small number of iterations.
Although many of the estimated parameters on the re-
duced set will be zero, the prior allows us to eventually
System K=1 K=2 K=4 K=8 K=16
Unsmoothed PCFG 40.2 ? ? ? ?
Bikel Parser 57.9 ? ? ? ?
Liang et al. 07 60.5 71.1 77.2 79.2 78.2
Berkeley Parser 60.8 74.4 78.4 79.1 78.7
Gibbs PCFG 61.0 71.3 76.6 78.7 78.0
Table 1: F1 scores for small English training data ex-
periments. ?K? is the number of latent annotations ?
K=1 represents a vanilla, unannotated PCFG.
recover this information in the full set. This allows us
to train on the full training set, which is desirable rela-
tive to training on a reduced set, while still allowing the
model sufficient iterations to burn in. The fact that we
are likely starting in a fairly good position within the
search space (due to estimating ? from the corpus) also
likely helps enable these lower iteration counts.
5 Results
We start with Tables 1 and 2, which show performance
when training on section 02 of the WSJ (pretending En-
glish is a ?low-resource? language). The results show
that the basic Gibbs PCFG (where K=1), with an F-
score of 61.0, substantially outperforms not only an
unsmoothed PCFG (the simplest baseline), but also the
Bikel parser (Bikel, 2004b) trained on the same amount
of data. Table 1 also shows further large gains are
obtained from using latent annotations?from 60.5 for
K=1 to 78.7 for K=8.
The Gibbs PCFG also compares quite favorably to
the PCFG-LA of Liang et al. (2009)?slightly better
for K=1 and K=2 and slightly worse for K=4 and K=8.
Table 2 shows that the Gibbs PCFG is able to produce
results with a smaller amount of variance relative to
the Berkeley Parser, even at low training sizes. This
trend is repeated in Table 3, which shows that the Gibbs
PCFG also produces less variance when training on dif-
ferent single sections of the WSJ relative to the Berke-
ley Parser, although it again produces slightly lower F1
scores.
We also use the small English corpus to determine
the effects of weighting the prior when sampling anno-
tations, varying ? between 0.1 and 10.0. Though per-
formance is not sensitive to varying ? for larger cor-
pora, Figure 1 shows it can make a substantial differ-
ence for smaller corpora (with an optimal value was
obtained with an ? value of 5 for this small training
set). This seems to indicate that the lower counts asso-
ciated with the smaller training sets should be compen-
sated for by weighting those counts more heavily when
processing the evaluation set, as we had anticipated.
System WSJ Sec. 02 KIN MLG
Berkeley Parser 78.3 ? 0.93 60.6 ? 1.1 52.2 ? 2.0
Gibbs PCFG 76.7 ? 0.63 67.2 ? 0.92 57.5 ? 1.1
Table 2: F1 scores with standard deviation over ten runs
of small training data, K=4.
295
System F1 / StDev
Berkeley Parser 77.5 ? 2.1
Gibbs PCFG 77.0 ? 1.4
Table 3: F1 scores with standard deviations over twenty
runs, training on individual WSJ sections (02-21).
Figure 1: Accuracy by varying ? levels for small En-
glish data.
To evaluate the effectiveness of the jump-start tech-
nique, we ran the full ENG data set with K=4 to com-
pare the results from the full training setup to jump-
starting. For this, we performed 100 training iterations
on the reduced training set (WSJ section 02) and then
used the resulting ? values to seed training on the full
training set. Those training runs varied between three
and nine iterations, and the results are shown in Figure
2. The full ENG K=4 F-score is 86.2, so these results
represent a slight step back. Nonetheless, the technique
is still valuable in that it allows for inferring latent an-
notations for higher K-values than would typically be
available to us in a reasonable timeframe.
Table 4 shows the results for the main experiments.
Sampling a vanilla PCFG (K=1) produces results that
are not state-of-the-art, but still good overall and al-
ways better than an unsmoothed PCFG. The benefits of
the latent annotations are further shown in the increase
Condition ENG CHI ITA KIN MLG
Unsmoothed PCFG 69.9 66.8 62.1 45.9 49.2
Liang et al. 07 87.1 ? ? ? ?
Huang & Harper09 ? 84.1 ? ? ?
Bikel Parser 86.9 81.1 74.5 55.7 49.5
Berkeley Parser 90.1 83.4 71.6 61.4 51.8
Gibbs PCFG,K=1 79.3 75.4 66.3 58.5 55.1
Gibbs PCFG,K=2 82.6 79.8 69.3 65.0 57.0
Gibbs PCFG,K=4 86.0 82.3 71.9 67.2 57.8
Gibbs PCFG,K=16 87.2 83.2 72.4 68.1 58.2
Gibbs PCFG,K=32 87.4 83.4 71.0 66.8 55.3
Table 4: F1 scores for experiments on sampled PCFGs.
Note that Wang and Blunsom (2013) obtain an ENG F-
score of 77.9% using collapsed VB for K=2. Though
they do not give exact numbers, their Fig. 7 indicates
an F-score of about 87% for K=16.
Figure 2: F-Score for K=4, varying full-set training it-
erations (with and without 100x jump start).
of F1 score in all languages, as compared to the vanilla
PCFG. Experiments were run up to K=32 primarily due
to time constraint. Although previous literature results
report increases up to the equivalent of K=64, it may
be the case that higher K values with no merge step
more easily lead to overfitting in our model ? reduc-
ing the effectiveness of those high values, as shown by
the overall poorer performance on several languages at
K=32 when compared to K=16 as well as the general
levelling-off seen at the high K values.
For English and Chinese, the previous Bayesian
framework parsers outperform our own, but only by
around two points. Additionally, our parsing of Chi-
nese improves on the Bikel parser (trained on our train-
ing data) despite the fact that the Bikel parser makes
use of language specific optimizations. Our parser
needs no changes to switch languages.
The Gibbs PCFG with K=16 is superior to the strong
Bikel and Berkeley Parser benchmarks for both KIN
and MLG, a promising result for future work on pars-
ing low-resource languages in general. Note also that
our parser exhibits less variance than Berkeley Parser
especially for KIN and MLG, which supports the fact
that the variance of Berkeley Parser is higher for mod-
els with few subcategories (Petrov et al., 2006).
Examples of the improvement across latent annota-
tions for a given tree are shown in Figure 3. The details
of the noun phrase ?no major bond offerings? were the
same for each tree, and are thus abstracted here. The
low K-value tree (K=2) is shown in 3a, and primarily
suffers from issues related to the prepositional phrase,
?in Europe friday?. In particular, the low K-value tree
incorrectly groups ?Europe friday? as a noun phrase ob-
ject of ?in?.
The higher K-value tree (K=8) is shown in 3b.
This tree manages to correctly analyze the preposi-
tional phrase, accurately separately the temporal loca-
tive ?Friday? from the actual prepositional phrase of
?in Europe?. However, the high K-value tree makes a
296
Figure 3: Examples of tree progression in the Gibbs PCFG with a) K=2, b) K=8, and c) gold tree.
different mistake that the low K-value tree did not; it
groups ?no major bond offerings in Europe Friday? as a
noun phrase, when it should be three separate phrases
(two noun phrases and a prepositional phrase). This er-
ror may be related to the additional latent annotations.
With more available noun phrase subtypes, it may be
the case that a more unusual noun phrase could be per-
mitted that would have been too low probability with
only a few subtypes.
To determine whether the substantial range in F1
scores across languages are primarily the result of the
much larger training corpora available for certain lan-
guages, two extreme training set reduction experiments
were conducted. The training sets for all languages
were reduced to a total of either 100 or 500 sen-
tences. This process was repeated 10 times in a cross-
validation setup, where 10 separate sets of sentences
were selected for each language. The results of these
experiments are shown in Table 5.
We conclude that while data availability is a major
factor in the higher performance of English and Chi-
nese in our original experiments, it is not the only is-
sue. Clearly, either the linguistic facts of particular
languages or perhaps choices of formalism and annota-
tion conventions in the corpora make some of the lan-
guages more difficult to parse than others. The primary
questions is why Gibbs-PCFG is able to achieve higher
relative performance on the KIN/MLG datasets when
compared to the other parsers, and why this advantage
does not necessarily transfer to the extreme small-scale
versions of the ENG/CHI/ITL data. Preliminary inves-
tigation into the properties of the corpora have revealed
a number of potential answers. For instance, the POS
tagsets for KIN/MLG are substantially reduced com-
pared to the other corpora, and there are differences
in the branching factor of the native versions of the
corpora as well: a typical maximum branching fac-
tor for a tree in ENG/CHI/ITL is around 4-5, while
for KIN/MLG it is almost always 2 (natively binary).
Branching factors above 5 essentially never occur in
KIN/MLG, while they are not rare in ENG/CHI/ITL.
The question of exactly why the Gibbs-PCFG seems to
perform well on these corpora remains an open ques-
tion, but these differences could provide a starting point
Condition In-Domain Out-of-Domain
Full English (K=4) 86.0 83.3
Small English (K=4) 76.6 75.7
Kinyarwanda (K=4) 67.2 65.1
Malagasy (K=4) 57.8 55.4
Table 6: Effect of differing regimes for handling un-
known words.
for future analysis.
In addition to the actual F1 scores, the relative uni-
formity of the standard deviation results indicates that
the individual parsers are not that much different in
terms of their ability to provide consistent results at
these small data extremes, as opposed to the slightly
higher training levels where the Gibbs-PCFG generated
smaller variances.
Considering the effects of unknown word handling,
Table 6 shows that using the evaluation set when creat-
ing the unknown word classifier does improve overall
parsing accuracy when compared to an unknown word
handler that is trained on out-of-domain texts. This
shows that results reported in previous work somewhat
overstate the accuracy of these parsers when used in the
wild?which matters greatly in the low-resource set-
ting.
6 Conclusion
Our experiments demonstrate that sampling vanilla
PCFGs, as well as PCFGs with latent annotations, is
feasible with the use of a Gibbs sampler technique
and produces results that are in line with previous
parsers on controlled test sets. Our results also show
that our methods are effective on a wide variety of
languages?including two low-resource languages?
with no language-specific model modifications needed.
Additionally, although not a uniform winner, the
Gibbs-PCFG shows a propensity for performing well
on naturally small corpora (here, KIN/MLG). The ex-
act reason for this remains slightly unclear, but the
fact that a similar advantage is not found for extremely
small versions of large corpora indicates that our ap-
proach may be particularly well-suited for application
in real low-resource environments as opposed to a sim-
297
Parser Size ENG CHI ITL KIN MLG
Bikel 100 54.7 ? 2.2 51.4 ? 3.0 51 ? 2.4 47.1 ? 2.3 44.4 ? 2.0
Berkeley 100 55.2 ? 2.6 53.9 ? 2.9 50 ? 2.8 47.8 ? 2.1 44.5 ? 2.3
Gibbs-PCFG 100 54.5 ? 2.0 51.7 ? 2.4 49.5 ? 3.6 50.3 ? 2.3 45.8 ? 1.8
Bikel 500 56.2 ? 2.0 54.1 ? 2.7 54.2 ? 2.4 ? ?
Berkeley 500 58.9 ? 2.2 56.4 ? 2.7 52.5 ? 2.7 ? ?
Gibbs-PCFG 500 58.1 ? 2.0 55.7 ? 2.3 51.1 ? 3.2 ? ?
Table 5: 100/500 sentence training set results, including st.dev over 10 runs. KIN/MLG did not have enough data
to run the 500 sentence version.
ulated environment.
Having established this procedure and its relative tol-
erance for low amounts of data, we would like to extend
the model to make use of partial bracketing information
instead of complete trees, perhaps in the form of Frag-
mentary Unlabeled Dependency Grammar annotations
(Schneider et al., 2013). This would allow the sam-
pling procedure to potentially operate using corpora
with lighter annotations than full trees, making initial
annotation effort not quite as heavy and potentially in-
creasing the amount of available data for low-resource
languages. Additionally, using the expert partial anno-
tations to help restrict the sample space could provide
good gains in terms of training time.
Acknowledgments
Supported by the U.S. Army Research Office un-
der grant number W911NF-10-1-0533. Any opin-
ions, findings, and conclusions or recommendations
expressed in this material are those of the author(s) and
do not necessarily reflect the view of the U.S. Army
Research Office.
References
Anita Alicante, Cristina Bosco, Anna Corazza, and
Alberto Lavelli. 2012. A treebank-based study
on the influence of Italian word order on pars-
ing performance. In Nicoletta Calzolari (Con-
ference Chair), Khalid Choukri, Thierry Declerck,
Mehmet Uur Doan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of LREC?12, Istanbul, Turkey. European
Language Resources Association (ELRA).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The Grammar Matrix: An Open-Source
Starter-Kit for the Rapid Development of Cross-
Linguistically Consistent Broad-Coverage Precision
Grammars. In John Carroll, Nelleke Oostdijk, and
Richard Sutcliffe, editors, Proceedings of the Work-
shop on Grammar Engineering and Evaluation at
the 19th International Conference on Computational
Linguistics, pages 8?14, Taipei, Taiwan.
Dan Bikel. 2004a. On The Parameter Space of Gener-
ative Lexicalized Statistical Parsing Models. Ph.D.
thesis, University of Pennsylvania.
Daniel M Bikel. 2004b. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Ezra Black, Fred Jelinek, John Lafferty, David M
Magerman, Robert Mercer, and Salim Roukos.
1992. Towards history-based grammars: Using
richer models for probabilistic parsing. In Proceed-
ings of the workshop on Speech and Natural Lan-
guage, pages 134?139. Association for Computa-
tional Linguistics.
Taylor L Booth and Richard A Thompson. 1973. Ap-
plying probability measures to abstract languages.
Computers, IEEE Transactions on, 100(5):442?450.
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a Treebank
for Italian: a Data-driven Annotation Schema. In In
Proceedings of the Second International Conference
on Language Resources and Evaluation LREC-2000
(pp. 99, pages 99?105.
Glenn Carroll and Eugene Charniak. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. Department of Computer Sci-
ence, Univ.
Eugene Charniak. 1996. Tree-bank grammars. In Pro-
ceedings of the National Conference on Artificial In-
telligence, pages 1031?1036.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st North
American chapter of the Association for Computa-
tional Linguistics conference, pages 132?139. Asso-
ciation for Computational Linguistics.
Noam Chomsky. 1956. Three models for the descrip-
tion of language. Information Theory, IRE Transac-
tions on, 2(3):113?124.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2012. Spectral learning
of latent-variable PCFGs. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
223?231. Association for Computational Linguis-
tics.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Pro-
ceedings of NAACL-HLT, pages 148?157.
298
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th annual meeting on Association for
Computational Linguistics, pages 184?191. Associ-
ation for Computational Linguistics.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Compu-
tational Linguistics, pages 16?23. Association for
Computational Linguistics.
Michael Collins. 2003. Head-driven statistical mod-
els for natural language parsing. Computational lin-
guistics, 29(4):589?637.
Jenny Rose Finkel, Christopher D Manning, and An-
drew Y Ng. 2006. Solving the problem of cascading
errors: Approximate Bayesian inference for linguis-
tic annotation pipelines. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 618?626. Association for
Computational Linguistics.
Dan Garrette and Jason Baldridge. 2013. Learning a
Part-of-Speech Tagger from Two Hours of Annota-
tion. In Proceedings of NAACL, Atlanta, Georgia.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-World Semi-Supervised Learning of
POS-Taggers for Low-Resource Languages. In Pro-
ceedings of the 51th annual meeting on Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, (6):721?741.
Joshua T Goodman. 1998. Parsing Inside-Out.
Ph.D. thesis, Harvard University Cambridge, Mas-
sachusetts.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2-Volume 2, pages 832?841.
Association for Computational Linguistics.
Rebecca Hwa, Philip Resnik, and Amy Weinberg.
Breaking the Resource Bottleneck for Multilingual
Parsing. In The Proceedings of the Workshop on Lin-
guistic Knowledge Acquisition and Representation:
Bootstrapping Annotated Language Data. Confer-
ence on Language Resources and Evaluation.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov Chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Jonas Kuhn. 2004a. Applying computational linguis-
tic techniques in a documentary project for Qanjobal
(Mayan, Guatemala). In In Proceedings of LREC
2004. Citeseer.
Jonas Kuhn. 2004b. Experiments in parallel-text based
grammar induction. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 470. Association for Computational
Linguistics.
Karim Lary and Steve J Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algrithm. Computer, Speech and Language,
4:35?56.
Percy Liang, Michael I Jordan, and Dan Klein. 2009.
Probabilistic grammars and hierarchical Dirichlet
processes. The handbook of applied Bayesian anal-
ysis.
David M Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, pages 276?283. Association for Computa-
tional Linguistics.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. COMPUTA-
TIONAL LINGUISTICS, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2005. Probabilistic CFG with latent annotations.
In Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 75?82.
Association for Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th annual meeting
on Association for Computational Linguistics, pages
128?135. Association for Computational Linguis-
tics.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In HLT-NAACL, pages
404?411.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable pars-
ing. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
867?876. Association for Computational Linguis-
tics.
299
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple Unsupervised Grammar Induction from Raw
Text with Cascaded Finite State Models. In ACL,
pages 1077?1086.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A Smith,
Chris Dyer, and Jason Baldridge. 2013. A
framework for (under) specifying dependency syn-
tax without overloading annotators. arXiv preprint
arXiv:1306.2091.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers-
Volume 1, pages 440?448. Association for Computa-
tional Linguistics.
Matthew A Taddy. 2011. On estimation and selection
for topic models. arXiv preprint arXiv:1109.4518.
Pengyu Wang and Phil Blunsom. 2013. Collapsed
Variational Bayesian Inference for PCFGs. In Pro-
ceedings of the Seventeenth Conference on Com-
putational Natural Language Learning, pages 173?
182, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Nat. Lang.
Eng., 11(2):207?238, June.
300
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 336?348,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Hierarchical Discriminative Classification for Text-Based Geolocation
Benjamin Wing
?
Jason Baldridge
?
?
Department of Linguistics, University of Texas at Austin
ben@benwing.com, jbaldrid@utexas.edu
Abstract
Text-based document geolocation is com-
monly rooted in language-based infor-
mation retrieval techniques over geodesic
grids. These methods ignore the natural
hierarchy of cells in such grids and fall
afoul of independence assumptions. We
demonstrate the effectiveness of using lo-
gistic regression models on a hierarchy of
nodes in the grid, which improves upon
the state of the art accuracy by several
percent and reduces mean error distances
by hundreds of kilometers on data from
Twitter, Wikipedia, and Flickr. We also
show that logistic regression performs fea-
ture selection effectively, assigning high
weights to geocentric terms.
1 Introduction
Document geolocation is the identification of the
location?a specific latitude and longitude?that
forms the primary focus of a given document. This
assumes that a document can be adequately associ-
ated with a single location, which is only valid for
certain documents, generally of fairly small size.
Nonetheless, there are many natural situations in
which such collections arise. For example, a great
number of articles in Wikipedia have been man-
ually geotagged; this allows those articles to ap-
pear in their geographic locations in geobrowsers
like Google Earth. Images in social networks such
as Flickr may be geotagged by a camera and their
textual tags can be treated as documents. Like-
wise, tweets in Twitter are often geotagged; in this
case, it is possible to view either an individual
tweet or the collection of tweets for a given user
as a document, respectively identifying the loca-
tion as the place from which the tweet was sent or
the home location of the user.
Early work on document geolocation used
heuristic algorithms, predicting locations based on
toponyms in the text (named locations, determined
with the aid of a gazetteer) (Ding et al., 2000;
Smith and Crane, 2001). More recently, vari-
ous researchers have used topic models for doc-
ument geolocation (Ahmed et al., 2013; Hong et
al., 2012; Eisenstein et al., 2011; Eisenstein et
al., 2010) or other types of geographic document
summarization (Mehrotra et al., 2013; Adams and
Janowicz, 2012; Hao et al., 2010). A number of
researchers have used metadata of various sorts
for document or user geolocation, including doc-
ument links and social network connections. This
research has sometimes been applied to Wikipedia
(Overell, 2009) or Facebook (Backstrom et al.,
2010) but more commonly to Twitter, focusing
variously on friends and followers (McGee et al.,
2013; Sadilek et al., 2012), time zone (Mahmud et
al., 2012), declared location (Hecht et al., 2011),
or a combination of these (Schulz et al., 2013).
We tackle document geolocation using super-
vised methods based on the textual content of
documents, ignoring their metadata. Metadata-
based approaches can achieve great accuracy (e.g.
Schulz et al. (2013) obtain 79% accuracy within
100 miles for a US-based Twitter corpus, com-
pared with 49% using our methods on a compa-
rable corpus), but are very specific to the partic-
ular corpus and the types of metadata it makes
available. For Twitter, the metadata includes the
user?s declared location and time zone, infor-
mation which greatly simplifies geolocation and
which is unavailable for other types of corpora,
such as Wikipedia. In many cases essentially no
metadata is available at all, as in historical corpora
in the digital humanities (Lunenfeld et al., 2012),
such as those in the Perseus project (Crane, 2012).
Text-based approaches can be applied to all types
of corpora; metadata can be additionally incorpo-
rated when available (Han and Cook, 2013).
We introduce a hierarchical discriminative clas-
sification method for text-based geotagging. We
336
apply this to corpora in three languages (English,
German and Portuguese). This method scales
well to large training sets and greatly improves
results across a wide variety of corpora, beat-
ing current state-of-the-art results by wide mar-
gins, including Twitter users (Han et al., 2014,
henceforth Han14; Roller et al., 2012, henceforth
Roller12); Wikipedia articles (Roller12; Wing and
Baldridge, 2011, henceforth WB11); and Flickr
images (O?Hare and Murdock, 2013, henceforth
OM13). Importantly, this is the first method that
improves upon straight uniform-grid Naive Bayes
on all of these corpora, in contrast with k-d trees
(Roller12) and the current state-of-the-art tech-
nique for Twitter users of geographically-salient
feature selection (Han14).
We also show, contrary to Han14, that logistic
regression when properly optimized is more ac-
curate than state-of-the-art techniques, including
feature selection, and fast enough to run on large
corpora. Logistic regression itself very effectively
picks out words with high geographic significance.
In addition, because logistic regression does not
assume feature independence, complex and over-
lapping features of various sorts can be employed.
2 Data
We work with six large datasets: two of geotagged
tweets, three of Wikipedia articles, and one of
Flickr photos. One of the two Twitter datasets is
primarily localized to the United States, while the
remaining datasets cover the whole world.
TWUS is a dataset of tweets compiled by
Roller12. A document in this dataset is the con-
catenation of all tweets by a single user, as long
as at least one of the user?s tweets is geotagged
with specific, GPS-assigned latitude/longitude co-
ordinates. The earliest such tweet determines the
user?s location. Tweets outside of a bounding box
covering the contiguous United States (including
parts of Canada and Mexico) were discarded, as
well as users that may be spammers or robots
(based on the number of followers, followees and
tweets). The resulting dataset contains 38M tweets
from 450K users, of which 10,000 each are re-
served for the development and test sets.
TWWORLD is a dataset of tweets compiled by
Han et al. (2012). It was collected in a simi-
lar fashion to TWUS but differs in that it covers
the entire Earth instead of primarily the United
States, and consists only of geotagged tweets.
Non-English tweets and those not near a city were
removed, and non-alphabetic, overly short and
overly infrequent words were filtered. The result-
ing dataset consists of 1.4M users, with 10,000
each reserved for the development and test sets.
ENWIKI13 is a dataset consisting of the 864K
geotagged articles (out of 14M articles in all) in
the November 4, 2013 English Wikipedia dump.
It is comparable to the dataset used in WB11 and
was processed using an analogous fashion. The
articles were randomly split 80/10/10 into training,
development and test sets.
DEWIKI14 is a similar dataset consisting of the
324K geotagged articles (out of 1.71M articles in
all) in the July 5, 2014 German Wikipedia dump.
PTWIKI14 is a similar dataset consisting of the
131K geotagged articles (out of 817K articles in
all) in the June 24, 2014 Portuguese Wikipedia
dump.
COPHIR (Bolettieri et al., 2009) is a large
dataset of images from the photo-sharing social
network Flickr. It consists of 106M images, of
which 8.7M are geotagged. Most images contain
user-provided tags describing them. We follow al-
gorithms described in OM13 in order to make di-
rect comparison possible. This involves removing
photos with empty tag sets and performing bulk
upload filtering, retaining only one of a set of pho-
tos from a given user with identical tag sets. The
resulting reduced set of 2.8M images is then di-
vided 80/10/10 into training, development and test
sets. The tag set of each photo is concatenated into
a single piece of text (in the process losing user-
supplied tag boundary information in the case of
multi-word tags).
Our code and processed corpora are available
for download.
1
3 Supervised models for document
geolocation
The dominant approach for text-based geolocation
comes from language modeling approaches in in-
formation retrieval (Ponte and Croft, 1998; Man-
ning et al., 2008). For this general strategy, the
Earth is sub-divided into a grid, and then each
training set document is associated with the cell
that contains it. Some model (typically Naive
Bayes) is then used to characterize each cell and
1
https://github.com/utcompling/
textgrounder/wiki/WingBaldridge_
EMNLP2014
337
enable new documents to be assigned a latitude
and longitude based on those characterizations.
There are several options for constructing the grid
and for modeling, which we review next.
3.1 Geodesic grids
The simplest grid is a uniform rectangular one
with cells of equal-sized degrees, which was used
by Serdyukov et al. (2009) for Flickr images and
WB11 for Twitter and Wikipedia. This has two
problems. Compared to a grid that takes document
density into account, it over-represents rural areas
at the expense of urban areas. Furthermore, the
rectangles are not equal-area, but shrink in width
away from the equator (although the shrinkage is
mild until near the poles). Roller12 tackle the for-
mer issue by using an adaptive grid based on k-d
trees, while Dias et al. (2012) handle the latter is-
sue with an equal-area quaternary triangular mesh.
An additional issue with geodesic grids is that
a single metro area may be divided between two
or more cells. This can introduce a statistical
bias known as the modifiable areal unit problem
(Gehlke and Biehl, 1934; Openshaw, 1983). One
way to mitigate this, implemented in Roller12?s
code but not investigated in their paper, is to di-
vide a cell in a k-d tree in such a way as to pro-
duce the maximum margin between the dividing
line and the nearest document on each side.
A more direct method is to use a city-based rep-
resentation, either with a full set of sufficiently-
sized cities covering the Earth and taken from
a comprehensive gazetteer (Han14) or a limited,
pre-specified set of cities (Kinsella et al., 2011;
Sadilek et al., 2012). Han14 amalgamate cities
into nearby larger cities within the same state (or
equivalent); an even more direct method would
use census-tract boundaries when available. Dis-
advantages of these methods are the dependency
on time-specific population data, making them un-
suitable for some corpora (e.g. 19th-century doc-
uments); the difficulty in adjusting grid resolution
in a principled fashion; and the fact that not all
documents are near a city (Han14 find that 8% of
tweets are ?rural? and cannot predicted by their
model).
We construct rectangular grids, since they are
very easy to implement and Dias et al. (2012)?s
triangular mesh did not yield consistently better
results over Wikipedia. We use both uniform grids
and k-d tree grids with midpoint splitting.
3.2 Naive Bayes
A geodesic grid of sufficient granularity creates a
large decision space, when each cell is viewed as
a label to be predicted by some classifier. This
situation naturally lends itself to simple, scalable
language-modeling approaches. For this general
strategy, each cell is characterized by a pseudo-
document constructed from the training docu-
ments that it contains. A test document?s location
is then chosen based on the cell with the most sim-
ilar language model according to standard mea-
sures such as Kullback-Leibler (KL) divergence
(Zhai and Lafferty, 2001), which seeks the cell
whose language model is closest to the test doc-
ument?s, or Naive Bayes (Lewis, 1998), which
chooses the cell that assigns the highest probabil-
ity to the test document.
Han14, Roller12 and WB11 follow this strat-
egy, using KL divergence in preference to Naive
Bayes. However, we find that Naive Bayes in con-
junction with Dirichlet smoothing (Smucker and
Allan, 2006) works at least as well when appropri-
ately tuned. Dirichlet smoothing is a type of dis-
counting model that interpolates between the un-
smoothed (maximum-likelihood) document distri-
bution
?
?
d
i
of a document d
i
and the unsmoothed
distribution
?
?
D
over all documents. A general
interpolation model for the smoothed distribution
?
d
i
has the following form:
P (w|?
d
i
) = (1? ?
d
i
)P (w|
?
?
d
i
) + ?
d
i
P (w|
?
?
D
) (1)
where the discount factor ?
d
i
indicates how much
probability mass to reserve for unseen words. For
Dirichlet smoothing, ?
d
i
is set as:
?
d
i
= 1?
|d
i
|
|d
i
|+m
(2)
where |d
i
| is the size of the document and m is
a tunable parameter. This has the effect of re-
lying more on d
i
?s distribution and less on the
global distribution for larger documents that pro-
vide more evidence than shorter ones. Naive
Bayes models are estimated easily, which allows
them to handle fine-scale grid resolutions with po-
tentially thousands or even hundreds of thousands
of non-empty cells to choose among.
Figure 1 shows a choropleth map of the behav-
ior of Naive Bayes, plotting the rank of cells for
338
Figure 1: Relative Naive Bayes rank of cells for
ENWIKI13 test document Pennsylvania Avenue
(Washington, DC), surrounding the true location.
the test document Pennsylvania Avenue (Washing-
ton, DC) in ENWIKI13, for a uniform 0.1
?
grid.
The top-ranked cell is the correct one.
3.3 Logistic regression
The use of discrete cells over the Earth?s sur-
face allows any classification strategy to be em-
ployed, including discriminative classifiers such as
logistic regression. Logistic regression often pro-
duces produces better results than generative clas-
sifiers at the cost of more time-consuming train-
ing, which limits the size of the problems it may
be applied to. Training is generally unable to scale
to encompass several thousand or more distinct la-
bels, as is the case with fine-scale grids of the sort
we may employ. Nonetheless we find flat logis-
tic regression to be effective on most of our large-
scale corpora, and the hierarchical classification
strategy discussed in ?4 allows us to take advan-
tage of logistic regression without incurring such
a high training cost.
3.4 Feature selection
Naive Bayes assumes that features are indepen-
dent, which penalizes models that must accom-
modate many features that are poor indicators and
which can gang up on the good features. Large
improvements have been obtained by reducing
the set of words used as features to those that
are geographically salient. Cheng et al. (2010;
2013) model word locality using a unimodal dis-
tribution taken from Backstrom et al. (2008) and
train a classifier to identify geographically lo-
cal words based on this distribution. This un-
fortunately requires a large hand-annotated cor-
pus for training. Han14 systematically investi-
gate various feature selection methods for find-
ing geo-indicative words, such as information gain
ratio (IGR) (Quinlan, 1993), Ripley?s K statis-
tic (O?Sullivan and Unwin, 2010) and geographic
density (Chang et al., 2012), showing significant
improvements on TWUS and TWWORLD (?2).
For comparison with Han14, we test against
an additional baseline: Naive Bayes combined
with feature selection done using IGR. Following
Han14, we first eliminate words which occur less
than 10 times, have non-alphabetic characters in
them or are shorter than 3 characters. We then
compute the IGR for the remaining words across
all cells at a given cell size or bucket size, select
the top N% for some cutoff percentage N (which
we vary in increments of 2%), and then run Naive
Bayes at the same cell size or bucket size.
4 Hierarchical classification
To overcome the limitations of discriminative clas-
sifiers in terms of the maximum number of cells
they can handle, we introduce hierarchical classifi-
cation (Silla Jr. and Freitas, 2011) for geolocation.
Dias et al. (2012) use a simple two-level genera-
tive hierarchical approach using Naive Bayes, but
to our knowledge no previous work implements a
multi-level discriminative hierarchical model with
beam search for geolocation.
To construct the hierarchy, we start with a root
cell c
root
that spans the entire Earth and from there
build a tree of cells at different scales, from coarse
to fine. A cell at a given level is subdivided to
create smaller cells at the next level of resolution
that altogether cover the same area as their parent.
We use the local classifier per parent approach
to hierarchical classification (Silla Jr. and Fre-
itas, 2011) in which an independent classifier is
learned for every node of the hierarchy above the
leaf nodes. The probability of any node in the hi-
erarchy is the product of the probabilities of that
node and all of its ancestors, up to the root. This
is defined recursively as:
P (c
root
) = 1.0
P (c
j
) = P (c
j
|?c
j
)P (?c
j
)
(3)
where ?c
j
indicates c
j
?s parent in the hierarchy.
In addition to allowing one to use many classi-
fiers that each have a manageable number of out-
comes, the hierarchical approach naturally lends
itself to beam search. Rather than computing the
339
probability of every leaf cell using equation 3, we
use a stratified beam search: starting at the root
cell, keep the b highest-probability cells at each
level until reaching the leaf node level. With a
tight beam?which we show to be very effective?
this dramatically reduces the number of model
evaluations that must be performed at test time.
Grid size parameters Two factors determine
the size of the grids at each level. The first-level
grid is constructed the same as for Naive Bayes
or flat logistic regression and is controlled by its
own parameter. In addition, the subdivision factor
N determines how we subdivide each cell to get
from one level to the next. Both factors must be
optimized appropriately.
For the uniform grid, we subdivide each cell
intoNxN subcells. In practice, there may actually
be fewer subcells, because some of the potential
subcells may be empty (contain no documents).
For the k-d grid, if level 1 is created using a
bucket size B (i.e. we recursively divide cells as
long as their size exceeds B), then level 2 is cre-
ated by continuing to recursively divide cells that
exceed a smaller bucket size B/N . At this point,
the subcells of a given level-1 cell are the leaf cells
contained with the cell?s geographic area. The
construction of level 3 proceeds similarly using
bucket size B/N
2
, etc.
Note that the subdivision factor has a different
meaning for uniform and k-d tree grids. Further-
more, because creating the subdividing cells for a
given cell involves dividing by N
2
for the uniform
grid but N for the k-d tree grid, greater subdivi-
sion factors are generally required for the k-d tree
grid to achieve similar-scale resolution.
Figure 2 shows the behavior of hierarchical LR
using k-d trees for the test document Pennsylva-
nia Avenue (Washington, DC) in ENWIKI13. Af-
ter ranking the first level, the beam zooms in on
the top-ranked cells and constructs a finer k-d tree
under each one (one such subtree is shown in the
top-right map callout).
5 Experimental Setup
Configurations. We experiment with several
methods for configuring the grid and selecting the
best cell. For grids, we use either a uniform or
k-d tree grid. For uniform grids, the main tunable
parameter is grid size (in degrees), while for k-d
trees it is bucket size (BK), i.e. the number of doc-
uments above which a node is divided in two.
Figure 2: Relative hierarchical LR rank of cells
for ENWIKI13 test document Pennsylvania Av-
enue (Washington, DC), surrounding the true lo-
cation. The first callout simply expands a portion
of level 1, while the second callout shows a level
1 cell subdivided down to level 2.
For cell choice, the options are:
? NB: Naive Bayes baseline
? IGR: Naive Bayes using features selected by
information gain ratio
? FlatLR: logistic regression model over all
leaf nodes
? HierLR: product of logistic regression mod-
els at each node in a hierarchical grid (eq. 3)
For Dirichlet smoothing in conjunction with Naive
Bayes, we set the Dirichlet parameter m =
1, 000, 000, which we found worked well in pre-
liminary experiments. For hierarchical classifica-
tion, there are additional parameters: subdivision
factor (SF) and beam size (BM) (?4), and hierar-
chy depth (D) (?6.4). All of our test-set results use
a depth of three levels.
Due to its speed and flexibility, we use Vowpal
Wabbit (Agarwal et al., 2014) for logistic regres-
sion, estimating parameters with limited-memory
BFGS (Nocedal, 1980; Byrd et al., 1995). Unless
otherwise mentioned, we use 26-bit feature hash-
ing (Weinberger et al., 2009) and 40 passes over
the data (optimized based on early experiments on
development data) and turn off the hold-out mech-
anism. For the subcell classifiers in hierarchical
classification, which have fewer classes and much
less data, we use 24-bit features and 12 passes.
Evaluation. To measure geolocation perfor-
mance, we use three standard metrics based on er-
ror distance, i.e. the distance between the correct
location and the predicted location. These metrics
are mean and median error distance (Eisenstein et
340
al., 2010) and accuracy at 161 km (acc@161), i.e.
within a 161-km radius, which was introduced by
Cheng et al. (2010) as a proxy for accuracy within
a metro area. All of these metrics are indepen-
dent of cell size, unlike the measure of cell accu-
racy (fraction of cells correctly predicted) used in
Serdyukov et al. (2009). Following Han14, we use
acc@161 on development sets when choosing al-
gorithmic parameter values such as cell and bucket
sizes.
6 Results
6.1 Twitter
We show the effect of varying cell size in Table 1
and k-d tree bucket size in Figure 3. The number
of non-empty cells is shown for each cell size and
bucket size. For NB, this is the number of cells
against which a comparison must be made for each
test document; for FlatLR, this is the number of
classes that must be distinguished. For HierLR, no
figure is given because it varies from level to level
and from classifier to classifier. For example, with
a uniform grid and subdivision factor of 3, each
level-2 subclassifier will have between 1 and 9 la-
bels to choose among, depending on which cells
are empty.
Method
Cell Size #Class Acc. Mean Med.
(Deg) (km) @161 (km) (km)
NB
0.17
?
11,671 36.6 929.5 496.4
0.50
?
2,838 35.4 889.3 466.6
IGR, CU90% 1.5
?
501 45.9 787.5 255.6
FlatLR
5
?
556 59 35.4 727.8 248.7
4
?
445 99 44.4 718.8 227.9
3
?
334 159 47.3 721.3 186.2
2.5
?
278 208 47.5 743.9 198.9
2
?
223 316 46.9 737.7 209.9
1.5
?
167 501 46.6 762.6 226.9
1
?
111 975 43.0 810.0 303.7
HierLR, D2, SF2, BM5 4
?
? ? 48.6 695.2 182.2
HierLR, D2, SF2, BM2 3
?
? ? 49.0 725.1 174.6
HierLR, D3, SF2, BM2 3
?
? ? 49.0 718.9 173.8
HierLR, D2, SF2, BM5 2.5
?
? ? 48.2 740.9 187.7
Table 1: Dev set performance for TWUS, with
uniform grids. HierLR and IGR parameters op-
timized using acc@161. Best metric numbers for
a given method are underlined, except that overall
best numbers are in bold.
FlatLR does much better than NB and IGR, and
HierLR is still better. This is despite logistic re-
gression needing to operate at a much lower res-
olution.
2
Interestingly, uniform-grid 2-level Hi-
erLR does better at 4
?
with a subdivision factor
2
The limiting factor for resolution for us was the 24-hour
per job limit on our computing cluster.
l
l
35
40
45
0 2500 5000 7500 10000Bucket size
acc
@16
1 (pc
t)
method
l HierLR
FlatLR
IGR
NB
Figure 3: Dev set performance for TWUS, with
k-d tree grids.
of 2 than the equivalent FlatLR run at 2
?
.
Table 2 shows the test set results for the vari-
ous methods and metrics described in ?5, on both
TWUS and TWWORLD.
3
HierLR is the best
across all metrics; the best acc@161km and me-
dian error is obtained with a uniform grid, while
HierLR with k-d trees obtains the best mean error.
Compared with vanilla NB, our implementa-
tion of NB using IGR feature selection obtains
large gains for TWUS and moderate gains for
TWWORLD, showing that IGR can be an effec-
tive geolocation method for Twitter. This agrees
in general with Han14?s findings. We can only
compare our figures directly with Han14 for k-d
trees?in this case they use a version of the same
software we use and report figures within 1% of
ours for TWUS. Their remaining results are com-
puted using a city-based grid and an NB imple-
mentation with add-one smoothing, and are signif-
icantly worse than our uniform-grid NB and IGR
figures using Dirichlet smoothing, which is known
to significantly outperform add-one smoothing
(Smucker and Allan, 2006). For example, for NB
they report 30.8% acc@161 for TWUS and 20.0%
for TWWORLD, compared with our 36.2% and
30.2% respectively. We suspect an additional rea-
son for the discrepancy is due to the limitations of
their city-based grid, which has no tunable param-
eter to optimize the grid size and requires that test
instances not near a city be reported as incorrect.
Our NB figures also beat the KL divergence fig-
ures reported in Roller12 for TWUS (which they
term UTGEO2011), perhaps again due to the dif-
3
Note that for TWWORLD, it was necessary to modify
the parameters normally passed to Vowpal Wabbit, moving
up to 27-bit features and 96 passes, and 24-bit features with
24 passes in sublevels of HierLR.
341
Corpus TWUS TWWORLD
Method Parameters A@161 Mean Med. Parameters A@161 Mean Med.
NB Uniform 0.17
?
36.2 913.8 476.3 1
?
30.2 1690.0 537.2
NB k-d BK1500 36.2 861.4 444.2 BK500 28.7 1735.0 566.2
IGR Uniform 1.5
?
, CU90% 46.1 770.3 233.9 1
?
, CU90% 31.0 2204.8 574.7
IGR k-d BK2500, CU90% 44.6 792.0 268.6 BK250, CU92% 29.4 2369.6 655.0
FlatLR Uniform 2.5
?
47.2 727.3 195.4 3.7
?
32.1 1736.3 500.0
FlatLR k-d BK4000 47.4 692.2 197.0 BK12000 27.8 1939.5 651.6
HierLR Uniform 3
?
, SF2, BM2 49.2 703.6 170.5 5
?
, SF2, BM1 32.7 1714.6 490.0
HierLR k-d BK4000, SF3, BM1 48.0 686.6 191.4 BK60000, SF5, BM1 31.3 1669.6 509.1
Table 2: Performance on the test sets of TWUS and TWWORLD for different methods and metrics.
ference in smoothing methods.
6.2 Wikipedia
Table 3 shows results on the test set of ENWIKI13
for various methods. Table 5 shows the corre-
sponding results for DEWIKI14 and PTWIKI14.
In all cases, the best parameters for each method
were determined using acc@161 on the develop-
ment set, as above.
l
l
l
l
l
l
l
l
l
lll
lll
lll l
l l l
l
l
l
l
l
82
84
86
88
0 50 100 150 200 250K?d subdivision factor
Acc
@16
1 (pc
t) beam size
l 1
2
Naive Bayes
Figure 4: Plot of subdivision factor vs. acc@161
for the ENWIKI13 dev set with 2-level k-d tree
HierLR, bucket size 1500. Beam sizes above 2
yield little improvement.
HierLR is clearly the stand-out winner among
all methods and metrics, and particularly so for the
k-d tree grid. This is achieved through a high sub-
division factor, especially in a 2-level hierarchy,
where a factor of 36 is best, as shown in Figure 4
for ENWIKI13. (For a 3-level hierarchy, the best
subdivision factor is 12.)
Unlike for TWUS, FlatLR simply cannot com-
Method Param #Class A@161 Med. Runtime
FlatLR
Uniform
10
?
648 19.2 314.1 11h
8.5
?
784 26.5 248.5 16h
7.5
?
933 30.1 232.0 19h
FlatLR
k-d
BK5000 257 57.1 133.5 5h
BK2500 501 67.5 94.9 9h
BK1500 825 74.7 69.9 16h
HierLR
Uniform
7.5
?
,SF2,BM1 ? 85.2 67.8 23h
7.5
?
,SF3,BM5 ? 86.1 34.2 27h
HierLR
k-d
BK1500,SF5,BM1 ? 88.2 19.6 23h
BK5000,SF10,BM5 ? 88.4 18.3 14h
BK1500,SF12,BM2 ? 88.8 15.3 33h
Table 4: Performance/runtime for FlatLR and 3-
level HierLR on the ENWIKI13 dev set, with vary-
ing parameters.
pete with NB in the larger Wikipedias (ENWIKI13
and DEWIKI14). ENWIKI13 especially has dense
coverage across the entire world, whereas TWUS
only covers the United States and parts of Canada
and Mexico. Thus, there are a much larger num-
ber of non-empty cells at a given resolution and
much coarser resolution required, especially with
the uniform grid. For example, at 7.5
?
there are
933 non-empty cells, comparable to 1
?
for TWUS.
Table 4 shows the number of classes and runtime
for FlatLR and HierLR at different parameter val-
ues. The hierarchical classification approach is
clearly essential for allowing us to scale the dis-
criminative approach for a large, dense dataset
across the whole world.
Moving from larger to smaller Wikipedias,
FlatLR becomes more competitive. In particular,
FlatLR outperforms NB and is close to HierLR for
PTWIKI14, the smallest of the three (and signifi-
cantly smaller than TWUS). In this case, the rel-
atively small size of the dataset and its greater ge-
ographic specificity (many articles are located in
Brazil or Portugal) allows for a fine enough reso-
lution to make FlatLR perform well?comparable
to or even finer than NB.
In all of the Wikipedias, NB k-d outperforms
342
Corpus ENWIKI13 COPHIR
Method Parameters A@161 Mean Med. Parameters A@161 Mean Med.
NB Uniform 1.5
?
84.0 326.8 56.3 1.5
?
65.0 1553.5 47.9
NB k-d BK100 84.5 362.3 21.1 BK3500 58.5 1726.9 70.0
IGR Uniform 1.5
?
, CU96% 81.4 401.9 58.2 1.5
?
, CU92% 60.8 1683.4 56.7
IGR k-d BK250, CU98% 80.6 423.9 34.3 BK1500, CU62% 54.7 2908.8 83.5
FlatLR Uniform 7.5
?
25.5 1347.8 259.4 2.0
?
60.6 1942.3 73.7
FlatLR k-d BK1500 74.8 253.2 70.0 BK3000 57.7 1961.4 72.5
HierLR Uniform 7.5
?
, SF3, BM5 86.2 228.3 34.0 7
?
, SF4, BM5 65.3 1590.2 16.7
HierLR k-d BK1500, SF12, BM2 88.9 168.7 15.3 BK100000, SF15, BM5 66.0 1453.3 17.9
Table 3: Performance on the test sets of ENWIKI13 and COPHIR for different methods and metrics.
NB uniform, and HierLR outperforms both, but
by greatly varying amounts, with only a 1% differ-
ence for DEWIKI14 but 12% for PTWIKI14. It?s
unclear what causes these variations, although it?s
worth noting that Roller12?s NB k-d figures on an
older English Wikipedia corpus were are notice-
ably higher than our figures: They report 90.3%
acc@161, compared with our 84.5%. We verified
that this is due to corpus differences: we obtain
their performance when we run on their Wikipedia
corpus. This suggests that the various differences
may be due to vagaries of the individual corpora,
e.g. the presence of differing numbers of geo-
tagged stub articles, which are very short and thus
hard to geolocate.
As for IGR, though it is competitive for Twitter,
it performs badly here?in fact, it is even worse
than plain Naive Bayes for all three Wikipedias
(likewise for COPHIR, in the next section).
6.3 CoPhIR
Table 3 shows results on the test set of COPHIR
for various methods, similarly to the ENWIKI13
results. HierLR is again the clear winner. Unlike
for ENWIKI13, FlatLR is able to do fairly well.
IGR performs poorly, especially when combined
with k-d.
In general, as can be seen, for COPHIR the
median figures are very low but the mean figures
very high, meaning there are many images that can
be very accurately placed while the remainder are
very difficult to place. (The former images likely
have the location mentioned in the tags, while the
latter do not.)
For COPHIR, and also TWWORLD, HierLR
performs best when the root level is significantly
coarser than the cell or bucket size that is best for
FlatLR. The best setting for the root level appears
to be correlated with cell accuracy, which in gen-
eral increases with larger cell sizes. The intuition
here is that HierLR works by drilling down from
a single top-level child of the root cell. Thus, the
higher the cell accuracy, the greater the fraction
of test instances that can be improved in this fash-
ion, and in general the better the ultimate values
of the main metrics. (The above discussion isn?t
strictly true for beam sizes above 1, but these tend
to produce marginal improvements, with little if
any gain from going above a beam size of 5.) The
large size of a coarse root-child cell, and corre-
spondingly poor results for acc@161, can be off-
set by a high subdivision factor, which does not
materially slow down the training process.
Our NB results are not directly comparable with
OM13?s results on COPHIR because they use var-
ious cell-based accuracy metrics while we use
cell-size-independent metrics. The closest to our
acc@161 metric is their Ac1 metric, which at a
cell size of 100 km corresponds to a 300km-per-
side square at the equator, roughly comparable to
our 161-km-radius circle. They report Ac1 figures
of 57.7% for term frequency and 65.3% for user
frequency, which counts the number of distinct
users in a cell using a given term and is intended to
offset bias resulting from users who upload a large
batch of similar photos at a given location. Our
term frequency figure of 65.0% significantly beats
theirs, but we found that user frequency actually
degraded our dev set results by 5%. The reason
for this discrepancy is unclear.
6.4 Parameterization variations
Optimizing for median. Note that better values
for the other metrics, especially median, can be
achieved by specifically optimizing for these met-
rics. In general, the best parameters for median
are finer-scale than those for acc@161: smaller
grid sizes and bucket sizes, and greater subdivision
factors. This is especially revealing in ENWIKI13
and COPHIR. For example, on the ENWIKI13
343
Corpus DEWIKI14 PTWIKI14
Method Parameters A@161 Mean Med. Parameters A@161 Mean Med.
NB Uniform 1
?
88.4 257.9 35.0 1
?
76.6 470.0 48.3
NB k-d BK25 89.3 192.0 7.6 BK100 77.1 325.0 45.9
IGR Uniform 2
?
, CU82% 87.1 312.9 68.2 2
?
, CU54% 71.3 594.6 89.4
IGR k-d BK50, CU100% 86.0 226.8 10.9 BK100, CU100% 71.3 491.9 57.7
FlatLR Uniform 5
?
55.1 340.4 150.1 2
?
88.9 320.0 70.8
FlatLR k-d BK350 82.0 193.2 24.5 BK25 86.8 320.8 30.0
HierLR Uniform 7
?
, SF3, BM5 88.5 184.8 30.0 7
?
, SF2, BM5 88.6 223.5 64.7
HierLR k-d BK3500, SF25, BM5 90.2 122.5 8.6 BK250, SF12, BM2 89.5 186.6 27.2
Table 5: Performance on the test sets of DEWIKI14 and PTWIKI14 for different methods and metrics.
dev set, the ?best? uniform NB parameter of 1.5
?
,
as optimized on acc@161, yields a median error
of 56.1 km, but an error of just 16.7 km can be
achieved with the parameter setting 0.25
?
(which,
however, drops acc@161 from 83.8% to 78.3%
in the process). Similarly, for the COPHIR dev
set, the optimized uniform 2-level HierLR median
error of 46.6 km can be reduced to just 8.1 km
by dropping from 7
?
to 3.5
?
and bumping up the
subdivision factor from 4 to 35?again, causing a
drop in acc@161 from 68.6% to 65.5%.
Hierarchy depth. We use a 3-level hierarchy
throughout for the test set results. Evaluation on
development data showed that 2-level hierarchies
perform comparably for several data sets, but are
less effective overall. We did not find improve-
ments from using more than three levels. When
using a simple local classifier per parent approach
as we do, which chains together spines of related
but independently trained classifiers when assign-
ing a probability to a leaf cell, most of the ben-
efit presumably comes from simply enabling lo-
gistic regression to be used with fine-grained leaf
cells, overcoming the limitations of FlatLR. Fur-
ther benefits of the hierarchical approach might be
achieved with the data-biasing and bottom-up er-
ror propagation techniques of Bennett and Nguyen
(2009) or the hierarchical Bayesian approach of
Gopal et al. (2012), which is able to handle large-
scale corpora and thousands of classes.
6.5 Feature Selection
The main focus of Han14 is identifying geograph-
ically salient words through feature selection. Lo-
gistic regression performs feature selection natu-
rally by assigning higher weights to features that
better discriminate among the target classes.
Table 6 shows the top 20 features ranked by fea-
ture weight for a number of different cells, labeled
by the largest city in the cell. The features were
produced using a uniform 5
?
grid, trained using
27-bit features and 40 passes over TWUS. The
high number of bits per feature were chosen to en-
sure as few collisions as possible of different fea-
tures (as it would be impossible to distinguish two
words that were hashed together).
Most words are clearly region specific, con-
sisting of cities, states and abbreviations, sports
teams (broncos, texans, niners, saints), well-
known streets (bourbon, folsom), characteristic
features (desert, bayou, earthquake, temple), local
brands (whataburger, soopers, heb), local foods
(gumbo, poutine), and dialect terms (hella, buku).
Top-IGR words Bottom-IGR words
lockerby presswiches plan times
killdeer haubrich party end
fordville yabbo men twitter
azilda presswich happy full
ahauah pozuelo show part
hutmacher akeley top forget
cere chewelah extra close
miramichi computacionales late dead
alamosa bevilacqua facebook cool
multiservicios presswiche friday enjoy
ghibran curtisinn black true
briaroaks guymon dream found
joekins dakotamart hey drink
numerica missoula face pay
bemidji mimbres finally meet
amn shingobee easy lost
roug gottsch time find
pbtisd uprr live touch
marcenado hesperus wow birthday
banerjee racingmason yesterday ago
Table 7: Top and bottom 40 features selected using
IGR for TWUS with a uniform 1.5
?
grid.
As a comparison, Table 7 shows the top and bot-
tom 40 features selected using IGR on the same
corpus. Unlike for logistic regression, the top IGR
features are mostly obscure words, only some of
344
Salt Lake San Francisco New Orleans Phoenix Denver Houston Montreal Seattle Tulsa Los Angeles
utah sacramento orleans tucson denver houston montreal seattle tulsa knotts
slc hella jtfo az colorado antonio mtl portland okc sd
salt sac prelaw phoenix broncos texans quebec tacoma oklahoma pasadena
byu niners saints arizona aurora sa magrib wa wichita diego
provo berkeley louisiana asu amarillo corpus rue vancouver ou ucla
ut safeway bourbon tempe soopers whataburger habs bellevue kansas disneyland
utes oakland kmsl scottsdale colfax heb canadian oregon ku irvine
idaho earthquake uptown phx springs otc ouest seahawks lawrence socal
orem sf joked chandler centennial utsa mcgill pdx shaki tijuana
sandy modesto wya fry pueblo mcallen coin uw ks riverside
rio exploit canal glendale larimer westheimer gmusic puyallup edmond pomona
ogden stockton metairie desert meadows pearland laval safeway osu turnt
lds hayward westbank harkins parker jammin poutine huskies stillwater angeles
temple cal bayou camelback blake mayne boul everett topeka usc
murray jose houma mesa cherry katy est seatac sooners chargers
menudito swaaaaggg lawd gilbert siiiiim jamming je ducks straighht oc
mormon folsom gtf pima coors tsu sherbrooke victoria kc compton
gateway roseville magazine dbacks englewood marcos pas beaverton manhattan meadowview
megaplex juiced gumbo mcdowell pikes laredo fkn hella boomer rancho
lake vallejo buku devils rockies texas centre sounders sooner ventura
Table 6: Top 20 features selected for various regions using logistic regression on TWUS with a uniform
5
?
grid.
which have geographic significance, while the bot-
tom words are quite common. To some extent this
is a feature of IGR, since it divides by the binary
entropy of each word, which is directly related
to its frequency. However, it shows why cutoffs
around 90% of the original feature set are neces-
sary to achieve good performance on the Twitter
corpora. (IGR does not perform well on Wikipedia
or COPHIR, as shown above.)
7 Conclusion
This paper demonstrates that major performance
improvements to geolocation based only on text
can be obtained by using a hierarchy of logistic
regression classifiers. Logistic regression also al-
lows for the use of complex, interdependent fea-
tures, beyond the simple unigram models com-
monly employed. Our preliminary experiments
did not show noticeable improvements from bi-
gram or character-based features, but it is pos-
sible that higher-level features such as morpho-
logical, part-of-speech or syntactic features could
yield further performance gains. And, of course,
these improved text-based models may help de-
crease error even further when metadata (e.g. time
zone and declared location) are available.
An interesting extension of this work is to rely
upon the natural clustering of related documents.
Joint modeling of geographic topics and loca-
tions has been attempted (see ?1), but has gener-
ally been applied to much smaller corpora than
those considered here. Skiles (2012) found sig-
nificant improvements by clustering the training
documents of large-scale corpora using K-means,
training separate models from each cluster, and es-
timating a test document?s location with the clus-
ter model returning the best overall similarity (e.g.
through KL divergence). Bergsma et al. (2013)
likewise cluster tweets using K-means but predict
location only at the country level. Such methods
could be combined with hierarchical classification
to yield further gains.
Acknowledgments
We would like to thank Grant Delozier for as-
sistance in generating choropleth graphs, and the
three anonymous reviewers for their feedback.
This research was supported by a grant from the
Morris Memorial Trust Fund of the New York
Community Trust.
References
Benjamin Adams and Krzysztof Janowicz. 2012. On
the geo-indicativeness of non-georeferenced text. In
John G. Breslin, Nicole B. Ellison, James G. Shana-
han, and Zeynep Tufekci, editors, ICWSM?12: Pro-
ceedings of the 6th International AAAI Conference
on Weblogs and Social Media. The AAAI Press.
Alekh Agarwal, Oliveier Chapelle, Miroslav Dud??k,
and John Langford. 2014. A reliable effective teras-
cale linear learning system. Journal of Machine
Learning Research, 15:1111?1133.
Amr Ahmed, Liangjie Hong, and Alexander J. Smola.
2013. Hierarchical geographical modeling of user
345
locations from social media posts. In Proceedings
of the 22nd International Conference on World Wide
Web, WWW ?13, pages 25?36, Republic and Canton
of Geneva, Switzerland. International World Wide
Web Conferences Steering Committee.
Lars Backstrom, Jon Kleinberg, Ravi Kumar, and Jas-
mine Novak. 2008. Spatial variation in search en-
gine queries. In Proceedings of the 17th Interna-
tional Conference on World Wide Web, WWW ?08,
pages 357?366, New York, NY, USA. ACM.
Lars Backstrom, Eric Sun, and Cameron Marlow.
2010. Find me if you can: improving geographi-
cal prediction with social and spatial proximity. In
Proceedings of the 19th international conference on
World wide web, WWW ?10, pages 61?70, New
York, NY, USA. ACM.
Paul N. Bennett and Nam Nguyen. 2009. Refined ex-
perts: improving classification in large taxonomies.
In James Allan, Javed A. Aslam, Mark Sanderson,
ChengXiang Zhai, and Justin Zobel, editors, SIGIR,
pages 11?18. ACM.
Shane Bergsma, Mark Dredze, Benjamin Van Durme,
Theresa Wilson, and David Yarowsky. 2013.
Broadly improving user classification via
communication-based name and location clus-
tering on twitter. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1010?1019, Atlanta,
Georgia, June. Association for Computational
Linguistics.
Paolo Bolettieri, Andrea Esuli, Fabrizio Falchi, Clau-
dio Lucchese, Raffaele Perego, Tommaso Piccioli,
and Fausto Rabitti. 2009. Cophir: a test col-
lection for content-based image retrieval. CoRR,
abs/0905.4627.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and
Ciyou Zhu. 1995. A limited memory algorithm for
bound constrained optimization. SIAM Journal on
Scientific Computing, 16(5):1190?1208.
Hau-Wen Chang, Dongwon Lee, Mohammed Eltaher,
and Jeongkyu Lee. 2012. @phillies tweeting from
philly? predicting twitter user locations with spatial
word usage. In Proceedings of the 2012 Interna-
tional Conference on Advances in Social Networks
Analysis and Mining (ASONAM 2012), pages 111?
118. IEEE Computer Society.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2010. You are where you tweet: A content-based ap-
proach to geo-locating twitter users. In Proceedings
of the 19th ACM International Conference on In-
formation and Knowledge Management, pages 759?
768.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee.
2013. A content-driven framework for geolocating
microblog users. ACM Trans. Intell. Syst. Technol.,
4(1):2:1?2:27, February.
Gregory Crane, 2012. The Perseus Project, pages 644?
653. SAGE Publications, Inc.
Duarte Dias, Ivo Anast?acio, and Bruno Martins. 2012.
A Language Modeling Approach for Georeferenc-
ing Textual Documents. In Proceedings of the Span-
ish Conference in Information Retrieval.
Junyan Ding, Luis Gravano, and Narayanan Shivaku-
mar. 2000. Computing geographical scopes of web
resources. In Proceedings of the 26th International
Conference on Very Large Data Bases, VLDB ?00,
pages 545?556, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1277?1287,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Jacon Eisenstein, Ahmed Ahmed, and Eric P. Xing.
2011. Sparse additive generative models of text. In
Proceedings of the 28th International Conference on
Machine Learning, pages 1041?1048.
Charles E. Gehlke and Katherine Biehl. 1934. Certain
effects of grouping upon the size of the correlation
coefficient in census tract material. Journal of the
American Statistical Association, 29(185):169?170.
Siddharth Gopal, Yiming Yang, Bing Bai, and Alexan-
dru Niculescu-Mizil. 2012. Bayesian models for
large-scale hierarchical classification. In Peter L.
Bartlett, Fernando C. N. Pereira, Christopher J. C.
Burges, Lon Bottou, and Kilian Q. Weinberger, edi-
tors, NIPS, pages 2420?2428.
Bo Han and Paul Cook. 2013. A stacking-based ap-
proach to twitter user geolocation prediction. In In
Proceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013):
System Demonstrations, pages 7?12.
Bo Han, Paul Cook, and Tim Baldwin. 2012. Geoloca-
tion prediction in social media data by finding loca-
tion indicative words. In International Conference
on Computational Linguistics (COLING), page 17,
Mumbai, India, December.
Bo Han, Paul Cook, and Tim Baldwin. 2014. Text-
based twitter user geolocation prediction. Journal
of Artificial Intelligence Research, 49(1):451?500.
Qiang Hao, Rui Cai, Changhu Wang, Rong Xiao,
Jiang-Ming Yang, Yanwei Pang, and Lei Zhang.
2010. Equip tourists with knowledge mined from
travelogues. In Proceedings of the 19th interna-
tional conference on World wide web, WWW ?10,
pages 401?410, New York, NY, USA. ACM.
Brent Hecht, Lichan Hong, Bongwon Suh, and Ed H.
Chi. 2011. Tweets from justin bieber?s heart: The
dynamics of the location field in user profiles. In
346
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ?11, pages 237?
246, New York, NY, USA. ACM.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the twit-
ter stream. In Proceedings of the 21st International
Conference on World Wide Web, WWW ?12, pages
769?778, New York, NY, USA. ACM.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in Glasgow?: Mod-
eling locations with tweets. In Proceedings of the
3rd International Workshop on Search and Mining
User-generated Contents, pages 61?68.
David D. Lewis. 1998. Naive (bayes) at forty: The in-
dependence assumption in information retrieval. In
Proceedings of the 10th European Conference on
Machine Learning, ECML ?98, pages 4?15, Lon-
don, UK, UK. Springer-Verlag.
Peter Lunenfeld, Anne Burdick, Johanna Drucker,
Todd Presner, and Jeffrey Schnapp. 2012. Digital
humanities. MIT Press, Cambridge, MA.
Jalal Mahmud, Jeffrey Nichols, and Clemens Drews.
2012. Where is this tweet from? inferring home
locations of twitter users. In John G. Breslin,
Nicole B. Ellison, James G. Shanahan, and Zeynep
Tufekci, editors, ICWSM?12: Proceedings of the 6th
International AAAI Conference on Weblogs and So-
cial Media. The AAAI Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK.
Jeffrey McGee, James Caverlee, and Zhiyuan Cheng.
2013. Location prediction in social media based on
tie strength. In Proceedings of the 22nd ACM In-
ternational Conference on Conference on Informa-
tion and Knowledge Management, CIKM ?13, pages
459?468, New York, NY, USA. ACM.
Rishabh Mehrotra, Scott Sanner, Wray Buntine, and
Lexing Xie. 2013. Improving lda topic models for
microblogs via tweet pooling and automatic label-
ing. In Proceedings of the 36th International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?13, pages 889?892,
New York, NY, USA. ACM.
Jorge Nocedal. 1980. Updating Quasi-Newton Matri-
ces with Limited Storage. Mathematics of Compu-
tation, 35(151):773?782.
Neil O?Hare and Vanessa Murdock. 2013. Modeling
locations with social media. Information Retrieval,
16(1):30?62.
Stan Openshaw. 1983. The modifiable areal unit prob-
lem. Geo Books.
David O?Sullivan and David J. Unwin, 2010. Point
Pattern Analysis, pages 121?155. John Wiley &
Sons, Inc.
Simon Overell. 2009. Geographic Information Re-
trieval: Classification, Disambiguation and Mod-
elling. Ph.D. thesis, Imperial College London.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SI-
GIR conference on Research and development in
information retrieval, SIGIR ?98, pages 275?281,
New York, NY, USA. ACM.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 1500?
1510, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proceedings of the 5th ACM Inter-
national Conference on Web Search and Data Min-
ing, pages 723?732.
Axel Schulz, Aristotelis Hadjakos, Heiko Paulheim,
Johannes Nachtwey, and Max M?uhlh?auser. 2013.
A multi-indicator approach for geolocalization of
tweets. In Emre Kiciman, Nicole B. Ellison, Bernie
Hogan, Paul Resnick, and Ian Soboroff, editors,
ICWSM?13: Proceedings of the 7th International
AAAI Conference on Weblogs and Social Media. The
AAAI Press.
Pavel Serdyukov, Vanessa Murdock, and Roelof van
Zwol. 2009. Placing flickr photos on a map. In
Proceedings of the 32nd international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?09, pages 484?491, New
York, NY, USA. ACM.
Carlos N. Silla Jr. and Alex A. Freitas. 2011. A survey
of hierarchical classification across different appli-
cation domains. Data Mining and Knowledge Dis-
covery, 22(1-2):182?196, January.
Erik David Skiles. 2012. Document geolocation using
language models built from lexical and geographic
similarity. Master?s thesis, University of Texas at
Austin.
David A. Smith and Gregory Crane. 2001. Disam-
biguating geographic names in a historical digital
library. In Proceedings of the 5th European Con-
ference on Research and Advanced Technology for
Digital Libraries, ECDL ?01, pages 127?136, Lon-
don, UK. Springer-Verlag.
347
Mark D. Smucker and James Allan. 2006. An inves-
tigation of Dirichlet prior smoothing?s performance
advantage. Technical report, University of Mas-
sachusetts, Amherst.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In Pro-
ceedings of the 26th Annual International Confer-
ence on Machine Learning, ICML ?09, pages 1113?
1120, New York, NY, USA. ACM.
Benjamin Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 955?964, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management, CIKM ?01, pages 403?410, New
York, NY, USA. ACM.
348
Proceedings of NAACL-HLT 2013, pages 138?147,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning a Part-of-Speech Tagger from Two Hours of Annotation
Dan Garrette
Department of Computer Science
The University of Texas at Austin
dhg@cs.utexas.edu
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@utexas.edu
Abstract
Most work on weakly-supervised learning for
part-of-speech taggers has been based on un-
realistic assumptions about the amount and
quality of training data. For this paper, we
attempt to create true low-resource scenarios
by allowing a linguist just two hours to anno-
tate data and evaluating on the languages Kin-
yarwanda and Malagasy. Given these severely
limited amounts of either type supervision
(tag dictionaries) or token supervision (labeled
sentences), we are able to dramatically im-
prove the learning of a hidden Markov model
through our method of automatically general-
izing the annotations, reducing noise, and in-
ducing word-tag frequency information.
1 Introduction
The high performance achieved by part-of-speech
(POS) taggers trained on plentiful amounts of la-
beled word tokens is a success story of computa-
tional linguistics (Manning, 2011). However, re-
search on learning taggers using type supervision
(e.g. tag dictionaries or morphological transducers)
has had a more checkered history. The setting is
a seductive one: by labeling the possible parts-of-
speech for high frequency words, one might learn
accurate taggers by incorporating the type informa-
tion as constraints to a semi-supervised generative
learning model like a hidden Markov model (HMM).
Early work showed much promise for this strategy
(Kupiec, 1992; Merialdo, 1994), but successive ef-
forts in recent years have continued to peel away and
address layers of unrealistic assumptions about the
size, coverage, and quality of the tag dictionaries
that had been used (Toutanova and Johnson, 2008;
Ravi and Knight, 2009; Hasan and Ng, 2009; Gar-
rette and Baldridge, 2012). This paper attempts to
strip away further layers so we can build better intu-
itions about the effectiveness of type-supervised and
token-supervised strategies in a realistic setting of
POS-tagging for low-resource languages.
In most previous work, tag dictionaries are ex-
tracted from a corpus of annotated tokens. To ex-
plore the type-supervised scenario, these have been
used as a proxy for dictionaries produced by lin-
guists. However, this overstates their effectiveness.
Researchers have often manually pruned tag dictio-
naries by removing low-frequency word/tag pairs;
this violates the assumption that frequency informa-
tion is not available. Others have also created tag
dictionaries by extracting every word/tag pair in a
large, labeled corpus, including the test data?even
though actual applications would never have such
complete lexical knowledge. Dictionaries extracted
from corpora are also biased towards including only
the most likely tag for each word type, resulting in
a cleaner dictionary than one would find in real sce-
nario. Finally, tag dictionaries extracted from anno-
tated tokens benefit from the annotation process of
labeling and review and refinement over an extended
collaboration period. Such high quality annotations
are simply not available for most low-resource lan-
guages.
This paper describes an approach to learning
a POS-tagger that can be applied in a truly low-
resource scenario. Specifically, we discuss tech-
niques that allow us to learn a tagger given only
138
the amount of labeled data that a human annotator
could provide in two hours. Here, we evaluate on
the languages Malagasy and Kinyarwanda, as well
as English as a control language. Furthermore, we
are interested in whether type-supervision or token-
supervision is more effective, given the strict time
constraint; accordingly, we had annotators produce
both a tag dictionary and a set of labeled sentences.
The data produced under our conditions differs in
several ways from the labeled data used in previous
work. Most obviously, there is less of it. Instead
of using hundreds of thousands of labeled tokens
to construct a tag dictionary (and hundreds of thou-
sands more as unlabeled (raw) data for training), we
only use the 1k-2k labeled tokens or types provided
by our annotators within the timeframe. Our train-
ing data is also much noisier than the data from a
typical corpus: the annotations were produced by
a single non-native-speaker working alone for two
hours. Therefore, dealing with the size and quality
of training data were core challenges to our task.
To learn a POS-tagger from so little labeled data,
we developed an approach that starts by generalizing
the initial annotations to the entire raw corpus. Our
approach uses label propagation (LP) (Talukdar and
Crammer, 2009) to infer tag distributions on unla-
beled tokens. We then apply a novel weighted vari-
ant of the model minimization procedure originally
developed by Ravi and Knight (2009) to estimate se-
quence and word-tag frequency information from an
unlabeled corpus by approximating the minimal set
of tag bigrams needed to explain the data. This com-
bination of techniques turns a tiny, unweighted, ini-
tial tag dictionary into a weighted tag dictionary that
covers the entire corpus?s vocabulary. This weighted
information limits the potential damage of tag dic-
tionary noise and bootstraps frequency information
to approximate a good starting point for the learning
of an HMM using expectation-maximization (EM),
and far outperforms just using EM on the raw an-
notations themselves.
2 Data
Our experiments use Kinyarwanda (KIN), Malagasy
(MLG), and English (ENG). KIN is a Niger-Congo
language spoken in Rwanda. MLG is an Austrone-
sian language spoken in Madagascar. Both KIN and
MLG are low-resource and KIN is morphologically-
rich. For each language, the word tokens are divided
into four sets: training data to be labeled by anno-
tators, raw training data, development data, and test
data. For consistency, we use 100k raw tokens for
each language.
Data sources For ENG, we used the Penn Tree-
bank (PTB) (Marcus et al, 1993). Sections 00-04
were used as raw data, 05-14 as a dev set, and 15-24
(473K tokens) as a test set. The PTB uses 45 dis-
tinct POS tags. The KIN texts are transcripts of testi-
monies by survivors of the Rwandan genocide pro-
vided by the Kigali Genocide Memorial Center. The
MLG texts are articles from the websites1 Lakroa and
La Gazette and Malagasy Global Voices,2 a citizen
journalism site.3 Texts in both KIN and MLG were
tokenized and labeled with POS tags by two linguis-
tics graduate students, each of which was studying
one of the languages. The KIN and MLG data have
14 and 24 distinct POS tags, respectively, and were
developed by the annotators.
Time-bounded annotation One of our main goals
is to evaluate POS-tagging for low-resource lan-
guages in experiments that correspond better to a
real-world scenario than previous work. As such, we
collected two forms of annotation, each constrained
by a two-hour time limit. The annotations were done
by the same linguists who had annotated the KIN
and MLG data mentioned above. Our experiments
are thus relevant to the reasonable context in which
one has access to a linguist who is familiar with the
target language and a given set of POS tags.
The first annotation task was to directly produce a
dictionary of words to their possible POS tags?i.e.,
collecting an actual tag dictionary of the form that is
typically simulated in POS-tagging experiments. For
each language, we compiled a list of word types, or-
dered starting with most frequent, and presented it
to the annotator with a list of admissible POS tags.
The annotator had two hours to specify POS tags for
as many words as possible. The word types and fre-
quencies used for this task were taken from the raw
training data and did not include the test sets. This
1www.lakroa.mg and www.lagazette-dgi.com
2mg.globalvoicesonline.org/
3The public-domain data is available at github.com/
dhgarrette/low-resource-pos-tagging-2013
139
data is used for what will call type-supervised train-
ing. The second task was annotating full sentences
with POS tags, again for two hours. We refer to this
as token-supervised training.
Having both sets of annotations allows us to in-
vestigate the relative value of each with respect to
training taggers. Token-supervision provides valu-
able frequency and tag context information, but
type-supervision produces larger dictionaries. This
can be seen in Table 1, where the dictionary size
column in the table gives the number of unique
word/tag pairs derived from the data.
We also wanted to directly compare the two an-
notators to see how the differences in their relative
annotation speeds and quality would affect the over-
all ability to learn an accurate tagger. We thus had
them complete the same two tasks for English. As
can be seen in Table 1, there are clear differences
between the two annotators. Most notably, annota-
tor B was faster at annotating full sentences while
annotator A was faster at annotating word types.
3 Approach
Our approach to learning POS-taggers is based on
Garrette and Baldridge (2012), which properly sep-
arated test data from learning data, unlike much pre-
vious work. The input to our system is a raw cor-
pus and either a human-generated tag dictionary or
human-tagged sentences. The majority of the sys-
tem is the same for both kinds of labeled training
data, but the following description will point out dif-
ferences. The system has four main parts, in order:
1. Tag dictionary expansion
2. Weighted model minimization
3. Expectation maximization (EM) HMM training
4. MaxEnt Markov Model (MEMM) training
3.1 Tag dictionary expansion
In a low-resource setting, most word types will not
be found in the initial tag dictionary. EM-HMM train-
ing uses the tag dictionary to limit ambiguity, so a
sparse tag dictionary is problematic because it does
not sufficiently confine the parameter space.4 Small
4This is of course not the case for purely unsupervised tag-
gers, though we also note that it is not at all clear they are actu-
ally learning taggers for part-of-speech.
sent. tok. dict.
KIN human sentences A 90 1537 750
KIN human TD A 1798
MLG human sentences B 92 1805 666
MLG human TD B 1067
ENG human sentences A 86 1897 903
ENG human TD A 1644
ENG human sentences B 107 2650 959
ENG human TD B 1090
Table 1: Statistics for Kinyarwanda, Malagasy, and
English data annotated by annotators A and B.
dictionaries also interact poorly with the model min-
imization of Ravi et al (2010): if there are too many
unknown words, and every tag must be considered
for them, then the minimal model will simply be the
one that assumes that they all have the same tag.
For these reasons, we automatically expand an
initial small dictionary into one that has coverage for
most of the vocabulary. We use label propagation
(LP)?specifically, the Modified Adsorption (MAD)
algorithm (Talukdar and Crammer, 2009)5?which
is a graph-based technique for spreading labels be-
tween related items. Our graphs connect token
nodes to each other via feature nodes and are seeded
with POS-tag labels from the human-annotated data.
Defining the LP graph Our LP graph has several
types of nodes, as shown in Figure 1. The graph
contains a TOKEN node for each token of the la-
beled corpus (when available) and raw corpus. Each
word type has one TYPE node that is connected to
its TOKEN nodes. Both kinds of nodes are con-
nected with feature nodes. The PREVWORD x and
NEXTWORD x nodes represent the features of a to-
ken being preceded by or followed by word type x in
the corpus. These bigram features capture extremely
simple syntactic information. To capture shallow
morphological relatedness, we use prefix and suffix
nodes that connect word types that share prefix or
suffix character sequences up to length 5. For each
node-feature pair, the connecting edge is weighted
as 1/N where N is the number of nodes connected
to the particular feature.
5The open-source MAD implementation is provided through
Junto: github.com/parthatalukdar/junto
140
TOKEN A 1 1 TOKEN walks 2 3
SUFFIX1 e
TOKEN barks 1 3
SUFFIX1 s
PREVWORD dog
SUFFIX2 he
TYPE A
TOKEN The 2 1 TOKEN walks 3 3TOKEN The 3 1
PREVWORD manNEXTWORD .
TYPE barksTYPE The
SUFFIX2 ksDICTPOS D
NEXTWORD dog
DICTPOS N DICTPOS V
TYPE walks
NEXTWORD manPREVWORD ?b?
Figure 1: Subsets of the LP graph showing regions of connected nodes. Graph represents the sentences ?A
dog barks .?, ?The dog walks .?, and ?The man walks .?
We also explored the effectiveness of using an ex-
ternal dictionary in the graph since this is one of the
few available sources of information for many low-
resource languages. Though a standard dictionary
probably will not use the same POS tag set that we
are targeting, it nevertheless provides information
about the relatedness of various word types. Thus,
we use nodes DICTPOS p that indicate that a particu-
lar word type is listed as having POS p in the dictio-
nary. Crucially, these tags bear no particular con-
nection to the tags we are predicting: we still tar-
get the tags defined by the linguist who annotated
the types or tokens used, which may be more or
less granular than those provided in the dictionary.
As external dictionaries, we use English Wiktionary
(614k entries), malagasyworld.org (78k entries),
and kinyarwanda.net (3.7k entries).6
Seeding the graph is straightforward. With token-
supervision, labels for tokens are injected into the
corresponding TOKEN nodes with a weight of 1.0.
In the type-supervised case, any TYPE node that ap-
pears in the tag dictionary is injected with a uniform
distribution over the tags in its tag dictionary entry.
Toutanova and Johnson (2008) (also, Ravi and
Knight (2009)) use a simple method for predict-
ing possible tags for unknown words: a set of 100
most common suffixes are extracted and then mod-
els of P(tag|suffix) are built and applied to unknown
words. However, these models suffer with an ex-
tremely small set of labeled data. Our method uses
character affix feature nodes along with sequence
feature nodes in the LP graph to get distributions
over unknown words. Our technique thus subsumes
6Wiktionary (wiktionary.org) has only 3,365 en-
tries for Malagasy and 9 for Kinyarwanda.
theirs as it can infer tag dictionary entries for words
whose suffixes do not show up in the labeled data (or
with enough frequency to be reliable predictors).
Extracting a result from LP LP assigns a label
distribution to every node. Importantly, each indi-
vidual TOKEN gets its own distribution instead of
sharing an aggregation over the entire word type.
From this graph, we extract a new version of the
raw corpus that contains tags for each token. This
provides the input for model minimization.
We seek a small set of likely tags for each token,
but LP gives each token a distribution over the entire
set of tags. Most of the tags are simply noise, some
of which we remove by normalizing the weights and
excluding tags with probability less than 0.1. Af-
ter applying this cutoff, the weights of the remain-
ing tags are re-normalized. We stress that this tag
dictionary cutoff is not like those used in past re-
search, which were done with respect to frequen-
cies obtained from labeled tokens: we use either no
word-tag frequency information (type-supervision)
or very small amounts of word-tag frequency infor-
mation indirectly through LP (token-supervision).7
Some tokens might not have any associated tag
labels after LP. This occurs when there is no
path from a TOKEN node to any seeded nodes or
when all tags for the TOKEN node have weights less
than the threshold. Since we require a distribution
for every token, we use a default distribution for
such cases. Specifically, we use the unsupervised
emission probability initialization of Garrette and
Baldridge (2012), which captures both the estimated
frequency of a tag and its openness using only a
7See Banko and Moore (2004) for further discussion of these
issues.
141
?b? The man saw the saw ?b?
?b?
D
N
V
1.0
1.0
1.0 0.2
0.8
1.0
0.7
0.3
1.0
Figure 2: Weighted, greedy model minimization
graph showing a potential state between the stages
of the tag bigram choosing algorithm. Solid edges:
selected bigrams. Dotted edges: holes in the path.
small tag dictionary and unlabeled text.
Finally, we ensure that tokens of words in the
original tag dictionary are only assigned tags from
its entry. With this filter, LP of course does not add
new tags to known words (without it, we found per-
formance drops). If the intersection of the small tag
dictionary entry and the token?s resulting distribu-
tion from LP (after thresholding) is empty, we fall
back to the filtered and renormalized default distri-
bution for that token?s type.
The result of this process is a sequence of (ini-
tially raw) tokens, each associated with a distribu-
tion over a subset of tags. From this we can extract
an expanded tag dictionary for use in subsequent
stages that, crucially, provides tag information for
words not covered by the human-supplied tag dic-
tionary. This expansion is simple: an unknown word
type?s set of tags is the union of all tags assigned to
its tokens. Additionally, we add the full entries of
word types given in the original tag dictionary.
3.2 Weighted model minimization
EM-HMM training depends crucially on having a
clean tag dictionary and a good starting point for the
emission distributions. Given only raw text and a
tag dictionary, these distributions are difficult to es-
timate, especially in the presence of a very sparse
or noisy tag dictionary. Ravi and Knight (2009) use
model minimization to remove tag dictionary noise
and induce tag frequency information from raw text.
Their method works by finding a minimal set of tag
bigrams needed to explain a raw corpus.
Model minimization is a natural fit for our system
since we start with little or no frequency informa-
tion and automatic dictionary expansion introduces
noise. We extend the greedy model minimization
procedure of Ravi et al (2010), and its enhance-
ments by Garrette and Baldridge (2012), to develop
a novel weighted minimization procedure that uses
the tag weights from LP to find a minimal model
that is biased toward keeping tag bigrams that have
consistently high weights across the entire corpus.
The new weighted minimization procedure fits well
in our pipeline by allowing us to carry the tag dis-
tributions forward from LP instead of simply throw-
ing that information away and using a traditional tag
dictionary.
In brief, the procedure works by creating a graph
such that each possible tag of each raw-corpus token
is a vertex (see Figure 2). Any edge that would con-
nect two tags of adjacent tokens is a potential tag bi-
gram choice. The algorithm first selects tag bigrams
until every token is covered by at least one bigram,
then selects tag bigrams that fill gaps between exist-
ing edges until there is a complete bigram path for
every sentence in the raw corpus.8
Ravi et al (2010) select tag bigrams that cover
the most new words (stage 1) or fill the most holes
in the tag paths (stage 2). Garrette and Baldridge
(2012) introduced the tie-breaking criterion that bi-
gram choices should seek to introduce the small-
est number of new word/tag pairs possible into the
paths. Our criteria adds to this by using the tag
weights on each token: a tag bigram b is chosen by
summing up the node weights of any not-yet cov-
ered words touched by the tag bigram b, dividing
this sum by one plus the number of new word/tag
pairs that would be added by b, and choosing the b
that maximizes this value.9
Summing node weights captures the intuition of
Ravi et al (2010) that good bigrams are those which
have high coverage of new words: each newly cov-
ered node contributes additional (partial) counts.
However, by using the weights instead of full counts,
we also account for the confidence assigned by LP.
Dividing by the number of new word/tag pairs added
focuses on bigrams that reuse existing tags for words
8Ravi et al (2010) include a third phase of iterative model
fitting; however, we found this stage to be not only expensive,
but also unhelpful because it frequently yields negative results.
9In the case of token-supervision, we pre-select all tag bi-
grams appearing in the labeled corpus since these are assumed
to be known high-quality tag bigrams and word/tag pairs.
142
and thereby limits the addition of new tags for each
word type.
At the start of model minimization, there are no
selected tag bigrams, and thus no valid path through
any sentence in the corpus. As bigrams are selected,
we can begin to cover subsequences and eventually
full sentences. There may be multiple valid taggings
for a sentence, so after each new bigram is selected,
we run the Viterbi algorithm over the raw corpus us-
ing the set of selected tag bigrams as a hard con-
straint on the allowable transitions. This efficiently
identifies the highest-weight path through each sen-
tence, if one exists. If such a path is found, we re-
move the sentence from the corpus and store the tags
from the Viterbi tagging. The algorithm terminates
when a path is found for every raw corpus sentence.
The result of weighted model minimization is this
set of tag paths. Since each path represents a valid
tagging of the sentence, we use this output as a nois-
ily labeled corpus for initializing EM in stage three.
3.3 Tagger training
Stage one provides an expansion of the initial la-
beled data and stage two turns that into a corpus of
noisily labeled sentences. Stage three uses the EM
algorithm initialized by the noisy labeling and con-
strained by the expanded tag dictionary to produce
an HMM.10 The initial distributions are smoothed
with one-count smoothing (Chen and Goodman,
1996). If human-tagged sentences are available as
training data, then we use their counts to supplement
the noisy labeled text for initialization and we add
their counts into every iteration?s result.
The HMM produced by stage three is not used
directly for tagging since it will contain zero-
probabilities for test-corpus words that were unseen
during training. Instead, we use it to provide a
Viterbi labeling of the raw corpus, following the
?auto-supervision? step of Garrette and Baldridge
(2012). This material is then concatenated with the
token-supervised corpus (when available), and used
to train a Maximum Entropy Markov Model tag-
ger.11 The MEMM exploits subword features and
10An added benefit of this strategy is that the EM algorithm
with the expanded dictionary runs much more quickly than
without it since it does not have to consider every possible tag
for unknown words, averaging 20x faster on PTB experiments.
11We use OpenNLP: opennlp.apache.org.
generally produces 1-2% better results than an HMM
trained on the same material.
4 Experiments12
Experimental results are shown in Table 2. Each ex-
periment starts with an initial data set provided by
annotator A or B. Experiment (1) simply uses EM
with the initial small tag dictionary to learn a tag-
ger from the raw corpus. (2) uses LP to infer an ex-
panded tag dictionary and tag distributions over raw
corpus tokens, but then takes the highest-weighted
tag from each token for use as noisily-labeled train-
ing data to initialize EM. (3) performs greedy model-
minimization on the LP output to derive that noisily-
labeled corpus. Finally, (4) is the same as (3), but
additionally uses external dictionary nodes in the LP
graph. In the case of token-supervision, we also in-
clude (0), in which we simply used the tagged sen-
tences as supervised data for an HMM without EM
(followed by MEMM training).
The results show that performance improves with
our LP and minimization techniques compared to
basic EM-HMM training. LP gives large across-the-
board improvements over EM training with only the
original tag dictionary (compare columns 1 & 2).
Weighted model minimization further improves re-
sults for type-supervision settings, but not for token
supervision (compare 2 & 3).
Using an external dictionary in the LP graph has
little effect for KIN, probably due to the available
dictionary?s very small size. However, MLG with
its larger dictionary obtains an improvement in both
scenarios. Results on ENG are mixed; this may be
because the PTB tagset has 45 tags (far more than
the dictionary) so the external dictionary nodes in
the LP graph may consequently serve to collapse dis-
tinctions (e.g. singular and plural) in the larger set.
Our results show differences between token- and
type-supervised annotations. Tag dictionary expan-
sion is helpful no matter what the annotations look
like: in both cases, the initial dictionary is too
small for effective EM learning, so expansion is nec-
essary. However, model minimization only ben-
efits the type-supervised scenarios, leaving token-
supervised performance unchanged. This suggests
12Our code is available at github.com/dhgarrette/
low-resource-pos-tagging-2013
143
Human Annotations 0. No EM 1. EM only 2. With LP 3. LP+min 4. LP(ed)+min
Initial data T K U T K U T K U T K U T K U
KIN tokens A 72 90 58 55 82 32 71 86 58 71 86 58 71 86 58
KIN types A 63 77 32 78 83 69 79 83 70 79 83 70
MLG tokens B 74 89 49 68 87 39 74 89 49 74 89 49 76 90 53
MLG types B 71 87 46 72 81 57 74 86 56 76 86 60
ENG tokens A 63 83 38 62 83 37 72 85 55 72 85 55 72 85 56
ENG types A 66 76 37 75 81 56 76 83 56 74 81 55
ENG tokens B 70 87 44 70 87 43 78 90 60 78 90 60 78 89 61
ENG types B 69 83 38 75 82 61 78 85 61 78 86 61
Table 2: Experimental results. Three languages are shown: Kinyarwanda (KIN), Malagasy (MLG), and
English (ENG). The letters A and B refer to the annotator. LP(ed) refers to label propagation including nodes
from an external dictionary. Each result given as percentages for Total (T), Known (K), and Unknown (U).
that minimization is working as intended: it induces
frequency information when none is provided. With
token-supervision, the annotator provides some in-
formation about which tag transitions are best and
which emissions are most likely. This is miss-
ing with type-supervision, so model minimization is
needed to bootstrap word/tag frequency guesses.
This leads to perhaps our most interesting result:
in a time-critical annotation scenario, it seems better
to collect a simple tag dictionary than tagged sen-
tences. While the tagged sentences certainly contain
useful information regarding tag frequencies, our
techniques can learn this missing information auto-
matically. Thus, having wider coverage of word type
information, and having that information be focused
on the most frequent words, is more important. This
can be seen as a validation of the last two decades
of work on (simulated) type-supervision learning for
POS-tagging?with the caveat that the additional ef-
fort we do is needed to realize the benefit.
Our experiments also allow us to compare how the
data from different annotators affects the quality of
taggers learned. Looking at the direct comparison
on English data, annotator B was able to tag more
sentences than A, but A produced more tag dictio-
nary entries in the type-supervision scenario. How-
ever, it appears, based on the EM-only training, that
the annotations provided by B were of higher quality
and produced more accurate taggers in both scenar-
ios. Regardless, our full training procedure is able
to substantially improve results in all scenarios.
Table 3 gives the recall and precision of the tag
Tag Dictionary Source R P
(1) human-annotated TD 18.42 29.33
(2) LP output 35.55 2.62
(3) model min output 30.49 4.63
Table 3: Recall (R) and precision (P) for tag dictio-
naries versus the test data in a ?MLG types B? run.
dictionaries for MLG for settings 1, 2 and 3. The ini-
tial, human-provided tag dictionary unsurprisingly
has the highest precision and lowest recall. LP ex-
pands that data to greatly improve recall with a large
drop in precision. Minimization culls many entries
and improves precision with a small relative loss in
recall. Of course, this is only a rough indicator of
the quality of the tag dictionaries since the word/tag
pairs of the test set only partially overlap with the
raw training data and annotations.
Because gold-standard annotations are available
for the English sentences, we also ran oracle ex-
periments using labels from the PTB corpus (es-
sentially, the kind of data used in previous work).
We selected the same amount of labeled tokens or
word/tag pairs as were obtained by the annotators.
We found similar patterns of improved performance
by using LP expansion and model minimization,
and all accuracies are improved compared to their
human-annotator equivalents (about 2-6%). Overall
accuracy for both type and token supervision comes
to 78-80%.
144
#Errors 11k 6k 5k 4k 3k
Gold TO NNP NN JJ NNP
Model IN NN JJ NN JJ
Table 4: Top errors from an ?ENG types B? run.
Error Analysis One potential source of errors
comes directly from the annotators themselves.
Though our approach is designed to be robust to an-
notation errors, it cannot correct all mistakes. For
example, for the ?ENG types B? experiment, the an-
notator listed IN (preposition) as the only tag for
word type ?to?. However, the test set only ever as-
signs tag TO for this type. This single error accounts
for a 2.3% loss in overall tagging accuracy (Table 4).
In many situations, however, we are able to auto-
matically remove improbable tag dictionary entries,
as shown in Table 5. Consider the word type ?for?.
The annotator has listed RP (particle) as a potential
tag, but only five out of 4k tokens have this tag. With
RP included, EM becomes confused and labels a ma-
jority of the tokens as RP when nearly all should be
labeled IN. We are able to eliminate RP as a possi-
bility, giving excellent overall accuracy for the type.
Likewise for the comma type, the annotator has in-
correctly given ?:? as a valid tag, and LP, which
uses the tag dictionary, pushes this label to many to-
kens with high confidence. However, minimization
is able to correct the problem.
Finally, the word type ?opposition? provides an
example of the expected behavior for unknown
words. The type is not in the tag dictionary, so
EM assumes all tags are valid and uses many labels.
LP expands the starting dictionary to cover the type,
limiting it to only two tags. Minimization then de-
termines that NN is the best tag for each token.
5 Related work
Goldberg et al (2008) trained a tagger for Hebrew
using a manually-created lexicon which was not de-
rived from an annotated corpus. However, their lexi-
con was constructed by trained lexicographers over a
long period of time and achieves very high coverage
of the language with very good quality. In contrast,
our annotated data was created by untrained linguis-
tics students working alone for just two hours.
Cucerzan and Yarowsky (2002) learn a POS-
for *IN *RP JJ NN CD
(1) EM 1,221 2764 9 5
(2) LP 4,003
(3) min 4,004 1
gold 3,999 5
, (comma) *, *: JJS PTD VBP
(1) EM 24,708 4 3 3
(2) LP 15,505 9226 1
(3) min 24,730
gold 24,732
opposition NN JJ DT NNS VBP
(1) EM 24 4 1 4 4
(2) LP 41 4
(3) min 45
gold 45
Table 5: Tag assignments in different scenarios. A
star indicates an entry in the human-provided TD.
tagger from existing linguistic resources, namely a
dictionary and a reference grammar, but these re-
sources are not available, much less digitized, for
most under-studied languages.
Subramanya et al (2010) apply LP to the prob-
lem of tagging for domain adaptation. They con-
struct an LP graph that connects tokens in low- and
high-resource domains, and propagate labels from
high to low. This approach addresses the prob-
lem of learning appropriate tags for unknown words
within a language, but it requires that the language
have at least one high-resource domain as a source
of high quality information. For low-resource lan-
guages that have no significant annotated resources
available in any domain, this technique cannot be
applied.
Das and Petrov (2011) and Ta?ckstro?m et al
(2013) learn taggers for languages in which there
are no POS-annotated resources, but for which par-
allel texts are available between that language and a
high-resource language. They project tag informa-
tion from the high-resource language to the lower-
resource language via alignments in the parallel text.
However, large parallel corpora are not available for
most low-resource languages. These are also ex-
pensive resources to create and would take consid-
erably more effort to produce than the monolingual
resources that our annotators were able to generate
145
in a two-hour timeframe. Of course, if they are avail-
able, such parallel text links could be incorporated
into our approach.
Furthermore, their approaches require the use of
a universal tag set shared between both languages.
As such, their approach is only able to induce POS
tags for the low-resource language if the same tag
set is used to tag the high-resource language. Our
approach does not rely on any such universal tag
set; we learn whichever tags the human annotator
chooses to use when they provide their annotations.
In fact, in our experiments we learn much more de-
tailed tag sets than the fairly coarse universal tag set
used by Das and Petrov (2011) or Ta?ckstro?m et al
(2013): we learn a tagger for the full Penn Treebank
tag set of 45 tags versus the 12 tags in the universal
set.
Ding (2011) constructed an LP graph for learning
POS tags on Chinese text by propagating labels from
an initial tag dictionary to a larger set of data. This
LP graph contained Wiktionary word/POS relation-
ships as features as well as Chinese-English word
alignment information and used it to directly esti-
mate emission probabilities to initialize an EM train-
ing of an HMM.
Li et al (2012) train an HMM using EM and an
initial tag dictionary derived from Wiktionary. Like
Das and Petrov (2011), they use a universal POS tag
set, so Wiktionary can be directly applied as a wide-
coverage tag dictionary in their case. Additionally,
they evaluate their approach on languages for which
Wiktionary has high coverage?which would cer-
tainly not get far with Kinyarwanda (9 entries). Our
approach does not rely on a high-coverage tag dic-
tionary nor is it restricted to use with a small tag set.
6 Conclusions and future work
With just two hours of annotation, we obtain 71-78%
accuracy for POS-tagging across three languages us-
ing both type and token supervision. Without tag
dictionary expansion and model minimization, per-
formance is much worse, from 63-74%. We dramat-
ically improve performance on unknown words: the
range of 37-58% improves to 53-70%.
We also have a provisional answer to whether an-
notation should be on types or tokens: use type-
supervision if you also expand and minimize. These
methods can identify missing word/tag entries and
estimate frequency information, and they produce as
good or better results compared to starting with to-
ken supervision. The case of Kinyarwanda was most
dramatic: 71% accuracy for token-supervision com-
pared to 79% for type-supervision. Studies using
more annotators and across more languages would
be necessary to make any stronger claim about the
relative efficacy of the two strategies.
Acknowledgements
We thank Kyle Jerro, Vijay John, Katrin Erk,
Yoav Goldberg, Ray Mooney, Slav Petrov, Oscar
Ta?ckstro?m, and the reviewers for their assistance
and feedback. This work was supported by the U.S.
Department of Defense through the U.S. Army Re-
search Office (grant number W911NF-10-1-0533)
and via a National Defense Science and Engineer-
ing Graduate Fellowship for the first author. Experi-
ments were run on the UTCS Mastodon Cluster, pro-
vided by NSF grant EIA-0303609.
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COLING,
Geneva, Switzerland.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of ACL, Santa Cruz, California,
USA.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of CoNLL, Taipei, Taiwan.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT, Portland, Oregon,
USA.
Weiwei Ding. 2011. Weakly supervised part-of-speech
tagging for Chinese using label propagation. Master?s
thesis, University of Texas at Austin.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-speech
tagging with incomplete tag dictionaries. In Proceed-
ings of EMNLP, Jeju, Korea.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings ACL.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
146
resource-scarce languages. In Proceedings of EACL,
Athens, Greece.
Julian Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech & Lan-
guage, 6(3).
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP, Jeju Island, Korea.
Christopher D. Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics? In
Proceedings of CICLing.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In Proceedings
EMNLP, Cambridge, MA.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. In
Transactions of the ACL. Association for Computa-
tional Linguistics.
Partha Pratim Talukdar and Koby Crammer. 2009. New
regularized algorithms for transductive learning. In
Proceedings of ECML-PKDD, Bled, Slovenia.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proceedings of NIPS.
147
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495?503,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Minimized models and grammar-informed initialization
for supertagging with highly ambiguous lexicons
Sujith Ravi1 Jason Baldridge2 Kevin Knight1
1University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
2Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
jbaldrid@mail.utexas.edu
Abstract
We combine two complementary ideas
for learning supertaggers from highly am-
biguous lexicons: grammar-informed tag
transitions and models minimized via in-
teger programming. Each strategy on its
own greatly improves performance over
basic expectation-maximization training
with a bitag Hidden Markov Model, which
we show on the CCGbank and CCG-TUT
corpora. The strategies provide further er-
ror reductions when combined. We de-
scribe a new two-stage integer program-
ming strategy that efficiently deals with
the high degree of ambiguity on these
datasets while obtaining the full effect of
model minimization.
1 Introduction
Creating accurate part-of-speech (POS) taggers
using a tag dictionary and unlabeled data is an
interesting task with practical applications. It
has been explored at length in the literature since
Merialdo (1994), though the task setting as usu-
ally defined in such experiments is somewhat arti-
ficial since the tag dictionaries are derived from
tagged corpora. Nonetheless, the methods pro-
posed apply to realistic scenarios in which one
has an electronic part-of-speech tag dictionary or
a hand-crafted grammar with limited coverage.
Most work has focused on POS-tagging for
English using the Penn Treebank (Marcus et al,
1993), such as (Banko and Moore, 2004; Gold-
water and Griffiths, 2007; Toutanova and John-
son, 2008; Goldberg et al, 2008; Ravi and Knight,
2009). This generally involves working with the
standard set of 45 POS-tags employed in the Penn
Treebank. The most ambiguous word has 7 dif-
ferent POS tags associated with it. Most methods
have employed some variant of Expectation Max-
imization (EM) to learn parameters for a bigram
or trigram Hidden Markov Model (HMM). Ravi
and Knight (2009) achieved the best results thus
far (92.3% word token accuracy) via a Minimum
Description Length approach using an integer pro-
gram (IP) that finds a minimal bigram grammar
that obeys the tag dictionary constraints and cov-
ers the observed data.
A more challenging task is learning supertag-
gers for lexicalized grammar formalisms such as
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000). For example, CCGbank (Hocken-
maier and Steedman, 2007) contains 1241 dis-
tinct supertags (lexical categories) and the most
ambiguous word has 126 supertags. This pro-
vides a much more challenging starting point
for the semi-supervised methods typically ap-
plied to the task. Yet, this is an important task
since creating grammars and resources for CCG
parsers for new domains and languages is highly
labor- and knowledge-intensive. Baldridge (2008)
uses grammar-informed initialization for HMM
tag transitions based on the universal combinatory
rules of the CCG formalism to obtain 56.1% accu-
racy on ambiguous word tokens, a large improve-
ment over the 33.0% accuracy obtained with uni-
form initialization for tag transitions.
The strategies employed in Ravi and Knight
(2009) and Baldridge (2008) are complementary.
The former reduces the model size globally given
a data set, while the latter biases bitag transitions
toward those which are more likely based on a uni-
versal grammar without reference to any data. In
this paper, we show how these strategies may be
combined straightforwardly to produce improve-
ments on the task of learning supertaggers from
lexicons that have not been filtered in any way.1
We demonstrate their cross-lingual effectiveness
on CCGbank (English) and the Italian CCG-TUT
1See Banko and Moore (2004) for a description of how
many early POS-tagging papers in fact used a number of
heuristic cutoffs that greatly simplify the problem.
495
corpus (Bos et al, 2009). We find a consistent im-
proved performance by using each of the methods
compared to basic EM, and further improvements
by using them in combination.
Applying the approach of Ravi and Knight
(2009) naively to CCG supertagging is intractable
due to the high level of ambiguity. We deal with
this by defining a new two-stage integer program-
ming formulation that identifies minimal gram-
mars efficiently and effectively.
2 Data
CCGbank. CCGbank was created by semi-
automatically converting the Penn Treebank to
CCG derivations (Hockenmaier and Steedman,
2007). We use the standard splits of the data
used in semi-supervised tagging experiments (e.g.
Banko and Moore (2004)): sections 0-18 for train-
ing, 19-21 for development, and 22-24 for test.
CCG-TUT. CCG-TUT was created by semi-
automatically converting dependencies in the Ital-
ian Turin University Treebank to CCG deriva-
tions (Bos et al, 2009). It is much smaller than
CCGbank, with only 1837 sentences. It is split
into three sections: newspaper texts (NPAPER),
civil code texts (CIVIL), and European law texts
from the JRC-Acquis Multilingual Parallel Corpus
(JRC). For test sets, we use the first 400 sentences
of NPAPER, the first 400 of CIVIL, and all of JRC.
This leaves 409 and 498 sentences from NPAPER
and CIVIL, respectively, for training (to acquire a
lexicon and run EM). For evaluation, we use two
different settings of train/test splits:
TEST 1 Evaluate on the NPAPER section of test
using a lexicon extracted only from NPAPER
section of train.
TEST 2 Evaluate on the entire test using lexi-
cons extracted from (a) NPAPER + CIVIL,
(b) NPAPER, and (c) CIVIL.
Table 1 shows statistics for supertag ambiguity
in CCGbank and CCG-TUT. As a comparison, the
POS word token ambiguity in CCGbank is 2.2: the
corresponding value of 18.71 for supertags is in-
dicative of the (challenging) fact that supertag am-
biguity is greatest for the most frequent words.
3 Grammar informed initialization for
supertagging
Part-of-speech tags are atomic labels that in and of
themselves encode no internal structure. In con-
Data Distinct Max Type ambig Tok ambig
CCGbank 1241 126 1.69 18.71
CCG-TUT
NPAPER+CIVIL 849 64 1.48 11.76
NPAPER 644 48 1.42 12.17
CIVIL 486 39 1.52 11.33
Table 1: Statistics for the training data used to ex-
tract lexicons for CCGbank and CCG-TUT. Dis-
tinct: # of distinct lexical categories; Max: # of
categories for the most ambiguous word; Type
ambig: per word type category ambiguity; Tok
ambig: per word token category ambiguity.
trast, supertags are detailed, structured labels; a
universal set of grammatical rules defines how cat-
egories may combine with one another to project
syntactic structure.2 Because of this, properties of
the CCG formalism itself can be used to constrain
learning?prior to considering any particular lan-
guage, grammar or data set. Baldridge (2008) uses
this observation to create grammar-informed tag
transitions for a bitag HMM supertagger based on
two main properties. First, categories differ in
their complexity and less complex categories tend
to be used more frequently. For example, two cat-
egories for buy in CCGbank are (S[dcl]\NP)/NP
and ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; the
former occurs 33 times, the latter once. Second,
categories indicate the form of categories found
adjacent to them; for example, the category for
sentential complement verbs ((S\NP)/S) expects
an NP to its left and an S to its right.
Categories combine via rules such as applica-
tion and composition (see Steedman (2000) for de-
tails). Given a lexicon containing the categories
for each word, these allow derivations like:
Ed might see a cat
NP (S\NP)/(S\NP) (S\NP)/NP NP/N N
>B >
(S\NP)/NP NP
>
S\NP
>
S
Other derivations are possible. In fact, every pair
of adjacent words above may be combined di-
rectly. For example, see and a may combine
through forward composition to produce the cate-
gory (S\NP)/N, and Ed?s category may type-raise
to S/(S\NP) and compose with might?s category.
Baldridge uses these properties to define tag
2Note that supertags can be lexical categories of CCG
(Steedman, 2000), elementary trees of Tree-adjoining Gram-
mar (Joshi, 1988), or types in a feature hierarchy as in Head-
driven Phrase Structure Grammar (Pollard and Sag, 1994).
496
transition distributions that have higher likeli-
hood for simpler categories that are able to
combine. For example, for the distribution
p(ti|ti?1=NP ), (S\NP)\NP is more likely than
((S\NP)/(N/N))\NP because both categories may
combine with a preceding NP but the former is
simpler. In turn, the latter is more likely than NP: it
is more complex but can combine with the preced-
ing NP. Finally, NP is more likely than (S/NP)/NP
since neither can combine, but NP is simpler.
By starting EM with these tag transition dis-
tributions and an unfiltered lexicon (word-to-
supertag dictionary), Baldridge obtains a tagging
accuracy of 56.1% on ambiguous words?a large
improvement over the accuracy of 33.0% obtained
by starting with uniform transition distributions.
We refer to a model learned from basic EM (uni-
formly initialized) as EM, and to a model with
grammar-informed initialization as EMGI .
4 Minimized models for supertagging
The idea of searching for minimized models is
related to classic Minimum Description Length
(MDL) (Barron et al, 1998), which seeks to se-
lect a small model that captures the most regularity
in the observed data. This modeling strategy has
been shown to produce good results for many nat-
ural language tasks (Goldsmith, 2001; Creutz and
Lagus, 2002; Ravi and Knight, 2009). For tagging,
the idea has been implemented using Bayesian
models with priors that indirectly induce sparsity
in the learned models (Goldwater and Griffiths,
2007); however, Ravi and Knight (2009) show a
better approach is to directly minimize the model
using an integer programming (IP) formulation.
Here, we build on this idea for supertagging.
There are many challenges involved in using IP
minimization for supertagging. The 1241 distinct
supertags in the tagset result in 1.5 million tag bi-
gram entries in the model and the dictionary con-
tains almost 3.5 million word/tag pairs that are rel-
evant to the test data. The set of 45 POS tags for
the same data yields 2025 tag bigrams and 8910
dictionary entries. We also wish to scale our meth-
ods to larger data settings than the 24k word tokens
in the test data used in the POS tagging task.
Our objective is to find the smallest supertag
grammar (of tag bigram types) that explains the
entire text while obeying the lexicon?s constraints.
However, the original IP method of Ravi and
Knight (2009) is intractable for supertagging, so
we propose a new two-stage method that scales to
the larger tagsets and data involved.
4.1 IP method for supertagging
Our goal for supertagging is to build a minimized
model with the following objective:
IPoriginal: Find the smallest supertag gram-
mar (i.e., tag bigrams) that can explain the en-
tire text (the test word token sequence).
Using the full grammar and lexicon to perform
model minimization results in a very large, diffi-
cult to solve integer program involving billions of
variables and constraints. This renders the mini-
mization objective IPoriginal intractable. One way
of combating this is to use a reduced grammar
and lexicon as input to the integer program. We
do this without further supervision by using the
HMM model trained using basic EM: entries are
pruned based on the tag sequence it predicts on
the test data. This produces an observed grammar
of distinct tag bigrams (Gobs) and lexicon of ob-
served lexical assignments (Lobs). For CCGbank,
Gobs and Lobs have 12,363 and 18,869 entries,
respectively?far less than the millions of entries
in the full grammar and lexicon.
Even though EM minimizes the model some-
what, many bad entries remain in the grammar.
We prune further by supplying Gobs and Lobs as
input (G,L) to the IP-minimization procedure.
However, even with the EM-reduced grammar and
lexicon, the IP-minimization is still very hard to
solve. We thus split it into two stages. The first
stage (Minimization 1) finds the smallest grammar
Gmin1 ? G that explains the set of word bigram
types observed in the data rather than the word
sequence itself, and the second (Minimization 2)
finds the smallest augmentation of Gmin1 that ex-
plains the full word sequence.
Minimization 1 (MIN1). We begin with a sim-
pler minimization problem than the original one
(IPoriginal), with the following objective:
IPmin 1: Find the smallest set of tag bigrams
Gmin1 ? G, such that there is at least one
tagging assignment possible for every word bi-
gram type observed in the data.
We formulate this as an integer program, creat-
ing binary variables gvari for every tag bigram
gi = tjtk in G. Binary link variables connect tag
bigrams with word bigrams; these are restricted
497
          :
          :
        t
i
 t
j
          :
          :
Input Grammar (G) word bigrams: 
w
1
 w
2
w
2
 w
3
:
:
w
i
 w
j
:
:
MIN 1
          :
          :
        t
i
 t
j
          :
          :
Input Grammar (G) word bigrams: 
w
1
 w
2
w
2
 w
3
:
:
w
i
 w
j
:
:
word sequence: 
w
1
       w
2          
w
3          
w
4        
w
5
t
1
t
2
t
3   
:
:
     
t
k
supertags
tag bigrams chosen in first minimization step (G
min1
)
(does not explain the word sequence)
word sequence: 
w
1
       w
2          
w
3          
w
4        
w
5
t
1
t
2
t
3   
:
:
     
t
k
supertags
tag bigrams chosen in second minimization step (G
min2
)
MIN 2
IP Minimization 1
IP Minimization 2
Figure 1: Two-stage IP method for selecting minimized models for supertagging.
to the set of links that respect the lexicon L pro-
vided as input, i.e., there exists a link variable
linkjklm connecting tag bigram tjtk with word bi-
gram wlwm only if the word/tag pairs (wl, tj) and
(wm, tk) are present in L. The entire integer pro-
gramming formulation is shown Figure 2.
The IP solver3 solves the above integer program
and we extract the set of tag bigrams Gmin1 based
on the activated grammar variables. For the CCG-
bank test data, MIN1 yields 2530 tag bigrams.
However, a second stage is needed since there is
no guarantee that Gmin1 can explain the test data:
it contains tags for all word bigram types, but it
cannot necessarily tag the full word sequence. Fig-
ure 1 illustrates this. Using only tag bigrams from
MIN1 (shown in blue), there is no fully-linked tag
path through the network. There are missing links
between words w2 and w3 and between words w3
and w4 in the word sequence. The next stage fills
in these missing links.
Minimization 2 (MIN2). This stage uses the
original minimization formulation for the su-
pertagging problem IPoriginal, again using an in-
teger programming method similar to that pro-
posed by Ravi and Knight (2009). If applied to
the observed grammar Gobs, the resulting integer
program is hard to solve.4 However, by using the
partial solution Gmin1 obtained in MIN1 the IP
optimization speeds up considerably. We imple-
ment this by fixing the values of all binary gram-
mar variables present in Gmin1 to 1 before opti-
mization. This reduces the search space signifi-
3We use the commercial CPLEX solver.
4The solver runs for days without returning a solution.
Minimize:
?
?gi?G
gvari
Subject to constraints:
1. For every word bigram wlwm, there exists at least
one tagging that respects the lexicon L.
?
? tj?L(wl), tk?L(wm)
linkjklm ? 1
where L(wl) and L(wm) represent the set of tags seen
in the lexicon for words wl and wm respectively.
2. The link variable assignments are constrained to re-
spect the grammar variables chosen by the integer pro-
gram.
linkjklm ? gvari
where gvari is the binary variable corresponding to tag
bigram tjtk in the grammar G.
Figure 2: IP formulation for Minimization 1.
cantly, and CPLEX finishes in just a few hours.
The details of this method are described below.
We instantiate binary variables gvari and lvari
for every tag bigram (in G) and lexicon entry (in
L). We then create a network of possible taggings
for the word token sequence w1w2....wn in the
corpus and assign a binary variable to each link
in the network. We name these variables linkcjk,
where c indicates the column of the link?s source
in the network, and j and k represent the link?s
source and destination (i.e., linkcjk corresponds to
tag bigram tjtk in column c). Next, we formulate
the integer program given in Figure 3.
Figure 1 illustrates how MIN2 augments the
grammar Gmin1 (links shown in blue) with addi-
498
Minimize:
?
?gi?G
gvari
Subject to constraints:
1. Chosen link variables form a left-to-right path
through the tagging network.
?c=1..n?2?k
?
j linkcjk =
?
j link(c+1)kj
2. Link variable assignments should respect the chosen
grammar variables.
for every link: linkcjk ? gvari
where gvari corresponds to tag bigram tjtk
3. Link variable assignments should respect the chosen
lexicon variables.
for every link: linkcjk ? lvarwctj
for every link: linkcjk ? lvarwc+1tk
where wc is the cth word in the word sequence w1...wn,
and lvarwctj is the binary variable corresponding to the
word/tag pair wc/tj in the lexicon L.
4. The final solution should produce at least one com-
plete tagging path through the network.
?
?j,k
link1jk ? 1
5. Provide minimized grammar from MIN1as partial
solution to the integer program.
?gi?Gmin1 gvari = 1
Figure 3: IP formulation for Minimization 2.
tional tag bigrams (shown in red) to form a com-
plete tag path through the network. The minimized
grammar set in the final solution Gmin2 contains
only 2810 entries, significantly fewer than the
original grammar Gobs?s 12,363 tag bigrams.
We note that the two-stage minimization pro-
cedure proposed here is not guaranteed to yield
the optimal solution to our original objective
IPoriginal. On the simpler task of unsupervised
POS tagging with a dictionary, we compared
our method versus directly solving IPoriginal and
found that the minimization (in terms of grammar
size) achieved by our method is close to the opti-
mal solution for the original objective and yields
the same tagging accuracy far more efficiently.
Fitting the minimized model. The IP-
minimization procedure gives us a minimal
grammar, but does not fit the model to the data.
In order to estimate probabilities for the HMM
model for supertagging, we use the EM algorithm
but with certain restrictions. We build the transi-
tion model using only entries from the minimized
grammar set Gmin2, and instantiate an emission
model using the word/tag pairs seen in L (pro-
vided as input to the minimization procedure). All
the parameters in the HMM model are initialized
with uniform probabilities, and we run EM for 40
iterations. The trained model is used to find the
Viterbi tag sequence for the corpus. We refer to
this model (where the EM output (Gobs, Lobs) was
provided to the IP-minimization as initial input)
as EM+IP.
Bootstrapped minimization. The quality of the
observed grammar and lexicon improves consid-
erably at the end of a single EM+IP run. Ravi
and Knight (2009) exploited this to iteratively im-
prove their POS tag model: since the first mini-
mization procedure is seeded with a noisy gram-
mar and tag dictionary, iterating the IP procedure
with progressively better grammars further im-
proves the model. We do likewise, bootstrapping a
new EM+IP run using as input, the observed gram-
mar Gobs and lexicon Lobs from the last tagging
output of the previous iteration. We run this until
the chosen grammar set Gmin2 does not change.5
4.2 Minimization with grammar-informed
initialization
There are two complementary ways to use
grammar-informed initialization with the IP-
minimization approach: (1) using EMGI output
as the starting grammar/lexicon and (2) using the
tag transitions directly in the IP objective function.
The first takes advantage of the earlier observation
that the quality of the grammar and lexicon pro-
vided as initial input to the minimization proce-
dure can affect the quality of the final supertagging
output. For the second, we modify the objective
function used in the two IP-minimization steps to
be:
Minimize:
?
?gi?G
wi ? gvari (1)
where, G is the set of tag bigrams provided as in-
put to IP, gvari is a binary variable in the integer
program corresponding to tag bigram (ti?1, ti) ?
G, and wi is negative logarithm of pgii(ti|ti?1)
as given by Baldridge (2008).6 All other parts of
5In our experiments, we run three bootstrap iterations.
6Other numeric weights associated with the tag bi-
grams could be considered, such as 0/1 for uncombin-
499
the integer program including the constraints re-
main unchanged, and, we acquire a final tagger in
the same manner as described in the previous sec-
tion. In this way, we combine the minimization
and GI strategies into a single objective function
that finds a minimal grammar set while keeping
the more likely tag bigrams in the chosen solution.
EMGI+IPGI is used to refer to the method that
uses GI information in both ways: EMGI output
as the starting grammar/lexicon and GI weights in
the IP-minimization objective.
5 Experiments
We compare the four strategies described in Sec-
tions 3 and 4, summarized below:
EM HMM uniformly initialized, EM training.
EM+IP IP minimization using initial grammar
provided by EM.
EMGI HMM with grammar-informed initializa-
tion, EM training.
EMGI+IPGI IP minimization using initial gram-
mar/lexicon provided by EMGI and addi-
tional grammar-informed IP objective.
For EM+IP and EMGI+IPGI , the minimization
and EM training processes are iterated until the
resulting grammar and lexicon remain unchanged.
Forty EM iterations are used for all cases.
We also include a baseline which randomly
chooses a tag from those associated with each
word in the lexicon, averaged over three runs.
Accuracy on ambiguous word tokens. We
evaluate the performance in terms of tagging accu-
racy with respect to gold tags for ambiguous words
in held-out test sets for English and Italian. We
consider results with and without punctuation.7
Recall that unlike much previous work, we do
not collect the lexicon (tag dictionary) from the
test set: this means the model must handle un-
known words and the possibility of having missing
lexical entries for covering the test set.
Precision and recall of grammar and lexicon.
In addition to accuracy, we measure precision and
able/combinable bigrams.
7The reason for this is that the ?categories? for punctua-
tion in CCGbank are for the most part not actual categories;
for example, the period ?.? has the categories ?.? and ?S?.
As such, these supertags are outside of the categorial system:
their use in derivations requires phrase structure rules that are
not derivable from the CCG combinatory rules.
Model ambig ambig all all
-punc -punc
Random 17.9 16.2 27.4 21.9
EM 38.7 35.6 45.6 39.8
EM+IP 52.1 51.0 57.3 53.9
EMGI 56.3 59.4 61.0 61.7
EMGI+IPGI 59.6 62.3 63.8 64.3
Table 2: Supertagging accuracy for CCGbank sec-
tions 22-24. Accuracies are reported for four
settings?(1) ambiguous word tokens in the test
corpus, (2) ambiguous word tokens, ignoring
punctuation, (3) all word tokens, and (4) all word
tokens except punctuation.
recall for each model on the observed bitag gram-
mar and observed lexicon on the test set. We cal-
culate them as follows, for an observed grammar
or lexicon X:
Precision =
|{X} ? {Observedgold}|
|{X}|
Recall =
|{X} ? {Observedgold}|
|{Observedgold}|
This provides a measure of model performance on
bitag types for the grammar and lexical entry types
for the lexicon, rather than tokens.
5.1 English CCGbank results
Accuracy on ambiguous tokens. Table 2 gives
performance on the CCGbank test sections. All
models are well above the random baseline, and
both of the strategies individually boost perfor-
mance over basic EM by a large margin. For the
models using GI, accuracy ignoring punctuation is
higher than for all almost entirely due to the fact
that ?.? has the supertags ?.? and S, and the GI
gives a preference to S since it can in fact combine
with other categories, unlike ?.??the effect is that
nearly every sentence-final period (?5.5k tokens) is
tagged S rather than ?.?.
EMGI is more effective than EM+IP; however,
it should be kept in mind that IP-minimization
is a general technique that can be applied to
any sequence prediction task, whereas grammar-
informed initialization may be used only with
tasks in which the interactions of adjacent labels
may be derived from the labels themselves. In-
terestingly, the gap between the two approaches
is greater when punctuation is ignored (51.0 vs.
59.4)?this is unsurprising because, as noted al-
ready, punctuation supertags are not actual cate-
500
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 7.5 32.9 52.6 68.1
Recall 26.9 13.2 34.0 19.8
Lexicon
Precision 58.4 63.0 78.0 80.6
Recall 50.9 56.0 71.5 67.6
Table 3: Comparison of grammar/lexicon ob-
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCGbank data.
gories, so EMGI is unable to model their distribu-
tion. Most importantly, the complementary effects
of the two approaches can be seen in the improved
results for EMGI+IPGI , which obtains about 3%
better accuracy than EMGI .
Accuracy on all tokens. Table 2 also gives per-
formance when taking all tokens into account. The
HMM when using full supervision obtains 87.6%
accuracy (Baldridge, 2008),8 so the accuracy of
63.8% achieved by EMGI+IPGI nearly halves the
gap between the supervised model and the 45.6%
obtained by basic EM semi-supervised model.
Effect of GI information in EM and/or IP-
minimization stages. We can also consider the
effect of GI information in either EM training or
IP-minimization to see whether it can be effec-
tively exploited in both. The latter, EM+IPGI ,
obtains 53.2/51.1 for all/no-punc?a small gain
compared to EM+IP?s 52.1/51.0. The former,
EMGI+IP, obtains 58.9/61.6?a much larger gain.
Thus, the better starting point provided by EMGI
has more impact than the integer program that in-
cludes GI in its objective function. However, we
note that it should be possible to exploit the GI
information more effectively in the integer pro-
gram than we have here. Also, our best model,
EMGI+IPGI , uses GI information in both stages
to obtain our best accuracy of 59.6/62.3.
P/R for grammars and lexicons. We can ob-
tain a more-fine grained understanding of how the
models differ by considering the precision and re-
call values for the grammars and lexicons of the
different models, given in Table 3. The basic EM
model has very low precision for the grammar, in-
dicating it proposes many unnecessary bitags; it
8A state-of-the-art, fully-supervised maximum entropy
tagger (Clark and Curran, 2007) (which also uses part-of-
speech labels) obtains 91.4% on the same train/test split.
achieves better recall because of the sheer num-
ber of bitags it proposes (12,363). EM+IP prunes
that set of bitags considerably, leading to better
precision at the cost of recall. EMGI ?s higher re-
call and precision indicate the tag transition dis-
tributions do capture general patterns of linkage
between adjacent CCG categories, while EM en-
sures that the data filters out combinable, but un-
necessary, bitags. With EMGI+IPGI , we again
see that IP-minimization prunes even more entries,
improving precision at the loss of some recall.
Similar trends are seen for precision and recall
on the lexicon. IP-minimization?s pruning of inap-
propriate taggings means more common words are
not assigned highly infrequent supertags (boosting
precision) while unknown words are generally as-
signed more sensible supertags (boosting recall).
EMGI again focuses taggings on combinable con-
texts, boosting precision and recall similarly to
EM+IP, but in greater measure. EMGI+IPGI then
prunes some of the spurious entries, boosting pre-
cision at some loss of recall.
Tag frequencies predicted on the test set. Ta-
ble 4 compares gold tags to tags generated by
all four methods for the frequent and highly am-
biguous words the and in. Basic EM wanders
far away from the gold assignments; it has little
guidance in the very large search space available
to it. IP-minimization identifies a smaller set of
tags that better matches the gold tags; this emerges
because other determiners and prepositions evoke
similar, but not identical, supertags, and the gram-
mar minimization pushes (but does not force)
them to rely on the same supertags wherever pos-
sible. However, the proportions are incorrect;
for example, the tag assigned most frequently to
in is ((S\NP)\(S\NP))/NP though (NP\NP)/NP
is more frequent in the test set. EMGI ?s tags
correct that balance and find better proportions,
but also some less common categories, such as
(((N/N)\(N/N))\((N/N)\(N/N)))/N, sneak in be-
cause they combine with frequent categories like
N/N and N. Bringing the two strategies together
with EMGI+IPGI filters out the unwanted cate-
gories while getting better overall proportions.
5.2 Italian CCG-TUT results
To demonstrate that both methods and their com-
bination are language independent, we apply them
to the Italian CCG-TUT corpus. We wanted
to evaluate performance out-of-the-box because
501
Lexicon Gold EM EM+IP EMGI EMGI+IPGI
the? (41 distinct tags in Ltrain) (14 tags) (18 tags) (9 tags) (25 tags) (12 tags)
NP[nb]/N 5742 0 4544 4176 4666
((S\NP)\(S\NP))/N 14 5 642 122 107
(((N/N)\(N/N))\((N/N)\(N/N)))/N 0 0 0 698 0
((S/S)/S[dcl])/(S[adj]\NP) 0 733 0 0 0
PP/N 0 1755 0 3 1
: : : : : :
in? (76 distinct tags in Ltrain) (35 tags) (20 tags) (17 tags) (37 tags) (14 tags)
(NP\NP)/NP 883 0 649 708 904
((S\NP)\(S\NP))/NP 793 0 911 320 424
PP/NP 177 1 33 12 82
((S[adj]\NP)/(S[adj]\NP))/NP 0 215 0 0 0
: : : : : :
Table 4: Comparison of tag assignments from the gold tags versus model tags obtained on the test set.
The table shows tag assignments (and their counts for each method) for the and in in the CCGbank test
sections. The number of distinct tags assigned by each method is given in parentheses. Ltrain is the
lexicon obtained from sections 0-18 of CCGbank that is used as the basis for EM training.
Model TEST 1 TEST 2 (using lexicon from:)
NPAPER+CIVIL NPAPER CIVIL
Random 9.6 9.7 8.4 9.6
EM 26.4 26.8 27.2 29.3
EM+IP 34.8 32.4 34.8 34.6
EMGI 43.1 43.9 44.0 40.3
EMGI+IPGI 45.8 43.6 47.5 40.9
Table 5: Comparison of supertagging results for
CCG-TUT. Accuracies are for ambiguous word
tokens in the test corpus, ignoring punctuation.
bootstrapping a supertagger for a new language is
one of the main use scenarios we envision: in such
a scenario, there is no development data for chang-
ing settings and parameters. Thus, we determined
a train/test split beforehand and ran the methods
exactly as we had for CCGbank.
The results, given in Table 5, demonstrate the
same trends as for English: basic EM is far more
accurate than random, EM+IP adds another 8-10%
absolute accuracy, and EMGI adds an additional 8-
10% again. The combination of the methods gen-
erally improves over EMGI , except when the lex-
icon is extracted from NPAPER+CIVIL. Table 6
gives precision and recall for the grammars and
lexicons for CCG-TUT?the values are lower than
for CCGbank (in line with the lower baseline), but
exhibit the same trends.
6 Conclusion
We have shown how two complementary
strategies?grammar-informed tag transitions and
IP-minimization?for learning of supertaggers
from highly ambiguous lexicons can be straight-
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 23.1 26.4 44.9 46.7
Recall 18.4 15.9 24.9 22.7
Lexicon
Precision 51.2 52.0 54.8 55.1
Recall 43.6 42.8 46.0 44.9
Table 6: Comparison of grammar/lexicon ob-
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCG-TUT.
forwardly integrated. We verify the benefits of
both cross-lingually, on English and Italian data.
We also provide a new two-stage integer program-
ming setup that allows model minimization to be
tractable for supertagging without sacrificing the
quality of the search for minimal bitag grammars.
The experiments in this paper use large lexi-
cons, but the methodology will be particularly use-
ful in the context of bootstrapping from smaller
ones. This brings further challenges; in particular,
it will be necessary to identify novel entries con-
sisting of seen word and seen category and to pre-
dict unseen, but valid, categories which are needed
to explain the data. For this, it will be necessary
to forgo the assumption that the provided lexicon
is always obeyed. The methods we introduce here
should help maintain good accuracy while open-
ing up these degrees of freedom. Because the lexi-
con is the grammar in CCG, learning new word-
category associations is grammar generalization
and is of interest for grammar acquisition.
502
Finally, such lexicon refinement and generaliza-
tion is directly relevant for using CCG in syntax-
based machine translation models (Hassan et al,
2009). Such models are currently limited to lan-
guages for which corpora annotated with CCG
derivations are available. Clark and Curran (2006)
show that CCG parsers can be learned from sen-
tences labeled with just supertags?without full
derivations?with little loss in accuracy. The im-
provements we show here for learning supertag-
gers from lexicons without labeled data may be
able to help create annotated resources more ef-
ficiently, or enable CCG parsers to be learned with
less human-coded knowledge.
Acknowledgements
The authors would like to thank Johan Bos, Joey
Frazee, Taesun Moon, the members of the UT-
NLL reading group, and the anonymous review-
ers. Ravi and Knight acknowledge the support
of the NSF (grant IIS-0904684) for this work.
Baldridge acknowledges the support of a grant
from the Morris Memorial Trust Fund of the New
York Community Trust.
References
J. Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 57?64,
Manchester, UK, August.
M. Banko and R. C. Moore. 2004. Part of speech
tagging in context. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), page 556, Morristown, NJ, USA.
A. R. Barron, J. Rissanen, and B. Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27?38, Milan, Italy.
S. Clark and J. Curran. 2006. Partial training for
a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 144?151, New
York City, USA, June.
S. Clark and J. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4).
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACLWork-
shop on Morphological and Phonological Learning,
pages 21?30, Morristown, NJ, USA.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL, pages 746?
754, Columbus, Ohio, June.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL, pages 744?751,
Prague, Czech Republic, June.
H. Hassan, K. Sima?an, and A. Way. 2009. A syntac-
tified direct translation model with linear-time de-
coding. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1182?1191, Singapore, August.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
A. Joshi. 1988. Tree Adjoining Grammars. In David
Dowty, Lauri Karttunen, and Arnold Zwicky, ed-
itors, Natural Language Parsing, pages 206?250.
Cambridge University Press, Cambridge.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2).
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
C. Pollard and I. Sag. 1994. Head Driven Phrase
Structure Grammar. CSLI/Chicago University
Press, Chicago.
S. Ravi and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 504?512, Suntec, Singapore, August.
M. Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of the Ad-
vances in Neural Information Processing Systems
(NIPS), pages 1521?1528, Cambridge, MA. MIT
Press.
503
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 955?964,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Simple Supervised Document Geolocation with Geodesic Grids
Benjamin P. Wing
Department of Linguistics
University of Texas at Austin
Austin, TX 78712 USA
ben@benwing.com
Jason Baldridge
Department of Linguistics
University of Texas at Austin
Austin, TX 78712 USA
jbaldrid@mail.utexas.edu
Abstract
We investigate automatic geolocation (i.e.
identification of the location, expressed as
latitude/longitude coordinates) of documents.
Geolocation can be an effective means of sum-
marizing large document collections and it is
an important component of geographic infor-
mation retrieval. We describe several simple
supervised methods for document geolocation
using only the document?s raw text as evi-
dence. All of our methods predict locations
in the context of geodesic grids of varying de-
grees of resolution. We evaluate the methods
on geotagged Wikipedia articles and Twitter
feeds. For Wikipedia, our best method obtains
a median prediction error of just 11.8 kilome-
ters. Twitter geolocation is more challenging:
we obtain a median error of 479 km, an im-
provement on previous results for the dataset.
1 Introduction
There are a variety of applications that arise from
connecting linguistic content?be it a word, phrase,
document, or entire corpus?to geography. Lei-
dner (2008) provides a systematic overview of
geography-based language applications over the
previous decade, with a special focus on the prob-
lem of toponym resolution?identifying and disam-
biguating the references to locations in texts. Per-
haps the most obvious and far-reaching applica-
tion is geographic information retrieval (Ding et al,
2000; Martins, 2009; Andogah, 2010), with ap-
plications like MetaCarta?s geographic text search
(Rauch et al, 2003) and NewsStand (Teitler et al,
2008); these allow users to browse and search for
content through a geo-centric interface. The Perseus
project performs automatic toponym resolution on
historical texts in order to display a map with each
text showing the locations that are mentioned (Smith
and Crane, 2001); Google Books also does this
for some books, though the toponyms are identified
and resolved quite crudely. Hao et al(2010) use
a location-based topic model to summarize travel-
ogues, enrich them with automatically chosen im-
ages, and provide travel recommendations. Eisen-
stein et al(2010) investigate questions of dialec-
tal differences and variation in regional interests in
Twitter users using a collection of geotagged tweets.
An intuitive and effective strategy for summa-
rizing geographically-based data is identification of
the location?a specific latitude and longitude?that
forms the primary focus of each document. De-
termining a single location of a document is only
a well-posed problem for certain documents, gen-
erally of fairly small size, but there are a number
of natural situations in which such collections arise.
For example, a great number of articles in Wikipedia
have been manually geotagged; this allows those ar-
ticles to appear in their geographic locations while
geobrowsing in an application like Google Earth.
Overell (2009) investigates the use of Wikipedia
as a source of data for article geolocation, in addition
to article classification by category (location, per-
son, etc.) and toponym resolution. Overell?s main
goal is toponym resolution, for which geolocation
serves as an input feature. For document geoloca-
tion, Overell uses a simple model that makes use
only of the metadata available (article title, incom-
ing and outgoing links, etc.)?the actual article text
955
is not used at all. However, for many document col-
lections, such metadata is unavailable, especially in
the case of recently digitized historical documents.
Eisenstein et al (2010) evaluate their geographic
topic model by geolocating USA-based Twitter
users based on their tweet content. This is essen-
tially a document geolocation task, where each doc-
ument is a concatenation of all the tweets for a single
user. Their geographic topic model receives super-
vision from many documents/users and predicts lo-
cations for unseen documents/users.
In this paper, we tackle document geolocation us-
ing several simple supervised methods on the textual
content of documents and a geodesic grid as a dis-
crete representation of the earth?s surface. Our ap-
proach is similar to that of Serdyukov et al (2009),
who geolocate Flickr images using their associated
textual tags.1 Essentially, the task is cast similarly
to language modeling approaches in information re-
trieval (Ponte and Croft, 1998). Discrete cells rep-
resenting areas on the earth?s surface correspond to
documents (with each cell-document being a con-
catenation of all actual documents that are located
in that cell); new documents are then geolocated to
the most similar cell according to standard measures
such as Kullback-Leibler divergence (Zhai and Laf-
ferty, 2001). Performance is measured both on geo-
tagged Wikipedia articles (Overell, 2009) and tweets
(Eisenstein et al, 2010). We obtain high accuracy on
Wikipedia using KL divergence, with a median error
of just 11.8 kilometers. For the Twitter data set, we
obtain a median error of 479 km, which improves
on the 494 km error of Eisenstein et al An advan-
tage of our approach is that it is far simpler, is easy
to implement, and scales straightforwardly to large
datasets like Wikipedia.
2 Data
Wikipedia As of April 15, 2011, Wikipedia has
some 18.4 million content-bearing articles in 281
language-specific encyclopedias. Among these, 39
have over 100,000 articles, including 3.61 mil-
lion articles in the English-language edition alone.
Wikipedia articles generally cover a single subject;
in addition, most articles that refer to geographically
1We became aware of Serdyukov et al (2009) during the
writing of the camera-ready version of this paper.
fixed subjects are geotagged with their coordinates.
Such articles are well-suited as a source of super-
vised content for document geolocation purposes.
Furthermore, the existence of versions in multiple
languages means that the techniques in this paper
can easily be extended to cover documents written
in many of the world?s most common languages.
Wikipedia?s geotagged articles encompass more
than just cities, geographic formations and land-
marks. For example, articles for events (like the
shooting of JFK) and vehicles (such as the frigate
USS Constitution) are geotagged. The latter type
of article is actually quite challenging to geolocate
based on the text content: though the ship is moored
in Boston, most of the page discusses its role in var-
ious battles along the eastern seaboard of the USA.
However, such articles make up only a small fraction
of the geotagged articles.
For the experiments in this paper, we used a full
dump of Wikipedia from September 4, 2010.2 In-
cluded in this dump is a total of 10,355,226 articles,
of which 1,019,490 have been geotagged. Excluding
various types of special-purpose articles used pri-
marily for maintaining the site (specifically, redirect
articles and articles outside the main namespace),
the dump includes 3,431,722 content-bearing arti-
cles, of which 488,269 are geotagged.
It is necessary to process the raw dump to ob-
tain the plain text, as well as metadata such as geo-
tagged coordinates. Extracting the coordinates, for
example, is not a trivial task, as coordinates can
be specified using multiple templates and in mul-
tiple formats. Automatically-processed versions of
the English-language Wikipedia site are provided by
Metaweb,3 which at first glance promised to signif-
icantly simplify the preprocessing. Unfortunately,
these versions still need significant processing and
they incorrectly eliminate some of the important
metadata. In the end, we wrote our own code to
process the raw dump. It should be possible to ex-
tend this code to handle other languages with little
difficulty. See Lieberman and Lin (2009) for more
discussion of a related effort to extract and use the
geotagged articles in Wikipedia.
The entire set of articles was split 80/10/10 in
2http://download.wikimedia.org/enwiki/
20100904/pages-articles.xml.bz2
3http://download.freebase.com/wex/
956
round-robin fashion into training, development, and
testing sets after randomizing the order of the arti-
cles, which preserved the proportion of geotagged
articles. Running on the full data set is time-
consuming, so development was done on a subset
of about 80,000 articles (19.9 million tokens) as a
training set and 500 articles as a development set.
Final evaluation was done on the full dataset, which
includes 390,574 training articles (97.2 million to-
kens) and 48,589 test articles. A full run with all the
six strategies described below (three baseline, three
non-baseline) required about 4 months of computing
time and about 10-16 GB of RAM when run on a 64-
bit Intel Xeon E5540 CPU; we completed such jobs
in under two days (wall clock) using the Longhorn
cluster at the Texas Advanced Computing Center.
Geo-tagged Microblog Corpus As a second eval-
uation corpus on a different domain, we use the
corpus of geotagged tweets collected and used by
Eisenstein et al (2010).4 It contains 380,000 mes-
sages from 9,500 users tweeting within the 48 states
of the continental USA.
We use the train/dev/test splits provided with the
data; for these, the tweets of each user (a feed) have
been concatenated to form a single document, and
the location label associated with each document is
the location of the first tweet by that user. This is
generally a fair assumption as Twitter users typically
tweet within a relatively small region. Given this
setup, we will refer to Twitter users as documents in
what follows; this keeps the terminology consistent
with Wikipedia as well. The training split has 5,685
documents (1.58 million tokens).
Replication Our code (part of the TextGrounder
system), our processed version of Wikipedia, and in-
structions for replicating our experiments are avail-
able on the TextGrounder website.5
3 Grid representation for connecting texts
to locations
Geolocation involves identifying some spatial re-
gion with a unit of text?be it a word, phrase, or
document. The earth?s surface is continuous, so a
4http://www.ark.cs.cmu.edu/GeoText/
5http://code.google.com/p/textgrounder/
wiki/WingBaldridge2011
natural approach is to predict locations using a con-
tinuous distribution. For example, Eisenstein et al
(2010) use Gaussian distributions to model the loca-
tions of Twitter users in the United States of Amer-
ica. This appears to work reasonably well for that
restricted region, but is likely to run into problems
when predicting locations for anywhere on earth?
instead, spherical distributions like the von Mises-
Fisher distribution would need to be employed.
We take here the simpler alternative of discretiz-
ing the earth?s surface with a geodesic grid; this al-
lows us to predict locations with a variety of stan-
dard approaches over discrete outcomes. There are
many ways of constructing geodesic grids. Like
Serdyukov et al (2009), we use the simplest strat-
egy: a grid of square cells of equal degree, such as
1? by 1?. This produces variable-size regions that
shrink latitudinally, becoming progressively smaller
and more elongated the closer they get towards the
poles. Other strategies, such as the quaternary trian-
gular mesh (Dutton, 1996), preserve equal area, but
are considerably more complex to implement. Given
that most of the populated regions of interest for us
are closer to the equator than not and that we use
cells of quite fine granularity (down to 0.05?), the
simple grid system was preferable.
With such a discrete representation of the earth?s
surface, there are four distributions that form the
core of all our geolocation methods. The first is a
standard multinomial distribution over the vocabu-
lary for every cell in the grid. Given a grid G with
cells ci and a vocabulary V with words wj , we have
?cij = P (wj |ci). The second distribution is the
equivalent distribution for a single test document dk,
i.e. ?dkj = P (wj |dk). The third distribution is the
reverse of the first: for a given word, its distribution
over the earth?s cells, ?ji = P (ci|wj). The final dis-
tribution is over the cells, ?i = P (ci).
This grid representation ignores all higher level
regions, such as states, countries, rivers, and moun-
tain ranges, but it is consistent with the geocod-
ing in both the Wikipedia and Twitter datasets.
Nonetheless, note that the ?ji for words referring
to such regions is likely to be much flatter (spread
out) but with most of the mass concentrated in a
set of connected cells. Those for highly focused
point-locations will jam up in a few disconnected
cells?in the extreme case, toponyms like Spring-
957
field which are connected to many specific point lo-
cations around the earth.
We use grids with cell sizes of varying granular-
ity d?d for d = 0.1?, 0.5?, 1?, 5?, 10?. For example,
with d=0.5?, a cell at the equator is roughly 56x55
km and at 45? latitude it is 39x55 km. At this reso-
lution, there are a total of 259,200 cells, of which
35,750 are non-empty when using our Wikipedia
training set. For comparison, at the equator a cell
at d=5? is about 557x553 km (2,592 cells; 1,747
non-empty) and at d=0.1? a cell is about 11.3x10.6
km (6,480,000 cells; 170,005 non-empty).
The geolocation methods predict a cell c? for a
document, and the latitude and longitude of the
degree-midpoint of the cell is used as the predicted
location. Prediction error is the great-circle distance
from these predicted locations to the locations given
by the gold standard. The use of cell midpoints pro-
vides a fair comparison for predictions with differ-
ent cell sizes. This differs from the evaluation met-
rics used by Serdyukov et al (2009), which are all
computed relative to a given grid size. With their
metrics, results for different granularities cannot be
directly compared because using larger cells means
less ambiguity when choosing c?. With our distance-
based evaluation, large cells are penalized by the dis-
tance from the midpoint to the actual location even
when that location is in the same cell. Smaller cells
reduce this penalty and permit the word distributions
?cij to be much more specific for each cell, but they
are harder to predict exactly and suffer more from
sparse word counts compared to courser granular-
ity. For large datasets like Wikipedia, fine-grained
grids work very well, but the trade-off between reso-
lution and sufficient training material shows up more
clearly for the smaller Twitter dataset.
4 Supervised models for document
geolocation
Our methods use only the text in the documents; pre-
dictions are made based on the distributions ?, ?, and
? introduced in the previous section. No use is made
of metadata, such as links/followers and infoboxes.
4.1 Supervision
We acquire ? and ? straightforwardly from the train-
ing material. The unsmoothed estimate of word wj?s
probability in a test document dk is:6
??dkj =
#(wj , dk)
?
wl?V
#(wl, dk)
(1)
Similarly for a cell ci, we compute the unsmoothed
word distribution by aggregating all of the docu-
ments located within ci:
??cij =
?
dk?ci
#(wj , dk)
?
dk?ci
?
wl?V
#(wl, dk)
(2)
We compute the global distribution ?Dj over the set
of all documents D in the same fashion.
The word distribution of document dk backs off
to the global distribution ?Dj . The probability mass
?dk reserved for unseen words is determined by the
empirical probability of having seen a word once in
the document, motivated by Good-Turing smooth-
ing. (The cell distributions are treated analogously.)
That is:7
?dk =
|wj ? V s.t.#(wj , dk)=1|
?
wj?V
#(wj , dk)
(3)
?(?dk)Dj =
?Dj
1? ?
wl?dk
?Dl
(4)
?dkj =
{
?dk?
(?dk)
Dj , if ??dkj = 0
(1??dk)??dkj, o.w.
(5)
The distributions over cells for each word simply
renormalizes the ?cij values to achieve a proper dis-
tribution:
?ji =
?cij
?
ci?G
?cij
(6)
A useful aspect of the ? distributions is that they can
be plotted in a geobrowser using thematic mapping
6We use #() to indicate the count of an event.
7?(?dk)Dj is an adjusted version of ?Dj that is normalized over
the subset of words not found in document dk. This adjustment
ensures that the entire distribution is properly normalized.
958
techniques (Sandvik, 2008) to inspect the spread of
a word over the earth. We used this as a simple way
to verify the basic hypothesis that words that do not
name locations are still useful for geolocation. In-
deed, the Wikipedia distribution for mountain shows
high density over the Rocky Mountains, Smokey
Mountains, the Alps, and other ranges, while beach
has high density in coastal areas. Words without
inherent locational properties also have intuitively
correct distributions: e.g., barbecue has high den-
sity over the south-eastern United States, Texas, Ja-
maica, and Australia, while wine is concentrated in
France, Spain, Italy, Chile, Argentina, California,
South Africa, and Australia.8
Finally, the cell distributions are simply the rela-
tive frequency of the number of documents in each
cell: ?i = |ci||D| .
A standard set of stop words are ignored. Also,
all words are lowercased except in the case of the
most-common-toponym baselines, where uppercase
words serve as a fallback in case a toponym cannot
be located in the article.
4.2 Kullback-Leibler divergence
Given the distributions for each cell, ?ci , in the grid,
we use an information retrieval approach to choose
a location for a test document dk: compute the sim-
ilarity between its word distribution ?dk and that of
each cell, and then choose the closest one. Kullback-
Leibler (KL) divergence is a natural choice for this
(Zhai and Lafferty, 2001). For distribution P and Q,
KL divergence is defined as:
KL(P ||Q) =
?
i
P (i) log P (i)Q(i) (7)
This quantity measures how good Q is as an encod-
ing for P ? the smaller it is the better. The best cell
c?KL is the one which provides the best encoding for
the test document:
c?KL = argmin
ci?G
KL(?dk ||?ci) (8)
The fact that KL is not symmetric is desired here:
the other direction, KL(?ci||?dk), asks which cell
8This also acts as an exploratory tool. For example, due to
a big spike on Cebu Province in the Philippines we learned that
Cebuanos take barbecue very, very seriously.
the test document is a good encoding for. With
KL(?dk ||?ci), the log ratio of probabilities for each
word is weighted by the probability of the word in
the test document, ?dkj log
?dkj
?cij
, which means that
the divergence is more sensitive to the document
rather than the overall cell.
As an example for why non-symmetric KL in this
order is appropriate, consider geolocating a page in
a densely geotagged cell, such as the page for the
Washington Monument. The distribution of the cell
containing the monument will represent the words
from many other pages having to do with muse-
ums, US government, corporate buildings, and other
nearby memorials and will have relatively small val-
ues for many of the words that are highly indicative
of the monument?s location. Many of those words
appear only once in the monument?s page, but this
will still be a higher value than for the cell and will
weight the contribution accordingly.
Rather than computing KL(?dk ||?ci) over the en-
tire vocabulary, we restrict it to only the words in the
document to compute KL more efficiently:
KL(?dk ||?ci) =
?
wj?Vdk
?dkj log
?dkj
?cij
(9)
Early experiments showed that it makes no differ-
ence in the outcome to include the rest of the vocab-
ulary. Note that because ?ci is smoothed, there are
no zeros, so this value is always defined.
4.3 Naive Bayes
Naive Bayes is a natural generative model for the
task of choosing a cell, given the distributions ?ci
and ?: to generate a document, choose a cell ci ac-
cording to ? and then choose the words in the docu-
ment according to ?ci :
c?NB = argmax
ci?G
PNB(ci|dk)
= argmax
ci?G
P (ci)P (dk|ci)
P (dk)
= argmax
ci?G
?i
?
wj?Vdk
?#(wj ,dk)cij (10)
959
This method maximizes the combination of the like-
lihood of the document P (dk|ci) and the cell prior
probability ?i.
4.4 Average cell probability
For each word, ?ji gives the probability of each cell
in the grid. A simple way to compute a distribution
for a document dk is to take a weighted average of
the distributions for all words to compute the aver-
age cell probability (ACP):
c?ACP = argmax
ci?G
PACP (ci|dk)
= argmax
ci?G
?
wj?Vdk
#(wj , dk)?ji
?
cl?G
?
wj?Vdk
#(wj , dk)?jl
= argmax
ci?G
?
wj?Vdk
#(wj , dk)?ji (11)
This method, despite its conceptual simplicity,
works well in practice. It could also be easily
modified to use different weights for words, such
as TF/IDF or relative frequency ratios between ge-
olocated documents and non-geolocated documents,
which we intend to try in future work.
4.5 Baselines
There are several natural baselines to use for com-
parison against the methods described above.
Random Choose c?rand randomly from a uniform
distribution over the entire grid G.
Cell prior maximum Choose the cell with the
highest prior probability according to ?: c?cpm =
argmaxci?G ?i.
Most frequent toponym Identify the most fre-
quent toponym in the article and the geotagged
Wikipedia articles that match it. Then identify
which of those articles has the most incoming links
(a measure of its prominence), and then choose c?mft
to be the cell that contains the geotagged location for
that article. This is a strong baseline method, but can
only be used with Wikipedia.
Note that a toponym matches an article (or equiv-
alently, the article is a candidate for the toponym) ei-
ther if the toponym is the same as the article?s title,
0
20
0
40
0
60
0
80
0
10
00
12
00
14
00
grid size (degrees)
m
e
a
n
 e
rr
o
r 
(km
)
0.1 0.5 1 5 10
Most frequent toponym
Avg. cell probability
Naive Bayes
Kullback?Leibler
Figure 1: Plot of grid resolution in degrees versus mean
error for each method on the Wikipedia dev set.
or the same as the title after a parenthetical tag or
comma-separated higher-level division is removed.
For example, the toponym Tucson would match ar-
ticles named Tucson, Tucson (city) or Tucson, Ari-
zona. In this fashion, the set of toponyms, and the
list of candidates for each toponym, is generated
from the set of all geotagged Wikipedia articles.
5 Experiments
The approaches described in the previous section
are evaluated on both the geotagged Wikipedia and
Twitter datasets. Given a predicted cell c? for a docu-
ment, the prediction error is the great-circle distance
between the true location and the center of c?, as de-
scribed in section 3.
Grid resolution and thresholding The major pa-
rameter of all our methods is the grid resolution.
For both Wikipedia and Twitter, preliminary ex-
periments on the development set were run to plot
the prediction error for each method for each level
of resolution, and the optimal resolution for each
method was chosen for obtaining test results. For the
Twitter dataset, an additional parameter is a thresh-
old on the number of feeds each word occurs in: in
the preprocessed splits of Eisenstein et al (2010), all
vocabulary items that appear in fewer than 40 feeds
are ignored. This thresholding takes away a lot of
very useful material; e.g. in the first feed, it removes
960
Figure 2: Histograms of distribution of error distances (in
km) for grid size 0.5? for each method on the Wikipedia
dev set.
both ?kirkland? and ?redmond? (towns in the East-
side of Lake Washington near Seattle), very useful
information for geolocating that user. This suggests
that a lower threshold would be better, and this is
borne out by our experiments.
Figure 1 graphs the mean error of each method for
different resolutions on the Wikipedia dev set, and
Figure 2 graphs the distribution of error distances
for grid size 0.5? for each method on the Wikipedia
dev set. These results indicate that a grid size even
smaller than 0.1? might be beneficial. To test this,
we ran experiments using a grid size of 0.05? and
0.01? using KL divergence. The mean errors on the
dev set increased slightly, from 323 km to 348 and
329 km, respectively, indicating that 0.1? is indeed
the minimum.
For the Twitter dataset, we considered both grid
size and vocabulary threshold. We recomputed the
distributions using several values for both parame-
ters and evaluated on the development set. Table 1
shows mean prediction error using KL divergence,
for various combinations of threshold and grid size.
Similar tables were constructed for the other strate-
gies. Clearly, the larger grid size of 5? is more op-
timal than the 0.1? best for Wikipedia. This is un-
surprising, given the small size of the corpus. Over-
all, there is a less clear trend for the other methods
Grid size (degrees)
Thr. 0.1 0.5 1 5 10
0 1113.1 996.8 1005.1 969.3 1052.5
2 1018.5 959.5 944.6 911.2 1021.6
3 1027.6 940.8 954.0 913.6 1026.2
5 1011.7 951.0 954.2 892.0 1013.0
10 1011.3 968.8 938.5 929.8 1048.0
20 1032.5 987.3 966.0 940.0 1070.1
40 1080.8 1031.5 998.6 981.8 1127.8
Table 1: Mean prediction error (km) on the Twitter dev
set for various combinations of vocabulary threshold (in
feeds) and grid size, using the KL divergence strategy.
in terms of optimal resolution. Our interpretation
of this is that there is greater sparsity for the Twit-
ter dataset, and thus it is more sensitive to arbitrary
aspects of how different user feeds are captured in
different cells at different granularities.
For the non-baseline strategies, a threshold be-
tween about 2 and 5 was best, although no one value
in this range was clearly better than another.
Results Based on the optimal resolutions for each
method, Table 2 provides the median and mean er-
rors of the methods for both datasets, when run on
the test sets. The results clearly show that KL di-
vergence does the best of all the methods consid-
ered, with Naive Bayes a close second. Prediction
on Wikipedia is very good, with a median value of
11.8 km. Error on Twitter is much higher at 479 km.
Nonetheless, this beats Eisenstein et al?s (2010) me-
dian results, though our mean is worse at 967. Us-
ing the same threshold of 40 as Eisenstein et al, our
results using KL divergence are slightly worse than
theirs: median error of 516 km and mean of 986 km.
The difference between Wikipedia and Twitter is
unsurprising for several reasons. Wikipedia articles
tend to use a lot of toponyms and words that corre-
late strongly with particular places while many, per-
haps most, tweets discuss quotidian details such as
what the user ate for lunch. Second, Wikipedia arti-
cles are generally longer and thus provide more text
to base predictions on. Finally, there are orders of
magnitude more training examples for Wikipedia,
which allows for greater grid resolution and thus
more precise location predictions.
961
Wikipedia Twitter
Strategy Degree Median Mean Threshold Degree Median Mean
Kullback-Leibler 0.1 11.8 221 5 5 479 967
Naive Bayes 0.1 15.5 314 5 5 528 989
Avg. cell probability 0.1 24.1 1421 2 10 659 1184
Most frequent toponym 0.5 136 1927 - - - -
Cell prior maximum 5 2333 4309 N/A 0.1 726 1141
Random 0.1 7259 7192 20 0.1 1217 1588
Eisenstein et al - - - 40 N/A 494 900
Table 2: Prediction error (km) on the Wikipedia and Twitter test sets for each of the strategies using the optimal grid
resolution and (for Twitter) the optimal threshold, as determined by performance on the corresponding development
sets. Eisenstein et al (2010) used a fixed Twitter threshold of 40. Threshold makes no difference for cell prior
maximum.
Ships One of the most difficult types of Wikipedia
pages to disambiguate are those of ships that either
are stored or had sunk at a particular location. These
articles tend to discuss the exploits of these ships,
not their final resting places. Location error on these
is usually quite large. However, prediction is quite
good for ships that were sunk in particular battles
which are described in detail on the page; examples
are the USS Gambier Bay, USS Hammann (DD-
412), and the HMS Majestic (1895). Another situa-
tion that gives good results is when a ship is retired
in a location where it is a prominent feature and is
thus mentioned in the training set at that location.
An example is the USS Turner Joy, which is in Bre-
merton, Washington and figures prominently in the
page for Bremerton (which is in the training set).
Another interesting aspect of geolocating ship ar-
ticles is that ships tend to end up sunk in remote bat-
tle locations, such that their article is the only one
located in the cell covering the location in the train-
ing set. Ship terminology thus dominates such cells,
with the effect that our models often (incorrectly)
geolocate test articles about other ships to such loca-
tions (and often about ships with similar properties).
This also leads to generally more accurate geoloca-
tion of HMS ships over USS ships; the former seem
to have been sunk in more concentrated regions that
are themselves less spread out globally.
6 Related work
Lieberman and Lin (2009) also work with geotagged
Wikipedia articles, but they do in order so to ana-
lyze the likely locations of users who edit such ar-
ticles. Other researchers have investigated the use
of Wikipedia as a source of data for other super-
vised NLP tasks. Mihalcea and colleagues have in-
vestigated the use of Wikipedia in conjunction with
word sense disambiguation (Mihalcea, 2007), key-
word extraction and linking (Mihalcea and Csomai,
2007) and topic identification (Coursey et al, 2009;
Coursey and Mihalcea, 2009). Cucerzan (2007)
used Wikipedia to do named entity disambiguation,
i.e. identification and coreferencing of named enti-
ties by linking them to the Wikipedia article describ-
ing the entity.
Some approaches to document geolocation rely
largely or entirely on non-textual metadata, which
is often unavailable for many corpora of interest,
Nonetheless, our methods could be combined with
such methods when such metadata is available. For
example, given that both Wikipedia and Twitter have
a linked structure between documents, it would be
possible to use the link-based method given in Back-
strom et al (2010) for predicting the location of
Facebook users based on their friends? locations. It
is possible that combining their approach with our
text-based approach would provide improvements
for Facebook, Twitter and Wikipedia datasets. For
example, their method performs poorly for users
with few geolocated friends, but results improved
by combining link-based predictions with IP address
predictions. The text written users? updates could be
an additional aid for locating such users.
962
7 Conclusion
We have shown that automatic identification of the
location of a document based only on its text can be
performed with high accuracy using simple super-
vised methods and a discrete grid representation of
the earth?s surface. All of our methods are simple
to implement, and both training and testing can be
easily parallelized. Our most effective geolocation
strategy finds the grid cell whose word distribution
has the smallest KL divergence from that of the test
document, and easily beats several effective base-
lines. We predict the location of Wikipedia pages
to a median error of 11.8 km and mean error of 221
km. For Twitter, we obtain a median error of 479
km and mean error of 967 km. Using naive Bayes
and a simple averaging of word-level cell distribu-
tions also both worked well; however, KL was more
effective, we believe, because it weights the words
in the document most heavily, and thus puts less im-
portance on the less specific word distributions of
each cell.
Though we only use text, link-based predictions
using the follower graph, as Backstrom et al (2010)
do for Facebook, could improve results on the Twit-
ter task considered here. It could also help with
Wikipedia, especially for buildings: for example,
the page for Independence Hall in Philadelphia links
to geotagged ?friend? pages for Philadelphia, the
Liberty Bell, and many other nearby locations and
buildings. However, we note that we are still pri-
marily interested in geolocation with only text be-
cause there are a great many situations in which such
linked structure is unavailable. This is especially
true for historical corpora like those made available
by the Perseus project.9
The task of identifying a single location for an en-
tire document provides a convenient way of evaluat-
ing approaches for connecting texts with locations,
but it is not fully coherent in the context of docu-
ments that cover multiple locations. Nonetheless,
both the average cell probability and naive Bayes
models output a distribution over all cells, which
could be used to assign multiple locations. Further-
more, these cell distributions could additionally be
used to define a document level prior for resolution
of individual toponyms.
9www.perseus.tufts.edu/
Though we treated the grid resolution as a param-
eter, the grids themselves form a hierarchy of cells
containing finer-grained cells. Given this, there are
a number of obvious ways to combine predictions
from different resolutions. For example, given a cell
of the finest grain, the average cell probability and
naive Bayes models could successively back off to
the values produced by their coarser-grained con-
taining cells, and KL divergence could be summed
from finest-to-coarsest grain. Another strategy for
making models less sensitive to grid resolution is to
smooth the per-cell word distributions over neigh-
boring cells; this strategy improved results on Flickr
photo geolocation for Serdyukov et al (2009).
An additional area to explore is to remove the
bag-of-words assumption and take into account the
ordering between words. This should have a num-
ber of obvious benefits, among which are sensitivity
to multi-word toponyms such as New York, colloca-
tions such as London, Ontario or London in Ontario,
and highly indicative terms such as egg cream that
are made up of generic constituents.
Acknowledgments
This research was supported by a grant from the
Morris Memorial Trust Fund of the New York Com-
munity Trust and from the Longhorn Innovation
Fund for Technology. This paper benefited from re-
viewer comments and from discussion in the Natu-
ral Language Learning reading group at UT Austin,
with particular thanks to Matt Lease.
References
Geoffrey Andogah. 2010. Geographically Constrained
Information Retrieval. Ph.D. thesis, University of
Groningen, Groningen, Netherlands, May.
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
the 19th international conference on World wide web,
WWW ?10, pages 61?70, New York, NY, USA. ACM.
Kino Coursey and Rada Mihalcea. 2009. Topic identi-
fication using wikipedia graph centrality. In Proceed-
ings of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
Association for Computational Linguistics, Compan-
ion Volume: Short Papers, NAACL ?09, pages 117?
963
120, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Kino Coursey, Rada Mihalcea, and William Moen. 2009.
Using encyclopedic knowledge for automatic topic
identification. In Proceedings of the Thirteenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL ?09, pages 210?218, Morristown, NJ,
USA. Association for Computational Linguistics.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
708?716, Prague, Czech Republic, June. Association
for Computational Linguistics.
Junyan Ding, Luis Gravano, and Narayanan Shivaku-
mar. 2000. Computing geographical scopes of web re-
sources. In Proceedings of the 26th International Con-
ference on Very Large Data Bases, VLDB ?00, pages
545?556, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
G. Dutton. 1996. Encoding and handling geospatial data
with hierarchical triangular meshes. In M.J. Kraak and
M. Molenaar, editors, Advances in GIS Research II,
pages 505?518, London. Taylor and Francis.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1277?1287, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Qiang Hao, Rui Cai, Changhu Wang, Rong Xiao, Jiang-
Ming Yang, Yanwei Pang, and Lei Zhang. 2010.
Equip tourists with knowledge mined from travel-
ogues. In Proceedings of the 19th international con-
ference on World wide web, WWW ?10, pages 401?
410, New York, NY, USA. ACM.
Jochen L. Leidner. 2008. Toponym Resolution in Text:
Annotation, Evaluation and Applications of Spatial
Grounding of Place Names. Dissertation.Com, Jan-
uary.
M. D. Lieberman and J. Lin. 2009. You are where you
edit: Locating Wikipedia users through edit histories.
In ICWSM?09: Proceedings of the 3rd International
AAAI Conference on Weblogs and Social Media, pages
106?113, San Jose, CA, May.
Bruno Martins. 2009. Geographically Aware Web Text
Mining. Ph.D. thesis, University of Lisbon.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, pages 233?242, New York, NY, USA.
ACM.
Rada Mihalcea. 2007. Using Wikipedia for Auto-
matic Word Sense Disambiguation. In North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL 2007).
Simon Overell. 2009. Geographic Information Re-
trieval: Classification, Disambiguation and Mod-
elling. Ph.D. thesis, Imperial College London.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ?98, pages 275?281, New York,
NY, USA. ACM.
Erik Rauch, Michael Bukatin, and Kenneth Baker. 2003.
A confidence-based framework for disambiguating ge-
ographic terms. In Proceedings of the HLT-NAACL
2003 workshop on Analysis of geographic references
- Volume 1, HLT-NAACL-GEOREF ?03, pages 50?54,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bjorn Sandvik. 2008. Using KML for thematic mapping.
Master?s thesis, The University of Edinburgh.
Pavel Serdyukov, Vanessa Murdock, and Roelof van
Zwol. 2009. Placing flickr photos on a map. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?09, pages 484?491, New York, NY,
USA. ACM.
David A. Smith and Gregory Crane. 2001. Disam-
biguating geographic names in a historical digital li-
brary. In Proceedings of the 5th European Confer-
ence on Research and Advanced Technology for Digi-
tal Libraries, ECDL ?01, pages 127?136, London, UK.
Springer-Verlag.
B. E. Teitler, M. D. Lieberman, D. Panozzo, J. Sankara-
narayanan, H. Samet, and J. Sperling. 2008. News-
Stand: A new view on news. In GIS?08: Proceedings
of the 16th ACM SIGSPATIAL International Confer-
ence on Advances in Geographic Information Systems,
pages 144?153, Irvine, CA, November.
Chengxiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, CIKM ?01, pages 403?410, New York, NY,
USA. ACM.
964
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1077?1086,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Simple Unsupervised Grammar Induction
from Raw Text with Cascaded Finite State Models
Elias Ponvert, Jason Baldridge and Katrin Erk
Department of Linguistics
The University of Texas at Austin
Austin, TX 78712
{ponvert,jbaldrid,katrin.erk}@mail.utexas.edu
Abstract
We consider a new subproblem of unsuper-
vised parsing from raw text, unsupervised par-
tial parsing?the unsupervised version of text
chunking. We show that addressing this task
directly, using probabilistic finite-state meth-
ods, produces better results than relying on
the local predictions of a current best unsu-
pervised parser, Seginer?s (2007) CCL. These
finite-state models are combined in a cascade
to produce more general (full-sentence) con-
stituent structures; doing so outperforms CCL
by a wide margin in unlabeled PARSEVAL
scores for English, German and Chinese. Fi-
nally, we address the use of phrasal punctua-
tion as a heuristic indicator of phrasal bound-
aries, both in our system and in CCL.
1 Introduction
Unsupervised grammar induction has been an ac-
tive area of research in computational linguistics for
over twenty years (Lari and Young, 1990; Pereira
and Schabes, 1992; Charniak, 1993). Recent work
(Headden III et al, 2009; Cohen and Smith, 2009;
Ha?nig, 2010; Spitkovsky et al, 2010) has largely
built on the dependency model with valence of Klein
and Manning (2004), and is characterized by its re-
liance on gold-standard part-of-speech (POS) anno-
tations: the models are trained on and evaluated us-
ing sequences of POS tags rather than raw tokens.
This is also true for models which are not successors
of Klein and Manning (Bod, 2006; Ha?nig, 2010).
An exception which learns from raw text and
makes no use of POS tags is the common cover links
parser (CCL, Seginer 2007). CCL established state-
of-the-art results for unsupervised constituency pars-
ing from raw text, and it is also incremental and ex-
tremely fast for both learning and parsing. Unfortu-
nately, CCL is a non-probabilistic algorithm based
on a complex set of inter-relating heuristics and a
non-standard (though interesting) representation of
constituent trees. This makes it hard to extend.
Note that although Reichart and Rappoport (2010)
improve on Seginer?s results, they do so by select-
ing training sets to best match the particular test
sentences?CCL itself is used without modification.
Ponvert et al (2010) explore an alternative strat-
egy of unsupervised partial parsing: directly pre-
dicting low-level constituents based solely on word
co-occurrence frequencies. Essentially, this means
segmenting raw text into multiword constituents. In
that paper, we show?somewhat surprisingly?that
CCL?s performance is mostly dependent on its ef-
fectiveness at identifying low-level constituents. In
fact, simply extracting non-hierarchical multiword
constituents from CCL?s output and putting a right-
branching structure over them actually works better
than CCL?s own higher level predictions. This result
suggests that improvements to low-level constituent
prediction will ultimately lead to further gains in
overall constituent parsing.
Here, we present such an improvement by using
probabilistic finite-state models for phrasal segmen-
tation from raw text. The task for these models is
chunking, so we evaluate performance on identifica-
tion of multiword chunks of all constituent types as
well as only noun phrases. Our unsupervised chun-
kers extend straightforwardly to a cascade that pre-
dicts higher levels of constituent structure, similar
to the supervised approach of Brants (1999). This
forms an overall unsupervised parsing system that
outperforms CCL by a wide margin.
1077
Mrs. Ward for one was relieved
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
1
(a) Chunks: (Mrs. Ward), (for one), and (was relieved)
All
came
from
Cray Research
? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
(b) Only one chunk extracted: (Cray Research)
Fig. 1: Examples of constituent chunks extracted from
syntactic trees
2 Data
We use the standard data sets for unsupervised con-
stituency parsing research: for English, the Wall
Street Journal subset of the Penn Treebank-3 (WSJ,
Marcus et al 1999); for German, the Negra corpus
v2 (Krenn et al, 1998); for Chinese, the Penn Chi-
nese Treebank v5.0 (CTB, Palmer et al, 2006). We
lower-case text but otherwise do not alter the raw
text of the corpus. Sentence segmentation and tok-
enization from the treebank is used. As in previous
work, punctuation is not used for evaluation.
In much unsupervised parsing work the test sen-
tences are included in the training material. Like Co-
hen and Smith, Headden III et al, Spitkovsky et al,
we depart from this experimental setup and keep the
evaluation sets blind to the models during training.
For English (WSJ) we use sections 00-22 for train-
ing, section 23 for test and we develop using section
24; for German (Negra) we use the first 18602 sen-
tences for training, the last 1000 sentences for de-
velopment and the penultimate 1000 sentences for
testing; for Chinese (CTB) we adopt the data-split
of Duan et al (2007).
3 Tasks and Benchmark
Evaluation. By unsupervised partial parsing, or
simply unsupervised chunking, we mean the seg-
mentation of raw text into (non-overlapping) multi-
word constituents. The models are intended to cap-
ture local constituent structure ? the lower branches
of a constituent tree. For this reason we evaluate
WSJ
Chunks 203K
NPs 172K
Chnk ? NPs 161K
Negra
Chunks 59K
NPs 33K
Chnk ? NPs 23K
CTB
Chunks 92K
NPs 56K
Chnk ? NPs 43K
Table 1: Constituent chunks and base NPs in the datasets.
% constituents % words
WSJ
Chunks 32.9 57.7
NPs 27.9 53.1
Negra
Chunks 45.4 53.6
NPs 25.5 42.4
CTB
Chunks 32.5 55.4
NPs 19.8 42.9
Table 2: Percentage of gold standard constituents and
words under constituent chunks and base NPs.
using what we call constituent chunks, the subset
of gold standard constituents which are i) branch-
ing (multiword) but ii) non-hierarchical (do not con-
tain subconstituents). We also evaluate our models
based on their performance at identifying base noun
phrases, NPs that do not contain nested NPs.
Examples of constituent chunks extracted from
treebank constituent trees are in Fig. 1. In English
newspaper text, constituent chunks largely corre-
spond with base NPs, but this is less the case with
Chinese and German. Moreover, the relationship be-
tween NPs and constituent chunks is not a subset re-
lation: some base NPs do have internal constituent
structure. The numbers of constituent chunks and
NPs for the training datasets are in Table 1. The per-
centage of constituents in these datasets which fall
under these definitions, and the percentage of words
under these constituents, are in Table 2.
For parsing, the standard unsupervised parsing
metric is unlabeled PARSEVAL. It measures preci-
sion and recall on constituents produced by a parser
as compared to gold standard constituents.
CCL benchmark. We use Seginer?s CCL as a
benchmark for several reasons. First, there is a
free/open-source implementation facilitating exper-
1078
imental replication and comparison.1 More im-
portantly, until recently it was the only unsuper-
vised raw text constituent parser to produce re-
sults competitive with systems which use gold POS
tags (Klein and Manning, 2002; Klein and Man-
ning, 2004; Bod, 2006) ? and the recent improved
raw-text parsing results of Reichart and Rappoport
(2010) make direct use of CCL without modifica-
tion. There are other raw-text parsing systems of
note, EMILE (Adriaans et al, 2000), ABL (van Za-
anen, 2000) and ADIOS (Solan et al, 2005); how-
ever, there is little consistent treebank-based evalu-
ation of these models. One study by Cramer (2007)
found that none of the three performs particularly
well under treebank evaluation. Finally, CCL out-
performs most published POS-based models when
those models are trained on unsupervised word
classes rather than gold POS tags. The only excep-
tion we are aware of is Ha?nig?s (2010) unsuParse+,
which outperforms CCL on Negra, though this is
shown only for sentences with ten or fewer words.
Phrasal punctuation. Though punctuation is usu-
ally entirely ignored in unsupervised parsing re-
search, Seginer (2007) departs from this in one key
aspect: the use of phrasal punctuation ? punctuation
symbols that often mark phrasal boundaries within a
sentence. These are used in two ways: i) they im-
pose a hard constraint on constituent spans, in that
no constituent (other than sentence root) may extend
over a punctuation symbol, and ii) they contribute to
the model, specifically in terms of the statistics of
words seen adjacent to a phrasal boundary. We fol-
low this convention and use the following set:
. ? ! ; , -- ? ?
The last two are ideographic full-stop and comma.2
4 Unsupervised partial parsing
We learn partial parsers as constrained sequence
models over tags encoding local constituent struc-
ture (Ramshaw and Marcus, 1995). A simple tagset
is unlabeled BIO, which is familiar from supervised
chunking and named-entity recognition: the tag B
1http://www.seggu.net/ccl
2This set is essentially that of Seginer (2007). While it is
clear from our analysis of CCL that it does make use of phrasal
punctuation in Chinese, we are not certain whether ideographic
comma is included.
denotes the beginning of a chunk, I denotes mem-
bership in a chunk andO denotes exclusion from any
chunk. In addition we use the tag STOP for sentence
boundaries and phrasal punctuation.
HMMs and PRLGs. The models we use for un-
supervised partial parsing are hidden Markov mod-
els, and a generalization we refer to as probabilis-
tic right linear grammars (PRLGs). An HMM mod-
els a sequence of observed states (words) x =
{x1, x2, . . . , xN} and a corresponding set of hid-
den states y = {y1, y2, . . . , yN}. HMMs may be
thought of as a special case of probabilistic context-
free grammars, where the non-terminal symbols are
the hidden state space, terminals are the observed
states and rules are of the form NONTERM ?
TERM NONTERM (assuming y1 and yN are fixed
and given). So, the emission and transition emanat-
ing from yn would be characterized as a PCFG rule
yn ? xn yn+1. HMMs factor rule probabilities into
emission and transition probabilities:
P (yn ? xn yn+1) = P (xn, yn+1|yn)
? P (xn|yn) P (yn+1|yn).
However, without making this independence as-
sumption, we can model right linear rules directly:
P (xn, yn+1|yn) = P (xn|yn, yn+1) P (yn+1|yn).
So, when we condition emission probabilities on
both the current state yn and the next state yn+1, we
have an exact model. This direct modeling of the
right linear grammar rule yn ? xn yn+1 is what
we call a probabilistic right-linear grammar. To be
clear, a PRLG is just an HMM without the indepen-
dence of emissions and transitions. See Smith and
Johnson (2007) for a discussion, where they refer to
PRLGs as Mealy HMMs.
We use expectation maximization to estimate
model parameters. For the E step, the forward-
backward algorithm (Rabiner, 1989) works identi-
cally for the HMM and PRLG. For the M step, we
use maximum likelihood estimation with additive
smoothing on the emissions probabilities. So, for
the HMM and PRLG models respectively, for words
1079
STOP B
O I
1
Fig. 2: Possible tag transitions as a state diagram.
STOP B I O
STOP .33 .33 .33
B 1
I .25 .25 .25 .25
O .33 .33 .33
Fig. 3: Uniform initialization of transition probabilities
subject to the constraints in Fig. 2: rows correspond to
antecedent state, columns to following state.
w and tags s, t:
P? (w|t) =
C(t, w) + ?
C(t) + ?V
P? (w|s, t) =
C(t, w, s) + ?
C(t, s) + ?V
where C are the soft counts of emissions C(t, w),
rules C(t, w, s) = C(t ? w s), tags C(t) and tran-
sitions C(t, s) calculated during the E step; V is the
number of terms w, and ? is a smoothing parameter.
We fix ? = .1 for all experiments; more sophisti-
cated smoothing could avoid dependence on ?.
We do not smooth transition probabilities (so
P? (s|t) = C(t, s)/C(t)) for two reasons. First, with
four tags, there is no data-sparsity concern with re-
spect to transitions. Second, the nature of the task
imposes certain constraints on transition probabili-
ties: because we are only interested in multiword
chunks, we expressly do not want to generate a B
following a B ? in other words P (B|B) = 0.
These constraints boil down to the observation
that the B and I states will only be seen in BII? se-
quences. This may be expressed via the state transi-
tion diagram in Fig. 2. The constraints of also dic-
tate the initial model input to the EM process. We
use uniform probability distributions subject to the
constraints of Fig. 2. So, initial model transition
probabilities are given in Fig. 3. In EM, if a parame-
ter is equal to zero, subsequent iterations of the EM
process will not ?unset? this parameter; thus, this
form of initialization is a simple way of encoding
constraints on model parameters. We also experi-
mented with random initial models (subject to the
constraints in Fig. 2). Uniform initialization usu-
ally works slightly better; also, uniform initializa-
tion does not require multiple runs of each experi-
ment, as random initialization does.
Motivating the HMMand PRLG. This approach
? encoding a chunking problem as a tagging prob-
lem and learning to tag with HMMs ? goes back
to Ramshaw and Marcus (1995). For unsupervised
learning, the expectation is that the model will learn
to generalize on phrasal boundaries. That is, the
models will learn to associate terms like the and a,
which often occur at the beginnings of sentences and
rarely at the end, with the tag B, which cannot occur
at the end of a sentence. Likewise common nouns
like company or asset, which frequently occur at the
ends of sentences, but rarely at the beginning, will
come to be associated with the I tag, which cannot
occur at the beginning.
The basic motivation for the PRLG is the assump-
tion that information is lost due to the independence
assumption characteristic of the HMM. With so few
states, it is feasible to experiment with the more fine-
grained PRLG model.
Evaluation. Using the low-level predictions of
CCL as as benchmark, we evaluate the HMM and
PRLG chunkers on the tasks of constituent chunk
and base NP identification. Models were initialized
uniformly as illustrated in Fig. 3. Sequence models
learn via EM. We report accuracy only after conver-
gence, that is after the change in full dataset per-
plexity (log inverse probability) is less than %.01
between iterations. Precision, recall and F-score are
reported for full constituent identification ? brack-
ets which do not match the gold standard exactly are
false positives.
Model performance results on held-out test
datasets are reported in Table 3. ?CCL? refers to the
lowest-level constituents extracted from full CCL
output, as a benchmark chunker. The sequence mod-
els outperform the CCL benchmark at both tasks and
on all three datasets. In most cases, the PRLG se-
quence model performs better than the HMM; the
exception is CTB, where the PRLG model is behind
the HMM in evaluation, as well as behind CCL.
As the lowest-level constituents of CCL were not
specifically designed to describe chunks, we also
1080
English / WSJ German / Negra Chinese / CTB
Task Model Prec Rec F Prec Rec F Prec Rec F
Chunking
CCL 57.5 53.5 55.4 28.4 29.6 29.0 23.5 23.9 23.7
HMM 53.8 62.2 57.7 35.0 37.7 36.3 37.4 41.3 39.3
PRLG 76.2 63.9 69.5 39.6 47.8 43.3 23.0 18.3 20.3
NP
CCL 46.2 51.1 48.5 15.6 29.2 20.3 10.4 17.3 13.0
HMM 47.7 65.6 55.2 23.8 46.2 31.4 17.0 30.8 21.9
PRLG 76.8 76.7 76.7 24.6 53.4 33.6 21.9 28.5 24.8
Table 3: Unsupervised chunking results for local constituent structure identification and NP chunking on held-out test
sets. CCL refers to the lowest constituents extracted from CCL output.
WSJ Negra CTB
Chunking 57.8 36.0 25.5
NPs 57.8 38.8 23.2
Table 4: Recall of CCL on the chunking tasks.
checked the recall of all brackets generated by CCL
against gold-standard constituent chunks. The re-
sults are given in Table 4. Even compared to this,
the sequence models? recall is almost always higher.
The sequence models, as well as the CCL bench-
mark, show relatively low precision on the Negra
corpus. One possible reason for this lies in the
design decision of Negra to use relatively flat tree
structures. As a result, many structures that in
other treebanks would be prepositional phrases with
embedded noun phrases ? and thus non-local con-
stituents ? are flat prepositional phrases here. Exam-
ples include ?auf die Wiesbadener Staatsanwaelte?
(on Wiesbaden?s district attorneys) and ?in Han-
novers Nachbarstadt? (in Hannover?s neighbor city).
In fact, in Negra, the sequence model chunkers
often find NPs embedded in PPs, which are not an-
notated as such. For instance, in the PP ?hinter den
Kulissen? (behind the scenes), both the PRLG and
HMM chunkers identify the internal NP, though this
is not identified in Negra and thus considered a false
positive. The fact that the HMM and PRLG have
higher recall on NP identification on Negra than pre-
cision is further evidence towards this.
Comparing the HMM and PRLG. To outline
some of the factors differentiating the HMM and
PRLG, we focus on NP identification in WSJ.
The PRLG has higher precision than the HMM,
while the two models are closer in recall. Com-
paring the predictions directly, the two models of-
POS Sequence # of errors
TO VB 673
NNP NNP 450
MD VB 407
DT JJ 368
DT NN 280
Table 5: Top 5 POS sequences of the false positives pre-
dicted by the HMM.
ten have the same correct predictions and often miss
the same gold standard constituents. The improved
results of the PRLG are based mostly on the fewer
overall brackets predicted, and thus fewer false pos-
itives: for WSJ the PRLG incorrectly predicts 2241
NP constituents compared to 6949 for the HMM.
Table 5 illustrates the top 5 POS sequences of the
false positives predicted by the HMM.3 (Recall that
we use gold standard POS only for post-experiment
results analysis?the model itself does not have ac-
cess to them.) By contrast, the sequence represent-
ing the largest class of errors of the PRLG is DT NN,
with 165 errors ? this sequence represents the largest
class of predictions for both models.
Two of the top classes of errors, MD VB and
TO VB, represent verb phrase constituents, which
are often predicted by the HMM chunker, but not
by the PRLG. The class represented by NNP NNP
corresponds with the tendency of the HMM chun-
ker to split long proper names: for example, it sys-
tematically splits new york stock exchange into two
chunks, (new york) (stock exchange), whereas the
PRLG chunker predicts a single four-word chunk.
The most interesting class is DT JJ, which rep-
resents the difficulty the HMM chunker has at dis-
3For the Penn Treebank tagset, see Marcus et al (1993).
1081
1 Start with raw text:
there is no asbestos in our products now
2 Apply chunking model:
there (is no asbestos) in (our products) now
3 Create pseudowords:
there is in our now
4 Apply chunking model (and repeat 1?4 etc.):
(there is ) (in our ) now
5 Unwind and create a tree:
there
is no asbestos
in
our products
now
1Fig. 4: Cascaded chunking illustrated. Pseudowords are
indicated with boxes.
tinguishing determiner-adjective from determiner-
noun pairs. The PRLG chunker systematically gets
DT JJ NN trigrams as chunks. The greater con-
text provided by right branching rules allows the
model to explicitly estimate separate probabilities
forP (I ? recent I) versusP (I ? recent O). That
is, recent within a chunk versus ending a chunk. Bi-
grams like the acquisition allow the model to learn
rules P (B ? the I) and P (I ? acquisition O).
So, the PRLG is better able to correctly pick out the
trigram chunk (the recent acquisition).
5 Constituent parsing with a cascade of
chunkers
We use cascades of chunkers for full constituent
parsing, building hierarchical constituents bottom-
up. After chunking is performed, all multiword con-
stituents are collapsed and represented by a single
pseudoword. We use an extremely simple, but effec-
tive, way to create pseudoword for a chunk: pick the
term in the chunk with the highest corpus frequency,
and mark it as a pseudoword. The sentence is now a
string of symbols (normal words and pseudowords),
to which a subsequent unsupervised chunking model
is applied. This process is illustrated in Fig. 4.
Each chunker in the cascade chunks the raw text,
then regenerates the dataset replacing chunks with
pseudowords; this process is iterated until no new
chunks are found. The separate chunkers in the cas-
Text : Mr. Vinken is chairman of Elsevier N.V.
Level 1 :
Mr. Vinken
is chairman of
Elsevier N.V.
1Level 2 :
Mr. Vinken is chairman
of
Elsevier N.V.
1
Level 3 : Mr. Vinken is chairman of
Elsevier N.V.
1
Fig. 5: PRLG cascaded chunker output.
NPs PPs
Lev 1 Lev 2 Lev 1 Lev 2
WSJ
HMM 66.5 68.1 20.6 70.2
PRLG 77.5 78.3 9.1 77.6
Negra
HMM 54.7 62.3 24.8 48.1
PRLG 61.6 65.2 40.3 44.0
CTB
HMM 33.3 35.4 34.6 38.4
PRLG 30.9 33.6 31.6 47.1
Table 7: NP and PP recall at cascade levels 1 and 2. The
level 1 NP numbers differ from the NP chunking numbers
from Table 3 since they include root-level constituents
which are often NPs.
cade are referred to as levels. In our experiments the
cascade process took a minimum of 5 levels, and a
maximum of 7. All chunkers in the cascade have the
same settings in terms of smoothing, the tagset and
initialization.
Evaluation. Table 6 gives the unlabeled PARSE-
VAL scores for CCL and the two finite-state models.
PRLG achieves the highest F-score for all datasets,
and does so by a wide margin for German and Chi-
nese. CCL does achieve higher recall for English.
While the first level of constituent analysis has
high precision and recall on NPs, the second level
often does well finding prepositional phrases (PPs),
especially in WSJ; see Table 7. This is illustrated
in Fig. 5. This example also illustrates a PP attach-
ment error, which are a common problem for these
models.
We also evaluate using short ? 10-word or less ?
sentences. That said, we maintain the training/test
split from before. Also, making use of the open
1082
Parsing English / WSJ German / Negra Chinese / CTB
Model Prec Rec F Prec Rec F Prec Rec F
CCL 53.6 50.0 51.7 33.4 32.6 33.0 37.0 21.6 27.3
HMM 48.2 43.6 45.8 30.8 50.3 38.2 43.0 29.8 35.2
PRLG 60.0 49.4 54.2 38.8 47.4 42.7 50.4 32.8 39.8
Table 6: Unlabeled PARSEVAL scores for cascaded models.
source implementation by F. Luque,4 we compare
on WSJ and Negra to the constituent context model
(CCM) of Klein and Manning (2002). CCM learns
to predict a set of brackets over a string (in prac-
tice, a string of POS tags) by jointly estimating con-
stituent and distituent strings and contexts using an
iterative EM-like procedure (though, as noted by
Smith and Eisner (2004), CCM is deficient as a gen-
erative model). Note that this comparison is method-
ologically problematic in two respects. On the one
hand, CCM is evaluated using gold standard POS
sequences as input, so it receives a major source of
supervision not available to the other models. On the
other hand, the other models use punctuation as an
indicator of constituent boundaries, but all punctu-
ation is dropped from the input to CCM. Also, note
that CCM performs better when trained on short sen-
tences, so here CCM is trained only on the 10-word-
or-less subsets of the training datasets.5
The results from the cascaded PRLG chunker
are near or better than the best performance by
CCL or CCM in these experiments. These and the
full-length parsing results suggest that the cascaded
chunker strategy generalizes better to longer sen-
tences than does CCL. CCM does very poorly on
longer sentences, but does not have the benefit of us-
ing punctuation, as do the raw text models; unfortu-
nately, further exploration of this trade-off is beyond
the scope of this paper. Finally, note that CCM has
higher recall, and lower precision, generally, than
the raw text models. This is due, in part, to the chart
structure used by CCM in the calculation of con-
stituent and distituent probabilities: as in CKY pars-
ing, the chart structure entails the trees predicted will
be binary-branching. CCL and the cascaded models
can predict higher-branching constituent structures,
4http://www.cs.famaf.unc.edu.ar/
?francolq/en/proyectos/dmvccm/
5This setup is the same as Seginer?s (2007), except the
train/test split.
Prec Rec F
WSJ
CCM 62.4 81.4 70.7
CCL 71.2 73.1 72.1
HMM 64.4 64.7 64.6
PRLG 74.6 66.7 70.5
Negra
CCM 52.4 83.4 64.4
CCL 52.9 54.0 53.0
HMM 47.7 72.0 57.4
PRLG 56.3 72.1 63.2
CTB
CCL 54.4 44.3 48.8
HMM 55.8 53.1 54.4
PRLG 62.7 56.9 59.6
Table 8: Evaluation on 10-word-or-less sentences. CCM
scores are italicized as a reminder that CCM uses gold-
standard POS sequences as input, so its results are not
strictly comparable to the others.
so fewer constituents are predicted overall.
6 Phrasal punctuation revisited
Up to this point, the proposed models for chunking
and parsing use phrasal punctuation as a phrasal sep-
arator, like CCL. We now consider how well these
models perform in absence of this constraint.
Table 9a provides comparison of the sequence
models? performance on the constituent chunking
task without using phrasal punctuation in training
and evaluation. The table shows absolute improve-
ment (+) or decline (?) in precision and recall
when phrasal punctuation is removed from the data.
The punctuation constraint seems to help the chun-
kers some, but not very much; ignoring punctuation
seems to improve chunker results for the HMM on
Chinese. Overall, the effect of phrasal punctuation
on the chunker models? performance is not clear.
The results for cascaded parsing differ strongly
from those for chunking, as Table 9b indicates. Us-
ing phrasal punctuation to constrain bracket predic-
tion has a larger impact on cascaded parsing re-
1083
0 20 40 60
2
2.5
3
3.5
EM Iterations
Le
ng
th
a) Average Predicted Constituent Length
Actual average chunk length
1
0 20 40 60
20
30
40
50
EM Iterations
Pr
ec
isi
on
W/ Punctuation
No Punctuation
b) Chunking Precision
1
0 20 40 60
20
30
40
50
EM Iterations
Pr
ec
isi
on
c) Precision at All Brackets
1
Fig. 6: Behavior of the PRLG model on CTB over the course of EM.
WSJ Negra CTB
Prec Rec Prec Rec Prec Rec
HMM ?5.8 ?9.8 ?0.1 ?0.4 +0.7 +4.9
PRLG ?2.5 ?2.1 ?2.1 ?2.1 ?7.0 +1.2
a) Constituent Chunking
WSJ Negra CTB
Prec Rec Prec Rec Prec Rec
CCL ?14.1 ?13.5 ?10.7 ?4.6 ?11.6 ?6.0
HMM ?7.8 ?8.6 ?2.8 +1.7 ?13.4 ?1.2
PRLG ?10.1 ?7.2 ?4.0 ?4.5 ?22.0 ?11.8
b) (Cascade) Parsing
Table 9: Effects of dropping phrasal punctuation in un-
supervised chunking and parsing evaluations relative to
Tables 3 and 6.
sults almost across the board. This is not surpris-
ing: while performing unsupervised partial parsing
from raw text, the sequence models learn two gen-
eral patterns: i) they learn to chunk rare sequences,
such as named entities, and ii) they learn to chunk
high-frequency function words next to lower fre-
quency content words, which often correlate with
NPs headed by determiners, PPs headed by prepo-
sitions and VPs headed by auxiliaries. When these
patterns are themselves replaced with pseudowords
(see Fig. 4), the models have fewer natural cues to
identify constituents. However, within the degrees
of freedom allowed by punctuation constraints as
described, the chunking models continue to find rel-
atively good constituents.
While CCL makes use of phrasal punctuation in
previously reported results, the open source imple-
mentation allows it to be evaluated without this con-
straint. We did so, and report results in Table 9b.
CCL is, in fact, very sensitive to phrasal punctu-
ation. Comparing CCL to the cascaded chunkers
when none of them use punctuation constraints, the
cascaded chunkers (both HMMs and PRLGs) out-
perform CCL for each evaluation and dataset.
For the CTB dataset, best chunking performance
and cascaded parsing performance flips from the
HMM to the PRLG. More to the point, the PRLG
is actually with worst performing model at the con-
stituent chunking task, but the best performing cas-
cade parser; also, this model has the most serious
degrade in performance when phrasal punctuation is
dropped from input. To investigate, we track the
performance of the chunkers on the development
dataset over iterations of EM. This is illustrated in
Fig. 6 with the PRLG model. First of all, Fig. 6a re-
veals the average length of the constituents predicted
by the PRLG model increases over the course of
EM. However, the average constituent chunk length
is 2.22. So, the PRLG chunker is predicting con-
stituents that are longer than the ones targeted in
the constituent chunking task: regardless of whether
they are legitimate constituents or not, often they
will likely be counted as false positives in this evalu-
ation. This is confirmed by observing the constituent
chunking precision in Fig. 6b, which peaks when
the average predicted constituent length is about the
same the actual average length of those in the eval-
uation. The question, then, is whether the longer
chunks predicted correspond to actual constituents
or not. Fig. 6c shows that the PRLG, when con-
strained by phrasal punctuation, does continue to
improve its constituent prediction accuracy over the
course of EM. These correctly predicted constituents
are not counted as such in the constituent chunking
or base NP evaluations, but they factor directly into
1084
improved accuracy when this model is part of a cas-
cade.
7 Related work
Our task is the unsupervised analogue of chunking
(Abney, 1991), popularized by the 1999 and 2000
Conference on Natural Language Learning shared
tasks (Tjong et al, 2000). In fact, our models follow
Ramshaw and Marcus (1995), treating structure pre-
diction as sequence prediction using BIO tagging.
In addition to Seginer?s CCL model, the unsu-
pervised parsing model of Gao and Suzuki (2003)
and Gao et al (2004) also operates on raw text.
Like us, their model gives special treatment to lo-
cal constituents, using a language model to char-
acterize phrases which are linked via a dependency
model. Their output is not evaluated directly using
treebanks, but rather applied to several information
retrieval problems.
In the supervised realm, Hollingshead
et al (2005) compare context-free parsers with
finite-state partial parsing methods. They find that
full parsing maintains a number of benefits, in spite
of the greater training time required: they can train
on less data more effectively than chunkers, and are
more robust to shifts in textual domain.
Brants (1999) reports a supervised cascaded
chunking strategy for parsing which is strikingly
similar to the methods proposed here. In both,
Markov models are used in a cascade to predict hi-
erarchical constituent structure; and in both, the pa-
rameters for the model at each level are estimated
independently. There are major differences, though:
the models here are learned from raw text with-
out tree annotations, using EM to train parameters;
Brants? cascaded Markov models use supervised
maximum likelihood estimation. Secondly, between
the separate levels of the cascade, we collapse con-
stituents into symbols which are treated as tokens
in subsequent chunking levels; the Markov models
in the higher cascade levels in Brants? work actu-
ally emit constituent structure. A related approach
is that of Schuler et al (2010), who report a su-
pervised hierarchical hidden Markov model which
uses a right-corner transform. This allows the model
to predict more complicated trees with fewer levels
than in Brants? work or this paper.
8 Conclusion
In this paper we have introduced a new subprob-
lem of unsupervised parsing: unsupervised partial
parsing, or unsupervised chunking. We have pro-
posed a model for unsupervised chunking from raw
text that is based on standard probabilistic finite-
state methods. This model produces better local
constituent predictions than the current best unsu-
pervised parser, CCL, across datasets in English,
German, and Chinese. By extending these proba-
bilistic finite-state methods in a cascade, we obtain
a general unsupervised parsing model. This model
outperforms CCL in PARSEVAL evaluation on En-
glish, German, and Chinese.
Like CCL, our models operate from raw (albeit
segmented) text, and like it our models decode very
quickly; however, unlike CCL, our models are based
on standard and well-understood computational lin-
guistics technologies (hidden Markov models and
related formalisms), and may benefit from new re-
search into these core technologies. For instance,
our models may be improved by the application
of (unsupervised) discriminative learning techniques
with features (Berg-Kirkpatrick et al, 2010); or by
incorporating topic models and document informa-
tion (Griffiths et al, 2005; Moon et al, 2010).
UPPARSE, the software used for the experiments
in this paper, is available under an open-source li-
cense to facilitate replication and extensions.6
Acknowledgments. This material is based upon
work supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der grant number W911NF-10-1-0533. Support for
the first author was also provided by Mike Hogg En-
dowment Fellowship, the Office of Graduate Studies
at The University of Texas at Austin.
This paper benefited from discussion in the Natu-
ral Language Learning reading group at UT Austin,
especially from Collin Bannard, David Beaver,
Matthew Lease, Taesun Moon and Ray Mooney. We
also thank the three anonymous reviewers for in-
sightful questions and helpful comments.
6 http://elias.ponvert.net/upparse.
1085
References
S. Abney. 1991. Parsing by chunks. In R. Berwick,
S. Abney, and C. Tenny, editors, Principle-based Pars-
ing. Kluwer.
P. W. Adriaans, M. Trautwein, and M. Vervoort. 2000.
Towards high speed grammar induction on large text
corpora. In SOFSEM.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In HLT-NAACL.
R. Bod. 2006. Unsupervised parsing with U-DOP. In
CoNLL.
T. Brants. 1999. Cascaded markov models. In EACL.
E. Charniak. 1993. Statistical Language Learning. MIT.
S. B. Cohen and N. A. Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In HLT-NAACL.
B. Cramer. 2007. Limitations of current grammar induc-
tion algorithms. In ACL-SRW.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic mod-
els for action-based Chinese dependency parsing. In
ECML/PKDD.
J. Gao and H. Suzuki. 2003. Unsupervised learning of
dependency structure for language modeling. In ACL.
J. Gao, J.Y. Nie, G. Wu, and G. Cao. 2004. Dependence
language model for information retrieval. In SIGIR.
T. L. Griffiths, M. Steyvers, D. M. Blei, and J. M. Tenen-
baum. 2005. Integrating topics and syntax. In NIPS.
C. Ha?nig. 2010. Improvements in unsupervised co-
occurence based parsing. In CoNLL.
W. P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency parsing
with richer contexts and smoothing. In HLT-NAACL.
K. Hollingshead, S. Fisher, and B. Roark. 2005. Com-
paring and combining finite-state and context-free
parsers. In HLT-EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In ACL.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency and
constituency. In ACL.
B. Krenn, T. Brants, W. Skut, and Hans Uszkoreit. 1998.
A linguistically interpreted corpus of German newspa-
per text. In Proceedings of the ESSLLI Workshop on
Recent Advances in Corpus Annotation.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech & Language, 4:35 ? 56.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Compuational Linguistics, pages
313?330.
M.P. Marcus, B. Santorini, M.A. Marcinkiewicz, and
A. Taylor, 1999. Treebank-3. LDC.
T. Moon, J. Baldridge, and K. Erk. 2010. Crouching
Dirichlet, hidden Markov model: Unsupervised POS
tagging with context local tag generation. In EMNLP.
M. Palmer, F. D. Chiou, N. Xue, and T. K. Lee, 2005.
Chinese Treebank 5.0. LDC.
F. Pereira and Y. Schabes. 1992. Inside-outside reesti-
mation from paritally bracketed corpora. In ACL.
E. Ponvert, J. Baldridge, and K. Erk. 2010. Simple unsu-
pervised prediction of low-level constituents. In ICSC.
L.R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking
using transformation-based learning. In Proc. of Third
Workshop on Very Large Corpora.
R. Reichart and A. Rappoport. 2010. Improved fully
unsupervised parsing with Zoomed Learning. In
EMNLP.
W. Schuler, S. AbdelRahman, T. Miller, and L. Schwartz.
2010. Broad-coverage parsing using human-like
memory constraints. Compuational Linguistics, 3(1).
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In ACL.
N. A. Smith and J. Eisner. 2004. Annealing techniques
for unsupervised statistical language learning. In ACL.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic CFGs. Computational Lingusitics.
Z. Solan, D. Horn, E. Ruppin, and S. Edelman. 2005.
Unsupervised learning of natural languages. PNAS,
102.
V. I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in
unsupervised dependency parsing. In NAACL-HLT.
E. F. Tjong, K. Sang, and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 Shared Task: Chunking. In
CoNLL-LLL.
M. van Zaanen. 2000. ABL: Alignment-based learning.
In COLING.
1086
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583?592,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Real-World Semi-Supervised Learning
of POS-Taggers for Low-Resource Languages
Dan Garrette1 Jason Mielens2
1Department of Computer Science 2Department of Linguistics
The University of Texas at Austin The University of Texas at Austin
dhg@cs.utexas.edu {jmielens,jbaldrid}@utexas.edu
Jason Baldridge2
Abstract
Developing natural language processing
tools for low-resource languages often re-
quires creating resources from scratch.
While a variety of semi-supervised meth-
ods exist for training from incomplete
data, there are open questions regarding
what types of training data should be used
and how much is necessary. We dis-
cuss a series of experiments designed to
shed light on such questions in the con-
text of part-of-speech tagging. We obtain
timed annotations from linguists for the
low-resource languages Kinyarwanda and
Malagasy (as well as English) and eval-
uate how the amounts of various kinds
of data affect performance of a trained
POS-tagger. Our results show that an-
notation of word types is the most im-
portant, provided a sufficiently capable
semi-supervised learning infrastructure is
in place to project type information onto
a raw corpus. We also show that finite-
state morphological analyzers are effective
sources of type information when few la-
beled examples are available.
1 Introduction
Low-resource languages present a particularly dif-
ficult challenge for natural language processing
tasks. For example, supervised learning meth-
ods can provide high accuracy for part-of-speech
(POS) tagging (Manning, 2011), but they per-
form poorly when little supervision is avail-
able. Good results in weakly-supervised tagging
have been obtained by training sequence models
such as hidden Markov models (HMM) using the
Expectation-Maximization algorithm (EM), how-
ever most work in this area has still relied on rel-
atively large amounts of data, both annotated and
unannotated, as well as an assumption that the an-
notations are very clean (Kupiec, 1992; Merialdo,
1994).
The ability to learn taggers using very little data
is enticing: only a tiny fraction of the world?s lan-
guages have enough data for standard supervised
models to work well. The collection or develop-
ment of resources is a time-consuming and expen-
sive process, creating a significant barrier for an
under-studied language where there are few ex-
perts and little funding. It is thus important to
develop approaches that achieve good accuracy
based on the amount of data that can be reasonably
obtained, for example, in just a few hours by a lin-
guist doing fieldwork on a non-native language.
Previous work explored learning taggers from
weak information, but the type, amount, quality,
and sources of data raise questions about the appli-
cability of those results to real-world low-resource
scenarios (Toutanova and Johnson, 2008; Ravi and
Knight, 2009; Hasan and Ng, 2009; Garrette and
Baldridge, 2012). Most research simulated weak
supervision with tag dictionaries extracted from
existing large, expertly-annotated corpora. These
resources have been developed over long periods
of time by trained annotators who collaborate to
produce high-quality analyses. They are also bi-
ased towards including only the most likely tag
for each word type, resulting in a cleaner dictio-
nary than one would find in a real scenario. As
such, these experiments do not reflect real-world
constraints.
One exception to this work is Goldberg et al
(2008): they use a manually-constructed lexicon
for Hebrew in order to learn an HMM tagger. How-
ever, this lexicon was constructed by trained lexi-
cographers over a long period of time and achieves
very high coverage of the language with very good
quality, much better than could be achieved by
our non-expert linguistics graduate student anno-
tators in just a few hours. Cucerzan and Yarowsky
583
(2002) learn a POS-tagger from existing linguis-
tic resources, namely a dictionary and a refer-
ence grammar, but these resources are not avail-
able, much less digitized, for most under-studied
languages. Haghighi and Klein (2006) develop a
model in which a POS-tagger is learned from a list
of POS tags and just three ?prototype? word types
for each tag, but their approach requires a vector
space to compute the distributional similarity be-
tween prototypes and other word types in the cor-
pus. Such distributional models are not feasible
for low-resource languages because they require
immense amounts of raw text, much more than is
available in these settings (Abney and Bird, 2010).
Further, they extracted their prototype lists directly
from a labeled corpus, something we are specif-
ically avoiding. Ta?ckstro?m et al (2013) evalu-
ate the use of mixed type and token constraints
generated by projecting information from a high-
resource language to a low-resource language via
a parallel corpus. However, large parallel corpora
are not available for most low-resource languages.
These are also expensive resources to create and
would take considerably more effort to produce
than the monolingual resources that our annotators
were able to generate in a four-hour timeframe.
Of course, if they are available, such parallel text
links could be incorporated into our approach.
In our previous work, we developed a differ-
ent strategy based on generalizing linguistic input
with a computational model: linguists annotated
either types or tokens for two hours, these anno-
tations are projected onto a corpus of unlabeled
tokens using label propagation and HMMs, and
a final POS-tagger is trained on this larger auto-
labeled corpus (Garrette and Baldridge, 2013).
That approach uses much more realistic types
and quantities of resources than previous work;
nonetheless, it leaves many open questions regard-
ing the effectiveness of incrementally more anno-
tation, the role of unannotated data, and whether
there is a good balance to be found using a combi-
nation of type- and token-supervision. We also did
not consider morphological analyzers as a form
of type supervision, as suggested by Merialdo
(1994).
This paper addresses these questions via a se-
ries of experiments designed to quantify the ef-
fect on performance given by the amount of time
spent finding or annotating training materials. We
specifically look at the impact of four types of data
collection:
1. Time annotating sentences (token supervision)
2. Time creating tag dictionary (type supervision)
3. Time constructing a finite state transducer
(FST) to analyze word-type morphology
4. Amount of raw data available for training
We explore these strategies in the context of POS-
tagging for Kinyarwanda and Malagasy. We also
include experiments for English, pretending as
though it is a low-resource language. The over-
whelming take away from our results is that type
supervision?when backed by an effective semi-
supervised learning approach?is the most impor-
tant source of linguistic information. Also, mor-
phological analyzers help for morphologically rich
languages when there are few labeled types or to-
kens (and, it never hurts to use them). Finally, per-
formance improves with more raw data, though we
see diminishing returns past 400,000 tokens. With
just four hours of type annotation, our system ob-
tains good accuracy across the three languages:
89.8% on English, 81.9% on Kinyarwanda, and
81.2% on Malagasy.
Our results compare favorably with previous
work despite using considerably less supervision
and a more difficult set of tags. For example, Li et
al. (2012) use the entirety of English Wiktionary
directly as a tag dictionary to obtain 87.1% accu-
racy on English, below our result. Ta?ckstro?m et al
(2013) average 88.8% across 8 major languages,
but for Turkish, a morphologically rich language,
they achieve only 65.2%, significantly below our
81.9% for morphologically-rich Kinyarwanda.
2 Data
Kinyarwanda (KIN) and Malagasy (MLG) are low-
resource, KIN is morphologically rich, and English
(ENG) is used for comparison. For each language,
sentences were divided into four sets: training data
to be labeled by annotators, raw training data, de-
velopment data, and test data.
Data sources The KIN texts are transcripts of
testimonies by survivors of the Rwandan geno-
cide provided by the Kigali Genocide Memorial
Center. The MLG texts are articles from the web-
sites1 Lakroa and La Gazette and Malagasy Global
Voices.2 Texts in both KIN and MLG were tok-
1www.lakroa.mg and www.lagazette-dgi.com
2mg.globalvoicesonline.org/
584
KIN MLG ENG - Experienced ENG - Novice
time type token type token type token type token
1:00 801 559 (1093) 660 422 (899) 910 522 (1124) 210 308 (599)
2:00 1814 948 (2093) 1363 785 (1923) 2660 1036 (2375) 631 646 (1429)
3:00 2539 1324 (3176) 2043 1082 (3064) 4561 1314 (3222) 1350 953 (2178)
4:00 3682 1651 (4119) 2773 1378 (4227) 6598 1697 (4376) 2185 1220 (2933)
Table 1: Annotations for each language and annotator as time increases. Shows the number of tag
dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses).
For brevity, the table only shows hourly progress.
enized and labeled with POS tags by two linguis-
tics graduate students, each of which was studying
one of the languages. The KIN and MLG data have
12 and 23 distinct POS tags, respectively.
The Penn Treebank (PTB) (Marcus et al, 1993)
is used as ENG data. Section 01 was used for
token-supervised annotation, sections 02-14 were
used as raw data, 15-18 for development of the
FST, 19-21 as a dev set and 22-24 as a test set.
The PTB uses 45 distinct POS tags.
Collecting annotations Linguists with non-
native knowledge of KIN and MLG produced anno-
tations for four hours (in 30-minute intervals) for
two tasks. In the first task, type-supervision, the
annotator was given a list of the words in the tar-
get language (ranked from most to least frequent),
and they annotated each word type with its poten-
tial POS tags. The word types and frequencies used
for this task were taken from the raw training data
and did not include the test sets. In the second
task, token-supervision, full sentences were anno-
tated with POS tags. The 30-minute intervals allow
us to investigate the incremental benefit of addi-
tional annotation of each type as well as how both
annotation types might be combined within a fixed
annotation budget.
Baldridge and Palmer (2009) found that anno-
tator expertise greatly influences effectiveness of
active learning for morpheme glossing, a related
task. To see how differences in annotator speed
and quality impact our task, we obtained ENG data
from an experienced annotator and a novice one.
Ngai and Yarowsky (2000) investigated the ef-
fectiveness of rule-writing versus annotation (us-
ing active learning) for chunking, and found the
latter to be far more effective. While we do not
explore a rule-writing approach to POS-tagging,
we do consider the impact of rule-based morpho-
logical analyzers as a component in our semi-
supervised POS-tagging system.
ENG - Exp. ENG - Nov.
time type tok type tok
1:00 0.05 0.03 0.01 0.02
2:00 0.15 0.05 0.03 0.03
3:00 0.24 0.06 0.07 0.05
4:00 0.32 0.08 0.11 0.06
Table 2: Tag dictionary recall against the test set
for ENG annotators on type and token annotations.
Annotations Table 1 gives statistics for all lan-
guages and annotators showing progress during
the 4-hour tasks. With token-annotation, tag
dictionary growth slows because high-frequency
words are repeatedly annotated, producing only
additional frequency and sequence information.
In contrast, every type-annotation label is a new
tag dictionary entry. For types, growth increases
over time, reflecting the fact that high-frequency
words (which are addressed first) tend to be more
ambiguous and thus require more careful thought
than later words. For ENG, we can compare the
tagging speed of the experienced annotator with
the novice: 50% more tokens and 3 times as many
types. The token-tagging speed stayed fairly con-
stant for the experienced annotator, but the novice
increased his rate, showing the result of practice.
Checking the annotators? output against the
gold tags in the PTB shows that both had good
tagging accuracy on tokens: 94-95%. Comparing
the tag dictionary entries versus the test data, pre-
cision starts in the high 80%s and falls to to the
mid-70%s in all cases. However, the differences
in recall, shown in Table 2, are more interesting.
On types, the experienced annotator maxed out at
32%, but the novice only reaches 11%. More-
over, the maximum for token annotations is much
lower due to high repeat-annotation. The discrep-
ancies between experienced and novice, and be-
tween type and token recall explain a great deal of
the performance disparity seen in the experiments.
585
3 Morphological Transducers
Finite-state transducers (FSTs) accept regular lan-
guages and can be constructed easily using regu-
lar expressions, which makes them quite useful for
phonology, morphology and limited areas of syn-
tax (Karttunen, 2001). Past work has used FSTs
for direct POS-tagging (Roche and Schabes, 1995),
but this requires tight coupling between the FST
and target tagset. We use FSTs for morphologi-
cal analysis: the FST accepts a word type and pro-
duces a set of morphological features. If there are
multiple possible analyses for a given word type,
the FST returns them all. For instance the Kin-
yarwanda verb sibatarazuka ?he is not yet resur-
rected? is analyzed in several ways:
? +NEG+CL2+1PL+V+arazuk+IMP
? +NEG+CL2+NOT.YET+PRES+zuk+IMP
? +NEG+CL2+NOT.YET+razuk+IMP
FSTs are particularly valuable for their ability
to analyze out-of-vocabulary items. By looking
for known affixes, FSTs can guess the stem of
a word and produce an analysis despite not hav-
ing knowledge of that stem. For morphologically
complex languages like KIN, this ability is espe-
cially useful. Other factors, such as a large num-
ber of morphologically-conditioned phonological
changes (seen in MLG) make out-of-vocabulary
guessing more challenging because of the large
number of potential stems (high ambiguity).
Development of the FSTs for all three languages
was done by iteratively adding rules and lexical
items with the goal of increasing coverage on a
raw dataset. To accomplish this on a fixed time
budget, the most frequently occurring unanalyzed
tokens were examined, and their stems plus any
observable morphological or phonological pat-
terns were added to the transducer. Addition-
ally, developers searched for known morpholog-
ical alternations to locate instances of phonolog-
ical change for inclusion. Coverage was checked
against a raw dataset which did not include the test
data used for the POS experiments.
The KIN and MLG FSTs were created by
English-speaking linguists who were familiar with
their respective language. They also used dictio-
naries and grammars. Each FST was developed
in 10 hours. To evaluate the benefits of more de-
velopment time, a version of the English FST was
saved every 30 minutes, as shown in Table 3.
elapsed
time
tokens types
count pct count pct
2:00 130k 61% 2.1k 12%
4:00 159k 75% 4.1k 24%
6:00 170k 80% 6.7k 39%
8:00 182k 86% 7.7k 44%
10:00 192k 91% 10.7k 62%
Table 3: Coverage of the English morphological
FST during development. For brevity, showing 2-
hour increments instead of 30-minute segments.
tokens types
cov. ambig. cov. ambig.
KIN 86% 2.62 82% 5.31
MLG 78% 2.98 37% 1.13
ENG 91% 1.19 62% 1.97
Table 4: Coverage and ambiguity of the final FST
for each language.
4 Approach
Learning under low-resource conditions is more
difficult than scenarios in most previous POS work
because the vast majority of the word types in the
training and test data are not covered by the an-
notations. When most words are unknown, learn-
ing algorithms such as EM struggle (Garrette and
Baldridge, 2012). Recall that most work on learn-
ing POS-taggers from tag dictionaries used tag dic-
tionaries culled from test sets (even when consid-
ering incomplete dictionaries). We thus build on
our previous approach, which exploits extremely
sparse, human-generated annotations that are pro-
duced without knowledge of which words appear
in the test set (Garrette and Baldridge, 2013).
This approach generalizes a small initial tag dic-
tionary to include unannotated word types appear-
ing in raw data. It estimates word/tag pair and
tag-transition frequency information using model-
minimization, which also reduces noise intro-
duced by automatic tag dictionary expansion. The
approach exploits type annotations effectively to
learn parameters for out-of-vocabulary words and
infer missing frequency and sequence informa-
tion. This pipeline is described in detail in the
previous work, so we give only a brief overview
and describe our additions.
The purpose of tag dictionary expansion is to es-
timate label distributions for tokens in a raw cor-
586
pus, including words missing in the annotations.
For this, a graph connecting annotated words to
unannotated words via features is constructed and
POS labels are pushed between these items using
label propagation (LP) (Talukdar and Crammer,
2009). LP has been used successfully for extend-
ing POS labels from high-resource languages to
low via parallel corpora (Das and Petrov, 2011;
Ta?ckstro?m et al, 2013; Ding, 2011) or high- to
low-resource domains (Subramanya et al, 2010),
among other tasks. These works have typically
used n-gram features (capturing basic syntax) and
character affixes (basic morphology).
The character n-gram affix-as-morphology ap-
proach produces many features, but only a fraction
of them represent actual morphemes. Incorrect
features end up pushing noise around the graph,
so affixes can lead to more false labels that drown
out the true labels. While affixes may be suffi-
cient for languages with limited morphology, their
effectiveness diminishes for morphology-rich lan-
guages, which have much higher type-to-token ra-
tios. More types means sparser word frequency
statistics and more out-of-vocabulary items, and
thus problems for EM. Here, we modify the LP
graph by supplementing or replacing generic af-
fix features with a focused set of morphological
features produced by an FST. These targeted mor-
phological features are effective during LP because
words that share them are much more likely to ac-
tually share POS tags.
FSTs produce multiple analyses, which is actu-
ally advantageous for LP. Ambiguities need not be
resolved since we just take the union of all mor-
phological features for all analyses and use them
as features in the graph. Note that each FST pro-
duces its own POS-tags as features, but these do
not correspond to the target POS tagset used by the
tagger. This is important because it decouples FST
development and the final POS task. Thus, any FST
for the language, regardless of its provenance, can
be used with any target POS tagset.
Since the LP graph contains a node for each cor-
pus token, and each node is labeled with a distri-
bution over POS tags, the graph provides a corpus
of sentences labeled with noisy tag distributions
along with an expanded tag dictionary. This out-
put is useful as input to EM because it contains
labels for all seen word types as well as sequence
and frequency information. There is a high degree
of noise in the LP output, so we employ the model
minimization strategy of Ravi et al (2010), which
finds a minimal set of tag bigrams needed to ex-
plain the sentences in the raw corpus. It outputs
a corpus of tagged sentences, which are used as
a good starting point for EM training of an HMM.
The expanded tag dictionary constrains the EM
search space by providing a limited tagset for each
word type, steering EM towards a desirable result.
Because the HMM trained by EM will con-
tain zero-probabilities for words that did not ap-
pear in the training corpus, we use the ?auto-
supervision? step from our previous work: a Max-
imum Entropy Markov Model tagger is trained
on a corpus that is noisily labeled by the HMM
(Garrette and Baldridge, 2012). While training
an HMM before the MEMM is not strictly neces-
sary, our tests have shown that this generative-
then-discriminative combination generally results
in around 3% accuracy improvement.
5 Experiments3
To better understand the effect that each type of
supervision has on tagger accuracy, we perform a
series of experiments, with KIN and MLG as true
low-resource languages. English experiments, for
which we had both experienced and novice an-
notators, allow for further exploration into issues
concerning data collection and preparation.
The overall best accuracies achieved by lan-
guage are 81.9% for KIN using all types, 81.2% for
MLG using half types and half tokens, and 89.8%
for ENG using all types and the maximal amount
of raw data. All of these best values were achieved
using both FST and affix LP features.
All results described in this section are averaged
over five folds of raw data.
5.1 Types versus tokens
Our primary question was the relationship be-
tween annotation type and time. Annotation must
be done by someone familiar with the target lan-
guage, linguistics, and the target POS tagset. For
many low-resource languages, such people, and
the time they have to spend, are likely to be in
short supply. To make the best use of their time,
we need to know which annotations are most use-
3Code and all MLG data available at github.com/
dhgarrette/low-resource-pos-tagging-2013
We are unable to provide the KIN or ENG data for down-
load due to licensing restrictions. However, ENG data may
be shared with those holding a license for the Penn Treebank
and KIN data may be shared on a case-by-case basis.
587
(a) KIN type annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
50
55
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(b) KIN token annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
50
55
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(c) MLG type annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(d) MLG token annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
Figure 1: Annotation time vs. tagger accuracy for type-only and token-only annotations.
Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
85
Experienced annotator ? TypesExperienced annotator ? TokensNovice annotator ? TypesNovice annotator ? Tokens
Figure 2: Annotation time vs. tagger accuracy for
ENG type-only and token-only annotations with
affix and FST LP features.
ful so that efforts can be concentrated there. Ad-
ditionally, it is useful to identify when returns on
annotation effort diminish so that annotators do
not spend time doing work that is unlikely to add
much value.
The annotators produced four hours each of
type and token annotations, each in 30-minute in-
crements. To assess the effects of annotation time,
we trained taggers cumulatively on each increment
and determine the value of each additional half-
hour of effort. Results are shown for KIN and MLG
in Figure 1 and ENG in Figure 2. In all scenarios,
the use of LP (and model minimization) delivers
huge performance gains. Additionally, the use of
FST features, usually along with affixes, yielded
better results than without. This indicates the LP
procedure makes effective use of the morpholog-
ical features produced by the FST and that the af-
fix features are able to capture missing information
without adding too much noise to the LP graph.
Furthermore, performance is considerably bet-
ter when type annotations are used than only to-
kens. Type annotations plateau much faster, so
a shorter amount of time must be spent annotat-
ing types than if token annotations are used. For
KIN it takes approximately 1.5 hours to reach near-
maximum accuracy for types, but 2.5 hours for to-
kens. This difference is due to the fact that the type
annotations started with the most frequent words
whereas the token annotations were on random
sentences. Thus, type annotations quickly cover a
significant portion of the language?s tokens. With
annotations directly on tokens, some of the highest
588
(a) KIN ? Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(b) MLG ? Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
70
72
74
76
78
80
No LPAffixes onlyFST onlyAffixes+FST
Figure 3: Annotation mixture vs. tagger accuracy. X-axis labels give annotation proportions, e.g. ?t2/s6?
indicates 2/8 of the time (1 hour) was spent annotating types and 6/8 (3 hours), full sentences.
Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
70
75
80
85
Exp. ? With LPNov. ? With LPExp. ? No LPNov. ? No LP
Figure 4: Annotation mixture vs. tagger accuracy
on ENG using affix and FST LP features for experi-
enced (Exp.) and novice (Nov.) annotators.
frequency types are covered, but annotation time
is also ineffectively used on low-frequency types
that happen to appear in those sentences.
Finally, the use of FST features yields the largest
gains for KIN, but only when small amounts of
annotation are available. This makes sense: KIN
is a morphologically rich language, so sparsity is
greater and crude affixes capture less actual mor-
phology. With little annotated data, LP relies heav-
ily on morphological features to make clean links
between words. But, with more annotations, the
gains of the FST over affix features alone dimin-
ishes: the affix features eventually capture enough
of the morphology to make up the difference.
Figure 2 shows the dramatic differences be-
tween the experienced and novice ENG annota-
tors.4 For the former, results using types and to-
4The ENG graph omits ?No LP? results since they fol-
lowed patterns similar to KIN and MLG. Additionally, the
results without FST features are not shown because they were
nearly identical (though slightly lower) than with the FST.
kens were similar after 30 minutes, but type an-
notations proved much more useful beyond that.
In contrast, the novice annotated types much more
slowly, so early on there were not enough anno-
tated types for the training to be as effective. Even
so, after three hours of annotation, type annota-
tions still win with the novice, and even beat the
experienced annotator labeling tokens.
5.2 Mixing type and token annotations
Because type and token annotations are each bet-
ter at providing different information ? a tag dic-
tionary of high-frequency words vs. sequence and
frequency information ? it is reasonable to ex-
pect that a combination of the two might yield
higher performance by each contributing differ-
ent but complementary information during train-
ing. This matters in low-resource settings because
type or token annotations will likely be produced
by the same people, so there is a tradeoff between
spending resources on one form of annotation over
the other. Understanding the best mixture of an-
notations can inform us on how to maximize the
benefit of a set annotation budget. To this end, we
ran experiments fixing the annotation time to four
hours while varying the mix of type and token an-
notations. Results are shown for KIN and MLG in
Figure 3 and ENG in Figure 4.
For KIN and ENG, tagger accuracy increases as
the proportion of type annotations increases for all
LP feature configurations. For MLG, however, as
the reliance on the FST increases, the optimal mix-
ture shifts toward higher type proportions. When
only affix features are used, the optimal mixture is
1 hour of types and 3 hours of tokens. When FST
and affix features are used, the optimum is 2 hours
589
each of types and tokens. When only FST features
are used, it is best to use 3.5 hours of types and
only 30 minutes of tokens. Because the FST op-
erates on word types, it is effective at exploiting
type annotations. Thus, when the LP focuses more
on FST features, it becomes more desirable to have
larger amounts of type annotations.
Types clearly win for ENG. The experienced an-
notator was much faster at annotating types and
the speed difference was less pronounced for to-
kens, so accuracy is most similar when only token
annotations are used. The performance disparity
grows with increasing the type proportion.
Ta?ckstro?m et al (2013) explore the use of
mixed type and token annotations in which a tag-
ger is learned by projecting information via par-
allel text. In their experiments, they?like us?
found that type information is more valuable than
token information. However, they were able to see
gains through the complementary effects of mix-
ing type and token annotations. It is likely that this
difference in our results is due to the amount of an-
notated data used. It seems that the amount of type
information collected in four hours is not sufficient
to saturate the system, meaning that switching to
annotating tokens tends to hurt performance.
5.3 FST development
The third set of experiments evaluate how the
amount of time spent developing an FST affects
the performance of trained tagger. To do this,
we had our ENG FST developer save progress af-
ter each hour (for ten hours). The results show
that, for ENG, the FST provided no value, regard-
less of how much time was spent on its develop-
ment. Moreover, since large gains in accuracy can
be achieved by spending a small amount of time
just annotating word types with POS tags, we are
led to conclude that time should be spent annotat-
ing types or tokens instead of developing an FST.
While it is likely that FST development time would
have a greater impact for morphologically rich
languages, we suspect that greater gains can still
be obtained by instead annotating types. Nonethe-
less, FSTs never seems to hurt performance, so if
one is readily available, it should be used.
5.4 The effect of more raw data
In addition to annotations, semi-supervised tagger
training requires a corpus of raw text. Raw data
can be easier to acquire since it does not need
the attention of a linguist. Even so, for many
Number of Raw Data Tokens
Acc
ura
cy
100k 200k 300k 400k 500k 600k
80
82
84
86
88
90
4hr types, FST, With LP4hr types, FST, No LP1hr types, No FST, With LP
Figure 5: Amount of raw data vs. tagger accuracy
for ENG using high vs. low amounts of annotation
and using LP vs. no LP., for experienced annotator
(novice results were similar).
low-resource languages, the amount of digitized
text, such as transcripts or websites, is very lim-
ited and may, in fact, require substantial effort
to accumulate, even with assistance from compu-
tational tools (Bird, 2011). Therefore, the col-
lection of raw data can be considered another
time-sensitive task for which the tradeoffs with
previously-discussed annotation efforts must con-
tend.
It could be the case that more raw data for train-
ing could make up for additional annotation and
FST development effort or make the LP proce-
dure unnecessary. Figure 5 shows that that in-
creased raw data does provide increasing gains,
but they diminish after 200k tokens. The best per-
formance is achieved by using more annotation
and LP. Most importantly, however, removing ei-
ther annotations or LP results in a significant de-
cline in accuracy, such that even with 600k train-
ing tokens, we are unable to achieve the results of
high annotation and LP using only 100k tokens.
5.5 Correcting existing annotations
For all of the ENG experiments, we also ran ?or-
acle? experiments using gold tags for the same
sentences or a tag dictionary containing the same
number of type/tag entries as the annotator pro-
duced, but containing only the most frequent
entries as determined by the gold-labeled cor-
pus. Using this simulated ?perfect annotator? data
shows we lose accuracy due to annotator mistakes:
for our experienced annotator and maximal FST,
using 4 hours of types the oracle accuracy is 90.5
vs. 88.5 while using only tokens we see 83.9 vs.
590
81.5. This indicates that there are gains to be made
by correcting mistakes in the annotations. This
is true even after the point of diminishing returns
on the learning curve, meaning that even when
adding more annotations no longer improves per-
formance, progress can still be made by correcting
errors, so it may be reasonable to ask annotators to
attempt to correct errors in their past annotations.
Automated techniques for facilitating error identi-
fication can be employed for this (Dickinson and
Meurers, 2003).
6 Conclusions and Future Work
Care must be taken when drawing conclusions
from small-scale annotation studies such as those
presented in this paper. Nonetheless, we have
explored realistic annotation scenarios for POS-
tagging for low-resource languages and found sev-
eral consistent patterns. Most importantly, it is
clear that type annotations are the most useful in-
put one can obtain from a linguist?provided a
semi-supervised algorithm for projecting that in-
formation reliably onto raw tokens is available. In
a sense, this result validates the research trajectory
of efforts of the past two decades put into learning
taggers from tag dictionaries: papers have succes-
sively removed layers of unrealistic assumptions,
and in doing so have produced pipelines for type-
supervision that easily beat token-supervision pre-
pared in comparable amounts of time.
The result of most immediate practical value is
that we show it is possible to train effective POS-
taggers on actual low-resource languages given
only a relatively small amount of unlabeled text
and a few hours of annotation by a non-native
linguist. Instead of having annotators label full
sentences as one might expect the natural choice
would be, it is much more effective to simply
extract a list of the most frequent word types in
the language and concentrate efforts on annotat-
ing these types with their potential parts of speech.
Furthermore, for languages with rich morphology,
a morphological transducer can yield significant
performance gains when large amounts of other
annotated resources are unavailable. (And it never
hurts performance.)
Finally, additional raw text does improve per-
formance. However, using substantial amounts of
raw text is unlikely to produce gains larger than
only a few hours spent annotating types. Thus,
when deciding whether to spend time locating
larger volumes of digitized text or to spend time
annotating types, choose types.
Despite the consistent superiority of type anno-
tations in our experiments, it of course may be the
case that techniques such as active learning may
better select sentences for token annotation, so this
should be explored in future work.
Acknowledgements
We thank Kyle Jerro, Vijay John, Jim Evans, Yoav
Goldberg, Slav Petrov, and the reviewers for their
assistance and feedback. This work was sup-
ported by the U.S. Department of Defense through
the U.S. Army Research Office (grant number
W911NF-10-1-0533) and through a National De-
fense Science and Engineering Graduate Fellow-
ship for the first author. Experiments were run
on the UTCS Mastodon Cluster, provided by NSF
grant EIA-0303609.
References
Steven Abney and Steven Bird. 2010. The human lan-
guage project: Building a universal corpus of the
worlds languages. In Proceedings of ACL.
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP, Singa-
pore.
Steven Bird. 2011. Bootstrapping the language
archive: New prospects for natural language pro-
cessing in preserving linguistic heritage. Linguistic
Issues in Language Technology, 6.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of CoNLL, Taipei, Tai-
wan.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT, Portland,
Oregon, USA.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of EACL.
Weiwei Ding. 2011. Weakly supervised part-of-
speech tagging for Chinese using label propagation.
Master?s thesis, University of Texas at Austin.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP, Jeju, Korea.
591
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL, Atlanta, Georgia.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings NAACL.
Kazi Saidul Hasan and Vincent Ng. 2009.
Weakly supervised part-of-speech tagging for
morphologically-rich, resource-scarce languages. In
Proceedings of EACL, Athens, Greece.
Lauri Karttunen. 2001. Applications of finite-state
transducers in natural language processing. Lecture
Notes in Computer Science, 2088.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of EMNLP, Jeju Island, Korea.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: Is it time for some linguis-
tics? In Proceedings of CICLing.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings ACL.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING.
Emmanuel Roche and Yves Schabes. 1995. Determin-
istic part-of-speech tagging with finite-state trans-
ducers. Computational Linguistics, 21(2).
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings EMNLP, Cambridge, MA.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. In Transactions of the ACL. Association for
Computational Linguistics.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In Proceedings of ECML-PKDD, Bled, Slove-
nia.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS.
592
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466?1476,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Text-Driven Toponym Resolution using Indirect Supervision
Michael Speriosu Jason Baldridge
Department of Linguistics
University of Texas at Austin
Austin, TX 78712 USA
{speriosu,jbaldrid}@utexas.edu
Abstract
Toponym resolvers identify the specific lo-
cations referred to by ambiguous place-
names in text. Most resolvers are based on
heuristics using spatial relationships be-
tween multiple toponyms in a document,
or metadata such as population. This pa-
per shows that text-driven disambiguation
for toponyms is far more effective. We ex-
ploit document-level geotags to indirectly
generate training instances for text classi-
fiers for toponym resolution, and show that
textual cues can be straightforwardly in-
tegrated with other commonly used ones.
Results are given for both 19th century
texts pertaining to the American Civil War
and 20th century newswire articles.
1 Introduction
It has been estimated that at least half of the
world?s stored knowledge, both printed and digi-
tal, has geographic relevance, and that geographic
information pervades many more aspects of hu-
manity than previously thought (Petras, 2004;
Skupin and Esperbe?, 2011). Thus, there is value
in connecting linguistic references to places (e.g.
placenames) to formal references to places (coor-
dinates) (Hill, 2006). Allowing for the querying
and exploration of knowledge in a geographically
informed way requires more powerful tools than a
keyword-based search can provide, in part due to
the ambiguity of toponyms (placenames).
Toponym resolution is the task of disambiguat-
ing toponyms in natural language contexts to geo-
graphic locations (Leidner, 2008). It plays an es-
sential role in automated geographic indexing and
information retrieval. This is useful for histori-
cal research that combines age-old geographic is-
sues like territoriality with modern computational
tools (Guldi, 2009), studies of the effect of histor-
ically recorded travel costs on the shaping of em-
pires (Scheidel et al, 2012), and systems that con-
vey the geographic content in news articles (Teitler
et al, 2008; Sankaranarayanan et al, 2009) and
microblogs (Gelernter and Mushegian, 2011).
Entity disambiguation systems such as those of
Kulkarni et al (2009) and Hoffart et al (2011)
disambiguate references to people and organiza-
tions as well as locations, but these systems do not
take into account any features or measures unique
to geography such as physical distance. Here we
demonstrate the utility of incorporating distance
measurements in toponym resolution systems.
Most work on toponym resolution relies on
heuristics and hand-built rules. Some use sim-
ple rules based on information from a gazetteer,
such as population or administrative level (city,
state, country, etc.), resolving every instance of
the same toponym type to the same location re-
gardless of context (Ladra et al, 2008). Others use
relationships between multiple toponyms in a con-
text (local or whole document) and look for con-
tainment relationships, e.g. London and England
occurring in the same paragraph or as the bigram
London, England (Li et al, 2003; Amitay et al,
2004; Zong et al, 2005; Clough, 2005; Li, 2007;
Volz et al, 2007; Jones et al, 2008; Buscaldi and
Rosso, 2008; Grover et al, 2010). Still others first
identify unambiguous toponyms and then disam-
biguate other toponyms based on geopolitical re-
lationships with or distances to the unambiguous
ones (Ding et al, 2000). Many favor resolutions of
toponyms within a local context or document that
cover a smaller geographic area over those that are
more dispersed (Rauch et al, 2003; Leidner, 2008;
Grover et al, 2010; Loureiro et al, 2011; Zhang
et al, 2012). Roberts et al (2010) use relation-
ships learned between people, organizations, and
locations from Wikipedia to aid in toponym reso-
lution when such named entities are present, but
do not exploit any other textual context.
1466
Most of these approaches suffer from a major
weakness: they rely primarily on spatial relation-
ships and metadata about locations (e.g., popu-
lation). As such, they often require nearby to-
ponyms (including unambiguous or containing to-
ponyms) to resolve ambiguous ones. This reliance
can result in poor coverage when the required in-
formation is missing in the context or when a doc-
ument mentions locations that are neither nearby
geographically nor in a geopolitical relationship.
There is a clear opportunity that most ignore:
use non-toponym textual context. Spatially rel-
evant words like downtown that are not explicit
toponyms can be strong cues for resolution (Hol-
lenstein and Purves, 2012). Furthermore, the con-
nection between non-spatial words and locations
has been successfully exploited in data-driven
approaches to document geolocation (Eisenstein
et al, 2010, 2011; Wing and Baldridge, 2011;
Roller et al, 2012) and other tasks (Hao et al,
2010; Pang et al, 2011; Intagorn and Lerman,
2012; Hecht et al, 2012; Louwerse and Benesh,
2012; Adams and McKenzie, 2013).
In this paper, we learn resolvers that use all
words in local or document context. For example,
the word lobster appearing near the toponym Port-
land indicates the location is Portland in Maine
rather than Oregon or Michigan. Essentially, we
learn a text classifier per toponym. There are no
massive collections of toponyms labeled with lo-
cations, so we train models indirectly using geo-
tagged Wikipedia articles. Our results show these
text classifiers are far more accurate than algo-
rithms based on spatial proximity or metadata.
Furthermore, they are straightforward to combine
with such algorithms and lead to error reductions
for documents that match those algorithms? as-
sumptions.
Our primary focus is toponym resolution, so we
evaluate on toponyms identified by human anno-
tators. However, it is important to consider the
utility of an end-to-end toponym identification and
resolution system, so we also demonstrate that
performance is still strong when toponyms are de-
tected with a standard named entity recognizer.
We have implemented all the models discussed
in this paper in an open source software package
called Fieldspring, which is available on GitHub:
http://github.com/utcompling/fieldspring
Explicit instructions are provided for preparing
data and running code to reproduce our results.
Figure 1: Points representing the United States.
2 Data
2.1 Gazetteer
Toponym resolvers need a gazetteer to obtain can-
didate locations for each toponym. Additionally,
many gazetteers include other information such as
population and geopolitical hierarchy information.
We use GEONAMES, a freely available gazetteer
containing over eight million entries worldwide.1
Each location entry contains a name (sometimes
more than one) and latitude/longitude coordinates.
Entries also include the location?s administrative
level (e.g. city or state) and its position in the
geopolitical hierarchy of countries, states, etc.
GEONAMES gives the locations of regional
items like states, provinces, and countries as single
points. This is clearly problematic when we seek
connections between words and locations: e.g. we
might learn that many words associated with the
USA are connected to a point in Kansas. To get
around this, we represent regional locations as a
set of points derived from the gazetteer. Since re-
gional locations are named in the entries for loca-
tions they contain, all locations contained in the
region are extracted (in some cases over 100,000
of them) and then k-means is run to find a smaller
set of spatial centroids. These act as a tractable
proxy for the spatial extent of the entire region. k
is set to the number of 1? by 1? grid cells covered
by that region. Figure 1 shows the points com-
puted for the United States.2 A nice property of
this representation is that it does not involve re-
gion shape files and the additional programming
infrastructure they require.
1Downloaded April 16, 2013 from www.geonames.
org.
2The representation also contains three points each in
Hawaii and Alaska not shown in Figure 1.
1467
Corpus docs toks types tokstop typestop ambavg ambmax
TRC-DEV 631 136k 17k 4356 613 15.0 857
TRC-DEV-NER - - - 3165 391 18.2 857
TRC-TEST 315 68k 11k 1903 440 13.7 857
TRC-TEST-NER - - - 1346 305 15.7 857
CWAR-DEV 228 33m 200k 157k 850 29.9 231
CWAR-TEST 113 25m 305k 85k 760 31.5 231
Table 1: Statistics of the corpora used for evaluation. Columns subscripted by top give figures for
toponyms. The last two columns give the average number of candidate locations per toponym token and
the number of candidate locations for the most ambiguous toponym.
A location for present purposes is thus a set of
points on the earth?s surface. The distance be-
tween two locations is computed as the great circle
distance between the closest pair of representative
points, one from each location.
2.2 Toponym Resolution Corpora
We need corpora with toponyms identified and re-
solved by human annotators for evaluation. The
TR-CONLL corpus (Leidner, 2008) contains 946
REUTERS news articles published in August
1996. It has about 204,000 words and articles
range in length from a few hundred words to sev-
eral thousand words. Each toponym in the corpus
was identified and resolved by hand.3 We place
every third article into a test portion (TRC-TEST)
and the rest in a development portion. Since our
methods do not learn from explicitly labeled to-
ponyms, we do not need a training set.
The Perseus Civil War and 19th Century Amer-
ican Collection (CWAR) contains 341 books (58
million words) written primarily about and during
the American Civil War (Crane, 2000). Toponyms
were annotated by a semi-automated process: a
named entity recognizer identified toponyms, and
then coordinates were assigned using simple rules
and corrected by hand. We divide CWAR into de-
velopment (CWAR-DEV) and test (CWAR-TEST)
sets in the same way as TR-CONLL.
Table 1 gives statistics for both corpora, includ-
ing the number and ambiguity of gold standard
toponyms for both as well as NER identified to-
3We found several systematic types of errors in the origi-
nal TR-CONLL corpus, such as coordinates being swapped
for some locations and some longitudes being zero or the neg-
ative of their correct values. We repaired many of these er-
rors, though some more idiosyncratic mistakes remain. We,
along with Jochen Leidner, will release this updated version
shortly and will link to it from our Fieldspring GitHub page.
ponyms for TR-CONLL.4 We use the pre-trained
English NER from the OpenNLP project.5
2.3 Geolocated Wikipedia Corpus
The GEOWIKI dataset contains over one million
English articles from the February 11, 2012 dump
of Wikipedia. Each article has human-annotated
latitude/longitude coordinates. We divide the cor-
pus into training (80%), development (10%), and
test (10%) at random and perform preprocessing
to remove markup in the same manner as Wing
and Baldridge (2011). The training portion is used
here to learn models for text-driven resolvers.
3 Toponym Resolvers
Given a set of toponyms provided via annotations
or identified using NER, a resolver must select a
candidate location for each toponym (or, in some
cases, a resolver may abstain). Here, we describe
baseline resolvers, a heuristic resolver based on
the usual cues used in most toponym resolvers,
and several text-driven resolvers. We also discuss
combining heuristic and text-driven resolvers.
3.1 Baseline Resolvers
RANDOM For each toponym, the RANDOM re-
solver randomly selects a location from those as-
sociated in the gazetteer with that toponym.
POPULATION The POPULATION resolver se-
lects the location with the greatest population for
each toponym. It is generally quite effective, but
when a toponym has several locations with large
populations, it is often wrong. Also, it can only be
used when such information is available, and it is
4States and countries are not annotated in CWAR, so we
do not evaluate end-to-end using NER plus toponym resolu-
tion for it as there are many (falsely) false positives.
5opennlp.apache.org
1468
less effective if the population statistics are from a
time period different from that of the corpus.
3.2 SPIDER
Leidner (2008) describes two general and useful
minimality properties of toponyms:
? one sense per discourse: multiple tokens of
a toponym in the same text generally do not
refer to different locations in the same text
? spatial minimality: different toponyms in a
text tend refer to spatially near locations
Many toponym resolvers exploit these (Smith and
Crane, 2001; Rauch et al, 2003; Leidner, 2008;
Grover et al, 2010; Loureiro et al, 2011; Zhang
et al, 2012). Here, we define SPIDER (Spatial
Prominence via Iterative Distance Evaluation and
Reweighting) as a strong representative of such
textually unaware approaches. In addition to cap-
turing both minimality properties, it also identifies
the relative prominence of the locations for each
toponym in a given corpus.
SPIDER resolves each toponym by finding the
location for each that minimizes the sum distance
to all locations for all other toponyms in the same
document. On the first iteration, it tends to select
locations that clump spatially: if Paris occurs with
Dallas, it will choose Paris, Texas even though the
topic may be a flight from Texas to France. Further
iterations bring Paris, France into focus by captur-
ing its prominence across the corpus. The key in-
tuition is that most documents will discuss Paris,
France and only a small portion of these mention
places close to Paris, Texas; thus, Paris, France
will be selected on the first iteration for many
documents (though not for the Dallas document).
SPIDER thus assigns each candidate location a
weight (initialized to 1.0), which is re-estimated
on each iteration. The adjusted distance between
two locations is computed as the great circle dis-
tance divided by the product of the two locations?
weights. At the end of an iteration, each candi-
date location?s weight is updated to be the frac-
tion of the times it was chosen times the number
of candidates for that toponym. The weights are
global, with one for each location in the gazetteer,
so the same weight vector is used for each token
of a given toponym on a given iteration.
For example, if after the first iteration Paris,
France is chosen thrice, Paris, Texas once, and
Paris, Arkansas never, the global weights of these
locations are (3/4)?3=2.25, (1/4)?3=.75, and
(0/4)?3=0, respectively (assume, for the exam-
ple, there are no other locations named Paris). The
sum of the weights remains equal to the number
of candidate locations. The updated weights are
used on the next iteration, so Paris, France will
seem ?closer? since any distance computed to it
is divided by a number greater than one. Paris,
Texas will seem somewhat further away, and Paris,
Arkansas infinitely far away. The algorithm con-
tinues for a fixed number of iterations or until the
weights do not change more than some thresh-
old. Here, we run SPIDER for 10 iterations; the
weights have generally converged by this point.
When only one toponym is present in a doc-
ument, we simply select the candidate with the
greatest weight. When there is no such weight in-
formation, such as when the toponym does not co-
occur with other toponyms anywhere in the cor-
pus, we select a candidate at random.
SPIDER captures prominence, but we stress it
is not our main innovation: its purpose is to be a
benchmark for text-driven resolvers to beat.
3.3 Text-Driven Resolvers
The text-driven resolvers presented in this section
all use local context windows, document context,
or both, to inform disambiguation.
TRIPDL We use a document geolocator
trained on GEOWIKI?s document location labels.
Others?such as Smith and Crane (2001)?have
estimated a document-level location to inform
toponym resolution, but ours is the first we are
aware of to use training data from a different
domain to build a document geolocator that uses
all words (not only toponyms) to estimate a
document?s location. We use the document geolo-
cation method of Wing and Baldridge (2011). It
discretizes the earth?s surface into 1? by 1? grid
cells and assigns Kullback-Liebler divergences to
each cell given a document, based on language
models learned for each cell from geolocated
Wikipedia articles. We obtain the probability of a
cell c given a document d by the standard method
of exponentiating the negative KL-divergence and
normalizing these values over all cells:
P (c|d) = exp(?KL(c, d))?
c? exp(?KL(c?, d))
This distribution is used for all toponyms t in d
to define distributions PDL(l|t, d) over candidate
1469
locations of t in document d to be the portion of
P (c|d) consistent with the t?s candidate locations:
PDL(l|t, d) =
P (cl|d)?
l??G(t) P (cl? |d)
where G(t) is the set of the locations for t in the
gazetteer, and cl is the cell containing l. TRIPDL
(Toponym Resolution Informed by Predicted Doc-
ument Locations) chooses the location that maxi-
mizes PDL.
WISTR While TRIPDL uses an off-the-shelf
document geolocator to capture the geographic
gist of a document, WISTR (Wikipedia Indirectly
Supervised Toponym Resolver) instead directly
targets each toponym. It learns text classifiers
based on local context window features trained on
instances automatically extracted from GEOWIKI.
To create the indirectly supervised training data
for WISTR, the OpenNLP named entity recog-
nizer detects toponyms in GEOWIKI, and can-
didate locations for each toponym are retrieved
from GEONAMES. Each toponym with a loca-
tion within 10km of the document location is con-
sidered a mention of that location. For example,
the Empire State Building Wikipedia article has a
human-provided location label of (40.75,-73.99).
The toponym New York is mentioned several times
in the article, and GEONAMES lists a New York at
(40.71,-74.01). These points are 4.8km apart, so
each mention of New York in the document is con-
sidered a reference to New York City.
Next, context windows w of twenty words to
each side of each toponym are extracted as fea-
tures. The label for a training instance is the
candidate location closest to the document loca-
tion. We extract 1,489,428 such instances for to-
ponyms relevant to our evaluation corpora. These
instances are used to train logistic regression clas-
sifiers P (l|t, w) for location l and toponym t. To
disambiguate a new toponym, WISTR chooses
the location that maximizes this probability.
Few such probabilistic toponym resolvers ex-
ist in the literature. Li (2007) builds a probabil-
ity distribution over locations for each toponym,
but still relies on nearby toponyms that could refer
to regions that contain that toponym and requires
hand construction of distributions. Other learn-
ing approaches to toponym resolution (e.g. Smith
and Mann (2003)) require explicit unambiguous
mentions like Portland, Maine to construct train-
ing instances, while our data gathering methodol-
ogy does not make such an assumption. Overell
and Ru?ger (2008) and Overell (2009) only use
nearby toponyms as features. Mani et al (2010)
and Qin et al (2010) use other word types but
only in a local context, and they require toponym-
labeled training data. Our approach makes use of
all words in local and document context and re-
quires no explicitly labeled toponym tokens.
TRAWL We bring TRIPDL, WISTR, and
standard toponym resolution cues about ad-
ministrative levels together with TRAWL (To-
ponym Resolution via Administrative levels and
Wikipedia Locations). The general form of a prob-
abilistic resolver that utilizes such information to
select a location l? for a toponym t in document d
may be defined as
l? = argmaxl P (l, al|t, d).
where al is the administrative level (country, state,
city) for l in the gazetteer. This captures the fact
that countries (like Sudan) tend to be referred to
more often than small cities (like Sudan, Texas).
The above term is simplified as follows:
P (l, al|t, d) = P (al|t, d)P (l|al, t, d)
? P (al|t)P (l|t, d)
where we approximate the administrative level
prediction as independent of the document, and
the location as independent of administrative level.
The latter term is then expressed as a linear combi-
nation of the local context (WISTR) and the doc-
ument context (TRIPDL):
P (l|t, d) = ?tP (l|t, ct) + (1??t)PDL(l|t, d).
?t, the weight of the local context distribution, is
set according to the confidence that a prediction
based on local context is correct:
?t = f(t)f(t)+C ,
where f(t) is the fraction of training instances
of toponym t of all instances extracted from
GEOWIKI. C is set experimentally; C=.0001 was
the optimal value for CWAR-DEV. Intuitively, the
larger C is, the greater f(t) must be for the local
context to be trusted over the document context.
We define P (a|t), the administrative level com-
ponent, to be the fraction of representative points
for a location l? out of the number of representa-
tives points for all candidate locations l ? t,
||Rl?||?
l??t ||Rl? ||
1470
where ||Rl|| is the number of representative points
of l. This boosts states and countries since higher
probability is assigned to locations with more
points (and cities have just one point).
Taken together, the above definitions yield the
TRAWL resolver, which selects the optimal can-
didate location l? according to
l? = argmaxl P (al|t)(?tP (l|t, ct) + (1??t)PDL(l|t, d)).
3.4 Combining Resolvers and Backoff
SPIDER begins with uniform weights for each
candidate location of each toponym. WISTR
and TRAWL both output distributions over these
locations based on outside knowledge sources,
and can be used as more informed initializa-
tions of SPIDER than the uniform ones. We
call these combinations WISTR+SPIDER and
TRAWL+SPIDER.6
WISTR fails to predict when encountering a
toponym it has not seen in the training data, and
TRIPDL fails when a toponym only has locations
in cells with no probability mass. TRAWL fails
when both of these are true. In these cases, we
select the candidate location geographically clos-
est to the most likely cell according to TRIPDL?s
P (c|d) distribution.
3.5 Document Size
For SPIDER, runtime is quadratic in the size
of documents, so breaking up documents vastly
reduces runtime. It also restricts the minimal-
ity heuristic?appropriately?to smaller spans of
text. For resolvers that take into account the sur-
rounding document when determining how to re-
solve a toponym, such as TRIPDL and TRAWL,
it can often be beneficial to divide documents into
smaller subdocuments in order to get a better esti-
mate of the overall geographic prominence of the
text surrounding a toponym, but at a more coarse-
grained level than the local context models pro-
vide. For these reasons, we simply divide each
book in the CWAR corpus into small subdocu-
ments of at most 20 sentences.
4 Evaluation
Many prior efforts use a simple accuracy metric:
the fraction of toponyms whose predicted location
6We scale each toponym?s distribution as output by
WISTR or TRAWL by the number of candidate locations
for that toponym, since the total weight for each toponym in
SPIDER is the number of candidate locations, not 1.
is the same as the gold location. Such a met-
ric can be problematic, however. The gazetteer
used by a resolver may not contain, for a given
toponym, a location whose latitude and longitude
exactly match the gold label for the toponym (Lei-
dner, 2008). Also, some errors are worse than oth-
ers, e.g. predicting a toponym?s location to be on
the other side of the world versus predicting it to
be a different city in the same country?accuracy
does not reflect this difference.
We choose a metric that instead measures the
distance between the correct and predicted loca-
tion for each toponym and compute the mean and
median of all such error distances. This is used
in document geolocation work (Eisenstein et al,
2010, 2011; Wing and Baldridge, 2011; Roller
et al, 2012) and is related to the root mean squared
distance metric discussed by Leidner (2008).
It is important to understand performance on
plain text (without gold toponyms), which is the
typical use case for applications using toponym
resolvers. Both the accuracy metric and the error-
distance metric encounter problems when the set
of predicted toponyms is not the same as the set
of gold toponyms (regardless of locations), e.g.
when a named entity recognizer is used to iden-
tify toponyms. In this case, we can use precision
and recall, where a true positive is defined as the
prediction of a correctly identified toponym?s lo-
cation to be as close as possible to its gold la-
bel, given the gazetteer used. False positives oc-
cur when the NER incorrectly predicts a toponym,
and false negatives occur when it fails to predict a
toponym identified by the annotator. When a cor-
rectly identified toponym receives an incorrect lo-
cation prediction, this counts as both a false nega-
tive and a false positive. We primarily present re-
sults from experiments with gold toponyms but in-
clude an accuracy measure for comparability with
results from experiments run on plain text with
a named entity recognizer. This accuracy met-
ric simply computes the fraction of toponyms that
were resolved as close as possible to their gold la-
bel given the gazetteer.
5 Results
Table 2 gives the performance of the resolvers
on the TR-CONLL and CWAR test sets when
gold toponyms are used. Values for RANDOM
and SPIDER are averaged over three trials. The
ORACLE row gives results when the candidate
1471
Resolver TRC-TEST CWAR-TEST
Mean Med. A Mean Med. A
ORACLE 105 19.8 100.0 0.0 0.0 100.0
RANDOM 3915 1412 33.5 2389 1027 11.8
POPULATION 216 23.1 81.0 1749 0.0 59.7
SPIDER10 2180 30.9 55.7 266 0.0 57.5
TRIPDL 1494 29.3 62.0 847 0.0 51.5
WISTR 279 22.6 82.3 855 0.0 69.1
WISTR+SPIDER10 430 23.1 81.8 201 0.0 85.9
TRAWL 235 22.6 81.4 945 0.0 67.8
TRAWL+SPIDER10 297 23.1 80.7 148 0.0 78.2
Table 2: Accuracy and error distance metrics on test sets with gold toponyms.
Figure 2: Visualization of how SPIDER clumps
most predicted locations in the same region
(above), on the CWAR-DEV corpus. TRAWL?s
output (below) is much more dispersed.
from GEONAMES closest to the annotated loca-
tion is always selected. The ORACLE mean and
median error values on TR-CONLL are nonzero
due to errors in the annotations and inconsisten-
cies stemming from the fact that coordinates from
GEONAMES were not used in the annotation of
TR-CONLL.
On both datasets, SPIDER achieves errors and
accuracies much better than RANDOM, validating
the intuition that authors tend to discuss places
near each other more often than not, while some
locations are more prominent in a given corpus
despite violating the minimality heuristic. The
text-driven resolvers vastly outperform SPIDER,
showing the effectiveness of textual cues for to-
ponym resolution.
The local context resolver WISTR is very
effective: it has the highest accuracy for
TR-CONLL, though two other text-based re-
solvers also beat the challenging POPULATION
baseline?s accuracy. TRAWL achieves a better
mean distance metric for TR-CONLL, and when
used to seed SPIDER, it obtains the lowest mean
error on CWAR by a large margin. SPIDER
seeded with WISTR achieves the highest accu-
racy on CWAR. The overall geographic scope
of CWAR, a collection of documents about the
American Civil War, is much smaller than that of
TR-CONLL (articles about international events).
This makes toponym resolution easier overall (es-
pecially error distances) for minimality resolvers
like SPIDER, which primarily seek tightly clus-
tered sets of locations. This behavior is quite
clear in visualizations of predicted locations such
as Figure 2.
On the CWAR dataset, POPULATION performs
relatively poorly, demonstrating the fragility of
population-based decisions for working with his-
torical corpora. (Also, we note that POPULATION
is not a resolver per se since it only ever predicts
one location for a given toponym, regardless of
context.)
Table 3 gives results on TRC-TEST when NER-
identified toponyms are used. In this case, the
ORACLE results are less than 100% due to the lim-
itations of the NER, and represent the best possible
results given the NER we used.
When resolvers are run on NER-identified to-
ponyms, the text-driven resolvers that use lo-
cal context again easily beat SPIDER. WISTR
achieves the best performance. The named en-
tity recognizer is likely better at detecting com-
mon toponyms than rare toponyms due to the na-
1472
Resolver P R F
ORACLE 82.6 59.9 69.4
RANDOM 25.1 18.2 21.1
POPULATION 71.6 51.9 60.2
SPIDER10 40.5 29.4 34.1
TRIPDL 51.8 37.5 43.5
WISTR 73.9 53.6 62.1
WISTR+SPIDER10 73.2 53.1 61.5
TRAWL 72.5 52.5 60.9
TRAWL+SPIDER10 72.0 52.2 60.5
Table 3: Precision, recall, and F-score of resolvers
on TRC-TEST with NER-identified toponyms.
ture of its training data, and many more local con-
text training instances were extracted from com-
mon toponyms than from rare ones in Wikipedia.
Thus, our model that uses only these local context
models does best when running on NER-identified
toponyms. We also measured the mean and me-
dian error distance for toponyms correctly identi-
fied by the named entity recognizer, and found that
they tended to be 50-200km worse than for gold
toponyms. This also makes sense given the named
entity recognizer?s tendency to detect common to-
ponyms: common toponyms tend to be more am-
biguous than others.
Results on TR-CONLL indicate much higher
performance than the resolvers presented by Lei-
dner (2008), whose F-scores do not exceed 36.5%
with either gold or NER toponyms.7 TRC-TEST
is a subset of the documents Leidner uses (he did
not split development and test data), but the results
still come from overlapping data. The most direct
comparison is SPIDER?s F-score of 39.7% com-
pared to his LSW03 algorithm?s 35.6% (both are
minimality resolvers). However, our evaluation is
more penalized since SPIDER loses precision for
NER?s false positives (Jack London as a location)
while Leidner only evaluated on actual locations.
It thus seems fair to conclude that the text-driven
classifiers, with F-scores in the mid-50?s, are much
more accurate on the corpus than previous work.
6 Error Analysis
Table 4 shows the ten toponyms that caused the
greatest total error distances from TRC-DEV with
gold toponyms when resolved by TRAWL, the re-
solver that achieves the lowest mean error on that
7Leidner (2008) reports precision, recall, and F-score val-
ues even with gold toponyms, since his resolvers can abstain.
dataset among all our resolvers.
Washington, the toponym contributing the most
total error, is a typical example of a toponym that
is difficult to resolve, as there are two very promi-
nent locations within the United States with the
name. Choosing one when the other is correct re-
sults in an error of over 4000 kilometers. This oc-
curs, for example, when TRAWL chooses Wash-
ington state in the phrase Israel?s ambassador to
Washington, where more knowledge about the
status of Washington, D.C. as the political cen-
ter of the United States (e.g. in the form of more
or better contextual training instances) could over-
turn the administrative level component?s prefer-
ence for states.
An instance of California in a baseball-related
news article is incorrectly predicted to be the town
California, Pennsylvania. The context is: ...New
York starter Jimmy Key left the game in the first
inning after Seattle shortstop Alex Rodriguez lined
a shot off his left elbow. The Yankees have lost
12 of their last 19 games and their lead in the AL
East over Baltimore fell to five games. At Califor-
nia, Tim Wakefield pitched a six-hitter for his third
complete game of the season and Mo Vaughn and
Troy O?Leary hit solo home runs in the second in-
ning as the surging Boston Red Sox won their third
straight 4-1 over the California Angels. Boston
has won seven of eight and is 20-6... The pres-
ence of many east coast cues?both toponym and
otherwise?make it unsurprising that the resolver
would predict California, Pennsylvania despite the
administrative level component?s heavier weight-
ing of the state.
The average errors for the toponyms Australia
and Russia are fairly small and stem from differ-
ences in how countries are represented across dif-
ferent gazetteers, not true incorrect predictions.
Table 5 shows the toponyms with the great-
est errors from CWAR-DEV with gold toponyms
when resolved by WISTR+SPIDER. Rome is
sometimes predicted as cities in Italy and other
parts of Europe rather than Rome, Georgia, though
it correctly selects the city in Georgia more often
than not due to SPIDER?s preference for tightly
clumped sets of locations. Mexico, however, fre-
quently gets incorrectly selected as a city in Mary-
land near many other locations in the corpus when
TRAWL?s administrative level component is not
present. Many other of the toponyms contributing
to the total error such as Jackson and Lexington are
1473
Toponym N Mean Total
Washington 25 3229 80717
Gaza 12 5936 71234
California 8 5475 43797
Montana 3 11635 34905
WA 3 11221 33662
NZ 2 14068 28136
Australia 88 280 24600
Russia 72 260 18712
OR 2 9242 18484
Sydney 12 1422 17067
Table 4: Toponyms with the greatest total error
distances in kilometers from TRC-DEV with gold
toponyms resolved by TRAWL. N is the number
of instances, and the mean error for each toponym
type is also given.
Toponym N Mean Total
Mexico 1398 2963 4142102
Jackson 2485 1210 3007541
Monterey 353 2392 844221
Haymarket 41 15663 642170
McMinnville 145 3307 479446
Alexandria 1434 314 450863
Eastport 184 2109 388000
Lexington 796 442 351684
Winton 21 15881 333499
Clinton 170 1401 238241
Table 5: Top errors from CWAR-DEV resolved by
TRAWL+SPIDER.
simply the result of many American towns sharing
the same names and a lack of clear disambiguating
context.
7 Conclusion
Our text-driven resolvers prove highly effective
for both modern day newswire texts and 19th cen-
tury texts pertaining to the Civil War. They eas-
ily outperform standard minimality toponym re-
solvers, but can also be combined with them. This
strategy works particularly well when predicting
toponyms on a corpus with relatively restricted
geographic extents. Performance remains good
when resolving toponyms identified automatically,
indicating that end-to-end systems based on our
models may improve the experience of digital hu-
manities scholars interested in finding and visual-
izing toponyms in large corpora.
Acknowledgements
We thank: the three anonymous reviewers, Grant
DeLozier, and the UT Austin Natural Language
Learning reading group, for their helpful feed-
back; Ben Wing, for his document geoloca-
tion software; Jochen Leidner, for providing the
TR-CONLL corpus as well as feedback on earlier
versions of this paper; and Scott Nesbit, for pro-
viding the annotations for the CWAR corpus. This
research was supported by a grant from the Morris
Memorial Trust Fund of the New York Commu-
nity Trust.
References
B. Adams and G. McKenzie. Inferring thematic
places from spatially referenced natural lan-
guage descriptions. Crowdsourcing Geographic
Knowledge, pages 201?221, 2013.
E. Amitay, N. Har?El, R. Sivan, and A. Soffer.
Web-a-Where: geotagging web content. In Pro-
ceedings of the 27th annual international ACM
SIGIR conference on Research and development
in information retrieval, pages 273?280, 2004.
D. Buscaldi and P. Rosso. A conceptual density-
based approach for the disambiguation of to-
ponyms. International Journal of Geographical
Information Science, 22(3):301?313, 2008.
P. Clough. Extracting metadata for spatially-
aware information retrieval on the internet. In
Proceedings of the 2005 workshop on Ge-
ographic information retrieval, pages 25?30.
ACM, 2005.
G. Crane. The Perseus Digital Library, 2000. URL
http://www.perseus.tufts.edu.
J. Ding, L. Gravano, and N. Shivakumar. Comput-
ing geographical scopes of web resources. In
Proceedings of the 26th International Confer-
ence on Very Large Data Bases, pages 545?556,
2000.
J. Eisenstein, B. O?Connor, N. Smith, and E. Xing.
A latent variable model for geographic lexical
variation. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1277?1287, 2010.
J. Eisenstein, A. Ahmed, and E. Xing. Sparse ad-
ditive generative models of text. In Proceedings
of the 28th International Conference on Ma-
chine Learning, pages 1041?1048, 2011.
1474
J. Gelernter and N. Mushegian. Geo-parsing mes-
sages from microtext. Transactions in GIS, 15
(6):753?773, 2011.
C. Grover, R. Tobin, K. Byrne, M. Woollard,
J. Reid, S. Dunn, and J. Ball. Use of the Ed-
inburgh geoparser for georeferencing digitized
historical collections. Philosophical Transac-
tions of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 368(1925):
3875?3889, 2010.
J. Guldi. The spatial turn. Spatial Humanities: a
Project of the Institute for Enabling, 2009.
Q. Hao, R. Cai, C. Wang, R. Xiao, J. Yang,
Y. Pang, and L. Zhang. Equip tourists with
knowledge mined from travelogues. In Pro-
ceedings of the 19th international conference on
World wide web, pages 401?410, 2010.
B. Hecht, S. Carton, M. Quaderi, J. Scho?ning,
M. Raubal, D. Gergle, and D. Downey. Ex-
planatory semantic relatedness and explicit spa-
tialization for exploratory search. In Proceed-
ings of the 35th international ACM SIGIR con-
ference on Research and development in infor-
mation retrieval, pages 415?424. ACM, 2012.
L. Hill. Georeferencing: The Geographic Associ-
ations of Information. MIT Press, 2006.
J. Hoffart, M. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. Robust disambiguation of named
entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 782?792. Association
for Computational Linguistics, 2011.
L. Hollenstein and R. Purves. Exploring place
through user-generated content: Using Flickr
tags to describe city cores. Journal of Spatial
Information Science, (1):21?48, 2012.
S. Intagorn and K. Lerman. A probabilistic ap-
proach to mining geospatial knowledge from
social annotations. In Conference on Infor-
mation and Knowledge Management (CIKM),
2012.
C. Jones, R. Purves, P. Clough, and H. Joho. Mod-
elling vague places with knowledge from the
web. International Journal of Geographical In-
formation Science, 2008.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. Collective annotation of
Wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining,
pages 457?466. ACM, 2009.
S. Ladra, M. Luaces, O. Pedreira, and D. Seco. A
toponym resolution service following the OGC
WPS standard. In Web and Wireless Geograph-
ical Information Systems, volume 5373, pages
75?85. 2008.
J. Leidner. Toponym resolution in text: Anno-
tation, Evaluation and Applications of Spatial
Grounding of Place Names. Universal Press,
Boca Raton, FL, USA, 2008.
H. Li, R. Srihari, C. Niu, and W. Li. InfoXtract lo-
cation normalization: a hybrid approach to geo-
graphic references in information extraction. In
Proceedings of the HLT-NAACL 2003 workshop
on Analysis of geographic references - Volume
1, pages 39?44, 2003.
Y. Li. Probabilistic toponym resolution and geo-
graphic indexing and querying. Master?s thesis,
The University of Melbourne, Melbourne, Aus-
tralia, 2007.
V. Loureiro, I. Anasta?cio, and B. Martins. Learn-
ing to resolve geographical and temporal ref-
erences in text. In Proceedings of the 19th
ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems,
pages 349?352, 2011.
M. Louwerse and N. Benesh. Representing spatial
structure through maps and language: Lord of
the Rings encodes the spatial structure of Mid-
dle Earth. Cognitive science, 36(8):1556?1569,
2012.
I. Mani, C. Doran, D. Harris, J. Hitzeman,
R. Quimby, J. Richer, B. Wellner, S. Mardis,
and S. Clancy. SpatialML: annotation scheme,
resources, and evaluation. Language Resources
and Evaluation, 44(3):263?280, 2010.
S. Overell. Geographic Information Retrieval:
Classification, Disambiguation and Modelling.
PhD thesis, Imperial College London, 2009.
S. Overell and S. Ru?ger. Using co-occurrence
models for placename disambiguation. Inter-
national Journal of Geographical Information
Science, 22:265?287, 2008.
Y. Pang, Q. Hao, Y. Yuan, T. Hu, R. Cai, and
L. Zhang. Summarizing tourist destinations
by mining user-generated travelogues and pho-
1475
tos. Computer Vision and Image Understand-
ing, 115(3):352 ? 363, 2011.
V. Petras. Statistical analysis of geographic and
language clues in the MARC record. Technical
report, The University of California at Berkeley,
2004.
T. Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang.
An efficient location extraction algorithm by
leveraging web contextual information. In Pro-
ceedings of the 18th SIGSPATIAL International
Conference on Advances in Geographic Infor-
mation Systems, pages 53?60. ACM, 2010.
E. Rauch, M. Bukatin, and K. Baker. A
confidence-based framework for disambiguat-
ing geographic terms. In Proceedings of the
HLT-NAACL 2003 workshop on Analysis of ge-
ographic references - Volume 1, pages 50?54,
2003.
K. Roberts, C. Bejan, and S. Harabagiu. Toponym
disambiguation using events. In Proceedings of
the 23rd International Florida Artificial Intelli-
gence Research Society Conference, pages 271?
276, 2010.
S. Roller, M. Speriosu, S. Rallapalli, B. Wing, and
J. Baldridge. Supervised text-based geolocation
using language models on an adaptive grid. In
Proceedings of EMNLP 2012, 2012.
J. Sankaranarayanan, H. Samet, B. Teitler,
M. Lieberman, and J. Sperling. TwitterStand:
news in tweets. In Proceedings of the 17th
ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems,
pages 42?51, 2009.
W. Scheidel, E. Meeks, and J. Weiland. ORBIS:
The Stanford geospatial network model of the
roman world. 2012.
A. Skupin and A. Esperbe?. An alternative map
of the United States based on an n-dimensional
model of geographic space. Journal of Vi-
sual Languages & Computing, 22(4):290?304,
2011.
D. Smith and G. Crane. Disambiguating geo-
graphic names in a historical digital library. In
Proceedings of the 5th European Conference on
Research and Advanced Technology for Digital
Libraries, pages 127?136, 2001.
D. Smith and G. Mann. Bootstrapping toponym
classifiers. In Proceedings of the HLT-NAACL
2003 workshop on Analysis of geographic ref-
erences - Volume 1, pages 45?49, 2003.
B. Teitler, M. Lieberman, D. Panozzo, J. Sankara-
narayanan, H. Samet, and J. Sperling. News-
Stand: a new view on news. In Proceedings of
the 16th ACM SIGSPATIAL international con-
ference on Advances in geographic information
systems, page 18. ACM, 2008.
R. Volz, J. Kleb, and W. Mueller. Towards
ontology-based disambiguation of geographical
identifiers. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, 2007.
B. Wing and J. Baldridge. Simple supervised doc-
ument geolocation with geodesic grids. In Pro-
ceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Hu-
man Language Technologies, pages 955?964,
2011.
Q. Zhang, P. Jin, S. Lin, and L. Yue. Extracting
focused locations for web pages. In Web-Age
Information Management, volume 7142, pages
76?89. 2012.
W. Zong, D. Wu, A. Sun, E. Lim, and D. Goh. On
assigning place names to geography related web
pages. In Proceedings of the 5th ACM/IEEE-
CS joint conference on Digital libraries, pages
354?362, 2005.
1476
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 53?63,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Twitter Polarity Classification with Label Propagation
over Lexical Links and the Follower Graph
Michael Speriosu
University of Texas at Austin
speriosu@mail.utexas.edu
Nikita Sudan
University of Texas at Austin
nsudan@utexas.edu
Sid Upadhyay
University of Texas at Austin
sid.upadhyay@utexas.edu
Jason Baldridge
University of Texas at Austin
jbaldrid@mail.utexas.edu
Abstract
There is high demand for automated tools that
assign polarity to microblog content such as
tweets (Twitter posts), but this is challenging
due to the terseness and informality of tweets
in addition to the wide variety and rapid evolu-
tion of language in Twitter. It is thus impracti-
cal to use standard supervised machine learn-
ing techniques dependent on annotated train-
ing examples. We do without such annota-
tions by using label propagation to incorpo-
rate labels from a maximum entropy classifier
trained on noisy labels and knowledge about
word types encoded in a lexicon, in combina-
tion with the Twitter follower graph. Results
on polarity classification for several datasets
show that our label propagation approach ri-
vals a model supervised with in-domain an-
notated tweets, and it outperforms the nois-
ily supervised classifier it exploits as well as
a lexicon-based polarity ratio classifier.
1 Introduction
Twitter is a microblogging service where users post
messages (?tweets?) of no more than 140 charac-
ters. With around 200 million users generating 140
million tweets per day, Twitter represents one of the
largest and most dynamic datasets of user generated
content. Along with other social networking web-
sites such as Facebook, the content on Twitter is real
time: tweets about everything from a friend?s birth-
day to a devastating earthquake can be found posted
during and immediately after an event in question.
This vast stream of real time data has major im-
plications for any entity interested in public opin-
ion and even acting on what is learned and engag-
ing with the public directly. Companies have the
opportunity to examine what customers and poten-
tial customers are saying about their products and
services without costly and time-consuming surveys
or explicit requests for feedback. Political organi-
zations and candidates might be able to determine
what issues the public is most interested in, as well
as where they stand on those issues. Manual inspec-
tion of tweets can be useful for many such analyses,
but many applications and questions require real-
time analysis of massive amounts of social media
content. Computational tools that automatically ex-
tract and analyze relevant information about opinion
expressed on Twitter and other social media sources
are thus in high demand.
Full sentiment analysis for a given question or
topic requires many stages, including but not lim-
ited to: (1) extraction of tweets based on an ini-
tial query, (2) filtering out spam and irrelevant items
from those tweets, (3) identifying subjective tweets,
and (4) identifying the polarity of those tweets. Like
most work in sentiment analysis, we focus on the
last stage, polarity classification. The simplest ap-
proaches are based on the presence of words or
emoticons that are indicators of positive or nega-
tive polarity (e.g. Twitter?s own API, O?Connor
et al (2010)), or calculating a ratio of positive to
negative terms (Choi and Cardie, 2009). Though
these are a useful first pass, the nuance of lan-
guage often defeats them (Pang and Lee, 2008).
Tweets provide additional challenges compared to
edited text; e.g. they are short and include infor-
mal/colloquial/abbreviated language.
53
Standard supervised classification methods im-
prove the situation somewhat (Pang et al, 2002),
but these require texts labeled with polarity as in-
put and they do not adapt to changes in language
use. One way around this is to use noisy labels (also
referred to as ?distant supervision?), e.g. by tak-
ing emoticons like ?:)? as positive and ?:(? as neg-
ative, and train a standard classifier (Read, 2005; Go
et al, 2009).1 Semi-supervised methods can also
reduce dependence on labeled texts: for example,
Sindhwani and Melville (2008) use a polarity lexi-
con combined with label propagation. Several have
used label propagation starting with a small number
of hand-labeled words to induce a lexicon for use
in polarity classification (Blair-Goldensohn et al,
2008; Rao and Ravichandran, 2009; Brody and El-
hadad, 2010).
In this paper, we bring together several of the
above approaches via label propagation using modi-
fied adsorption (Talukdar and Crammer, 2009). This
also allows us to explore the possibility of exploit-
ing the Twitter follower graph to improve polarity
classification, under the assumption that people in-
fluence one another or have shared affinities about
topics. We construct a graph that has users, tweets,
word unigrams, word bigrams, hashtags, and emoti-
cons as its nodes; users are connected based on the
Twitter follower graph, users are connected to the
tweets they created, and tweets are connected to
the unigrams, bigrams, hashtags and emoticons they
contain. We seed the graph using the polarity values
in the OpinionFinder lexicon (Wilson et al, 2005),
the known polarity of emoticons, and a maximum
entropy classifier trained on 1.8 million tweets with
automatically assigned labels based on the presence
of positive and negative emoticons, like Read (2005)
and Go et al (2009).
We compare the label propagation approach to
the noisily supervised classifier itself and to a stan-
dard lexicon-based method using positive/negative
ratios. Evaluation is performed on several datasets
of tweets that have been annotated for polarity: the
Stanford Twitter Sentiment set (Go et al, 2009),
1Davidov et al (2010) use 15 emoticons and 50 Twitter
hashtags as proxies for sentiment in a similar manner, but their
evaluation is indirect. Rather than predicting gold standard sen-
timent labels, they instead predict whether those same emoti-
cons and hashtags would be appropriate for other tweets.
tweets from the 2008 debate between Obama and
McCain (Shamma et al, 2009), and a new dataset
of tweets about health care reform that we have cre-
ated. In addition to performing standard per-tweet
accuracy, we also measure per-target accuracy (for
health care reform) and an aggregate error metric
over all users in our test set that captures how simi-
lar predicted positivity of each user is to their actual
positivity. Across all datasets and measures, we find
that label propagation is consistently better than the
noisily supervised classifier, which in turn outper-
forms the lexicon-based method. Additionally, for
the health care reform dataset, the label propagation
approach?which uses no gold labeled tweets, just a
hand-created lexicon?outperforms a maximum en-
tropy classifier trained on gold labels. However, we
do not find the follower graph to improve perfor-
mance with our current implementation.
2 Datasets
We use several different Twitter datasets as train-
ing or evaluation resources. From the annotated
datasets, only tweets with positive or negative polar-
ity are used, so neutral tweets are ignored. While im-
portant, subjectivity detection is largely a different
problem from polarity classification. For example,
Pang and Lee (2004) use minimum cuts in graphs
for the former and machine-learned text classifica-
tion for the latter. We also do not give any special
treatment to retweets, though doing so is a possible
future improvement.
2.1 Emoticon-based training set (EMOTICON)
Emoticons are commonly exploited as noisy in-
dicators of polarity?including by Twitter?s own
advanced search ?with positive/negative attitude.?
While imperfect, there is potential for millions of
tweets containing emoticons to serve as a source
of noisy training material for a supervised classi-
fier. We create such a training set from a sample
of the ?garden hose?2 Twitter feed, from September
to December, 2009. At the time of collection, this
included up to 15% of all tweets worldwide.
From this feed, 6,265,345 tweets containing at
least one of the emoticons listed in Table 1 are ex-
tracted; 5,156,277 contain a positive emoticon and
2http://dev.twitter.com/pages/streaming_api
54
+ :) :D =D =) :] =] :-) :-D :-] ;) ;D ;] ;-) ;-D ;-]
? :( =( :[ =[ :-( :-[ :?( :?[ D:
Table 1: Positive and negative emoticons.
+ #ff, congrats, gracias, yay, thx, smile,
awesome, hello, excited, moon, loving, glad,
sweet, wonderful, birthday, enjoy, goodnight,
amazing, cute, bom
? nickjonas, murphy, brittany, rip, triste, sad,
hurts, died, snow, huhu, headache, upset,
crying, throat, poor, sucks, ugh, sakit,
stomach, horrible
Table 2: Top 20 most predictive common unigram fea-
tures for the positive and negative classes, in order from
more predictive to less predictive.
1,109,068 contain a negative emoticon. A small
number of tweets contain both negative and posi-
tive emoticons. These are permitted to appear twice,
once for each label. Then, a balanced ratio of
positive/negative labels is obtained by keeping only
1,109,068 of the positive tweets. Finally, a large pro-
portion of non-English tweets are excluded by a fil-
ter that requires a tweet to have at least two words
(with at least two characters) from the CMU Pro-
nouncing Dictionary.3 A few non-English tweets
pass through this filter and some English tweets
with very unusual words or incorrect spelling are
dropped, but this simple strategy works well over-
all. The final training set contains 1,839,752 tweets,
still balanced for positive and negative emoticons.
Table 2 shows the 20 most predictive unigram
features of each class in the EMOMAXENT classi-
fier (described below) that are among the 1000 most
common unigrams in this dataset and are not them-
selves emoticons. A few non-English (but polar-
ized) words (e.g. gracias, bom, triste) make it past
our simple language filter and onto these lists, but
the majority of the most predictive words are En-
glish. Other highly predictive words are artifacts
of the particular tweet sample that comprises the
EMOTICON dataset, such as ?nickjonas,? ?brittany,?
and ?murphy,? the latter two explained by the abun-
3The dictionary contains 133k English words, including in-
flected forms and proper nouns. http://www.speech.
cs.cmu.edu/cgi-bin/cmudict
Dataset Use Size % Pos
STS dev 183 59.0
OMD dev 1898 73.1
HCR-TRAIN train 488 43.2
HCR-DEV dev 534 32.2
HCR-TEST test 396 38.6
Table 3: Basic properties of the annotated datasets used
in this paper.
dance of negative tweets after actress Brittany Mur-
phy?s death. Most others are intuitively good mark-
ers of positive or negative polarity.
2.2 Datasets with polarity annotations
Three annotated datasets, summarized in Table 3 and
described below, are used for training, development,
or evaluation of polarity classifiers.
Stanford Twitter Sentiment (STS). Go et al
(2009) created a collection of 216 annotated tweets
on various topics.4 Of these, 108 tweets are positive
and 75 are negative.
Obama-McCain Debate (OMD). Shamma et al
(2009) used Amazon Mechanical Turk to annotate
3,269 tweets posted during the presidential debate
on September 26, 2008 between Barack Obama and
John McCain. Each tweet was annotated by one
or more Turkers for the categories positive, nega-
tive, mixed, or other. We filter this dataset with two
constraints in order to ensure high inter-annotator
agreement. First, at least three votes must have
been provided for a tweet to be included. Second,
more than half of the votes must have been posi-
tive or negative; the majority label is taken as the
gold standard for that tweet. This results in a set of
1,898 tweets. Of these, 705 had positive gold labels
and 1192 had negative gold labels, and the average
inter-annotator agreement of the Turk votes for these
tweets was 83.7%. To our knowledge, we are the
first to perform automatic polarity classification on
this dataset.
Health Care Reform (HCR). We create a new
annotated dataset based on tweets about health care
reform in the USA. This was a strongly debated
4http://twittersentiment.appspot.com/
55
topic that created a large number of polarized tweets,
especially in the run up to the signing of the health
care bill on March 23, 2010. We extract tweets con-
taining the health care reform hashtag ?#hcr? from
early 2010; a subset of these are annotated by us and
colleagues for polarity (positive, negative, neutral,
irrelevant) and polarity targets (health care reform,
Obama, Democrats, Republicans, Tea Party, conser-
vatives, liberals, and Stupak). These are separated
into training, dev and test sets. As with the other
datasets, we restrict attention in this paper only to
positive and negative tweets.5
2.3 The Twitter follower graph
One of the key ideas we test in this paper is whether
social connections can be used to improve polarity
classification for individual tweets and users. We
construct the Twitter follower graphs for the users in
the above datasets in stages using publicly available
data from the Twitter API. From the full list of each
user?s followers, we retain only followers found
within the datasets; this prunes unknown users who
did not tweet about the topic and thus are unlikely to
provide useful information. This method for graph
construction offers nearly complete graphs, but has
two main disadvantages. First, many users have
raised their privacy levels over time, which hinders
the ability to view their follower graph. In these
cases only their tweet information is known. Sec-
ondly, due to the rapid pace of growth on Twit-
ter, user graphs tend to grow quickly; thus our con-
structed graph is a representation of the user?s cur-
rent social graph and not the exact graph that existed
at the time of the tweet.
3 Approach
We compare three main approaches: using lexicon-
based positive/negative ratios, maximum entropy
classification and label propagation.
3.1 Lexicon-based baseline (LEXRATIO)
A reasonable baseline to use in polarity classifica-
tion is to count the number of positive and negative
terms in a tweet and pick the category with more
terms (O?Connor et al, 2010). This actually uses
5A public release of this data, along with our code, is avail-
able at https://bitbucket.org/speriosu/updown.
supervision at the level of word types. Like most
others, we use the OpinionFinder subjectivity lexi-
con,6 which contains 2,304 words annotated as pos-
itive and 4,153 words as negative. If the number of
positive and negative words in a tweet is equal (in-
cluding zero for both), the label is chosen at random.
3.2 Maximum entropy classifier (MAXENT)
The OpenNLP Maximum Entropy package7 is used
to train polarity classifiers using either EMOTICON
or HCR-TRAIN, henceforth referred to as EMO-
MAXENT and GOLDMAXENT, respectively. After
tokenizing on whitespace, unigram and bigram
features are extracted. All characters are lowercased
and non-alphanumeric characters are trimmed from
the left and right sides of tokens. However, tokens
that contain no alphanumeric characters are not
trimmed. Stop words8 are excluded as unigram
features. However, bigram features are extracted be-
fore stop words are removed since many stop words
are informative in the context of content words: e.g.,
contrast shit (negative) from the shit (very positive).
The beginning and end of tweets are indicated by
?$? in bigram features. Thus, the full feature set for
the tweet I love my new iPod Touch! :D is [love,
ipod, touch, $ i, i love, love my,
my ipod, ipod touch, touch :D, :D
$]. The same tokenization method is used for all
datasets in this paper.
3.3 Label Propagation (LPROP)
Tweets are not created in isolation?each tweet is
linked to other tweets by the same author, and each
author is influenced by the tweets of those he or she
follows. Common vocabulary and topics of discus-
sion also connect tweets to each other. Graph-based
methods such as label propagation (Zhu and Ghahra-
mani, 2002; Baluja et al, 2008; Talukdar and Cram-
mer, 2009) provide a natural means to represent and
exploit such relationships in order to improve classi-
fication, often while requiring less supervision than
with standard classification. Label propagation al-
gorithms spread label distributions from a small set
6http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
7http://incubator.apache.org/opennlp/
8Taken from: http://www.ranks.nl/resources/
stopwords.html
56
Opinion Finder
N-Grams
Hashtags
Emoticons
U1
Ahhh 
#Obamacare
I Love 
#NY!
{Tweet3} {Tweetn}
U2
We can't pass 
this :( #Killthebill 
love
hate
enjoy
i love
love ny
we can't
#ny
#obamacare
#killthebill
:):( =]
U3
{Tweetn}
Un
{Tweetn}
...
                  EmoMaxent Seeds
                  Labeled Seeds
                  Unseeded
...
...
. . .
...
Figure 1: An illustration of our graph with All-edges and Noisy-seed (see text for description).
of nodes seeded with some initial label information
(always noisy, heuristic information rather than gold
instance labels in our case) throughout the graph.
Label distributions are spread across a graph G =
{V,E,W} where V is the set of n nodes, E is a set
ofm edges andW is an n?nmatrix of weights, with
wij as the weight of edge (i, j). We use Modified
Adsorption (MAD) (Talukdar and Crammer, 2009)
over a graph with nodes representing tweets, authors
and features, while varying the seed information and
the construction of the edge sets. The spreading of
the label distributions can be viewed as a controlled
random walk with three possible actions: (i) inject-
ing a seeded node with its seed label, (ii) continu-
ing the walk from the current node to a neighbor-
ing node, and (iii) abandoning the walk. MAD takes
three parameters, ?1, ?2 and ?3, which control the
relative importance of each of these actions, respec-
tively. We use the Junto Label Propagation Toolkit?s
implementation of MAD in this paper.9
Modified Adsorption requires some nodes in the
graph to have seed distributions, which can come for
a variety of knowledge sources. We consider the fol-
lowing variants for seeding the graph:
? Maxent-seed: EMOMAXENT is trained on the
EMOTICON dataset; every tweet node is seeded
9http://code.google.com/p/junto/
with its polarity predictions for the tweet.
? Lexicon-seed: Nodes are created for every word
in the OpinionFinder lexicon. Positive words are
seeded as 90% positive if they are strongly subjec-
tive and 80% positive if weakly subjective; simi-
larly and conversely for negative words. Every
tweet is connected by an edge to every word in
the polarity lexicon it contains, using the weight-
ing scheme discussed with Feature-edges below.
? Emoticon-seed: Nodes are created for emoticons
from Table 1 and seeded as 90% positive or nega-
tive depending on their polarity.
? Annotated-seed: The annotations in HCR-
TRAIN are used to seed the tweets from that
dataset as 100% positive or negative, in accor-
dance with the label.
We use Noisy-seed as a collective term for all of the
above seed sets except Annotated-seed.
The other main aspect of graph construction is
specifying edges and their weights. We consider the
following variants:
? Follower-edges: When a user A follows another
user B, we add an edge from A to B with a weight
of 1.0, a weight that is comparable to that of a
moderately frequent word in Feature-edges below.
? Feature-edges: Nodes are added for hashtags and
the features described in ?3.2 and connected to the
57
tweets that contain them. An edge connecting a
tweet t to a feature f has weight wtf using rel-
ative frequency ratios of the feature between the
dataset d in question and the EMOTICON dataset
as a reference corpus r:
wtf =
{
log Pd(f)Pr(f) if Pd(f) > Pr(f)
0 o.w.
(1)
We use All-edges when combining both edge sets.
Figure 1 illustrates the connections for All-edges
and Noisy-seed by example. Each user un is at-
tached to anyone who follows them or who they
follow. Each user is also connected to the tweets
they authored. Words from OpinionFinder are con-
nected to tweets that contain those words, and sim-
ilarly for hashtags, emoticons, unigrams, and bi-
grams. Emoticons and words from OpinionFinder
are seeded according to the explanation above. All
edges other than Feature-edges are given a weight of
1.0.
4 Results
4.1 Parameter tuning
We evaluated our models on the STS, OMD, and
HCR-DEV datasets during development and kept
HCR-TEST as a final held-out test set used once, af-
ter all relevant parameters had been set. For Mod-
ified Adsorption, 100 iterations were used, and a
seed injection parameter ?1 of .005 gave the best
balance of allowing seed distributions to affect other
nodes without overwhelming them. The Junto de-
fault value of .01 was used for both ?2 and ?3.
4.2 Per-tweet accuracy
Table 4 shows the per-tweet accuracy results of
the random baseline, the LEXRATIO baseline, the
EMOMAXENT classifier alone, the LPROP classifier
run only on Follower-edges with Maxent-seed, the
LPROP classifier run on the full graph from Figure 1
only seeded with Lexicon-seed, and the LPROP clas-
sifier run on All-edges and Noisy-seed.
For all datasets, LPROP with Feature-edges and
Noisy-seed outperforms or matches all other meth-
ods. For STS, our best result of 84.7% accu-
racy beats Go et al (2009)?s reported best result
Classifier MSE
Random .167
LEXRATIO .170
EMOMAXENT .233
LPROP (Follower-edges, Maxent-seed) .233
LPROP (All-edges, Lexicon-seed) .187
LPROP (Feature-edges, Noisy-seed) .148
LPROP (All-edges, Noisy-seed) .148
Table 5: Mean squared error (MSE) per-user on HCR-
TEST, for users with at least 3 tweets
of 82.7%. Their approach uses a Maxent classifier
trained on a noisily labeled emoticon training set
similar to our EMOTICON dataset. Note that they
also remove neutral tweets from the test set.
Our semi-supervised label propagation method
compares favorably to fully supervised approaches.
For example, a graph with Feature-edges seeded
with gold labels from HCR-TRAIN (i.e. Annotated-
seed) obtains only 64.6% per-tweet accuracy on
HCR-TEST. A maximent entropy classifier trained
on HCR-TRAIN achieves 66.7%. Our best label
propagation approach surpasses both of these at
71.2%.
We find that in general Follower-edges are not
helpful as implemented here. Further work is needed
to explore more nuanced ways of modeling the so-
cial graph, such as allowing leaders to influence fol-
lowers more than vice versa.
4.3 Per-user error
In many sentiment analysis applications, it is of in-
terest to know what the polarity of a given individual
or the overall polarity toward a particular product is.
Here we compare the positivity ratio predicted by
our methods to that in the gold standard labels on a
per-user basis, using the mean squared error between
the predicted positivity ratios ppr and the actual ra-
tios apr for all users:
MSE(ppr, apr) =
?
i
(apri ? ppri)
2
Where apri and ppri are the actual and predicted
positivity ratios of the ith user.
Table 5 gives MSE results on HCR-TEST for
users with at least 3 tweets. LPROP (Feature-edges,
58
Classifier STS OMD HCR-DEV HCR-TEST
Random 50.0 50.0 50.0 50.0
LEXRATIO 72.1 59.1 54.3 58.1
EMOMAXENT 83.1 61.3 58.6 62.9
LPROP (Follower-edges, Maxent-seed) 83.1 61.2 57.9 62.9
LPROP (All-edges, Lexicon-seed) 70.0 62.6 64.6 64.6
LPROP (Feature-edges, Noisy-seed) 84.7 66.7 65.7 71.2
LPROP (All-edges, Noisy-seed) 84.7 66.5 65.2 71.0
Table 4: Per-tweet accuracy percentages. The models and parameters were developed while tracking performance on
STS, OMD, and HCR-DEV, and HCR-TEST results were obtained from a single, blind run.
+ pow pow, good debate, hack the, hack
$ barackobama, barackobama, the vp,
good job, to vote, john is, is to, obama did,
they both, gergen, knowledge, voting for,
for veterans, the veterans, america, will take
? language, this was, drinking, terrorists,
government, china, obama i, that we, father,
obama in, mc, diplomacy, wars, afghanistan,
debt, simply, financial, the spin, the bottom,
bottom
Table 7: Top 20 most positive and most negative n-grams
in OMD after running LPROP with All-edges and Noisy-
seed. Note that ?$? indicates the beginning or end of a
tweet.
Noisy-seed) and LPROP (All-edges, Noisy-seed) are
tied for the lowest error.
4.4 Per-target accuracy
Table 6 gives results on a per-target basis for the
five most common targets in the HCR-TEST dataset,
in order from most common to least common: hcr,
dems, obama, gop, and conservatives. The per-
centages reflect the fraction of tweets correctly la-
beled for each target. These distributions are highly
skewed: the hcr target covers about 69% of the
tweets, while the conservatives target covers only
about 5%. Thus performance on the hcr target
tweets is most important for overall accuracy.
5 Discussion
Polar language An attractive property of label
propagation algorithms is that label distributions can
be obtained for nodes other than the tweets (and im-
+ human, stupak, you do, sunday, fired
vote for, yes on, $ we, vote yes, to vote,
vote on, goal, nation, do it, up to, ago, votes,
this #hcr, #hcr is, on #hcr
? gop, #tlot #hcr, #tcot #tlot, 12, #topprog,
medicare, #tlot, #tlot $, #ocra, cbo,
tea party, tea, passes, #hhrs, $ dems, #hc,
#obamacare, #sgp, dems, do not
Table 8: Top 20 most positive and most negative n-grams
in HCR-TEST after running LPROP with All-edges and
Noisy-seed.
portantly, nodes that were unseeded). For example,
all of the feature nodes?unigrams, bigrams, and
hashtags?have a loading for the positive and neg-
ative labels. These could be used for various vi-
sualizations of the results of the polarity classifica-
tion, including terms that are the most positive and
negative and also highlighting or bolding such terms
when showing a user individual tweets.
Table 7 shows the 20 unigrams and bigrams with
the highest and lowest ratio of positive label prob-
ability to negative label probability after running
LPROP with All-edges and Noisy-seed. These lists
are restricted to terms that had an edge weight of at
least 1.0, i.e. that were twice as frequent in OMD
compared to the reference corpus, that had a raw
count of at least 5 in OMD, and that didn?t al-
ready appear in the OpinionFinder lexicon. Some of
the terms are intuitively positive and negative, e.g.
good job and wars. Others reflect more specific as-
pects of the OMD dataset, such as good debate and
afghanistan.
Table 8 shows the top 20 for HCR-TEST. Many
59
hcr dems obama gop conservatives
Classifier (274) (27) (26) (22) (20)
LEXRATIO 58.0 64.8 69.2 50.0 52.5
EMOMAXENT 62.4 66.7 73.1 68.2 60.0
LPROP (Follower-edges, Maxent-seed) 62.4 66.7 73.1 68.2 60.0
LPROP (All-edges, Lexicon-seed) 60.6 85.2 73.1 86.4 60.0
LPROP (Feature-edges, Noisy-seed) 69.0 81.5 80.8 86.4 70.0
LPROP (All-edges, Noisy-seed) 69.0 77.8 80.8 86.4 70.0
Table 6: Per-target accuracy percentages for HCR-TEST. The number of tweets for each target is given in parentheses.
terms simply reflect a rallying to either pass or defeat
the healthcare reform bill (vote for, do not). Other
positive words represent more abstract concepts pro-
ponents of the bill may be expressing (human, goal).
Conversely, opponents such as those who would at-
tend a tea party are concerned about what they call
#obamacare.
Domain differences There are several reasons
why performance is much lower on both the OMD
and HCR datasets than on STS. First, both the
EMOTICON (noisy) training set and the STS dev set
are general in topic. Correct estimations of the posi-
tivity and negativity of general words in the training
set like yay and upset are more likely to be useful
in a broad-domain evaluation set, whereas misesti-
mations of the weights of more specific words and
bigrams are likely to be washed out. In contrast,
the OMD and HCR datasets contain a very differ-
ent vocabulary distribution from the STS set. Words
and phrases referring to specific political issues like
health care and iraq war have frequencies that are
orders of magnitude higher than either the EMOTI-
CON training set or the STS dev set. Thus, misesti-
mations of the positivity or negativity of these fea-
tures will be amplified in evaluation. Lastly, expres-
sion of political opinions tends to be more nuanced
than the general opinions and feelings, simply due
to the complex nature of political issues. Everyone
agrees that a sore throat is bad, while it is less ob-
vious how much government involvement in health
care is beneficial.
LEXRATIO vs. EMOMAXENT LEXRATIO has
low coverage for words that tend to indicate positive
and negative sentiment in particular domains. For
example, STS has the tweet In montreal for a long
weekend of R&R. Much needed, with a positive gold
label. The only word in this tweet in the Opinion-
Finder lexicon is long, which is labeled as negative.
Thus, LEXRATIO incorrectly classifies the tweet as
negative. EMOMAXENT correctly labels this tweet
positive due to features like weekend being strong
indicators of the positive class. Similarly, the tweet
Booz Allen Hamilton has a bad ass homegrown so-
cial collaboration platform. Way cool! #ttiv is la-
beled negative by LEXRATIO due to the presence of
bad. While EMOMAXENT has a negative preference
for both bad and ass, it has a strong positive prefer-
ence for bad ass, as well as both cool and way cool.
EMOMAXENT vs. LPROP As seen from the per-
tweet and per-user results, LPROP does consistently
better than MAXENT. We now discuss one example
of this improvement from the OMD set. One user
authored the following four tweets:
? t1: obama +3 the conspicuousness of their pres-
ence is only matched by our absence #tweetdebate
? t2: Fundamentally, if McCain fundamentally uses
?fundamental? one more time, I?m gonna go nuts.
#tweetdebate
? t3: McCain likes the bears in Montana joke too
much#tweetdebate #current
? t4: We are less respected now... Obama #current
#debate08 And I give credit to McCain... NOOO
The gold label for t1 is positive and the rest are nega-
tive. All of the LPROP classifiers correctly predicted
the labels for all four tweets. EMOMAXENT missed
t2 and t3, so this primarily negative user is incor-
rectly indicated as primarily positive by EMOMAX-
ENT. LPROP gets around this by propagating senti-
ment polarity through unigram features in this case.
60
The unigram mccain has an edge weight to tweets
that contain it of 8.6 for the OMD corpus, meaning
mccain is much more frequent in this corpus than
the reference corpus, so any sentiment associated
with mccain is propagated strongly. In this case, the
output of label propagation seeded with Noisy-seed
reveals that mccain has negative sentiment for this
dataset.
6 Related Work
Much work in sentiment analysis involves the use
and generation of dictionaries capturing the senti-
ment of words. These methods range from manual
approaches of developing domain-dependent lexi-
cons (Das and Chan, 2001) to semi-automated ap-
proaches (Hu and Liu, 2004) and fully automated
approaches (Turney, 2002). Melville et al (2009)
use a unified framework combining background lex-
ical information in terms of word-class associations
and refine this information for specific domains us-
ing any available training examples. They produce
better results than using either a lexicon or training.
O?Connor et al (2010) use the OpinionFinder
subjectivity lexicon to label the polarity of tweets
about Barack Obama and compare daily aggregate
sentiment scores to the Gallup poll time series of
manually gathered approval ratings of Obama. Even
with this simple polarity determination, they find
significant correlation between their predicted ag-
gregate sentiment per day and the Gallup poll.
Using the OMD dataset, Shamma et al (2009)
find that amount of Twitter activity is a good pre-
dictor of topic changes during the debate, and that
the content of concurrent tweets reflects a mix of
the current debate topic and Twitter users? reactions
to that topic. Diakopoulos and Shamma (2010) use
the same dataset to develop analysis and visualiza-
tion techniques to aid journalists and others in un-
derstanding the relationship between the live debate
event and the timestamped tweets.
Bollen et al (2010) perform aggregate sentiment
analysis on tweets over time, comparing predicted
sentiment to time series such as the stock market
and crude oil prices, as well as major events such
as election day and Thanksgiving. However, the au-
thors use hand-built rules for classification based on
the Profile of Mood States (POMS) and largely eval-
uate based on inspection.
7 Conclusion
We have improved upon existing tweet polarity clas-
sification methods by combining several knowledge
sources with a noisily supervised label propagation
algorithm. We show that a maximum entropy clas-
sifier trained with distant supervision works better
than a lexicon-based ratio predictor, improving the
accuracy for polarity classification on our held-out
test set from 58.1% to 62.9%. By using the predic-
tions of that classifier in combination with a graph
that incorporates tweets and lexical features, we ob-
tain even better accuracy of 71.2%.
We did not find overall gains from using the fol-
lower graph as implemented here. There is room
for improvement in the way the follower graph is
encoded in our graph, particularly with respect to
using asymmetric relationships rather than an undi-
rected graph, and in how follower relationships are
weighted.
Another source of information that could be used
to improve results is the text in pages that have
been linked to from a tweet. In many cases, it is
only possible to know what the polarity is by look-
ing at the page being linked to. Our label propa-
gation setup can incorporate this straightforwardly
by adding nodes for those pages plus edges between
them and all tweets that reference them.
Acknowledgments
This research was supported by a grant from the
Morris Memorial Trust Fund of the New York Com-
munity Trust. We thank Leif Johnson for providing
the tweets from the Twitter firehose for the EMOTI-
CON and HCR datasets, Partha Talukdar for the
Junto label propagation toolkit, and the UT Natural
Language Learning reading group for helpful feed-
back.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing,
Jay Yagnik, Shankar Kumar, Deepak Ravichandran,
and Mohamed Aly. Video suggestion and discovery
for youtube: taking random walks through the view
graph. In WWW ?08: Proceeding of the 17th interna-
tional conference on World Wide Web, pages 895?904,
New York, NY, USA, 2008. ACM.
61
S. Blair-Goldensohn, K. Hannan, R. McDonald, T. Ney-
lon, G. Reis, and J. Reynar. Building a sentiment
summarizer for local service reviews. In WWW
Workshop on NLP in the Information Explosion Era
(NLPIX), 2008. URL http://www.ryanmcd.
com/papers/local_service_summ.pdf.
J. Bollen, A. Pepe, and H. Mao. Modeling public mood
and emotion: Twitter sentiment and socio-economic
phenomena. In Proceedings of the 19th International
World Wide Web Conference, 2010.
Samuel Brody and Noemie Elhadad. An unsupervised
aspect-sentiment model for online reviews. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, HLT ?10,
pages 804?812, Stroudsburg, PA, USA, 2010. As-
sociation for Computational Linguistics. ISBN 1-
932432-65-5. URL http://portal.acm.org/
citation.cfm?id=1857999.1858121.
Yejin Choi and Claire Cardie. Adapting a polar-
ity lexicon using integer linear programming for
domain-specific sentiment classification. In Proceed-
ings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 590?
598. Association for Computational Linguistics, 2009.
URL http://www.aclweb.org/anthology/
D/D09/D09-1062.
S. Das and M. Chan. Extracting market sentiment from
stock message boards. Asia Pacific Finance Associa-
tion, 2001, 2001.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. En-
hanced sentiment learning using twitter hashtags and
smileys. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, 2010.
Nicholas A. Diakopoulos and David A. Shamma. Char-
acterizing debate performance via aggregated twitter
sentiment. In Proceedings of the 28th international
conference on Human factors in computing systems,
pages 1195?1198, 2010.
Alec Go, Richa Bhayani, and Lei Huang. Twitter senti-
ment classification using distant supervision. Unpub-
lished manuscript. Stanford University, 2009.
Minqing Hu and Bing Liu. Mining and summarizing cus-
tomer reviews. In KDD ?04: Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177, New
York, NY, USA, 2004. ACM. ISBN 1-58113-888-1.
doi: http://doi.acm.org/10.1145/1014052.1014073.
Prem Melville, Wojciech Gryc, and Richard D.
Lawrence. Sentiment analysis of blogs by combin-
ing lexical knowledge with text classification. In KDD
?09: Proceedings of the 15th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 1275?1284, New York, NY, USA, 2009.
ACM. ISBN 978-1-60558-495-9. doi: http://doi.acm.
org/10.1145/1557019.1557156.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. From tweets
to polls: Linking text sentiment to public opinion
time series. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?:
sentiment classification using machine learning tech-
niques. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing-
Volume 10, pages 79?86, 2002.
Bo Pang and Lillian Lee. A sentimental education: senti-
ment analysis using subjectivity summarization based
on minimum cuts. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, 2004.
Bo Pang and Lillian Lee. Opinion mining and sentiment
analysis. Foundations and Trends in Information Re-
trieval, 2(1-2):1?135, 2008.
Delip Rao and Deepak Ravichandran. Semi-supervised
polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 675?682. Association for Com-
putational Linguistics, 2009. URL http://www.
aclweb.org/anthology/E09-1077.
Jonathon Read. Using emoticons to reduce dependency
in machine learning techniques for sentiment classifi-
cation. In Proceedings of the ACL Student Research
Workshop, ACLstudent ?05, pages 43?48, Strouds-
burg, PA, USA, 2005. Association for Computational
Linguistics. URL http://portal.acm.org/
citation.cfm?id=1628960.1628969.
David A. Shamma, Lyndon Kennedy, and Elizabeth F.
Churchill. Tweet the debates: understanding commu-
nity annotation of uncollected sources. In Proceedings
of the first SIGMM workshop on Social media, pages
3?10, 2009.
Vikas Sindhwani and Prem Melville. Document-word co-
regularization for semi-supervised sentiment analysis.
In Proceedings of IEEE International Conference on
Data Mining (ICDM-08), 2008.
Partha Talukdar and Koby Crammer. New regularized
algorithms for transductive learning. In Wray Bun-
tine, Marko Grobelnik, Dunja Mladenic, and John
Shawe-Taylor, editors, Machine Learning and Knowl-
edge Discovery in Databases, volume 5782, pages
442?457. Springer Berlin / Heidelberg, 2009.
62
P. D. Turney. Thumbs up or thumbs down? semantic ori-
entation applied to unsupervised classification of re-
views. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
417?424, 2002.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. Opin-
ionFinder: A system for subjectivity analysis. In Proc.
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP-2005) Companion Volume
(software demonstration), 2005.
Xiaojin Zhu and Zoubin Ghahramani. Learning from la-
beled and unlabeled data with label propagation. Tech-
nical Report CMU-CALD-02-107, Carnegie Mellon
University, 2002.
63
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141?150,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Weakly-Supervised Bayesian Learning of a CCG Supertagger
Dan Garrette
?
Chris Dyer
?
Jason Baldridge
?
Noah A. Smith
?
?
Department of Computer Science, The University of Texas at Austin
?
School of Computer Science, Carnegie Mellon University
?
Department of Linguistics, The University of Texas at Austin
?
Corresponding author: dhg@cs.utexas.edu
Abstract
We present a Bayesian formulation for
weakly-supervised learning of a Combina-
tory Categorial Grammar (CCG) supertag-
ger with an HMM. We assume supervi-
sion in the form of a tag dictionary, and
our prior encourages the use of cross-
linguistically common category structures
as well as transitions between tags that
can combine locally according to CCG?s
combinators. Our prior is theoretically ap-
pealing since it is motivated by language-
independent, universal properties of the
CCG formalism. Empirically, we show
that it yields substantial improvements
over previous work that used similar bi-
ases to initialize an EM-based learner. Ad-
ditional gains are obtained by further shap-
ing the prior with corpus-specific informa-
tion that is extracted automatically from
raw text and a tag dictionary.
1 Introduction
Unsupervised part-of-speech (POS) induction is a
classic problem in NLP. Many proposed solutions
are based on Hidden Markov models (HMMs), with
various improvements obtainable through: induc-
tive bias in the form of tag dictionaries (Kupiec,
1992; Merialdo, 1994), sparsity constraints (Lee
et al., 2010), careful initialization of parameters
(Goldberg et al., 2008), feature based represen-
tations (Berg-Kirkpatrick et al., 2010; Smith and
Eisner, 2005), and priors on model parameters
(Johnson, 2007; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011, inter alia).
When tag dictionaries are available, a situa-
tion we will call type-supervision, POS induc-
tion from unlabeled corpora can be relatively suc-
cessful; however, as the number of possible tags
increases, performance drops (Ravi and Knight,
2009). In such cases, there are a large number
of possible labels for each token, so picking the
right one simply by chance is unlikely; the pa-
rameter space tends to be large; and devising good
initial parameters is difficult. Therefore, it is un-
surprising that the unsupervised (or even weakly-
supervised) learning of a Combinatory Categorial
Grammar (CCG) supertagger, which labels each
word with one of a large (possibly unbounded)
number of structured categories called supertags,
is a considerable challenge.
Despite the apparent complexity of the task, su-
pertag sequences have regularities due to univer-
sal properties of the CCG formalism (?2) that can
be used to reduce the complexity of the problem;
previous work showed promising results by using
these regularities to initialize an HMM that is then
refined with EM (Baldridge, 2008). Here, we ex-
ploit CCG?s category structure to motivate a novel
prior over HMM parameters for use in Bayesian
learning (?3). This prior encourages (i) cross-
linguistically common tag types, (ii) tag bigrams
that can combine using CCG?s combinators, and
(iii) sparse transition distributions. We also go be-
yond the use of these universals to show how ad-
ditional, corpus-specific information can be auto-
matically extracted from a combination of the tag
dictionary and raw data, and how that information
can be combined with the universal knowledge for
integration into the model to improve the prior.
We use a blocked sampling algorithm to sam-
ple supertag sequences for the sentences in the
training data, proportional to their posterior prob-
ability (?4). We experimentally verify that
our Bayesian formulation is effective and sub-
stantially outperforms the state-of-the-art base-
line initialization/EM strategy in several languages
(?5). We also evaluate using tag dictionaries that
are unpruned and have only partial word coverage,
finding even greater improvements in these more
realistic scenarios.
141
2 CCG and Supertagging
CCG (Steedman, 2000; Steedman and Baldridge,
2011) is a grammar formalism in which each lex-
ical token is associated with a structured category,
often referred to as a supertag. CCG categories are
defined by the following recursive definition:
C ? {S, N, NP, PP, ...}
C ? {C/C,C\C}
A CCG category can either be an atomic cate-
gory indicating a particular type of basic gram-
matical phrase (S for a sentence, N for a noun,
NP for a noun phrase, etc), or a complex category
formed from the combination of two categories
by one of two slash operators. In CCG, complex
categories indicate a grammatical relationship be-
tween the two operands. For example, the cate-
gory (S\NP)/NP might describe a transitive verb,
looking first to its right (indicated by /) for an ob-
ject, then to its left (\) for a subject, to produce a
sentence. Further, atomic categories may be aug-
mented with features, such as S
dcl
, to restrict the
set of atoms with which they may unify. The task
of assigning a category to each word in a text is
called supertagging (Bangalore and Joshi, 1999).
Because they are recursively defined, there is
an infinite number of potential CCG categories
(though in practice it is limited by the number
of actual grammatical contexts). As a result, the
number of supertags appearing in a corpus far ex-
ceeds the number of POS tags (see Table 1). Since
supertags specify the grammatical context of a to-
ken, and high frequency words appear in many
contexts, CCG grammars tend to have very high
lexical ambiguity, with frequent word types asso-
ciating with a large number of categories. This
ambiguity has made type-supervised supertagger
learning very difficult because the typical ap-
proaches to initializing parameters for EM become
much less effective.
Grammar-informed supertagger learning.
Baldridge (2008) was successful in extending the
standard type-supervised tagger learning to the
task of CCG supertagging by setting the initial
parameters for EM training of an HMM using
two intrinsic properties of the CCG formalism:
the tendency for adjacent tags to combine, and
the tendency to use less complex tags. These
properties are explained in detail in the original
work, but we restate the ideas briefly throughout
this paper for completeness.
X/Y Y ? X (>)
Y X\Y ? X (<)
X/Y Y/Z ? X/Z (>B)
Y \Z X\Y ? X\Z (<B)
Y/Z X\Y ? X/Z (<B
?
)
Figure 1: Combination rules used by CCGBank.
S
NP
NP/N
N
S\NP
(S\NP)/NP
NP
NP/N
N
The
man
walks
a
dog
Figure 2: CCG parse for ?The man walks a dog.?
Tag combinability. A CCG parse of a sentence is
derived by recursively combining the categories of
sub-phrases. Category combination is performed
using only a small set of generic rules (see Fig-
ure 1). In the tree in Figure 2, we can see that
a and dog can combine via Forward Application
(>), with NP/N and N combining to produce NP.
The associativity engendered by CCG?s compo-
sition rules means that most adjacent lexical cate-
gories may be combined. In the Figure 2 tree, we
can see that instead of combining (walks?(a?dog)),
we could have combined ((walks?a)?dog) since
(S\NP)/NP and NP/N can combine using >B.
3 Model
In this section we define the generative process
we use to model a corpus of sentences. We begin
by generating the model parameters: for each
supertag type t in the tag set T , the transition
probabilities to the next state (pi
t
) and the emis-
sion probabilities (?
t
) are generated by draws
from Dirichlet distributions parameterized with
per-tag mean distributions (pi
0
t
and ?
0
t
, respec-
tively) and concentration parameters (?
pi
and
?
?
). By setting ?
pi
close to zero, we can encode
our prior expectation that transition distributions
should be relatively peaked (i.e., that each tag
type should be followed by relatively few tag
types). The prior means, discussed below, encode
both linguistic intuitions about expected tag-tag
transition behavior and automatically-extracted
corpus information. Given these parameters, we
next generate the sentences of the corpus. This
process is summarized as follows:
142
Parameters:
?
t
? Dirichlet(?
?
, ?
0
t
) ?t ? T
pi
t
? Dirichlet(?
pi
, pi
0
t
) ?t ? T
Sentence:
y
1
? Categorical(pi
?S?
)
for i ? {1, 2, . . .}, until y
i
= ?E?
x
i
| y
i
? Categorical(?
y
i
)
y
i+1
| y
i
? Categorical(pi
y
i
)
This model can be understood as a Bayesian
HMM (Goldwater and Griffiths, 2007). We next
discuss how the prior distributions are constructed
to build in additional inductive bias.
3.1 Transition Prior Means (pi
0
t
)
We use the prior mean for each tag?s transition dis-
tribution to build in two kinds of bias. First, we
want to favor linguistically probable tags. Second,
we want to favor transitions that result in a tag
pair that combines according to CCG?s combina-
tors. For simplicity, we will define pi
0
t
as a mixture
of two components, the first, P
pi
(u) is an (uncon-
ditional) distribution over category types u that fa-
vors cross-linguistically probable categories. The
second component, P
pi
(u | t), conditions on the
previous tag type, t, and assigns higher probabil-
ity to pairs of tags that can be combined. That is,
the probability of transitioning from t to u in the
Dirichlet mean distribution is given by
1
pi
0
t
(u) = ? ? P
pi
(u) + (1? ?) ? P
pi
(u | t).
We discuss the two mixture components in turn.
3.1.1 Unigram Category Generator (P
pi
(u))
In this section, we define a CCG category gener-
ator that generates cross-linguistically likely cat-
egory types. Baldridge?s approach estimated the
likelihood of a category using the inverse number
of sub-categories: P
CPLX
(u) ? 1/complexity(u).
We propose an improvement, P
G
, expressed as a
probabilistic grammar:
2
C ? a p
term
?p
atom
(a)
C ? A/A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A/B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
C ? A\A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A\B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
1
Following Baldridge (2008), we fix ? = 0.5 for our ex-
periments.
2
For readability, we use the notation p = (1? p).
where A,B,C are categories and a is an atomic
category (and terminal): a ? {S, N, NP, ...}.
3
We have designed this grammar to capture sev-
eral important CCG characteristics. In particular
we encode four main ideas, each captured through
a different parameter of the grammar and dis-
cussed in greater detail below:
1. Simpler categories are more likely: e.g. N/N is
a priori more likely than (N/N)/(N/N).
2. Some atoms are more likely than others: e.g.
NP is more likely than S, much more than NP
nb
.
3. Modifiers are more likely: e.g. (S\NP)/(S\NP)
is more likely than (S\NP)/(NP\NP).
4. Operators occur with different frequencies.
The first idea subsumes the complexity measure
used by Baldridge, but accomplishes the goal nat-
urally by letting the probabilities decrease as the
category grows. The rate of decay is governed
by the p
term
parameter: the marginal probability
of generating a terminal (atomic) category in each
expansion. A higher p
term
means a stronger em-
phasis on simplicity. The probability distribution
over categories is guaranteed to be proper so long
as p
term
>
1
2
since the probability of the depth of a
tree will decrease geometrically (Chi, 1999).
The second idea is a natural extension of the
complexity concept and is particularly relevant
when features are used. The original complex-
ity measure treated all atoms uniformly, but e.g.
we would expect NP
expl
/N to be less likely than
NP/N since it contains the more specialized, and
thus rarer, atom NP
expl
. We define the distribution
p
atom
(a) as the prior over atomic categories.
Due to our weak, type-only supervision, we
have to estimate p
atom
from just the tag dictionary
and raw corpus, without frequency data. Our goal
is to estimate the number of each atom in the su-
pertags that should appear on the raw corpus to-
kens. Since we don?t know what the correct su-
pertags are, we first estimate counts of supertags,
from which we can extract estimated atom counts.
Our strategy is to uniformly distribute each raw
corpus token?s counts over all of its possible su-
pertags, as specified in the tag dictionary. Word
types not appearing in the tag dictionary are ig-
3
While very similar to standard probabilistic context-free
grammars seen in NLP work, this grammar is not context-free
because modifier categories must have matching operands.
However, this is not a problem for our approach since the
grammar is unambiguous, defines a proper probability distri-
bution, and is only used for modeling the relative likelihoods
of categories (not parsing categories).
143
nored for the purposes of these estimates. Assum-
ing that C(w) is the number of times that word
type w is seen in the raw corpus, atoms(a, t) is the
number of times atom a appears in t, TD(w) is the
set of tags associated with w, and TD(t) is the set
of word types associated with t:
C
supertag
(t) =
?
w?TD(t)
(C(w)+?)/|TD(w)|
C
atom
(a) =
?
t?T
atoms(a, t) ? C
supertag
(t)
p
atom
(a) ? C
atom
(a) + ?
Adding ? smooths the estimates.
Using the raw corpus and tag dictionary data to
set p
atom
allows us to move beyond Baldridge?s
work in another direction: it provides us with a
natural way to combine CCG?s universal assump-
tions with corpus-specific data.
The third and fourth ideas pertain only to com-
plex categories. If the category is complex, then
we consider two additional parameters. The pa-
rameter p
fw
is the marginal probability that the
complex category?s operator specifies a forward
argument. The parameter p
mod
gives the amount
of marginal probability mass that is allocated for
modifier categories. Note that it is not necessary
for p
mod
to be greater than
1
2
to achieve the de-
sired result of making modifier categories more
likely than non-modifier categories: the number
of potential modifiers make up only a tiny fraction
of the space of possible categories, so allocating
more than that mass as p
mod
will result in a cate-
gory grammar that gives disproportionate weight
to modifiers, increasing the likelihood of any par-
ticular modifier from what it would otherwise be.
3.1.2 Bigram Category Generator (P
pi
(u | t))
While the above processes encode important prop-
erties of the distribution over categories, the in-
ternal structure of categories is not the full story:
cross-linguistically, the categories of adjacent to-
kens are much more likely to be combinable via
some CCG rule. This is the second component of
our mixture model.
Baldridge derives this bias by allocating the ma-
jority of the transition probability mass from each
tag t to tags that can follow t according to some
combination rule. Let ?(t,u) be an indicator of
whether t connects to u; for ? ? [0, 1]:
4
P
?
(u | t) =
{
? ? uniform(u) if ?(t,u)
(1? ?) ? uniform(u) otherwise
4
Again, following Baldridge (2008), we fix ? = 0.95 for
our experiments.
There are a few additional considerations that
must be made in defining ?, however. In assum-
ing the special tags ?S? and ?E? for the start and
end of the sentence, respectively, we can define
?(?S?,u) = 1 when u seeks no left-side argu-
ments (since there are no tags to the left with
which to combine) and ?(t, ?E?) = 1 when t seeks
no right-side arguments. So ?(?S?, NP/N) = 1, but
?(?S?, S\NP) = 0. If atoms have features asso-
ciated, then the atoms are allowed to unify if the
features match, or if at least one of them does
not have a feature. So ?(NP
nb
, S\NP) = 1, but
?(NP
nb
, S\NP
conj
) = 0. In defining ?, it is also im-
portant to ignore possible arguments on the wrong
side of the combination since they can be con-
sumed without affecting the connection between
the two. To achieve this for ?(t,u), it is assumed
that it is possible to consume all preceding argu-
ments of t and all following arguments of u. So
?(NP, (S\NP)/NP) = 1. This helps to ensure the
associativity discussed earlier. Finally, the atom
NP is allowed to unify with N if N is the argument.
So ?(N, S\NP) = 1, but ?(NP/N, NP) = 0. This is
due to the fact that CCGBank assumes that N can
be rewritten as NP.
Type-supervised initialization. As above, we
want to improve upon Baldridge?s ideas by en-
coding not just universal CCG knowledge, but
also automatically-induced corpus-specific infor-
mation where possible. To that end, we can de-
fine a conditional distribution P
tr
(u | t) based on
statistics from the raw corpus and tag dictionary.
We use the same approach as we did above for set-
ting p
atom
(and the definition of ?
0
t
below): we esti-
mate by evenly distributing raw corpus counts over
the tag dictionary entries. Assume that C(w
1
, w
2
)
is the (?-smoothed) count of times word type w
1
was directly followed byw
2
in the raw corpus, and
ignoring any words not found in the tag dictionary:
C(t,u) = ?+
?
w
1
?TD(t), w
2
?TD(u)
C(w
1
, w
2
)
|TD(w
1
)| ? |TD(w
2
)|
P
tr
(u | t) = C(t,u)/
?
u
?
C(t,u
?
)
Then the alternative definition of the compatibility
distribution is as follows:
P
tr
?
(u | t) =
{
? ? P
tr
(u | t) if ?(t,u)
(1??) ? P
tr
(u | t) otherwise
144
Our experiments compare performance when
pi
0
t
is set using P
pi
(u)=P
CPLX
(experiment 3) ver-
sus our category grammar P
G
(4?6), and using
P
pi
(u | t) = P
?
as the compatibility distribution
(3?4) versus P
tr
?
(5?6).
3.2 Emission Prior Means (?
0
t
)
For each supertag type t, ?
0
t
is the mean distri-
bution over words it emits. While Baldridge?s
approach used a uniform emission initialization,
treating all words as equally likely, we can,
again, induce token-level corpus-specific informa-
tion:
5
To set ?
0
t
, we use a variant and simplifica-
tion of the procedure introduced by Garrette and
Baldridge (2012) that takes advantage of our prior
over categories P
G
.
Assuming that C(w) is the count of word type
w in the raw corpus, TD(w) is the set of supertags
associated with word type w in the tag dictionary,
and TD(t) is the set of known word types associ-
ated with supertag t, the count of word/tag pairs
for known words (words appearing in the tag dic-
tionary) is estimated by uniformly distributing a
word?s (?-smoothed) raw counts over its tag dic-
tionary entries:
C
known
(t, w) =
{
C(w)+?
|TD(w)|
if t ? TD(w)
0 otherwise
For unknown words, we first use the idea of tag
?openness? to estimate the likelihood of a partic-
ular tag t applying to an unknown word: if a tag
applies to many word types, it is likely to apply to
some new word type.
P (unk | t) ? |known words w s.t. t ? TD(w)|
Then, we apply Bayes? rule to get P (t | unk), and
use that to estimate word/tag counts for unknown
words:
P (t | unk) ? P (unk | t) ? P
G
(t)
C
unk
(t, w) = C(w) ? P (t | unk)
Thus, with the estimated counts for all words:
P
em
(w | t) =
C
known
(t, w) + C
unk
(t, w)
?
w
?
C
known
(t, w
?
) + C
unk
(t, w
?
)
We perform experiments comparing perfor-
mance when ?
0
t
is uniform (3?5) and when
?
0
t
(w) = P
em
(w | t) (6).
5
Again, without gold tag frequencies.
4 Posterior Inference
We wish to find the most likely supertag of each
word, given the model we just described and a cor-
pus of training data. Since there is exact inference
with these models is intractable, we resort to Gibbs
sampling to find an approximate solution. At a
high level, we alternate between resampling model
parameters (?
t
, pi
t
) given the current tag sequence
and resampling tag sequences given the current
model parameters and observed word sequences.
It is possible to sample a new tagging from the
posterior distribution over tag sequences for a sen-
tence, given the sentence and the HMM parameters
using the forward-filter backward-sample (FFBS)
algorithm (Carter and Kohn, 1996). To effi-
ciently sample new HMM parameters, we exploit
Dirichlet-multinomial conjugacy. By repeating
these alternating steps and accumulating the num-
ber of times each supertag is used in each position,
we obtain an approximation of the required poste-
rior quantities.
Our inference procedure takes as input the tran-
sition prior means pi
0
t
, the emission prior means
?
0
t
, and concentration parameters ?
pi
and ?
?
,
along with the raw corpus and tag dictionary. The
set of supertags associated with a word w will be
known as TD(w). We will refer to the set of word
types included in the tag dictionary as ?known?
words and others as ?unknown? words. For sim-
plicity, we will assume that TD(w), for any un-
known word w, is the full set of CCG categories.
During sampling, we always restrict the possible
tag choices for a word w to the categories found in
TD(w). We refer to the sequence of word tokens
as x and tags as y.
We initialize the sampler by setting pi
t
= pi
0
t
and ?
t
= ?
0
t
and then sampling tagging sequences
using FFBS.
To sample a tagging for a sentence x, the strat-
egy is to inductively compute, for each token x
i
starting with i = 0 and going ?forward?, the prob-
ability of generating x
0
, x
1
, . . . , x
i
via any tag se-
quence that ends with y
i
= u:
p(y
i
= u | x
0:i
) =
?
u
(x
i
) ?
?
t?T
pi
t
(u) ? p(y
i?1
= t | x
0:i?1
)
We then pass through the sequence again, this time
?backward? starting at i = |x| ? 1 and sampling
y
i
| y
i+1
? p(y
i
= t | x
0:i
) ? pi
t
(y
i+1
).
145
num. raw TD TD ambiguity dev test
Corpus tags tokens tokens entries type token tokens tokens
English
CCGBank POS 50
158k 735k
45k 3.75 13.11 ? ?
CCGBank 1,171 65k 56.98 296.18 128k 127k
Chinese CTB-CCG 829 99k 439k 60k 96.58 323.37 59k 85k
Italian CCG-TUT 955 6k 27k 9k 178.88 426.13 5k 5k
Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT
is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates
are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English
POS statistics are shown only for comparison; only CCG experiments were run.
The block-sampling approach of choosing new
tags for a sentence all at once is particularly ben-
eficial given the sequential nature of the model of
the HMM. In an HMM, a token?s adjacent tags tend
to hold onto its current tag due to the relation-
ships between the three. Resampling all tags at
once allows for more drastic changes at each it-
eration, providing better opportunities for mixing
during inference. The FFBS approach has the ad-
ditional advantage that, by resampling the distri-
butions only once per iteration, we are able to re-
sample all sentences in parallel. This is not strictly
true of all HMM problems with FFBS, but because
our data is divided by sentence, and each sentence
has a known start and end tag, the tags chosen dur-
ing the sampling of one sentence cannot affect the
sampling of another sentence in the same iteration.
Once we have sampled tags for the entire cor-
pus, we resample pi and ?. The newly-sampled
tags y are used to compute C(w, t), the count of
tokens with word type w and tag t, and C(t,u),
the number of times tag t is directly followed by
tag u. We then sample, for each t ? T where T is
the full set of valid CCG categories:
pi
t
? Dir
(
??
pi
? pi
0
t
(u) + C(t,u)?
u?T
)
?
t
? Dir
(
??
?
? ?
0
t
(w) + C(w, t)?
w?V
)
It is important to note that this method of re-
sampling allows the draws to incorporate both the
data, in the form of counts, and the prior mean,
which includes all of our carefully-constructed bi-
ases derived from both the intrinsic, universal CCG
properties as well as the information we induced
from the raw corpus and tag dictionary.
With the distributions resampled, we can con-
tinue the procedure by resampling tags as above,
and then resampling distributions again, until a
maximum number of iterations is reached.
5 Experiments
6
To evaluate our approach, we used CCGBank
(Hockenmaier and Steedman, 2007), which is
a transformation of the English Penn Treebank
(Marcus et al., 1993); the CTB-CCG (Tse and
Curran, 2010) transformation of the Penn Chinese
Treebank (Xue et al., 2005); and the CCG-TUT
corpus (Bos et al., 2009), built from the TUT cor-
pus of Italian text (Bosco et al., 2000). Statistics
on the size and ambiguity of these datasets are
shown in Table 1.
For CCGBank, sections 00?15 were used for
extracting the tag dictionary, 16?18 for the raw
corpus, 19?21 for development data, and 22?24
for test data. For TUT, the first 150 sentences of
each of the CIVIL LAW and NEWSPAPER sections
were used for raw data, the next sentences 150?
249 of each was used for development, and the
sentences 250?349 were used for test; the remain-
ing data, 457 sentences from CIVIL LAW and 548
from NEWSPAPER, plus the much smaller 132-
sentence JRC ACQUIS data, was used for the tag
dictionary. For CTB-CCG, sections 00?11 were
used for the tag dictionary, 20?24 for raw, 25?27
for dev, and 28?31 for test.
Because we are interested in showing the rel-
ative gains that our ideas provide over Baldridge
(2008), we reimplemented the initialization pro-
cedure from that paper, allowing us to evaluate
all approaches consistently. For each dataset, we
ran a series of experiments in which we made fur-
ther changes from the original work. We first ran
a baseline experiment with uniform transition and
emission initialization of EM (indicated as ?1.? in
Table 2) followed by our reimplementation of the
initialization procedure by Baldridge (2). We then
6
All code and experimental scripts are available
at http://www.github.com/dhgarrette/
2014-ccg-supertagging
146
Corpus English Chinese Italian
TD cutoff 0.1 0.01 0.001 no 0.1 0.01 0.001 no 0.1 0.01 0.001 no
1. uniform EM 77 62 47 38 64 39 30 26 51 32 30 30
2. init (Baldridge) EM 78 67 55 41 66 43 33 28 54 36 33 32
3. init Bayes 74 68 56 42 65 56 47 37 52 46 40 40
4. P
G
Bayes 74 70 59 42 64 57 47 36 52 40 39 40
5. P
G
, P
tr
?
Bayes 75 72 61 50 66 58 49 44 52 44 41 43
6. P
G
, P
tr
?
, P
em
Bayes 80 80 73 51 69 62 56 49 53 47 45 46
Table 2: Experimental results: test-set per-token supertag accuracies. ?TD cutoff? indicates the level of
tag dictionary pruning; see text. (1) is uniform EM initialization. (2) is a reimplementation of (Baldridge,
2008). (3) is Bayesian formulation using only the ideas from Baldridge: P
CPLX
, P
?
, and uniform emis-
sions. (4?6) are our enhancements to the prior: using our category grammar in P
G
instead of P
CPLX
, using
P
tr
?
instead of P
?
, and using P
em
instead of uniform.
experimented with the Bayesian formulation, first
using the same information used by Baldridge, and
then adding our enhancements: using our category
grammar in P
G
, using P
tr
?
as the transition com-
patability distribution, and using P
em
as ?
0
t
(w).
For each dataset, we ran experiments using four
different levels of tag dictionary pruning. Prun-
ing is the process of artificially removing noise
from the tag dictionary by using token-level anno-
tation counts to discard low-probability tags; for
each word, for cutoff x, any tag with probability
less than x is excluded. Tag dictionary pruning
is a standard procedure in type-supervised train-
ing, but because it requires information that does
not truly conform to the type-supervised scenario,
we felt that it was critical to demonstrate the per-
formance of our approach under situations of less
pruning, including no artificial pruning at all.
We emphasize that unlike in most previous
work, we use incomplete tag dictionaries. Most
previous work makes the unrealistic assumption
that the tag dictionary contains an entry for ev-
ery word that appears in either the training or test-
ing data. This is a poor approximation of a real
tagging system, which will never have complete
lexical knowledge about the test data. Even work
that only assumes complete knowledge of the tag-
ging possibilities for the lexical items in the train-
ing corpus is problematic (Baldridge, 2008; Ravi
et al., 2010). This still makes learning unrealisti-
cally easy since it dramatically reduces the ambi-
guity of words that would have been unseen, and,
in the case of CCG, introduces additional tags that
would not have otherwise been known. To ensure
that our experiments are more realistic, we draw
our tag dictionary entries from data that is totally
disjoint from both the raw and test corpora. Dur-
ing learning, any unknown words (words not ap-
pearing in the tag dictionary) are unconstrained so
that they may take any tag, and are, thus, maxi-
mally ambiguous.
We only performed minimal parameter tuning,
choosing instead to stay consistent with Baldridge
(2008) and simply pick reasonable-seeming val-
ues for any additional parameters. Any tuning that
was performed was done with simple hill-climbing
on the development data of English CCGBank.
All parameters were held consistent across exper-
iments, including across languages. For EM, we
used 50 iterations; for FFBS we used 100 burn-
in iterations and 200 sampling iterations.
7
For
all experiments, we used ? = 0.95 for P
(tr)
?
and
? = 0.5 for pi
0
t
to be consistent with previous
work, ?
pi
= 3000, ?
?
= 7000, p
term
= 0.6,
p
fw
= 0.5, p
mod
= 0.8, and ? = 1000 for p
atom
.
Test data was run only once, for the final figures.
The final results reported were achieved by us-
ing the following training sequence: initialize pa-
rameters according to the scenario, train an HMM
using EM or FFBS starting with that set of parame-
ters, tag the raw corpus with the trained HMM, add-
0.1 smooth counts from the now-tagged raw cor-
pus, and train a maximum entropy Markov model
(MEMM) from this ?auto-supervised? data.
8
Results are shown in Table 2. Most notably, the
contributions described in this paper improve re-
sults in nearly every experimental scenario. We
can see immediate, often sizable, gains in most
7
Final counts are averaged across the sampling iterations.
8
Auto-supervised training of an MEMM increases accu-
racy by 1?3% on average (Garrette and Baldridge, 2013). We
use the OpenNLP MEMM implementation with its standard
set of features: http://opennlp.apache.org
147
cases simply by using the Bayesian formulation.
Further gains are seen from adding each of the
other various contributions of this paper. Perhaps
most interestingly, the gains are only minimal with
maximum pruning, but the gains increase as the
pruning becomes less aggressive ? as the scenar-
ios become more realistic. This indicates that our
improvements make the overall procedure more
robust.
Error Analysis Like POS-taggers, the learned
supertagger frequently confuses nouns (N) and
their modifiers (N/N), but the most frequent er-
ror made by the English (6) experiment was
(((S\NP)\(S\NP))/N) instead of (NP
nb
/N). How-
ever, these are both determiner types, indicating an
interesting problem for the supertagger: it often
predicts an object type-raised determiner instead
of the vanilla NP/N, but in many contexts, both cat-
egories are equally valid. (In fact, for parsers that
use type-raising as a rule, this distinction in lexical
categories does not exist.)
6 Related Work
Ravi et al. (2010) also improved upon the work by
Baldridge (2008) by using integer linear program-
ming to find a minimal model of supertag transi-
tions, thereby generating a better starting point for
EM than the grammatical constraints alone could
provide. This approach is complementary to the
work presented here, and because we have shown
that our work yields gains under tag dictionaries
of various levels of cleanliness, it is probable that
employing minimization to set the base distribu-
tion for sampling could lead to still higher gains.
On the Bayesian side, Van Gael et al. (2009)
used a non-parametric, infinite HMM for truly un-
supervised POS-tagger learning (Van Gael et al.,
2008; Beal et al., 2001). While their model is not
restricted to the standard set of POS tags, and may
learn a more fine-grained set of labels, the induced
labels are arbitrary and not grounded in any gram-
matical formalism.
Bisk and Hockenmaier (2013) developed an ap-
proach to CCG grammar induction that does not
use a tag dictionary. Like ours, their procedure
learns from general properties of the CCG formal-
ism. However, while our work is intended to pro-
duce categories that match those used in a partic-
ular training corpus, however complex they might
be, their work produces categories in a simplified
form of CCG in which N and S are the only atoms
and no atoms have features. Additionally, they as-
sume that their training corpus is annotated with
POS tags, whereas we assume truly raw text.
Finally, we find the task of weakly-supervised
supertagger learning to be particularly relevant
given the recent surge in popularity of CCG.
An array of NLP applications have begun using
CCG, including semantic parsing (Zettlemoyer and
Collins, 2005) and machine translation (Weese et
al., 2012). As CCG finds more applications, and
as these applications move to lower-resource do-
mains and languages, there will be increased need
for the ability to learn without full supervision.
7 Conclusion and Future Work
Standard strategies for type-supervised HMM es-
timation are less effective as the number of cat-
egories increases. In contrast to POS tag sets,
CCG supertags, while quite numerous, have struc-
tural clues that can simplify the learning prob-
lem. Baldridge (2008) used this formalism-
specific structure to inform an initialization pro-
cedure for EM. In this work, we have shown that
CCG structure can instead be used to motivate an
effective prior distribution over the parameters of
an HMM supertagging model, allowing our work
to outperform Baldridge?s previously state-of-the-
art approach, and to do so in a principled manner
that lends itself better to future extensions such as
incorporation in more complex models.
This work also improves on Baldridge?s simple
?complexity? measure, developing instead a prob-
abilistic category grammar over supertags that al-
lows our prior to capture a wider variety of inter-
esting and useful properties of the CCG formalism.
Finally, we were able to achieve further gains
by augmenting the universal CCG knowledge with
corpus-specific information that could be automat-
ically extracted from the weak supervision that is
available: the raw corpus and the tag dictionary.
This allows us to combine the cross-linguistic
properties of the CCG formalism with corpus- or
language-specific information in the data into a
single, unified Bayesian prior.
Our model uses a relatively large number of pa-
rameters, e.g., p
term
, p
fw
, p
mod
, p
atom
, in the prior.
Here, we fixed each to a single value (i.e., a ?fully
Bayesian? approach). Future work might explore
sensitivity to these choices, or empirical Bayesian
or maximum a posteriori inference for their values
(Johnson and Goldwater, 2009).
148
In this work, as in most type-supervised work,
the tag dictionary was automatically extracted
from an existing tagged corpus. However, a tag
dictionary could instead be automatically induced
via multi-lingual transfer (Das and Petrov, 2011)
or generalized from human-provided information
(Garrette and Baldridge, 2013; Garrette et al.,
2013). Again, since the approach presented here
has been shown to be somewhat robust to tag dic-
tionary noise, it is likely that the model would
perform well even when using an automatically-
induced tag dictionary.
Acknowledgements
This work was supported by the U.S. Department
of Defense through the U.S. Army Research Of-
fice (grant number W911NF-10-1-0533). Exper-
iments were run on the UTCS Mastodon Cluster,
provided by NSF grant EIA-0303609.
References
Jason Baldridge. 2008. Weakly supervised supertag-
ging with grammar-informed initialization. In Pro-
ceedings of COLING.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The innite hidden Markov
model. In NIPS.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
model for inducing combinatory categorial gram-
mars. Transactions of the Association for Compu-
tational Linguistics, 1.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of ACL.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In M. Pas-
sarotti, Adam Przepi?orkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8).
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank
for Italian: a data-driven annotation schema. In Pro-
ceedings of LREC.
Christopher K. Carter and Robert Kohn. 1996. On
Gibbs sampling for state space models. Biometrika,
81(3):341?553.
Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1).
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of POS-
taggers for low-resource languages. In Proceedings
of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: Ex-
periments on unsupervised word segmentation with
adaptor grammars. In Proceedings of NAACL.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
149
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010.
Minimized models and grammar-informed initial-
ization for supertagging with highly ambiguous lex-
icons. In Proceedings of ACL, pages 495?503.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Borjars, editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar. Wiley-
Blackwell.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: Extracting CCG derivations from the Penn
Chinese treebank. In Proceedings of COLING.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite hidden Markov model. In Proceedings of
ICML.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of EMNLP.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proceedings of WMT.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
150
