Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 713?720
Manchester, August 2008
Almost Flat Functional Semantics for Speech Translation
Manny Rayner1, Pierrette Bouillon1, Beth Ann Hockey2, Yukie Nakao3
1 University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch
2 UCSC UARC, Mail Stop 19-26
NASA Ames Research Center
Moffet Field, CA 94035
bahockey@ucsc.edu
3 University of Nantes, LINA
2, rue de la Houssinie`re
BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
We introduce a novel semantic represen-
tation formalism, Almost Flat Functional
semantics (AFF), which is designed as an
intelligent compromise between linguis-
tically motivated predicate/argument se-
mantics and ad hoc engineering solutions
based on flat feature/value lists; the cen-
tral idea is to tag each semantic element
with the functional marking which most
closely surrounds it. We argue that AFF is
well-suited for medium-vocabulary speech
translation applications, and describe sim-
ple and general algorithms for parsing,
generating and performing transfer using
AFF representations. The formalism has
been fully implemented within a medium-
vocabulary interlingua-based Open Source
speech translation system which translates
between English, French, Japanese and
Arabic.
1 Introduction
Many speech translation architectures require
some way to represent the meaning of spoken ut-
terances, but even a brief review of the literature
reveals a serious divergence of opinion as to how
this may best be done. At risk of oversimplifying
a little, there are two competing heritages. On the
one hand, there is the mainstream computational
semantics approach, which ultimately goes back
to philosophers like Montague, Russell and Frege
and views predicate calculus as the paradigm rep-
resentation language. On this view of things, a
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
suitable way to represent meaning is to use com-
plex structures, in which components and relation-
ships are based on deep grammatical functions.
Typical ways to realise this strategy are unscoped
logical forms, neo-Davidsonian semantics, mini-
mal recursion semantics, and similar formalisms.
Thus a sentence like ?I want a pepperoni pizza?
might be represented as something like
want1(E, X, Y),
ref(X, pronoun(i)),
quant(Y, indef), pizza1(Y),
pepperoni1(Z), nn(Z, Y)
Approaches based in the linguistic tradition were
dominant about 10 to 15 years ago, when they were
used in major systems like Germany?s Verbmo-
bil (Wahlster, 2000) and SRI?s Spoken Language
Translator (Rayner et al, 2000). They are still
reasonably popular today, as exemplified by major
systems like PARC?s XLE (Riezler et al, 2002).
The competing heritage has its roots in engi-
neering approaches to spoken language systems,
which historically have been intimately connected
with Machine Learning. On this view of things,
a typical semantic representation is a flat list of
feature-value pairs, with the features represent-
ing semantic concepts: here, ?I want a pepperoni
pizza? would be represented as something like
[utterance_type=request,
food=pizza, type=pepperoni]
It is interesting to see how little contact there
has been between these two traditions. Writers
on formal semantics usually treat ad hoc feature-
value representations as not even worthy of serious
discussion. Conversely, proponents of engineer-
ing/machine learning approaches often assume in
practice that all semantic representations will be
some version of a flat feature-value list; a good
713
example of this tendency is Young?s widely cited
2002 survey of machine learning approaches to
spoken dialogue (Young, 2002).
Trying to be as neutral as possible, it is reason-
able to argue that both approaches have important
things to offer, and that it is worth trying to find
some compromise between them. Other things be-
ing equal, flat feature-value representations have
desirable formal properties: they are simple, and
easy to manipulate and reason with. Their draw-
back is that they are an impoverished represen-
tation language, which can lose important infor-
mation. This means that concepts may be im-
possible to represent, or, alternately viewed, that
the representation format may conflate concepts
which we would prefer to distinguish. In the other
direction, hierarchical logic-based representations
are highly expressive, but pose much more serious
challenges in terms of formal manipulability. Al-
though they are more easily capable of represent-
ing semantic distinctions, it is harder to use them to
perform concrete reasoning operations. In transla-
tion systems, these abstract issues manifest them-
selves in a tradeoff between complexity of trans-
lation rules, and ambiguity of semantic represen-
tations. A flat semantic representation formalism
means that translation rules are simple to write;
however, it also means that the semantic represen-
tations they operate on are more likely to be am-
biguous.
In this paper, we will explore the tradeoffs be-
tween the two competing positions outlined above
in the context of a concrete Open Source sys-
tem, the MedSLT medical speech translator. Pre-
vious versions of MedSLT have used a represen-
tation strategy intermediate between the ?logic-
based semantics? and the ?flat semantics? ap-
proaches, though much closer to the ?flat? end of
the scale. We will discuss the strengths and weak-
nesses of the original MedSLT representation for-
malism, and then present a revised version, ?Al-
most Flat Functional Semantics? (AFF). As the
name suggests, AFF incorporates functional mark-
ings, characteristic of a logic-based semantics ap-
proach, into a representation formalism which still
mainly consists of flat list structures. We will show
how grammars using semantics written in AFF can
be compiled into parsers and generators, and de-
scribe a simple formalism that can be used to spec-
ify rules for translating AFF expressions into AFF
expressions. Finally, we will show how use of AFF
in MedSLT has allowed us to address in a princi-
pled way most of the examples which are problem-
atic for the original version of the system, while
still retaining a simple and transparent framework
for writing translation rules.
2 The MedSLT System
MedSLT (Bouillon et al, 2005) is a medium-
vocabulary Open Source speech translation system
for medical domains, implemented using the Open
Source Regulus compiler (Rayner et al, 2006)
and the Nuance recognition platform. Process-
ing is primarily rule-based. Recognition uses a
grammar-based language model, which produces
a source-langage semantic representation. This is
first translated by one set of rules into an interlin-
gual form, and then by a second set into a target
language representation. A target-language gram-
mar, compiled into generation form, turns this into
one or more possible surface strings, after which
a set of generation preferences picks one out. Fi-
nally, the selected string is realised in spoken form.
There is also some use of corpus-based statistical
methods, both to tune the language model (Rayner
et al, 2006, Section 11.5) and to drive a robust em-
bedded help system (Chatzichrisafis et al, 2006).
The treatment of syntactic structure is a care-
fully thought-out compromise between linguistic
and engineering traditions. All grammars used
are extracted from general linguistically motivated
resource grammars, using corpus-based methods
driven by small sets of examples (Rayner et al,
2006, Chapter 9). This results in a simpler and flat-
ter grammar specific to the domain, whose struc-
ture is similar to the ad hoc phrasal grammars typ-
ical of engineering approaches. The treatment of
semantics is however less sophisticated, and ba-
sically represents a minimal approach in the en-
gineering tradition. Each lexicon item contributes
a set of zero or more feature-value pairs (in most
cases exactly one pair). Most of the grammar rules
simply concatenate the sets of pairs received from
their daughters. A small number of rules, primarily
those for subordinate clauses, create a nested sub-
structure representing the embedded clause. Fig-
ure 1 shows an example representation.
It should be obvious from the example that the
flat representation is potentially very ambiguous,
since nearly all information about grammatical
functions has been lost. The example also illus-
trates, however, why this is often unimportant in
714
[[utterance_type,sentence],
[pronoun,vous],
[path_proc,avoir],
[voice,active],
[tense,present],
[cause,nause?e], [sc,quand],
[clause,
[[pronoun,vous],
[symptom,mal],
[path_proc,avoir],
[voice,active],
[tense,present]]])]
Figure 1: Semantic representation produced by the
current MedSLT system for the French sentence
Avez-vous des nause?es quand vous avez mal? (?Do
you have nausea when you have the pain??)
practice. From a purely syntactic point of view,
the fragment
[[pronoun,vous],
[path_proc,avoir],
[cause,nause?e]]
could either represent vous avez des nause?es (?you
have nausea?) or des nause?es vous ont (?nausea
has you?). Except, possibly, in certain kinds of
literary contexts, the second realisation is so im-
plausible that it can be discounted. It is thus rea-
sonable to add sortal constraints to the lexical en-
tries involved, which permit des nause?es to occur
in well-formed utterances as the object of avoir,
but not as its subject. Thus the representation is
in fact unambiguous, and will only generate one
surface realisation.
With the moderate vocabularies used by Med-
SLT (for example, the current French module has
a vocabulary of about 1 100 surface words), the
vast majority of constructions can be rendered un-
ambiguous using similar strategies. The result is
that most translation rules are easy to write, since
they have to do no more than map lists of feature-
value pairs to lists of feature-value pairs. To take
a typical example, the Japanese question itami wa
koutoubu desu ka (?pain-TOPIC back-part-head is-
Q?) receives the representation
[[utterance_type,sentence],
[symptom,itami],
[body_part,koutoubu],
[verb,desu], [tense,present]]
which we wish to map to the interlingua represen-
tation
[[utterance_type,ynq], [verb,be],
[tense,present], [voice,active],
[symptom,pain], [prep,in_loc],
[part,back], [body_part,head]]
(?is the pain in the back of the head?). In a more
expressive semantic framework, the structural mis-
matches here would be non-trivial to resolve. In
the flat MedSLT notation, we only need the fol-
lowing two list-to-list translation rules:
transfer_rule(
[[body_part,koutoubu]],
[[prep,in_loc], [part,back],
[body_part,head]]).
(koutoubu ? ?in the back of the head?) and
transfer_rule(
[[verb, desu]],
[[verb, be]]).
(desu ? ?is?).
As usual, however, we pay a price for simplicity.
In the terminology of Statistical Machine Transla-
tion, what we are essentially doing here is weaken-
ing the channel model, and relying on the strength
of the target language model. This is a reason-
able strategy partly because of the restricted nature
of the domain, and partly because of the fact that
the initial parsing stage makes it possible for us
to work with bags of concepts rather than bags of
words; clearly, bags of concepts are more expres-
sive.
None the less, it is normal to expect the un-
derspecified channel model to cause some prob-
lems, and this indeed proves to be the case. Al-
though most semantic relationships in the domain
are unambiguous even as bags of concepts (?back
of the head? is possible; ?head of the back? isn?t),
there are unpleasant counterexamples. For in-
stance, {?visit?, ?doctor?, ?patient?} can be re-
alised as either ?patient visits doctor? or ?doc-
tor visits patient?. Similarly, {?precede?, ?nau-
sea?, ?headache?} can be either ?nausea precedes
headache? or ?headache precedes nausea?. Cases
like these must be dealt with using ad hoc solu-
tions based on domain pragmatics. In the current
version of the system, ?patient visits doctor? is
forced by producing both surface realisations, and
defining a generation preference. In the case of
{?precede?, ?nausea?, ?headache?}, the problem
is addressed by dividing symptoms into ?primary?
(the symptom the patient is being examined for,
e.g. ?headache?) and ?secondary? (other possibly
715
utterance:[sem=concat(Verb, [[tag, obj, Np]])] -->
verb:[sem=Verb], np:[sem=Np].
np:[sem=concat(Adj, Noun)] -->
spec:[], ?adj:[sem=Adj], noun:[sem=Noun].
np:[sem=concat(Np, PP)] -->
np:[sem=Np], pp:[sem=PP].
pp:[sem=[[tag, Tag, Np]]] -->
prep:[sem=Tag], np:[sem=Np].
verb:[sem=[[action, grasp]]] --> grasp.
noun:[sem=[[thing, block]]] --> block.
noun:[sem=[[loc, table]]] --> table.
adj:[sem=[[colour, red]]] --> red.
spec:[] --> the.
prep:[sem=on] --> on.
Figure 2: Toy grammar with nested predicate-argument semantics.
related symptoms, e.g. ?nausea?). It is reasonable
in practice to assume that the doctor will only be
interested in secondary symptoms that may cause
primary ones, and hence will precede them.
Although each language in the current version
of MedSLT only contains a handful of similar
cases, solutions like those outlined above are both
inelegant and brittle. It would be desirable to find
some more principled way to deal with them; we
would, however, like to do this without sacrificing
the appealing simplicity of the translation rule for-
malism. In the next section, we will show how it is
possible to reconcile these two conflicting goals.
3 Almost Flat Functional Semantics
As we have seen, the problem with a simple bag-
of-concepts representation is its ambiguity; what
we would like to do is find some principled way to
reduce that ambiguity, without greatly increasing
the formalism?s representational complexity. At
this point, a linguistic intuition is helpful. The
bag-of-concepts representation can reasonably be
thought of as an artificial free word-order lan-
guage. There are many natural free word-order
languages; the reason why they are in general no
more ambiguous than fixed word-order languages
is that they use case-marking to convey functional
information which constrains the space of pos-
sible interpretations. For speakers of European
languages, the best-known example will proba-
bly be Classical Latin. For instance, when St.
Jerome wrote Amor ordinem nescit (?love-NOM
order-ACC not-know-PRES-3-SING?), the case-
markings make it clear that he meant ?Love does
not know order? rather than ?Order does not know
love?.
The comparison with free word-order languages
suggests a natural extension of the original bag-
of-concepts representation, where each element is
associated with an additional functional tag which
does the work that a case-marking would do in a
natural free word-order language. It also suggests
a simple construction which can be used to cre-
ate an unordered linear representation that includes
functional tags. We start by defining a standard
nested predicate-argument semantics; we then flat-
ten the representation of each clause S, marking
each primitive semantic element with the imme-
diately surrounding functional tag in S, or with a
null marking if there is no such tag. The resulting
semantic representations still represent each clause
as an unordered list, but in contrast to the MedSLT
bag-of-concepts representation now include func-
tional information. We will call this style of rep-
resentation Almost Flat Functional (AFF) seman-
tics; the ?almost? comes from the fact that there is
still a minimal amount of nested structure, repre-
senting the distinction between main and embed-
ded clauses.
Figures 2 and 3 give a concrete illustration of the
716
[[action, grasp], [null=[action, grasp],
[tag, obj, obj=[colour, red],
[[colour, red], obj=[thing, block],
[thing, block], on=[loc, table]]
[tag, on,
[[loc, table]]]]]]
Figure 3: Construction of AFF representation for ?grasp the red block on the table?. The AFF represen-
tation (right) is a flattened version of the original nested predicate-argument one (left).
AFF construction. Figure 2 presents a toy Regulus
grammar, which allows a few sentences like ?grasp
the red block on the table? and assigns a nested
functional semantics to them. The representations
of most constituents are unordered lists. In the case
of utterance and np, these are formed by con-
catenating the representations of their daughters.
There are two examples of functional markings:
the rule for utterance wraps an [tag, obj
...] around its np daughter, and the rule for pp
wraps a tag around its np daughter, whose label is
determined by the semantic value of the p daugh-
ter.
Figure 3 introduces the AFF construction itself.
The left-hand side of the figure shows the nested
predicate-argument representation of the sentence,
in which elements of the form
[tag, Tag, Arg]
represent tags and their associated arguments. The
right-hand side shows the derived AFF representa-
tion, where each element that is within the scope of
a [tag ...] has been marked with the tag that
would be immediately above it in the nested ver-
sion. Thus the element [loc, table] is inside
the scope of both the obj tag and the on tag; how-
ever, the AFF version assigns it the on tag, since
this is the innermost one.
In the rest of this section, we will describe how
we can parse surface strings into AFF represen-
tations, generate surface strings from AFF repre-
sentations, and define translation rules which map
AFF representations to AFF representations.
3.1 Analysis and generation
For both analysis and generation, the starting point
is a grammar with a nested predicate-argument se-
mantics like the one shown in the left half of Fig-
ure 3. Analysis is straightforward. We first use a
standard parser-generator to compile the grammar
into a parser; the nested predicate-argument repre-
sentations it produces are then subjected to a post-
processing phase, which flattens them in the way
illustrated in the figure.
This simple approach is however not feasible for
generation, since the flattening operation is highly
non-deterministic in the reverse direction; finding
all possible ?unflattenings? and then attempting
to generate from each one would in most cases
be hopelessly inefficient. A better solution is to
transform the original grammar into one with AFF
semantics, where the current functional marking
is specified as an extra features on relevant con-
stituents, and percolated through the rules. In ef-
fect, the ?unflattening? and generation operations
can now proceed simultaneously, with each one
constraining the other.
Figure 4 presents an example, showing the re-
sult of performing this transformation on the toy
Regulus grammar from Figure 2. Here, the origi-
nal [tag, ...] wrappers have been removed,
and replaced by the new feature tag, which has
been added to all constituents whose semantics is
a list of items of the form Tag=Value. The value
of the tag feature on each constituent where it
is defined is the tag for the most closely enclos-
ing [tag, ...] in the original grammar; these
values are percolated down to the lexical rules,
where they unify with the tags on the semantic
fragment contributed by the rule. The transforma-
tion is straightforward to define in its general form,
and the transformed grammars can be readily com-
piled into efficient generators by standard feature-
grammar generator-compiler algorithms like Se-
mantic Head-Driven Generation (Shieber et al,
1990). For the concrete experiments described
later, we used a slightly extended version of the
Open Source Regulus generator compiler.
3.2 Transfer
Our basic strategy for defining transfer between
AFF expressions is to make it as close as pos-
sible to transfer on the original bag-of-concepts
717
utterance:[sem=concat(Verb, Np)] -->
verb:[sem=Verb, tag=null], np:[sem=Np, tag=obj].
np:[sem=concat(Adj, Noun), tag=Tag] -->
spec:[], ?adj:[sem=Adj, tag=Tag], noun:[sem=Noun, tag=Tag].
np:[sem=concat(Np, PP), tag=Tag] -->
np:[sem=Np, tag=Tag], pp:[sem=PP, tag=Tag].
pp:[sem=Np] -->
prep:[sem=Tag], np:[sem=Np, tag=Tag].
verb:[sem=[Tag=[action, grasp]], tag=Tag] --> grasp.
noun:[sem=[Tag=[thing, block]], tag=Tag] --> block.
noun:[sem=[Tag=[loc, table]], tag=Tag] --> table.
adj:[sem=[Tag=[colour, red]], tag=Tag] --> red.
spec:[] --> the.
prep:[sem=on] --> on.
Figure 4: Version of grammar from Figure 2 after transformation to AFF semantics.
representations, which is conditional mapping of
lists to lists. Since AFF is an extension of bag-
of-concepts, and bag-of-concepts is usually suffi-
ciently unambiguous as it stands, we only want
to add the functional markings in the cases where
they are required. Most of our rules will thus still
be transfer rules like the ones shown in Sec-
tion 2, except that they now map lists of function-
marking-tagged items to lists of function-marking-
tagged items; however, in accordance with the
stated design principles, we allow tags to be omit-
ted when desired, with the convention that an omit-
ted tag denotes an uninstantiated tag value.
One of the underlying linguistic intuitions be-
hind AFF is that there are correspondences be-
tween functional markings in different languages,
with each given functional marking f
s
in the
source language typically mapping to a specific
functional marking f
t
in the target language.
For this reason, it would be highly unnatural
only to specify transformations of tag values us-
ing transfer rules. We consequently intro-
duce a second kind of rule, which we call a
tag transfer rule; as the name suggests,
this defines a direct mapping from tags to tags.
Given the fact that functional tags have some claim
to universality, it is reasonable to hope that many
tags will map onto themselves. Thus a typical tag
rule might map the English subj tag to the Arabic
subj tag, which we write as
tag_transfer_rule(subj, subj).
Most tag transfer rules will be of the above
simple form. However, there are always cases
where languages diverge structurally, and here it
will be necessary to make the tag transfer rule con-
ditional on its surrounding context. For example,
English constructions with the verb ?last? (?Does
the headache last more than ten minutes??) are
realised differently in Arabic, using the transitive
verb tahus bi (?feel?), thus here hal tahus bi al
soudaa li akthar min achr daqayq? (?Do (you)
feel the headache during more than ten minutes??).
Here, ?headache? is marked as subj in English,
but the correspoding Arabic word, soudaa, is the
obj of tahus bi. We express the general fact that
we wish to map subj to obj in the context of the
verb ?last? using the rule
tag_transfer_rule(subj, obj) :-
context([state, last]).
We also require a normal transfer rule
which maps ?last? to tahus bi. This also has to
introduce an implicit second person subject, so the
full rule is
transfer_rule(
[[state, last]],
[[state, tahus_bi],
subj=[pronoun, anta]]).
(anta = ?you?). Related sets of rules of this kind
can be written more concisely with a small exten-
718
sion to the formalism, as follows:
transfer_rule(
[[state, last]],
[[state, tahus_bi],
subj=[pronoun, anta]],
[subj:obj]).
An important question we have so far postponed
discussing is how to fill in unspecified tag values
on the RHS of a transfer rule application.
At first, we believed that several possible strate-
gies were feasible; rather to our surprise, examina-
tion of some examples convinced us that only one
of these strategies actually made sense. The algo-
rithm is as follows. We assume a transfer -
rule R, whose LHS has successfully matched a
set of tag/concept pairs, and consider the following
cases:
1. R explicitly assigns values to all of the tags
on its RHS. There is nothing more to do.
2. Not all of the tags on the RHS are assigned
values by R. Apply tag transfer rules
to all the matched tags on the LHS which
were not originally assigned values by R, giv-
ing a set of tags {T
1
...T
n
}. There are now two
subcases:
(a) n = 1, i.e. only one transferred tag is
produced. Set the values of all the unin-
stantiated tags on the transferred RHS to
T
1
.
(b) n > 1, i.e. several different transferred
tags are produced. Leave the values of
the ininstantiated tags on the transferred
RHS uninstantiated.
The least obvious part of this is (2a), which
is easier to understand when we consider some
more specific cases. The simplest and most com-
mon example is the case where R is a ?lex-
ical? transfer rule which contains exactly
one tag/concept pair on each side, each tag be-
ing left unspecified. We evidently need to apply
a tag transfer rule to the tag matched by the
single pair on the LHS, to get the value of the tag
attached to the transferred RHS.
To take a slightly more complex case, consider
an English ? Japanese rule which maps the ex-
pression ?back of the head? to the single word
koutoubu. We could write this as
transfer_rule(
[[part, back],
of=[body_part,head]],
[[body_part, koutoubu]])
Here, it is clear that we want to translate the tag
on the source-language pair that matches [part,
back], and assign it to the target-language ele-
ment [body part, koutoubu]. The transla-
tion of the tag of is irrelevant.
4 Using AFF in MedSLT
We have implemented and tested a version of AFF
inside the Open Source MedSLT system, building
AFF versions of the grammars for English, French
Japanese, Arabic and the Interlingua. We also cre-
ated AFF versions of the translation rules between
the four surface languages and the Interlingua, in
both directions. Coverage and performance of the
two versions of the system on development data
were essentially the same; the key differences were
architectural in nature. We now briefly summarise
these differences.
The basic tradeoff is between analysis and gen-
eration on one hand, and translation on the other.
The more expressive AFF formalism implies that
representations are less ambiguous, which means
fewer problems in the analysis and generation
components. The downside is that the translation
rules become more complex. On the positive side,
switching from bag-of-concepts to AFF allowed us
to implement clean solutions to a substantial num-
ber of problems which were previously handled in
an ad hoc manner. As previously noted, English
constructions using verbs like ?precede?, ?cause?,
?accompany?, ?visit? and ?be in contact with? are
in general ambiguous in the bag-of-words repre-
sentation, and had to be solved by artificially con-
straining their arguments; AFF makes it possible
to do this by simply differentiating between subj
and obj tags. Similar considerations applied to
constructions in the other two languages. For ex-
ample, using bag-of-words, the Arabic frequency
expressions thalath marrat fi al ousbou (?three
times a week?) and marra kul thalathat assabii
(?once every three weeks?) were previously rep-
resented in the same way, necessitating addition of
a brittle generation preference. AFF once again
allows the two expressions to be cleanly distin-
guished.
It was evident from the start that we would
win on this kind of example; what was less clear
719
was the price we would have to pay, in terms
of increased complexity of the transfer rule set.
Gratifyingly, the conservative nature of the ex-
tension meant that this price turned out to be
quite low. We had originally wondered whether
it would be necessary to write many condi-
tional tag transfer rules, or add functional
tags to a large proportion of the transfer -
rules. In fact, out of the total of 4444 rules
used by the eight language pairs together, only
39 (0.9%) were conditional tag transfer -
rules, and 524 (11.8%) were transfer rules
containing at least one functional tag. A fur-
ther 120 rules (2.7%) were unconditional tag -
transfer rules. The remaining 3761 rules
(84.6%) were transfer rules which did not
explicitly mention functional tags, and were thus
essentially bag-of-concepts mapping rules. To
summarise, less than a sixth of the rules were af-
fected by moving to the new framework.
5 Summary and Conclusions
We have described Almost Flat Functional seman-
tics, a formalism which adds functional markings
to a flat atheoretical feature/value representation.
The additional functional information in AFF is
sufficient to resolve nearly all of the representa-
tional ambiguities which caused problems for the
flat bag-of-concepts formalism. In terms of repre-
sentational complexity, however, the AFF formal-
ism appears to be only slightly less tractable than
bag-of-concepts. It seems reasonable to us that,
like bag-of-concepts, it could also support learn-
able surface-oriented parsing; this could be com-
bined with statistical recognition to provide a ro-
bust back-up to grammar-based speech processing
(Rayner et al, 2005), a claim that we hope to inves-
tigate empirically in the near future. It is much less
clear that full logic-based representations could be
used for such purposes.
What we find interesting here, from a general
perspective, is that we were able to create a re-
duced, but still essentially clean, form of a main-
stream linguistic treatment, and incorporate it into
an ad hoc engineering framework in a way that
only marginally affected that framework?s perfor-
mance characteristics. Without wishing to exag-
gerate the importance of our results, we think ex-
amples like AFF suggest that the gulf between
these two types of approach is not, perhaps, as
wide as is sometimes suggested.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Rayner, M., D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors. 2000. The Spoken Language
Translator. Cambridge University Press.
Rayner, M., P. Bouillon, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for comparing grammar-based and robust ap-
proaches to speech understanding. In Proceedings
of the 9th International Conference on Spoken Lan-
guage Processing (ICSLP), pages 1103?1107, Lis-
boa, Portugal.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Riezler, S., T.H. King, R.M. Kaplan, R. Crouch, J.T.
Maxwell, and M. Johnson. 2002. Parsing the
wall street journal using a lexical-functional gram-
mar and discriminative estimation techniques. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (demo track),
Philadelphia, PA.
Shieber, S., G. van Noord, F.C.N. Pereira, and R.C.
Moore. 1990. Semantic-head-driven generation.
Computational Linguistics, 16(1).
Wahlster, W., editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
Young, S. 2002. Talking to machines (statistically
speaking). In Proceedings of the 7th International
Conference on Spoken Language Processing (IC-
SLP), pages 9?16, Denver, CO.
720
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 26?27,
Vancouver, October 2005.
Japanese Speech Understanding Using Grammar Specialization
Manny Rayner, Nikos Chatzichrisafis, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
mrayner@riacs.edu
{Pierrette.Bouillon,Nikolaos.Chatzichrisafis}@issco.unige.ch
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Beth Ann Hockey
UCSC/NASA Ames Research Center
Moffet Field, CA 94035
bahockey@riacs.edu
Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Marianne.Starlander@eti.unige.ch
The most common speech understanding archi-
tecture for spoken dialogue systems is a combination
of speech recognition based on a class N-gram lan-
guage model, and robust parsing. For many types
of applications, however, grammar-based recogni-
tion can offer concrete advantages. Training a
good class N-gram language model requires sub-
stantial quantities of corpus data, which is gen-
erally not available at the start of a new project.
Head-to-head comparisons of class N-gram/robust
and grammar-based systems also suggest that users
who are familiar with system coverage get better re-
sults from grammar-based architectures (Knight et
al., 2001). As a consequence, deployed spoken dia-
logue systems for real-world applications frequently
use grammar-based methods. This is particularly
the case for speech translation systems. Although
leading research systems like Verbmobil and NE-
SPOLE! (Wahlster, 2000; Lavie et al, 2001) usu-
ally employ complex architectures combining sta-
tistical and rule-based methods, successful practical
examples like Phraselator and S-MINDS (Phrasela-
tor, 2005; Sehda, 2005) are typically phrasal trans-
lators with grammar-based recognizers.
Voice recognition platforms like the Nuance
Toolkit provide CFG-based languages for writing
grammar-based language models (GLMs), but it is
challenging to develop and maintain grammars con-
sisting of large sets of ad hoc phrase-structure rules.
For this reason, there has been considerable inter-
est in developing systems that permit language mod-
els be specified in higher-level formalisms, normally
some kind of unification grammar (UG), and then
compile these grammars down to the low-level plat-
form formalisms. A prominent early example of this
approach is the Gemini system (Moore, 1998).
Gemini raises the level of abstraction signifi-
cantly, but still assumes that the grammars will be
domain-dependent. In the Open Source REGULUS
project (Regulus, 2005; Rayner et al, 2003), we
have taken a further step in the direction of increased
abstraction, and derive all recognizers from a sin-
gle linguistically motivated UG. This derivation pro-
cedure starts with a large, application-independent
UG for a language. An application-specific UG is
then derived using an Explanation Based Learning
(EBL) specialization technique. This corpus-based
specialization process is parameterized by the train-
ing corpus and operationality criteria. The training
corpus, which can be relatively small, consists of ex-
amples of utterances that should be recognized by
the target application. The sentences of the corpus
are parsed using the general grammar, then those
parses are partitioned into phrases based on the op-
erationality criteria. Each phrase defined by the
operationality criteria is flattened, producing rules
of a phrasal grammar for the application domain.
This application-specific UG is then compiled into
26
a CFG, formatted to be compatible with the Nuance
recognition platform. The CFG is compiled into the
runtime recognizer using Nuance tools.
Previously, the REGULUS grammar specialization
programme has only been implemented for English.
In this demo, we will show how we can apply the
same methodology to Japanese. Japanese is struc-
turally a very different language from English, so it
is by no means obvious that methods which work
for English will be applicable in this new context:
in fact, they appear to work very well. We will
demo the grammars and resulting recognizers in the
context of Japanese ? English and Japanese ?
French versions of the Open Source MedSLT medi-
cal speech translation system (Bouillon et al, 2005;
MedSLT, 2005).
The generic problem to be solved when building
any sort of recognition grammar is that syntax alone
is insufficiently constraining; many of the real con-
straints in a given domain and use situation tend to
be semantic and pragmatic in nature. The challenge
is thus to include enough non-syntactic constraints
in the grammar to create a language model that can
support reliable domain-specific speech recognition:
we sketch our solution for Japanese.
The basic structure of our current general
Japanese grammar is as follows. There are four main
groups of rules, covering NP, PP, VP and CLAUSE
structure respectively. The NP and PP rules each as-
sign a sortal type to the head constituent, based on
the domain-specific sortal constraints defined in the
lexicon. VP rules define the complement structure
of each syntactic class of verb, again making use of
the sortal features. There are also rules that allow
a VP to combine with optional adjuncts, and rules
which allow null constituents, in particular null sub-
jects and objects. Finally, clause-level rules form a
clause out of a VP, an optional subject and optional
adjuncts. The sortal features constrain the subject
and the complements combining with a verb, but the
lack of constraints on null constituents and optional
adjuncts still means that the grammar is very loose.
The grammar specialization mechanism flattens the
grammar into a set of much simpler structures, elim-
inating the VP level and only permitting specific pat-
terns of null constituents and adjuncts licenced by
the training corpus.
We will demo several different versions of the
Japanese-input medical speech translation system,
differing with respect to the target language and
the recognition architecture used. In particular, we
will show a) that versions based on the specialized
Japanese grammar offer fast and accurate recogni-
tion on utterances within the intended coverage of
the system (Word Error Rate around 5%, speed un-
der 0.1?RT), b) that versions based on the original
general Japanese grammar are much less accurate
and more than an order of magnitude slower.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari,
P. Coletti, L. Taddei, and F. Balducci. 2001. Ar-
chitecture and design considerations in NESPOLE!:
a speech translation system for e-commerce applica-
tions. In Proceedings of HLT: Human Language Tech-
nology Conference, San Diego, California.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 9 June 2005.
R. Moore. 1998. Using natural language knowledge
sources in speech recognition. In Proceedings of the
NATO Advanced Studies Institute.
Phraselator, 2005. http://www.phraselator.com/. As of 9
June 2005.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 9 June 2005.
Sehda, 2005. http://www.sehda.com/. As of 9 June 2005.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
27
A Limited-Domain English to Japanese Medical Speech Translator
Built Using REGULUS 2
Manny Rayner
Research Institute for Advanced
Computer Science (RIACS),
NASA Ames Research Center,
Moffet Field, CA 94035
mrayner@riacs.edu
Pierrette Bouillon
University of Geneva
TIM/ISSCO,
40, bvd du Pont-d?Arve,
CH-1211 Geneva 4,
Switzerland
pierrette.bouillon@issco.unige.ch
Vol Van Dalsem III
El Camino Hospital
2500 Grant Road
Mountain View, CA 94040
vvandal3@aol.com
Hitoshi Isahara, Kyoko Kanzaki
Communications Research Laboratory
3-5 Hikaridai
Seika-cho, Soraku-gun
Kyoto, Japan 619-0289
{isahara,kanzaki}@crl.go.jp
Beth Ann Hockey
Research Institute for Advanced
Computer Science (RIACS),
NASA Ames Research Center,
Moffet Field, CA 94035
bahockey@riacs.edu
Abstract
We argue that verbal patient diagnosis is a
promising application for limited-domain
speech translation, and describe an ar-
chitecture designed for this type of task
which represents a compromise between
principled linguistics-based processing on
the one hand and efficient phrasal transla-
tion on the other. We propose to demon-
strate a prototype system instantiating this
architecture, which has been built on top
of the Open Source REGULUS 2 platform.
The prototype translates spoken yes-no
questions about headache symptoms from
English to Japanese, using a vocabulary of
about 200 words.
1 Introduction and motivation
Language is crucial to medical diagnosis. Dur-
ing the initial evaluation of a patient in an emer-
gency department, obtaining an accurate history of
the chief complaint is of equal importance to the
physical examination. In many parts of the world
there are large recent immigrant populations that re-
quire medical care but are unable to communicate
fluently in the local language. In the US these im-
migrants are especially likely to use emergency fa-
cilities because of insurance issues. In an emer-
gency setting there is acute need for quick accurate
physician-patient communication but this communi-
cation is made substantially more difficult in cases
where there is a language barrier. Our system is
designed to address this problem using spoken ma-
chine translation.
Designing a spoken translation system to obtain
a detailed medical history would be difficult if not
impossible using the current state of the art. The
reason that the use of spoken translation technol-
ogy is feasible is because what is actually needed in
the emergency setting is more limited. Since medi-
cal histories traditionally are obtained through two-
way physician-patient conversations that are mostly
physician initiative, there is a preestablished limiting
structure that we can follow in designing the trans-
lation system. This structure allows a physician to
sucessfully use one way translation to elicit and re-
strict the range of patient responses while still ob-
taining the necessary information.
Another helpful constraint on the conversational
requirements is that the majority of medical condi-
tions can be initiatlly characterized by a relatively
small number of key questions about quality, quan-
tity and duration of symptoms. For example, key
questions about chest pain include intensity, loca-
tion, duration, quality of pain, and factors that in-
crease or decrease the pain. These answers to these
questions can be sucessfully communicated by a
limited number of one or two word responses (e.g.
yes/no, left/right, numbers) or even gestures (e.g.
pointing to an area of the body). This is clearly a
domain in which the constraints of the task are suf-
ficient for a limited domain, one way spoken trans-
lation system to be a useful tool.
2 An architecture for limited-domain
speech translation
The basic philosophy behind the architecture of the
system is to attempt an intelligent compromise be-
tween fixed-phrase translation on one hand (e.g.
(IntegratedWaveTechnologies, 2002)) and linguisti-
cally motivated grammar-based processing on the
other (e.g. VERBMOBIL (Wahlster, 2000) and Spo-
ken Language Translator (Rayner et al, 2000a)).
At run-time, the system behaves essentially like a
phrasal translator which allows some variation in the
input language. This is close in spirit to the approach
used in most normal phrase-books, which typically
allow ?slots? in at least some phrases (?How much
does ? cost??; ?How do I get to ? ??). However,
in order to minimize the overhead associated with
defining and maintaining large sets of phrasal pat-
terns, these patterns are derived from a single large
linguistically motivated unification grammar; thus
the compile-time architecture is that of a linguisti-
cally motivated system. Phrasal translation at run-
time gives us speed and reliability; the linguistically
motivated compile-time architecture makes the sys-
tem easy to extend and modify.
The runtime system comprises three main mod-
ules. These are respectively responsible for source
language speech recognition, including parsing and
production of semantic representation; transfer and
generation; and synthesis of target language speech.
The speech processing modules (recognition and
synthesis) are implemented on top of the standard
Nuance Toolkit platform (Nuance, 2003). Recogni-
tion is constrained by a CFG language model written
in Nuance Grammar Specification Language (GSL),
which also specifies the semantic representations
produced. This language model is compiled from
a linguistically motivated unification grammar us-
ing the Open Source REGULUS 2 platform (Rayner
et al, 2003; Regulus, 2003); the compilation pro-
cess is driven by a small corpus of examples. The
language processing modules (transfer and genera-
tion) are a suite of simple routines written in SICStus
Prolog. The speech and language processing mod-
ules communicate with each other through a mini-
mal file-based protocol.
The semantic representations on both the source
and target sides are expressed as attribute-value
structures. In accordance with the generally mini-
malistic design philosophy of the project, semantic
representations have been kept as simple as possi-
ble. The basic principle is that the representation of
a clause is a flat list of attribute-value pairs: thus for
example the representation of ?Did your headache
start suddenly?? is the attribute-value list
[[utterance_type,ynq],[tense,past],
[symptom,headache],[state,start],
[manner,suddenly]]
In a broad domain, it is of course trivial to con-
struct examples where this kind of representation
runs into serious problems. In the very narrow do-
main of a phrasebook translator, it has many desir-
able properties. In particular, operations on semantic
representations typically manipulate lists rather than
trees. In a broad domain, we would pay a heavy
price: the lack of structure in the semantic represen-
tations would often make them ambiguous. The very
simple ontology of the phrasebook domain however
means that ambiguity is not a problem; the compo-
nents of a flat list representation can never be de-
rived from more than one functional structure, so
this structure does not need to be explicitly present.
Transfer rules define mappings of sets of attribute-
value pairs to sets of attribute-value pairs; the ma-
jority of the rules map single attribute-value pairs
to single attribute-value pairs. Generation is han-
dled by a small Definite Clause Grammar (DCG),
which converts attribute-value structures into sur-
face strings; its output is passed through a minimal
post-transfer component, which applies a set of rules
which map fixed strings to fixed strings. Speech syn-
thesis is performed either by the Nuance Vocalizer
TTS engine or by concatenation of recorded wave-
files, depending on the output language.
One of the most important questions for a med-
ical translation system is that of reliability; we ad-
dress this issue using the methods of (Rayner and
Bouillon, 2002). The GSL form of the recognition
grammar is run in generation mode using the Nu-
ance generate utility to generate large numbers
of random utterances, all of which are by construc-
tion within system coverage. These utterances are
then processed through the system in batch mode us-
ing all-solutions versions of the relevant processing
algorithms. The results are checked automatically
to find examples where rules are either deficient or
ambiguous. With domains of the complexity under
consideration here, we have found that it is feasible
to refine the rule-sets in this way so that holes and
ambiguities are effectively eliminated.
3 A medical speech translation system
We have built a prototype medical speech transla-
tion system instantiating the functionality outlined
in Section 1 and the architecture of Section 2. The
system permits spoken English input of constrained
yes/no questions about the symptoms of headaches,
using a vocabulary of about 200 words. This is
enough to support most of the standard examina-
tion questions for this subdomain. There are two
versions of the system, producing spoken output in
French and Japanese respectively. Since English ?
Japanese is distinctly the more interesting and chal-
lenging language pair, we will focus on this version.
Speech recognition and source language analy-
sis are performed using REGULUS 2. The grammar
is specialised from the large domain-independent
grammar using the methods sketched in Section 2.
The training corpus has been constructed by hand
from an initial corpus supplied by a medical pro-
fessional; the content of the questions was kept un-
changed, but where necessary the form was revised
to make it more appropriate to a spoken dialogue.
When we felt that it would be difficult to remem-
ber what the canonical form of a question would
be, we added two or three variant forms. For exam-
ple, we permit ?Does bright light make the headache
worse?? as a variant for ?Is the headache aggra-
vated by bright light??, and ?Do you usually have
headaches in the morning?? as a variant for ?Does
the headache usually occur in the morning??. The
current training corpus contains about 200 exam-
ples.
The granularity of the phrasal rules learned by
grammar specialisation has been set so that the con-
stituents in the acquired rules are VBARs, post-
modifier groups, NPs and lexical items. VBARs
may include both inverted subject NPs and adverbs1.
Thus for example the training example ?Are the
headaches usually caused by emotional upset?? in-
duces a top-level rule whose context-free skeleton is
UTT --> VBAR, VBAR, POSTMODS
For the training example, the first VBAR in the in-
duced rule spans the phrase ?are the headaches usu-
ally?, the second VBAR spans the phrase ?caused?,
and the POSTMODS span the phrase ?by emotional
upset?. The same rule could potentially be used to
cover utterances like ?Is the pain sometimes pre-
ceded by nausea?? and ?Is your headache ever as-
sociated with blurred vision??. The same training
example will also induce several lower-level rules,
the least trivial of which are rules for VBAR and
POSTMODS with context-free skeletons
VBAR --> are, NP, ADV
POSTMODS --> P, NP
The grammar specialisation method is described in
full detail in (Rayner et al, 2000b).
With regard to the transfer component, we have
had two main problems to solve. Firstly, it is well-
known that translation from English to Japanese re-
quires major reorganisation of the syntactic form.
Word-order is nearly always completely different,
and category mismatches are very common. It is
mainly for this reason that we chose to use a flat
semantic representation. As long as the domain is
simple enough that the flat representations are un-
ambiguous, transfer can be carried out by mapping
lists of elements into lists of elements. For example,
we translate ?are your headaches caused by fatigue?
as ?tsukare de zutsu ga okorimasu ka? (lit. ?fatigue-
CAUSAL headache-SUBJ occur-PRESENT QUES-
TION?). Here, the source-language representation is
[[utterance_type,ynq],
[tense,present],
[symptom,headache],
[event,cause],
[cause,fatigue]]
and the target-language one is
[[utterance_type,sentence],
[tense,present],
[symptom,zutsu],
1This non-standard definition of VBAR has technical advan-
tages discussed in (Rayner et al, 2000c)
do your headaches often appear at night ?
yoku yoru ni zutsu ga arimasu ka
(often night-AT headache-SUBJ is-PRES-Q)
is the pain in the front of the head ?
itami wa atama no mae no hou desu ka
(pain-TOPIC head-OF front side is-PRES-Q)
did your headache start suddenly ?
zutsu wa totsuzen hajimari mashita ka
(headache-TOPIC sudden start-PRES-Q)
have you had headaches for weeks ?
sushukan zutsu ga tsuzuite imasu ka
(weeks headache-SUBJ have-CONT-PRES-Q)
is the pain usually superficial ?
itsumo itami wa hyomenteki desu ka
(usually pain-SUBJ superficial is-PRES-Q)
is the severity of the headaches increasing ?
zutsu wa hidoku natte imasu ka
(headache-TOPIC severe becoming is-PRES-Q)
Table 1: Examples of utterances covered by the pro-
totype
[event,okoru],[postpos,causal],
[cause,tsukare]]
Each line in the source representation maps into the
corresponding one in the target in the obvious way.
The target-language grammar is constrained enough
that there is only one Japanese sentence which can
be generated from the given representation.
The second major problem for transfer relates to
elliptical utterances. These are very important due
to the one-way character of the interaction: instead
of being able to ask a WH-question (?What does
the pain feel like??), the doctor needs to ask a se-
ries of Y-N questions (?Is the pain dull??, ?Is the
pain burning??, ?Is the pain aching??, etc). We
rapidly found that it was much more natural for
questions after the first one to be phrased ellipti-
cally (?Is the pain dull??, ?Burning??, ?Aching??).
English and Japanese have however different con-
ventions as to what types of phrase can be used
elliptically. Here, for example, it is only pos-
sible to allow some types of Japanese adjectives
to stand alone. Thus we can grammatically and
semantically say ?hageshii desu ka? (lit. ?burn-
ing is-QUESTION?) but not ?*uzukuyona desu
ka? (lit. ?*aching is-QUESTION?). The prob-
lem is that adjectives like ?uzukuyona? must com-
bine adnominally with a noun in this context:
thus we in fact have to generate ?uzukuyona itami
desu ka? (?aching-ADNOMINAL-USAGE pain is-
QUESTION?). Once again, however, the very lim-
ited domain makes it practical to solve the problem
robustly. There are only a handful of transforma-
tions to be implemented, and the extra information
that needs to be added is always clear from the sortal
types of the semantic elements in the target represen-
tation.
Table 1 gives examples of utterances covered by
the system, and the translations produced.
References
IntegratedWaveTechnologies, 2002. http://www.i-w-
t.com/investor.html. As of 15 Mar 2002.
Nuance, 2003. http://www.nuance.com. As of 25 Febru-
ary 2003.
M. Rayner and P. Bouillon. 2002. A phrasebook style
medical speech translator. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (demo track), Philadelphia, PA.
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and
M. Wire?n, editors. 2000a. The Spoken Language
Translator. Cambridge University Press.
M. Rayner, D. Carter, and C. Samuelsson. 2000b. Gram-
mar specialisation. In Rayner et al (Rayner et al,
2000a).
M. Rayner, B.A. Hockey, and F. James. 2000c. Compil-
ing language models from a linguistically motivated
unification grammar. In Proceedings of the Eighteenth
International Conference on Computational Linguis-
tics, Saarbrucken, Germany.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2003. http://sourceforge.net/projects/regulus/.
As of 24 April 2003.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
In: Proceedings o/CoNLL-2000 and LLL-2000, pages 111-114, Lisbon, Portugal, 2000. 
Minimal Commitment  and Full Lexical Disambiguation: 
Balancing Rules and Hidden Markov Models 
Pat r i ck  Ruch  and Rober t  Baud and P ie r re t te  Bou i l lon  and G i lber t  Rober t*  
Medical Informatics Division, University Hospital of Geneva 
ISSCO, University of Geneva 
{ruch, baud}@dim.hcuge.ch, {bouillon, robert}@issco.unige.ca 
Abst ract  
In this paper we describe the construction of 
a part-of-speech tagger both for medical doc- 
ument retrieval purposes and XP extraction. 
Therefore we have designed a double system: for 
retrieval purposes, we rely on a rule-based ar- 
chitecture, called minimal commitment, which 
is likely to be completed by a data-driven tool 
(HMM) when full disambiguation is necessary. 
1 I n t roduct ion  
Nowadays, most medical information is stored 
in textual documents 1, but such large amount 
of data may remain useless if retrieving the rel- 
evant information in a reasonable time becomes 
impossible. Although some large-scale informa- 
tion retrieval (IR) evaluations, made on unre- 
stricted corpora (Hersh and al., 1998), and on 
medical texts (Hersh, 1998), are quite critical 
towards linguistic engineering, we believe that 
natural anguage processing is the best solution 
to face two major problems of text retrieval en- 
gines: expansion of the query and lexical dis- 
ambiguation. Disambiguation can be separated 
between MS (morpho-syntactic, i.e. the part-of- 
speech (POS) and some other features) and WS 
(word-sense) disambiguation. Although we aim 
at developing a common architecture for pro- 
cessing both the MS and the WS disambigua- 
tion (Ruch and al., 1999), this paper focuses on 
the MS tagging. 
? We would like to thank Thierry Etchegoyhen and Erik 
Tjong Kim Sang for their helpful assistance while writing 
this paper. The Swiss National Fundation supported the 
present study. 
1While our studies were made on French corpora, the 
examples are provided in English -when possible- for the 
sake of clarity. 
2 Background 
Before starting to develop our own MS tagger, 
some preliminary studies on general available 
systems were conducted; if these studies go far 
beyong the scope of this paper, we would like 
to report on the main conclusions. Both statis- 
tical taggers (HMM) and constraint-based sys- 
tems were assessed. Two guidelines were fram- 
ing the study: performances and minimal com- 
mitment. We call minimal commitment 2 the 
property of a system, which does not attempt 
to solve ambiguities when it is not likely to solve 
it well! Such property seems important for IR 
purposes, where we might prefer noise rather 
than silence in the recall process. However, it 
must remain optional, as some other tasks (such 
as the NP extraction, or the phrase chunking 
(Abney, 1991)) may need a full disambiguation. 
2.1 Data -dr iven  too ls  
We adapted the output of our morphological 
analyser for tagging purposes (Bouillon et al, 
1999). We trained and wrote manual biases 
for an HMM tagger, but results were never far 
above 97% (i.e. about 3% of error); with an av- 
erage ambiguity level of around 16%, it means 
that almost 20% of the ambiguities were at- 
tributed a wrong tag! We attempted to set 
a confidence threshold, so that for similarly 
weighted transitions, the system would keep the 
ambiguity, as in (Weischedel and al., 1993), but 
results were not satisfying. 
2.2 Const ra in t -based  sys tems 
We also looked at more powerful principle- 
based parsers, and tests were conducted on 
2The first one using this expression was maybe M. 
Marcus, lately we can find a quite similar idea in Sil- 
berztein (1997). 
111 
Token Lemma Lexical tag(s) 
fast a fast 
section section nc\[s\] 
face/toface 
of of sp 
the the dad 
internal internal a 
faces ~n 
Token Lexical tags Disambiguated tag 
fast a a 
section nc\[s\] nc\[s\] 
of sp sp 
the dad dad 
internal a a 
faces nc\[p\]/v\[s03\] nc\[p\] 
Table 1: Tag-like representation f MS lexical 
features 
FIPSTAG 3 (a Government and Binding chart- 
parser (Wehrli, 1992)). Although this system 
performed well on general texts, with about 
0.7% of errors, its results on medical texts 
were about the same as stochastic taggers. As 
we could not adapt our medical morphological 
analyser on this very integrated system, it had 
to cope with several unknown words. 
3 Methods  
In order to assess the system, we selected a 
corpus (40000 tokens) based equally on three 
types of documents: reports of surgery, dis- 
charge summaries and follow-up notes. This ad 
hoc corpus is split into 5 equivalent sets. The 
first one (set A, 8520 words) will serve to write 
the basic rules of the tagger, while the other sets 
(set B, 8480 tokens, C, 7447 tokens, D, 7311 
tokens, and E, 8242 tokens), will be used for 
assessment purposes and incremental improve- 
ments of the system. 
3.1 Lexicon, morphological  analysis 
and guesser 
The lexicon, with around 20000 entries, covers 
exhaustively the whole ICD-10. The morpho- 
logical analyser ismorpheme-based (Baud et al, 
1998), it maps each inflected surface form of a 
word to its canonical lexical form, followed by 
the relevant morphological features. Words ab- 
sent from the lexicon follow a two-step guess- 
ing process. First, the unknown token is anal- 
ysed regarding its respective morphemes, if this 
first stage fails then a last attempt is made to 
guess the hypothetical MS tags of the token. 
The first stage is based on the assumption that 
aFor a MULTEXT-like description of the FIP- 
STAG tagset see Ruch P, 1997: Table de cot- 
respondance GRACE/FIPSTAG, available at 
http://latl.unige.ch/doc/etiquettes.ps 
Table 2: Example of tagging 
unknown words in medical documents axe very 
likely to belong to the medical jargon, the sec- 
ond one supposed that neologisms follow regular 
inflectional patterns. If regarding the morpho- 
syntax, both stages are functionally equivalent, 
as each one provides a set of morpho-syntactic 
information, they radically behave differently 
regarding the WS information. For guessing 
WS categories only the  first stage guesser is 
relevant, as inflectional patterns are not suffi- 
cient for guessing the semantic of a given token. 
Thus, the ending able characterises very proba- 
bly an adjective, but does not provide any se- 
mantic information 4 on it. 
Let us consider two examples of words absent 
from the lexicon. First, allomorph: the prefix 
part allo, and the suffix part, morph, are listed 
in the lexicon, with all the MS and the WS fea- 
tures, therefore it is recognized by the first-stage 
guesser. Second, allocution, it can not be split 
into any affix, as cution is not a morpheme, but 
the ending tion refers to some features (noun, 
singular) in the second-stage guesser. As the 
underlying objective of the project is to retrieve 
documents, the main and most complete infor- 
mation is provided by the first-stage guesser, 
and the second-stage is only interesting for MS 
tagging, as in (Chanod and Tapanainen, 1995). 
Finally (tab. 1), some of the morpho-syntactic 
features provided by the lemmatizer are ex- 
pressed into the MS tagset 5, to be processed 
by the tagger (tab. 2). 
4A minimal set of lexical semantic types, based on 
the UMLS, has been defined in (Ruch and al., 1999). 
5The MS tagset ends to follow the MULTEXT lexi- 
cal description for French, modified within the GRACE 
action (http://www.limsi.fr/TLP/grace/doc/GTR-3- 
2.1.tex). However, it is not always possible, as this 
description does not allow any morpheme annotation. 
112 
Evaluation 1-Set B 2-Set C 3-Set D 4-Set E 
Tokens with lexical ambiguities 
Tokens correctly tagged 
1178 (13.9) 
8243 (97.2) 
1273 (17.1) 
7177 (96.4) 
1132 (15.5) 
7137 (97.6) 
1221 (14.8) 
8082 (98.1) 
Tokens still ambiguous, with GC 161 (1.9) 183 (2.5) 136 (1.9) 101 (1.2) 
Tokens ambiguous, without GC 9 (0.1) 2 (0) i 9 (0.1) 
Tokens incorrectly tagged 76 (0.9) i 78 (1.0) 36 (0.5) i 51 (0.6) 
Table 3: Results for each evaluation (GC stands for good candidates) 
Statistical evaluation on the residual ambiguity MFT HMM 
Tokens correctly tagged 8136 (98.7) 8165 (99.1) 
Tokens incorrectly tagged 107 (1.3) 78 (0.9) 
Table 4: Processing the residual ambiguity 
3.2 Studying the ambiguities 
Our first investigations aimed at assessing the 
overall ambiguity of medical texts. We found 
that 1227 tokens (14.4% of the whole sample 6) 
were ambiguous in set A, and 511 tokens (6.0%) 
were unknown. We first decided not to care 
about unknown words, therefore they were not 
taking into account in the first assessment (cf. 
Performances). However, some frequent words 
were missing, so that together with the MS 
guesser, we would improve the guessing score by 
adding some lexemes. Thus, adding 232 entries 
in the lexicon and linking it with the Swiss com- 
pendium (for drugs and chemicals) provides an 
unknown word rate of less than 3%. This result 
includes also the pre-processing of patients and 
physicians names (Ruch and al., 2000). Con- 
cerning the ambiguities, we found that 5 to- 
kens were responsible for half of the ambiguities, 
while in unrestricted corpora this number seems 
around 16 (Chanod and Tapanainen, 1995). 
3.2.1 Local  rules 
We separated the set A in 8 subsets of about 
1000 tokens, in order to write the rules. We 
wrote around 50 rules (which generated more 
than 150 operative rules) for the first subset, 
while for the 8th, only 12 rules were necessary 
to reach a score close to 100% on set A. These 
rules are using intermediate symbols (such as 
the Kleene star) in order to ease and improve 
the rule-writing process, these symbols are re- 
placed when the operative rules are generated. 
6For comparison, the average ambiguity rate is about 
25-30% in unrestricted corpora. 
Here is an example of a rule: 
prop\[**\];v\[**\]/nc\[**\] ---+ prop\[**\];v\[**\] 
This rule says 'if a token is ambiguous be- 
tween (/) a verb (v), whatever (**) features it 
has (3rd or lst /2nd person, singular or plural), 
and a common noun, whatever (**) features it 
has, and such token is preceded by a personal 
pronoun (prop), whatever (**) features this pro- 
noun has (3rd or lst /2nd person), then the am- 
biguous token can be rewritten as a verb, keep- 
ing its original features (**)'. 
4 Per fo rmances  
4.1 Maximizing the minimal 
commitment  
Four successive evaluations were conducted 
(tab. 3); after each session, the necessary rules 
were added in order to get a tagging score close 
to 100%. In parallel, words were entered into 
the lexicon, and productive ndings were added 
into the MS guesser. The second, third, and 
fourth evaluations were performed with activat- 
ing the MS guesser. Let us note that translation 
phenomena (Paroubek and al., 1998), which 
turn the lexical category of a word into another 
one, seem rare in medical texts (only 3 cases 
were not foreseen in the lexicon). 
A success rate of 98% (tab. 3, evaluation 
4) is not a bad result for a tagger, but the 
main result concerns the error rate, with less 
than 1% of error, the system seems particularly 
minimally committed 7. Another interesting re- 
sult concerns the residual ambiguity (tokens till 
rLet us note that in the assessment 1, he system had 
113 
ambiguous, with GC): in the set E, at least half 
of these ambiguities could be handled by writ- 
ing more rules. However some of these ambigui- 
ties are clearly untractable with such contextual 
rules, and would demand more lexical informa- 
tion, as in le patient prdsente une douleur ab- 
dominale brutale et diffuse (the patient shows 
an acute and diffuse abdominal pain/the pa- 
tient shows an acute abdominal pain and dis- 
tributes*S), where diffuse could be adjective or 
verb. 
4.2 Max imiz ing  the  success  ra te  
A last experiment is made: on the set E, which 
has been disambiguated by the rule-based tag- 
ger, we decided to apply two more disambigua- 
tions, in order to handle the residual ambi- 
guity. First, we apply the most frequent tag 
(MFT) model, as baseline, then the HMM. Both 
the MFT and the HMM transitions are calcu- 
lated on the set B-t-C?D, tagged manually, but 
without any manual improvement (bias) of the 
model. 
Table 4 shows that for the residual ambiguity, 
i.e. the ambiguity, which remained untractable 
by the rule-based tagger, the HMM provides an 
interesting disambiguation accuracy 9.
5 Conc lus ion  
We have presented a rule-based tagger for elec- 
tronic medical records. The first target of 
this tool is the disambiguation for IR purposes, 
therefore we decided to design a system with- 
out any heuristics. As second target, the system 
will be used for conducting NP extraction tasks 
and shallow parsing: the system must be able 
to provide a fully disambiguated output; there- 
fore we used the HMM tool for completing the 
disambiguation task. 
Re ferences  
Abney, Steven. 1991. Parsing by chunks. In R. 
Berwick and S. Abney and C. Tenny, editors, 
Principle-based parsing, pages 257-278. Kluwer. 
Robert Baud, C. Lovis, and AM. Rassinoux. 1998. 
Morpho-semantic parsing of medical expressions. 
about 1000 operative rules, while the assessment 4 was 
conducted with more than 2000 rules. 
SThe lexical information on the valence + OBJECT 
is necessary for disambiguating the verb form of diffuse. 
9The accuracy of the HMM tagger, on the fully am- 
biguous version of set E, was 96.3%, while the MFT per- 
formed about 93.5%. 
In Proceedings ofAMIA '1998, pages 760-764, Or- 
lando. 
Pierrette Bouillon, R Baud, G Robert, and P Ruch. 
1999. Indexing by statistical tagging. In Proceed- 
ings of the JADT'2000, pages 35-42, Lausanne. 
Jean-Pierre Chanod and Pasi Tapanainen. 1995. 
Tagging french: comparing a statistical and 
a constraint-based method. In Proceedings of 
EACL'95, pages 149-156, Dublin. 
William, Hersh and S. Price and D. Kraemer and 
B. Chan and L. Sacherek and D. Olson. 1998. 
A large-scale comparison of boolean vs. natural 
language searching for the trec-7 interactive track. 
In TREC 1998, pages 429-438. 
William Hersh. 1998. Information retrieval at the 
millenium. In Proceedings of AMIA'1998, pages 
38-45, Lake Buena Vista, FL. 
Paroubek, Patrick and G. Adda and J. Mariani and 
J. Lecomte and M. Rajman. 1998. The GRACE 
french part-of-speech tagging evaluation task. In 
Proceedings of LREC'1998, Granada. 
Ruch, Patrick and J. Wagner and P. Bouillon and R. 
Baud and A.-M. Rassinoux and G. Robert. 1999. 
Medtag: Tag-like semantics for medical document 
indexing. In Proceedings of AMIA '99, pages 35- 
42, Washington. 
Ruch, Patrick and R. Baud and A.-M. Rassinoux 
and P. Bouillon and G. Robert 2000. Medical 
document anonymization with a semantic lexicon. 
In Proceedings of AMIA '2000, Los Angeles. 
Max Silberztein. 1997. The lexical analysis of nat- 
ural languages. In Emmanuel Roche and Yves 
Shabes, editors, Finite-State Language Process- 
ing, pages 176-205. MIT Press. 
Eric Wehrli. 1992. The IPS system. In Proceedings 
COLING-92, pages 870-874. 
Weischedel, Ralph and M. Meeler and R. Shwartz 
and L. Ramshaw and J. Palmucci, 1993. Cop- 
ing with ambiguity and unknown words through 
probabilistic models. Computational Linguistics, 
19(2):359-382. 
114 
J 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 199-208, Lisbon, Portugal, 2000. 
Inductive Logic Programming for 
Corpus-Based Acquisition of Semantic Lexicons 
Pasca le  S4bi l lot  
IRISA - Campus de Beaulieu - 35042 Rennes cedex - France 
sebillot@irisa, fr 
P ier re t te  Bou i l lon  
TIM/ ISSCO - ETI - Universit4 de Gen~ve - 40 Bvd du Pont-d'Arve - 
CH-1205 Geneva-  Switzerland 
Pierrette. Bouillon@issco. unige, ch 
C4ci le Fabre  
ERSS - Universit@ de Toulouse II - 5 all@es A. Machado - 31058 Toulouse cedex - France 
cfabre@univ-tlse2, fr 
Abst ract  
In this paper, we propose an Inductive Logic 
Programming learning method which aims at 
automatically extracting special Noun-Verb (N- 
V) pairs from a corpus in order to build up 
semantic lexicons based on Pustejovsky's Gen- 
erative Lexicon (GL) principles (Pustejovsky, 
1995). In one of the components of this lex- 
ical model, called the qualia structure, words 
are described in terms of semantic roles. For 
example, the relic role indicates the purpose or 
function of an item (cut for knife), the agen- 
tive role its creation mode (build for house), 
etc. The qualia structure of a noun is mainly 
made up of verbal associations, encoding rela- 
tional information. The Inductive Logic Pro- 
gramming learning method that we have devel- 
oped enables us to automatically extract from 
a corpus N-V pairs whose elements axe linked 
by one of the semantic relations defined in the 
qualia structure in GL, and to distinguish them, 
in terms of surrounding categorial context from 
N-V pairs also present in sentences ofthe corpus 
but not relevant. This method has been theoret- 
ically and empirically validated, on a technical 
corpus. The N-V pairs that have been extracted 
will further be used in information retrieval ap- 
plications for index expansion 1. 
1This works is funded by the Agence universi- 
taire de la Francophonie (AUF) (Action de recherche 
partag4e "Acquisition automatique d'dldments du Lex- 
Keywords:  Lexicon learning, Generative 
Lexicon, Inductive Logic Programming, Infor- 
mation indexing. 
1 In t roduct ion  
Information retrieval (IR) systems aim at pro- 
viding a user who asks a query to a database of 
documents with the most relevant exts. The 
quality of these systems is usually measured 
with the help of two criteria: the recall rate, 
which corresponds to the proportion of relevant 
answers that have been given by the system 
compared to the total number of relevant an- 
swers in the database, and the precision rate, 
which denotes the proportion of relevant an- 
swers that are present among the given answers. 
In these IR systems, texts and queries are 
usually represented by indexes, that is, a col- 
lection of some of the words that they contain. 
The quality of the systems therefore highly de- 
pends on the type of indexing language that has 
been chosen. Two kinds of indexes exist: sim- 
ple indexes, which correspond to simple nouns 
(N), verbs (V) and/or adjectives (A) that oc- 
cur in a text or a query 2, and complex indexes, 
which correspond to the compounds (for exam- 
ple, NN compounds) present in the document or 
ique Gdndratif pour amdliorer les performances de 
syst~mes de recherche d'information", r@seau FRAN-  
CIL). 
2All the simple N, V and/or A can be kept as indexes, 
or the most frequent ones for a given text, or those whose 
frequencies in this text are especially high compared to 
their frequencies in the database, etc. 
199 
the question. The solutions that are given for 
a user query are the texts whose indexes better 
match the query index. 
In order to obtain the hightest performances, 
IR systems usually offer some possibilities to 
expand both query and text indexes. Tra- 
ditional index expansion concerns morpho- 
syntactic similarities; for example, the same in- 
dex words in plural and singular forms can be 
matched. Some other systems deal with a kind 
of semantic similarities: if they possess a lin- 
guistic knowledge database, they can, for ex- 
ample, expand a nominal index by following 
synonymy or hyperonymy links. These systems 
are however usually limited to intra-categorial 
expansion, especially N-to-N one. Here we 
deal with a new kind of expansion that has 
been proven particularly useful (Grefenstette, 
1997; Fabre and S~billot, 1999) for document 
database questioning. It concerns N-V links 
and aims at allowing matching between ominal 
and verbal formulations that are semantically 
close. For example, our objective is to permit a 
matching between a query index disk store and 
the text formulation to sell disks, related by the 
typical function of a store. 
N-V index expansion however has to be con- 
trolled in order to ensure that the same con- 
cept is involved in the two formulations. We 
have chosen Pustejovsky's Generative Lexicon 
(GL) framework (Pustejovsky, 1995; Bouillon 
and Busa, 2000) to define what a relevant N- 
V link is, that is, what is a N-V pair in which 
the N and the V are related by a semantic link 
which is close, and which can therefore be used 
to expand indexes. 
In GL formalism, lexical entries consist in 
structured sets of predicates that define a word. 
In one of the components of this lexical model, 
called the qualia structure, words are described 
in terms of semantic roles. The telic role in- 
dicates the purpose or function of an item (for 
example, cut for knife), the agentive role its cre- 
ation mode (build for house), the constitutive 
role its constitutive parts (handle for handcup) 
and the formal role its semantic ategory (con- 
tain (information) for book). The qualia struc- 
ture of a noun is mainly made up of verbal as- 
sociations, encoding relational information. We 
assert hat these N-V links are especially rele- 
vant for index expansion in IR systems (Fabre 
and S~billot, 1999), and what we call a relevant 
N-V pair afterwards in the paper is a pair com- 
posed of a N and a V which are related by one of 
the four semantic relations defined in the qualia 
structure in GL. 
GL is however currently just a formalism; no 
generative l xicons exist that are precise nough 
for every domain and every application (for eg. 
IR), and the cost of a manual construction of 
a lexicon based on GL principles is prohibitive. 
Moreover the real N-V links that are the key- 
point of this formalism cannot be defined a pri- 
ori and have to be acquired from corpora of 
the studied domain. The aim of this paper is 
therefore to present a machine learning method, 
developed in the Inductive Logic Programming 
framework, that enables us to automatically ex- 
tract from a corpus N-V pairs whose elements 
are linked by one of the semantic relations de- 
fined in the qualia structure in GL, and to dis- 
tinguish them, in terms of surrounding cate- 
gorial (Part-of-Speech, POS) context from N- 
V pairs also present in sentences of the corpus 
but not relevant. It will be divided in three 
parts. Section 2 focusses on the motivation of 
this project regarding the use of GL. Section 3 
explains the machine learning method that we 
have developed. Section 4 is dedicated to its 
theoretical nd empirical validations, and to the 
results of its application to a technical corpus. 
2 Mot ivat ion  
As stated in the introduction, our work makes 
two strong claims: firstly N-V associations de- 
fined in GL are relevant for IR and secondly 
this information can be acquired from a corpus 
on the basis of surrounding POS context. These 
presuppositions have to be motivated before ex- 
plaining the learning method: 
1. The aim of GL is to define underspec- 
ified lexical representations that will acquire 
their specifications in context. For example, the 
qualia structure of book indicates that its de- 
fault function is read and that it is created by 
the act of writing. But this information has to 
be enriched in context in order to characterize 
how words are used in specific domains. For 
example, the qualia structure of book will also 
have to indicate that the book can be shelved or 
indexed if this information is necessary to inter- 
pret texts from information science domain. GL 
200 
is therefore a theory of words in context. It can 
also be seen as a way to structure information 
in corpora and, in that sense, the relations it 
defines are therefore privileged information for 
IR. In this perspective, GL has been preferred 
to existing lexical resources uch as WordNet 
(Fellbaum, 1998) for two main reasons: lexical 
relations that we want to exhibit - namely N-V 
links - are unavailable in WordNet, which fo- 
cuses on paradigmatic lexical relations; Word- 
Net is a domain-independent, static resource, 
which can not be used as such to describe lexi- 
cal associations in specific texts, considering the 
great variability of semantic associations from 
one domain to another. 
2. In GL, the qualia structures are not arbi- 
trary repository of information. They contain 
the information ecessary to explain the syn- 
tactic behaviour of the item. We would there- 
fore expect that there are strong connections 
between some specific syntactic phenomena and 
some specific qualia relations. For example, the 
middle construction seems to be only possible if 
a telic relation holds between the N and V (Bas- 
sac and Bouillon, 2000) (for example: ??this 
book writes well vs this book reads well). Sim- 
ilarly, imperative constructions (e.g. open the 
door, follow the links) or adjectival sentences (a 
book difficult to write/read) may also indicate 
a qualia relation. These are some of the con- 
structions that we want to identify primilarly 
in corpora by the learning method. 
3 The  mach ine  learn ing  method 
Trying to infer lexical semantic information 
from corpora is not new: lots of works have 
already been conducted on this subject, espe- 
cially in the statistical learning domain (see 
(Grefenstette, 1994b), for e.g., or (Habert et 
al., 1997) and (Pichon and S~billot, 1997) for 
surveys of this field). Following Harris's frame- 
work (Harris et al, 1989), such research tries to 
extract both syntagmatic and paradigmatic n- 
formation, respectively studying the words that 
appear in the same window-based or syntactic 
contexts as a considered lexical unit (first or- 
der word affinities (Grefenstette, 1994a)), or the 
words that generate the same contexts as the 
key word (second order word affinities). For ex- 
ample, (Briscoe and Carroll, 1997) and (Faure 
and N~dellec, 1999) try to automatically learn 
verbal argument structures and selectional re- 
strictions; (Agarwal, 1995) and (Bouaud et al, 
1997) build semantic classes; (Hearst, 1992) 
and (Morin, 1997) focus on particular lexi- 
cal relations, like hyperonymy. Some of these 
works are concerned with automatically ob- 
taining more complete lexical semantic repre- 
sentations ((Grefenstette, 1994b; Pichon and 
S~billot, 1999). Among these studies, (Puste- 
jovsky et al, 1993) presents a research whose 
aim is to acquire GL nominal qualia structures 
from a corpus; this work is however quite dif- 
ferent from ours because it supposes that the 
qualia structure contents are initialized and are 
only refined with the help of the corpus by using 
the type coercion 3 mechanism. 
In order to automatically acquire N-V pairs 
whose elements are linked by one of the seman- 
tic relations defined in the qualia structure in 
GL, we have decided to use a machine learning 
method. This section is devoted to the expla- 
nation of this choice and to the description of 
the method that we have developed. 
Machine learning aims at automatically 
building programs from examples that are 
known to be positive or negative examples of 
their runnings. According to Mitchell (Mitchell, 
1997), "a computer program is said to learn 
from experience E with respect to some class 
of tasks T and performance measure P, if  its 
performance at tasks in T, as measured by P, 
improve with experience E". 
Among different machine learning techniques, 
we have chosen the Inductive Logic Program- 
ming framework (ILP) (Muggleton and De- 
Raedt, 1994) to learn from a textual corpus N-V 
pairs that are related in terms of one of the re- 
lations defined in the qualia structure in GL. 
Programs that are infered from a set of facts 
and a background knowledge are here logic pro- 
grams, that is, sets of Horn clauses. In the ILP 
framework, the main idea is to obtain a set of 
generalized clauses that is sufficiently generic 
to cover the majority of the positive examples 
(E+), and sufficiently specific to rightly corre- 
spond to the concept we want to learn and to 
cover no (or a few - some noise can be allowed) 
negative xample(s) (E - ) .  For our experiment, 
3A semantic operation that converts an argument to 
the type which is expected by a function, where it would 
otherwise result in a type error. 
201 
we furnish a set of N-V pairs related by one of 
the qualia relations within a POS context (E+), 
and a set of N-V pairs that are not semantically 
linked (E-),  and the method infers general rules 
(clauses) that explain these E +. This particular 
explanatory characteristic of ILP has motivated 
our choice: ILP does not just provide a predic- 
tor (this N-V pair is relevant, this one is not) 
but also a data-based theory. Contrary to some 
statistical methods, it does not just give raw 
results but explains the concept hat is learnt 4. 
We use Progol (Muggleton, 19915) for our 
project, Muggleton's ILP implementation that 
has already been proven well suited to deal with 
a big amount of data in multiple domains, and 
to lead to results comparable to other ILP im- 
plementations (Roberts et al, 1998). 
In this section we briefly describe the corpus 
on which our experiment has been conducted. 
We then explain the elaboration of E + and E -  
for Progol. We finally present he generalized 
clauses that we obtain. The validation of the 
method is detailed in section 4. 
3.1 The corpus 
The French corpus used in this project is 
a 700 kBytes handbook of helicopter main- 
tenance, given to us by MATRA CCR 
A@rospatiale, which contains more than 104000 
word occurrences 5. The MATRA CCR corpus 
has some special characteristics that are espe- 
cially well suited for our task: it is coherent; 
it contains lots of concrete terms (screw, door, 
etc.) that are frequently used in sentences to- 
gether with verbs indicating their telic (screws 
must be tightened, etc.) or agentive roles. 
This corpus has been POS-tagged with the 
help of annotation tools developed in the MUL- 
TEXT project (Armstrong, 1996); sentences and 
words are first segmented with MtSeg; words 
are analyzed and lemmatized with Mmorph (Pe- 
titpierre and Russell, 1998; Bouillon et al, 
1998), and finally disambiguated by the Tatoo 
tool, a Hidden Markov Model tagger (Arm- 
strong et al, 1995). Each word therefore only 
receive one POS-tag, with less than 2% of er- 
4Learning with ILP has already been successfully 
used in natural language processing, for example incor- 
pus POS-tagging (Cussens, 1996) or semantic nterpre- 
tation (Mooney, 1999). 
5104212 word occurrences. 
rors. 
3.2 Example  const ruct ion  
The first task consists in building up E + and 
E -  for Progol, in order for it to infer gener- 
alized clauses that explain what, in the POS 
context of N-V pairs, distinguishes the relevant 
pairs from the not relevant ones. Work has to 
be done to determine what is the most appro- 
priate context for this task. We just present 
here the solution we have finally chosen. Sec- 
tion 4 describes methods and measures to eval- 
uate the "quality" of the learning that enable 
us to choose between the different contextual 
possibilities. Here is our methodology for the 
construction of the examples. 
We first consider all the nouns of the MA- 
TRA CCR corpus. More precisely, we only deal 
with a 81314 word occurrence subcorpus of the 
MATRA CCR corpus, which is formed by all 
the sentences that contain at least one N and 
one V. This subcorpus contains 1489 different 
N (29633 noun occurrences) and 567 different 
V (9522 verb occurrences). For each N of this 
subcorpus, the 10 most strongly associated V, in 
terms of Chi-square, are selected. This first step 
both produces pairs that are really bound by 
one qualia relation ((dcrou, serrer)) 6 and pairs 
that are fully irrelevant ((roue, prescrire)) 7.
Each pair is manually annotated as relevant 
or irrelevant according to Pustejovsky's qualia 
structure principles. A Perl program is then 
used to find the occurrences of these N-V pairs 
in the sentences of the corpus. 
For each occurrence of each pair that is sup- 
posed to be used to build one E +, that is for 
each of the previous pairs that has been glob- 
ally annotated as relevant, a manual control has 
to be done to ensure that the N and the V really 
are in the expected relation within the studied 
sentence. After this control, a second Perl pro- 
gram automatically produces the E +. Here is 
the form of the positive examples: 
POSITiVE(category_before_N, category_after.N, 
category_before_V, V_type, distance, position). 
where V_type indicates if the V is an infinitive 
form, etc., distance corresponds to the number 
6(nut, tighten). 
7(wheel, prescribe) 
202 
of verbs between the N and the V, and position 
is POS (for positive) if the V appears before the 
N in the sentence, NEG if the N appears before 
the V. 
For example, 
POSITIVE(VRBINF, P_DE, VID, VRBINF~ 0, 
POS). 
means that a N-V pair, in which the N is 
surrounded with an infinitive verb on its left 
(VRBINF) and a preposition de s (P.DE) on its 
right, in which the V is preceded by nothing 9
(VID) 1? and is an infinitive one (VRBINF), in 
which no verb exists between the N and the V 
(0), and in which the V appears before the N 
in the sentence (POS), is a relevant pair (for ex- 
ample, in ouvrir la porte de ...). 
The E -  are elaborated in the same way than 
the E +, with the same Perl program. E -  and 
E + forms are identical, except he presence of a 
sign :- before the predicate POSITIVE to denote 
aE- :  
:-POSITIVE (category_before.N, 
category_after_N, category_before_V, V_type, 
distance, position). 
These E -  are automatically built from the 
previous highly correlated N-V pairs that have 
been manually annotated as irrelevant. For ex- 
ample, 
:-POSITIVE(VID, P_PAR, NC, VRBPP, 0, NEG). 
means that a N-V pair, in which the N has noth- 
ing on its left (VID) and a preposition par n 
(P_PAR) on its right, in which the V is preceded 
by a noun (NC) and is a past participle (VRBPP), 
in which no verb exists between the N and the 
V (0), and in which the V appears after the N 
in the sentence (NEG), is an irrelevant pair (for 
example, in freinage par goupilles fendues). 
4031 E + and about 7000 E -  are automati- 
cally produced this way from the corpus. 
sOl. 
9Or by one of the three categories that we do not 
consider for example laboration, that is, determiners, 
adverbs and adjectives. 
1?Empty. 
nBy. 
3.3 Learn ing  w i th  the  he lp  of  P rogo l  
These E + and E -  are then furnish to Progol 
in order for it to try to infer generalized clauses 
that explain the concept "qualia pair" versus 
"not qualia pair". We do not discuss here ei- 
ther parameter setting that concerns the choice 
of the example POS context, or evaluation cri- 
teria; this discussion is postponed to next sec- 
tion; we simply present he learning method and 
the type of generalized clauses that we have ob- 
tained. 
Some information have to be given to Progol 
for it to know what are the categories that can 
undergo a generalization. For example, if two 
E + are identical but possess different locative 
prepositions as second arguments (for eg. sur 12 
and sous13), must Progol produce a generaliza- 
tion corresponding to the same clause except 
that the second argument is replaced by the 
general one: locative-preposition, or by a still 
more general one: preposition? 
The background knowledge used by Progol is 
knowledge on the domain. For example here, it 
contains the fact that a verb can be found in 
the corpus in an infinitive or a conjugated form, 
etc. 
verbe( V ) :- infinitif( V ). 
verbe( V ) :- conjugue( V ). 
and that an infinitive form is denoted by the 
tag VERBINF, and a conjugated form by the tags 
VERB-PL and VER.B-SG, etc. 
infinitif( verbinf ). 
conjugue( verb-pl ). 
conjugue( verb-sg ). 
When Progol is provided with all this knowl- 
edge, learning can begun. The output of Progol 
is of two kinds: some clauses that have not at 
all been generalized (that is, some of the E+), 
and some generalized clauses; we call the set of 
these generalized clauses G, and it is this set G 
that interests us here. Here is an example of one 
of the generalized clauses that we have obtained 
in our experiment: 
POSITIVE(A, C, C, D, E, F) :- 
PREPOSITIONLIEU(A), VIDE(C), VERBINF(D), 
PRES(E). (1) 
12On" 
13Under. 
203 
which means that N-V pairs (i) in which the 
category before the N is a locative preposition 
(PREPOSITIONLIEU(A)), (ii) in which there is 
nothing after the N and before the V (VIDE(C) 
for the second and third arguments), (iii) in 
which the V is an infinitive one (VERBINF(D)), 
and (iv) in which there is no verb between the N 
and the V (proximity denoted by P:aEs(E)14), 
are relevant. No constraint is set on N/V order 
in the sentences. 
This generalized clause covers, for example, 
the following E+: 
POSITIVE(P_SUR, VID, VID, VERBINF, 0, POS). 
which corresponds to the relevant pair (prise, 
brancher) 15 that is detected in the corpus in the 
sentence "Brancher les connecteurs sur les prises 
~lectriques.". 
Some of the generalized clauses in G cover 
lots of E +, others far less. We now present a 
method to detect what the "good" c, lauses are, 
that is, the clauses that explain the concept that 
we want to learn, and a measure of the "quality" 
of the learning that has been conducted. 
4 Learn ing  va l idat ion  and  resu l ts  
This section is dedicated to two aspects of 
the validation of our machine learning method. 
First we define the theoretical validation of the 
learning, that is, we focus on the determination 
of a means to detect what are the "good" gen- 
eralized clauses, and of a measure of the quality 
of the concept learning; this parameter setting 
and evaluation criterion phase explains how we 
have chosen the precise POS context for N-V 
pairs in the E + and E -  (as described in subsec- 
tion 3.2): the six contextual elements in exam- 
ples are the combination that leads to the best 
results in terms of the learning quality measure 
that we have chosen. The second step of the 
validation is the empirical one. We have applied 
the generalized clauses that have been selected 
to the Mat ra  CCR corpus and  haw~ evaluated 
the quality of the results in terms of pairs that 
are indicated relevant or not. Here  are these 
two phases. 
14Close(E). 
l~(plug, to plug in). 
4.1 Theoret ical  val idation 
As we have previously noticed, among the gen- 
eralized clauses produced from our E + and E -  
by Progol (set G), some of them cover a lot of 
E +, others only a few of them. What we want 
is to get a way to automatically find what are 
the generalized clauses that have to be kept in 
order to explain the concept we want to learn. 
We have first defined a measure of the theo- 
retical generality of the clauses 16. The theoreti- 
cal generality of a generalized clause is the num- 
ber of not generalized clauses (E +) that this 
clause can cover. For example, both 
POSITIVE(P_AUTOURDE, VID, VID, VERBINF, 
0, NEG). 
and 
POSITIVE(P_CHEZ, VID, VID, VERBINF, 0, 
POS). 
can be covered by clause (1) (cf. subsec- 
tion 3.3). During the study of, for example, 
the distribution of the number of clauses in G 
on these different heoretical generality values, 
our "hope" is to obtain a gaussian-like graph 
in order to automatically select all the clauses 
present under the gaussian plot, or to calculate 
two thresholds that cover 95% of these clauses 
and to reject the other 5%. This distribution is
however not a gaussian one. 
Our second try has not only concerned the 
theoretical coverage of G clauses but also their 
empirical coverage. This second measure that 
we have defined is the number of E + that are 
really covered by each clause of G. We then con- 
sider the distribution of the empirical coverage 
of G clauses on the theoretical coverages of these 
clauses, that is, we consider the graph in which, 
for each different heoretical measure value for 
G clauses, we draw a line whose length corre- 
sponds to the total number of E + covered by 
the G clauses that have this theoretical cover- 
age value. Here two gaussians clearly appear 
(cf. figure 1), one for rather specific lauses and 
the other for more general ones. We have there- 
fore decided to keep all the generalized clauses 
produced by Progol. 
16We thank J. Nicolas, INRIA researcher at IRISA, for 
his help on this point. 
204 
800 
700 
600 
5OO 
400 
ul 300  
200 
100 
!!iiii~i~ii!ii!i!iiii!iiii!iiii~iiiii!iil 
Theoretical coverage 
Figure 1: Distribution of 
The second point concerns the determination 
of a measure of the quality of the learning for the 
parameter setting. We are especially interested 
in the percentage ofE + that are covered by the 
generalized clauses, and if we permit some noise 
in Progol parameter adjustment to allow more 
generalizations, by the percentage of E -  that 
are rejected by these generalized clauses. The 
measure of the recall and the precision rates of 
the learning method can be summarized in a 
Pearson coefficient: 
Pearson = (TP ,TN) - (FP ,FN)  
x /P rP*PrN*AP*AN 
where A = actual, Pr = predicated, P -- pos- 
itive, N= negative, T= true, F= false; the more 
close to 1 this value is, the better the learning 
is. 
The results for our learning method with a 
rate of Progol noise equal to 0 are the following: 
from the 4031 initial E + and the 6922 initial E- ,  
the 109 generalized clauses produced by Progol 
cover 2485 E + and 0 E-; 1546 E + and 6922 E-  
positive examples on clauses 
are therefore uncovered; the value of the Pear- 
son coefficient is 0.71. (NB: Figure 1 illustrates 
these results). 
We have developed a Perl program whose role 
is to find which Progol noise rate leads to the 
best results. This Progol noise rate is equal to 
37. With this rate, the results are the following: 
from the 4031 initial E + and the 6922 initial E-,  
the 66 generalized clauses produced by Progol 
cover 3547 E + and 348 E-; 484 E + and 6574 E-  
are therefore uncovered; the value of the Pear- 
son coefficient is 0.84. The stability of the set 
of learnt generalized clauses has been tested. 
4.2 Empir ica l  val idat ion 
In order to evaluate the empirical validity of our 
learning method, we have applied the 66 gen- 
eralized clauses to the Matra CCR corpus and 
have studied the appropriateness of the pairs 
that are stated relevant or irrelevant by them. 
Of course, it is impossible to test all the N-V 
combinations present in such a corpus. Our 
evaluation has focussed on some of the signif- 
205 
icant nouns of the domain. 
A Perl program presents to one expert all the 
N-V pairs that appear in one sentence in a part 
of the corpus and include one of the studied 
nouns. The expert manually tags each pair as 
relevant or not. This tagging is then compared 
to the results obtained for these N-V pairs of 
the same part of the corpus by the application 
of the generalized clauses learnt wit\]h Progol. 
The results for seven significant nouns (vis, 
@crou, porte, voyant, prise, capot, bouchon) 17 
are presented in table 1. In the left column, one 
N-V pair is considered as tagged "relevant" by 
the generalized clauses if at least one of them 
covers this pair; in the right one, at least six 
different clauses of G must cover a pair for it 
to be said correctly detected by the generalized 
clauses; the aim of this second test is to reduce 
noise in the results. 
1 occurrence 6 occurrences 
correctly found: 49 correctly found: 23 
incorrectly found: 54 incorrectly found: 4 
not found: 10 not found: 36 
Pearson = 0.5138 Pearson = 0.5209 
Table 1: Empirical validation on Matra CCR 
corpus 
The results are quite promising, especially if 
we compare them to those obtain by Chi-square 
correlation (cf. table 2). This comparison is 
interesting because Chi-square is the first step 
of our selection of N-V couples in the corpus (cf. 
subsection 3.2). 
correctly found: 38 
incorrectly found: 124 
not found: 21 
Pearson = 0.1206 
Table 2: Chi-square results on Matra CCR cor- 
pus 
5 Conc lus ion 
The Inductive Logic Programming learning 
method that we have proposed in order to de- 
fine what is a N-V pair whose elements are 
17(screw, nut, door, indicator signal, plug, cowl, cap). 
bound by one of the qualia relations in Puste- 
jovsky's Generative Lexicon formalism leads to 
very promising results: 83.05% of relevant pairs 
(after one occurrence) are detected for seven sig- 
nificant nouns; these results have to be com- 
pared with the 64% results of Chi-square. It 
is worth noticing that beyond this simple com- 
parison with one of the possible pure statis- 
tics based method is, the interest of using ILP 
learning is its explanatory characteristic; and it 
is this characteristic that have motivated our 
choice: contrary to statistical methods, our ILP 
method does not just extract statistically corre- 
lated pairs but it permits to automatically earn 
rules that distinguish relevant pairs from others. 
The fact that noise has to be used in Progol to 
obtain these results however means that some- 
thing is missing in our E + to fully define the 
concept "qualia pair" versus "not qualia pair"; 
some E -  have to be covered to define it better. 
A piece of information, maybe syntactic and/or 
semantic is missing in our E + to fully character- 
ize it. This fact can be easily illustrated by the 
following example: 'Verbinf det N' structures 
are generally relevant (ouvrir la porte 19, etc.), 
except when the N indicates a collection of ob- 
jects (nettoyer l'ensemble du rdservoir 2?) or a 
part of an object (vider le fond du rdservoir21). 
A simple POS-tagging of the sentences offers 
no difference between them. We are currently 
working on a semantic tagging of the Matra 
CCR corpus in order to improve the results. 
Another future work concerns the automatic 
distinction between the various qualia roles dur- 
ing learning. The last phase of the project will 
deal with the real use of the N-V pairs obtained 
by the machine learning method within one in- 
formation retrieval system and the evaluation of 
the improvement of its performances. 
Re ferences  
Rajeev Agarwal. 1995. Semantic Feature Extraction 
from Technical Texts with Limited Human Inter- 
vention. Ph.D. thesis, Mississippi State Univer- 
sity, USA. 
Susan Armstrong, Pierrette Bouillon, and 
Gilbert Robert. 1995.  Tagger Overview. 
lSThis comparison could be extended to other corpus 
frequency based technics (mutual information, etc.). 
19Open the door. 
2?Clean the whole tank. 
21Empty the tank bottom. 
206 
Technical report, ISSCO, (http://issco- 
www.unige.ch/staff/robert/tatoo/tagger.html). 
Susan Armstrong. 1996. Multext: Multilingual 
Text Tools and Corpora. In H. Feldweg and 
W. Hinrichs, editors, Lexikon und Text, pages 
107-119. Tiibingen: Niemeyer. 
Christian Bassac and Pierrette Bouillon. 2000. The 
Polymorphism of Verbs Exhibiting Middle Tran- 
sitive Alternations in English. Technical report, 
ISSCO. 
Jacques Bouaud, Beno~t Habert, Adeline Nazarenko, 
and Pierre Zweigenbaum. 1997. Regroupe- 
ments issus de d@pendances syntaxiques en cor- 
pus : cat@gorisation et confrontation avec deux 
mod@lisations conceptuelles. In Proceedings of 
Ingdnierie de la Connaissance, Roscoff, France. 
Pierrette Bouillon and Federica Busa. 2000. Gener- 
ativity in the Lexicon. CUP:Cambridge, In Press. 
Pierrette Bouillon, Sabine Lehmann, Sandra 
Manzi, and Dominique Petitpierre. 1998. 
DSveloppement de lexiques ~ grande @chelle. 
In Proceedings of colloque de Tunis 1997 "La 
mgmoire des mots', Tunis, Tunisie. 
Ted Briscoe and John Carroll. 1997. Automatic Ex- 
traction of Subcategorisation from Corpora. In 
Proceedings of 5th ACL conference on Applied 
Natural Language Processing, Washington, USA. 
James Cussens. 1996. Part-of-Speech Disambigua- 
tion using ILP. Technical report, Oxford Univer- 
sity Computing Laboratory. 
C@cile Fabre and Pascale S~billot. 1999. Seman- 
tic Interpretation of Binominal Sequences and In- 
formation Retrieval. In Proceedings of Interna- 
tional ICSC Congress on Computational Intelli- 
gence: Methods and Applications, CIMA '99, Sym- 
posium on Advances in Intelligent Data Analysis 
AIDA '99, Rochester, N.Y., USA. 
David Faure and Claire N@dellec. 1999. Knowledge 
Acquisition of Predicate Argument Structures 
from Technical Texts using Machine Learning: 
the System ASIUM. In Dieter Fensel Rudi Studer, 
editor, Proceedings of 11th European Workshop 
EKAW'99, Dagstuhl, Germany. Springer-Verlag. 
Christiane Fellbaum, editor. 1998. WordNet: An 
Electronic Lexical Database. MIT Press, Cam- 
bridge, MA. 
Gregory Grefenstette. 1994a. Corpus-Derived First, 
Second and Third-Order Word Affinities. In 
Proceedings of EURALEX'9~, Amsterdam, The 
Netherlands. 
Gregory Grefenstette. 1994b. Explorations in Auto- 
matic Thesaurus Discovery. Dordrecht: Kluwer 
Academic Publishers. 
Gregory Grefenstette. 1997. SQLET: Short Query 
Linguistic Expansion Techniques, Palliating One- 
Word Queries by Providing Intermediate Struc- 
ture to Text. In McGill-University, editor, Pro- 
ceedings of Recherche d'Informations Assistde 
par Ordinateur, RIAO'g7, Montr@al, Qu@bec, 
Canada. 
Beno~t Habert, Adeline Nazarenko, and Andr@ 
Salem. 1997. Les linguistiques de corpus. Ar- 
mand Collin/Masson, Paris. 
Zelig Harris, Michael Gottfried, Thomas Ryckman, 
Paul Mattick(Jr), Anne Daladier, Tzvee N. Har- 
ris, and Suzanna Harris. 1989. The Form of 
Information in Science, Analysis of Immunology 
Sublanguage. Kluwer Academic Publisher, Dor- 
drecht. 
Marti A. Hearst. 1992. Automatic Acquisition 
of Hyponyms from Large Text Corpora. In 
Proceedings of 15th International Conference on 
Computational Linguistics, COLING-92, Nantes, 
France. 
Tom M. Mitchell. 1997. Machine Learning. 
McGraw-Hill. 
Raymond Mooney. 1999. Learning for Semantic In- 
terpretation: Scaling Up without Dumbing Down. 
In Proceedings of Learning Language in Logic, 
LLL99, Bled, Slovenia. 
Emmanuel Morin. 1997: Extraction de liens 
s@mantiques entre termes dans des corpus de 
textes techniques : application ~ l'hyponymie. 
In Proceedings of Traitement Automatique des 
Langues Naturelles, TALN'97, Grenoble, France. 
Stephen Muggleton and Luc De-Raedt. 1994. In- 
ductive Logic Programming: Theory and Meth- 
ods. Journal of Logic Programming, 19-20:629- 
679. 
Stephen Muggleton. 1995. Inverse Entailment and 
Progol. New Generation Computing, 13(3-4):245- 
286. 
Dominique Petitpierre and Graham Russell. 1998. 
Mmorph - the Multext Morphology Program. 
Technical report, ISSCO. 
Ronan Pichon and Pascale S~billot. 1997. Acquisi- 
tion automatique d'informations lexicales ~ partir 
de corpus : un brian. Research report n?3321, IN- 
RIA, Rennes. 
Ronan Pichon and Pascale S@billot. 1999. From 
Corpus to Lexicon: from Contexts to Semantic 
Features. In Proceedings of Practical Applications 
in Language Corpora, PALC'99, to appear, Lodz, 
Poland. 
James Pustejovsky, Peter Anick, and Sabine Bergler. 
1993. Lexical Semantic Techniques for Corpus 
Analysis. Computational Linguistics, 19(2). 
James Pustejovsky. 1995. The Generative Lexicon. 
Cambridge:MIT Press. 
Sam Roberts, Wim Van-Laer, Nico Jacobs, Stephen 
Muggleton, and Jeremy Broughton. 1998. A 
Comparison of ILP and Propositional Systems on 
207 
Propositional Data. In Springer-Verlag, editor, 
Proceedings of 8th International Workshop on In- 
ductive Logic Programming, ILP-98, :Berlin, Ger- 
many. LNAI 1446. 
208 
  	
ffEvaluating Task Performance for a Unidirectional Controlled Language 
Medical Speech Translation System 
 
 
Nikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma, 
Marianne Starlander 
University of Geneva, TIM/ISSCO 
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland 
 
Nikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch, 
Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch, 
Marianne.Starlander@eti.unige.ch  
 
Beth Ann Hockey 
UCSC 
NASA Ames Research Center 
Moffett Field, CA 94035 
bahockey@email.arc.nasa.gov 
 
  
Abstract 
We present a task-level evaluation of the 
French to English version of MedSLT, a 
medium-vocabulary unidirectional con-
trolled language medical speech transla-
tion system designed for doctor-patient 
diagnosis interviews. Our main goal was 
to establish task performance levels of 
novice users and compare them to expert 
users. Tests were carried out on eight 
medical students with no previous expo-
sure to the system, with each student us-
ing the system for a total of three 
sessions. By the end of the third session, 
all the students were able to use the sys-
tem confidently, with an average task 
completion time of about 4 minutes. 
1 Introduction 
Medical applications have emerged as one of the 
most promising application areas for spoken lan-
guage translation, but there is still little agreement 
about the question of architectures. There are in 
particular two architectural dimensions which we 
will address: general processing strategy (statistical 
or grammar-based), and top-level translation func-
tionality (unidirectional or bidirectional transla-
tion). Given the current state of the art in 
recognition and machine translation technology, 
what is the most appropriate combination of 
choices along these two dimensions? 
Reflecting current trends, a common approach 
for speech translation systems is the statistical one. 
Statistical translation systems rely on parallel cor-
pora of source and target language texts, from 
which a translation model is trained. However, this 
is not necessarily the best alternative in safety-
critical medical applications. Anecdotally, many 
doctors express reluctance to trust a translation 
device whose output is not readily predictable, and 
most of the speech translation systems which have 
reached the stage of field testing rely on various 
types of grammar-based recognition and rule-based 
translation (Phraselator, 2006; S-MINDS, 2006; 
MedBridge, 2006). Even though statistical systems 
exhibit many desirable properties (purely data-
driven, domain independence), grammar-based 
systems utilizing probabilistic context-free gram-
mar tuning appear to deliver better results when 
training data is sparse (Rayner et al, 2005a). 
One drawback of grammar-based systems is that 
out-of-coverage utterances will be neither recog-
nized nor translated, an objection that critics have 
sometimes painted as decisive. It is by no means 
obvious, however, that restricted coverage is such 
a serious problem. In text processing, work on sev-
eral generations of controlled language systems has 
developed a range of techniques for keeping users 
within the bounds of system coverage (Kittredge, 
2003; Mitamura, 1999). If these techniques work 
for text processing, it is surely not inconceivable 
that variants of them will be equally successful for 
spoken language applications. Users are usually 
able to adapt to a controlled language system given 
enough time. The critical questions are how to 
provide efficient support to guide them towards the 
system's coverage, and how much time they will 
then need before they have acclimatized. 
With regard to top-level translation functional-
ity, the choice is between unidirectional and bidi-
rectional systems. Bidirectional systems are 
certainly possible today1, but the arguments in fa-
vor of them are not as clear-cut as might first ap-
pear. Ceteris paribus, doctors would certainly 
prefer bidirectional systems; in particular, medical 
students are trained to conduct examination dia-
logues using ?open questions? (WH-questions), 
and to avoid leading the patient by asking YN-
questions. 
The problem with a bidirectional system is, 
however, that open questions only really work well 
if the system can reliably handle a broad spectrum 
of replies from the patients, which is over-
optimistic given the current state of the art. In prac-
tice, the system's coverage is always more or less 
restricted, and some experimentation is required 
before the user can understand what language it is 
capable of handling. A doctor, who uses the system 
regularly, will acquire the necessary familiarity. 
The same might be true for a few patients, if spe-
cial circumstances mean that they encounter 
speech translation applications reasonably fre-
quently. Most patients, however, will have had no 
previous exposure to the system, and may be un-
willing to use a type of technology which they 
have trouble understanding.  
A unidirectional system, in which the doctor 
mostly asks YN-questions, will never be ideal. If, 
                                                          
1
 For example, the S-MINDS system (S-MINDS, 2006) 
offers bidirectional translation. 
however, the doctor can become proficient in using 
it, it may still be very much better than the alterna-
tive of no translation assistance at all.  
To summarize, today?s technology definitely 
lets us build unidirectional grammar-based medical 
speech translation systems which work for regular 
users who have had time to adapt to their limita-
tions. While bidirectional systems are possible, the 
case for them is less obvious, since users on the 
patient side may not in practice be able to use them 
effectively. 
In this paper, we will empirically investigate the 
ability of medical students to adapt to the coverage 
of unidirectional spoken language translation sys-
tem. We report a series of experiments, carried out 
using a French to English speech translation sys-
tem, in which medical students with no previous 
experience to the system were asked to use it to 
carry out a series of verbal examinations on sub-
jects who were simulating the symptoms of various 
types of medical conditions. Evaluation will be 
focused on usability. We primarily want to know 
how quickly subjects learn to use the system, and 
how their performance compares to that of expert 
users. 
2 The MedSLT system 
MedSLT (MedSLT, 2005; Bouillon et al, 2005) 
is a unidirectional, grammar-based medical speech 
translation system intended for use in doctor-
patient diagnosis dialogues. The system is built on 
top of Regulus (Regulus, 2006), an Open Source 
platform for developing grammar-based speech 
applications. Regulus supports rapid construction 
of complex grammar-based language models using 
an example-based method (Rayner et al, 2003; 
Rayner et al, 2006), which extracts most of the 
structure of the model from a general linguistically 
motivated resource grammar. Regulus-based rec-
ognizers are reasonably easy to maintain, and 
grammar structure is shared automatically across 
different subdomains. Resource grammars are now 
available for several languages, including English, 
Japanese (Rayner et al, 2005b), French (Bouillon 
et al, 2006) and Spanish. 
MedSLT includes a help module, whose purpose 
is to add robustness to the system and guide the 
user towards the supported coverage. The help 
module uses a second backup recognizer, equipped 
with a statistical language model; it matches the 
results from this second recognizer against a cor-
pus of utterances, which are within system cover-
age and have already been judged to give correct 
translations. In previous studies (Rayner et al, 
2005a; Starlander et al, 2005), we showed that the 
grammar-based recognizer performs much better 
than the statistical one on in-coverage utterances, 
and rather worse on out-of-coverage ones. We also 
found that having the help module available ap-
proximately doubled the speed at which subjects 
learned to use the system, measured as the average 
difference in semantic error rate between the re-
sults for their first quarter-session and their last 
quarter-session. It is also possible to recover from 
recognition errors by selecting one of the displayed 
help sentences; in the cited studies, we found that 
this increased the number of acceptably processed 
utterances by about 10%. 
The version of MedSLT used for the experi-
ments described in the present paper was config-
ured to translate from spoken French into spoken 
English in the headache subdomain. Coverage is 
based on standard headache-related examination 
questions obtained from a doctor, and consists 
mostly of yes/no questions. WH-questions and el-
liptical constructions are also supported. A typical 
short session with MedSLT might be as follows: 
- is the pain in the side of the head? 
- does the pain radiate to the neck? 
- to the jaw? 
- do you usually have headaches in the morn-
ing ?  
The recognizer?s vocabulary is about 1000 sur-
face words; on in-grammar material, Word Error 
Rate is about 8% and semantic error rate (per ut-
terance) about 10% (Bouillon et al, 2006). Both 
the main grammar-based recognizer and the statis-
tical recognizer used by the help system were 
trained from the same corpus of about 975 utter-
ances. Help sentences were also taken from this 
corpus. 
3 Experimental Setup 
In previous work, we have shown how to build a 
robust and extendable speech translation system. 
We have focused on performance metrics defined 
in terms of recognition and translation quality, and 
tested the system on na?ve users without any medi-
cal background (Bouillon et al, 2005; Rayner et 
al., 2005a; Starlander et al, 2005). 
In this paper, our primary goal was rather to fo-
cus on task performance evaluation using plausible 
potential users. The basic methodology used is 
common in evaluating usability in software sys-
tems in general, and spoken language systems in 
particular (Cohen et. al 2000). We defined a simu-
lated situation, where a French-speaking doctor 
was required to carry out a verbal examination of 
an English-speaking patient who claimed to be suf-
fering from a headache, using the MedSLT system 
to translate all their questions. The patients were 
played by members of the development team, who 
had been trained to answer questions consistently 
with the symptoms of different medical conditions 
which could cause headaches. We recruited eight 
native French-speaking medical students to play 
the part of the doctor. All of the students had com-
pleted at least four years of medical school; five of 
them were already familiar with the symptoms of 
different types of headaches, and were experienced 
in real diagnosis situations. 
The experiment was designed to study how well 
users were able to perform the task using the 
MedSLT system. In particular, we wished to de-
termine how quickly they could adapt to the re-
stricted language and limited coverage of the 
system. As a comparison point, representing near-
perfect performance, we also carried out the same 
test on two developers who had been active in im-
plementing the system, and were familiar with its 
coverage. 
Since it seemed reasonable to assume that most 
users would not interact with the system on a daily 
basis, we conducted testing in three sessions, with 
an interval of two days between each session. At 
the beginning of the first session, subjects were 
given a standardized 10-minute introduction to the 
system. This consisted of instruction on how to set 
up the microphone, a detailed description of the 
MedSLT push-to-talk interface, and a video clip 
showing the system in action. At the end of the 
presentation, the subject was given four sample 
sentences to get familiar with the system. 
After the training was completed, subjects were 
asked to play the part of a doctor, and conduct an 
examination through the system. Their task was to 
identify the headache-related condition simulated 
by the ?patient?, out of nine possible conditions. 
Subjects were given definitions of the simulated 
headache types, which included conceptual infor-
mation about location, duration, frequency, onset 
and possible other symptoms the particular type of 
headache might exhibit. 
Subjects were instructed to signal the conclusion 
of their examination when they were sure about the 
type of simulated headache. The time required to 
reach a conclusion was noted in the experiment 
protocols by the experiment supervisor. 
The subjects repeated the same diagnosis task on 
different predetermined sets of simulated condi-
tions during the second and third sessions. The ses-
sions were concluded either when a time limit of 
30 minutes was reached, or when the subject com-
pleted three headache diagnoses. At the end of the 
third session, the subject was asked to fill out a 
questionnaire. 
4 Results 
Performance of a speech translation system is 
best evaluated by looking at system performance 
as a whole, and not separately for each subcompo-
nent in the systems processing pipeline (Rayner et. 
al. 2000, pp. 297-pp. 312). In this paper, we conse-
quently focus our analysis on objective and subjec-
tive usability-oriented measures. 
In Section 4.1, we present objective usability 
measures obtained by analyzing user-system inter-
actions and measuring task performance. In Sec-
tion 4.2, we present subjective usability figures and 
a preliminary analysis of translation quality. 
4.1 Objective Usability Figures 
4.1.1 Analysis of User Interactions 
Most of our analysis is based on data from the 
MedSLT system log, which records all interactions 
between the user and the system. An interaction is 
initiated when the user presses the ?Start Recogni-
tion? button. The system then attempts to recog-
nize what the user says. If it can do so, it next 
attempts to show the user how it has interpreted the 
recognition result, by first translating it into the 
Interlingua, and then translating it back into the 
source language (in this case, French). If the user 
decides that the back-translation is correct, they 
press the ?Translate? button. This results in the 
system attempting to translate the Interlingua rep-
resentation into the target language (in this case, 
English), and speak it using a Text-To-Speech en-
gine. The system also displays a list of ?help sen-
tences?, consisting of examples that are known to 
be within coverage, and which approximately 
match the result of performing recognition with the 
statistical language model. The user has the option 
of choosing a help sentence from the list, using the 
mouse, and submitting this to translation instead.  
We classify each interaction as either ?success-
ful? or ?unsuccessful?. An interaction is defined to 
be unsuccessful if either 
i) the user re-initiates recognition without 
asking the system for a translation, or 
ii) the system fails to produce a correct 
translation or back translation. 
Our definition of ?unsuccessful interaction? in-
cludes instances where users accidentally press the 
wrong button (i.e. ?Start Recognition? instead of 
?Translate?), press the button and then say nothing, 
or press the button and change their minds about 
what they want to ask half way through. We ob-
served all of these behaviors during the tests. 
Interactions where the system produced a trans-
lation were counted as successful, irrespective of 
whether the translation came directly from the 
user?s spoken input or from the help list. In at least 
some examples, we found that when the translation 
came from a help sentence it did not correspond 
directly to the sentence the user had spoken; to our 
surprise, it could even be the case that the help sen-
tence expressed the directly opposite question to 
the one the user had actually asked. This type of 
interaction was usually caused by some deficiency 
in the system, normally bad recognition or missing 
coverage. Our informal observation, however, was 
that, when this kind of thing happened, the user 
perceived the help module positively: it enabled 
them to elicit at least some information from the 
patient, and was less frustrating than being forced 
to ask the question again. 
Table I to Table III show the number of total in-
teractions per session, the proportion of successful 
interactions, and the proportion of interactions 
completed by selecting a sentence from the help 
list. The total number of interactions required to 
complete a session decreased over the three ses-
sions, declining from an average of 98.6 interac-
tions in the first session to 63.4 in the second (36% 
relative) and 53.9 in the third (45% relative). It is 
interesting to note that interactions involving the 
help system did not decrease in frequency, but re-
mained almost constant over the first two sessions 
(15.5% and 14.0%), and were in fact most com-
mon during the third session (21.7%). 
 
Session 1 
Subject Interactions % Successful % Help 
User 1 57 56.1% 0.0% 
User 2 98 52.0% 25.5% 
User 3 91 63.7% 15.4% 
User 4 156 69.9% 10.3% 
User 5 86 64.0% 22.1% 
User 6 134 47.0% 19.4% 
User 7 56 53.6% 5.4% 
User 8 111 63.1% 26.1% 
AVG 98.6 58.7% 15.5% 
Table I Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 1st session 
 
Session 2 
Subject Interactions % Successful % Help 
User 1 50 74.0% 2.0% 
User 2 63 55.6% 27.0% 
User 3 34 88.2% 23.5% 
User 4 96 57.3% 17.7% 
User 5 64 65.6% 21.9% 
User 6 93 68.8% 10.8% 
User 7 48 60.4% 4.2% 
User 8 59 79.7% 5.1% 
AVG 63.4 68.7% 14.0% 
Table II Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 2nd session 
 
Session 3 
Subject Interactions % Successful % Help 
User 1 33 90.9% 33.3% 
User 2 57 56.1% 22.8% 
User 3 48 72.9% 29.2% 
User 4 67 70.2% 16.4% 
User 5 68 73.5% 27.9% 
User 6 60 70.0% 6.7% 
User 7 41 65.9% 14.6% 
User 8 57 56.1% 22.8% 
AVG 53.9 69.5% 21.7% 
Table III Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 3rd session 
In order to establish a performance baseline, we 
also analyzed interaction data for two expert users, 
who performed the same experiment. The expert 
users were two native French-speaking system de-
velopers, which were both familiar with the diag-
nosis domain. Table IV summarizes the results of 
those users. One of our expert users, listed as Ex-
pert 2, is the French grammar developer, and had 
no failed interactions. This confirms that recogni-
tion is very accurate for users who know the cov-
erage. 
 
Session 1 / Expert Users 
Subject Interactions % Successful % Help 
Expert 1 36 77.8% 13.9% 
Expert 2 30 100.0% 3.3% 
AVG 33 88.9% 8.6% 
Table IV Number of interactions, and percentages 
of successful interactions, and interactions 
involving the help component 
 
The expert users were able to complete the ex-
periment using an average of 33 interaction rounds. 
Similar performance levels were achieved by some 
subjects during the second and third session, which 
suggests that it is possible for at least some new 
users to achieve performance close to expert level 
within a few sessions. 
4.1.2 Task Level Performance 
One of the important performance indicators for 
end users is how long it takes to perform a given 
task. During the experiments, the instructors noted 
completion times required to reach a definite diag-
nosis in the experiment log. Table VI shows task 
completion times, categorized by session (col-
umns) and task within the session (rows).  
 Session 1 Session 2 Session 3 
Diagnosis 1 17:00 min 11:00 min 7:54 min 
Diagnosis 2 11:00 min 6:18 min 5:34 min 
Diagnosis 3 7:54 min 4:10 min 4:00 min 
Table V Average time required by subjects to 
complete diagnoses 
 
In the last two sessions, after subjects had ac-
climatized to the system, a diagnosis takes an aver-
age of about four minutes to complete. This 
compares to a three-minute average required to 
complete a diagnosis by our expert users. 
4.1.3 System coverage 
Table VI shows the percentage of in-coverage 
sentences uttered by the users on interactions that 
did not involve invocation of the help component. 
 
 IN-COVERAGE SENTENCES 
Session 1 54.9% 
Session 2 60.7% 
Session 3 64.6% 
Table VI Percentage of in-coverage sentences 
 
This indicates that subjects learn and adapt to 
the system coverage as they use the system more. 
The average proportion of in-coverage utterances 
is 10 percent higher during the third session than 
during the first session. 
4.2 Subjective Usability Measures 
4.2.1 Results of Questionnaire 
After finishing the third session, subjects were 
asked to fill in a short questionnaire, where re-
sponses were on a five-point scale ranging from 1 
(?strongly disagree?) to 5 (?strongly agree?). The 
results are presented in Table VIII. 
 
STATEMENT SCORE 
I quickly learned how to use the system. 4.4 
System response times were generally 
satisfactory. 
4.5 
When the system did not understand me, 
the help system usually showed me an-
other way to ask the question. 
4.6 
When I knew what I could say, the sys-
tem usually recognized me correctly. 
4.3 
I was often unable to ask the questions I 
wanted. 
3.8 
I could ask enough questions that I was 
sure of my diagnosis. 
4.3 
This system is more effective than non-
verbal communication using gestures. 
4.3 
I would use this system again in a simi-
lar situation. 
4.1 
Table VIII Subject responses to questionnaire. 
Scores are on a 5-point scale, averaged over all 
answers. 
 
Answers were in general positive, and most of 
the subjects were clearly very comfortable with the 
system after just an hour and a half of use. Interest-
ingly, even though most of the subjects answered 
?yes? to the question ?I was often unable to ask the 
questions I wanted?, the good performance of the 
help system appeared to compensate adequately for 
missing coverage. 
4.2.2 Translation Performance 
In order to evaluate the translation quality of the 
newly developed French-to-English system, we 
conducted a preliminary performance evaluation, 
similar to the evaluation method described in 
(Bouillon 2005). 
We performed translation judgment in two 
rounds. In the first round, an English-speaking 
judge was asked to categorize target utterances as 
comprehensible or not without looking at corre-
sponding source sentences. 91.1% of the sentences 
were judged as comprehensible. The remaining 
8.9% consisted of sentences where the terminology 
used was not familiar to the judge and of sentences 
where the translation component failed to produce 
a sufficiently good translation. An example sen-
tence is 
- Are the headaches better when you experi-
ence dark room? 
which stems from the French source sentence 
- Vos maux de t?te sont ils soulag?s par obs-
curit?? 
In the second round, English-speaking judges, 
sufficiently fluent in French to understand source 
language utterances, were shown the French source 
utterance, and asked to decide whether the target 
language utterance correctly reflected the meaning 
of the source language utterance. They were also 
asked to judge the style of the target language ut-
terance. Specifically, judges were asked to classify 
sentences as ?BAD? if the meaning of the English 
sentence did not reflect the meaning of the French 
sentence. Sentences were categorized as ?OK? if 
the meaning was transferred correctly and the sen-
tence was comprehensible, but the style of the re-
sulting English sentence was not perfect. Sentences 
were judged as ?GOOD? when they were compre-
hensible, and both meaning and style were consid-
ered to be completely correct. Table VIII 
summarizes results of two judges. 
 
 Good OK Bad  
Judge 1 15.8% 73.80% 10.3% 
Judge 2 46.6% 47.1% 6.3% 
Table VIII Judgments of the quality of the transla-
tions of 546 utterances 
 
It is apparent that translation judging is a highly 
subjective process. When translations were marked 
as ?bad?, the problem most often seemed to be re-
lated to lexical items where it was challenging to 
find an exact correspondence between French and 
English. Two common examples were ?troubles de 
la vision?, which was translated as ?blurred vi-
sion?, and ?faiblesse musculaire?, which was trans-
lated as ?weakness?. It is likely that a more careful 
choice of lexical translation rules would deal with 
at least some of these cases. 
5 Summary 
We have presented a first end-to-end evaluation 
of the MedSLT spoken language translation sys-
tem. The medical students who tested it were all 
able to use the system well, with performance in 
some cases comparable to that of that of system 
developers after only two sessions. At least for the 
fairly simple type of diagnoses covered by our sce-
nario, the system?s performance appeared clearly 
adequate for the task.  
This is particularly encouraging, since the 
French to English version of the system is quite 
new, and has not yet received the level of attention 
required for a clinical system. The robustness 
added by the help system was sufficient to com-
pensate for that, and in most cases, subjects were 
able to find ways to maneuver around coverage 
holes and other problems. It is entirely reasonable 
to hope that performance, which is already fairly 
good, would be substantially better with another 
couple of months of development work. 
In summary, we feel that this study shows that 
the conservative architecture we have chosen 
shows genuine potential for use in medical diagno-
sis situations. Before the end of 2006, we hope to 
have advanced to the stage where we can start ini-
tial trials with real doctors and patients. 
 
 
Acknowledgments 
We would like to thank Agnes Lisowska, Alia 
Rahal, and Nancy Underwood for being impartial 
judges over our system?s results. 
This work was funded by the Swiss National 
Science Foundation. 
References 
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. 
Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain 
medical speech translation. In Proceedings of the 
10th Conference of the European Association for 
Machine Translation (EAMT), Budapest, Hungary. 
P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-
taholma, M. Starlander, and N. Chatzichrisafis. 2006. 
Une grammaire multilingue partag?e pour la recon-
naissance et la g?n?ration. In Proceedings of TALN 
2006, Leuwen, Belgium. 
M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User 
Interface Design. Addison Wesley Publishing. 
R. I. Kittredge. 2003. Sublanguages and comtrolled 
languages. In R. Mitkov, editor, The Oxford Hand-
book of Computational Linguistics, pages 430?447. 
Oxford University Press. 
MedBridge, 2006. http://www.medtablet.com/. As of 
15th March 2006. 
MedSLT, 2005. http://sourceforge.net/projects/medslt/. 
As of 15th March 2006. 
T. Mitamura. 1999. Controlled language for multilin-
gual machine translation. In Proceedings of Machine 
Translation Summit VII, Singapore. 
Phraselator, 2006. http://www.phraselator.com. As of 
15 February 2006. 
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An 
open source environment for compiling typed unifi-
cation grammars into speech recognisers. In Pro-
ceedings of the 10th EACL (demo track), Budapest, 
Hungary. 
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma,M. Starlander, H. Isahara, 
K. Kankazi, and Y. Nakao. 2005a. A methodology for 
comparing grammar-based and robust approaches to 
speech understanding. In Proceedings of the 9th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Lisboa, Portugal. 
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. 
Wir?n. 2000. The Spoken Language Translator, 
Cambridge University Press.  
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Put-
ting Linguistics into Speech Recognition: The 
Regulus Grammar Compiler. CSLI Press, Chicago.  
Regulus, 2006. http://sourceforge.net/projects/regulus/. 
As of 15 March 2006. 
S-MINDS, 2006. http://www.sehda.com/. As of 15 
March 2006. 
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. San-
taholma, M. Rayner, B.A. Hockey, H. Isahara, K. 
Kanzaki, and Y. Nakao. 2005. Practicing controlled 
language through a help system integrated into the 
medical speech translation system (MedSLT). In Pro-
ceedings of the MT Summit X, Phuket, Thailand 
 
MedSLT: A Limited-Domain Unidirectional Grammar-Based Medical
Speech Translator
Manny Rayner, Pierrette Bouillon, Nikos Chatzichrisafis, Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Marianne.Starlander@eti.unige.ch
Beth Ann Hockey
UCSC/NASA Ames Research Center, Moffet Field, CA 94035
bahockey@email.arc.nasa.gov
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Abstract
MedSLT is a unidirectional medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several differ-
ent language pairs and subdomains. Vo-
cabulary ranges from about 350 to 1000
surface words, depending on the language
and subdomain. We will demo both the
system itself and the development envi-
ronment, which uses a combination of
rule-based and data-driven methods to
construct efficient recognisers, generators
and transfer rule sets from small corpora.
1 Overview
The mainstream in speech translation work is for the
moment statistical, but rule-based systems are still a
very respectable alternative. In particular, nearly all
systems which have actually been deployed are rule-
based. Prominent examples are (Phraselator, 2006;
S-MINDS, 2006; MedBridge, 2006).
MedSLT (MedSLT, 2005; Bouillon et al, 2005)
is a unidirectional medical speech translation system
for use in doctor-patient diagnosis dialogues, which
covers several different language pairs and subdo-
mains. Recognition is performed using grammar-
based language models, and translation uses a rule-
based interlingual framework. The system, includ-
ing the development environment, is built on top of
Regulus (Regulus, 2006), an Open Source platform
for developing grammar-based speech applications,
which in turn sits on top of the Nuance Toolkit.
The demo will show how MedSLT can be used
to carry out non-trivial diagnostic dialogues. In par-
ticular, we will demonstrate how an integrated intel-
ligent help system counteracts the brittleness inher-
ent in rule-based processing, and rapidly leads new
users towards the supported system coverage. We
will also demo the development environment, and
show how grammars and sets of transfer rules can be
efficiently constructed from small corpora of a few
hundred to a thousand examples.
2 The MedSLT system
The MedSLT demonstrator has already been exten-
sively described elsewhere (Bouillon et al, 2005;
Rayner et al, 2005a), so this section will only
present a brief summary. The main components are
a set of speech recognisers for the source languages,
a set of generators for the target languages, a transla-
tion engine, sets of rules for translating to and from
interlingua, a simple discourse engine for dealing
with context-dependent translation, and a top-level
which manages the information flow between the
other modules and the user.
MedSLT also includes an intelligent help mod-
ule, which adds robustness to the system and guides
the user towards the supported coverage. The help
module uses a backup recogniser, equipped with a
statistical language model, and matches the results
from this second recogniser against a corpus of utter-
ances which are within system coverage and trans-
late correctly. In previous studies, we showed that
the grammar-based recogniser performs much bet-
ter than the statistical one on in-coverage utterances,
but worse on out-of-coverage ones. Having the help
system available approximately doubled the speed
at which subjects learned, measured as the average
difference in semantic error rate between the results
for their first quarter-session and their last quarter-
session (Rayner et al, 2005a). It is also possible to
recover from recognition errors by selecting a dis-
played help sentence; this typically increases the
number of acceptably processed utterances by about
10% (Starlander et al, 2005).
We will demo several versions of the system, us-
ing different source languages, target languages and
subdomains. Coverage is based on standard exami-
nation questions obtained from doctors, and consists
mainly of yes/no questions, though there is also sup-
port for WH-questions and elliptical utterances. Ta-
ble 1 gives examples of the coverage in the English-
input headache version, and Table 2 summarises
recognition performance in this domain for the three
main input languages. Differences in the sizes of the
recognition vocabularies are primarily due to differ-
ences in use of inflection. Japanese, with little in-
flectional morphology, has the smallest vocabulary;
French, which inflects most parts of speech, has the
largest.
3 The development environment
Although the MedSLT system is rule-based, we
would, for the usual reasons, prefer to acquire these
rules from corpora using some well-defined method.
There is, however, little or no material available for
most medical speech translation domains, including
ours. As noted in (Probst and Levin, 2002), scarcity
of data generally implies use of some strategy to ob-
tain a carefully structured training corpus. If the cor-
pus is not organised in this way, conflicts between
alternate learned rules occur, and it is hard to in-
Where?
?do you experience the pain in your jaw?
?does the pain spread to the shoulder?
When?
?have you had the pain for more than a month?
?do the headaches ever occur in the morning?
How long?
?does the pain typically last a few minutes?
?does the pain ever last more than two hours?
How often?
?do you get headaches several times a week?
?are the headaches occurring more often?
How?
?is it a stabbing pain?
?is the pain usually severe?
Associated symptoms?
?do you vomit when you get the headaches?
?is the pain accompanied by blurred vision?
Why?
?does bright light make the pain worse?
?do you get headaches when you eat cheese?
What helps?
?does sleep make the pain better?
?does massage help?
Background?
?do you have a history of sinus disease?
?have you had an e c g?
Table 1: Examples of English MedSLT coverage
duce a stable set of rules. As Probst and Levin sug-
gest, one obvious way to attack the problem is to
implement a (formal or informal) elicitation strat-
egy, which biases the informant towards translations
which are consistent with the existing ones. This is
the approach we have adopted in MedSLT.
The Regulus platform, on which MedSLT
is based, supports rapid construction of com-
plex grammar-based language models; it uses an
example-based method driven by small corpora
of disambiguated parsed examples (Rayner et al,
2003; Rayner et al, 2006), which extracts most of
the structure of the model from a general linguis-
tically motivated resource grammar. The result is
a specialised version of the general grammar, tai-
lored to the example corpus, which can then be com-
piled into an efficient recogniser or into a genera-
Language Vocab WER SemER
English 441 6% 18%
French 1025 8% 10%
Japanese 347 4% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognisers.
?Vocab? = number of surface words in source lan-
guage recogniser vocabulary; ?WER? = Word Error
Rate for source language recogniser, on in-coverage
material; ?SemER? = semantic error rate for source
language recogniser, on in-coverage material.
tion module. Regulus-based recognisers and gen-
erators are easy to maintain, and grammar struc-
ture is shared automatically across different subdo-
mains. Resource grammars are available for several
languages, including English, Japanese, French and
Spanish.
Nuance recognisers derived from the resource
grammars produce both a recognition string and a
semantic representation. This representation con-
sists of a list of key/value pairs, optionally including
one level of nesting; the format of interlingua and
target language representations is similar. The for-
malism is sufficiently expressive that a reasonable
range of temporal and causal constructions can be
represented (Rayner et al, 2005b). A typical exam-
ple is shown in Figure 1. A translation rule maps
a list of key/value pairs to a list of key/value pairs,
optionally specifying conditions requiring that other
key/value pairs either be present or absent in the
source representation.
When developing new coverage for a given lan-
guage pair, the developer has two main tasks. First,
they need to add new training examples to the
corpora used to derive the specialised grammars
used for the source and target languages; second,
they must add translation rules to handle the new
key/value pairs. The simple structure of the Med-
SLT representations makes it easy to support semi-
automatic acquisition of both of these types of in-
formation. The basic principle is to attempt to find
the minimal set of new rules that can be added to the
existing set, in order to cover the new corpus exam-
ple; this is done through a short elicitation dialogue
with the developer. We illustrate this with a simple
example.
Suppose we are developing coverage for the En-
glish ? Spanish version of the system, and that
the English corpus sentence ?does the pain occur at
night? fails to translate. The acquisition tool first
notes that processing fails when converting from in-
terlingua to Spanish. The interlingua representation
is
[[utterance_type,ynq],
[pronoun,you],
[state,have_symptom],
[symptom,pain],[tense,present],
[prep,in_time],[time,night]]
Applying Interlingua ? Spanish rules, the result is
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
failed:[time,night]]
where the tag failed indicates that the element
[time,night] could not be processed. The tool
matches the incomplete transferred representation
against a set of correctly translated examples, and
shows the developer the English and Spanish strings
for the three most similar ones, here
does it appear in the morning
-> tiene el dolor por la man?ana
does the pain appear in the morning
-> tiene el dolor por la man?ana
does the pain come in the morning
-> tiene el dolor por la man?ana
This suggests that a translation for ?does the pain
occur at night? consistent with the existing rules
would be ?tiene el dolor por la noche?. The devel-
oper gives this example to the system, which parses
it using both the general Spanish resource grammar
and the specialised grammar used for generation in
the headache domain. The specialised grammar fails
to produce an analysis, while the resource grammar
produces two analyses,
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[[utterance_type,ynq],[pronoun,you],[state,have_symptom],
[tense,present],[symptom,headache],[sc,when],
[[clause,[[utterance_type,dcl],[pronoun,you],
[action,drink],[tense,present],[cause,coffee]]]]
Figure 1: Representation of ?do you get headaches when you drink coffee?
[tense,present],
[prep,por_temporal],
[temporal,noche]]
and
[[utterance_type,dcl],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
[temporal,noche]]
The first of these corresponds to the YN-question
reading of the sentence (?do you have the pain at
night?), while the second is the declarative reading
(?you have the pain at night?). Since the first (YN-
question) reading matches the Interlingua represen-
tation better, the acquisition tool assumes that it is
the intended one. It can now suggest two pieces of
information to extend the system?s coverage.
First, it adds the YN-question reading of ?tiene
el dolor por la noche? to the corpus used to train
the specialised generation grammar. The piece
of information acquired from this example is that
[temporal,noche] should be realised in this
domain as ?la noche?. Second, it compares the cor-
rect Spanish representation with the incomplete one
produced by the current set of rules, and induces a
new Interlingua to Spanish translation rule. This will
be of the form
[time,night] -> [temporal,noche]
In the demo, we will show how the development
environment makes it possible to quickly add new
coverage to the system, while also checking that old
coverage is not broken.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
MedBridge, 2006. http://www.medtablet.com/index.html.
As of 15 March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15 March 2005.
Phraselator, 2006. http://www.phraselator.com. As of 15
March 2006.
K. Probst and L. Levin. 2002. Challenges in automatic
elicitation of a controlled bilingual corpus. In Pro-
ceedings of the 9th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kankazi,
and Y. Nakao. 2005a. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. In Proceedings of the 9th International
Conference on Spoken Language Processing (ICSLP),
Lisboa, Portugal.
M. Rayner, P. Bouillon, M. Santaholma, and Y. Nakao.
2005b. Representational and architectural issues in a
limited-domain medical speech translator. In Proceed-
ings of TALN/RECITAL, Dourdan, France.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santa-
holma, M. Rayner, B.A. Hockey, H. Isahara, K. Kan-
zaki, and Y. Nakao. 2005. Practicing controlled lan-
guage through a help system integrated into the medi-
cal speech translation system (MedSLT). In Proceed-
ings of the MT Summit X, Phuket, Thailand.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Adapting a Medical Speech to Speech Translation System (MedSLT) to 
Arabic 
Pierrette Bouillon 
University of Geneva, TIM/ISSCO, ETI 
40, Bd. Du Pont d'Arve 
CH-1211 Geneva 4, Switzerland 
Pierrette.Bouillon@issco.unige.ch 
 
Manny Rayner 
Powerset Inc 
475 Brannan Str. 
San Francisco 
CA 94107, USA 
manny@powerset.com 
 
Sonia Halimi 
University of Geneva, TIM/ISSCO, ETI 
40, Bd. Du Pont d'Arve 
CH-1211 Geneva 4, Switzerland 
Sonia.Halimi@eti.unige.ch 
 
Beth Ann Hockey 
Mail Stop 19-26, UCSC UARC, NASA 
Ames Research Center, Moffett Field, 
CA 94035-1000 
bahockey@ucsc.edu 
 
 
 
Abstract 
We describe the adaptation for Arabic of 
the grammar-based MedSLT medical 
speech system. The system supports simple 
medical diagnosis questions about head-
aches using vocabulary of 322 words. We 
show that the MedSLT architecture based 
on motivated general grammars produces 
very good results, with a limited effort. 
Based on the grammars for other languages 
covered by the system, it is in fact very 
easy to develop an Arabic grammar and to 
specialize it efficiently for the different 
system tasks. In this paper, we focus on 
generation. 
1 Introduction 
MedSLT is a medical speech translation system. It 
allows a doctor to ask diagnosis questions in medi-
cal subdomains, such as headaches, abdominal 
pain, etc, covering a wide range of questions that 
doctors generally ask their patients. The grammar-
based architecture, built using specialization from 
reusable general grammars, is designed to allow a 
rapid development of different domains and lan-
guages. Presently, it supports English, French, 
Japanese, Spanish and Catalan. This article focuses 
on the system development for Arabic.  
In general, translation in this context raises two 
specific questions: 1) how to achieve recognition 
quality that is good enough for translation, and 2) 
how to get translations to be as idiomatic as possi-
ble so they can be understood by the patient. For 
close languages and domains where accuracy is not 
very important (e.g. information requests), it may 
be possible to combine a statistical recognizer with 
a commercial translation system as it is often done 
in commercial tools such as SpokenTranslation 
(Seligman and Dillinger, 2006). However, for this 
specific application in a multilingual context, this 
solution is not applicable at all: even if perfect rec-
ognition were possible (which is far from being the 
case), current commercial tools for translating to 
Arabic do not guarantee good quality. The domain 
dealt with here contains, in fact, many structures 
specific to this type of oral dialogue that can not be 
handled by these systems. For example, all the 
doctor?s interactions with the MedSLT system 
consist of questions whose structures differ from 
one language to another, with each language hav-
ing its own constraints. Consequently, two types of 
errors occur in Arabic translation systems. Either 
they do not recognize the interrogative structure as 
in example (1), or they produce ungrammatical 
sentences by copying the original structure as in 
example (2): 
 
 
41
(1) was the pain severe?  
 ? ? ?
?  (Google) 
(kana al alam chadid)  
 ?have-past-3 the pain severe? 
 
is the pain aggravated by exertion? 
 
 ? (Systran) 
(al alam yufaqim bi juhd)  
?the pain escalate-3 with effort? 
 
(2) is the headache aggravated by bright light? 
  ? 
 Proceedings of SPEECHGRAM 2007, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Bidirectional Grammar-Based Medical Speech Translator
Pierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1
Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey4
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
gflores@mcw.edu
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
manny@powerset.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
Abstract
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician?s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient?s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
1 Background
There is an urgent need for medical speech trans-
lation systems. The world?s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al, 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today?s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
41
of a well-defined controlled language. MedSLT
(Bouillon et al, 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al, 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA?s Clarissa (Rayner et al, 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al, 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al, 2003). A bidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
42
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question ?How
long have you had a sore throat??, Version 1 allows
the patient to respond both ?Desde algunos d??as?
(?For several days?) and ?Me ha dolido la garganta
desde algunos d??as? (?I have had a sore throat for
several days?), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician?s question, the
physician could ask ?Have you had a sore throat for
more than three days??; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion ?More than a week??, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
2 Top-level architecture
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
?Doctor? and ?Patient?. Initially, the ?Doctor? view
(the one shown in Figure 1) is active. The physician
presses the ?Push to talk? button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading ?Question?, we can see the actual
words returned by speech recognition. Here, these
words are ?Have you had rapid strep test?. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked ?Sys-
tem understood? shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle ?a?, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the ?Play? button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
?Patient? configuration shown in Figure 2. This dif-
fers from the ?Doctor? configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
3 Grammar-based processing
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
43
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the ?Play? button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play? button. The
help pane shows known valid responses to similar questions.
44
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of ?operationality criteria?, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single ?flatter? rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar ?
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al, 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al, 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al, 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al, 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al, 2005a) is specialised to cover only
second-person questions (?Do you get headaches
in the mornings??), while the generator grammar
used in the present application covers only first-
person declarative statements (?I visited the doctor
last Monday.?). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (?Do the headaches
occur when you are stressed??) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (?I have recently been in contact with
someone who has strep throat?), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician?s questions, while the other
produces back-translations of the patient?s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al, 2006;
Bouillon et al, to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, ?He consultado un
me?dico? could in principle mean either ?I visited a
doctor? or ?Did I visit a doctor??. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
4 Ellipsis processing and contextual
interpretation
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
45
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al, to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase ?In your stomach? in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: ?In your head? (PP) becomes
?La cabeza? (NP).
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient?s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser?s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (?Did you visit the doctor??; ?El lunes?
? ?I visited the doctor on Monday?), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (?What is
2It is also necessary to replace second-person pronouns with
first-person counterparts.
your temperature??; ?Cuarenta grados?? ?My tem-
perature is forty degrees?), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of ?semantically similar?.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al, to appear 2007a).
5 Help system
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
46
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
6 Evaluation
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al, 2005) and Spanish
(Bouillon et al, to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al, 2005a; Starlander et al, 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al, to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the ?patient? was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
(3) Doctor: For how long have you
had your sore throat?
Patient: Desde hace ma?s de
una semana
(Trans): I have had a sore
throat for more than one week
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: S?? el lunes
(Trans): I visited the doctor
for my sore throat monday
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si ma?s de dos semanas
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
7 Conclusions
We have presented a bidirectional grammar-based
English ? Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
References
[Baker et al1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
47
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partage?e pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syste`me de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partage?e multi-ta?che pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9?16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255?299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
of Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
[Michie et al2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197?206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32?39,
New York.
[Starlander et al2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423?
1433.
48
Proceedings of SPEECHGRAM 2007, pages 49?52,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Development Environment
for Building Grammar-Based Speech-Enabled Applications
Elisabeth Kron1, Manny Rayner1,2, Marianne Santaholma1, Pierrette Bouillon1
1 University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
elisabethkron@yahoo.co.uk
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
2 Powerset, Inc.
475 Brannan Street
San Francisco, CA 94107
manny@powerset.com
Abstract
We present a development environment for
Regulus, a toolkit for building unification
grammar-based speech-enabled systems, fo-
cussing on new functionality added over the
last year. In particular, we will show an
initial version of a GUI-based top-level for
the development environment, a tool that
supports graphical debugging of unification
grammars by cutting and pasting of deriva-
tion trees, and various functionalities that
support systematic development of speech
translation and spoken dialogue applications
built using Regulus.
1 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled sys-
tems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Uni-
versity, and is freely available for download from
the SourceForge website1. Regulus has been used
to build several large systems, including Geneva
University?s MedSLT medical speech translator
(Bouillon et al, 2005) and NASA?s Clarissa proce-
dure browser (Rayner et al, 2005b)2.
Regulus is described at length in
(Rayner et al, 2006), the first half of which consists
of an extended tutorial introduction. The release
1http://sourceforge.net/projects/regulus/
2http://ic.arc.nasa.gov/projects/clarissa/
also includes extensive online documentation,
including several example applications.
The core functionality offered by Regulus is com-
pilation of typed unification grammars into parsers,
generators, and Nuance-formatted CFG language
models, and hence also into Nuance recognition
packages. Small unification grammars can be com-
piled directly into executable forms. The central
idea of Regulus, however, is to base as much of
the development work as possible on large, domain-
independent resource grammars. A resource gram-
mar for English is available from the Regulus web-
site; similar grammars for several other languages
have been developed under the MedSLT project at
Geneva University, and can be downloaded from the
MedSLT SourceForge website3.
Large resource grammars of this kind are over-
general as they stand, and it is not possible to com-
pile them directly into efficient recognisers or gener-
ators. The platform, however, provides tools, driven
by small corpora of examples, that can be used to
create specialised versions of these general gram-
mars using the Explanation Based Learning (EBL)
algorithm. We have shown in a series of exper-
iments that suitably specialised grammars can be
compiled into efficient executable forms. In particu-
lar, recognisers built in this way are very competitive
with ones created using statistical training methods
(Rayner et al, 2005a).
The Regulus platform also supplies a framework
for using the compiled resources ? parsers, gen-
erators and recognisers ? to build speech transla-
tion and spoken dialogue applications. The envi-
ronment currently supports 75 different commands,
3http://sourceforge.net/projects/medslt
49
which can be used to carry out a range of func-
tions including compilation of grammars into var-
ious forms, debugging of grammars and compiled
resources, and testing of applications. The environ-
ment exists in two forms. The simpler one, which
has been available from the start of the project, is a
command-line interface embedded within the SICS-
tus Prolog top-level. The focus will however be on
a new GUI-based environment, which has been un-
der development since late 2006, and which offers
a more user-friendly graphical/menu-based view of
the underlying functionality.
In the rest of the paper, we outline how Regulus
supports development both at the level of grammars
(Section 2), and at the level of the applications that
can be built using the executable forms derived from
them (Section 3).
2 Developing unification grammars
The Regulus grammar development toolset borrows
ideas from several other systems, in particular the
SRI Core Language Engine (CLE) and the Xerox
Language Engine (XLE). The basic functionalities
required are uncontroversial. As usual, the Regulus
environment lets the user parse example sentences
to create derivation trees and logical forms; in the
other direction, if the grammar has also been com-
piled into a generator, the user can take a logical
form and use it to generate a surface string and an-
other derivation tree. Once a derivation tree has been
created, either through parsing or through genera-
tion, it is possible to examine individual nodes to
view the information associated with each one. Cur-
rently, this information consists of the syntactic fea-
tures, the piece of logical form built up at the node,
and the grammar rule or lexical entry used to create
it.
The Regulus environment also provides a more
elaborate debugging tool, which extends the ear-
lier ?grammar stepper? implemented under the CLE
project. Typically, a grammar development problem
has the following form. The user finds a bad sen-
tence B which fails to get a correct parse; however,
there are several apparently similar or related sen-
tences G1...Gn which do get correct parses. In most
cases, the explanation is that some rule which would
appear in the intended parse for B has an incorrect
feature-assignment.
A simple strategy for investigating problems of
this kind is just to examine the structures of B and
G1...Gn by eye, and attempt to determine what the
crucial difference is. An experienced developer,
who is closely familiar with the structure of the
grammar, will quite often be able to solve the prob-
lem in this way, at least in simple cases. ?Solving
by inspection? is not, however, very systematic, and
with complex rule bugs it can be hard even for ex-
perts to find the offending feature assignment. The
larger the grammar becomes, especially in terms of
the average number of features per category, the
more challenging the ad hoc debugging approach
becomes.
A more systematic strategy was pioneered in the
CLE grammar stepper. The developer begins by
looking at the working examples G1...Gn, to de-
termine what the intended correct structure would
be for B. They then build up the corresponding
structure for the bad example, starting at the bot-
tom with the lexical items and manually selecting
the rules used to combine them. At some point, a
unification will fail, and this will normally reveal the
bad feature assignment. The problem is that manual
bottom-up construction of the derivation tree is very
time-consuming, since even quite simple trees will
usually have at least a dozen nodes.
The improved strategy used in the Regulus gram-
mar stepper relies on the fact that the G1...Gn can
usually be constructed to include all the individual
pieces of the intended derivation tree for B, since in
most cases the feature mis-match arises when com-
bining two subtrees which are each internally con-
sistent. We exploit this fact by allowing the devel-
oper to build up the tree for B by cutting up the trees
for G1...Gn into smaller pieces, and then attempting
to recombine them. Most often, it is enough to take
two of the Gi, cut an appropriate subtree out of each
one, and try to unify them together; this means that
the developer can construct the tree for B with only
five operations (two parses, two cuts, and a join),
rather than requiring one operation for each node in
B, as in the bottom-up approach.
A common pattern is that B and G1 are identical,
except for one noun-phrase constituent NP , and G2
consists of NP on its own. To take an example from
the MedSLT domain, B could be ?does the morning
50
Figure 1: Example of using the grammar stepper to discover a feature mismatch. The window on the
right headed ?Stepper? presents the list of available trees, together with the controls. The windows headed
?Tree 1? and ?Tree 4? present the trees for item 1 (?does red wine give you headaches?) and item 4 (?the
morning?). The popup window on the lower right presents the feature mismatch information.
give you headaches??, G1 the similar sentence ?does
red wine give you headaches?? and G2 the single
NP ?the morning?. We cut out the first NP subtree
from G1 to produce what is in effect a tree with an
NP ?slash category?, that can be rendered as ?does
NP give you headaches??; call this G?1. We then cut
out the single NP subtree (this accounts for most,
but not all, of the derivation) from G2, to produce
G?2. By attempting to unify G?2 with the NP ?hole?
left in G?1, we can determine the exact nature of the
feature mismatch. We discover that the problem is
in the sortal features: the value of the sortal feature
on G?2 is time, but the corresponding feature-value
in the NP ?hole? is action\/cause.
Figure 1 contains a screenshot of the development
environment in the example above, showing the state
when the feature mismatch is revealed. A detailed
example, including screenshots for each step, is in-
cluded in the online Regulus GUI tutorial4.
4Available in the file doc/RegulusGUITutorial.pdf from the
SourceForge Regulus website
3 Developing applications
The Regulus platform contains support for both
speech translation and spoken dialogue applications.
In each case, it is possible to run the development
top-loop in a mode appropriate to the type of appli-
cation, including carrying out systematic regression
testing using both text and speech input. For both
types of application, the platform assumes a uniform
architecture with pre-specified levels of representa-
tion.
Due to shortage of space, and because it is the
better-developed of the two, we focus on speech
translation. The framework is interlingua-based,
and also permits simple context-based translation
involving resolution of ellipsis5. Processing goes
through the following sequence of representations:
1. Spoken utterance in source language.
2. Recognised words in source language.
5Although it is often possible to translate ellipsis as ellipsis
in closely related language pairs, this is usually not correct in
more widely separated ones.
51
3. Source logical form. Source logical form and
all other levels of representation are (almost)
flat lists of attribute/value pairs.
4. ?Source discourse representation?. A regu-
larised version of the source logical form, suit-
able for carrying out ellipsis resolution.
5. ?Resolved source discourse representation?.
The output resulting from carrying out any nec-
essary ellipsis processing on the source dis-
course representation. Typically this will add
material from the preceding context represen-
tation to create a representation of a complete
clause.
6. Interlingua. A language-independent version
of the representation.
7. Target logical form.
8. Surface words in target language.
The transformations from source logical form
to source discourse representation, from resolved
source discourse representation to interlingua, and
from interlinga to target logical form are defined
using translation rules which map lists of at-
tribute/value pairs to lists of attribute/value pairs.
The translation trace includes all the levels of rep-
resentation listed above, the translation rules used at
each stage, and other information omitted here. The
?translation mode? window provided by the devel-
opment environment makes all these fields available
in a structured form which allows the user to select
for display only those that are currently of interest.
The framework for spoken dialogue systems is simi-
lar, except that in the last three steps ?Interlingua? is
replaced by ?Dialogue move?, ?Target logical form?
by ?Abstract response?, and ?Surface words in target
language? by ?Concrete response?.
The platform contains tools for performing sys-
tematic regression testing of both speech translation
and spoken dialogue applications, using both text
and speech input. Input in the required modality is
taken from a specified file and passed through all
stages of processing, with the output being written
to another file. The user is able to annotate the re-
sults with respect to correctness (the GUI presents
a simple menu-based interface for doing this) and
save the judgements permanently, so that they can
be reused for future runs.
The most interesting aspects of the framework
involve development of spoken dialogue systems.
With many other spoken dialogue systems, the ef-
fect of a dialogue move is distributed throughout the
program state, and true regression testing is very dif-
ficult. Here, our side-effect free approach to dia-
logue management means that the DM can be tested
straightforwardly as an isolated component, since
the context is fully encapsulated as an object. The
theoretical issues involved are explored further in
(Rayner and Hockey, 2004).
References
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Rayner and Hockey2004] M. Rayner and B.A. Hockey.
2004. Side effect free dialogue management in a voice
enabled procedure browser. In Proceedings of the 8th
International Conference on Spoken Language Pro-
cessing (ICSLP), Jeju Island, Korea.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. Methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the international
space station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
52
Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 54?62,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Using Artiially Generated Data
to Evaluate Statistial Mahine Translation
Manny Rayner, Paula Estrella, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland
fEmmanuel.Rayner,Paula.Estrella,Pierrette.Bouillongunige.h
Beth Ann Hokey
Mail Stop 19-26, UCSC UARC
NASA Ames Researh Center, Moffett Field, CA 94035?1000
bahokeyus.edu
Yukie Nakao
LINA, Nantes University, 2, rue de la Houssiniere, BP 92208 44322 Nantes Cedex 03
yukie.nakaouniv-nantes.fr
Abstrat
Although Statistial Mahine Translation
(SMT) is now the dominant paradigm
within Mahine Translation, we argue that
it is far from lear that it an outperform
Rule-Based Mahine Translation (RBMT)
on small- to medium-voabulary applia-
tions where high preision is more impor-
tant than reall. A partiularly important
pratial example is medial speeh trans-
lation. We report the results of exper-
iments where we ongured the various
grammars and rule-sets in an Open Soure
medium-voabulary multi-lingual medial
speeh translation system to generate large
aligned bilingual orpora for English !
Frenh and English ! Japanese, whih
were then used to train SMT models based
on the ommon ombination of Giza++,
Moses and SRILM. The resulting SMTs
were unable fully to reprodue the per-
formane of the RBMT, with performane
topping out, even for English ! Frenh,
with less than 70% of the SMT translations
of previously unseen sentenes agreeing
with RBMT translations. When the out-
puts of the two systems differed, human
judges reported the SMT result as fre-
quently being worse than the RBMT re-
sult, and hardly ever better; moreover, the
added robustness of the SMT only yielded
a small improvement in reall, with a large
penalty in preision.
1 Introdution
When Statistial Mahine Translation (SMT) was
rst introdued in the early 90s, it enountered a
hostile reeption, and many people in the researh
ommunity were unwilling to believe it ould ever
be a serious ompetitor to symboli approahes
(f. for example (Arnold et al, 1994)). The pendu-
lum has now swung all the way to the other end of
the sale; right now, the prevailing wisdom within
the researh ommunity is that SMT is the only
truly viable arhiteture, and that rule-based ma-
hine translation (RBMT) is ultimately doomed to
failure. In this paper, one of our initial onerns
will be to argue for a ompromise position. In our
opinion, the initial septiism about SMT was not
groundless; the arguments presented against it of-
ten took the form of examples involving deep lin-
guisti reasoning, whih, it was laimed, would be
hard to address using surfae methods. Proponents
of RBMT had, however, greatly underestimated
the extent to whih SMT would be able to takle
the problem of robustness, where it appears to be
far more powerful than RBMT. For most mahine
translation appliations, robustness is the entral
issue, so SMT's urrent preeminene is hardly sur-
prising.
Even for the large-voabulary tasks where SMT
does best, the situation is by no means as lear as
one might imagine: aording to (Wilks, 2007),
purely statistial systems are still unable to out-
perform SYSTRAN. In this paper, we will how-
ever be more onerned with limited-domain MT
tasks, where robustness is not the key requirement,
and auray is paramount. An immediate exam-
54
ple is medial speeh translation, whih is estab-
lishing itself as an an appliation area of some sig-
niane (Bouillon et al, 2006; Bouillon et al,
2008a). Translation in medial appliations needs
to be extremely aurate, sine mistranslations
an have serious or even fatal onsequenes. At
the panel disussion at the 2008 COLING work-
shop on safety-ritial speeh translation (Rayner
et al, 2008), the onsensus opinion, based on in-
put from pratising physiians, was that an appro-
priate evaluation metri for medial appliations
would be heavily slanted towards auray, as op-
posed to robustness. If the metri is normalised so
as to award 0 points for no translation, and 1 point
for a orret translation, the estimate was that a
suitable sore for an inorret translation would
be something between ?25 and ?100 points. With
these requirements, it seems unlikely that a robust,
broad-overage arhiteture has muh hane of
suess. The obvious strategy is to build a limited-
domain ontrolled-language system, and tune it to
the point where auray reahes the desired level.
For systems of this kind, it is at least oneiv-
able that RBMT may be able to outperform SMT.
The next question is how to investigate the issues
in a methodologially even-handed way. A few
studies, notably (Seneff et al, 2006), suggest that
rule-based translation may in fat be preferable in
these ases. (Another related experiment is de-
sribed in (Dugast et al, 2008), though this was
arried out in a large-voabulary system). These
studies, however, have not been widely ited. One
possible explanation is suspiion about method-
ologial issues. Seneff and her olleagues trained
their SMT system on 20 000 sentene pairs, a
small number by the standards of SMT. It is a pri-
ori not implausible that more training data would
have enabled them to reate an SMT system that
was as good as, or better than, the rule-based sys-
tem.
In this paper, our primary goal is to take this
kind of objetion seriously, and develop a method-
ology designed to enable a tight omparison be-
tween rule-based and statistial arhitetures. In
partiular, we wish to examine the widely be-
lieved laim that SMT is now inherently better
than RBMT. In order to do this, we start with a
limited-domain RBMT system; we use it to auto-
matially generate a large orpus of aligned pairs,
whih is used to train a orresponding SMT sys-
tem. We then ompare the performane of the two
systems.
Our argument will be that this situation essen-
tially represents an upper bound for what is possi-
ble using the SMT approah in a limited domain.
It has been widely remarked that quality, as well
as quantity, of training data is important for good
SMT; in many projets, signiant effort is ex-
pended to lean the original training data. Here,
sine the data is automatially generated by a rule-
based system, we an be sure that it is already
ompletely lean (in the sense of being internally
onsistent), and we an generate as large a quan-
tity of it as we require. The appliation, more-
over, uses only a smallish voabulary and a fairly
onstrained syntax. If the derived SMT system is
unable to math the original RBMT system's per-
formane, it seems reasonable to laim that this
shows that there are types of appliations where
RBMT arhitetures are superior.
The experiments desribed have been arried
out using MedSLT, an Open Soure interlingua-
based limited-domain medial speeh translation
system. The rest of the paper is organised as fol-
lows. Setion 2 provides bakground on the Med-
SLT system. Setion 3 desribes the experimen-
tal framework, and Setion 4 the results obtained.
Setion 5 onludes.
2 The MedSLT System
MedSLT (Bouillon et al, 2005; Bouillon et al,
2008b) is a medium-voabulary interlingua-based
Open Soure speeh translation system for dotor-
patient medial examination questions, whih
provides any-language-to-any-language transla-
tion apabilities for all languages in the set En-
glish, Frenh, Japanese, Arabi, Catalan. Both
speeh reognition and translation are rule-based.
Speeh reognition runs on the Nuane 8.5 reog-
nition platform, with grammar-based language
models built using the Open Soure Regulus om-
piler. As desribed in (Rayner et al, 2006),
eah domain-spei language model is extrated
from a general resoure grammar using orpus-
based methods driven by a seed orpus of domain-
spei examples. The seed orpus, whih typi-
ally ontains between 500 and 1500 utteranes,
is then used a seond time to add probabilisti
weights to the grammar rules; this substantially
improves reognition performane (Rayner et al,
2006, x11.5). Voabulary sizes and performane
measures for speeh reognition in the three lan-
55
guages where serious evaluations have been ar-
ried out are shown in Figure 1.
Language Voab WER SemER
English 447 6% 11%
Frenh 1025 8% 10%
Japanese 422 3% 4%
Table 1: Reognition performane for English,
Frenh and Japanese MedSLT reognisers. ?Vo-
ab? = number of surfae words in soure lan-
guage reogniser voabulary; ?WER? = Word Er-
ror Rate for soure language reogniser, on in-
overage material; ?SemER? = semanti error rate
(proportion of utteranes failing to produe orret
interlingua) for soure language reogniser, on in-
overage material.
At run-time, the reogniser produes a soure-
langage semanti representation. This is rst
translated by one set of rules into an interlingual
form, and then by a seond set into a target lan-
guage representation. A target-language Regu-
lus grammar, ompiled into generation form, turns
this into one or more possible surfae strings, af-
ter whih a set of generation preferenes piks
one out. Finally, the seleted string is realised in
spoken form. Robustness issues are addressed by
means of a bak-up statistial reogniser, whih
drives a robust embedded help system. The pur-
pose of the help system (Chatzihrisas et al,
2006) is to guide the user towards supported ov-
erage; it performs approximate mathing of out-
put from the statistial reogniser again a library
of sentenes whih have been marked as orretly
proessed during system development, and then
presents the losest mathes to the user.
Examples of typial English domain sentenes
and their translations into Frenh and Japanese are
shown in Figure 2.
3 Experimental framework
In the literature on language modelling, there is
a known tehnique for bootstrapping a statisti-
al language model (SLM) from a grammar-based
language model (GLM). The grammar whih
forms the basis of the GLM is sampled randomly
in order to reate an arbitrarily large orpus of ex-
amples; these examples are then used as a train-
ing orpus to build the SLM (Jurafsky et al, 1995;
Jonson, 2005). We adapt this proess in a straight-
forward way to onstrut an SMT for a given
language pair, using the soure language gram-
mar, the soure-to-interlingua translation rules, the
interlingua-to-target-language rules, and the tar-
get language generation grammar. We start in the
same way, using the soure language grammar to
build a randomly generated soure language or-
pus; as shown in (Hokey et al, 2008), it is im-
portant to have a probabilisti grammar. We then
use the omposition of the other omponents to
attempt to translate eah soure language sentene
into a target language equivalent, disarding the
examples for whih no translation is produed.
The result is an aligned bilingual orpus of ar-
bitrary size, whih an be used to train an SMT
model.
We used this method to generate aligned or-
pora for the two MedSLT language pairs English
! Frenh and English ! Japanese. For eah lan-
guage pair, we rst generated one million soure-
language utteranes; we next ltered them to keep
only examples whih were full sentenes, as op-
posed to elliptial phrases, and nally used the
translation rules and target-language generators to
attempt to translate eah sentene. This reated
approximately 305K aligned sentene-pairs for
English ! Frenh (1901K words English, 1993K
words Frenh), and 311K aligned sentene-pairs
for English ! Japanese (1941K words English,
2214K words Japanese). We held out 2.5% of
eah set as development data, and 2.5% as test
data. Using Giza++, Moses and SRILM (Oh and
Ney, 2000; Koehn et al, 2007; Stolke, 2002), we
trained SMT models from inreasingly large sub-
sets of the training portion, using the development
portion in the usual way to optimize parameter val-
ues. Finally, we used the resulting models to trans-
late the test portion.
Our primary goal was to measure the extent to
whih the derived versions of the SMT were able
to approximate the original RBMT on data whih
was within the RBMT's overage. There is a sim-
ple and natural way to perform this measurement:
we apply the BLEU metri (Papineni et al, 2001),
with the RBMT's translation taken as the refer-
ene. This means that perfet orrespondene be-
tween the two translations would yield a BLEU
sore of 1.0.
This raises an important point. The BLEU
sores we are using here are non-standard; they
measure the extent to whih the SMT approxi-
mates the RBMT, rather than, as usual, measuring
56
English Is the pain above your eye?
Frenh Avez-vous mal au dessus des yeux?
Japanese Itami wa me no ue no atari desu ka?
English Have you had the pain for more than a month?
Frenh Avez-vous mal depuis plus d'un mois?
Japanese Ikkagetsu ijou itami wa tsuzuki mashita ka?
English Is the pain assoiated with nausea?
Frenh Avez-vous des nause?es quand vous avez la douleur?
Japanese Itamu to hakike wa okori masu ka?
English Does bright light make the pain worse?
Frenh La douleur est-elle aggrave?e par une lumiere forte?
Japanese Akarui hikari wo miru to zutsu wa hidoku nari masu ka?
Table 2: Examples of English domain sentenes, and the system's translations into Frenh and Japanese.
the extent to whih it approximates human trans-
lations. It is important to bring in human judge-
ment, to evaluate the ases where the SMT and
RBMT differ. If, in these ases, it transpired that
human judges typially thought that the SMT was
as good as the RBMT, then the differene would
be purely aademi. We need to satisfy ourselves
that human judges typially asribe differenes be-
tween SMT and RBMT to shortomings in the
SMT rather than in the RBMT.
Conretely, we olleted all the different
hSoure, SMT-translation, RBMT-translationi
triples produed during the ourse of the ex-
periments, and extrated those where the two
translations were different. We randomly seleted
a set of examples for eah language pair, and
asked human judges to lassify them into one of
the following ategories:
 RBMT better: The RBMT translation was
better, in terms of preserving meaning and/or
being grammatially orret;
 SMT better: The SMT translation was bet-
ter, in terms of preserving meaning and/or be-
ing grammatially orret;
 Similar: Both translations were about
equally good OR the soure sentene was
meaningless in the domain.
In order to show that our metris are intuitively
meaningful, it is sufient to demonstrate that the
frequeny of ourrene of RBMT better is both
large in omparison to that of SMT better, and
aounts for a substantial proportion of the total
population.
Finally, we onsider the question of whether
the SMT, whih is apable of translating out-of-
grammar sentenes, an add useful robustness to
the base system. We olleted, from the set used in
the experiments desribed in (Rayner et al, 2005),
all the English sentenes whih failed to be trans-
lated into Frenh. We used the best version of
the English ! Frenh SMT to translate eah of
these sentenes, and asked human judges to eval-
uate the translations as being learly aeptable,
learly unaeptable, or borderline.
In the next setion, we present the results of the
various experiments we have just desribed.
4 Results
We begin with Figure 1, whih shows non-
standard BLEU sores for versions of the English
! Frenh SMT system trained on quantities of
data inreasing from 14 287 to 285 740 pairs. As
an be seen, translation performane improves up
to about 175 000 pairs. After this, it levels out
at around BLEU = 0.90, well below that of the
RBMT system with whih it is being ompared.
A more diret way to report the result is simply to
ount the proportion of test sentenes that are not
in the training data, whih are translated similarly
by the SMT and the RBMT. This gure tops out at
around 68%.
The results strongly suggest that the SMT is
unable to repliate the RBMT's performane at
all losely even in an easy language-pair, irre-
spetive of the amount of training data available.
Out of uriosity, and to reassure ourselves that the
automati generation proedure was doing some-
thing useful, we also tried training the English !
Frenh SMT on pairs derived from the 669 ut-
57
Figure 1: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, hene overlapping.
terane ?seed orpus? used to generate the gram-
mar (f. Setion 2). This produed utterly dis-
mal performane, with BLEU = 0.52. The result is
more interesting than it may rst appear, sine, in
speeh reognition, the differene in performane
between the SLMs trained from seed orpora and
large generated orpora is fairly small (Hokey et
al., 2008).
It seemed possible that the improvement in per-
formane with inreased quantities of training data
might, in effet, only be due to the SMT fun-
tioning as a translation memory; sine training
and test data are independently generated by the
same random proess, they overlap, with the de-
gree of overlap inreasing as the training set gets
larger. In order to investigate this hypothesis,
we repeated the experiments with data whih had
been uniqued, so that the training and test sets
were ompletely disjoint, and neither ontained
any dupliate sentenes
1
. In fat, Figure 2 show
that the graph for uniqued English ! Frenh data
are fairly similar to the one for the original non-
uniqued data shown in Figures 1. The main differ-
ene is that the non-standard BLEU sore for the
1
Our opinion is that this is not a realisti way to evaluate
the performane of a small-voabulary system; for example,
in MedSLT, one expets that at least some training sentenes,
e.g. ?Where is the pain??, will also our frequently in test
data.
Figure 2: Non-standard BLEU sores against
number of pairs of training sentenes for English
! Frenh; training and test data both indepen-
dently generated, then uniqued to remove dupli-
ates and overlapping items.
uniqued data, unsurprisingly, tops out at a lower
level, reeting the fat that a ?translation mem-
ory? effet does indeed our to some extent.
Results for English ! Japanese showed the
same trends as English ! Frenh, but were more
pronouned. Table 3 ompares the performane
of the best versions of the SMTs for the two
language-pairs, using both plain and artiially
uniqued data. We see that, with plain data, the
English ! Japanese SMT falls even further short
of repliating the performane of the RBMT than
was the ase for English ! Frenh; BLEU is
only 0.76. The differene between the plain and
uniqued versions is also more extreme. BLEU
(0.64) is onsiderably lower for the version trained
on uniqued data, suggesting that the SMT for this
language pair is nding it harder to generalise,
and is in effet loser to funtioning as a trans-
lation memory. This is onrmed by ounting
the sentenes in test data and not in training data
whih were translated similarly by the SMT and
the RBMT; we nd that the gure tops out at the
very low value of 26%.
As noted in our disussion of the experimental
framework, the non-standard BLEU sores only
address the question of whether the performane
of the SMT and RBMT systems is the same. It is
58
Training data Test data BLEU
English ! Frenh
Generated Generated 0.90
Gen/uniqued Gen/uniqued 0.85
English ! Japanese
Generated Generated 0.76
Gen/uniqued Gen/uniqued 0.64
Table 3: Translation performane, in terms of non-
standard BLEU metri, for different ongura-
tions, training on all available data of the spe-
ied type. ?Generated? = data randomly gener-
ated; ?Gen/uniqued? = data randomly generated,
then uniqued so that dupliates are removed and
test and training pairs do not overlap.
neessary to establish what the differenes mean
in terms of human judgements. We onsequently
turn to evaluation of the pairs for whih the SMT
and the RBMT systems produed different trans-
lation results.
Table 4 shows the ategorisation, aording to
the riteria outlined at the end of Setion 3, for 500
English ! Frenh pairs randomly seleted from
the set of examples where RBMT and SMT gave
different results; we asked three judges to evalu-
ate them independently, and ombined their judg-
ments by majority deision where appropriate. We
observed a very heavy bias towards the RBMT,
with unanimous agreement among the judges that
the RBMT translation was better in 201/500 ases,
and 2-1 agreement in a further 127. In ontrast,
there were only 4/500 ases where the judges
unanimously thought that the SMT translation was
preferable, with a further 12 supported by a ma-
jority deision. The rest of the table gives the
ases where the RBMT and SMT translations were
judged the same or ases in whih the judges dis-
agreed; there were only 41/500 ases where no
majority deision was reahed. Our overall on-
lusion is that we are justied in evaluating the
SMT by using the BLEU sores with the RBMT as
the referene. Of the ases where the two systems
differ, only a tiny fration, at most 16/500, indi-
ate a better translation from the SMT, and well
over half are translated better by the RBMT. Ta-
ble 5 presents typial examples of bad SMT trans-
lations in the English ! Frenh pair, ontrasted
with the translations produed by the RBMT. The
rst two are grammatial errors (a superuous ex-
tra verb in the rst, and agreement errors in the
seond). The third is an bad hoie of tense and
preposition; although grammatial, the target lan-
guage sentene fails to preserve the meaning, and,
rather than referring to a 20 day period ending
now, instead refers to a 20 day period some time
in the past.
Result Agreement Count
RBMT better all judges 201
RBMT better majority 127
SMT better all judges 4
SMT better majority 12
Similar all judges 34
Similar majority 81
Unlear disagree 41
Total 500
Table 4: Comparison of RBMT and SMT perfor-
mane on 500 randomly hosen English ! Frenh
translation examples, evaluated independently by
three judges.
Table 6 shows a similar evaluation for the En-
glish ! Japanese. Here, the differene between
the SMT and RBMT versions was so pronouned
that we felt justied in taking a smaller sample, of
only 150 sentenes. This time, 92/150 ases were
unanimously judged as having a better RBMT
translation, and there was not a single ase where
even a majority found that the SMT was better.
Agreement was good here too, with only 8/150
ases not yielding at least a majority deision.
Result Agreement Count
RBMT better all judges 92
RBMT better majority 32
SMT better all judges 0
SMT better majority 0
Similar all judges 2
Similar majority 16
Unlear disagree 8
Total 150
Table 6: Comparison of RBMT and SMT per-
formane on 150 randomly hosen English !
Japanese translation examples, evaluated indepen-
dently by three judges.
Finally, we look at the performane of the SMT
on material whih the RBMT is not able to trans-
late. This would seem to be a situation where
59
English does a temperature hange ause the headahe
RBMT Frenh vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(your headahes are-they aused by hanges of temperature)
SMT Frenh avez-vous vos maux de t?ete sont-ils ause?s par des hangements de tempe?rature
(have-you your headahes are-they aused by hanges of temperature)
English are headahes relieved in the afternoon
RBMT Frenh vos maux de t?ete diminuent-ils l'apres-midi
(your headahes (MASC-PLUR) derease-MASC-PLUR the afternoon)
SMT Frenh vos maux de t?ete diminue-t-elle l'apres-midi
(your headahes (MASC-PLUR) derease-FEM-SING the afternoon)
English have you had them for twenty days
RBMT Frenh avez-vous vos maux de t?ete depuis vingt jours
(have-you your headahes sine twenty days)
SMT Frenh avez-vous eu vos maux de t?ete pendant vingt jours
(have-you had your headahes during twenty days)
Table 5: Examples of inorret SMT translations from English into Frenh. Errors are highlighted in
bold.
the SMT ould have an advantage; robustness is
generally a strength of statistial approahes. We
return to English ! Frenh in Table 7, whih
presents the result of running the best SMT model
on the 357 examples from the test set in (Rayner
et al, 2005) whih failed to be translated by the
RBMT. We divide the set into ategories based on
the reason for failure of the RBMT.
In the most populous group, translations that
failed due to out of voabulary items, the SMT
was, more or less by onstrution, also unable
to produe a translation. For the 110 items that
were out of grammar overage for the RBMT, the
SMT produed 38 good translations, and another 4
borderline translations. There were 50 items that
were within the soure grammar overage of the
RBMT, but failed somewhere in transfer and gen-
eration proessing. Of those, the majority (32)
represented ?bad? soure sentenes, onsidered as
ill-formed for the purposes of this experiment. Out
of the remaining items that were within RBMT
grammar overage, the SMT managed to produe
5 good translations and 1 borderline translation. In
total, on the most lenient interpretation, the SMT
produed 48 additional translations out of 357.
While this improvement in reall is arguably worth
having, it would ome at the prie of a substantial
deline in preision.
5 Disussion and Conlusions
We have presented a novel methodology for om-
paring RBMT and SMT, and tested it on a spe-
Result Count
Out of voabulary
Bad translation 187
Out of soure grammar overage
Good translation 38
Bad translation 44
Borderline translation 4
Bad soure sentene 34
In soure grammar overage
Good translation 5
Bad translation 12
Borderline translation 1
Bad soure sentene 32
Total 357
Table 7: English ! Frenh SMT performane on
examples from the test set whih failed to be trans-
lated by the RBMT, evaluated by one judge.
i pair of RBMT and SMT arhitetures. Our
laim is that these results show that the version
of SMT used here is not in fat apable of repro-
duing the output of the RBMT system. Although
there has been some interest in attempting to train
SMT systems from RBMT output, the evaluation
issues that arise when omparing SMT and RBMT
versions of a high-preision limited-domain sys-
tem are different from those arising in most MT
tasks, and neessitate a orrespondingly different
methodology. It is easy to gain the impression that
it is unsound, and that the experiment has been set
60
up in suh a way that only one result is possible.
This is not, in fat, true.
When we have disussed the methodology with
people who work primarily with SMT, we have
heard two main objetions. The rst is that the
SMT is being trained on RBMT output, and hene
an only be worse; a ommon suggestion is that
a system trained on human-produed translations
ould yield better results. It is not at all implau-
sible that an SMT trained on this kind of data
might perform better on material whih is outside
the overage of the RBMT system. In this do-
main, however, the important issue is preision,
not reall; what is ritial is the ability to trans-
late aurately on material that is within the on-
strained language dened by the RBMT overage.
The RBMT engine gives very good performane
on in-overage data, as has been shown in other
evaluations of the MedSLT system, e.g. (Rayner et
al., 2005); over 97% of all in-overage sentenes
are orretly translated. Human-generated transla-
tions would often, no doubt, be more natural than
those produed by the RBMT, and there would be
slightly fewer outright mistranslations. But the
primary reason why the SMT is doing badly is
not that the training material ontains bad trans-
lations, but rather that the SMT is inapable of
orretly reproduing the translations it sees in the
training data. Even in the easy English ! Frenh
language-pair, the SMT often produes a different
translation from the RBMT. It ould a priori have
been oneivable that the differenes were unin-
teresting, in the sense that SMT outputs different
from RBMT outputs were as good, or even better.
In fat, Table 4 show that this is not true; when the
two translations differ, although the SMT transla-
tion an oasionally be better, it is usually worse.
Table 6 shows that this problem is onsiderably
more aute in English ! Japanese. Thus the
SMT system's inability to model the RBMT sys-
tem points to a real limitation.
If the SMT had instead been trained on human-
generated data, its performane on in-overage
material ould only have improved substantially if
the SMT for some reason found it easier to learn to
reprodue patterns in human-generated data than
in RBMT-generated data. This seems unlikely.
The SMT is being trained from a set of translation
pairs whih are guaranteed to be ompletely on-
sistent, sine they have been automatially gener-
ated by the RBMT; the fat that the RBMT system
only has a small voabulary should also work in
its favour. If the SMT is unable to reprodue the
RBMT's output, it is reasonable to assume it will
have even greater difulty reproduing transla-
tions present in normal human-generated training
data, whih is always far from onsistent, and will
have a larger voabulary.
The seond objetion we have heard is that the
non-standard BLEU sores whih we have used to
measure performane use the RBMT translations
as a referene. People are quik to point out that,
if real human translations were sored in this way,
they would do less well on the non-standard met-
ris than the RBMT translations. This is, indeed,
absolutely true, and explains why it was essential
to arry out the omparison judging shown in Ta-
bles 4 and 6. If we had ompared human transla-
tions with RBMT translations in the same way, we
would have found that human translations whih
differed from RBMT translations were sometimes
better, and hardly ever worse. This would have
shown that the non-standard metris were inap-
propriate for the task of evaluating human trans-
lations. In the atual ase onsidered in this paper,
we nd a ompletely different pattern: the differ-
enes are one-sided in the opposite diretion, in-
diating that the non-standard metris do in fat
agree with human judgements here.
A general objetion to all these experiments is
that there may be more powerful SMT arhite-
tures. We used the Giza++/Moses/SRILM om-
binination beause it is the de fato standard. We
have posted the data we used at http://www.
bahr.net/geaf2009; this will allow other
groups to experiment with alternate arhitetures,
and determine whether they do in fat yield sig-
niant improvements. For the moment, however,
we think it is reasonable to laim that, in domains
where high auray is required, it remains to be
shown that SMT approahes are apable of ahiev-
ing the levels of performane that rule-based sys-
tems an deliver.
61
Referenes
D. Arnold, L. Balkan, S. Meijer, R.L. Humphreys, and
L. Sadler. 1994. Mahine Translation: An Introdu-
tory Guide. Blakwell, Oxford.
P. Bouillon, M. Rayner, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generi multi-
lingual open soure platform for limited-domain
medial speeh translation. In Proeedings of the
10th Conferene of the European Assoiation for
Mahine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
P. Bouillon, F. Ehsani, R. Frederking, and M. Rayner,
editors. 2006. Proeedings of the HLT-NAACL In-
ternational Workshop on Medial Speeh Transla-
tion, New York.
P. Bouillon, F. Ehsani, R. Frederking, M. MTear,
and M. Rayner, editors. 2008a. Proeedings of
the COLING Workshop on Speeh Proessing for
Safety Critial Translation and Pervasive Applia-
tions, Manhester.
P. Bouillon, G. Flores, M. Georgesul, S. Halimi,
B.A. Hokey, H. Isahara, K. Kanzaki, Y. Nakao,
M. Rayner, M. Santaholma, M. Starlander, and
N. Tsourakis. 2008b. Many-to-many multilingual
medial speeh translation on a PDA. In Proeed-
ings of The Eighth Conferene of the Assoiation
for Mahine Translation in the Amerias, Waikiki,
Hawaii.
N. Chatzihrisas, P. Bouillon, M. Rayner, M. San-
taholma, M. Starlander, and B.A. Hokey. 2006.
Evaluating task performane for a unidiretional
ontrolled language medial speeh translation sys-
tem. In Proeedings of the HLT-NAACL Interna-
tional Workshop on Medial Speeh Translation,
pages 9?16, New York.
L. Dugast, J. Senellart, and P. Koehn. 2008. Can we
relearn an RBMT system? In Proeedings of the
Third Workshop on Statistial Mahine Translation,
pages 175?178, Columbus, Ohio.
B.A. Hokey, M. Rayner, and G. Christian. 2008.
Training statistial language models from grammar-
generated data: A omparative ase-study. In Pro-
eedings of the 6th International Conferene on Nat-
ural Language Proessing, Gothenburg, Sweden.
R. Jonson. 2005. Generating statistial language mod-
els from interpretation grammars in dialogue sys-
tems. In Proeedings of the 11th EACL, Trento,
Italy.
A. Jurafsky, C. Wooters, J. Segal, A. Stolke, E. Fos-
ler, G. Tajhman, and N. Morgan. 1995. Us-
ing a stohasti ontext-free grammar as a language
model for speeh reognition. In Proeedings of
the IEEE International Conferene on Aoustis,
Speeh and Signal Proessing, pages 189?192.
P. Koehn, H. Hoang, A. Birh, C. Callison-Burh,
M. Federio, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open soure
toolkit for statistial mahine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 2.
F.J. Oh and H. Ney. 2000. Improved statistial align-
ment models. In Proeedings of the 38th Annual
Meeting of the Assoiation for Computational Lin-
guistis, Hong Kong.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automati evaluation of ma-
hine translation. Researh Report, Computer Si-
ene RC22176 (W0109-022), IBM Researh Divi-
sion, T.J.Watson Researh Center.
M. Rayner, P. Bouillon, N. Chatzihrisas, B.A.
Hokey, M. Santaholma, M. Starlander, H. Isahara,
K. Kanzaki, and Y. Nakao. 2005. A methodol-
ogy for omparing grammar-based and robust ap-
proahes to speeh understanding. In Proeedings
of the 9th International Conferene on Spoken Lan-
guage Proessing (ICSLP), pages 1103?1107, Lis-
boa, Portugal.
M. Rayner, B.A. Hokey, and P. Bouillon. 2006.
Putting Linguistis into Speeh Reognition: The
Regulus Grammar Compiler. CSLI Press, Chiago.
M. Rayner, P. Bouillon, G. Flores, F. Ehsani, M. Star-
lander, B. A. Hokey, J. Brotanek, and L. Biewald.
2008. A small-voabulary shared task for medial
speeh translation. In Proeedings of the COLING
Workshop on Speeh Proessing for Safety Criti-
al Translation and Pervasive Appliations, Manh-
ester.
S. Seneff, C. Wang, and J. Lee. 2006. Combining lin-
guisti and statistial methods for bi-diretional En-
glish Chinese translation in the ight domain. In
Proeedings of AMTA 2006.
A. Stolke. 2002. SRILM - an extensible language
modeling toolkit. In Seventh International Confer-
ene on Spoken Language Proessing. ISCA.
Y. Wilks. 2007. Stone soup and the Frenh room. In
K. Ahmad, C. Brewster, and M. Stevenson, editors,
Words and Intelligene I: Seleted Papers by Yorik
Wilks, pages 255?265.
62
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63
Manchester, August 2008
A Small-Vocabulary Shared Task for Medical Speech Translation
Manny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3
Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
farzad@fluentialinc.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
5 Dolores Labs
lukeab@gmail.com
Abstract
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English ? Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
1 Introduction
In computer science research, a ?shared task? is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
60
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a ?deep? evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
2 Data
The data we would use in the task is for the English
? Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor?s task was to use the translation sys-
tem to determine the likely reason for the patient?s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask ?How long
have you had a sore throat??, and the patient would
respond Hace dos d??as (?for two days?). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to ?How long have you had a sore throat??, the
patient could say either Hace dos d??as (?for two
days?) or Tengo dolor en la garganta hace dos d??as
(?I have had a sore throat for two days?).
Data was collected in 64 sessions, carried out
3http://www.issco.unige.ch/projects/
medslt/
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the ?doctor? was played by a real physi-
cian, and the part of the ?patient? by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the ?doctor?, the ?patient?, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
3 Evaluation metrics
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
3.1 Defining categories
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be ?1 point for a correct translation, 0
points for no translation, ?1000 points for an in-
correct translation.?
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
61
lations of the Spanish sentence La penicilina me
da alergias. Ideally, we would like the system to
translate this as ?I am allergic to penicillin?. If it
instead says ?I am allergic to the penicillin?, the
translation is slightly imperfect, but it is hard to see
any important misunderstanding arising as a result.
In contrast, the translation ?I am not allergic to
penicillin?, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
?The allergy penicillin does me?. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates ?Hello? as Buenas d??as, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate ?How long has this
cough been troubling you??, but correctly handled
the simpler formulation ?How long have you had a
cough??, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
4. Imperfect translation, potentially dangerous.
5. Nonsense.
6. No translation produced, but later rephrased
in a way the system handled adequately.
7. No translation produced, but not rephrased in
a way the system handled adequately.
3.2 Assigning utterances to categories
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon?s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
4http://www.mturk.com/mturk/welcome
62
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: ?It
ain?t what you don?t know that gets you into trou-
ble. It?s what you know for sure that just ain?t so?.
3.4 Discarding uncertain responses
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a ?fil-
ter?: for each utterance, we produce both a transla-
tion in the target language, and a ?reference trans-
lation? in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this ?ref-
erence translation? is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the ?penicillin? example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
?I am not allergic to penicillin?, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the ?filtered? version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
4 Summary and conclusions
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
63
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 9?16
Manchester, August 2008
Making Speech Look Like Text
in the Regulus Development Environment
Elisabeth Kron
3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
Manny Rayner, Marianne Santaholma, Pierrette Bouillon, Agnes Lisowska
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Marianne.Santaholma@eti.unige.ch
Pierrette.Bouillon@issco.unige.ch
Agnes.Lisowska@issco.unige.ch
Abstract
We present an overview of the de-
velopment environment for Regulus, an
Open Source platform for construction of
grammar-based speech-enabled systems,
focussing on recent work whose goal has
been to introduce uniformity between text
and speech views of Regulus-based appli-
cations. We argue the advantages of be-
ing able to switch quickly between text and
speech modalities in interactive and offline
testing, and describe how the new func-
tionalities enable rapid prototyping of spo-
ken dialogue systems and speech transla-
tors.
1 Introduction
Sex is not love, as Madonna points out at the be-
ginning of her 1992 book Sex, and love is not
sex. None the less, even people who agree with
Madonna often find it convenient to pretend that
these two concepts are synonymous, or at least
closely related. Similarly, although text is not
speech, and speech is not text, it is often conve-
nient to pretend that they are both just different as-
pects of the same thing.
In this paper, we will explore the similarities and
differences between text and speech, in the con-
crete setting of Regulus, a development environ-
ment for grammar based spoken dialogue systems.
Our basic goal will be to make text and speech
processing as similar as possible from the point
of view of the developer. Specifically, we arrange
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
things so that the developer is able to develop her
system using a text view; she will write text-based
rules, and initially test the system using text in-
put and output. At any point, she will be able to
switch to a speech view, compiling the text-based
processing rules into corresponding speech-based
versions, and test the resulting speech-based sys-
tem using speech input and output.
Paradoxically, the reason why it is so important
to be able to switch seamlessly between text and
speech viewpoints is that text and speech are in
fact not the same. For example, a pervasive prob-
lem in speech recognition is that of easily confus-
able pairs of words. This type of problem is of-
ten apparent after just a few minutes when running
the system in speech mode (the recogniser keeps
recognising one word as the other), but is invis-
ible in text mode. More subtly, some grammar
problems can be obvious in text mode, but hard
to see in speech mode. For instance, articles like
?the? and ?a? are short, and usually pronounced
unstressed, which means that recognisers can be
reasonably forgiving about whether or not to hy-
pothesise them when they are required or not re-
quired by the recognition grammar. In text mode, it
will immediately be clear if the grammar requires
an article in a given NP context: incorrect vari-
ants will fail to parse. In speech mode, the symp-
toms are far less obvious, and typically amount to
no more than a degradation in recognition perfor-
mance.
The rest of the paper is structured as follows.
Sections 2 and 3 provide background on the Reg-
ulus platform and development cycle respectively.
Section 4 describes speech and text support in the
interactive development environment, and 5 de-
scribes how the framework simplifies the task of
9
switching between modalities in regression testing.
Section 6 concludes.
2 The Regulus platform
The Regulus platform is a comprehensive toolkit
for developing grammar-based speech-enabled
systems that can be run on the commercially avail-
able Nuance recognition environment. The plat-
form has been developed by an Open Source con-
sortium, the main partners of which have been
NASA Ames Research Center and Geneva Univer-
sity, and is freely available for download from the
SourceForge website1. In terms of ideas (though
not code), Regulus is a descendent of SRI Inter-
national?s CLE and Gemini platforms (Alshawi,
1992; Dowding et al, 1993); other related systems
are LKB (Copestake, 2002), XLE (Crouch et al,
2008) and UNIANCE (Bos, 2002).
Regulus has already been used to build sev-
eral large applications. Prominent examples
are Geneva University?s MedSLT medical speech
translator (Bouillon et al, 2005), NASA?s Clarissa
procedure browser (Rayner et al, 2005) and Ford
Research?s experimental SDS in-car spoken dia-
logue system, which was awarded first prize at
the 2007 Ford internal demo fair. Regulus is de-
scribed at length in (Rayner et al, 2006), the first
half of which consists of an extended tutorial in-
troduction. The release includes a command-line
development environment, extensive online docu-
mentation, and several example applications.
The core functionality offered by Regulus is
compilation of typed unification grammars into
parsers, generators, and Nuance-formatted CFG
language models, and hence also into Nuance
recognition packages. These recognition packages
produced by Regulus can be invoked through the
Regulus SpeechServer (?Regserver?), which pro-
vides an interface to the underlying Nuance recog-
nition engine. The value added by the Regserver
is to provide a view of the recognition process
based on the Regulus unification grammar frame-
work. In particular, recognition results, originally
produced in the Nuance recognition platform?s in-
ternal format, are reformatted into the semantic no-
tation used by the Regulus grammar formalism.
There is extensive support within the Regulus
toolkit for development of both speech translation
and spoken dialogue applications. Spoken dia-
1http://sourceforge.net/projects/
regulus/
logue applications (Rayner et al, 2006, Chapter 5)
use a rule-based side-effect free state update model
similar in spirit to that described in (Larsson and
Traum, 2000). Very briefly, there are three types of
rules: state update rules, input management rules,
and output management rules. State update rules
take as input the current state, and a ?dialogue
move?; they produce as output a new state, and an
?abstract action?. Dialogue moves are abstract rep-
resentations of system inputs; these inputs can ei-
ther be logical forms produced by the grammar, or
non-speech inputs (for example, mouse-clicks in a
GUI). Similarly, abstract actions are, as the name
suggests, abstract representations of the concrete
actions the dialogue system will perform, for ex-
ample speaking or updating a visual display. Input
management rules map system inputs to dialogue
moves; output management rules map abstract ac-
tions to system outputs.
Speech translation applications are also rule-
based, using an interlingua model (Rayner et al,
2006, Chapter 6). The developer writes a second
grammar for the target language, using Regulus
tools to compile it into a generator; mappings from
source representation to interlingua, and from in-
terlingua to target representation, are defined by
sets of translation rules. The interlingua itself is
specified using a third Regulus grammar (Bouillon
et al, 2008).
To summarise, the core of a Regulus application
consists of several different linguistically oriented
rule-sets, some of which can be interpreted in ei-
ther a text or a speech modality, and all of which
need to interact correctly together. In the next sec-
tion, we describe how this determines the nature of
the Regulus development cycle.
3 The Regulus development cycle
Small unification grammars can be compiled di-
rectly into executable forms. The central idea
of Regulus, however, is to base as much of
the development work as possible on large,
domain-independent, linguistically motivated re-
source grammars. A resource grammar for En-
glish is available from the Regulus website; similar
grammars for several other languages have been
developed under the MedSLT project at Geneva
University, and can be downloaded from the Med-
SLT SourceForge website2. Regulus contains
2http://sourceforge.net/projects/
medslt
10
an extensive set of tools that permit specialised
domain-specific grammars to be extracted from the
larger resource grammars, using example-based
methods driven by small corpora (Rayner et al,
2006, Chapter 7). At the beginning of a project,
these corpora can consist of just a few dozen exam-
ples; for a mature application, they will typically
have grown to something between a few hundred
and a couple of thousand sentences. Specialised
grammars can be compiled by Regulus into effi-
cient recognisers and generators.
As should be apparent from the preceding de-
scription, the Regulus architecture is designed to
empower linguists to the maximum possible ex-
tent, in terms of increasing their ability directly
to build speech enabled systems; the greater part
of the core development teams in the large Reg-
ulus projects mentioned in Section 1 have indeed
come from linguistics backgrounds. Experience
with Regulus has however shown that linguists are
not quite as autonomous as they are meant to be,
and in particular are reluctant to work directly with
the speech view of the application. There are sev-
eral reasons.
First, non-toy Regulus projects require a range
of competences, including both software engineer-
ing and linguistics. In practice, linguist rule-
writers have not been able to test their rules in
the speech view without writing glue code, scripts,
and other infrastructure required to tie together the
various generated components. These are not nec-
essarily things that they want to spend their time
doing. The consequence can easily be that the lin-
guists end up working exclusively in the text view,
and over-refine the text versions of the rule-sets.
From a project management viewpoint, this results
in bad prioritisation decisions, since there are more
pressing issues to address in the speech view.
A second reason why linguist rule-writers have
been unhappy working in the speech view is the
lack of reproducibility associated with speech in-
put. One can type ?John loves Mary? into a text-
processing system any number of times, and ex-
pect to get the same result. It is much less reason-
able to expect to get the same result each time if
one says ?John loves Mary? to a speech recogniser.
Often, anomalous results occur, but cannot be de-
bugged in a systematic fashion, leading to general
frustration. The result, once again, is that linguists
have preferred to stick with the text view, where
they feel at home.
Yet another reason why rule-writers tend to
limit themselves to the text view is simply the
large number of top-level commands and inter-
mediate compilation results. The current Regulus
command-line environment includes over 110 dif-
ferent commands, and compilation from the initial
resource grammar to the final Nuance recognition
package involves creating a sequence of five com-
pilation steps, each of which requires the output
created by the preceding one. This makes it diffi-
cult for novice users to get their bearings, and in-
creases their cognitive load. Additionally, once the
commands for the text view have been mastered,
there is a certain temptation to consider that these
are enough, since the text and speech views can
reasonably be perceived as fairly similar.
In the next two sections, we describe an en-
hanced development environment for Regulus,
which addresses the key problems we have just
sketched. From the point of view of the linguist
rule-writer, we want speech-based development to
feel more like text-based development.
4 Speech and text in the online
development environment
The Regulus GUI (Kron et al, 2007) is intended
as a complete redesign of the development envi-
ronment, which simultaneously attacks all of the
central issues. Commands are organised in a struc-
tured set of functionality-based windows, each of
which has an appropriate set of drop-down menus.
Following normal GUI design practice (Dix et al,
1998, Chapters 3 and 4); (Jacko and Sears, 2003,
Chapter 13), only currently meaningful commands
are executable in each menu, with the others shown
greyed out.
Both compile-time and run-time speech-related
functionality can be invoked directly from the
command menus, with no need for external scripts,
Makefiles or glue code. Focussing for the moment
on the specific case of developing a speech transla-
tion application, the rule-writer will initially write
and debug her rules in text mode. She will be able
to manipulate grammar rules and derivation trees
using the Stepper window (Figure 1; cf. also (Kron
et al, 2007)), and load and test translation rules
in the Translate window (Figure 2). As soon as
the grammar is consistent, it can at any point be
compiled into a Nuance recognition package us-
ing the command menus. The resulting recogniser,
together with other speech resources (license man-
11
Figure 1: Using the Stepper window to browse trees in the Toy1 grammar from (Rayner et al, 2006,
Chapter 4). The upper left window shows the analysis tree for ?switch on the light in the kitchen?; the
lower left window shows one of the subtrees created by cutting the first tree at the higher NP node. Cut
subtrees can be recombined for debugging purposes (Kron et al, 2007).
Figure 2: Using the Translate window to test the toy English ? French translation application from
(Rayner et al, 2006, Chapter 6). The to- and from-interlingua rules used in the example are shown in the
two pop-up windows at the top of the figure.
12
regulus_config(regulus_grammar,
[toy1_grammars(toy1_declarations),
toy1_grammars(toy1_rules),
toy1_grammars(toy1_lexicon)]).
regulus_config(top_level_cat, ?.MAIN?).
regulus_config(nuance_grammar, toy1_runtime(recogniser)).
regulus_config(to_interlingua_rules,
toy1_prolog(?eng_to_interlingua.pl?)).
regulus_config(from_interlingua_rules,
toy1_prolog(?interlingua_to_fre.pl?)).
regulus_config(generation_rules, toy1_runtime(?generator.pl?)).
regulus_config(nuance_language_pack,
?English.America?).
regulus_config(nuance_compile_params, [?-auto_pron?, ?-dont_flatten?]).
regulus_config(translation_rec_params,
[package=toy1_runtime(recogniser), grammar=?.MAIN?]).
regulus_config(tts_command,
?vocalizer -num_channels 1 -voice juliedeschamps -voices_from_disk?).
Figure 3: Config file for a toy English ? French speech translation application, showing items relevant
to the speech view. Some declarations have been omitted for expositional reasons.
ager, TTS engine etc), can then be started using a
single menu command.
In accordance with the usual Regulus design
philosophy of declaring all the resources associ-
ated with a given application in its config file, the
speech resources are also specified here. Figure 3
shows part of the config file for a toy translation
application, in particular listing all the declara-
tions relevant to the speech view. If we needed to
change the speech resources, this would be done
just by modifying the last four lines. For example,
the config file as shown specifies construction of
a recogniser using acoustic models appropriate to
American English. We could change this to British
English by replacing the entry
regulus_config(nuance_language_pack,
?English.America?).
with
regulus_config(nuance_language_pack,
?English.UK?).
When the speech resources have been loaded,
the Translate window can take input equally easily
in text or speech mode; the Translate button pro-
cesses written text from the input pane, while the
Recognise button asks for spoken input. In each
case, the input is passed through the same process-
ing stages of source-to-interlingua and interlingua-
to-target translation, followed by target-language
generation. If a TTS engine or a set of recorded
target language wavfiles is specified, they are used
to realise the final result in spoken form (Figure 4).
Every spoken utterance submitted to recognition
is logged as a SPHERE-headed wavfile, in a time-
stamped directory started at the beginning of the
current session; this directory also contains a meta-
data file, which associates each recorded wavfile
with the recognition result it produced. The Trans-
late window?s History menu is constructed using
the meta-data file, and allows the user to select any
recorded utterance, and re-run it through the sys-
tem as though it were a new speech input. The
consequence is that speech input becomes just as
reproducible as text, with corresponding gains for
interactive debugging in speech mode.
5 Speech and text in regression testing
In earlier versions of the Regulus development
environment (Rayner et al, 2006, ?6.6), regres-
sion testing in speech mode was all based on
Nuance?s batchrec utility, which permits of-
fline recognition of a set of recorded wavfiles.
A test suite for spoken regression testing conse-
quently consisted of a list of wavfiles. These
were first passed through batchrec; outputs
were then post-processed into Regulus form, and
finally passed through Regulus speech understand-
ing modules, such as translation or dialogue man-
agement.
As Regulus applications grow in complexity,
this model has become increasingly inadequate,
since system input is very frequently not just a
list of monolingual speech events. In a multi-
modal dialogue system, input can consist of either
speech or screen events (text/mouse-clicks); con-
text is generally important, and the events have to
be processed in the order in which they occurred.
Dialogue systems which control real or simulated
robots, like the Wheelchair application of (Hockey
13
Figure 4: Speech to speech translation from the GUI, using a Japanese to Arabic translator built from
MedSLT components (Bouillon et al, 2008). The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
and Miller, 2007) will also receive asynchronous
inputs from the robot control and monitoring pro-
cess; once again, all inputs have to be processed in
the appropriate temporal order. A third example is
contextual bidirectional speech translation (Bouil-
lon et al, 2007). Here, the problem is slightly
different ? we have only speech inputs, but they
are for two different languages. The basic issue,
however, remains the same, since inputs have to be
processed in the right order to maintain the correct
context at each point.
With examples like these in mind, we have also
effected a complete redesign of the Regulus envi-
ronment?s regression testing facilities. A test suite
is now allowed to consist of a list of items of any
type ? text, wavfile, or non-speech input ? in any
order. Instead of trying to fit processing into the
constraints imposed by the batchrec utility, of-
fline processing now starts up speech resources in
the same way as the interactive environment, and
submits each item for appropriate processing in the
order in which it occurs. By adhering to the prin-
ciple that text and speech should be treated uni-
formly, we arrive at a framework which is simpler,
less error-prone (the underlying code is less frag-
ile) and above all much more flexible.
6 Summary and conclusions
The new functionality offered by the redesigned
Regulus top-level is not strikingly deep. In the
context of any given application, it could all
have been duplicated by reasonably simple scripts,
which linked together existing Regulus compo-
nents. Indeed, much of this new functionality is
implemented using code derived precisely from
such scripts. Our observation, however, has been
that few developers have actually taken the time
to write these scripts, and that when they have
been developed inside one project they have usu-
ally not migrated to other ones. One of the things
we have done, essentially, is to generalise previ-
ously ad hoc application-dependent functionality,
and make it part of the top-level development en-
vironment. The other main achievements of the
new Regulus top-level are to organise the existing
functionality in a more systematic way, so that it is
easier to find commands, and to package it all as a
normal-looking Swing-based GUI.
Although none of these items sound dramatic,
they make a large difference to the platform?s over-
14
all usability, and to the development cycle it sup-
ports. In effect, the Regulus top-level becomes
a generic speech-enabled application, into which
developers can plug their grammars, rule-sets and
derived components. Applications can be tested in
the speech view much earlier, giving a correspond-
ingly better chance of catching bad design deci-
sions before they become entrenched. The mecha-
nisms used to enable this functionality do not de-
pend on any special properties of Regulus, and
could readily be implemented in other grammar-
based development platforms, such as Gemini and
UNIANCE, which support compilation of feature
grammars into grammar-based language models.
At risk of stating the obvious, it is also worth
pointing out that many users, particularly younger
ones who have grown up using Windows and Mac
environments, expect as a matter of course that de-
velopment platforms will be GUI-based rather than
command-line. Addressing this issue, and sim-
plifying the transition between text- and speech-
based, views has the pleasant consequence of im-
proving Regulus as a vehicle for introducing lin-
guistics students to speech technology. An initial
Regulus-based course at the University of Santa
Cruz, focussing on spoken dialogue systems, is de-
scribed in (Hockey and Christian, 2008); a similar
one, but oriented towards speech translation and
using the new top-level described here, is currently
under way at the University of Geneva. We expect
to present this in detail in a later paper.
References
Alshawi, H., editor. 1992. The Core Language Engine.
MIT Press, Cambridge, Massachusetts.
Bos, J. 2002. Compilation of unification grammars
with compositional semantics to speech recognition
packages. In Proceedings of the 19th International
Conference on Computational Linguistics, Taipei,
Taiwan.
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Copestake, A. 2002. Implementing Typed Feature
Structure Grammars. CSLI Press, Chicago.
Crouch, R., M. Dalrymple, R. Kaplan, T. King,
J. Maxwell, and P. Newman, 2008. XLE Documenta-
tion. http://www2.parc.com/isl/groups/nltt/xle/doc.
As of 29 Apr 2008.
Dix, A., J.E. Finlay, G.D. Abowd, and R. Beale, edi-
tors. 1998. Human Computer Interaction. Second
ed. Prentice Hall, England.
Dowding, J., M. Gawron, D. Appelt, L. Cherny,
R. Moore, and D. Moran. 1993. Gemini: A natural
language system for spoken language understanding.
In Proceedings of the Thirty-First Annual Meeting of
the Association for Computational Linguistics.
Hockey, B.A. and G. Christian. 2008. Zero to spoken
dialogue system in one quarter: Teaching computa-
tional linguistics to linguists using regulus. In Pro-
ceedings of the Third ACL Workshop on Teaching
Computational Linguistics (TeachCL-08), Colum-
bus, OH.
Hockey, B.A. and D. Miller. 2007. A demonstration of
a conversationally guided smart wheelchair. In Pro-
ceedings of the 9th international ACM SIGACCESS
conference on Computers and accessibility, pages
243?244, Denver, CO.
Jacko, J.A. and A. Sears, editors. 2003. The
human-computer interaction handbook: Fundamen-
tals, evolving technologies and emerging applica-
tions. Lawerence Erlbaum Associates, Mahwah,
New Jersey.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Larsson, S. and D. Traum. 2000. Information state and
dialogue management in the TRINDI dialogue move
engine toolkit. Natural Language Engineering, Spe-
cial Issue on Best Practice in Spoken Language Di-
alogue Systems Engineering, pages 323?340.
Rayner, M., B.A. Hockey, J.M. Renders,
N. Chatzichrisafis, and K. Farrell. 2005. A
voice enabled procedure browser for the interna-
tional space station. In Proceedings of the 43rd
15
Annual Meeting of the Association for Compu-
tational Linguistics (interactive poster and demo
track), Ann Arbor, MI.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
16
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 109?116,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Two Approaches to Correcting Homophone Confusions
in a Hybrid Machine Translation System
Pierrette Bouillon1, Johanna Gerlach1, Ulrich Germann2, Barry Haddow2, Manny Rayner1
(1) FTI/TIM, University of Geneva, Switzerland
{Pierrette.Bouillon,Johanna.Gerlach,Emmanuel.Rayner}@unige.ch
(2) School of Informatics, University of Edinburgh, Scotland
{ugermann,bhaddow}@inf.ed.ac.uk
Abstract
In the context of a hybrid French-to-
English SMT system for translating on-
line forum posts, we present two meth-
ods for addressing the common problem
of homophone confusions in colloquial
written language. The first is based on
hand-coded rules; the second on weighted
graphs derived from a large-scale pro-
nunciation resource, with weights trained
from a small bicorpus of domain language.
With automatic evaluation, the weighted
graph method yields an improvement of
about +0.63 BLEU points, while the rule-
based method scores about the same as the
baseline. On contrastive manual evalua-
tion, both methods give highly significant
improvements (p < 0.0001) and score
about equally when compared against each
other.
1 Introduction and motivation
The data used to train Statistical Machine Transla-
tion (SMT) systems is most often taken from the
proceedings of large multilingual organisations,
the generic example being the Europarl corpus
(Koehn, 2005); for academic evaluation exercises,
the test data may well also be taken from the same
source. Texts of this kind are carefully cleaned-up
formal language. However, real MT systems of-
ten need to handle text from very different genres,
which as usual causes problems.
This paper addresses a problem common in do-
mains containing informally written text: spelling
errors based on homophone confusions. Con-
cretely, the work reported was carried out in the
context of the ACCEPT project, which deals with
the increasingly important topic of translating on-
line forum posts; the experiments we describe
were performed using French data taken from the
Symantec forum, the concrete task being to trans-
late it into English. The language in these posts is
very far from that which appears in Hansard. Peo-
ple write quickly and carelessly, and no attempt is
made to clean up the results. In particular, spelling
is often uncertain.
One of the particular challenges in the task
considered here is that French has a high fre-
quency of homophones, which often cause confu-
sion in written language. Everyone who speaks
English is familiar with the fact that careless writ-
ers may confuse its (?of or belonging to it?) and
it?s (contraction of ?it is? or ?it has?). French has
the same problem, but to a much greater degree.
Even when someone is working in an environment
where an online spell-checker is available, it is
easy to write ou (?or?) instead of ou` (?where?),
la (?the-feminine?) instead of la` (?there?) or ce
(?this?) instead of se (?him/herself?). Even worse,
there is systematic homophony in verb-form end-
ings: for example, utiliser (?to use?) utilisez (?you
use?) and utilise? (?used?) are all homophones.
In French posts from the Symantec forum, we
find that between 10% and 15% of all sentences
contain at least one homophone error, depending
on exactly how the term is defined1. Substituting
a word with an incorrect homophone will often re-
sult in a translation error. Figure 1 shows typical
examples of homophone errors and their effect on
translation.
The core translation engine in our application
is a normal SMT system, bracketed between pre-
and post-editing phases. In what follows, we con-
trast two different approaches to handling homo-
phone errors, which involve pre-editing in dif-
ferent ways. The first approach is based on
knowledge-intensive construction of regular ex-
pression rules, which use the surrounding context
to correct the most frequent types of homophone
1Unclear cases include hyphenation, elison and some ex-
amples of missing or incorrect accents.
109
source automatic translation
original La sa ne pose pas de proble`me ... The its is not the issue ...
corrected La` c?a ne pose pas de proble`me ... Here it is not a problem
original ... (du moins on ne recoit pas l?alerte). ... (at least we do not recoit alert).
corrected ... (du moins on ne rec?oit pas l?alerte). .. (at least it does not receive the alert).
Figure 1: Examples of homophone errors in French forum data, contrasting English translations produced
by the SMT engine from plain and corrected versions.
confusions.
The second is an engineering method: we use a
commercial pronunciation-generation tool to gen-
erate a homophone dictionary, then use this dictio-
nary to turn the input into a weighted graph where
each word is replaced by a weighted disjunction of
homophones. Related, though less elaborate, work
has been reported by Bertoldi et al (2010), who
address spelling errors using a character-level con-
fusion network based on common character con-
fusions in typed English and test them on artifi-
cially created noisy data. Formiga and Fonollosa
(2012) also used character-based models to correct
spelling on informally written English data.
The two approaches in the present paper ex-
ploit fundamentally different knowledge sources
in trying to identify and correct homophone er-
rors. The rule-based method relies exclusively
on source-side information, encoding patterns in-
dicative of common French homophone confu-
sions. The weighted graph method shifts the bal-
ance to the target side; the choice between poten-
tial homophone alternatives is made primarily by
the target language model, though the source lan-
guage weights and the translation model are also
involved.
The rest of the paper is organised as follows.
Section 2 describes the basic framework in more
detail, and Section 3 the experiments. Section 4
summarises and concludes.
2 Basic framework
The goal of the ACCEPT project is to provide
easy cross-lingual access to posts in online fo-
rums. Given the large variety of possible techni-
cal topics and the limited supply of online gurus,
it frequently happens that users, searching forum
posts online, find that the answer they need is in a
language they do not know.
Currently available tools, for example Google
Translate, are of course a great deal better than
nothing, but still leave much to be desired. When
one considers that advice given in an online fo-
rum may not be easy to follow even for native lan-
guage speakers, it is unsurprising that a Google-
translated version often fails to be useful. There is
consequently strong motivation to develop an in-
frastructure explicitly designed to produce high-
quality translations. ACCEPT intends to achieve
this by a combination of three technologies: pre-
editing of the source; domain-tuned SMT; and
post-editing of the target. The pre- and post-
editing stages are performed partly using auto-
matic tools, and partly by manual intervention on
the part of the user communities which typically
grow up around online forums. We now briefly
describe the automatic parts of the system.
2.1 SMT engine and corpus data
The SMT engine used is a phrase-based system
trained with the standard Moses pipeline (Koehn
et al, 2007), using GIZA++ (Och and Ney,
2000) for word alignment and SRILM (Stolcke,
2002) for the estimation of 5-gram Kneser-Ney
smoothed (Kneser and Ney, 1995) language mod-
els.
For training the translation and lexicalised re-
ordering models we used the releases of europarl
and news-commentary provided for the WMT12
shared task (Callison-Burch et al, 2012), together
with a dataset from the ACCEPT project consist-
ing mainly of technical product manuals and mar-
keting materials.
For language modelling we used the target sides
of all the parallel data, together with approx-
imately 900 000 words of monolingual English
data extracted from web forums of the type that
we wish to translate. Separate language models
were trained on each of the data sets, then these
were linearly interpolated using SRILM to min-
imise perplexity on a heldout portion of the forum
data.
110
For tuning and testing, we extracted 1022 sen-
tences randomly from a collection of monolin-
gual French Symantec forum data (distinct from
the monolingual English forum data), translated
these using Google Translate, then post-edited
to create references. The post-editing was per-
formed by a native English speaker, who is also
fluent in French. This 1022-sentence parallel text
was then split into two equal halves (devtest a
and devtest b) for minimum error rate tuning
(MERT) and testing, respectively.
2.2 Rule-based pre-editing engine
Rule-based processing is carried out using the
Acrolinx engine (Bredenkamp et al, 2000), which
supports spelling, grammar, style and terminology
checking. These methods of pre-editing were orig-
inally designed to be applied by authors during the
technical documentation authoring process. The
author gets error markings and improvement sug-
gestions, and decides about reformulations. It is
also possible to apply the provided suggestions
automatically as direct reformulations. Rules are
written in a regular-expression-based formalism
which can access tagger-generated part-of-speech
information. The rule-writer can specify both pos-
itive evidence (patterns that will trigger applica-
tion of the rule) and negative evidence (patterns
that will block application).
3 Experiments
We compared the rule-based and weighted graph
approaches, evaluating each of them on the 511
sentence devtest b corpus. The baseline SMT
system, with no pre-editing, achieves an average
BLEU score of 42.47 on this set.
3.1 The rule-based approach
Under the ACCEPT project, a set of lightweight
pre-editing rules have been developed specifically
for the Symantec Forum translation task. Some
of the rules are automatic (direct reformulations);
others present the user with a set of suggestions.
The evaluations described in Gerlach et al (2013)
demonstrate that pre-editing with the rules has a
significant positive effect on the quality of SMT-
based translation.
The implemented rules address four main phe-
nomena: differences between informal and for-
mal language (Rayner et al, 2012), differences
between local French and English word-order, el-
lision/punctuation, and word confusions. Rules
for resolving homophone confusions belong to the
fourth group. They are shown in Table 1, together
with approximate frequencies of occurrence in the
development corpus.
Table 1: Hand-coded rules for homophone confu-
sions and per-sentence frequency of applicability
in the development corpus. Some of the rules also
cover non-homophone errors, so the frequency fig-
ures are slight overestimates as far as homophones
are concerned.
Rule Freq.
a/as/a` 4.17%
noun phrase agreement 3.20%
incorrect verb ending (er/e?/ez) 2.90%
missing hyphenation 2.08%
subject verb agreement 1.90%
missing elision 1.26%
du/du? 0.35%
la/la` 0.32%
ou/ou` 0.28%
ce/se 0.27%
Verb/noun 0.23%
tous/tout 0.22%
indicative/imperative 0.19%
future/conditional tense 0.14%
sur/su?r 0.10%
quel que/quelque 0.08%
ma/m?a 0.06%
quelle/qu?elle/quel/quels 0.05%
c?a/sa 0.04%
des/de`s 0.04%
et/est 0.02%
ci/si 0.01%
m?y/mi/mis 0.01%
other 0.17%
Total 18.09%
The set of Acrolinx pre-editing rules potentially
relevant to resolution of homophone errors was
applied to the devtest b set test corpus (Sec-
tion 2.1). In order to be able to make a fair com-
parison with the weighted-graph method, we only
used rules with a unique suggestion, which could
be run automatically. Applying these rules pro-
duced 430 changed words in the test corpus, but
did not change the average BLEU score signifi-
cantly (42.38).
Corrections made with a human in the loop,
used as ?oracle? input for the SMT system, by the
111
way, achieve an average BLEU score2 of 43.11 ?
roughly on par with the weighted-graph approach
described below.
3.2 The weighted graph approach
In our second approach, the basic idea is to trans-
form the input sentence into a confusion network
(Bertoldi et al, 2008) which presents the trans-
lation system with a weighted list of homophone
alternatives for each input word. The system is
free to choose a path through a network of words
that optimizes the internal hypothesis score; the
weighting scheme for the alternatives can be used
to guide the decoder. The conjecture is that the
combination of the confusion network weights, the
translation model and the target language model
can resolve homophone confusions.
3.2.1 Defining sets of confusable words
To compile lists of homophones, we used the com-
mercial Nuance Toolkit pronounce utility as
our source of French pronunciation information.
We began by extracting a list of all the lexical
items which occurred in the training portion of
the French Symantec forum data, giving us 30 565
words. We then ran pronounce over this list.
The Nuance utility does not simply perform table
lookups, but is capable of creating pronunciations
on the fly; it could in particular assign plausible
pronunciations to most of the misspellings that oc-
curred in the corpus. In general, a word is given
more than one possible pronunciation. This can be
for several reasons; in particular, some sounds in
French can systematically be pronounced in more
than one way, and pronunciation is often also de-
pendent on whether the word is followed by a con-
sonant or vowel. Table 2 shows examples.
Using the data taken from pronounce, we
grouped words together into clusters which have
a common pronunciation; since words typically
have more than one pronunciation, they will typi-
cally also belong to more than one cluster. We then
contructed sets of possible alternatives for words
by including, for each word W , all the words W ?
such that W and W ? occurred in the same cluster;
since careless French writing is also characterised
by mistakes in placing accents, we added all words
W ? such that W and W ? are identical up to drop-
ping accents. Table 3 shows typical results.
2With parameter sets from tuning the system on raw in-
put and input preprocessed with the fully automatic rules; cf.
Sec. 3.3.
Word Pronunciation
ans A?
A?z
pre?vu p r E v y
p r e v y
que?bec k e b E k
roule r u l
r u l *
Table 2: Examples of French pronunciations gen-
erated by pronounce. The format used is the
Nuance version of ARPABET.
Intuitively, it is in general unlikely that, on see-
ing a word which occurs frequently in the corpus,
we will want to hypothesize that it may be a mis-
spelling of one which occurs very infrequently.
We consequently filtered the sets of alternatives
to remove all words on the right whose frequency
was less than 0.05 times that of the word on the
left.
Table 3: Examples of sets of possible alternatives
for words, generated by considering both homo-
phone and accent confusions.
Word Alternatives
aux au aux haut
cre?er cre?er cre?ez cre?e? cre?e?e cre?e?es cre?e?s
co?te cote cote? co?te co?te? quot quote
ho?te haut haute ho?te ho?tes
il e elle elles il ils l le y
me`ne main mene? me`ne
nom nom noms non
ou ou ou`
saine sain saine saines sce`ne seine
traits trait traits tray tre tres tre`s
3.2.2 Setting confusion network weights
In a small series of preliminary experiments we
first tested three na??ve weighting schemes for the
confusion networks.
? using a uniform distribution that assigns
equal weight to all spelling alternatives;
? setting weights proportional to the unigram
probability of the word in question;
? computing the weights as state probabilities
in a trellis with the forward-backward algo-
rithm (Rabiner, 1989), an algorithm widely
112
Table 4: Decoder performance with different con-
fusion network weighting schemes.
weighting scheme av. BLEUa std.
none (baseline system) 42.47 ? .22
uniform 41.50 ? .37
unigram 41.58 ? .26
fwd-bwd (bigram) 41.81 ? .16
bigram context
(interpolated)
43.10 ? .32
aBased on muliple tuning runs with random parameter ini-
tializations.
used in speech recognition. Suppose that
each word w?i in the observed translation in-
put sentence is produced while the writer has
a particular ?true? word wi ? Ci in mind,
where Ci is the set of words confusable with
w?i. For the sake of simplicity, we assume that
within a confusion set, all ?true word? op-
tions are equally likely, i.e., p(w?i |wi = x) =
1
|Ci| for x ? Ci. The writer chooses the next
word wi+1 according to the conditional word
bigram probability p(wi+1 |wi).
The forward probability fwd i(x) is the prob-
ability of arriving in state wi = x at time
i, regardless of the sequence of states visited
en-route; the backward probability bwd i(x)
is the probability of arriving at the end of the
sentence coming from state wi = x, regard-
less of the path taken. These probabilities can
be computed efficiently with dynamic pro-
gramming.
The weight assigned to a particular ho-
mophone alternative x at position i in the
confusion network is the joint forward and
backward probability:
weight i(x) = fwd i(x) ? bwd i(x).
In practice, it turns out that these three na??ve
weighting schemes do more harm than good, as
the results in Table 4 show. Clearly, they rely too
much on overall language statistics (unigram and
bigram probabilities) and pay too little attention to
the actual input.
We therefore designed a fourth weighting
scheme (?bigram context interpolated?) that
gives more weight to the observed input and com-
putes the weights as the average of two score com-
ponents. The first is a binary feature function
that assigns 1 to each word actually observed in
the input, and 0 to its homophone alternatives.
The second component is the bigram-based in-
context probability of each candidate. Unlike the
forward-backward weighting scheme, which con-
siders all possible context words for each candi-
date (as specified in the respective confusion sets),
the new scheme only considers the words in the
actual input as context words.
It would have be desirable to keep the two score
components separate and tune their weights to-
gether with all the other parameters of the SMT
system. Unfortunately, the current implementa-
tion of confusion network-based decoding in the
Moses decoder allows only one single weight in
the specification of confusion networks, so that we
had to combine the two components into one score
before feeding the confusion network into the de-
coder.
With the improved weighting scheme, the con-
fusion network approach does outperform the
baseline system, giving an average BLEU of 43.10
(+0.63).
3.3 Automatic evaluation (BLEU)
Due to the relatively small size of the evalua-
tion set and instability inherent in minimum error
rate training (Foster and Kuhn, 2009; Clark et al,
2011), results of individual tuning and evaluation
runs can be unreliable. We therefore preformed
multiple tuning and evaluation runs for each sys-
tem (baseline, rule-based and weighted graph). To
illustrate the precision of the BLEU score on our
data sets, we plot in Fig. 2 for each individual tun-
ing run the BLEU score achieved on the tuning
set (x-axis) against the performance on the evalua-
tion set (y-axis). The variance along the x-axis for
each system is due to search errors in parameter
optimization. Since the search space is not con-
vex, the tuning process can get stuck in local max-
ima. The apparent poor local correlation between
performance on the tuning set and performance on
the evaluation set for each system shows the effect
of the sampling error.
With larger tuning and evaluation sets, we
would expect the correlation between the two
to improve. The scatter plot suggests that the
weighted-graph system does on average produce
significantly better translations (with respect to
BLEU) than both the baseline and the rule-based
system, whereas the difference between the base-
line and the rule-based system is within the range
113
41.6
41.8
42
42.2
42.4
42.6
42.8
43
43.2
43.4
43.6
43.8
46.4 46.6 46.8 47 47.2 47.4 47.6 47.8
B
LE
U
sc
o
re
o
n
ev
al
u
at
io
n
se
t
BLEU score on tuning set
BLEU scores on evaluation set
? ? .95 conf. int.
? baseline 42.47 .22 42.04?42.89
? rule-based 42.38 .23 41.94?42.83
+ weighted graph 43.10 .32 42.48?43.72
Figure 2: BLEU scores (in points) for the baseline, rule-based and weighted graph-based systems.
of statistical error.
To study the effect of tuning condition (tun-
ing on raw vs. input pre-processed by rules), we
also translated both the raw and the pre-processed
evaluation corpus with all parameter setting that
we had obtained during the various experiments.
Figure 3 plots (with solid markers) performance
on raw input (x-axis) against translation of pre-
processed input (y-axis). We observe that while
preprocessing harms performance for certain pa-
rameter settings, most of the time proprocessing
does lead to improvements in BLEU score. The
slight deterioration we observed when comparing
system tuned on exactly the type of input that they
were to translate later (i.e., raw or preprocessed)
seems to be a imprecision in the measurement
caused by training instability and sampling error
rather than the result of systematic input deterio-
ration due to preprocessing. Overall, the improve-
ments are small and not statistically significant,
but there appears to be a positive trend.
To gauge the benefits of more extensive pre-
processing and input error correction we produced
and translated ?oracle? input by also applying rules
from the Acrolinx engine that currently require a
human in the loop who decides whether or not the
rule in question should be applied. The boost in
performance is shown by the hollow markers in
Fig. 3. Here, translation of pre-processed input
consistently fares better than translation of the raw
input.
3.4 Human evaluation
Although BLEU suggests that the weighted-graph
method significantly outscores both the baseline
and the rule-based method (p < 0.05 over 25 tun-
ing runs), the absolute differences are small, and
we decided that it would be prudent to carry out a
human evaluation as well. Following the method-
ology of Rayner et al (2012), we performed con-
trastive judging on the Amazon Mechanical Turk
(AMT) to compare different versions of the sys-
tem. Subjects were recruited from Canada, a bilin-
gual French/English country, requesting English
native speakers with good written French; we also
limited the call to AMT workers who had already
completed at least 50 assignments, at least 80%
of which had been accepted. Judging assignments
were split into groups of 20 triplets, where each
triplet consisted of a source sentence and two dif-
ferent target sentences; the judge was asked to
say which translation was better, using a five-point
scale {better, slightly-better, about-equal, slightly-
worse, worse}. The order of the two targets was
114
 41.5
 42
 42.5
 43
 43.5
 41.6  41.8  42  42.2  42.4  42.6  42.8  43
BL
EU
 s
co
re
 o
n 
in
pu
t p
re
pr
oc
es
se
d 
by
 ru
le
s 
/ o
ra
cle
 in
pu
t
BLEU score on raw baseline input
baseline vs. oracle input; system tuned on baseline input
baseline vs. oracle input; system tuned on preprocessed input
baseline vs. rule-processed input; system tuned on baseline input
baseline vs. rule-processed input; system tuned on preprocessed input
threshold for improvement (above this line) vs. deterioration (below)
Figure 3: BLEU scores (in points) the two input conditions ?baseline? and ?rule-based? (solid markers).
The hollow markers show the BLEU score on human-corrected ?oracle? input using a more extensive set
of rules / suggestions from the Acrolinx engine that require a human in the loop.
randomised. Judges were paid $1 for each group
of 20 triplets. Each triplet was judged three times.
Using the above method, we posted AMT tasks
Table 5: Comparison between baseline, rule-based
and weighted-graph versions, evaluated on the
511-utterance devtest b corpus and judged by
three AMT-recruited judges. Figures are presented
both for majority voting and for unanimous deci-
sions only.
Majority Unanimous
baseline vs rule-based
baseline better 83 16.2% 48 9.4%
r-based better 204 40.0% 161 31.5%
Unclear 36 7.0% 93 18.1%
Equal 188 36.8% 209 40.9%
baseline vs weighted-graph
baseline better 115 22.5% 52 10.1%
w-graph better 193 37.8% 119 23.3%
Unclear 46 9.0% 99 19.4%
Equal 157 30.7% 241 47.2%
rule-based vs weighted-graph
r-based better 141 27.6% 68 13.3%
w-graph better 123 24.1% 70 13.7%
Unclear 25 4.9% 142 27.8%
Equal 222 43.4% 231 45.2%
to compare a) the baseline system against the
rule-based system, b) the baseline system against
the best weighted-graph system (interpolated-
bigram) from Section 3.2.2 and c) the rule-
based system and the weighted-graph system
against each other. The results are shown in
Table 5; in the second and third columns, dis-
agreements are resolved by majority voting, and
in the fourth and fifth we only count cases
where the judges are unanimous, the others be-
ing scored as unclear. In both cases, we re-
duce the original five-point scale to a three-point
scale {better, equal/unclear, worse}3. Irrespec-
tive of the method used to resolve disagreements,
the differences ?rule-based system/baseline? and
?weighted-graph system/baseline? are highly sig-
nificant (p < 0.0001) according to the McNe-
mar sign test, while the difference ?rule-based
system/weighted-graph system? is not significant.
We were somewhat puzzled that BLEU makes
the weighted-graph system clearly better than the
rule-based one, while manual evaluation rates
them as approximately equal. The explanation
seems to be to do with the fact that manual evalu-
ation operates at the sentence level, giving equal
importance to all sentences, while BLEU oper-
3For reasons we do not fully understand, we get better
inter-judge agreement this way than we do when we origi-
nally ask for judgements on a three-point scale.
115
ates at the word level and consequently counts
longer sentences as more important. If we calcu-
late BLEU on a per-sentence basis and then av-
erage the scores, we find that the results for the
two systems are nearly the same; per-sentence
BLEU differences also correlate reasonably well
with majority judgements (Pearson correlation co-
efficient of 0.39). It is unclear to us, however,
whether the difference between per-sentence and
per-word BLEU evaluation points to anything par-
ticularly interesting.
4 Conclusions
We have presented two methods for addressing
the common problem of homophone confusions in
colloquial written language in the context of an
SMT system. The weighted-graph method pro-
duced a small but significant increase in BLEU,
while the rule-based one was about the same as
the baseline. Both methods, however, gave clearly
significant improvements on contrastive manual
evaluation carried out through AMT, with no sig-
nificant difference in performance when the two
were compared directly.
The small but consistent improvements in
BLEU score that we observed with the human-
in-the-loop oracle input over the fully automatic
rule-based setup invite further investigation. How
many of the decisions currently left to the hu-
man can be automated? Is there a fair way of
comparing and evaluating fully automatic against
semi-automatic setups? Work on these topics is in
preparation and will be reported elsewhere.
Acknowledgements
The work described in this paper was performed
as part of the Seventh Framework Programme AC-
CEPT project, under grant agreement 288769.
References
Bertoldi, Nicola, Mauro Cettolo, and Marcello
Federico. 2010. ?Statistical machine translation
of texts with misspelled words.? NAACL. Los
Angeles, CA, USA.
Bertoldi, Nicola, Richard Zens, Marcello Fed-
erico, and Wade Shen. 2008. ?Efficient speech
translation through confusion network decod-
ing.? IEEE Transactions on Audio, Speech &
Language Processing, 16(8):1696?1705.
Bredenkamp, Andrew, Berthold Crysmann, and
Mirela Petrea. 2000. ?Looking for errors : A
declarative formalism for resource-adaptive lan-
guage checking.? LREC. Athens, Greece.
Callison-Burch, Chris, Philipp Koehn, Christof
Monz, et al (eds.). 2012. Seventh Workshop
on Statistical Machine Translation (WMT).
Montre?al, Canada.
Clark, Jonathan H., Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. ?Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability.? ACL-HLT. Port-
land, OR, USA.
Formiga, Lluis and Jose? A. R. Fonollosa. 2012.
?Dealing with input noise in statistical machine
translation.? COLING. Mumbai, India.
Foster, George and Roland Kuhn. 2009. ?Sta-
bilizing minimum error rate training.? WMT.
Athens, Greece.
Gerlach, Johanna, Victoria Porro, Pierrette Bouil-
lon, and Sabine Lehmann. 2013. ?La pre?-
e?dition avec des re`gles peu cou?teuses, utile pour
la TA statistique?? TALN-RECITAL. Sables
d?Olonne, France.
Kneser, Reinhard and Hermann Ney. 1995. ?Im-
proved backing-off for m-gram language mod-
eling.? ICASSP. Detroit, MI, USA.
Koehn, Philipp. 2005. ?Europarl: A parallel cor-
pus for statistical machine translation.? MT
Summit X. Phuket, Thailand.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
et al 2007. ?Moses: Open source toolkit for
statistical machine translation.? ACL Demon-
stration Session. Prague, Czech Republic.
Och, Franz Josef and Hermann Ney. 2000. ?Im-
proved statistical alignment models.? ACL.
Hong Kong.
Rabiner, Lawrence R. 1989. ?A tutorial on hid-
den markov models and selected applications in
speech recognition.? Proceedings of the IEEE,
257?286.
Rayner, Manny, Pierrette Bouillon, and Barry
Haddow. 2012. ?Using source-language trans-
formations to address register mismatches in
SMT.? AMTA. San Diego, CA, USA.
Stolcke, Andreas. 2002. ?SRILM - an extensi-
ble language modeling toolkit.? ICSLP. Denver,
CO, USA.
116
Workshop on Humans and Computer-assisted Translation, pages 66?71,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
The ACCEPT Portal: An Online Framework for the Pre-editing and
Post-editing of User-Generated Content
Violeta Seretan
FTI/TIM
University of Geneva
Switzerland
Violeta.Seretan@unige.ch
Johann Roturier
Symantec Ltd.
Dublin, Ireland
johann roturier@symantec.com
David Silva
Symantec Ltd.
Dublin, Ireland
David Silva@symantec.com
Pierrette Bouillon
FTI/TIM
University of Geneva
Switzerland
Pierrette.Bouillon@unige.ch
Abstract
With the development of Web 2.0, a
lot of content is nowadays generated on-
line by users. Due to its characteristics
(e.g., use of jargon and abbreviations, ty-
pos, grammatical and style errors), the
user-generated content poses specific chal-
lenges to machine translation. This pa-
per presents an online platform devoted to
the pre-editing of user-generated content
and its post-editing, two main types of hu-
man assistance strategies which are com-
bined with domain adaptation and other
techniques in order to improve the trans-
lation of this type of content. The plat-
form has recently been released publicly
and is being tested by two main types of
user communities, namely, technical fo-
rum users and volunteer translators.
1 Introduction
User-generated content ? i.e., information posted
by Internet users in social communication chan-
nels like blogs, forum posts, social networks ? is
one of the main sources of information available
today. Huge volumes of such content are created
each day, reach a very broad audience instantly.
1
The democratisation of content creation due
to the emergence of the Web 2.0 paradigm also
means a diversification of the languages used on
the Internet.
2
Despite its availability, the new con-
tent is only accessible to the speakers of the lan-
guage in which it was created. The automatic
translation of user-generated content is therefore
one of the key issues to be addressed in the field of
human language technologies. However, as stated
1
For instance, 58 million tweets are sent on aver-
age per day (http://www.statisticbrain.com/
twitter-statistics/).
2
See http://en.wikipedia.org/wiki/
Languages_used_on_the_Internet for statistics.
by Jiang et al. (2012), despite the obvious bene-
fits, there are relatively little attempts at translating
user-generated content.
The reason may lie in the fact that user-ge-
nerated content is very challenging for machine
translation. As shown, among others, by Nagara-
jan and Gamon (2011), there are several charac-
teristics of this content that pose new process-
ing challenges with respect to traditional content:
informal style, slang, abbreviations, specific ter-
minology, irregular grammar and spelling. In-
deed, Internet users are rarely professional writ-
ers.
3
They often write in a language which is not
their own, and sacrifice quality for speed, not pay-
ing attention to spelling, punctuation, or grammar
rules.
The ACCEPT project
4
addresses these chal-
lenges by developing a technology integrating
modules for automatic and manual content pre-
editing, statistical machine translation, as well
as output evaluation and post-editing. Thus, the
project aims to improve the translation of user-ge-
nerated content by proposing a full workflow, in
which the participation of humans is essential.
The application scenario considered in the
project are user communities sharing specific in-
formation on a given topic. The project focuses,
more specifically, on the following use cases:
1. the commercial use case, in which the tar-
get community is the user community built
around a software company in order for
members to help each other with issues re-
lated to products;
2. the NGO use case, in which non-go-
vernmental organisations such as Doctors
Without Borders produce health-care content
for distributions in areas of need.
3
Even when they are, as in the case of government agen-
cies, the type of content produced (e.g., tweets) still poses
?multiple challenges? to translation (Gotti et al., 2013).
4
http://www.accept-project.eu/
66
The language pairs considered in the project are
English to French, German and Japanese, as well
as French into English for the first use case (in-
volving technical forum information), and French
to and from English for the second use case (in-
volving healthcare information).
Past halfway into its research program, the
project has accomplished significant progress in
the main areas mentioned above (pre-editing, sta-
tistical machine translation, post-editing, and eval-
uation). The ACCEPT technology has recently
been released to the broad public as an on-
line framework, which demonstrates the different
modules of the workflow and provides access to
associated software components (plug-ins, APIs),
as well as to documentation. The pre-editing tech-
nology has been deployed on the targeted user fo-
rum
5
, allowing users to check their messages be-
fore posting them. The post-editing technology is
being used by a community of translators, which
provide pro-bono translation services to the NGOs
considered in our second use case.
In this paper, we describe the framework by pre-
senting its architecture and main modules (Sec-
tion 2). We discuss related work in Section 3 and
conclude in Section 4.
2 The Framework
The ACCEPT technology has been made acces-
sible to a broad audience in the form of an on-
line framework, i.e., an integrated environment
where registered users can perform pre-editing,
post-editing and evaluation work. The framework
? henceforth, the ACCEPT Portal ? is hosted on a
cloud computing infrastructure and is available at
www.accept-portal.eu.
2.1 Architecture of the Framework
As explained in Section 1, the ACCEPT techno-
logy consists of the following main modules:
1. Pre-editing module;
2. Machine translation module,
3. Post-editing module,
4. Evaluation module.
The typical workflow is incremental, but the
modules are independent. They can be used both
within and outside the portal, as they are built on a
REST API facilitating integration.
5
https://community.norton.com/
In the remaining of this section, we introduce
each of the framework modules.
6
2.2 Pre-editing Module
The pre-editing module leverages existing ling-
ware which provides authoring support rules
aimed at language professionals, by relying on
shallow language processing (Bredenkamp et al.,
2000). The existing English checker and the lin-
guistic resources on which it relies have been ex-
tended and adapted to suit the type of data gener-
ated by community users. In particular, the soft-
ware extension consisted of designing a number
of pre-editing rules aimed at source normalisation,
for the purpose of making the input text easier
to handle by the SMT systems. In the case of
French, the pre-editing rules have been designed
from scratch. The pre-editing rules pertain to the
levels of spelling, grammar, style and terminology.
They are defined using the original lingware?s rule
formalism and are incorporated into a server dedi-
cated to the project.
The rule development was corpus-driven and
was performed on data collected for this purpose.
A stable set of pre-edition rules is available in
the portal for each of the domains and source
languages considered (i.e., technical forum and
heathcare data in English and French). The rules
are described in detail in the project deliverable
D 2.2 (2013).
The rules proposed have been evaluated individ-
ually and in combination (Roturier et al., 2012;
Gerlach et al., 2013; Seretan et al., 2014). As
a general observation, it is important to notice
that, for SMT, the improvement of the input text
does not go hand in hand with the improvement of
translation. For example, in French the rule for
correcting verbal forms to the subjunctive tense
had a negative impact since the subjunctive is not
frequent in the training data. Conversely, it was
possible to define lexical reformulations which de-
graded the quality of the input text, but had a po-
sitive impact on translation quality.
The combined impact of the rule applica-
tion was measured in a variety of settings in a
large-scale evaluation campaign involving transla-
tion students (Seretan et al., 2014). As the rules
are divided into two major groups, those automati-
cally applicable and those requiring human inter-
6
The MT module will be omitted, as it is not part of the
portal. The interested reader is referred to D 4.2 (2013).
67
Figure 1: The ACCEPT Pre-edit plug-in in action (screen capture)
vention, the evaluation was carried out for the full
set of rules, as well as for the automatic rules only.
In addition, the evaluation was performed in both
a monolingual and a bilingual setting, i.e., with
the evaluators having or not access to the source
text, and it involved evaluation scales of different
granularities. The evaluation results showed a sys-
tematic statistically significant improvement over
the baseline when pre-editing is performed on the
source content. More details about the evalua-
tion methodology and results can be found in the
project deliverable D 9.2.2 (2013).
A data excerpt illustrating the impact of pre-
editing on translation quality is presented in Ex-
ample 1 below. The simple correction of an ac-
cented letter, du? d?u, leads to the change of seve-
ral target words, and to a much better translation of
the input sentence.
1. a) Source (original):
J?ai du m?absenter hier apr`es midi.
b) Source (pre-edited):
J?ai d?u m?absenter hier apr
`es midi.
c) Target (original):
I have the leave me yesterday afternoon.
d) Target (pre-edited):
I had to leave yesterday afternoon.
The pre-editing component of the ACCEPT
technology is available as a JQuery plug-in, which
can be downloaded and installed by Web applica-
tion owners, so that it can be used with text areas
and other text-bearing elements. APIs and ac-
companying documentation have also been made
available, so that the pre-editing rules can be
leveraged in automatic steps, without the plug-in,
across devices and platforms. A demo site illus-
trating the use of the plug-in in a TinyMCE envi-
ronment is available on the portal (see Figure 1).
The latest developments of the pre-editing mo-
dule include the possibility for users to customise
the application of rule sets, in particular, to ignore
specific rules and to manage their own dictionary,
in order to prevent the activation of checking flags.
2.3 Post-editing Module
The post-editing module of the framework (see
also Roturier et al., (2013)) is designed to fulfil
the project?s objective of collecting post-editing
data in order to learn correction rules and, through
feedback loops, to integrate them into the SMT
engines (with the goal of automating corrections
whenever possible). The project relies on the par-
ticipation of volunteer community members, who
are subject matter experts, native speakers of the
68
Figure 2: The ACCEPT Portal showing the post-editing demo (screen capture)
target language and, possibly, of the source lan-
guage. Accordingly, the post-editing environment
(see Figure 2) provides functionalities for both
monolingual and bilingual post-editing.
The post-editing text is organised in tasks be-
longing to post-editing projects. The latter are
created and managed by project administrators,
by defining the project settings (e.g., source and
target languages, monolingual or bilingual mode,
collaborative or non-collaborative type
7
), upload-
ing the text for each task
8
, inviting participants by
e-mail, and monitoring revision progress.
The post-editors edit the target text in a
sentence-by-sentence fashion. They have access
to the task guidelines and to help documentation.
The interface of the post-editing window displays
the whole text, through which they can navigate
with next-previous buttons or by clicking on a
specific sentence. Users can check the text they
are editing by accessing, with a button, the con-
tent checking technology described in Section 2.2.
Their actions ? in terms of keystrokes and usage
7
In a collaborative editing scenario, users may see edits
from other users and do not have to repeat them when work-
ing on the same project task. Conflicts are avoided by pre-
venting concurrent access.
8
Currently, the JSON format is used for the input data.
of translation options ? and time spent editing are
recorded in the portal.
9
When they are done edi-
ting, they can click on a button marking the com-
pletion of the task. At any time, they can interrupt
their work and save their results for later.
Users can enter a comment on the post-editing
task they have performed. The feedback elicited
from users include the difficulty of the task and
their sentiment (Was it easy to post-edit? Did you
enjoy the post-editing task?). For systematically
collecting user feedback, the project administra-
tors can specify on the project configuration page
a link to a post-task survey, which will be sent to
users after completing their tasks.
The post-editing module includes a JQuery
plug-in for deployment in any Web-based envi-
ronment; a dedicated section of the portal; APIs
enabling the use of the post-editing functionality
outside the portal; and sample evaluation projects
for several language pairs.
The post-editing technology has been exten-
sively used in specific post-editing campaigns in-
volving translator volunteers and Amazon Me-
chanical Turk
10
workers. The campaigns, includ-
9
The post-editing data is exported in XLIFF format.
10
The integration was done via the ACCEPT API.
69
ing reports on post-task surveys, are documented
inter alia in deliverable D 8.1.2 (2013). A notable
finding was that professional translators, who were
reticent towards MT before the task, had a more
positive sentiment after post-editing and their mo-
tivation to post-edit in the future increased.
2.4 Evaluation Module
The role of the evaluation module is to support the
collection of user ratings for assessing the quality
of source, machine-translated and post-edited con-
tent, and, ultimately, to support the development
of the technology created in the project.
This module groups several software compo-
nents: an evaluation environment available as a
section of the portal; APIs enabling the collection
of user evaluations in-context; and a third com-
ponent which is a customisation of the Appraise
toolkit for the collaborative collection of human
judgements (Federmann, 2012).
As in the case of post-editing module, this mod-
ule provides functionality for creating and man-
aging projects. Using the evaluation environ-
ment/APIs, project creators can define question
categories, add questions and possible answers,
and upload evaluation data (in JSON format). For
traditional evaluation projects, the Appraise sys-
tem is used instead.
3 Related Work
Transforming the source text in order to better
fit the needs of machine translation is a well-
investigated area of research. Strategies like
source control, source re-ordering, or source sim-
plification at the lexical or structural level have
been largely explored; for reviews, see, for in-
stance, Huhn (2013), Kazemi (2013), and Feng
(2008), respectively.
User-generated content has been investigated
in the context of machine translation in recent
work dealing specifically with spelling correc-
tion (Bertoldi et al., 2010; Formiga and Fonol-
losa, 2012); lexical normalisation by substituting
ill-formed words with their correct counterpart,
e.g., makn ? making (Han and Baldwin, 2011);
missing word ? e.g., zero-pronoun ? recovery and
punctuation correction (Wang and Ng, 2013).
Rather than focusing on specific phenomena or
Web genres (i.e., tweets), we adopt a more gen-
eral approach in which we address the problem of
source normalisation at multiple levels ? punctua-
tion, spelling, grammar, and style ? for any type of
linguistically imperfect text.
Another peculiarity of our approach is that it
is rule-based and does not require parallel data
for learning corrections. In exchange, a limi-
tation of our pre-editing approach is that it is
language-dependent, as the underlying technology
is based on shallow analysis and is therefore time-
expensive to extend to a new language.
The post-editing technology differs from exist-
ing (standalone or Web-based) dedicated tools ?
e.g., iOmegaT
11
or MateCat
12
? in that it is tai-
lored to community users, and, consequently, it
is lighter, it generates more concise reports, and
a simpler interface replaces the grid-like format
for presenting data. Another specificity is that it
is sufficiently flexible to be used in other environ-
ments (e.g., Amazon Mechanical Turk, cf. ?2.3).
4 Conclusion
The technology outlined in this paper demon-
strates a specific case of human-computer interac-
tion, in which, for the first time, several modules
are integrated in a full process in which human
pre-editors, post-editors and evaluators play a key
role for improving the translation of community
content. The technology is freely accessible in the
online portal, has been deployed on a major user
forum, and can be downloaded for integration in
other Web-based environments. Since it is built on
top of a REST API, it is portable across devices
and platforms. The technology would be useful to
anyone who needs information instantly and relia-
bly translated, despite linguistic imperfections.
One of the main future developments concerns
the further improvement of SMT, by exploring,
in particular, the use of text analytics and senti-
ment detection. In addition, by incorporating post-
editing rules and developing techniques to change
the phrase table and system parameters dynam-
ically, it will be possible to reduce the amount
of error corrections that human post-editors have
to perform repeatedly. Another major develop-
ment (joint work with the CASMACAT European
project) will focus on novel types of assistance for
translators, aimed specifically at helping transla-
tors by identifying problematic parts of the ma-
chine translation output and signalling the para-
phrases that are more likely to be useful.
11
http://try-and-see-mt.org/
12
http://www.matecat.com/
70
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement n
o
288769.
References
Nicola Bertoldi, Mauro Cettolo, and Marcello Fe-
derico. 2010. Statistical machine translation of texts
with misspelled words. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 412?419, Los Angeles,
California.
Andrew Bredenkamp, Berthold Crysmann, and Mirela
Petrea. 2000. Looking for errors: A declarative for-
malism for resource-adaptive language checking. In
Proceedings of the Second International Conference
on Language Resources and Evaluation, Athens,
Greece.
2013. ACCEPT deliverable D 2.2 Definition of
pre-editing rules for English and French (final
version). http://www.accept.unige.
ch/Products/D2_2_Definition_of_
Pre-Editing_Rules_for_English_and_
French_with_appendixes.pdf.
2013. ACCEPT deliverable D 9.2.2: Survey of
evaluation results. http://www.accept.
unige.ch/Products/D_9_2_Survey_of_
evaluation_results.pdf.
2013. ACCEPT deliverable D 4.2 Report on
robust machine translation: domain adap-
tation and linguistic back-off. http:
//www.accept.unige.ch/Products/
D_4_2_Report_on_robust_machine_
translation_domain_adaptation_and_
linguistic_back-off.pdf.
2013. ACCEPT deliverable D 8.1.2 Data and
report from user studies - Year 2. http:
//www.accept.unige.ch/Products/D_
8_1_2_Data_and_report_from_user_
studies_-_Year_2.pdf.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics (PBML), 98:25?35.
Lijun Feng. 2008. Text simplification: A survey.
Technical report, CUNY.
Llu??s Formiga and Jos?e A. R. Fonollosa. 2012. Dea-
ling with input noise in statistical machine transla-
tion. In Proceedings of COLING 2012: Posters,
pages 319?328, Mumbai, India.
Johanna Gerlach, Victoria Porro, Pierrette Bouillon,
and Sabine Lehmann. 2013. La pr?e?edition avec
des r`egles peu co?uteuses, utile pour la TA statistique
des forums ? In Actes de la 20e conf?erence sur
le Traitement Automatique des Langues Naturelles
(TALN?2013), pages 539?546, Les Sables d?Olonne,
France.
Fabrizio Gotti, Philippe Langlais, and Atefeh Farzin-
dar. 2013. Translating government agencies? tweet
feeds: Specificities, problems and (a few) solutions.
In Proceedings of the Workshop on Language Anal-
ysis in Social Media, pages 80?89, Atlanta, Georgia.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 368?378, Port-
land, Oregon.
Jie Jiang, Andy Way, and Rejwanul Haque. 2012.
Translating user-generated content in the social net-
working space. In Proceedings of the Tenth Biennial
Conference of the Association for Machine Transla-
tion in the Americas (AMTA-2012), San Diego, Cali-
fornia.
Arefeh Kazemi, Amirhassan Monadjemi, and Moham-
madali Nematbakhsh. 2013. A quick review on re-
ordering approaches in statistical machine transla-
tion systems. IJCER, 2(4).
Tobias Kuhn. 2013. A survey and classification of con-
trolled natural languages. Computational Linguis-
tics.
Meenakshi Nagarajan and Michael Gamon, editors.
2011. Proceedings of the Workshop on Language
in Social Media (LSM 2011). Portland, Oregon.
Johann Roturier, Linda Mitchell, Robert Grabowski,
and Melanie Siegel. 2012. Using automatic ma-
chine translation metrics to analyze the impact of
source reformulations. In Proceedings of the Con-
ference of the Association for Machine Translation
in the Americas (AMTA), San Diego, California.
Johann Roturier, Linda Mitchell, and David Silva.
2013. The ACCEPT post-editing environment: a
flexible and customisable online tool to perform and
analyse machine translation post-editing. In Pro-
ceedings of MT Summit XIV Workshop on Post-edi-
ting Technology and Practice, Nice, France.
Violeta Seretan, Pierrette Bouillon, and Johanna Ger-
lach. 2014. A large-scale evaluation of pre-edi-
ting strategies for improving user-generated content
translation. In Proceedings of the 9th Edition of the
Language Resources and Evaluation Conference,
Reykjavik, Iceland.
Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
471?481, Atlanta, Georgia.
71
