	



	
			


Chinese Word Sense Disambiguation with PageRank and HowNet 
Jinghua Wang 
Beijing University of Posts  
and  Telecommunications 
Beijing, China 
wjh_smile@163.com 
Jianyi Liu 
Beijing University of Posts 
and Telecommunications 
Beijing, China 
jianyilui@sohu.com 
Ping Zhang 
Shenyang Normal  
University 
Shenyang, China 
pinney58@163.com 
 
 
Abstract 
Word sense disambiguation is a basic 
problem in natural language processing. 
This paper proposed an unsupervised word 
sense disambiguation method based 
PageRank and HowNet. In the method, a 
free text is firstly represented as a sememe 
graph with sememes as vertices and 
relatedness of sememes as weighted edges 
based on HowNet. Then UW-PageRank is 
applied on the sememe graph to score the 
importance of sememes. Score of each 
definition of one word can be computed 
from the score of sememes it contains. 
Finally, the highest scored definition is 
assigned to the word. This approach is 
tested on SENSEVAL-3 and the 
experimental results prove practical and 
effective. 
1 Introduction 
Word sense disambiguation, whose purpose is to 
identify the correct sense of a word in context, is 
one of the most important problems in natural 
language processing. There are two different 
approaches: knowledge-based and corpus-based 
(Montoyo, 2005). Knowledge-based method 
disambiguates words by matching context with 
information from a prescribed knowledge source, 
such as WordNet and HowNet. Corpus-based 
methods are also divided into two kinds: 
unsupervised and supervised (Lu Z, 2007). 
Unsupervised methods cluster words into some 
sets which indicate the same meaning, but they can 
not give an exact explanation. Supervised 
machine-learning method learns from annotated 
sense examples. Though corpus-based approach 
usually has better performance, the mount of words 
it can disambiguate essentially relies on the size of 
training corpus, while knowledge-based approach 
has the advantage of providing larger coverage. 
Knowledge-based methods for word sense 
disambiguation are usually applicable to all words 
in the text, while corpus-based techniques usually 
target only few selected word for which large 
corpora are made available (Mihalcea, 2004). 
This paper presents an unsupervised word sense 
disambiguation algorithm based on HowNet. 
Words? definition in HowNet is composed of some 
sememes which are the smallest, unambiguous 
sense unit. First, a free text is represented as a 
sememe graph, in which sememes are defined as 
vertices and relatedness of sememes are defined as 
weighted edges. Then UW-PageRank is applied on 
this graph to score the importance of sememes. 
Score of each definition of one word can be 
deduced from the score of sememes it contains. 
Finally, the highest scored definition is assigned to 
the word. This algorithm needs no corpus, and is 
able to disambiguate all the words in the text at one 
time. The experiment result shows that our 
algorithm is effective and practical. 
2 HowNet 
HowNet (Dong, Z. D, 2000) is not only a machine 
readable dictionary, but also a knowledge base 
which organizes words or concepts as they 
represent in the object world. It has been widely 
used in word sense disambiguation and pruning, 
text categorization, text clustering, text retrieval, 
machine translation, etc (Dong, Z. D, 2007). 
39
Sixth SIGHAN Workshop on Chinese Language Processing
2.1 The content and structure of HowNet 
HowNet is an online common-sense knowledge 
based unveiling inter-conceptual relations and 
inter-attribute relations of concepts as connoting in 
lexicons of the Chinese and English equivalents. 
There are over 16000 word records in the 
dictionary. This is an example 
 
No.=017625 No.=017630 
W_C=? W_C=? 
G_C=V G_C=V 
E_C=?? E_C=??? 
W_E=hit W_E=buy 
G_E=V G_E=V 
DEF=beat|? DEF=buy|??
commercial|? 
 
This is two of the concepts of word ???: ?No.? 
is the entry number of the concept in the dictionary; 
?G_C? is the part of speech of this concept in 
Chinese, and ?G_E? is that in English; ?E_C? is 
the example of the concept; ?W_E? is the concept 
in English; ?DEF? is the definition. 
Definitions of words are composed of a series of 
sememes (usually more than one ? like DEF 
No.017630 contains ?buy|? ? and ?commercial|
? ?) ? like ?beat|? ? which is the smallest 
unambiguous unit of concept. First sememe of the 
definition like ?buy|?? of DEF No.017630 is the 
main attribution of the definition. Sememes have 
been classified into 8 categories, such as attribute, 
entity, event role and feature, event, quantity value, 
quantity, secondary feature and syntax. Sememes 
in one category form a tree structure with 
hypernymy / hyponymy relation. Sememes 
construct concepts, e.g. definition, so the word 
sense disambiguation task of assigning definition 
to word can be done through the computation of 
sememes. 
2.2 The similarity of sememes 
The tree structure of sememes makes it possible to 
judge the relatedness of them with a precision 
mathematical method. Rada (Rada, R, 1989) 
defined the conceptual distance between any two 
concepts as the shortest path through a semantic 
network. The shortest path is the one which 
includes the fewest number of intermediate 
concepts. With Rada?s algorithm, the more similar 
two concepts are, the smaller their shortest path is, 
and so we use the reciprocal of the length of 
shortest path as the similarity. Leacock and 
Chodorow (Leacock, C, 1998) define it as follows: 
1 2 1 2( , ) max[ log( ( , ) /(2 ))lchsim c c length c c D= ?  
where length(c1, c2) is the shortest path length 
between the two concepts and D is the maximum 
depth of the taxonomy. 
Wu and Palmer (Wu, Z., 1994) define another 
formula to measure the similarity 
)()(
)),((2
),(
21
21
21 cdepthcdepth
cclcsdepth
ccsimwup +
?=
                                 
depth is the distance from the concept node to the 
root of the hierarchy. lcs(c1,c2) is the most specific 
concept that two concepts have in common, that is 
the lowest common subsumer. 
3 PageRank on Sememe Graph 
PageRank is an algorithm of deciding the 
importance of vertices in a graph. Sememes from 
HowNet can be viewed as an undirected weighted 
graph, which defines sememes as vertices, 
relations of sememes as edges and the relatedness 
of connected sememes as the weights of edges. 
Because PageRank formula is defined for directed 
graph, a modified PageRank formula is applied to 
use on the undirected weighted graph from 
HowNet. 
3.1 PageRank 
PageRank (Page, L., 1998) which is widely used 
by search engines for ranking web pages based on 
the importance of the pages on the web is an 
algorithm essentially for deciding the importance 
of vertices within a graph. The main idea is that: in 
a directed graph, when one vertex links to another 
one, it is casting a vote for that other vertex. The 
more votes one vertex gets, the more important this 
vertex is. PageRank also takes account the voter: 
the more important the voter is, the more important 
the vote itself is. In one word, the score associated 
with a vertex is determined based on the votes that 
are cast for it, and the score of the vertex casting 
these votes. So this is the definition: 
Let G=(V,E) be a directed graph with the set of 
vertices V and set of edges E, when E is a subset of 
V?V. For a given vertex Vi, let In(Vi) be the set of 
vertices that point to it, and let Out(Vi) be the set of 
edges going out of vertex Vi. The PageRank score 
of vertex Vi is 
40
Sixth SIGHAN Workshop on Chinese Language Processing
?
?
+?=
)( )(
)(
*)1()(
iVInj j
j
i
VOut
VS
ddVS
                                  
d is a damping factor that can be set between 0 and 
1,and usually set at 0.85 which is the value we use 
in this paper (Mihalcea, R., 2004). 
PageRank starts from arbitrary values assigned 
to each vertex in the graph, and ends when the 
convergence below a given threshold is achieved. 
Experiments proved that it usually stops computing 
within 30 iterations (Mihalcea, R., 2004). 
PageRank can be also applied on undirected 
graph, in which case the out-degree of a vertex is 
equal to the in-degree of the vertex. 
3.2 PageRank on sememe graph 
Sememes from HowNet can be organized in a 
graph, in which sememes are defined as vertices, 
and similarity of connected sememes are defined 
as weight of edges. The graph can be constructed 
as an undirected weighted graph.  
We applied PageRank on the graph with a 
modified formula 
?
?
?+?=
)( )(
)()(
*)1()(
iVCj j
jij
i
VD
VSEweight
ddVS                         
C(Vi)is the set of edges connecting with Vj, 
weight(Eij)is the weight of edge Eij connecting 
vertex Vi and Vj, and D(Vj) is the degree of Vj. 
This formula is named UW-PageRank. In sememe 
graph, we define sememes as vertices, relations of 
sememes as edges and the relatedness of connected 
sememes as the weights of edges. UW-PageRank 
is applied on this graph to measure the importance 
of the sememes. The higher score one sememe 
gets, the more important it is. 
4 Word sense disambiguation based on 
PageRank 
To disambiguate words in the text, firstly the text 
is converted to an undirected weighted sememe 
graph based on HowNet. The sememes which are 
from all the definitions for all the words in the text 
form the vertices of the graph and they are 
connected by edges whose weight is the similarity 
of the two sememes. Then, we use UW-PageRank 
to measure the importance of the vertex in the 
graph, so all the sememes are scored. So each 
definition of one word can be scored based on the 
score of the sememes it contains. Finally, the 
highest scored definition is assigned to the word as 
its meaning. 
4.1 Text representation as a graph 
To use PageRank algorithm to do disambiguation, 
a graph which represents the text and interconnects 
the words with meaningful relations should be 
built first. All the words in the text should be POS 
tagged first, and then find all the definitions 
pertaining to the word in HowNet with its POS. 
Different sememes from these definitions form the 
vertices of the graph. Edges are added between the 
vertices whose weights are the similarity of the 
sememes. The similarity can be measured by the 
algorithm in Section 2.2. As mentioned in Section 
2.1, all the sememes in HowNet are divided into 
eight categories, and in each category, sememes 
are connected in a tree structure. So based on the 
algorithms in Section 2.2, each two sememes in 
one category, i.e. in one tree, have a similarity 
more than 0, but if they are in different category, 
they will have a similarity equal to 0. As a result, a 
text will be represented in a sememe graph that is 
composed of several small separate fully connected 
graphs.
Assumed that a text containing ?word1 word2 
word3? is to be represented in a graph. The 
definition (DEF) and sememes for each word are 
listed in Table 1. 
 
Table 1. ? Word1 Word2Word3? 
Word Definition Sememes 
DEF11 S1,S5 
DEF12 S2 Word1
DEF13 S8 
DEF21 S6 Word2
DEF22 S7,S9 
DEF31 S3 Word3
DEF32 S4 
 
Sememes are linked together with the weight of 
relatedness. For example, S1 and S2 are connected 
with an edge weighted 0.3.The relation of word, 
DEF and sememes is represented in Figure1, and 
sememe graph is in Figure 2. 
41
Sixth SIGHAN Workshop on Chinese Language Processing
 
Figure 1. Word-DEF-Sememe Relation 
 
 
Figure 2. Sememe Graph 
4.2 Word sense disambiguation based on 
PageRank 
Text has been represented in a sememe graph with 
sememes as vertices and similarity of sememes as 
the weight of the edges. Then, UW-PageRank is 
used to measure the importance of the vertex, i.e. 
sememes in the graph. The score of all the vertices 
in Figure 1 is in Table 2. 
 
Table 2. Score of Sememes 
Vertex S1 S2 S3 S4 S5 
UW-PageRank Score 0.179 0.175 0.170 0.165 0.202
Vertex S6 S7 S8 S9  
UW-PageRank Score 0.208 0.176 0.181 0.181  
Each definition of the words is scored based on 
the score of the sememes it contains.  
))((maxarg)(
1
i
mi
DEFScoreWordSense
??
=   
WordDEFi ? ,    DEFi is the i sense of the word. 
We use two methods to score the definition: 
Mean method 
HowNet uses sememes to construct definitions, 
so the score of the definition can be measured 
through an average score of all the sememes it 
contains. And we chose the definition of the 
highest score as the result.  
?
??
=
ni
iSScoren
DEFScore
1
)(
1
)(    
DEFSi ? , Si is the i sememe of DEF. 
First Sememe method 
First sememe of one DEF is defined as the most 
important meaning of the DEF. So we use another 
method to assign one DEF to one word taking first 
sememe into consideration. For all the DEF of one 
word, if one first sememe of one DEF gets the 
highest score, the DEF is assigned to the word. 
)()( eFirstSememScoreDEFScore =                                     
If several DEFs have the same first sememe or 
have the same score, we sort all the other sememes 
are from high score to low score, then comparison 
is made among this sememes from the beginning to 
the end until one of the sememes has the highest 
score among them, and finally the DEF containing 
this sememe is assigned to the word. 
The performance of the two methods will be 
tested and compared in Section5. 
With the ?Means? (M) and ?First Sememe? (FS) 
methods, text in Section 4.1 gets the result in Table 
3. 
 
Table3. Result of ?Word1 Word2 Word3? 
Word Definition Score (M) Result(M) Result(FS)
DEF11 0.191
DEF12 0.175Word1
DEF13 0.181
DEF11 DEF13 
DEF21 0.208Word2
DEF22 0.179
DEF21 DEF21 
DEF31 0.170Word3
DEF32 0.165
DEF31 DEF31 
 
42
Sixth SIGHAN Workshop on Chinese Language Processing
 
Table 4. Experimental Result 
Word Baseline R+M L +M W+M R+FS L +FS W+FS Li 
?? 0.25 0.53 0.53 0.53 0.53 0.53 0.53 0.32 
?? 0.33 0.6 0.5 0.6 0.4 0.4 0.3 0.74 
? 0.1 0.42 0.42 0.46 0.35 0.35 0.38 0.26 
?? 0.25 0.73 0.75 0.56 0.67 0.75 0.56 0.39 
?? 0.17 0.5 0.57 0.64 0.43 0.5 0.64 0.67 
?? 0.33 0.47 0.27 0.13 0.47 0.27 0.13 0.27 
Average 
Precision 
0.24 0.54 0.51 0.49 0.48 0.47 0.42 0.44 
 
5 Experiment and evaluation 
We chose 96 instances of six words from 
SENSEVAl-3 Chinese corpus as the test corpus. 
Words are POS tagged. We use precision as the 
measure of performance and random tagging as the 
baseline. We crossly use Rada?s (R), Leacock & 
Chodorowp?s (L), and Wu and Palmer?s (W) 
methods to measure the similarity of sememes with 
mean method (M) and first sememe (FS) scoring 
the DEF. The precision of the combination 
algorithm is listed in Table 4. 
Li (Li W., 2005) used naive bayes classifier with 
features extracted from People?s Daily News to do 
word sense disambiguation on SENSEVAL-3. The 
precision is listed in line ?Li? of table as a 
comparison. 
The average precision of our algorithm is around 
two times higher than the baseline, and 5 of the 6 
combination algorithm gets better performance 
than Li. And for 5/6 word case, our algorithm gets 
the best performance. Among the three methods of 
measure the similarity of sememes, Rada?s method 
gets the best performance. And between the two 
methods of scoring definition, ?Mean method? 
works better, which indicates that although the first 
sememe is the most important meaning of one 
definition, the other sememes are also very 
important, and the importance of other sememes 
also should be taken into consideration while 
scoring the definition. Of all the combination of 
algorithms, ?Rada + Mean? gets the best 
performance, which takes a reasonable way to 
measure the similarity of two sememes and 
comprehensively scores the definition based on the 
importance of its sememes in the sememe graph 
from the whole text. 
6 Related works 
Many works in Chinese word sense 
disambiguation with HowNet. Chen Hao (Chen 
Hao, 2005) brought up a k-means cluster method 
base on HowNet, which firstly convert contexts 
that include ambiguous words into context vectors; 
then, the definitions of ambiguous words in 
Hownet can be determined by calculating the 
similarity between these context vectors. To do 
disambiguation, Yan Rong (Yan Rong, 2006) first 
extracted some most relative words from the text 
based on the co-occurrence, then calculate the 
similarity between each definition of ambiguous 
word and its relative words, and finally find the 
most similar definition as its meaning. The 
similarity of definitions is measured by the 
weighted mean of the similarity of sememes, and 
the similarity of sememes is measured by a 
modified Rada?s formula. Gong YongEn (Gong 
YongEn, 2006) used a similar method with Yan, 
and more over, he took recurrence of sememes into 
consideration. Compare with those methods, our 
method has a more precious sememes? similarity 
measure method, and make full use of the structure 
of its tree structure by representing text in graph 
and use UW-PageRank to judge sememes? 
importance in the whole text, that is the most 
obvious difference from them. Mihalceal 
(Mihalceal, 2004) first provide the semantic graph 
method to do word sense disambiguation, but her 
work is totally on English with WordNet, which is 
definitely different in meaning representation from 
HowNet. WordNet uses synsets to group similar 
concepts together and differentiate them, while 
HowNet use a close set of sememes to construct 
concept definitions. In Mihalceal?s method, the 
43
Sixth SIGHAN Workshop on Chinese Language Processing
vertexes of graph are synsets, and in ours are 
sememes. And after measure the importance of 
sememes, an additional strategy is used to judge 
the score of definition based on the sememes. 
7 Conclusion 
An unsupervised method is applied to word sense 
disambiguation based on HowNet. First, a free text 
is represented as a sememe graph with sememes as 
vertices and relatedness of sememes as weighted 
edges. Then UW-PageRank is applied on this 
graph to score the importance of sememes. Score 
of each definition of one word can be deduced 
from the score of sememes it contains. Finally, the 
highest scored definition is assigned to the word. 
Our algorithm is tested on SENSEVAL-3 and the 
experimental results prove our algorithm to be 
practical and effective. 
Acknowledgment 
This study is supported by Beijing 
Natural Science Foundation of (4073037) and 
Ministry of Education Doctor 
Foundation (20060013007). 
References 
Chen hao, He Tingting, Ji Donghong, Quan Changqing, 
2005. An Unsupervised Approach to Chinese Word 
Sense Disambiguation Based on Hownet, 
Computational Linguistics and Chinese Language 
Processing, Vol. 10, No. 4, pp. 473-482 
Dong, Z.D., Dong, Q.2000. ?Hownet,? 
http://keenage.com. 
Dong Zhendong, Dong Qiang, Hao Changling, 2007. 
Theoretical Findings of HowNet, Journal of Chinese 
Information Processing, Vol. 21, No. 4, P3-9 
Gong Y., Yuan C., Wu G., 2006. Word Sense 
Disambiguation Algorithm Based on Semantic 
Information, Application Research of Computers, 41-
43. 
Leacock, C., Chodorow, M., 1998.Combing local 
context and WordNet Similarity for word sense 
identification, in: C.Fellbaum (Ed.), WordNet: An 
electronic lexical database, MIT Press, 305-332 
Li W., Lu Q., Li W., 2005. Integrating Collocation 
Features in Chinese Word Sense Disambiguation, 
Integrating Collocation Features in Chinese Word 
Sense Disambiguation. In Proceedings of the Fourth 
Sighan Workshop on Chinese Language Processing, 
87-94. 
Lu Z., Liu T., Li, S., 2007. Chinese word sense 
disambiguation based on extension theory, Journal of 
Harbin Institute of Technology, Vol.38 No.12, 2026-
2035 
Mihalcea, R., Tarau, P., Figa, E., 2004. PageRank on 
Semantic Networks, with application to Word Sense 
Disambiguation, in Proceedings of The 20st 
International Conference on Computational 
Linguistics  
Montoyo, A., Suarez, A., Rigau, G. and Palomar, M. 
2005. Combining Knowledge- and Corpus-based 
Word-Sense-Disambiguation Methods, Volume 23, 
Journal of Machine learning research , 299-330.  
Page, L., Brin, S., Motwani, R., and wingorad, T., 1998. 
The pagerank citation ranking: Bringing order to the 
web Technical report, Stanford Digital Library 
Technologies Project. 
Rada, R., Mili,E,.Bicknell, Blettner, M., 1989. 
Development and application of a metric on semantic 
nets, IEEE Transactions on Systems, Man and 
Cybernetics 19(1) 17-30 
Wu, Z., Plamer, M., 1994. Verb semantics and lexical 
selection, in 32nd Annual Meeting of the Association 
for Computational Linguistics, Las Cruces, New 
Mexico, 133-138 
Yan R., Zhang L., 2006. New Chinese Word Sense 
Disambiguation Method, Computer Technology and 
Development, Vol. 16 No.3, 22-25 
44
Sixth SIGHAN Workshop on Chinese Language Processing
Incorporating New Words Detection with Chinese Word Segmentation 
Hua-Ping ZHANG1   Jian GAO1  Qian MO 2  He-Yan HUANG1 
1
 Beijing Institute of Technology, Beijing, P.R.C 100081 
2
 Beijing Technology and Business University, Beijing, P.R.C 100048 
Email: kevinzhang@bit.edu.cn 
 
     
Abstract 
With development in Chinese words 
segmentation, in-vocabulary word 
segmentation and named entity 
recognition achieves state-of-art 
performance. However, new words 
become bottleneck to Chinese word 
segmentation. This paper presents the 
result from Beijing Institute of 
Technology (BIT) in the Sixth 
International Chinese Word 
Segmentation Bakeoff in 2010. Firstly, 
the author reviewed the problem caused 
by the new words in Chinese texts, then 
introduced the algorithm of new words 
detection. The final section provided 
the official evaluation result in this 
bakeoff and gave conclusions.  
1  Introduction 
With the rapid development of Internet with 
Chinese language, word segmentation received 
extensive attention. In-vocabulary word segmentation 
and named entity recognition have achieved state-of-art 
performance.  Chinese words are actually not well 
defined, and there is not a commonly accepted 
segmentation lexicon. It is hard to collect all 
possible new words, or predict new words occurred 
in the future. New words is the bottleneck to 
Chinese word segmentation. The problem became 
more severe with word segmentation on special 
domain texts, such as computer, medicine and 
finance. There are much specialized words which 
are difficult to be exported to the lexicon. So new 
words detection is very important, which would 
have more substantial impact on the performance 
of word segmentation than ambiguous 
segmentation.  
In this paper?we presented a method of new 
words detection, and then detailed the process of 
Chinese word segmentation incorporating new 
words detection. The last section provided the 
evaluation and gave our conclusions. 
2 Problem with new words 
In the process of Chinese word Segmentation, 
there are many mistakes because of new words. 
These new words are Out of vocabulary (OOV), so 
the system couldn?t distinguish them from original 
texts, and then impacted the results of word 
segmentation.  
We gave an example from Text C in medicine 
domain to explain and detect the new words. 
???????????????????
?????? 12???????? PAD???
????????????????????
??????ABI?????????????
????? 
The sentence should be segmented as follows? 
???  ?  ????  ??  ??  ?
?    ?  ??  ??????  ??  12  ?  
?  ?  ???  ??  PAD  ??  ?  ?  
?  ??  ??  ?  ??  ??  ??  ?  
?  ?  ??  ??  ?  ??  ??  ?    
ABI  ??  ??  ?  ??  ??  ????  
?  ??  ?  ? 
Here, both ?????? and ???????? 
are domain words, or new words beyond general 
segmentation lexicon. Therefore, new words from 
domain should be detected and added to 
segmentation lexicon before word segmentation.  
3 Word segmentation with new words 
detection 
3.1 Framework 
Word Segmentation With 
general lexicon+domain 
lexicon
Frequent String 
Detection
No
AV statistics, language 
modeling
Generate New Words
Domain 
Lexicon
Yes
New words 
threshold
Output 
words
Figure 1: The framework of Chinese word segmentation 
incorporating with new words detection
General 
Lexicon
Input Sentence
 
 
As illustrated in Figure 1, Chinese word 
segmentation with new words detection is a 
recursive process.  The process is given as 
follows: 
1. Making Chinese word segmentation with 
domain lexicon beyond general lexicon. 
2. Frequent string (over twice) finding with 
postfix tree algorithm, and taking them as 
new words candidate. 
3. Access Variety statistics [Haodi Feng etc. 
2004], and language modeling on word 
formation. [Hemin, 2006] 
4. Exporting new words to domain lexicon. 
5. Recursively, until no more new word 
detected. 
6. Output final word sequence. 
 
 
 
3.2 The process of new words detection 
Simple word segmentation is the first step of 
processing of Chinese language when we deal with 
a very long Chinese article. The method of word 
segmentation is based on HHMM, and Zhang and 
Liu (2003) have given detailed explanation about 
this. 
During the process of word segmentation in 
the first, the system records the words which occur 
frequently. We can set a threshold value of words? 
occurrence frequency. As long as the word 
occurrence frequency reaches this value, this word 
could be recorded in the system as frequent string.  
With the frequent strings detected, we can do 
the further analysis. For every frequent string, we 
check its left and right adjacent one in the original 
text segmented respectively. Through this step, we 
find the adjacent words which occur next to some 
frequent string detected. If the adjacent word also 
occurs very frequently, or even it occurs at the left 
or right of the frequent string every time, it?s great 
possibility that the string detected and the adjacent 
word could merge into one word.  
With the detection in above steps, we gain 
new words from Chinese texts. Then we import 
these new words into domain lexicon and our 
lexicon is updated. With the lexicon containing 
new words, we can do the next cycle recursively 
and revise continually. 
Then, we can see this is a recursive structure. 
Through the continued process of word 
segmentation and new words detection, the state of 
segmentation tends to be steady. The condition of 
steady state has several kinds such as no more new 
words detected or the latest result equal to the 
previous one. At this time, we can break the 
recursion and output the final result. 
This is an example. This sentence is from 
Text D in finance domain 
???????????????????
???????????? (?The financial market 
has been stable and the stock has rebounded in less 
than one year time after Lehman Brother 
Corporation went bankrupt.?) 
After word segmentation with original 
lexicon, this altered sentence is: 
??/?/??/??/??/?/?/?/?/?/?? 
??/??/??/?/??/?/?/??/?/? 
?????? is a new word as a organization 
name and it is hard to be collected. Like this kind 
of word, there are difficulties to add new words to 
update the lexicon in time. So it is normal to 
segment this word ?????? into three words. 
Through frequent string detection, we gain these 
three words ???, ???and ????. With the 
adjacent analysis, we find the word ??? occurs 6 
times, ??? 3 times and ???? 3 times.  
The character ??? occurs 3 times in the 
detected word ????? and 3 times at the left of 
the word ???. So we can consider the word ??
?? as a whole word. 
Then we can easily find the words ???? are 
always at the right of words ????. So it?s 
necessary to consider ?????? as a whole 
word.  
4  Evaluation 
 The performance of word segmentation is 
measured by test precision (P), test recall (R), F 
score (which is defined as 2PR/(P+R)) and the 
OOV recall rate. 
 In this competition, our test corpus involved 
literature, computer, medicine and Finance, totally 
425KB. We take 6 months data of The People's 
Daily to be the training corpus. From Table 1, we 
can see the official evaluation result. 
 Table 1. Official evaluation result 
 Our system got high Precision Rate and 
Recall Rate after testing the texts in four domains, 
especially Recall Rate is all over 95%. And we 
also could see that this system detected most new 
words through several measures of OOV, 
especially IV RR is all over 97.5%. This proved 
that the system could be able to get a nice result 
through processing professional articles in 
literature, computer, medicine and finance domains, 
and we believed it also could do well in other 
domains. This also proved that the method of new 
words detection with Chinese word segmentation 
was competitive. 
 
5Conclusion 
Through this competition, we?ve found a lot 
of problems needed to be solved in Chinese word 
segmentation and tried our best to improve the 
system. Finally, we proposed the method of new 
words detection in Chinese word segmentation. 
But we still had some shortage during the 
evaluation and need to improve in the future. 
 
References 
Lawrence. R.Rabiner.1989. A Tutorial on Hidden 
Markov Models and Selected Applications in Speech 
Recognition. Proceedings of IEEE 77(2): pp.257-286. 
Hua-Ping Zhang, Qun Liu. Model of Chinese Words 
Rough Segmentation Based on N-Shortest-Paths 
Method. Journal of Chinese information processing, 
2002,16(5):1-7 (in Chinese) 
ZHANG Hua-Ping, LIU Qun, Zhang Hao and Cheng 
Xue-Qi. 2002. Automatic Recognition of Chinese 
Unknown Words Recognition. Proc. of First SigHan 
attached on COLING 2002  
ZHANG Hua-Ping, LIU Qun, YU Hong-Kui, CHENG 
Xue-Qi, BAI Shuo.  Chinese Named Entity 
Recognition Using Role Model. International Journal of 
Computational Linguistics and Chinese language 
processing, 2003,Vol. 8 (2) 
Mao-yuan Zhang, Zheng-ding Lu, Chun-yan Zou. A 
Chinese word segmentation based on language 
situation in processing ambiguous words. Information 
Sciences 162 (2004) 275?285 
Gao, Jianfeng, Andi Wu, Mu Li, Chang-Ning 
Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin. 
Adaptive Chinese word segmentation. ACL2004. July 
21-26. 
Haodi Feng, Kang Chen, Xiaotie Deng, Weimin Zheng 
Accessor Variety Criteria for Chinese Word 
Extraction, Computational Linguistics March 2004, 
Vol. 30, No. 1: 75?93. 
Hemin, Web-Oriented Chinese Meaningful String 
Mining, M.Sc Thesis of Graduate University of 
Chinese Academy of Scienses. 2006 
  R P F1 
OOV 
R 
OOV 
RR 
IV 
RR 
A-Literature 0.965 0.94 0.952 0.069 0.814 0.976 
B-Computer 0.951 0.926 0.938 0.152 0.775 0.982 
C-Medicine 0.953 0.913 0.933 0.11 0.704 0.984 
D-Finance 0.963 0.938 0.95 0.087 0.758 0.982 
Chinese Personal Name Disambiguation Based on Person Modeling 
Hua-Ping ZHANG1   Zhi-Hua LIU2   Qian MO3  He-Yan HUANG1 
1
 Beijing Institute of Technology, Beijing, P.R.C 100081 
2
 North China University of Technology, P.R.C 100041 
3
 Beijing Technology and Business University, Beijing, P.R.C 100048 
Email: kevinzhang@bit.edu.cn 
 
 
 
Abstract 
This document presents the bakeoff re-
sults of Chinese personal name in the 
First CIPS-SIGHAN Joint Conference 
on Chinese Language Processing. The 
authors introduce the frame of person 
disambiguation system LJPD, which 
uses a new person model. LJPD was 
built in short time, and it is not given 
enough training and adjustment. Evalua-
tion on LJPD shows that the precision is 
competitive, but the recall is very low. It 
has more space for further improvement. 
1 Introduction 
We participated in the First CIPS-SIGHAN Joint 
Conference on Chinese Language Processing. 
And have taken task 3: Chinese Personal Name 
disambiguation. 
Chinese personal name disambiguation in-
cludes two stages: words are segmented to rec-
ognize Chinese personal name, and documents 
are clustered to disambiguate different person 
with the same personal name.  
In our system, it involves the following 
steps: 
1) Segmenting words and tagging the part-of-
speech, and then recognizing Chinese personal 
name using ICTCLAS 2010 system1. 
2) Extracting personal feature to create the per-
son attribution model on each document. 
3) Generating initial clusters according to fea-
tures in person model, and then clustering the 
initial clusters until the stop criteria is reached. 
The processing flow is illustrated in figure 1. 
                                                 
1
 It can be downloaded from 
http://hi.baidu.com/drkevinzhang 
 
 
Figure 1 Step of Person Disambiguation 
 
As illustrated in figure 1, the whole system 
addresses four problems: personal name recogni-
tion, anaphora resolution of personal name, per-
son model creation and clustering. 
 
2 Personal Name Recognition 
Chinese personal name recognition is more dif-
ficult than English. Such difficulties usually 
combine with Chinese word segmentation. The 
set of Chinese personal name is infinite, and the 
rule of name construction is varied. Chinese per-
sonal name is often made up of a usual word, 
and has ambiguity with its context. 
To solve the difficulties mentioned above, 
Chinese personal name recognition based on role 
tagging is given in [Zhang etc., 2002]. The ap-
proach is: tokens after segmentation are tagged 
using Viterbi algorithm with different roles ac-
cording to their functions in the generation of 
Chinese personal name; the possible names are 
recognized after maximum pattern matching on 
the roles sequence [ZHANG, etc., 2002]. With 
this approach, the precision of ICTCLAS 
reaches 95.57% and the recall is 95.23% in an 
opening corpus which contains 1,108,049 words. 
In the corpus, the count of the personal name is 
15,888. And ICTCLAS is a Chinese lexical 
analysis system witch combines part-of-speech 
tagging, word segmentation, unknown words 
recognition. It can meet our requirements, so 
ICTCLAS provides personal name recognition 
in our system. 
3 Anaphora Resolution of Personal 
Name 
Anaphora is very common in natural language. 
Resolve this problem can help us get more in-
formation of the person from a document. 
Anaphora resolution of personal name is an 
important part of anaphora resolution. At present, 
much advancement in anaphora resolution have 
occurred [Saliha 1998]. Anaphora resolution of 
personal pronouns is an especially complicate 
problem in anaphora resolution of personal name. 
In our system, we don?t process this problem. 
The reason is that anaphora resolution of per-
sonal name will take side effect to personal 
name disambiguation unless its precision is defi-
nitely high. So we just process the anaphora of 
the first name or the second name. For example, 
?Jianmin Wang? in above context and ?Profes-
sor Wang? will be resolved in our system. 
4 Personal Model 
We propose a person model to represent the 
person in the document: 
Person = {N, P, Q, R} 
where: 
N is the collection of appellation of person, 
such as name, nickname, alias, and so on 
P is the collection of the basic attributes of 
person 
Q is the collection of the other attributes of 
person 
R is the collection of the terms co-
occurrence with person name, witch is called 
term field 
In the system, we focused on seven attrib-
utes such as sex, nationality, birthday, native 
place, address, profession, family members and 
personal name, co-occurrence terms. In these 
features, name?N, {sex, nationality, birthday, 
native place}?P, {address, profession, family 
members}?Q, {co-occurrence term}?R. Table 
1 is the examples of person model. 
In view of the co-occurrence personal name 
is especially important for person disambigua-
tion. We separate it as another field in R. 
4.1  Attributes Feature 
The components N, P and Q of person model are 
attributes feature. The dimension of these fea-
tures for a person is different. For example, the 
sex of a person is constant in life, while his or 
her address may be different in different time. 
Take DOM to represent the dimension of the 
attributes features. Then: 
 DOM(Ni) = 1; (1?i?n) 
 DOM(Pi) = 1; (1?i?k) 
 DOM(Qi) ? 1; (1?i?m) 
For a person, N and P are constant in life. If 
one attribute of N or P between two persons is 
different, they are not the same person. 
To get the attributes feature, we have three 
steps: First, segment word and tag part-of-speech 
for the input document. Second, we identify the 
triggering word which is defined as attributes 
value and the Max-Noun Phrase. The triggering 
words are identified by their POS and a hand-
built triggering word thesaurus.  At last, a classi-
fier determines the attribute belongs to the left 
personal name or the right to the attribute. The 
classifier is trained by the corpus which is hand-
tagged documents from internet.
 
Figure 2 Step of Person Attributes Extraction 
4.2 Term Field 
In person model, R is the collection of the terms 
co-occurrence within person. We adopt Vector 
Space Model to represent these terms. The i-th 
term is represented by ti, and its weight is repre-
sented by wi, and the weight shows the impor-
tance of the term for the person.  
R = (t1, w1; t2, w2; ? ; tH, wH) 
To get the person?s term field, we identify 
a scope witch these terms occurred. We con-
sider three kinds of scope for term field: the 
total document, the paragraph where the per-
sonal name is present, sentence where the per-
sonal name is present. And then segment words 
and tag part-of-speech for these fragments. Next, 
filter out the attribute terms and filter by part-
of-speech and leave only nouns, verb, adjective, 
adverb and name entry. Third, we make a stop 
word list, and filter out these stop terms. Last, 
according to the term?s DF, filter out high fre-
quency and low frequency terms, and only the 
terms witch DF is not lower than 2 and not 
higher than N/3(N is the total count of docu-
ments) are left. 
In collection R, we have separated term 
field to co-occurrence personal name vector and 
co-occurrence common term vector. Because 
the two vectors have different affect to person 
disambiguation. This difference manifests in the 
different method to compute these weight. The 
common term?s weight is computed by tf-idf 
algorithm: 
)1/log()1),(log(),( +?+= tnNdttfdtw  
where: 
),( dtw  is the weight of term t in document 
d  
),( dttf  is the frequency of occurrence of t 
in d  
N is the total count of documents 
nt is the count of documents which contain 
term t 
 sex nationality birthday Native place address 
Family 
members profession 
Co-occurrence 
personal name 
Co-occurrence 
terms field 
Name1 ? ? 1949  ??  ?? ? ?? 
Name2 ?   ??  ?? ?? ? ?? 
Name3 ? ?   ??  ?? ? ?? 
Table 1 Examples of Person Model
 
The co-occurrence personal name?s weight is 
computed below: 
)1/'log()1),(log(),( +?+= namenNpnamenfpnamew  
where: 
),( pnamew  is the weight of co-occurrence 
name name  
),( pnamenf  is the frequency of co-
occurrence of name  and person p  
name  is the count of the co-occurrence of 
name  and the other personal name 
The similarity of term field between two persons 
is calculated by the angle cosine: 
? ?
?
==
i i
ii
i
ii
yx
yx
YXYXSim
22
*
*
),cos(),(  
5 Clustering 
Person model ?Person = {N, P, Q, R}? is multi-
dimensional. First, we adopted two rules to gen-
erate original clusters: 
Rule 1: For two persons whose name is same, if 
one of the birthday (accurate to month) or rela-
tive is matched, these two persons are the same 
person. 
Rule 2: For two persons whose name is same, if 
one of the sex, nationality, native place or birth-
day is not matched, these two persons are differ-
ent. 
There are profession, co-occurrence per-
sonal name and co-occurrence common terms 
left. For two persons whose name is same, we 
apply rule 1 and 2 first. If both of the two rules 
are not activating, compute the similarity Simposi-
tion(X, Y), cosname(X, Y), costerm(X, Y). And then 
synthesize these three similarities. 
Assume the three factors profession, co-
occurrence personal name and co-occurrence 
common terms are independent, and adopt Stan-
ford certainty theory to synthesize the three 
similarities. The Stanford certainty theory cre-
ates confidence measures and some simple rules 
for combing these confidences. Assume E1, E2, 
E2 are the Stanford certainty factors of event B, 
and CF represent the confidence, then the confi-
dence of event B is : 
)3()2()1()3()2()3(
)1()2()1()3()2()1()(
ECFECFECFECFECFECF
ECFECFECFECFECFECFBCF
??+??
????++=
 
For example, if the confidence of the three 
factors for event B is respectively: 88%, 74%, 
66%, then the confidence for event B is 88??
74??66??88??74??88??66??76??
66??88??74??66??98.93?. 
To compute the confidence of the factors, 
we should get the threshold (represented by ui) 
of the similarity for factors. If the similarity of 
the factor reaches the threshold, its confidence is 
100%: 
i
E
i u
sim
ECF i=)(  ]1,0[)( ?iECF  
The training method is: clustering training 
data according to the single factor, select the 
threshold with which the recall is higher with the 
premise that the precision is not lower than 98%. 
We get three thresholds 3, 0.5, 0.25 respectively 
for factor profession, co-occurrence personal 
name and co-occurrence common terms.  
Overall, the algorithm takes two steps: 
1) Adopt rule 1 and 2 to group the persons to 
the original clusters 
2) Adopt agglomerative hierarchical cluster-
ing algorithm to clustering these original 
clusters. 
(1) Take each original cluster as a single 
cluster 
(2) Select two clusters which are most 
likelihood and merge to one cluster 
(3) If there is only one cluster or reaches 
stop criteria, exit. Else, go to step (2). 
In the process of merging the clusters, we 
should merge the fragment of person. For term 
field vector, we simply compute the average of 
the term weights. For attribute feature, we adopt 
rule method to merge two clusters. 
6 Task 
We would introduce the operation of some dif-
ferent track in this section. 
In formal test, we first get a query name 
and its all files. Then we segment these files and 
extract the related information of our person 
model and output to files. At last, we cluster 
these person models and output to result xml. 
In the diagnosis test, the basic process is 
same to the formal test. The difference is that the 
element of clustering is changed to the subfolder 
of a real name. When all the subfolders are clus-
tered for a query name, we merge the results to 
one xml file. 
 
B-Cubed P-IP  
precision recall F score P IP F score 
Formal test 80.2 68.75 68.4 86.12 76.37 77.54 
Diagnosis test 94.62 63.32 72.48 96.44 72.78 80.85 
Table 2 Evaluation result of Personal Disambiguation 
7 Conclusion 
Through the first bakeoff, we have learned 
much about the development in Chinese per-
sonal name recognition and person disambigua-
tion. At the same time, we really find our prob-
lems during the evaluation. The bakeoff is inter-
esting and helpful. We look forward to partici-
pate in forthcoming bakeoff. 
References 
 
ZHANG Hua-Ping, LIU Qun, YU Hong-Kui, 
CHENG Xue-Qi, BAI Shuo.  Chinese Named En-
tity Recognition Using Role Model. International 
Journal of Computational Linguistics and Chinese 
language processing, 2003,Vol. 8 (2) 
Azzam Saliha, Kevin Humphreys & Robert Gai-
zauskas. Coreference resolution in a multilingual 
information extraction. In the Proc. of the Work-
shop on Linguistic Coference. Granada, 
Spain.1998. 
Yu Manquan.  Research on Knowledge Mining in 
Person Tracking.  Ph.D.Thesis of GUCAS. 2006 
