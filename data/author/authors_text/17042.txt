Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1396?1404,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Device-Dependent Readability for Improved Text Understanding
A-Yeong Kim Hyun-Je Song Seong-Bae Park Sang-Jo Lee
School of Computer Science and Engineering
Kyungpook National University
Daegu, 702-701, Korea
{aykim,hjsong,sbpark}@sejong.knu.ac.kr, sjlee@knu.ac.kr
Abstract
Readability is used to provide users with high-
quality service in text recommendation or text
visualization. With the increasing use of hand-
held devices, reading device is regarded as
an important factor for readability. There-
fore, this paper investigates the relationship
between readability and reading devices such
as a smart phone, a tablet, and paper. We sug-
gest readability factors that are strongly related
with the readability of a specific device by
showing the correlations between various fac-
tors in each device and human-rated readabil-
ity. Our experimental results show that each
device has its own readability characteristics,
and thus different weights should be imposed
on readability factors according to the device
type. In order to prove the usefulness of the
results, we apply the device-dependent read-
ability to news article recommendation.
1 Introduction
Readability is a function that maps a given text into a
readability score by considering ?how easily the text is
read and understood? (Richards et al., 1992; Zamanian
and Heydari, 2012). Normally, the readability score is
formulated as a combination of various factors. These
factors reflect the easiness and understanding of the
text and include text presentation format, font size, av-
erage ratio of annotated images, and sentence length
(Hasegawa et al., 2008; Kitson, 1927; Ma et al., 2012;
?
Oquist, 2006). Therefore, readability can be used to
provide satisfiable services in text recommendation or
text visualization.
The study on readability has begun in the education
field to measure the level of a text. With the success
of using readability in education (Franc?ois and Fairon,
2012; Heilman et al., 2008; Ma et al., 2012), read-
ability has been used in a range of domains recently.
For example, in document retrieval, readability is used
to provide documents to non-expert users so that they
can read the retrieved documents easily (Jameel et al.,
2012; Yan et al., 2006). In text mining, readability has
been employed to analyze the characteristics of text.
Especially, Hillbom showed the differences in readabil-
ity between broadsheet newspapers and tabloids that
share a similar political stance (Hillbom, 2009).
There is one important issue of readability that has
not been studied in natural language processing. It is a
reading device. That is, previous studies focused only
on text printed on paper. However, with the increasing
use of hand-held devices, people in these days use var-
ious reading devices such as a tablet and a smart phone
as well as a paper. Readability score can be different
according to the device type, because each device has
its own idiosyncrasy. For example, assume that a sys-
tem recommends the same news article to both user A
who reads it in her smart phone and user B who reads
it on paper. Although both users read the same article,
user A might believe that her article is more difficult to
read than user B because of the screen size of her smart
phone.
This paper explores the relationship between reading
devices and readability. For this purpose, we first inves-
tigate whether readability changes according to device
type or not. Then, we analyze which readability fac-
tors are affected by reading devices. To see the rela-
tionship between readability factors and devices, var-
ious well-known readability factors are computed for
news articles collected from an Internet portal. At the
same time, the readability of each article is also man-
ually rated. When the readability is rated manually, it
is done three times for different reading devices of a
smart phone, a tablet, and paper. The factors that af-
fect the readability actually in each device are found
out through the correlations between the factors and the
manually-labeled readability. Some factors are impor-
tant to the readability of smart phone, but insignificant
to that of paper. Therefore, we discover the importance
of each readability factor for each device by analyzing
the correlations.
The usefulness of the device-dependent readability
is proven by applying it to news article recommenda-
tion. That is, different importance weights for read-
ability factors are considered according to device type
when recommending news articles. Our experimental
results show that the performance of news article rec-
ommendation gets best when the device used for read-
ing news articles is identical to the device used for mea-
suring readability. Therefore, it is essential to consider
different importance weights according to device type
1396
in news article recommendation. It also proves that
the proposed device-dependent readability reflects the
characteristics of reading devices well.
The rest of this paper is organized as follows. We
first review related studies on readability. Next, we
introduce various readability factors and propose the
device-dependent readability. Then, the news article
recommendation using the device-dependent readabil-
ity is explained. This recommendation is prepared to
prove the usefulness of the device-dependent readabil-
ity. In the experiments, we present the experimental
results on the relationship between reading devices and
readability. We also describe the experiments on news
recommendation using the device-dependent readabil-
ity and present their results. Finally, we summarize our
research.
2 Related work
The history of readability studies began in the 1800s.
Early studies focused on the frequency of easy words,
sentence length, and word length (Huld?en, 2004).
Flesch designed a formula to calculate ?reading ease?
using only the average word length and sentence length
(Flesch, 1948). He adjusted the relative importance
between word length and sentence length using 100
words selected randomly from a corpus. This formula
is called the Flesch-Kincaid formula, and is generally
used in measuring the readability of a textbook (Kin-
caid et al., 1975). Dale and Chall (1949) defined a list
of 3,000 easy words. Then, they used the average sen-
tence length and the percentage of words not included
in the list. These studies simply used superficial fac-
tors, and thus do not reflect syntactic factors.
Recent studies on readability use various factors in-
cluding syntactic ones, and combine them to produce
a highly predictive model of readability. Franc?ois and
Faircon (2012) proposed a readability formula with 46
textual factors for French as a foreign language. The
factors represent lexical, syntactic, and semantic char-
acteristics of sentences, and the specificities of French.
They are extracted from 28 French Foreign Language
(FFL) textbooks written for adults learning FFL. On the
other hand, Pitler and Nenkova (2008) showed the rela-
tion between readability factors and readability. They
used human ratings from the Wall Street Journal cor-
pus, and computed the correlations between the read-
ability factors and the average human ratings. Accord-
ing to their results, the average number of verb phrases
in a sentence, the number of words in an article, the
likelihood of the vocabulary, and the likelihood of the
discourse relations are highly correlated with human
ratings. However, these studies did not consider the
reading devices, but focused on how well a text is writ-
ten. Since the readability can be differentiated accord-
ing to reading device, a reading device should be con-
sidered when computing the readability of a given text.
To the best of our knowledge, there are few studies
on the readability on mobile devices that do not con-
sider language-related aspects. Most studies on mobile
devices focused on the development of new text format
and layout to help users read documents easily.
?
Oquist
(2006) proposed a new text presentation format called
the dynamic Rapid Serial Visual Presentation. Accord-
ing to his experimental results, this format helps to re-
duce eye movements. On the other hand, Hasegawa
et al. (2008) evaluated the readability of documents
on mobile devices with regard to screen and font size.
They reported that the readability is improved when the
characters are vertically enlarged. Readability on mo-
bile devices is not reflected only by the visualization
factors, but also by textual factors. Therefore, this pa-
per explores the readability factors that reflect the lexi-
cal and grammatical complexity of text and are affected
by reading devices.
3 Readability Factors
Table 1 lists the readability factors used in this paper.
Basically, they are based on the factors proposed by
Pitler and Nenkova (2008). However, some factors are
excluded and some new factors are added. This is be-
cause some of their factors are computationally infeasi-
ble and language-dependent. As a result, we have thir-
teen readability factors. These readability factors are
divided into four types: superficial, lexical, syntactic
factors, and lexical cohesion.
3.1 Superficial Factors
Superficial factors were used in most early readability
studies (Dale and Chall, 1949; Flesch, 1948; Kincaid et
al., 1975), and reflect the construction of a text. We in-
vestigate four factors: text length (TL), sentence length
(SL), average number of words per sentence (WS), and
average number of characters per word (CW). Since
longer text is perceived as ?harder-to-read? than short
one, these factors are all reciprocally related with read-
ability.
The first two factors are related to length. TL counts
the number of characters in a text, whereas SL com-
putes the number of sentences. When a writer attempts
to write many topics in a text, she tends to use many
kinds of words simultaneously. As a result, the text be-
comes longer and more complex. Such long length of
text disturbs a reader?s comprehension of the text, and
then it is more difficult for the reader to read the text
(Heilman et al., 2008).
WS counts the average number of words per sen-
tence, and CW reflects the average number of characters
per word. When they are large, the sentence is diffi-
cult to read, which leads to difficulties in understanding
the text. Especially, CW reflects compound nouns and
technical words. For instance, compound nouns in Ko-
rean are usually long, because there is no spacing be-
tween words in a compound noun. For example, let us
consider a compound noun, ?Daehanmingukjungboo,?
which means the Korean government. Actually this
compound noun consists of two independent nouns.
1397
Type of Factors Abbr. Description
Superficial factors
TL The number of characters in a text
SL The number of sentences in a text
WS Average number of words per sentence
CW Average number of characters per word
Lexical factor LL Article likelihood estimated by language model
Syntactic factors
PTD Average parse tree depths per sentence
NP Average number of noun phrases per sentence
VP Average number of verb phrases per sentence
SBAR Average number of subordinate clauses per sentence
Lexical cohesion
COS Average cosine similarity between pairs of adjacent sentences
WO Average word overlap between pairs of adjacent sentences
NPO Average word overlap over noun and pronoun only
PRP Average number of pronouns per sentence
Table 1: Description of readability factors
One is ?Daehanminguk? meaning Korea and the other
is ?Jungboo? meaning a government. The two are con-
catenated to form a compound noun and become a long
single word. In addition, many difficult words such as
domain-specific terms tend to be long. Such lengthy
words make it difficult to read a text.
3.2 Lexical Factor
Lexical factor determines whether a given text con-
sists of frequent words. Texts that express a new trend
in various fields often use many newly coined words.
Such neologisms make it difficult to read and under-
stand a text. Therefore, an easily-understandable text
is composed of widely-used words rather than unusual
words.
In order to compute the use of frequent words in a
text, a unigram language model is used as in the work
of Pitler and Nenkova (2008). In this model, the log
likelihood of text t is computed by
?
w?t
C(w) ? logP (w|B). (1)
where P (w|B) is the probability of a word w according
to a background corpus B, and C(w) is the number of
times that w appears in t.
This factor examines the familiarity of the words
used in the text. The more frequently a word appears
in the background corpus, the more familiar it is re-
garded. The frequency of a word w is then reflected
into P (w|B) computed from the independent back-
ground corpus B. Therefore, the factor LL is positively
related with readability.
3.3 Syntactic Factors
Syntactic factors reflect sentence complexity directly
that affects human processing of a sentence. We con-
sider the average parse tree depth per sentence (PTD),
the average number of noun phrases per sentence (NP),
the average number of verb phrases per sentence (VP),
and the average number of subordinate clauses per sen-
tence (SBAR) as syntactic factors. These four factors
were defined by Schwarm and Ostendorf (2005).
A reader regards a text as difficult when the sen-
tences in the text have large parse tree depths or many
subordinate clauses. Thus, PTD and SBAR are related
negatively with readability. On the other hand, the re-
lationship of NP and VP to readability are not one way.
The large number of noun phrases in a text requires
a reader to remember more items (Barzilay and Lap-
ata, 2008; Pitler and Nenkova, 2008). However, it also
makes the text more interesting. The texts written for
adults actually contain more entities than those writ-
ten for children (Barzilay and Lapata, 2008). The same
is true for VP. The large number of verb phrases in a
sentence makes the sentence more complex. However,
people feel that a text is more easier to comprehend
when related clauses are grouped together (Bailin and
Grafstein, 2001).
3.4 Lexical Cohesion
Lexical cohesion denotes how the sentences in a text
are semantically connected. People usually bring con-
tinuous sentences into their mind at the same time, and
interpret them as a single unit (Okazaki et al., 2005). In
other words, a reader prefers text whose sentences are
smoothly connected to text whose sentences are inde-
pendent of one another. Therefore, sentence continuity
plays a primary role in understanding an entire text.
In the classic study of cohesion, various uses of
cohesive elements such as pronouns, definite articles,
and topic continuity have been discussed (Halliday and
Hasan, 1976). This paper uses the average cosine sim-
ilarity (COS), word overlap (WO), word overlap over
just nouns and pronouns (NPO) between pairs of adja-
cent sentences, and the average number of pronouns per
sentence (PRP). COS, WO, and NPO are superficial mea-
sures of topic continuity, whereas PRP is an indicative
feature of sentence continuity. High values for these
factors imply that the sentences in the text are related
somehow. Therefore, these factors are believed to be
related positively with readability.
1398
3.5 Measurement of Readability
When a reading device d is given, the readability of
text t, represented as R(t|d), is formulated as a com-
bination of readability factors with their corresponding
weight in the device. We assume that w
i|d
, the weight
of a readability factor f
i
, is dependent on the reading
device d. Following the previous work of Pitler and
Nenkova (2008), we also assume that each readabil-
ity factor affects readability independently. Therefore,
readability is calculated as a weighted linear sum of all
readability factors. That is, R(t|d) is computed by
R(t|d) =
?
i?{1,2,...,M}
w
i|d
? f
i
(t) (2)
where M is the number of readability factors.
Each weight w
i|d
is determined from a set of news
articles T . We collected a large number of news arti-
cles from an Internet news portal. The readability of
each article was manually labeled. This is done three
times, since we have three different devices of a smart
phone, a tablet, and paper. Since human rating of each
article t ? T is available for each device, w
i|d
?s can
be estimated by linear regression. These weights are
different according to the devices.
4 News Article Recommendation by
Device-Dependent Readability
The fact that the weights w
i|d
in Equation (2) are differ-
ent for each device d implies that the readability mea-
surement should be different depending on the device
type. In order to see the usefulness of this device-
dependent readability, we apply it to news article rec-
ommendation. News article recommendation aims to
provide a user with news articles that interest the user.
Thus, it selects a few articles that meet user preference
from a gigantic amount of news events. Various meth-
ods have reported notable results in news article rec-
ommendation (Das et al., 2007; Li et al., 2010; Liu et
al., 2010). In addition, with the recent interest in hand-
held devices, the demand for news recommendation on
hand-held devices is increasing. However, there has
been, at least as far as we know, no study on the read-
ability of hand-held devices.
Device-dependent readability is reflected into news
article recommendation through a re-ranking frame-
work. Figure 1 depicts the overall process of suggest-
ing news articles for a specific device with the device-
dependent readability. The point of this figure is to
measure how appropriate a news article is for a spe-
cific reading device. For this, a news recommendation
system first chooses a set of news articles from a news
repository based on its own criterion. Then, we re-rank
them by the device-dependent readability to obtain the
final set of ranked news articles for the device.
Formally, a news article recommendation ranks a set
of articles, A = {a
1
, a
2
, ..., a
m
}, where a
i
represents
the i-th article. The order between ranks a
1
 a
2

Min Max Average
Article length 68 610 346.5
# of sentences 1 14 6.24
# of words per sentence 8 33 16.93
# of words per article 17 178 99.34
Table 2: Statistics of the news article data
...  a
m
should be satisfied by the criterion of the
recommendation system. That is, assuming that the
system has a score function score(a
i
), score(a
i
) >
score(a
j
) has to be met if a
i
 a
j
. Then, the top
k(k ? m) articles of A by the score function are sug-
gested as appropriate news articles. After that, the se-
lected articles are re-ranked by another criterion, the
device-dependent readability. That is, the final rank of
an article within the selected set is determined by an-
other function, rerank. Since this function has to re-
flect the device-dependent readability, it takes two pa-
rameters. One is an article, and the other is a device
type. The re-rank function is modeled as
rerank(a, d) = R(a|d)
=
?
i?{1,2,...,M}
w
i|d
? f
i
(a). (3)
As a result, the readability-based re-ranking module
suggests the news articles based on how easily the ar-
ticles are read on a specific reading device. Note that
even the same article would be ranked differently ac-
cording to the device type because the article is re-
ranked by the device-dependent readability. At last, the
top k
?
(k
?
? k) re-ranked articles among them are sug-
gested as final news articles.
5 Experiments
5.1 Experiments on Readability Factors
5.1.1 Experiment Settings
For the experiments of analyzing relationship between
readability factors and readability, we collected a Ko-
rean news corpus from Naver News
1
. This corpus con-
tains news articles from June 10, 2013 to June 25,
2013. We selected 74 articles randomly from the cor-
pus which were used for readability formula and show-
ing the relationships between readability factors. All
selected articles belong to one of three categories: ?Pol-
itics?, ?Entertainment?, and ?Sports?. A set of these 74
news articles becomes T , and is used to compute the
weights in Equation (2). Table 2 describes a simple
statistics of the selected news articles. The shortest ar-
ticle consists of 68 characters, whereas the longest one
has 610 characters. The average length of article is
346.5. The shortest article is written in one sentence,
and the longest has 14 sentences. One article has ap-
proximately 6.24 sentences on average. In addition, the
1
A Korean news portal of which web address is
http://news.naver.com.
1399
News
Repository
Device
dependent
re-ranking
News
Articles
News
Recommendation
System
Readability
Figure 1: Overall process of re-ranking news articles based on device-dependent readability
number of words per sentence ranges from 8 to 33, and
the average is 16.93. The minimum number of words
in an article is 17, and the maximum number of words
is 178. An article is composed of 99.34 words on aver-
age.
In order to compute the lexical factor LL by Equa-
tion (1), a background corpus B is required. Since this
corpus should be independent from the news articles
explained above, the Naver News is adopted again to
generate B. For the background corpus B, we col-
lected news articles from January 1, 2013 to September
6, 2013, but excluded the articles from June 10 to June
25, because they are already used. This corpus consists
of 298,729 articles with 3,264,104 distinct words.
The readability score for each article was manually
labeled by three undergraduate students. To investigate
the relationship between reading devices and readabil-
ity, each article was read using three different reading
devices. The Galaxy Note 1 with a 5-inch screen is
used as the smart phone, Galaxy Tab 10.1 with a 10.1-
inch screen is used as the tablet, and A4-size paper
is used for the paper. That is, the human annotators
read and rated 74 articles per device. The order of the
devices where the annotators evaluated readability is
smart phone, tablet, and paper. This order was main-
tained for all the experiments. All aspects but content
texts were under control. For instance, font = ?Gothic,
12 pt? (this is most commonly used font and size that
most Korean web pages and textbooks use), font color
= ?black?, alignment = ?both? were used for all three
devices. In addition, the non-content aspects were ex-
actly same for devices because the annotators of read-
ability and the recommended articles shared the read-
ing devices. Although these aspects affect readability
and many previous studies already proved it, it is not
our concern. We only attempt to capture how read-
Reading device Min Max Average
Smart phone 1.67 5 3.423 ? 0.741
Tablet 1.33 5 3.531 ? 0.837
Paper 2 5 3.360 ? 0.594
Table 3: Readability scores given by human annotators
ability is affected by the content in different types of
devices.
Human annotators can remember the content of
news articles when they read articles with three de-
vices. The human annotators were asked to read and
evaluate many articles within a relatively short period.
Therefore, before the main experiments, we performed
a pilot experiment on the memory effects of previously
read articles and verified it empirically. We hired three
undergraduate students who were not involved in our
main experiments. The students read the same 250 ar-
ticles four times, and these also come from Naver News
corpus which are not included the previous 74 articles.
After their first reading, they read the articles again in
3, 7, and 14 days later. After 3 days, two students re-
membered the articles somewhat, but one student re-
membered them vaguely. Since they almost forgot the
articles after 7 days, we placed 7 days interval between
devices.
The readability score of an article was rated by the
annotators using the questions in the work of Pitler and
Nenkova (2008). We use only two of the questions,
while they used four questions for the annotators. Their
questions are intended to measure the extent of how
well a text is written, how it fits together, how easy
it is to understand, and how interesting it is. We can
consider ?well-written? and ?fit-together? as a syntac-
tic perspective, whereas ?easy to understand? and ?in-
teresting? belong to a content perspective. For such a
1400
Smart phone Tablet Paper
Factor Value Factor Value Factor Value
SL -0.394 SL -0.370 NP 0.298
TL -0.293 WS 0.321 WS 0.278
WS 0.288 LL 0.253 LL 0.268
LL 0.249 NP 0.240 VP 0.244
Table 4: Pearson correlation coefficients of important
readability factors
reason, four questions can be summarized in two ques-
tions. The two questions used are
? How well-written is this article?
? How interesting is this article?
For these two questions, each annotator assigns a score
between 1 and 5 to each article. Here, 1 point means
that the article is worst and 5 point implies that it is
best. A readability score of one human annotator is
composed with the average of two questions (well-
written, interesting). We used the average of three hu-
man annotators? readability scores in our experiments.
Table 3 shows the readability scores of the articles for
each device. According to this table, the readability
score ranges from 1.67 to 5 for the smart phone, 1.33
to 5 for the tablet, and ranges from 2 to 5 for the paper.
The average readability is 3.423 for the smart phone,
3.531 for the tablet, and 3.360 for the paper. To see
the inter-judge agreement among annotators, the Kappa
coefficient (Fleiss, 1971) is used. The Kappa values
for the ?smart phone?, ?tablet?, and ?paper? are 0.342,
0.333, and 0.361, respectively. All these values corre-
spond to fair agreement.
5.1.2 Experimental Results
In order to see the importance of each factor in a spe-
cific device, we adopt the Pearson correlation coeffi-
cients between readability factors and reading devices.
Table 4 lists the four most important factors in each
device and their Pearson correlation coefficients. Espe-
cially, p-value is smaller than 0.05 for all factors in this
table.
For the smart phone, SL, the number of sentences in
a text, is the most important readability factor. Its cor-
relation with the smart phone is -0.394. TL, the number
of characters, is the second important factor and has a
negative correlation of -0.293. These results imply that
readers are negatively sensitive to the length of an arti-
cle because of the small display size of a smart phone.
That is, in the smart phone, longer articles are recog-
nized as difficult to read compared to shorter ones. The
number of words per sentence, WS, is the third impor-
tant factor with correlation of 0.288. The log-likelihood
of an article, LL, is also positively related with the read-
ability, which proves that widely-used words make it
easy to understand an article. The top three factors are
superficial with regard to text length. Therefore, the su-
perficial factors are more important than other types of
factors for the smart phone.
SL is the most critical readability factor even for the
tablet. It affects readability with high correlation of -
0.370. The second important factor is WS with correla-
tion of 0.321. Both of these factors are superfical. The
third important factor, LL, is positively related with
readability as expected. The fourth factor that affects
readability is the number of noun phrases, NP. It is nat-
ural for NP to be positively related with the readability.
Finally, for the paper, NP is most strongly related to
readability with correlation of 0.298. The second im-
portant factor is WS, whose correlation is 0.278. LL is
the third important factor and shows a positive relation-
ship. Note that WS and LL are important readability
factors for all devices. The next important readabil-
ity factor for the paper is the average number of verb
phrases (VP). The articles with many noun phrases and
verb phrases are perceived as easier-to-read for the pa-
per. Note that the importance of superficial factors is
limited for the paper. We expected that WS is negatively
related, but, it is positively related with readability for
all three devices. The reason for this could be that the
annotators thought the articles with higher WS are more
interesting.
The important factors for the smart phone are differ-
ent from those for the paper. On the other hand, the
tablet shares many factors with both the smart phone
and the paper. Because the screen size of a tablet is
similar to the size of an A4 paper, the tablet and the pa-
per share readability factors. However, length-related
factors play a more important role than syntactic fac-
tors in the smart phone because a smart phone has a
smaller screen.
5.2 Experiments on News Recommendation
5.2.1 Experiment Settings
Experiments for news article recommendation were
performed to see the effectiveness of device-dependent
readability. The process of news recommendation with
device-dependent readability is as follows. For a spe-
cific device,
1. Select top-k news articles from a news repository
by the criterion of the recommendation system.
2. Re-rank the k articles by the readability of the de-
vice using Equation (3).
3. Select top-k
?
news articles by the new rank.
4. Human annotators read and rate the k
?
articles
with the device.
5. Compare the ranks of k
?
articles by device-
dependent readability with those by human rat-
ings.
Since we have three types of devices, this process is
performed three times with a different device.
The news articles from September 10, 2013 to
September 12, 2013 collected from Naver News were
1401
Min Max Average
Article length 277 6,077 990.68
# of sentences 4 199 22.85
# of words per sentence 4 100 15.73
# of words per article 71 2,034 301.61
Table 5: Statistics of news data for recommendation
Reading device Min Max Average
Smart phone 1 5 3.513 ? 0.962
Tablet 1 5 3.344 ? 0.852
Paper 1 5 3.250 ? 0.907
Table 6: Scores of news articles by human annotators
in news recommendation
used as the news repository. The number of times that
a news article was actually read by its anonymous read-
ers at the portal site is used as the criterion for the rec-
ommendation system. Since this criterion is provided
on a daily basis and news articles were collected for
three days, the process explained above is performed
three times. The top twenty articles were selected by
the criterion every day. That is, k = 20. Table 5 shows
the statistics of the total 60 articles. The shortest arti-
cle consists of 277 characters, and the longest article
has 6,077 characters. On average, an article is writ-
ten with 990.68 characters. The minimum number of
sentences in an article is 4, and the maximum number
of sentences is 199. An article is composed of 22.85
sentences on average. The average number of words in
a sentence is 15.73, whereas a sentence length ranges
from 4 to 100 words. The shortest article has 71 words,
and the longest article has 2,034 words. One article has
approximately 301.61 words on average.
Three human annotators labeled the scores of the
news articles manually. The annotators were the same
persons who labeled the readability scores. Similar to
the previous experiments, 7 days intervals was placed
among devices to reduce the memory effect. The same
two questions used in the previous section were used
again for this experiment. The annotators assigned a
score between 1 and 5 to every article for each ques-
tion. The final score of an article was obtained by aver-
aging six scores (two questions from three annotators).
Table 6 summarizes the scores of the articles by the
human annotators. As shown in this table, the article
scores vary for all reading devices. The average scores
for smart phone, tablet, and paper are 3.513, 3.344,
and 3.250 respectively. The Kappa value for the ?smart
phone? is 0.402, and that for both the ?tablet? and the
?paper? is 0.393. Thus, the value of ?smart phone? falls
into moderate agreement, whereas those of the ?tablet?
and ?paper? correspond to fair agreement. The perfor-
mance of the news article recommendation is evaluated
with the Normalized Discounted Cumulative Gain at
top P (NDCG@P ) (J?arvelin and Kek?al?ainen, 2002).
Figure 2: NDCG@k
?
scores with various k
?
for the
smart phone.
Figure 3: NDCG@k
?
scores with various k
?
for the
tablet.
5.2.2 Experimental Results
For the a baseline criterion, we use the news article
recommendation system in Naver, which recommends
news article by the number of article hits. Figures 2 to 4
show the NDCG@k
?
scores with 1 ? k
?
? 10 for the
three devices. Each graph in these figures compares the
performance of various devices when the readability
for a specific device is used. That is, Figure 2 depicts
the NDCG@k
?
scores for the recommended news arti-
cles when the articles are shown in the smart phone, the
tablet, and the paper respectively. In computing their
NDCG@k
?
scores, the news articles are re-ranked by
readability for the smart phone. Therefore, in this fig-
ure we expect that the NDCG@k
?
score for using the
smart phone is higher than those for using the tablet and
paper. In the same way, Figure 3 and Figure 4 compare
the NDCG@k
?
scores when the readabilities for the
tablet and paper are used.
In all three graphs, the best news recommendation
performance is achieved when the device used to read
1402
Figure 4: NDCG@k
?
scores with various k
?
for the
paper.
news articles is the same as the device used for read-
ability. In Figure 2, the use of the smart phone outper-
forms those of other devices when k
?
? 6. This proves
that the quality of highly ranked news articles is much
better for the smart phone than for other devices, when
the readability for smart phone is used.
Figure 3 shows the NDCG@k
?
scores for using var-
ious devices when the news articles are re-ranked by
readability for the tablet. In this figure, the use of
the tablet as a reading device is better than using the
smart phone or the paper. The performance difference
is largest at k
?
= 3. The difference becomes smaller
as k
?
increases up to 10, but the performance of tablet
is still higher than those of others. In Figure 2 and 3,
when k
?
= 1, the baseline outperforms other devices.
We believe this happens because the baseline chooses
news articles by user-hit. Therefore, many articles rec-
ommended by the baseline are interesting because peo-
ple tend to click more often when an article is inter-
esting. As noted, readability reflects users? interests,
which leads to high performance of the baseline. The
performance of paper is best in Figure 4, since the ar-
ticles are re-ranked by the readability for paper. Paper
outperforms all other devices for all k
?
s. Note that the
performances of the baseline are always lowest regard-
less of reading device.
From all results above, we can infer that the use of
device-dependent readability is helpful to news article
recommendation. This is because the readability fac-
tors that affect the readers of news articles are different
according to the reading device. Therefore, it is im-
portant to reflect the characteristics of a reading device
when recommending news articles.
6 Conclusion
In this paper, we have proposed a device-dependent
readability. Since a reading device is one of the most
important features of readability, different weights have
been assigned to the readability factors according to de-
vice type. We have shown that the important readabil-
ity factors are distinct according to the reading device
by investigating the correlation between the readability
factors and the reading device. Through the correlation,
we found that tablet shares many important factors with
both smart phone and paper.
The experiments on the news articles collected from
an Internet portal proved that readability is actually af-
fected by the reading device. In addition, the validity of
the device-dependent readability was shown by apply-
ing it to the news article recommendation. The news
articles were first ranked by the criterion of the recom-
mendation system. Then, they were re-ranked by the
device-dependent readability. Our experiments showed
that the recommendation performance of the re-ranked
articles gets best when the device used for readability is
the same as the reading device. These two types of ex-
periments proved the importance and effectiveness of
the device-dependent readability.
Acknowledgments
This work was supported by the IT R&D program of
MSIP/KEIT (10044494, WiseKB: Big data based self-
evolving knowledge base and reasoning platform) and
the Industrial Strategic Technology Development Pro-
gram (10035348, Development of a Cognitive Planning
and Learning Model for Mobile Platforms) funded by
the Ministry of Knowledge Economy(MKE, Korea).
References
Alan Bailin and Ann Grafstein. 2001. The linguistic
assumptions underlying readability formulae: A cri-
tique. Language & Communication, 21(3):285?301.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Edgar Dale and Jeanne Chall. 1949. The concept of
readability. Elementary English, 26(1):19?26.
Abhinandan Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personaliza-
tion: scalable online collaborative filtering. In Pro-
ceedings of the 16th International Conference on
World Wide Web, pages 271?280.
Joseph Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological bulletin,
76(5):378?382.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of Applied Psychology, 32(3):221?233.
Thomas Franc?ois and C?edrick Fairon. 2012. An
AI readability formula for French as a foreign lan-
guage. In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 466?477.
1403
Michael Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman Group Ltd.
Satoshi Hasegawa, Kazuhiro Fujikake, Masako Omori,
and Masaru Miyao. 2008. Readability of charac-
ters on mobile phone liquid crystal displays. In-
ternational Journal of Occupational Safety and Er-
gonomics (JOSE), 14(3):293?304.
Michael Heilman, Kevyn Collins-Thompson, and
Maxine Eskenazi. 2008. An analysis of statistical
models and features for reading difficulty prediction.
In Proceedings of the Third Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 71?79.
Kristina Hillbom. 2009. Newspaper Readability: a
Broadsheet vs. a Tabloid. Ph.D. thesis, University of
G?avle.
M?ans Huld?en. 2004. Linguistic complexity in
two major american newspapers and the associated
press newswire, 1900?2000. Master?s thesis,
?
Abo
Akademi University.
Shoaib Jameel, Wai Lam, and Xiaojun Qian. 2012.
Ranking text documents based on conceptual dif-
ficulty using term embedding and sequential dis-
course cohesion. In Proceedings of the The 2012
IEEE/WIC/ACM International Joint Conferences on
Web Intelligence and Intelligent Agent Technology-
Volume 01, pages 145?152.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cu-
mulated gain-based evaluation of IR techniques.
ACM Transactions on Information Systems (TOIS),
20(4):422?446.
J. Peter Kincaid, Robert Fishburne Jr., Richard Rogers,
and Brad Chissom. 1975. Derivation of new read-
ability formulas (automated readability index, fog
count and flesch reading ease formula) for navy en-
listed personnel. Technical report, DTIC Document.
Harry Kitson. 1927. The mind of the buyer. MacMil-
lan Company.
Lihong Li, Wei Chu, John Langford, and Robert E.
Schapire. 2010. A contextual-bandit approach to
personalized news article recommendation. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 661?670.
Jiahui Liu, Peter Dolan, and Elin R. Pedersen. 2010.
Personalized news recommendation based on click
behavior. In Proceedings of the 15th International
Conference on Intelligent User Interfaces, pages 31?
40.
Yi Ma, Eric Fosler-Lussier, and Robert Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 548?552.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru
Ishizuka. 2005. Improving chronological ordering
of sentences extracted from multiple newspaper ar-
ticles. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4(3):321?339.
Gustav
?
Oquist. 2006. Evaluating readability on mo-
bile devices. Ph.D. thesis, Uppsala University.
Emily Pitler and Ani Nenkova. 2008. Revisiting read-
ability: A unified framework for predicting text qual-
ity. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
186?195.
Jack Richards, John Platt, Heidi Platt, and Christophe
Candlin. 1992. Longman Dictionary of Language
Teaching and Applied Linguistics, volume 78. Long-
man London.
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 523?530.
Xin Yan, Dawei Song, and Xue Li. 2006. Concept-
based document readability in domain specific infor-
mation retrieval. In Proceedings of the 15th ACM In-
ternational Conference on Information and Knowl-
edge Management, pages 540?549.
Mostafa Zamanian and Pooneh Heydari. 2012. Read-
ability of texts: State of the art. Theory and Practice
in Language Studies, 2(1):43?53.
1404
Proceedings of NAACL-HLT 2013, pages 888?896,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Just-In-Time Keyword Extraction from Meeting Transcripts
Hyun-Je Song Junho Go Seong-Bae Park Se-Young Park
School of Computer Science and Engineering
Kyungpook National University
Daegu, Korea
{hjsong,jhgo,sbpark,sypark}@sejong.knu.ac.kr
Abstract
In a meeting, it is often desirable to extract
keywords from each utterance as soon as it is
spoken. Thus, this paper proposes a just-in-
time keyword extraction from meeting tran-
scripts. The proposed method considers two
major factors that make it different from key-
word extraction from normal texts. The first
factor is the temporal history of preceding ut-
terances that grants higher importance to re-
cent utterances than old ones, and the sec-
ond is topic relevance that forces only the pre-
ceding utterances relevant to the current utter-
ance to be considered in keyword extraction.
Our experiments on two data sets in English
and Korean show that the consideration of the
factors results in performance improvement in
keyword extraction from meeting transcripts.
1 Introduction
A meeting is generally accomplished by a number
of participants and a wide range of subjects are dis-
cussed. Therefore, it would be helpful to meeting
participants to provide them with some additional
information related to the current subject. For in-
stance, assume that a participant is discussing a spe-
cific topic with other participants at a meeting. The
summary of previous meetings on the topic is then
one of the most important resources for her discus-
sion.
In order to provide information on a topic to par-
ticipants, keywords should be first generated for the
topic since keywords are often representatives of a
topic. A number of techniques have been proposed
for automatic keyword extraction (Frank et al, 1999;
Turney, 2000; Mihalcea and Tarau, 2004; Wan et al,
2007), and they are designed to extract keywords
from a written document. However, they are not
suitable for meeting transcripts. In a meeting, it is
often desirable to extract keywords at the time at
which a new utterance is made for just-in-time ser-
vice of additional information. Otherwise, the ex-
tracted keywords become just the important words
at the end of the meeting.
Two key factors for just-in-time keyword extrac-
tion from meeting transcripts are time of preceding
utterances and topic of current utterance. First, cur-
rent utterance is affected by temporal history of pre-
ceding utterances. That is, when a new utterance
is made it is likely to be related more closely with
latest utterances than old ones. Second, the preced-
ing utterances which carry similar topics to current
utterance are more important than irrelevant utter-
ances. Since a meeting consists of several topics,
the utterances that have nothing to do with current
utterance are inappropriate as a history of the cur-
rent utterance.
This paper proposes a graph-based keyword ex-
traction to reflect these factors. The proposed
method represents an utterance as a graph of which
nodes are candidate keywords. The preceding utter-
ances are also expressed as a history graph in which
the weight of an edge is the temporal importance
of the keywords connected by the edge. To reflect
the temporal history of utterances, forgetting curve
(Wozniak, 1999) is adopted in updating the weights
of edges in the history graph. It expresses effectively
not only the reciprocal relation between memory re-
888
tention and time, but also active recall that makes
frequent words more consequential in keyword ex-
traction. Then, a subgraph that is relevant to the
current utterance is derived from the history graph,
and used as an actual history of the current utterance.
The keywords of the current utterance are extracted
by TextRank (Mihalcea and Tarau, 2004) from the
merged graph of the current utterance and the his-
tory graphs.
The proposed method is evaluated with two kinds
of data sets: the National Assembly transcripts
in Korean and the ICSI meeting corpus (Janin et
al., 2003) in English. The experimental results
show that it outperforms both the TFIDF frame-
work (Frank et al, 1999; Liu et al, 2009) and the
PageRank-based graph model (Wan et al, 2007).
One thing to note is that the proposed method im-
proves even the supervised methods that do not re-
flect utterance time and topic relevance for the ICSI
corpus. This proves that it is critical to consider time
and content of utterances simultaneously in keyword
extraction from meeting transcripts.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on keyword extrac-
tion. Section 3 explains the overall process of the
proposed method, and Section 4 addresses its de-
tailed description how to reflect meeting character-
istics. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
2 Related Work
Keyword extraction has been of interest for a long
time in various fields such as information retrieval,
document clustering, summarization, and so on.
Thus, there have been many studies on automatic
keyword extraction. The frequency-based key-
word extraction with TFIDF weighting (Frank et al,
1999) and the graph-based keyword extraction (Mi-
halcea and Tarau, 2004) are two base models for this
task. Many studies recently tried to extend them by
incorporating specific information such as linguistic
knowledge (Hulth, 2003), web-based resource (Tur-
ney, 2003), and semantic knowledge (Chen et al,
2010). As a result, they show good performance on
written text. However, it is difficult to use them di-
rectly for spoken genres, since spoken genres have
significantly different characteristics from written
text.
There have been a few studies focused on key-
word extraction from spoken genres. Among them,
the extraction from meetings has attracted more con-
cern, since the need for grasping important points
of a meeting or an opinion of each participant has
increased. The studies on meetings focused on
the exterior features of meeting dialogues such as
unstructured and ill-formed sentences. Liu et al
(2009) used some knowledge sources such as Part-
of-Speech (POS) filtering, word clustering, and sen-
tence salience to reflect dialogue features, and they
found out that a simple TFIDF-based keyword ex-
traction using these knowledge sources works rea-
sonably well. They also extended their work by
adopting various features such as decision making
sentence features, speech-related features, and sum-
mary features that reflect meeting transcripts better
(Liu et al, 2011). Chen et al (2010) extracted key-
words from spoken course lectures. In this study,
they considered prosodic information from HKT
forced alignment and topics in a lecture generated
by Probabilistic Latent Semantic Analysis (pLSA).
These studies focused on the exterior characteris-
tics of spoken genres, since they assumed that entire
scripts are given in advance and then they extracted
keywords that best describe the scripts. However, to
the best of our knowledge, there is no previous study
considered time of utterances which is an intrinsic
element of spoken genres.
The relevance between current utterance and pre-
ceding utterances is also a critical feature in keyword
extraction from meeting transcripts. The study that
considers this relevance explicitly is CollabRank
proposed by Wan and Xiao (2008). This is collabo-
rative approach to extract keywords in a document.
In this study, it is assumed that a few neighbor doc-
uments close to a current document can help extract
keywords. Therefore, they applied a clustering al-
gorithm to a document set and then extracted words
that are reinforced by the documents within a clus-
ter. However, this method also does not consider the
utterance time, since it is designed to extract key-
words from normal documents. As a result, if it is
applied to meeting transcripts, all preceding utter-
ances would affect the current utterance uniformly,
which leads to a poor performance.
889
Current
utterance 
graph (G1)
History 
graph (G2)
Subgraph (G3)
Expanded graph (G4)
Keyword 
graph (G5)
Subgraph
extraction
Expand
Keyword
extraction
Merge
Keywords
Current 
utterance
Filter
Figure 1: The overall process of the just-in-time keyword extraction from meeting transcripts.
3 Just-In-Time Keyword Extraction for a
Meeting
Figure 1 depicts the overall process of extracting
keywords from an utterance as soon as it is spo-
ken. We represent all the components in a meeting
as graphs. This is because graphs are effective to ex-
press the relationship between words, and the graph
operations that are required for keyword extraction
are also efficiently performed. That is, whenever an
utterance is spoken, it is represented as a graph (G1)
of which nodes are the potential keywords in the ut-
terance. This graph is named as current utterance
graph.
The summary of all preceding utterances is also
represented as a history graph (G2). We assume that
only the preceding utterances that are directly re-
lated with the current utterance are important for ex-
tracting keywords from the current utterance. There-
fore, a subgraph of G2 that maximally covers the
current utterance graph (G1) is extracted. This sub-
graph is labeled as G3 in Figure 1. Then, the current
utterance graph G1 is expanded by merging it and
G3. This expanded graph (G4) is a combined rep-
resentation of the current and preceding utterances,
and then the keywords of the current utterance is ex-
tracted from this graph. The keywords are so-called
hub nodes of G4.
After keywords are extracted from the current ut-
terance, the current utterance becomes a part of the
history graph for the next utterance. For this, the
extracted keywords are also represented as a graph
(G5), and it is merged into the current history G2.
This merged graph becomes a new history graph
for the next utterance. In merging two graphs, the
weight of each edge in G2 is updated to reflect the
temporal history. If an edge is connecting two nouns
from an old utterance, its weight becomes small. In
the same way, the weights for the edges from recent
utterances get large. The weights of the edges from
G5 are 1, the largest possible value.
4 Graph Representation and Weight
Update
4.1 Current Utterance Graph and History
Graph
Current utterance graph is a graph-representation of
the current utterance. When current utterance con-
sists of m words, we first extract the potential key-
890
words from the current utterance. Since all words
within the current utterance are not keywords, some
words are filtered out. For this filtering out, we fol-
low the POS filtering approach proposed by Liu et
al. (2009). This approach filters out non-keywords
using a stop-word list and POS tags of the words.
Assume that n words remain after the filtering out,
where n ? m. These n words become the vertices
of the current utterance graph.
Formally, the current utterance graph G1 =
(V1, E1) is an undirected graph, where |V1| = n.
E1 is a set of edges and each edge implies that the
nouns connected by the edge co-occur within a win-
dow sizedW . For each e1ij ? E1 that connects nodes
v1i and v
1
j , its weight is given by
w1ij =
{
1 if v1i &v
1
j cooccur within the window,
0 otherwise.
(1)
In a meeting, preceding utterances affect the cur-
rent utterance. We assume that only the keywords
of preceding utterances are effective. Therefore, the
history graph G2 = (V2, E2) is an undirected graph
of keywords in the preceding utterances. That is,
all vertices in V2 are keywords extracted from one
or more previous utterances, and the edge between
two keywords implies that they co-occurred at least
once. Every edge in E2 has a weight that represents
its temporal importance.
The history graph is updated whenever keywords
are extracted from a new utterance. This is because
the current utterance becomes a part of the history
graph for the next utterance. As a history, old ut-
terances are less important than recent ones. Thus,
the temporal importance should decrease gradually
according to the passage of time. In addition, the
keywords which occur frequently at a meeting are
more important than those mentioned just once or
twice. Since the frequently-mentioned keywords are
normally major topics of the meeting, their influence
should last for a long time.
To model these characteristics, the forgetting
curve (Wozniak, 1999) is adopted in updating the
history graph. It models the decline of memory re-
tention in time. Figure 2 shows a typical represen-
tation of the forgetting curve. The X-axis of this
figure is time and the Y-axis is memory retention.
As shown in this figure, memory retention of new
Time
Me
mo
ry
Re
ten
tio
n
Figure 2: Memory retention according to time.
information decreases gradually by the exponential
nature of forgetting. However, whenever the infor-
mation is repeated, it is recalled longer. This is for-
mulated as
R = e?
t
S ,
where R is memory retention, t is time, and S is the
relative strength of memory.
Based on the forgetting curve, the weight of each
edge e2ij ? E2 between keywords v
2
i and v
2
j is set as
w2ij = exp
? tf(vi,vj) , (2)
where t is the elapse of utterance time and f(vi, vj)
is the frequency that vi and vj co-occur from the
beginning of the meeting to now. According to
this equation, the temporal importance between key-
words decreases gradually as time passes by, but the
keyword relations repeated during the meeting are
remembered for a long time in the history graph.
4.2 Keyword Extraction by Merging Current
Utterance and History Graphs
All words within the history graph are not equally
important in extracting keywords from the current
utterance. In general, many participants discuss a
wide range of topics in a meeting. Therefore, some
preceding utterances that shares topics with the cur-
rent utterance are more significant. We assume that
the preceding utterances that contain the nouns in
the current utterance share topics with the current
utterance. Thus, only a subgraph of G2 that contain
words in G1 is relevant for keyword extraction from
G1.
891
Given the current utterance graph G1 = (V1, E1)
and the history graph G2 = (V2, E2), the relevant
graph G3 = (V3, E3) is a subgraph of G2. Here,
V3 = (V1?V2)?adjacency(V1) and adjacency(V1)
is a set of vertices from G2 which are directly con-
nected to the words in V1. That is, V3 contains
the words of G1 and their direct neighbor words in
G2. E3 is a subset of E2. Only the edges that ap-
pear in E2 are included in E3. The weight w3ij of
each e3ij ? E3 is also borrowed from G2. That is,
w3ij = w
2
ij . Therefore, G3 is a 1-walk subgraph
1 of
G2 in which words in G1 and their neighbor words
appear.
The keywords of the current utterance should re-
flect the relevant history as well as the current utter-
ance itself. For this purpose, G1 is expanded with
respect to G3. The expanded graph G4 = (V4, E4)
of G1 is defined as
V4 = V1 ? V3,
E4 = E1 ? E3.
For each edge e4ij ? E4, its weightw
4
ij is determined
to be the larger value between w1ij and w
3
ij if it ap-
pears in both G1 and G3. When it appears in only
one of the graphs, w4ij is set to be the weight of its
corresponding graph. That is,
w4ij =
?
???
???
max(w1ij , w
3
ij) if e
4
ij ? E1 and e
4
ij ? E3,
w1ij if e
4
ij ? E1 and e
4
ij /? E3,
w3ij otherwise.
From this expanded graph G4, the keywords are
extracted by TextRank (Mihalcea and Tarau, 2004).
TextRank is an unsupervised graph-based method
for keyword extraction. It singles out the key ver-
tices of a graph by providing a ranking mechanism.
In order to rank the vertices, it computes the score
of each vertex v4i ? V4 by
S(v4i ) = (1? d)+ d ?
?
v4j?adj(v
4
i )
w4ji
?
v4k?adj(v
4
j )
w4jk
S(v4j ),
(3)
1If a m-walk subgraph (m > 1) is used, more affluent his-
tory is used. However, this graph contains some words irrel-
evant to the current utterance. According to our experiments,
1-walk subgraph outperforms other m-walk subgraphs where
m > 1. In addition, extracting G3 becomes expensive for large
m.
where 0 ? d ? 1 is a damping factor and adj(vi)
denotes vi?s neighbors. Finally, the words whose
score is larger than a specific threshold ? are cho-
sen as keywords. Especially when the current utter-
ance is the first utterance of a meeting, the history
graph does not exist. In this case, the current utter-
ance graph becomes the expanded graph (G4 = G1),
and keywords are extracted from the current utter-
ance graph.
The proposed method extracts keywords when-
ever an utterance is spoken. Thus, it tries to extract
keywords even if the current utterance is not related
to the topics of a meeting or is too short. However,
if the current utterance is irrelevant to the meeting,
it has just a few connections with other previous ut-
terances, and thus the potential keywords in this ut-
terance are apt to have a low score. The proposed
method, however, does not select the words whose
score is smaller than the threshold ? as keywords.
As a result, it extracts only the relevant keywords
during the meeting.
Since the keywords for the current utterance
should be the history for the next utterance, they
have to be reflected into the history graph. There-
fore, a keyword graph G5 = (V5, E5) is constructed
from the keywords. Here, V5 is a set of keywords
extracted from G4, and E5 is a subset of E4 that
corresponds to V5. The weights of edges in E5 are
same with those in E4. That is, w5ij = w
4
ij . The key-
word graph G5 is then merged into the history graph
G2 in the same way that G1 and G3 are merged. As
stated above, the weights of the edges in the history
graph G2 are updated by Equation (2). Therefore,
before merging G5 and G2, all weights of G2 are
updated by increasing t as t + 1 to reflect temporal
importance of preceding utterances.
5 Experiments
The proposed method is evaluated with two kinds of
data sets: the National Assembly transcripts in Ko-
rean and the ICSI meeting corpus in English. Both
data sets are the records of meetings that are manu-
ally dictated by human transcribers.
892
Table 1: Simple statistics of the National Assembly transcripts
the first meeting the second meeting
No. of utterances 1,280 573
Average No. of words per utterance 7.22 10.17
5.1 National Assembly Transcripts in Korean
The first corpus used to evaluate our method is the
National Assembly transcripts2. This corpus is ob-
tained from the Knowledge Management System
of the National Assembly of KoreaIt is transcribed
from the 305th assembly record of the Knowledge
Economy Committee in 2012. Table 1 summa-
rizes simple statistics of the National Assembly tran-
scripts. The 305th assembly record actually consists
of two meetings. The first meeting contains 1,280
utterances and the second has 573 utterances. The
average number of words per utterance in the first
meeting is 7.22 while the second meeting contains
10.17 words per utterance on average. The second
meeting transcript is used as a development data set
to determine window size W of Equation (1), the
damping factor d of Equation (3), and the threshold
?. For all experiments below, d is set 0.85, W is 10,
and ? is 0.28. The remaining first meeting transcript
is used as a data set to extract keywords since this
transcript contains more utterances. Only nouns are
considered as potential keywords. That is, only the
words whose POS tag is NNG (common noun) or
NNP (proper noun) can be a keyword.
Three annotators are engaged to extract keywords
manually for each utterance in the first meeting
transcript, since the Knowledge Management Sys-
tem does not provide the keywords3. The aver-
age number of keywords per utterance is 2.58. To
see the inter-judge agreement among the annotators,
the Kappa coefficient (Carletta, 1996) was investi-
gated. The kappa agreement of the National Assem-
bly transcript is 0.31 that falls under the category of
?Fair?. Even though all congressmen in the transcript
belong to the same committee, they discussed vari-
ous topics at the meeting. As a result, the keywords
are difficult to be agreed unanimously by all three
2The data set is available: http://ml.knu.ac.kr/
dataset/keywordextraction.html
3A guideline was given to the annotators that keywords must
be a single word and the maximum number of keywords per
utterance is five.
annotators. Therefore, in this paper the words that
are recommended by more than two annotators are
chosen as keywords.
The evaluation is done with two metics: F-
measure and the weighted relative score (WRS).
Since the previous work by Liu et al (2009) re-
ported only F-measure and WRS, F-measure instead
of precision/recall are used for the comparison with
their method. The weighted relative score is de-
rived from Pyramid metric (Nenkova and Passon-
neau, 2004). When a keyword extraction system
generates keywords which many annotators agree,
a higher score is given to it. On the other hand, a
lower score is given if fewer annotators agree.
The proposed method is compared with two base-
line models to see its relative performance. One is
the frequency-based keyword extraction with TFIDF
weighting (Frank et al, 1999) and the other is Tex-
tRank in which the weight of edges is mutual in-
formation between vertices (Wan et al, 2007). In
TFIDF, each utterance is considered as a document,
and thus all utterances including the current one
are regarded as whole documents. The frequency-
based TFIDF chooses top-K words according to
their TFIDF value from the set of words appearing in
the meeting transcript. Since the human annotators
are restricted to extract up to five keywords, the key-
word extraction systems including our method are
also requested to select top-5 keywords when more
than five keywords are produced.
In order to see the effect of preceding utterances in
baseline models, the performances are measured ac-
cording to the number of preceding utterances used.
Figure 3 shows the results. The X-axis of this fig-
ure is the number of preceding utterances and the Y-
axis represents F-measures. As shown in this figure,
the performance of the baseline models improves
monotonically at first as the number of preceding
utterances increases. However, the performance im-
provement stops when many preceding utterances
are involved, and the performance begins to drop
893
Figure 3: The performance of baseline models according
to the number of preceding utterances
Table 2: The experimental results on the National Assem-
bly transcripts
Methods F-measure WRS
TextRank 0.478 0.387
TFIDF 0.481 0.394
Proposed method 0.533 0.421
when too many utterances are considered. The per-
formance of TextRank model drops from 20 preced-
ing utterances, while that of TFIDF model begins to
drops at 50 utterances. When too many preceding
utterances are taken into account, it is highly pos-
sible that some of their topics are irrelevant to the
current utterance, which leads to performance drop.
Table 2 compares our method with the baseline
models on the National Assembly transcripts. The
performances of baseline models are obtained when
they show the best performance for various number
of preceding utterances. TextRank model achieves
F-measure of 0.478 and weighted relative score of
0.387, while TFIDF reports its best F-measure of
0.481 and weighted relative score of 0.394. Thus,
the difference between TFIDF and TextRank is not
significant. However, F-measure and weighted rel-
ative score of the proposed method are 0.533 and
0.421 respectively, and they are much higher than
those of baseline models. In addition, our method
achieves precision of 0.543 and recall of 0.523 and
Table 3: The importance of temporal history
F-measure WRS
With Temporal History 0.533 0.421
Without Temporal History 0.518 0.413
this is much higher performance than TextRank
whose precision is just 0.510. Since the proposed
method uses, as history, the preceding utterances
relevant to the current utterance, its performance is
kept high even if whole utterances are used. There-
fore, it could be inferred that it is important to adopt
only the relevant history in keyword extraction from
meeting transcripts.
One of the key factors of our method is the tem-
poral history. Its importance is given in Table 3. As
explained above, the temporal history is achieved by
Equation (2). Thus, the proposed model does not
reflect the temporal importance of preceding utter-
ances if w2ij = 1 always. That is, under w
2
ij = 1,
old utterances are regarded as important as recent ut-
terances. Without temporal history, F-measure and
weighted relative score are just 0.518 and 0.413 re-
spectively. These poor performances prove the im-
portance of the temporal history in keyword extrac-
tion from meeting transcripts.
5.2 ICSI Meeting Corpus in English
The proposed method is also evaluated on the ICSI
meeting corpus (Janin et al, 2003) which consists of
naturally occurring meetings recordings. This cor-
pus is widely used for summarizing and extracting
keywords of meetings. We followed all the exper-
imental settings proposed by Liu et al (2009) for
this corpus. Among 26 meeting transcripts chosen
by Liu et al from 161 transcripts of the ICSI meet-
ing corpus, 6 transcripts are used as development
data and the remaining transcripts are used as data
to extract keywords. The parameters for the ICSI
meeting corpus are set to be d = 0.85,W = 10,
and ? = 0.20. Each meeting of the corpus consists
of several topic segments, and every topic segment
contains three sets of keywords that are annotated by
three annotators. Up to five keywords are annotated
for a topic segment.
Table 4 shows simple statistics of the ICSI meet-
ing data. Total number of topic segments in the 26
meetings is originally 201, but some of them do not
894
Table 4: Simple statistics of the ICSI meeting data
Information Value
# of meetings 26
# of topic segments 201
# of topic segments used actually 140
Average # of utterances per topic segment 260
Average # of words per utterance 7.22
Table 5: The experimental results on the ICSI corpus
Methods F-measure WRS
TFIDF-Liu 0.290 0.404
TextRank-Liu 0.277 0.380
ME model 0.312 0.401
Proposed method 0.334 0.533
have keywords. Such segments are discarded, and
the remaining 140 topic segments are actually used.
The average number of utterances in a topic segment
is 260 and the average number of words per utter-
ance is 7.22.
Unlike the National Assembly transcripts, the
keywords of the ICSI meeting corpus are annotated
at the topic segment level, not the utterance level.
Therefore, the proposed method which extracts key-
words at the utterance level can not be applied di-
rectly to this corpus. In order to obtain keywords
for a topic segment with the proposed method, the
keywords are first extracted from each utterance in
the segment by the proposed method and then they
are all accumulated. The proposed method extracts
keywords for a topic segment from these accumu-
lated utterance-level keywords as follows. Assume
that a topic segment consists of l utterances. Since
our method can extract up to 5 keywords for each
utterance, the number of keywords for the segment
can reach to 5 ? l. From these keywords, we select
top-5 keywords ranked by Equation (3).
The proposed method is compared with three pre-
vious studies. The first two are the methods pro-
posed by Liu et al (2009) One is the frequency-
based method of TFIDF weighting with the fea-
tures such as POS filtering, word clustering, and sen-
tence salience score, and the other is the graph-based
method with POS filtering. The last method is a
maximum entropy model applied to this task (Liu
et al, 2008). Note that the maximum entropy is a
supervised learning model.
Table 6: The effect of considering topic relevance
Methods F-measure WRS
With topic relevance 0.334 0.533
Without topic relevance 0.291 0.458
Table 5 summarizes the comparison results. As
shown in this table, the proposed method outper-
forms all previous methods. Our method achieves
precision of 0.311 and recall of 0.361, and thus
the F-score is 0.334. The weight relative score
of the proposed method is 0.533. This is the im-
provement of up to 0.044 in F-measure and 0.129
in weighted relative score over other unsupervised
methods (TFIDF-Liu and TextRank-Liu). It should
be also noted that the proposed method outperforms
even the supervised method (ME model). The differ-
ence between our method and the maximum entropy
model in weighted relative score is 0.132.
One possible variant of the proposed method for
the ICSI corpus is to simply merge the current utter-
ance graph (G1) with the history graph (G2) rather
than to extract keywords from each utterance. Af-
ter the current utterance graph of the last utterance
in a topic segment is merged into the history graph,
the keywords for the segment are extracted from the
history graph. This variant and the proposed method
both rely on the temporal history, but the difference
is that the history graph of the variant accumulates
all information within the topic segment. Thus, the
keywords extracted from the history graph by this
variant are those without consideration of topic rel-
evance.
Table 6 compares the proposed method with the
variant. The performance of the variant is higher
than those of TFIDF-Liu and TextRank-Liu. This
proves the importance of the temporal history in
keyword extraction from meeting transcripts. How-
ever, the proposed method still outperforms the vari-
ant, and it demonstrates the importance of topic rel-
evance. Therefore, it can be concluded that the con-
sideration of temporal history and topic relevance
is critical in keyword extraction from meeting tran-
scripts.
895
6 Conclusion
In this paper, we have proposed a just-in-time key-
word extraction from meeting transcripts. Whenever
an utterance is spoken, the proposed method extracts
keywords from the utterance that best describe the
utterance. Based on the graph representation of all
components in a meeting, the proposed method ex-
tracts keywords by TextRank with some graph oper-
ations.
Temporal history and topic of the current utter-
ance are two major factors especially in keyword ex-
traction from meeting transcripts. This is because re-
cent utterances are more important than old ones and
only the preceding utterances of which topic is rele-
vant to the current utterance are important. To model
the temporal importance of the preceding utterances,
the concept of forgetting curve is used in updating
the history graph of preceding utterances. In addi-
tion, the subgraph of the history graph that shares
words appearing in the current utterance graph is
used to extract keywords rather than whole history
graph. The proposed method was evaluated with the
National Assembly transcripts and the ICSI meeting
corpus. According to our experimental results on
these data sets, the performance of keyword extrac-
tion is improved when we consider temporal history
and topic relevance.
Acknowledgments
This research was supported by the Converging Re-
search Center Program funded by the Ministry of
Education, Science and Technology (2012K001342)
References
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Yun-Nung Chen, Yu Huang, Sheng-Yi Kong, , and Lin-
Shan Lee. 2010. Automatic key term extraction from
spoken course lectures using branching entropy and
prosodic/semantic features. In Proceedings of IEEE
Workshop on Spoken Language Technology, pages
265?270.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the 18th International Joint Conference on Artificial
intelligence, pages 668?671.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of International Conference on Empirical Meth-
ods in Natural Language Processing, pages 216?223.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin, Thilo
Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck
Wooters. 2003. The icsi meeting corpus. In Proceed-
ings of International Conference on Acoustics, Speech,
and Signal Processing, pages 364?367.
Fei Liu, Feifan Liu, and Yang Liu. 2008. Automatic key-
word extraction for the meeting corpus using super-
vised approach and bigram expansion. In Proceedings
of IEEE Spoken Language Technology, pages 181?
184.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu. 2009.
Unsupervised approaches for automatic keyword ex-
traction using meeting transcripts. In Proceedings of
Annual Conference of the North American Chapter of
the ACL, pages 620?628.
Fei Liu, Feifan Liu, and Yang Liu. 2011. A super-
vised framework for keyword extraction from meeting
transcripts. IEEE Transactions on Audio, Speech, and
Language Processing, 19(3):538?548.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of International
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 404?411.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of Annual Conference of the
North American Chapter of the ACL, pages 145?152.
Peter D. Turney. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval, 2:303?
336.
Peter D. Turney. 2003. Coherent keyphrase extrac-
tion via web mining. In Proceedings of the 18th In-
ternational Joint Conference on Artificial intelligence,
pages 434?439.
Xiaojun Wan and Jianguo Xiao. 2008. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of International
Conference on Computational Linguistics, pages 969?
976.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. To-
wards an iterative reinforcement approach for simulta-
neous document summarization and keyword extrac-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 552?
559.
Robert H. Wozniak. 1999. Classics in Psychology,
1855?1914: Historical Essays. Thoemmes Press.
896
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1025?1034,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Cost Sensitive Part-of-Speech Tagging:
Differentiating Serious Errors from Minor Errors
Hyun-Je Song1 Jeong-Woo Son1 Tae-Gil Noh2 Seong-Bae Park1,3 Sang-Jo Lee1
1School of Computer Sci. & Eng. 2Computational Linguistics 3NLP Lab.
Kyungpook Nat?l Univ. Heidelberg University Dept. of Computer Science
Daegu, Korea Heidelberg, Germany University of Illinois at Chicago
{hjsong,jwson,tgnoh}@sejong.knu.ac.kr sbpark@uic.edu sjlee@knu.ac.kr
Abstract
All types of part-of-speech (POS) tagging er-
rors have been equally treated by existing tag-
gers. However, the errors are not equally im-
portant, since some errors affect the perfor-
mance of subsequent natural language pro-
cessing (NLP) tasks seriously while others do
not. This paper aims to minimize these serious
errors while retaining the overall performance
of POS tagging. Two gradient loss functions
are proposed to reflect the different types of er-
rors. They are designed to assign a larger cost
to serious errors and a smaller one to minor
errors. Through a set of POS tagging exper-
iments, it is shown that the classifier trained
with the proposed loss functions reduces se-
rious errors compared to state-of-the-art POS
taggers. In addition, the experimental result
on text chunking shows that fewer serious er-
rors help to improve the performance of sub-
sequent NLP tasks.
1 Introduction
Part-of-speech (POS) tagging is needed as a pre-
processor for various natural language processing
(NLP) tasks such as parsing, named entity recogni-
tion (NER), and text chunking. Since POS tagging is
normally performed in the early step of NLP tasks,
the errors in POS tagging are critical in that they
affect subsequent steps and often lower the overall
performance of NLP tasks.
Previous studies on POS tagging have shown
high performance with machine learning techniques
(Ratnaparkhi, 1996; Brants, 2000; Lafferty et al,
2001). Among the types of machine learning ap-
proaches, supervised machine learning techniques
were commonly used in early studies on POS tag-
ging. With the characteristics of a language (Rat-
naparkhi, 1996; Kudo et al, 2004) and informa-
tive features for POS tagging (Toutanova and Man-
ning, 2000), the state-of-the-art supervised POS tag-
ging achieves over 97% of accuracy (Shen et al,
2007; Manning, 2011). This performance is gen-
erally regarded as the maximum performance that
can be achieved by supervised machine learning
techniques. There have also been many studies on
POS tagging with semi-supervised (Subramanya et
al., 2010; S?gaard, 2011) or unsupervised machine
learning methods (Berg-Kirkpatrick et al, 2010;
Das and Petrov, 2011) recently. However, there still
exists room to improve supervised POS tagging in
terms of error differentiation.
It should be noted that not all errors are equally
important in POS tagging. Let us consider the parse
trees in Figure 1 as an example. In Figure 1(a),
the word ?plans? is mistagged as a noun where it
should be a verb. This error results in a wrong parse
tree that is severely different from the correct tree
shown in Figure 1(b). The verb phrase of the verb
?plans? in Figure 1(b) is discarded in Figure 1(a)
and the whole sentence is analyzed as a single noun
phrase. Figure 1(c) and (d) show another tagging er-
ror and its effect. In Figure 1(c), a noun is tagged as
a NNS (plural noun) where its correct tag is NN (sin-
gular or mass noun). However, the error in Figure
1(c) affects only locally the noun phrase to which
?physics? belongs. As a result, the general structure
of the parse tree in Figure 1(c) is nearly the same as
1025
SVP
VP
NP
The treasury 
to
raise 150 billion in cash.
DT NNP
TO
VB CD CD IN NN
S
plans
NNS
(a) A parse tree with a serious error.
S
VPNP
The   treasury 
DT NNP
S
VP
VPto
raise 150 billion in cash.
TO
VB CD CD IN NN
plans
VBZ
(b) The correct parse tree of the sentence?The treasury
plans . . .?.
S
NP VP
We
PRP
altered
VBN
NP
NP PP
the chemistry and physics
DT
of the atmosphere
NN CC NNS INDT NN
(c) A parse tree with a minor error.
S
NP VP
We
PRP
altered
VBN
NP
NP PP
the chemistry and physics
DT
of the atmosphere
NN CC NN INDT NN
(d) The correct parse tree of the sentence ?We altered
. . .?.
Figure 1: An example of POS tagging errors
the correct one in Figure 1(d). That is, a sentence
analyzed with this type of error would yield a cor-
rect or near-correct result in many NLP tasks such
as machine translation and text chunking.
The goal of this paper is to differentiate the seri-
ous POS tagging errors from the minor errors. POS
tagging is generally regarded as a classification task,
and zero-one loss is commonly used in learning clas-
sifiers (Altun et al, 2003). Since zero-one loss con-
siders all errors equally, it can not distinguish error
types. Therefore, a new loss is required to incorpo-
rate different error types into the learning machines.
This paper proposes two gradient loss functions to
reflect differences among POS tagging errors. The
functions assign relatively small cost to minor er-
rors, while larger cost is given to serious errors.
They are applied to learning multiclass support vec-
tor machines (Tsochantaridis et al, 2004) which is
trained to minimize the serious errors. Overall accu-
racy of this SVM is not improved against the state-
of-the-art POS tagger, but the serious errors are sig-
nificantly reduced with the proposed method. The
effect of the fewer serious errors is shown by apply-
ing it to the well-known NLP task of text chunking.
Experimental results show that the proposed method
achieves a higher F1-score compared to other POS
taggers.
The rest of the paper is organized as follows. Sec-
tion 2 reviews the related studies on POS tagging. In
Section 3, serious and minor errors are defined, and
it is shown that both errors are observable in a gen-
eral corpus. Section 4 proposes two new loss func-
tions for discriminating the error types in POS tag-
ging. Experimental results are presented in Section
5. Finally, Section 6 draws some conclusions.
2 Related Work
The POS tagging problem has generally been solved
by machine learning methods for sequential label-
1026
Tag category POS tags
Substantive NN, NNS, NNP, NNPS, CD, PRP, PRP$
Predicate VB, VBD, VBG, VBN, VBP, VBZ, MD, JJ, JJR, JJS
Adverbial RB, RBR, RBS, RP, UH, EX, WP, WP$, WRB, CC, IN, TO
Determiner DT, PDT, WDT
Etc FW, SYM, POS, LS
Table 1: Tag categories and POS tags in Penn Tree Bank tag set
ing. In early studies, rich linguistic features and su-
pervised machine learning techniques are applied by
using annotated corpora like the Wall Street Journal
corpus (Marcus et al, 1994). For instance, Ratna-
parkhi (1996) used a maximum entropy model for
POS tagging. In this study, the features for rarely
appearing words in a corpus are expanded to im-
prove the overall performance. Following this direc-
tion, various studies have been proposed to extend
informative features for POS tagging (Toutanova
and Manning, 2000; Toutanova et al, 2003; Man-
ning, 2011). In addition, various supervised meth-
ods such as HMMs and CRFs are widely applied to
POS tagging. Lafferty et al (2001) adopted CRFs
to predict POS tags. The methods based on CRFs
not only have all the advantages of the maximum
entropy markov models but also resolve the well-
known problem of label bias. Kudo et al (2004)
modified CRFs for non-segmented languages like
Japanese which have the problem of word boundary
ambiguity.
As a result of these efforts, the performance of
state-of-the-art supervised POS tagging shows over
97% of accuracy (Toutanova et al, 2003; Gime?nez
and Ma`rquez, 2004; Tsuruoka and Tsujii, 2005;
Shen et al, 2007; Manning, 2011). Due to the high
accuracy of supervised approaches for POS tagging,
it has been deemed that there is no room to im-
prove the performance on POS tagging in supervised
manner. Thus, recent studies on POS tagging focus
on semi-supervised (Spoustova? et al, 2009; Sub-
ramanya et al, 2010; S?gaard, 2011) or unsuper-
vised approaches (Haghighi and Klein, 2006; Gold-
water and Griffiths, 2007; Johnson, 2007; Graca et
al., 2009; Berg-Kirkpatrick et al, 2010; Das and
Petrov, 2011). Most previous studies on POS tag-
ging have focused on how to extract more linguistic
features or how to adopt supervised or unsupervised
approaches based on a single evaluation measure,
accuracy. However, with a different viewpoint for
errors on POS tagging, there is still some room to
improve the performance of POS tagging for subse-
quent NLP tasks, even though the overall accuracy
can not be much improved.
In ordinary studies on POS tagging, costs of er-
rors are equally assigned. However, with respect
to the performance of NLP tasks relying on the re-
sult of POS tagging, errors should be treated differ-
ently. In the machine learning community, cost sen-
sitive learning has been studied to differentiate costs
among errors. By adopting different misclassifica-
tion costs for each type of errors, a classifier is op-
timized to achieve the lowest expected cost (Elkan,
2001; Cai and Hofmann, 2004; Zhou and Liu, 2006).
3 Error Analysis of Existing POS Tagger
The effects of POS tagging errors to subsequent
NLP tasks vary according to their type. Some errors
are serious, while others are not. In this paper, the
seriousness of tagging errors is determined by cat-
egorical structures of POS tags. Table 1 shows the
Penn tree bank POS tags and their categories. There
are five categories in this table: substantive, pred-
icate, adverbial, determiner, and etc. Serious tag-
ging errors are defined as misclassifications among
the categories, while minor errors are defined as mis-
classifications within a category. This definition fol-
lows the fact that POS tags in the same category
form similar syntax structures in a sentence (Zhao
and Marcus, 2009). That is, inter-category errors are
treated as serious errors, while intra-category errors
are treated as minor errors.
Table 2 shows the distribution of inter-category
and intra-category errors observed in section 22?
24 of the WSJ corpus (Marcus et al, 1994) that is
tagged by the Stanford Log-linear Part-Of-Speech
1027
Predicted category
Substantive Predicate Adverbial Determiner Etc
Substantive 614 479 32 10 15
Predicate 585 743 107 2 14
True category Adverbial 41 156 500 42 2
Determiner 13 7 47 24 0
Etc 23 11 3 1 0
Table 2: The distribution of tagging errors on WSJ corpus by Stanford Part-Of-Speech Tagger.
Tagger (Manning, 2011) (trained with WSJ sections
00?18). In this table, bold numbers denote inter-
category errors while all other numbers show intra-
category errors. The number of total errors is 3,471
out of 129,654 words. Among them, 1,881 errors
(54.19%) are intra-category, while 1,590 of the er-
rors (45.81%) are inter-category. If we can reduce
these inter-category errors under the cost of mini-
mally increasing intra-category errors, the tagging
results would improve in quality.
Generally in POS tagging, all tagging errors are
regarded equally in importance. However, inter-
category and intra-category errors should be distin-
guished. Since a machine learning method is opti-
mized by a loss function, inter-category errors can
be efficiently reduced if a loss function is designed
to handle both types of errors with different cost. We
propose two loss functions for POS tagging and they
are applied to multiclass Support Vector Machines.
4 Learning SVMs with Class Similarity
POS tagging has been solved as a sequential labeling
problem which assumes dependency among words.
However, by adopting sequential features such as
POS tags of previous words, the dependency can be
partially resolved. If it is assumed that words are
independent of one another, POS tagging can be re-
garded as a multiclass classification problem. One
of the best solutions for this problem is by using an
SVM.
4.1 Training SVMs with Loss Function
Assume that a training data set D =
{(x1, y1), (x2, y2), . . . , (xl, yl)} is given where
xi ? Rd is an instance vector and yi ? {+1,?1}
is its class label. SVM finds an optimal hyperplane
satisfying
xi ? w + b ? +1 for yi = +1,
xi ? w + b ? ?1 for yi = ?1,
where w and b are parameters to be estimated from
training data D. To estimate the parameters, SVMs
minimizes a hinge loss defined as
?i = Lhinge(yi, w ? xi + b)
= max{0, 1 ? yi ? (w ? xi + b)}.
With regularizer ||w||2 to control model complexity,
the optimization problem of SVMs is defined as
min
w,?
1
2
||w||2 + C
l
?
i=1
?i,
subject to
yi(xi ? w + b) ? 1? ?i, and ?i ? 0 ?i,
where C is a user parameter to penalize errors.
Crammer et al (2002) expanded the binary-class
SVM for multiclass classifications. In multiclass
SVMs, by considering all classes the optimization
of SVM is generalized as
min
w,?
1
2
?
k?K
||wk||2 + C
l
?
i=1
?i,
with constraints
(wyi ? ?(xi, yi))? (wk ? ?(xi, k)) ? 1? ?i,
?i ? 0 ?i, ?k ? K \ yi,
where ?(xi, yi) is a combined feature representation
of xi and yi, and K is the set of classes.
1028
POS
SUBSTANTIVE
PREDICATE ADVERBIAL
OTHERS
NOUN
PRONOUN
DETERMINER
DT
PDT
NNS
NN NNP
NNPS
CD
PRP PRP$
VERB
VBD
VB
VBG
VBN
VBP
VBZ
MD
ADJECT
JJR
JJ JJS
SYM
FW POS
LS
ADVERB
WH- CONJUNCTION
RBR
RB RBS
RP
UH
EX
WP
WP$
WRB
IN
CC TO
WDT
Figure 2: A tree structure of POS tags.
Since both binary and multiclass SVMs adopt a
hinge loss, the errors between classes have the same
cost. To assign different cost to different errors,
Tsochantaridis et al (2004) proposed an efficient
way to adopt arbitrary loss function, L(yi, yj) which
returns zero if yi = yj , otherwise L(yi, yj) > 0.
Then, the hinge loss ?i is re-scaled with the inverse
of the additional loss between two classes. By scal-
ing slack variables with the inverse loss, margin vi-
olation with high loss L(yi, yj) is more severely re-
stricted than that with low loss. Thus, the optimiza-
tion problem with L(yi, yj) is given as
min
w,?
1
2
?
k?K
||wk||2 + C
l
?
i=1
?i, (1)
with constraints
(wyi ? ?(xi, yi))? (wk ? ?(xi, k)) ? 1?
?i
L(yi, k)
,
?i ? 0 ?i, ?k ? K \ yi,
With the Lagrange multiplier ?, the optimization
problem in Equation (1) is easily converted to the
following dual quadratic problem.
min
?
1
2
l
?
i,j
?
ki?K\yi
?
kj?K\yj
?i,ki?j,kj ?
J(xi, yi, ki)J(xj , yj, kj)?
l
?
i
?
ki?K\yi
?i,ki ,
with constraints
? ? 0 and
?
ki?K\yi
?i,ki
L(yi, ki)
? C, ?i = 1, ? ? ? , l,
where J(xi, yi, ki) is defined as
J(xi, yi, ki) = ?(xi, yi)? ?(xi, ki).
4.2 Loss Functions for POS tagging
To design a loss function for POS tagging, this paper
adopts categorical structures of POS tags. The sim-
plest way to reflect the structure of POS tags shown
in Table 1 is to assign larger cost to inter-category
errors than to intra-category errors. Thus, the loss
function with the categorical structure in Table 1 is
defined as
Lc(yi, yj) =
?
?
?
?
?
?
?
0 if yi = yj ,
? if yi 6= yj but they belong
to the same POS category,
1 otherwise,
(2)
where 0 < ? < 1 is a constant to reduce the value of
Lc(yi, yj) when yi and yj are similar. As shown in
this equation, inter-category errors have larger cost
than intra-category errors. This loss Lc(yi, yj) is
named as category loss.
The loss function Lc(yi, yj) is designed to reflect
the categories in Table 1. However, the structure
of POS tags can be represented as a more complex
structure. Let us consider the category, predicate.
1029
?
Class NN Class NNS
Class VB
(a) Multiclass SVMs with hinge loss
Class NN Class NNS
Class VB
?
L(NN, VB)
?
L(NN, NNS)
(b) Multiclass SVMs with the proposed loss
function
Figure 3: Effect of the proposed loss function in multiclass SVMs
This category has ten POS tags, and can be further
categorized into two sub-categories: verb and ad-
ject. Figure 2 represents a categorical structure of
POS tags as a tree with five categories of POS tags
and their seven sub-categories.
To express the tree structure of Figure 2 as a loss,
another loss function Lt(yi, yj) is defined as
Lt(yi, yj) =
1
2
[Dist(Pi,j , yi) +Dist(Pi,j, yj)]? ?, (3)
where Pi,j denotes the nearest common parent of
both yi and yj , and the function Dist(Pi,j, yi) re-
turns the number of steps from Pi,j to yi. The user
parameter ? is a scaling factor of a unit loss for a
single step. This loss Lt(yi, yj) returns large value
if the distance between yi and yj is far in the tree
structure, and it is named as tree loss.
As shown in Equation (1), two proposed loss
functions adjust margin violation between classes.
They basically assign less value for intra-category
errors than inter-category errors. Thus, a classi-
fier is optimized to strictly keep inter-category er-
rors within a smaller boundary. Figure 3 shows a
simple example. In this figure, there are three POS
tags and two categories. NN (singular or mass noun)
and NNS (plural noun) belong to the same cate-
gory, while VB (verb, base form) is in another cat-
egory. Figure 3(a) shows the decision boundary of
NN based on hinge loss. As shown in this figure, a
single ? is applied for the margin violation among
all classes. Figure 3(b) also presents the decision
boundary of NN, but it is determined with the pro-
posed loss function. In this figure, the margin vio-
lation is applied differently to inter-category (NN to
VB) and intra-category (NN to NNS) errors. It re-
sults in reducing errors between NN and VB even if
the errors between NN and NNS could be slightly
increased.
5 Experiments
5.1 Experimental Setting
Experiments are performed with a well-known stan-
dard data set, the Wall Street Journal (WSJ) corpus.
The data is divided into training, development and
test sets as in (Toutanova et al, 2003; Tsuruoka and
Tsujii, 2005; Shen et al, 2007). Table 3 shows some
simple statistics of these data sets. As shown in
this table, training data contains 38,219 sentences
with 912,344 words. In the development data set,
there are 5,527 sentences with about 131,768 words,
those in the test set are 5,462 sentences and 129,654
words. The development data set is used only to se-
lect ? in Equation (2) and ? in Equation (3).
Table 4 shows the feature set for our experiments.
In this table, wi and ti denote the lexicon and POS
tag for the i-th word in a sentence respectively. We
use almost the same feature set as used in (Tsuruoka
and Tsujii, 2005) including word features, tag fea-
1030
Training Develop Test
Section 0?18 19?21 22?24
# of sentences 38,219 5,527 5,462
# of terms 912,344 131,768 129,654
Table 3: Simple statistics of experimental data
Feature Name Description
Word features wi?2, wi?1, wi, wi+1, wi+2wi?1 ? wi, wi ? wi+1
Tag features
ti?2, ti?1, ti+1, ti+2
ti?2 ? ti?1, ti+1 ? ti+2
ti?2 ? ti?1 ? ti+1, ti?1 ? ti+1 ? ti+2
ti?2 ? ti?1 ? ti+1 ? ti+2
Tag/Word
combination
ti?2?wi, ti?1 ?wi, ti+1?wi, ti+2?wi
ti?1 ? ti+1 ? wi
Prefix features prefixes of wi (up to length 9)
Suffix features suffixes of wi (up to length 9)
Lexical features
whether wi contains capitals
whether wi has a number
whether wi has a hyphen
whether wi is all capital
whether wi starts with capital and
locates at the middle of sentence
Table 4: Feature template for experiments
tures, word/tag combination features, prefix and suf-
fix features as well as lexical features. The POS tags
for words are obtained from a two-pass approach
proposed by Nakagawa et al (2001).
In the experiments, two multiclass SVMs with the
proposed loss functions are used. One is CL-MSVM
with category loss and the other is TL-MSVM with
tree loss. A linear kernel is used for both SVMs.
5.2 Experimental Results
CL-MSVM with ? = 0.4 shows the best overall per-
formance on the development data where its error
rate is as low as 2.71%. ? = 0.4 implies that the
cost of intra-category errors is set to 40% of that of
inter-category errors. The error rate of TL-MSVM
is 2.69% when ? is 0.6. ? = 0.4 and ? = 0.6 are set
in the all experiments below.
Table 5 gives the comparison with the previous
work and proposed methods on the test data. As can
be seen from this table, the best performing algo-
rithms achieve near 2.67% error rate (Shen et al,
2007; Manning, 2011). CL-MSVM and TL-MSVM
Error
(%)
# of Intra
error
# of Inter
error
(Gime?nez and Ma`rquez,
2004) 2.84
1,995
(54.11%)
1,692
(45.89%)
(Tsuruoka and Tsujii,
2005) 2.85 - -
(Shen et al, 2007) 2.67 1,856(53.52%)
1,612
(46.48%)
(Manning, 2011) 2.68 1,881(54.19%)
1,590
(45.81%)
CL-MSVM (? = 0.4) 2.69 1,916(55.01%)
1,567
(44.99%)
TL-MSVM (? = 0.6) 2.68 1,904(54.74%)
1,574
(45.26%)
Table 5: Comparison with the previous works
achieve an error rate of 2.69% and 2.68% respec-
tively. Although overall error rates of CL-MSVM
and TL-MSVM are not improved compared to the
previous state-of-the-art methods, they show reason-
able performance.
For inter-category error, CL-MSVM achieves the
best performance. The number of inter-category er-
ror is 1,567, which shows 23 errors reduction com-
pared to previous best inter-category result by (Man-
ning, 2011). TL-MSVM also makes 16 less inter-
category errors than Manning?s tagger. When com-
pared with Shen?s tagger, both CL-MSVM and TL-
MSVM make far less inter-category errors even if
their overall performance is slightly lower than that
of Shen?s tagger. However, the intra-category er-
ror rate of the proposed methods has some slight
increases. The purpose of proposed methods is to
minimize inter-category errors but preserving over-
all performance. From these results, it can be found
that the proposed methods which are trained with the
proposed loss functions do differentiate serious and
minor POS tagging errors.
5.3 Chunking Experiments
The task of chunking is to identify the non-recursive
cores for various types of phrases. In chunking, the
POS information is one of the most crucial aspects in
identifying chunks. Especially inter-category POS
errors seriously affect the performance of chunking
because they are more likely to mislead the chunk
compared to intra-category errors.
Here, chunking experiments are performed with
1031
POS tagger Accuracy (%) Precision Recall F1-score
(Shen et al, 2007) 96.08 94.03 93.75 93.89
(Manning, 2011) 96.08 94 93.8 93.9
CL-MSVM (? = 0.4) 96.13 94.1 93.9 94.00
TL-MSVM (? = 0.6) 96.12 94.1 93.9 94.00
Table 6: The experimental results for chunking
a data set provided for the CoNLL-2000 shared
task. The training data contains 8,936 sentences
with 211,727 words obtained from sections 15?18
of the WSJ. The test data consists of 2,012 sentences
and 47,377 words in section 20 of the WSJ. In order
to represent chunks, an IOB model is used, where
every word is tagged with a chunk label extended
with B (the beginning of a chunk), I (inside a chunk),
and O (outside a chunk). First, the POS informa-
tion in test data are replaced to the result of our POS
tagger. Then it is evaluated using trained chunking
model. Since CRFs (Conditional Random Fields)
has been shown near state-of-the-art performance in
text chunking (Fei Sha and Fernando Pereira, 2003;
Sun et al, 2008), we use CRF++, an open source
CRF implementation by Kudo (2005), with default
feature template and parameter settings of the pack-
age. For simplicity in the experiments, the values
of ? in Equation (2) and ? in Equation (3) are set
to be 0.4 and 0.6 respectively which are same as the
previous section.
Table 6 gives the experimental results of text
chunking according to the kinds of POS taggers in-
cluding two previous works, CL-MSVM, and TL-
MSVM. Shen?s tagger and Manning?s tagger show
nearly the same performance. They achieve an ac-
curacy of 96.08% and around 93.9 F1-score. On the
other hand, CL-MSVM achieves 96.13% accuracy
and 94.00 F1-score. The accuracy and F1-score of
TL-MSVM are 96.12% and 94.00. Both CL-MSVM
and TL-MSVM show slightly better performances
than other POS taggers. As shown in Table 5, both
CL-MSVM and TL-MSVM achieve lower accura-
cies than other methods, while their inter-category
errors are less than that of other experimental meth-
ods. Thus, the improvement of CL-MSVM and TL-
MSVM implies that, for the subsequent natural lan-
guage processing, a POS tagger should considers
different cost of tagging errors.
6 Conclusion
In this paper, we have shown that supervised POS
tagging can be improved by discriminating inter-
category errors from intra-category ones. An inter-
category error occurs by mislabeling a word with
a totally different tag, while an intra-category error
is caused by a similar POS tag. Therefore, inter-
category errors affect the performances of subse-
quent NLP tasks far more than intra-category errors.
This implies that different costs should be consid-
ered in training POS tagger according to error types.
As a solution to this problem, we have proposed
two gradient loss functions which reflect different
costs for two error types. The cost of an error type is
set according to (i) categorical difference or (ii) dis-
tance in the tree structure of POS tags. Our POS
experiment has shown that if these loss functions
are applied to multiclass SVMs, they could signif-
icantly reduce inter-category errors. Through the
text chunking experiment, it is shown that the multi-
class SVMs trained with the proposed loss functions
which generate fewer inter-category errors achieve
higher performance than existing POS taggers.
We have shown that cost sensitive learning can be
applied to POS tagging only with multiclass SVMs.
However, the proposed loss functions are general
enough to be applied to other existing POS taggers.
Most supervised machine learning techniques are
optimized on their loss functions. Therefore, the
performance of POS taggers based on supervised
machine learning techniques can be improved by ap-
plying the proposed loss functions to learn their clas-
sifiers.
Acknowledgments
This research was supported by the Converg-
ing Research Center Program funded by the
Ministry of Education, Science and Technology
(2011K000659).
References
Yasemin Altun, Mark Johnson, and Thomas Hofmann.
2003. Investigating Loss Functions and Optimiza-
tion Methods for Discriminative Learning of Label Se-
quences. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing. pp.
145?152.
1032
Talyor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Un-
supervised Learning with Features. In Proceedings
of the North American Chapter of the Association for
Computational Linguistics. pp. 582?590.
Thorsten Brants. 2000. TnT-A Statistical Part-of-Speech
Tagger. In Proceedings of the Sixth Applied Natural
Language Processing Conference. pp. 224?231.
Lijuan Cai and Thomas Hofmann. 2004. Hierarchi-
cal Document Categorization with Support Vector Ma-
chines. In Proceedings of the Thirteenth ACM Inter-
national Conference on Information and Knowledge
Management. pp. 78?87.
Koby Crammer, Yoram Singer. 2002. On the Algorith-
mic Implementation of Multiclass Kernel-based Vec-
tor Machines. Journal of Machine Learning Research,
Vol. 2. pp. 265?292.
Dipanjan Das and Slav Petrov. 2011. Unsupervised Part-
of-Speech Tagging with Bilingual Graph-Based Pro-
jections. In Proceedings of the 49th Annual Meeting
of the Association of Computational Linguistics. pp.
600?609.
Charles Elkan. 2001. The Foundations of Cost-Sensitive
Learning. In Proceedings of the Seventeenth Interna-
tional Joint Conference on Artificial Intelligence. pp.
973?978.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation.
pp. 43?46.
Sharon Goldwater and Thomas T. Griffiths. 2007. A
fully Bayesian Approach to Unsupervised Part-of-
Speech Tagging. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics. pp. 744?751.
Joao Graca, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs Parameter Sparsity in La-
tent Variable Models. In Advances in Neural Informa-
tion Processing Systems 22. pp. 664?672.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
Learning for Sequence Models. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics. pp. 320?327.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of the 2007 Joint Meet-
ing of the Conference on Empirical Methods in Natu-
ral Language Processing and the Conference on Com-
putational Natural Language Learning. pp. 296?305.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to
Japanese Morphological Analysis. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 230?237.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning. pp. 282?289.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?.
In Proceedings of the 12th International Conference
on Intelligent Text Processing and Computational Lin-
guistics. pp. 171?189.
Tetsuji Nakagawa, Taku Kudo, and Yuji Matsumoto.
2001. Unknown Word Guessing and Part-of-Speech
Tagging Using Support Vector Machines. In Proceed-
ings of the Sixth Natural Language Processing Pacific
Rim Symposium. pp. 325?331.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 133?142.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
the Human Language Technology and North American
Chapter of the Association for Computational Linguis-
tics. pp. 213?220.
Libin Shen, Giorgio Satta, and Aravind K. Joshi 2007.
Guided Learning for Bidirectional Sequence Classifi-
cation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics. pp.
760?767.
Anders S?gaard 2011. Semisupervised condensed near-
est neighbor for part-of-speech tagging. In Proceed-
ings of the 49th Annual Meeting of the Association of
Computational Linguistics. pp. 48?52.
Drahom??ra ?johanka? Spoustova`, Jan Hajic?, Jan Raab,
and Miroslav Spousta 2009. Semi-supervised training
for the averaged perceptron POS tagger. In Proceed-
ings of the European Chapter of the Association for
Computational Linguistics. pp. 763?771.
Amarnag Subramanya, Slav Petrov and Fernando Pereira
2010. Efficient Graph-Based Semi-Supervised Learn-
ing of Structured Tagging Models. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 167?176.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara
and Jun?ichi Tsujii 2008. Modeling Latent-Dynamic
in Shallow Parsing: A Latent Conditional Model with
Improved Inference. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics.
pp. 841?848.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network.
1033
In Proceedings of the Human Language Technology
and North American Chapter of the Association for
Computational Linguistics. pp. 252?259.
Kristina Toutanova and Christopher D. Manning. 2000.
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing. pp. 63?70.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemi Altun. 2004. Support Vec-
tor Learning for Interdependent and Structured Output
Spaces. In Proceedings of the 21st International Con-
ference on Machine Learning. pp. 104?111.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional Inference with the Easiest-First Strategy for
Tagging Sequence Data. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing. pp. 467?474.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, Vol. 19, No.2 . pp. 313?330.
Qiuye Zhao and Mitch Marcus. 2009. A Simple Un-
supervised Learner for POS Disambiguation Rules
Given Only a Minimal Lexicon. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing. pp. 688?697.
Zhi-Hua Zhou and Xu-Ying Liu 2006. On Multi-Class
Cost-Sensitive Learning. In Proceedings of the AAAI
Conference on Artificial Intelligence. pp. 567?572.
1034
