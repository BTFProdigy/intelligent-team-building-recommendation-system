2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 656?666,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Learning from Bullying Traces in Social Media
Jun-Ming Xu, Kwang-Sung Jun, Xiaojin Zhu
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706, USA
{xujm,deltakam,jerryzhu}@cs.wisc.edu
Amy Bellmore
Department of Educational Psychology
University of Wisconsin-Madison
Madison, WI 53706, USA
abellmore@wisc.edu
Abstract
We introduce the social study of bullying to
the NLP community. Bullying, in both physi-
cal and cyber worlds (the latter known as cy-
berbullying), has been recognized as a seri-
ous national health issue among adolescents.
However, previous social studies of bully-
ing are handicapped by data scarcity, while
the few computational studies narrowly re-
strict themselves to cyberbullying which ac-
counts for only a small fraction of all bullying
episodes. Our main contribution is to present
evidence that social media, with appropriate
natural language processing techniques, can
be a valuable and abundant data source for the
study of bullying in both worlds. We iden-
tify several key problems in using such data
sources and formulate them as NLP tasks, in-
cluding text classification, role labeling, senti-
ment analysis, and topic modeling. Since this
is an introductory paper, we present baseline
results on these tasks using off-the-shelf NLP
solutions, and encourage the NLP community
to contribute better models in the future.
1 Introduction to Bullying
Bullying, also called peer victimization, has been
recognized as a serious national health issue by
the White House (The White House, 2011), the
American Academy of Pediatrics (The American
Academy of Pediatrics, 2009), and the American
Psychological Association (American Psychological
Association, 2004). One is being bullied or victim-
ized when he or she is exposed repeatedly over time
to negative actions on the part of others (Olweus,
1993). Far-reaching and insidious sequelae of bul-
lying include intrapersonal problems (Juvonen and
Graham, 2001; Jimerson, Swearer, and Espelage,
2010) and lethal school violence in the most extreme
cases (Moore et al, 2003). Youth who experience
peer victimization report more symptoms of depres-
sion, anxiety, loneliness, and low self-worth com-
pared to their nonvictimized counterparts (Bellmore
et al, 2004; Biggs, Nelson, and Sampilo, 2010; Gra-
ham, Bellmore, and Juvonen, 2007; Hawker and
Boulton, 2000). Other research suggests that victim-
ized youth have more physical complaints (Fekkes
et al, 2006; Nishina and Juvonen, 2005; Gini and
Pozzoli, 2009). Victimized youth are absent from
school more often and get lower grades than nonvic-
timized youth (Ladd, Kochenderfer, and Coleman,
1997; Schwartz et al, 2005; Juvonen and Gross,
2008).
Bullying happens traditionally in the physical
world and, recently, online as well; the latter is
known as cyberbullying (Cassidy, Jackson, and
Brown, 2009; Fredstrom, Adams, and Gilman,
2011; Wang, Iannotti, and Nansel, 2009; Vande-
bosch and Cleemput, 2009). Bullying usually starts
in primary school, peaks in middle school, and lasts
well into high school and beyond (Nansel et al,
2001; Smith, Madsen, and Moody, 1999; Cook et
al., 2010). Across a national sample of students in
grades 4 through 12, 38% of students reported be-
ing bullied by others and 32% reported bullying oth-
ers (Vaillancourt et al, 2010).
656
reinforcer
bystander
bully victim
assistant defender reporter
accuser
Figure 1: The roles in a bullying episode. Solid circles
represent traditional roles in social science, while dotted
circles are new roles we augmented for social media. The
width of the edges represents interaction strength.
1.1 The Structure of a Bullying Episode
Bullying takes multiple forms, most noticeably face-
to-face physical (e.g., hitting), verbal (e.g., name-
calling), and relational (e.g., exclusion) (Archer and
Coyne, 2005; Little et al, 2003; Nylund et al,
2007). Cyberbullying reflects a venue (other than
face to face contact) through which verbal and rela-
tional forms can occur.
A main reason individuals are targeted with bul-
lying is perceived differences, i.e., any characteristic
that makes an individual stand out differently from
his or her peers. These include race, socio-economic
status, gender, sexuality, physical appearance, and
behaviors.
Participants in a bullying episode take well-
defined roles (see Figure 1). More than one person
can have the same role in a bullying episode. Roles
include the bully (or bullies), the victims, bystanders
(who saw the event but did not intervene), defend-
ers of the victim, assistants to the bully (who did
not initiate but went along with the bully), and rein-
forcers (who did not directly join in with the bully
but encouraged the bully by laughing, for exam-
ple) (Salmivalli, 1999). This recognition that bully-
ing involves multiple roles makes evident the broad-
ranging impact of bullying; any child or adolescent
is susceptible to participation in bullying, even those
who are not directly involved (Janosz et al, 2008;
Rivers et al, 2009).
1.2 Some Scientific Questions NLP can Answer
Like many complex social issues, effective solutions
to bullying go beyond technology alone and require
the concerted efforts of parents, educators, and law
enforcement. To guide these efforts it is paramount
to study the dynamics of bullying. Such study criti-
cally depends on text in the form of self-report social
study surveys and electronic communication among
participants. Such text is often fragmental, noisy,
and covers only part of a bullying episode from a
specific role?s perspective. As such, the NLP com-
munity can help answer a host of scientific ques-
tions: Which pieces of text refer to the same under-
lying bullying episode? What is the form, reason,
location, time, etc. of a bullying episode? Who are
the participants of each episode, and what are their
roles? How does a person?s role evolve over time?
This paper presents our initial investigation on some
of these questions, while leaving others to future re-
search by the NLP community.
1.3 Limitations of the State-of-the-Art
The social science study of bullying has a long his-
tory. However, a fundamental problem there is data
acquisition. The standard approach is to conduct
time-consuming personal surveys in schools. The
sample size is typically in the hundreds, and partici-
pants typically write 3 to 4 sentences about each bul-
lying episode (Nishina and Bellmore, 2010). Such a
small corpus fails to assess the true frequency of bul-
lying over the population, and cannot determine the
evolution of roles. The computational study of bul-
lying is largely unexplored, with the exception of a
few studies on cyberbullying (Lieberman, Dinakar,
and Jones, 2011; Dinakar, Reichart, and Lieber-
man, 2011; Ptaszynski et al, 2010; Kontostathis,
Edwards, and Leatherman, 2010; Bosse and Stam,
2011; Latham, Crockett, and Bandar, 2010). These
studies did not consider the much more frequent bul-
lying episodes in the physical world.
2 Bullying Traces in Social Media
The main contribution of the present paper is not
on novel algorithms, but rather on presenting evi-
dence that social media data and off-the-shelf NLP
tools can be an effective combination for the study
of bullying. Participants of a bullying episode (in ei-
ther physical or cyber venues) often post social me-
dia text about the experience. We collectively call
such social media posts bullying traces. Bullying
657
traces include but far exceed incidences of cyberbul-
lying. Most of them are in fact responses to a bul-
lying experience ? the actual attack is hidden from
view. Bullying traces are valuable, albeit fragmental
and noisy, data which we can use to piece together
the underlying episodes.
In the rest of the paper, we focus on publicly
available Twitter ?tweets,? though our methods
apply readily to other social media services, too.
Here are some examples of bullying traces:
? Reporting a bullying episode: ?some tweens
got violent on the n train, the one boy got off
after blows 2 the chest... Saw him cryin as he
walkd away :( bullying not cool?
? Accusing someone as a bully: ?@USERNAME
i didnt jump around and act like a monkey T T
which of your eye saw that i acted like a monkey
:( you?re a bully?
? Revealing self as a victim: ?People bullied me
for being fat. 7 years later, I was diagnosed
with bulimia. Are you happy now??
? Cyber-bullying direct attack: ?Lauren is a fat
cow MOO BITCH?
Bullying traces are abundant. From the publicly
available 2011 TREC Microblog track corpus (16
million tweets sampled between January 23rd and
February 8th, 2011), we uniformly sampled 990
tweets for manual inspection by five experienced an-
notators (not the authors of the present paper). Of
the 990 tweets, the annotators labeled 617 as non-
English, 371 as English but not bullying traces, and
2 as English bullying traces. The Maximum Likeli-
hood Estimate of the frequency of English bullying
traces, out of all tweets, is 2/990 ? 0.002. The
exact Binomial 95% confidence interval is (0.0002,
0.0073). This is a tiny fraction. Nonetheless, it rep-
resents an abundance of tweets: by some estimates,
Twitter produces 250 million tweets per day in late
2011. Even with the lower bound in the confidence
interval, it translates into 50,000 English bullying
traces per day. The actual number can be much
higher.
Bullying traces contain valuable information. For
example, Figure 2 shows the daily number of bully-
ing traces identified by our classifier, to be discussed
Figure 2: Temporal variation of bullying traces
in section 3. A weekly pattern was obvious in late
August. A small peak was caused by 14-year-old
bullying victim Jamey Rodemeyer?s suicide on Sept.
18. This was followed by a large peak after Lady
Gaga dedicated a song to him on Sept. 24.
In the following sections, we identify several key
problems in using social media for the study of bul-
lying. We formulate each key problem as an NLP
task. We then present standard off-the-shelf NLP ap-
proaches to establish baseline performances. Since
bullying traces account for only a tiny fraction of all
tweets, it posed a significant challenge for our an-
notators to find enough bullying traces without la-
beling an unreasonable amount of tweets. For this
reason, in the rest of the paper we restrict ourselves
to an ?enriched dataset.? This enriched dataset is ob-
tained by collecting tweets using the public Twitter
streaming API, such that each tweet contains at least
one of the following keywords: ?bully, bullied, bul-
lying.? We further removed re-tweets (the analogue
of forwarded emails) by excluding tweets containing
the acronym ?RT.? The enrichment process is meant
to retain many first-hand bullying traces at the cost
of a selection bias.
3 NLP Task A: Text Categorization
One important task is to distinguish bullying traces
from other social media posts. Our enriched dataset,
generated by simple keyword filtering, still contains
many irrelevant tweets. For example, ?Forced veg-
anism by removing a persons choice is just another
form of bullying? is not a bullying trace, since it does
658
not describe a bullying episode. Our task is to dis-
tinguish posts like this from true bullying traces such
as those mentioned in the previous section. We for-
mulate it as a binary text categorization task.
Methods. The same annotators who labeled the
TREC corpus labeled 1762 tweets sampled uni-
formly from the enriched dataset on August 6, 2011.
Among them, 684 (39%) were labeled as bullying
traces.
Following (Settles, 2011), these 1762 tweets were
case-folded but without any stemming or stop-
word removal. Any user mentions preceded by a
?@? were replaced by the anonymized user name
?@USERNAME?. Any URLs starting with ?http?
were replaced by the token ?HTTPLINK?. Hashtags
(compound words following ?#?) were not split and
were treated as a single token. Emoticons, such as
?:)? or ?:D?, were also included as tokens.
After these preprocessing procedures, we created
three different sets of feature representations: un-
igrams (1g), unigrams+bigrams (1g2g), and POS-
colored unigrams+bigrams (1g2gPOS). POS tag-
ging was done with the Stanford CoreNLP pack-
age (Toutanova et al, 2003). POS-coloring was
done by expanding each token into token:POS.
We chose four commonly used text classifiers,
namely, Naive Bayes, SVM with linear kernel
(SVM(linear)), SVM with RBF kernel (SVM(RBF))
and Logistic Regression (equivalent to MaxEnt). We
used the WEKA (Hall et al, 2009) implementation
for the first three (calling LibSVM (Chang and Lin,
2011) with WEKA?s interfaces for SVMs), and the
L1General package (Schmidt, Fung, and Rosales,
2007) for the fourth.
We held out 262 tweets for test, and systemat-
ically varied training set size among the remain-
ing tweets, from 100 to 1500 with the step-size
100. We tuned all parameters jointly by 5-fold
cross validation on the training set with the grid
{2?8, 2?6, . . . , 28}. All the four text classifiers were
trained on the training sets and tested on the test set.
The whole procedure was repeated 30 times for each
feature representation.
Results. Figure 3 reports the held-out set accu-
racy as the training set size increases. The error bars
are ?1 standard error. With the largest training set
size (1500), the combination of SVM(linear) + 1g
achieves an average accuracy 79.7%. SVM(linear)
+ 1g2g achieves 81.3%, which is significantly bet-
ter (t-test, p = 4 ? 10?6). It shows that in-
cluding bigrams can significantly improve the clas-
sification performance. SVM(linear) + 1g2gPOS
achieves 81.6%, though the improvement is not sta-
tistically significant (p = 0.088), which indicates
that POS coloring does not help too much on this
task. SVM(RBF) gives similar performance, Logis-
tic Regression is slightly worse and Naive Bayes is
much worse, for a large range of training set sizes.
In summary, SVM(linear) + 1g2g is the preferred
model because of its accuracy and simplicity. We
also note that these accuracies are much better than
the majority class baseline of 61%. On the held-
out set, SVM(linear) + 1g2g achieves precision
P=0.76, recall R=0.79, and F-measure 0.77.
Discussions. Note that the learning curves are
still increasing, suggesting that better accuracy can
be obtained if we annotate more training data. As to
why the best accuracy is not close to 1, one hypoth-
esis is noisy labels caused by intrinsic disagreement
among labelers. Tweets are short and some are am-
biguous. Without prior knowledge about the users
and their other tweets, labelers interpret the tweets
in their own ways. For example, for the very short
tweet feels like a bully..... our annotators disagreed
on whether it is a bullying trace. Labelers may have
different views on these ambiguous tweets and cre-
ated noisy bullying trace labels.
A future direction is to categorize bullying traces
at a finer granularity, e.g., by forms, reasons, etc.
This can be solved by multi-class classification
methods. Another direction is to extend the clas-
sifiers from the ?enriched data? to the full range of
tweets. Recall that the difference is whether we pre-
filter the tweets by keywords. Clearly, they have
different tweet distributions. Techniques used for
covariate shift may be adapted to solve this prob-
lem (Blitzer, 2008).
4 NLP Task B: Role Labeling
Identifying participants? bullying roles (Figure 1) is
another important task, which is also a prerequi-
site of studying how a person?s role evolves over
time. For bullying traces in social media, we aug-
ment the traditional role system with two new roles:
reporter (may not be present during the episode, un-
659
(a) 1g (b) 1g2g (c) 1g2gPOS
Figure 3: Learning Curves for different feature sets and classification algorithms
like a bystander) and accuser (accusing someone as
the bully). Both roles can be a victim, a defender,
or a bystander in the traditional sense ? there is just
not enough information in the tweet. Accuser (A),
bully (B), reporter (R) and victim (V) are the four
most frequent roles observed in social media. We
merged all remaining roles into a generic category
?other? (O) in the following study. Our task is to
classify the role (A, B, R, V, O) of the tweet author
and any person-mentions in a tweet. For example,
AUTHOR(R): ?We(R) visited my(V) cousin(V) today
& #Itreallymakesmemad that he(V) barely eats bec
he(V) was bullied . :( I(R) wanna kick the crap out
of those mean(B) kids(B).? Note that the special to-
ken ?AUTHOR? is introduced to hold the label of
the author?s role.
Labeling author?s role and other person-mention?s
role are two different sub-tasks. The former can be
formulated as a multi-class text classification task;
the latter is better formulated as a sequential tagging
task. We will discuss them separately below.
4.1 Author?s Roles
Methods. Our annotators labeled the author?s role
for each of the 684 positive bullying traces in Task A
(296 R, 162 V, 98 B, 86 A, 42 O). We used the same
classifiers and features in Section 3. We conducted
10-fold cross validation to evaluate all combinations
of classifiers and feature sets. Like before, we tuned
all parameters jointly by 5-fold cross validation on
the training set with the grid {2?8, 2?6, . . . , 28}.
Results. The best combination is SVM(linear)
+ 1g2g with cross validation accuracy 61%. Even
though it is far from perfect, it is significantly better
than the majority class (R) baseline of 43%. It shows
predicted as
A B R V O
A 33 3 39 10 1
B 5 25 57 11 0
R 15 5 249 27 0
V 1 4 48 109 0
O 1 1 37 3 0
Table 1: Confusion Matrix of Author Role Classification
that there is signal in the text to infer the authors?
roles.
Table 1 shows the confusion matrix of the best
model. Most R and V authors are correctly rec-
ognized, but not B and A. The model misclassified
many authors as R. It is possible that the tweets au-
thored by reporters are diverse in topic and style, and
overlap with other classes in the feature space.
Discussions. As tweets are short, our feature rep-
resentation may not be the best for predicting au-
thor?s role. Many authors mentioned themselves
in the tweets with first-person pronouns, making
it advantageous to consider joint classification by
merging sections 4.1 and 4.2. Furthermore, assum-
ing roles change infrequently, it may be helpful to
jointly classify many tweets authored by the same
person.
4.2 Person-Mention?s Roles
This sub-task labels each person-mention with a
bullying role. It uses Named Entity Recognition
(NER) (Finkel, Grenager, and Manning, 2005; Rati-
nov and Roth, 2009; Ritter et al, 2011) as a sub-
routine to identify named person entities, though we
are also interested in unnamed persons such as ?my
teacher? and pronouns. It is related to Semantic Role
660
Labeling (SRL) (Gildea and Jurafsky, 2002; Pun-
yakanok, Roth, and Yih, 2008) but differs critically
in that our roles are not tied to specific verb predi-
cates.
Methods. Our annotators labeled each token
in the 684 bullying traces with the tags A, B,
R, V, O and N for not-a-person. There are
11,751 tokens in total. Similar to the sequen-
tial tagging formulation (Ma`rquez et al, 2005; Liu
et al, 2010), we trained a linear CRF to label
each token in the tweet with the CRF++ package
(http://crfpp.sourceforge.net/).
As standard in linear CRFs, we used pairwise la-
bel features f(yi?1, yi) and input features f(yi,w),
where f ?s are binary indicator functions on the val-
ues of their arguments and w is the text. In the fol-
lowing, we introduce our input features using the ex-
ample tweet ?@USERNAME i?ll tell vinny you bul-
lied me.? with the current token wi =?vinny?:
(i) The token, lemma, and POS tag of the
five tokens around position i. For example,
fbully,wi?1=tell(yi,w) will be 1 if the current to-
ken has label yi = ?bully?? and wi?1 = ?tell??.
Similarly, fvictim,POSi+2=V BD(yi,w) will be 1 if
yi = ?victim?? and the POS of wi+2 is VBD.
(ii) The NER tag of wi.
(iii) Whether wi is a person mention. This is a
Boolean feature which is true if wi is tagged as PER-
SON by NER, or if POSi = pronoun (excluding
?it?), or if wi is @USERNAME. For example, this
feature is true on ?vinny? because it is tagged as
PERSON by NER.
(iv) The relevant verb vi of wi, vi?s lemma, POS,
and the combination of vi with the lemma/POS of
wi. The relevant verb vi of wi is defined by the
semantic dependency between wi and the verb, if
one exists. Otherwise, vi is the closest verb to wi.
For example, the relevant verb of wi = ?vinny?? is
vi = ?tell?? because ?vinny? is found as the object
of ?tell? by dependency parsing.
(v) The distance, relative position (left or right)
and dependency type between vi and wi. For ex-
ample, the distance between ?vinny? and its relevant
verb ?tell? is 1. ?vinny? is on the right and is the
object of ?tell?.
The lemma, POS tags, NER tags and dependency
relationship were obtained using Stanford CoreNLP.
As a baseline, we trained SVM(linear) with the
Accuracy Precision Recall F-1
CRF 0.87 0.53 0.42 0.47
SVM 0.85 0.42 0.31 0.36
Table 2: Cross Validation Result of Person-Mention
Roles
same input features as CRF. Classification is done
individually on each token. We randomly split the
684 tweets into 10 folds and conducted cross vali-
dation based on this split. For CRF, we trained on
the tweets in the training set with their labels, and
tested the model on those in the test set. For SVM,
we trained and tested at the token level in the corre-
sponding sets.
Results. Table 2 reports the cross validation ac-
curacy, precision, recall and F-1 measure. Accu-
racy measures the percentage of tokens correctly
assigned the groundtruth labels, including N (not-
a-person) tokens. Precision measures the fraction
of correctly labeled person-mention tokens over all
tokens that are not N according to the algorithm.
Recall measures the fraction of correctly labeled
person-mention tokens over all tokens that are not
N according to the groundtruth. F-1 is the har-
monic mean of precision and recall. Linear CRF
achieved an accuracy 0.87, which is higher than the
baseline of majority class predictor (N, 0.80) (t-
test, p = 10?10). However, the precision and re-
call is low potentially because the tweets are short
and noisy. CRF outperforms SVM in all measures,
showing the value of joint classification.
Discussions. Table 3 shows the confusion ma-
trix of person-mention role labeling by linear CRF.
There are several reasons for these mistakes. First,
words like ?teacher?, ?sister?, or ?girl? were missed
by our person mention feature (iii). Second, the
NER tagger was trained on formal English which is
a mismatch for the informal tweets, leading to NER
errors. Third, noisy labeling continues to affect ac-
curacy. For example, some annotators considered
?other people? as an entity and labeled both tokens
as person mentions; others labeled ?people? only.
In general, bullying role labeling may be im-
proved by jointly considering multiple tweets at the
episode level. Co-reference resolution should im-
prove the performance as well.
661
predicted as
A B R V O N
A 0 4 5 10 0 4
B 0 406 13 125 103 302
R 0 28 31 67 0 13
V 0 142 28 380 43 202
O 0 112 4 42 156 86
N 0 78 4 41 16 9306
Table 3: Confusion Matrix of Person-Mention Roles by
CRF
5 NLP Task C: Sentiment Analysis
Sentiment analysis on participants involved in a bul-
lying episode is of significant importance. As Fig-
ure 4 suggests, there are a wide range of emotions in
bullying traces. For example, victims usually expe-
rience negative emotions such as depression, anxiety
and loneliness; Some emotions are more violent or
even suicidal. Detecting at-risk individuals via sen-
timent analysis enables potential interventions. In
addition, social scientists are interested in sentiment
analysis of bullying participants to understand their
motivations.
In the present paper we investigate a special form
of sentiment in bullying traces, namely teasing. We
observed that many bullying traces were written jok-
ingly. One example of a teasing post is ?@USER-
NAME lol stop being a cyber bully lol :p.? Teas-
ing may indicate the lack of severity of a bullying
episode; It may also be a manifest of coping strate-
gies in bullying victims. Therefore, there is consid-
erable interest among social scientists to understand
teasing in bullying traces.
Methods. One first task is to identify teasing bul-
lying traces. We formulated it as a binary classifi-
cation problem, similar to classic positive/negative
sentiment classification (Pang and Lee, 2004). Our
annotators labeled each of the 684 bullying traces in
Task A as teasing (99) or not (585). We used the
same feature representations, classifiers and param-
eter tuning as in Section 3 and 10-fold cross valida-
tion procedure.
Results. The best cross validation accuracy of
89% is obtained by SVM(linear) + 1g2g. This
is significantly better than the majority class (not-
teasing) baseline of 86% (t-test, p = 10?33). It
shows that even simple features and off-the-shelf
predicted as
Tease Not
Tease 52 47
Not 26 559
Table 4: Confusion Matrix of Teasing Classification
classifier can detect some signal in the text. How-
ever, the accuracy is not high. Table 4 shows the
confusion matrix. About half of the tease examples
were misclassified. We found several possible ex-
planations. First, teasing is not always accompanied
by joking emoticons or tokens like ?LOL,? ?lmao,?
?haha.? For example, ?I may bully you but I love
you lots. Just like jelly tots!? and ?Been bullied into
watching a scary film, I love my friends!? Such teas-
ing sentiment requires deeper NLP or much larger
training sets. Second, tweets containing those jok-
ing emoticons and tokens are not necessarily teas-
ing. For example, ?This Year I?m Standing Up For
The Kids That Are Being Bullied All Over The Na-
tion :) .? Third, the joking tokens have diverse
spellings. For example, ?lol? was spelled as ?loll,?
?lolol,? ?lollll,? ?loool,? ?LOOOOOOOOOOOL?;
?haha? was spelled as ?HAHAHAHA,? ?Hahaha,?
?Bwahahaha,? ?ahahahah,? ?hahah.?
Discussions. Specialized word normalization for
social media text may significantly improve perfor-
mance. For example, word lengthening can be iden-
tified and used as cues for teasing (Brody and Di-
akopoulos, 2011). Teasing is diverse in its form
and content. Our training set is perhaps too small.
Borrowing training data from other corpora, such as
one-liner jokes (Mihalcea and Strapparava, 2005),
may be helpful.
6 NLP Task D: Latent Topic Modeling
Methods. Given the large volume of bullying traces,
methods for automatically analyzing what people
are talking about are needed. Latent topic models
allow us to extract the main topics in bullying traces
to facilitate understanding. We used latent Dirich-
let alocation (LDA) (Blei, Ng, and Jordan, 2003) as
our exploratory tool. Specifically, we ran a collapsed
Gibbs sampling implementation of LDA (Griffiths
and Steyvers, 2004).
The corpus consists of 188K enriched tweets from
Aug. 21 to Sept. 17, 2011 that are classified as
662
bullying traces by our classifier in Task A. We per-
formed stopword removal and further removed word
types occurring less than 7 times, resulting in a vo-
cabulary of size 12K. We set the number of topics
to 50, Dirichlet parameter for word multinomials to
? = 0.01, Dirichlet parameter for document topic
multinomial to ? = 1, and ran Gibbs sampling for
10K iterations.
Results. Space precludes a complete list of top-
ics. Figure 4 shows six selected topics discovered by
LDA. Recall that each topic in LDA is a multinomial
distribution over the vocabulary. The figure shows
each topic?s top 20 words with size proportional to
p(word | topic). The topic names are manually as-
signed.
These topics contain semantically coherent words
relevant to bullying: (feelings) how people feel
about bullying; (suicide) discussions of suicide
events; (family) sibling names probably used in a
good buddy sense; (school) the school environment
where bullying commonly occurs; (verbal bullying)
derogatory words such as fat and ugly; (physical bul-
lying) actions such as kicking and pushing.
We also ran a variational inference implementa-
tion of LDA (Blei, Ng, and Jordan, 2003). The re-
sults were similar, thus we omit discussion of them.
Discussions. Some recovered topics, including
the ones shown here, provide valuable insight into
bullying traces. However, not all topics are inter-
pretable to social scientists. It may be helpful to al-
low scientists the ability to combine their domain
knowledge with latent topic modeling, thus arriv-
ing at more useful topics. For example, the scien-
tists can formulate their knowledge in First-Order
Logic, which can then be combined with LDA with
stochastic optimization (Andrzejewski et al, 2011).
7 Conclusion and Future Work
We introduced social media as a large-scale, near
real-time, dynamic data source for the study of bul-
lying. Social media offers a broad range of bully-
ing traces that include but go beyond cyberbullying.
In the present paper, we have identified several key
problems in using social media to study bullying and
formulated them as familiar NLP tasks. Our baseline
performance with standard off-the-shelf approaches
shows that it is feasible to learn from bullying traces.
?feelings? ?suicide?
?family? ?school?
?verbal bullying? ?physical bullying?
Figure 4: Selected topics discovered by latent Dirichlet
allocation.
Much work remains in this new research direc-
tion. In the short term, we need to develop spe-
cialized NLP tools for processing bullying traces in
social media, similar to (Ritter et al, 2011; Liu et
al., 2010), to achieve better performance than mod-
els trained on formal English. In the long term, we
need to tackle the problem of piecing together the
underlying bullying episodes from fragmental bully-
ing traces. Consider two separate bullying episodes
with the following participants and roles:
E1: B: Buffy, V: Vivian & Virginia, O: Debra
E2: B: Burton, V: Buffy, O: Irene
The corresponding bullying traces can be three posts
in this order:
w1 Debra: Virginia, I heard Buffy call you and
Vivian fat?ignore her!
w2 Buffy to Irene: Burton picked on me again
because I?m only 5?1
w3 Vivian: Buffy I?m not fat! Stop calling me that.
Reconstructing E1, E2 fromw1,w2,w3 is challeng-
ing for a number of reasons: (1) There is no explicit
episode index in the posts. (2) Posts from a single
episode may be dispersed in time (e.g., w1,w3 be-
long to E1, but not w2), each containing only part
663
of an episode. (3) The number of episodes and peo-
ple can grow indefinitely as more posts arrive. (4)
People may switch roles in different episodes (e.g.,
Buffy was the bully in E1 but the victim in E2). Joint
probabilistic modeling over multiple posts using so-
cial network structures hold great promise in solving
this problem.
To facilitate bullying research in the NLP com-
munity, we make our annotations and software
publicly available at http://research.cs.
wisc.edu/bullying.
Acknowledgments
We thank Wei-Ting Chen, Rachael Hansen, Ting-
Lan Ma, Ji-in You and Bryan Gibson for their help
on the data and the paper.
References
[American Psychological Association2004] American
Psychological Association. 2004. APA reso-
lution on bullying among children and youth.
http://www.apa.org/about/governance/
council/policy/bullying.pdf.
[Andrzejewski et al2011] Andrzejewski, David, Xiaojin
Zhu, Mark Craven, and Ben Recht. 2011. A frame-
work for incorporating general domain knowledge into
Latent Dirichlet Allocation using First-Order Logic.
In the 22nd IJCAI, pages 1171?1177.
[Archer and Coyne2005] Archer, John and Sarah M.
Coyne. 2005. An integrated review of indirect, re-
lational, and social aggression. Personality and Social
Psychology Review, 9:212?230.
[Bellmore et al2004] Bellmore, Amy D., Melissa R.
Witkow, Sandra Graham, and Jaana Juvonen. 2004.
Beyond the individual: The impact of ethnic context
and classroom behavioral norms on victims? adjust-
ment. Developmental Psychology, 40:1159?1172.
[Biggs, Nelson, and Sampilo2010] Biggs, Bridget K.,
Jennifer Mize Nelson, and Marilyn L. Sampilo. 2010.
Peer relations in the anxiety-depression link: Test
of a mediation model. Anxiety, Stress & Coping,
23(4):431?447.
[Blei, Ng, and Jordan2003] Blei, David M., Andrew Y.
Ng, and Michael I. Jordan. 2003. Latent dirichlet al
location. JMLR, 3:993?1022.
[Blitzer2008] Blitzer, John. 2008. Domain Adaptation of
Natural Language Processing Systems. Ph.D. thesis,
University of Pennsylvania.
[Bosse and Stam2011] Bosse, Tibor and Sven Stam.
2011. A normative agent system to prevent cyberbul-
lying. In WI-IAT 2011, pages 425?430.
[Brody and Diakopoulos2011] Brody, Samuel and
Nicholas Diakopoulos. 2011. Cooooooooooooooolll-
lllllllllll!!!!!!!!!!!!!! using word lengthening to detect
sentiment in microblogs. In EMNLP 2011, pages
562?570.
[Cassidy, Jackson, and Brown2009] Cassidy, Wanda,
Margaret Jackson, and Karen N. Brown. 2009. Sticks
and stones can break my bones, but how can pixels
hurt me? students? experiences with cyber-bullying.
School Psychology Int?l, 30(4):383?402.
[Chang and Lin2011] Chang, Chih-Chung and Chih-Jen
Lin. 2011. LIBSVM: A library for support vector ma-
chines. ACM Trans. on Intelligent Systems and Tech-
nology, 2:27:1?27:27.
[Cook et al2010] Cook, Clayton R., Kirk R. Williams,
Nancy G. Guerra, Tia E. Kim, and Shelly Sadek.
2010. Predictors of bullying and victimization in
childhood and adolescence: A meta-analytic investi-
gation. School Psychology Quarterly, 25(2):65?83.
[Dinakar, Reichart, and Lieberman2011] Dinakar, K.,
R. Reichart, and H. Lieberman. 2011. Modeling the
detection of textual cyberbullying. In International
Conference on Weblog and Social Media - Social
Mobile Web Workshop, Barcelona, Spain.
[Fekkes et al2006] Fekkes, Minne, Frans I.M. Pijpers,
A. Miranda Fredriks, Ton Vogels, and S. Pauline
Verloove-Vanhorick. 2006. Do bullied children get
ill, or do ill children get bullied? a prospective cohort
study on the relationship between bullying and health-
related symptoms. Pediatrics, 117:1568?1574.
[Finkel, Grenager, and Manning2005] Finkel, Jenny R.,
Trond Grenager, and Christopher Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In the 43rd ACL,
pages 363?370.
[Fredstrom, Adams, and Gilman2011] Fredstrom, Brid-
get K., Ryan E. Adams, and Rich Gilman. 2011. Elec-
tronic and school-based victimization: Unique con-
texts for adjustment difficulties during adolescence. J.
Youth and Adolescence, 40(4):405?415.
[Gildea and Jurafsky2002] Gildea, Daniel and Daniel Ju-
rafsky. 2002. Automatic labeling of semantic roles.
Comput. Linguist., 28(3):245?288.
[Gini and Pozzoli2009] Gini, Gianluca and Tiziana Poz-
zoli. 2009. Association between bullying and psy-
chosomatic problems: a meta-analysis. Pediatrics,
123(3):1059?1065.
[Graham, Bellmore, and Juvonen2007] Graham, Sandra,
Amy Bellmore, and Jaana Juvonen. 2007. Peer vic-
timization in middle school: When self- and peer
664
views diverge. In Joseph E. Zins, Maurice J. Elias, and
Charles A. Maher, editors, Bullying, victimization, and
peer harassment: A handbook of prevention and inter-
vention. Haworth Press, New York, NY, pages 121?
141.
[Griffiths and Steyvers2004] Griffiths, Thomas L. and
Mark Steyvers. 2004. Finding scientific topics.
PNAS, 101(suppl. 1):5228?5235.
[Hall et al2009] Hall, Mark, Eibe Frank, Geoffrey
Holmes, Bernhard Pfahringer, Peter Reutemann, and
Ian H. Witten. 2009. The weka data mining software:
an update. ACM SIGKDD Explorations Newsletter,
11:10?18.
[Hawker and Boulton2000] Hawker, David S. J. and
Michael J. Boulton. 2000. Twenty years? research on
peer victimization and psychosocial maladjustment: A
meta-analytic review of cross-sectional studies. J. of
Child Psychology And Psychiatry, 41(4):441?455.
[Janosz et al2008] Janosz, Michel, Isabelle Archambault,
Linda S. Pagani, Sophie Pascal, Alexandre J.S. Morin,
and Franc?ois Bowen. 2008. Are there detrimental
effects of witnessing school violence in early adoles-
cence? J. of Adolescent Health, 43(6):600?608.
[Jimerson, Swearer, and Espelage2010] Jimerson,
Shane R., Susan M. Swearer, and Dorothy L.
Espelage. 2010. Handbook of Bullying in Schools:
An international perspective. Routledge/Taylor &
Francis Group, New York, NY.
[Juvonen and Graham2001] Juvonen, Jaana and Sandra
Graham. 2001. Peer harassment in school: The plight
of the vulnerable and victimized. Guilford Press, New
York, NY.
[Juvonen and Gross2008] Juvonen, Jaana and Elisheva F.
Gross. 2008. Extending the school grounds? ? Bul-
lying experiences in cyberspace. J. of School Health,
78:496?505.
[Kontostathis, Edwards, and Leatherman2010]
Kontostathis, April, Lynne Edwards, and Amanda
Leatherman. 2010. Text mining and cybercrime.
In Michael W. Berry and Jacob Kogan, editors, Text
Mining: Applications and Theory. John Wiley &
Sons, Ltd, Chichester, UK.
[Ladd, Kochenderfer, and Coleman1997] Ladd, Gary W.,
Becky J. Kochenderfer, and Cynthia C. Coleman.
1997. Classroom peer acceptance, friendship, and vic-
timization: Distinct relational systems that contribute
uniquely to children?s school adjustment? Child De-
velopment, 68:1181?1197.
[Latham, Crockett, and Bandar2010] Latham, Annabel,
Keeley Crockett, and Zuhair Bandar. 2010. A
conversational expert system supporting bullying
and harassment policies. In the 2nd ICAART, pages
163?168.
[Lieberman, Dinakar, and Jones2011] Lieberman, Henry,
Karthik Dinakar, and Birago Jones. 2011. Let?s gang
up on cyberbullying. Computer, 44:93?96.
[Little et al2003] Little, Todd D., Christopher C. Hen-
rich, Stephanie M. Jones, and Patricia H. Hawley.
2003. Disentangling the ?whys? from the ?whats? of
aggressive behavior. Int?l J. of Behavioral Develop-
ment, 27:122?133.
[Liu et al2010] Liu, Xiaohua, Kuan Li, Bo Han, Ming
Zhou, Long Jiang, Zhongyang Xiong, and Changning
Huang. 2010. Semantic role labeling for news tweets.
In the 23rd COLING, pages 698?706.
[Ma`rquez et al2005] Ma`rquez, Llu??s, Pere Comas, Jesu?s
Gime?nez, and Neus Catala`. 2005. Semantic role la-
beling as sequential tagging. In the 9th CoNLL, pages
193?196.
[Mihalcea and Strapparava2005] Mihalcea, Rada and
Carlo Strapparava. 2005. Making computers laugh:
Investigations in automatic humor recognition. In
EMNLP 2005, pages 531?538.
[Moore et al2003] Moore, Mark H., Carol V. Petrie, An-
thony A. Braga, and Brenda L. McLaughlin. 2003.
Deadly lessons: Understanding lethal school violence.
The National Academies Press, Washington, DC.
[Nansel et al2001] Nansel, Tonja R., Mary Overpeck,
Ramani S. Pilla, W. June Ruan, Bruce Simons-
Morton, and Peter Scheidt. 2001. Bullying behav-
iors among US youth: prevalence and association with
psychosocial adjustment. J. Amer. Medical Assoc.,
285(16):2094?2100.
[Nishina and Bellmore2010] Nishina, Adrienne and
Amy D. Bellmore. 2010. When might aggression,
victimization, and conflict matter most?: Contextual
considerations. J. of Early Adolescence, pages 5?26.
[Nishina and Juvonen2005] Nishina, Adrienne and Jaana
Juvonen. 2005. Daily reports of witnessing and ex-
periencing peer harassment in middle school. Child
Development, 76:435?450.
[Nylund et al2007] Nylund, Karen, Amy Bellmore, Adri-
enne Nishina, and Sandra Graham. 2007. Subtypes,
severity, and structural stability of peer victimization:
What does latent class analysis say? Child Develop-
ment, 78:1706?1722.
[Olweus1993] Olweus, Dan. 1993. Bullying at school:
What we know and what we can do. Blackwell, Ox-
ford, UK.
[Pang and Lee2004] Pang, Bo and Lillian Lee. 2004. A
sentimental education: Sentiment analysis using sub-
jectivity summarization based on minimum cuts. In
the 42nd ACL, pages 271?278.
[Ptaszynski et al2010] Ptaszynski, Michal, Pawel Dy-
bala, Tatsuaki Matsuba, Fumito Masui, Rafal Rzepka,
665
and Kenji Araki. 2010. Machine learning and af-
fect analysis against cyber-bullying. In the 36th AISB,
pages 7?16.
[Punyakanok, Roth, and Yih2008] Punyakanok, Vasin,
Dan Roth, and Wen-tau Yih. 2008. The importance
of syntactic parsing and inference in semantic role
labeling. Comput. Linguist., 34(2):257?287.
[Ratinov and Roth2009] Ratinov, Lev and Dan Roth.
2009. Design challenges and misconceptions in
named entity recognition. In the 13th CoNLL, pages
147?155.
[Ritter et al2011] Ritter, Alan, Sam Clark, Mausam, and
Oren Etzioni. 2011. Named entity recognition in
tweets: An experimental study. In EMNLP 2011,
pages 1524?1534.
[Rivers et al2009] Rivers, Ian, V. Paul Poteat, Nathalie
Noret, and Nigel Ashurst. 2009. Observing bullying
at school: The mental health implications of witness
status. School Psychology Quarterly, 24(4):211?223.
[Salmivalli1999] Salmivalli, Christina. 1999. Participant
role approach to school bullying: Implications for in-
tervention. J. of Adolescence, 22(4):453?459.
[Schmidt, Fung, and Rosales2007] Schmidt, Mark W.,
Glenn Fung, and Ro?mer Rosales. 2007. Fast opti-
mization methods for l1 regularization: A comparative
study and two new approaches. In the 18th ECML,
pages 286?297.
[Schwartz et al2005] Schwartz, David, Andrea Hop-
meyer Gorman, Jonathan Nakamoto, and Robin L. To-
blin. 2005. Victimization in the peer group and chil-
dren?s academic functioning. J. of Educational Psy-
chology, 87:425?435.
[Settles2011] Settles, Burr. 2011. Closing the loop: Fast,
interactive semi-supervised annotation with queries on
features and instances. In the EMNLP 2011, pages
1467?1478.
[Smith, Madsen, and Moody1999] Smith, Peter K.,
Kirsten C. Madsen, and Janet C. Moody. 1999. What
causes the age decline in reports of being bullied at
school? Towards a developmental analysis of risks of
being bullied. Educational Research, 41(3):267?285.
[The American Academy of Pediatrics2009] The Ameri-
can Academy of Pediatrics. 2009. Policy statement?
role of the pediatrician in youth violence prevention.
Pediatrics, 124(1):393?402.
[The White House2011] The White House. 2011.
Background on White House conference on bul-
lying prevention. http://www.whitehouse.gov/the-
press-office/2011/03/10/background-white-house-
conference-bullying-prevention.
[Toutanova et al2003] Toutanova, Kristina, Dan Klein,
Christopher D. Manning, and Yoram Singer. 2003.
Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In NAACL-HLT 2003, pages 173?
180.
[Vaillancourt et al2010] Vaillancourt, Tracy, Vi Trinh,
Patricia McDougall, Eric Duku, Lesley Cunning-
ham, Charles Cunningham, Shelley Hymel, and Kathy
Short. 2010. Optimizing population screening of bul-
lying in school-aged children. J. of School Violence,
9:233?250.
[Vandebosch and Cleemput2009] Vandebosch, Heidi and
Katrien Van Cleemput. 2009. Cyberbullying among
youngsters: profiles of bullies and victims. New media
& society, 11(8):1349?1371.
[Wang, Iannotti, and Nansel2009] Wang, Jing, Ronald J.
Iannotti, and Tonja R. Nansel. 2009. School bully-
ing among adolescents in the united states: Physical,
verbal, relational, and cyber. J. Adolescent Health,
45(4):368?375.
666
Proceedings of NAACL-HLT 2013, pages 697?702,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
An Examination of Regret in Bullying Tweets
Jun-Ming Xu, Benjamin Burchfiel, Xiaojin Zhu
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706, USA
{xujm,burchfie,jerryzhu}@cs.wisc.edu
Amy Bellmore
Department of Educational Psychology
University of Wisconsin-Madison
Madison, WI 53706, USA
abellmore@wisc.edu
Abstract
Social media users who post bullying related
tweets may later experience regret, potentially
causing them to delete their posts. In this pa-
per, we construct a corpus of bullying tweets
and periodically check the existence of each
tweet in order to infer if and when it becomes
deleted. We then conduct exploratory analy-
sis in order to isolate factors associated with
deleted posts. Finally, we propose the con-
struction of a regrettable posts predictor to
warn users if a tweet might cause regret.
1 Introduction
A large body of literature suggests that participants
in bullying events, including victims, bullies, and
witnesses, are likely to report psychological adjust-
ment problems (Jimerson, Swearer, and Espelage,
2010). One potential source of therapy for these is-
sues can be self-disclosure of the experience to an
adult or friend (Mishna and Alaggia, 2005); exist-
ing research suggests that victims who seek advice
and help from others report less maladjustment than
victims who do not (Shelley and Craig, 2010).
Disclosure of bullying experiences through so-
cial media may be a particularly effective mecha-
nism for participants seeking support because so-
cial media has the potential to reach large audi-
ences and because participants may feel less inhi-
bition when sharing private information in an on-
line setting (Walther, 1996). Furthermore, there is
evidence that online communication stimulates self-
disclosure, which leads to higher quality social rela-
tionships and increased well-being (Valkenburg and
Peter, 2009).
Online disclosure may also present risks for
those involved in bullying however, such as re-
victimization, embarrassment, and social ostraciza-
tion. Evidence exists that some individuals may re-
act to these risks retroactively, by deleting their so-
cial media posts (Child et al, 2011; Christofides,
Muise, and Desmarais, 2009). Several relevant mo-
tives have been found to be associated with delet-
ing posted information, including conflict manage-
ment, safety, fear of retribution, impression manage-
ment, and emotional regulation (Child, Haridakis,
and Petronio, 2012).
Our previous work (Xu et al, 2012) demonstrates
that social media can be a valuable data source when
studying bullying, and proposes a text categorization
method to recognize social media posts describing
bullying episodes, bullying traces. To better under-
stand, and possibly prevent, user regret after posting
bullying related tweets, we collect bullying traces
using the same method and perform regular status
checks to determine if and when tweets become in-
accessible. While a tweet becoming inaccessible
does not guarantee it has been deleted, we attempt to
leverage http response codes to rule out other com-
mon causes of inaccessibility. Speculating that re-
gret may be a major cause of deletion, we first con-
duct exploratory analysis on this corpus and then re-
port the results of an off-the-shelf regret predictor.
2 Data Collection
We adopt the procedure used in (Xu et al, 2012) to
obtain bullying traces; each identified trace contains
697
at least one bullying related keyword and passes a
bullying-or-not text classifier.
Our data was collected in realtime using the
Twitter streaming API; once a tweet is collected,
we query its url (https://twitter.com/
USERID/status/TWEETID) at regular intervals
and infer its status from the resulting http response
code. We interpret an HTTP 200 response as an indi-
cation a tweet still exists and an HTTP 404 response,
which indicates the tweet is unavailable, as indicat-
ing deletion. A user changing their privacy settings
can also result in an HTTP 403 response; we do not
consider this to be a deletion. Other response codes,
which appear quite rarely, are treated as anomalies
and ignored. All non HTTP 200 responses are re-
tried twice to ensure they are not transient oddities.
To determine when a tweet is deleted, we at-
tempted to access each tweet at time points Ti =
5 ? 4i minutes for i = 0, 1 . . . 7 after the creation
time. These roughly correspond to periods of 5 min-
utes, 20 minutes, 1.5 hours, 6 hours, 1 day, 4 days,
2 weeks, and 2 months. While we assume that user
deletion is the main cause of a tweet becoming un-
available, other causes are possible such as the cen-
sorship of illegal contents by Twitter (Twitter, 2012).
Our sample data was collected from July 31
through October 31, 2012 and contains 522,984 bul-
lying traces. Because of intermittent network and
computer issues, several multiple day data gaps ex-
ist in the data. To combat this, we filter our data to
include only tweets of unambiguous status. If any
check within the 20480 minutes (about two weeks)
interval returns an HTTP 404 code, the tweet is
no longer accessible and we consider it deleted. If
the 20480 minute or 81920 minute check returns an
HTTP 200 response, that tweet is still accessible and
we consider it surviving. The union of the surviving
and deleted groups formed our cleaned dataset, con-
taining 311,237 tweets in total.
3 Exploratory Data Analysis
A user?s decision to delete a bullying trace may be
the result of many factors which we would like to
isolate and understand. In this section we will ex-
amine several such possible factors.
3.1 Word Usage
Our dataset contains 331,070 distinct words and we
are interested in isolating those with a significantly
higher presence among either deleted or surviving
tweets. We define the odds ratio of a word w
r(w) =
P (w | deleted)
P (w | surviving)
,
where P (w | deleted) is the probability of word w
occurring in a deleted tweet, and P (w | surviving) is
the probability of w appearing in a surviving tweet.
In order to ensure stability in the probability estima-
tion, we only considered words appearing at least 50
times in either the surviving or deleted corpora.
Following (Bamman, OConnor, and Smith,
2012), we qualitatively analyzed words with ex-
treme values of r(w), and found some interesting
trends. There was a significant tendency for ?jok-
ing? words to occur with r(w) < 0.5; examples in-
clude ?xd,? ?haha,? and ?hahaha.? Joking words oc-
cur less frequently in deleted tweets than surviving
ones. On the other end of the spectrum, there were
no joking words with r(w) > 2. What we found
instead were words such as ?rip,? ?fat,? ?kill,? and
?suicide.? While it is relatively clear that joking is
less likely to occur in deleted tweets, there was less
of a trend among words appearing more frequently
in deleted tweets.
3.2 Surviving Time
Let N be the total number of tweets in our cor-
pus, and D(Ti) be the number of tweets that were
first detected as deleted at minute Ti after creation.
Note that D(Ti) is not cumulative over time: it in-
cludes only deletions that occurred in the time inter-
val (Ti?1, Ti]. Then we may define the deletion rate
at time Ti as
RT (Ti) =
D(Ti)
N(Ti ? Ti?1)
.
In other words, RT (t) is the fraction of tweets that
are deleted during the one minute period (t, t+ 1).
We plot RT vs. t using logarithmic scales on both
axes in Figure 1 and the result is a quite strong linear
trend. Fitting the plot with a linear regression, we
derive an inverse relationship between RT and t of
the form
RT (t) ? 1/t.
698
5 minutes 1.5 hours 1 day 2 weeks1E?7
1E?6
1E?5
1E?4
1E?3
t
R T(
t)
Figure 1: Deletion rate decays over time.
This result makes sense; the social effects of a par-
ticular bullying tweet may decay over time, making
regret less of a factor. Furthermore, the author may
assume an older tweet has already been seen, render-
ing deletion ineffective. Additionally, because the
drop off in deletion rate is so extreme, we are able to
safely exclude deletions occurring after two weeks
from our filtered dataset without introducing a sig-
nificant amount of noise. Finally,
??
t=0RT (t) gives
the overall fraction of deletion, which in our case is
around 4%.
3.3 Location and Hour of Creations
Some bullying traces contain location meta-data in
the form of GPS coordinates or a user-created profile
string. We employed a reverse geocoding database
(http://www.datasciencetoolkit.org)
and a rule-based string matching method to map
these tweets to their origins (at the state level; only
for tweets within the USA). This also allowed us to
convert creation timestamps from UTC to local time
by mapping user location to timezone. Because
many users don?t share their location, we were only
able to successfully map 85,465 bullying traces to a
US state s, and local hour of day h. Among these
traces, 3,484 were deleted which translates to an
overall deletion rate of about 4%.
Let N(s, h) be the count of bullying traces cre-
ated in state s and hour h. Aggregating these counts
temporally yields NS(s) =
?
hN(s, h), while ag-
gregating spatially produces NH(h) =
?
sN(s, h).
Similarly, we can defineD(s, h),DS(s) andDH(h)
as the corresponding counts of deleted traces. We
can now compute the deletion rate
RH(h) =
DH(h)
NH(h)
, and RS(s) =
DS(s)
NS(s)
.
The top row of Figure 2 shows NH(h), DH(h),
and RH(h). We find that NH(h) and DH(h) peak
in the evening, indicating social media users are gen-
erally more active at that time. The peak of RH(h)
appears at late night and, while there are multiple
potential causes for this, we hypothesize that users
may fail to fully evaluate the consequences of their
posts when tired. The bottom row of Figure 2 shows
NS(s), DS(S), and RS(s). The plot of NS(s)
shows that bullying traces are more likely to origi-
nate in California, Texas or New York which is the
result of a population effect. Importantly however,
the deletion rate RS(s) is not affected by population
bias and we see, as expected, that spatial differences
in RS(s) are small. We performed ?2-test to see if
a state?s deletion rate is significantly different from
the national average. We chose the significance level
at 0.05 and used Bonferroni correction for multiple
testing. Only four states have significantly differ-
ent deletion rates from the average: Arizona (6.3%,
p = 5.9?10?5), California (5.2%, p = 2.7?10?7),
Maryland (1.9%, p = 2.3 ? 10?5), and Oklahoma
(7.1%, p = 3.5? 10?5).
3.4 Author?s Role
Participants in a bullying episode assume well-
defined roles which dramatically affect the view-
point of the author describing the event. We trained
a text classifier to determine author role (Xu et al,
2012), and used it to label each bullying trace in the
cleaned corpus by author role: Accuser, Bully, Re-
porter, Victim or Other.
Table 1 shows that compared to tweets produced
by bullies, victims create more bullying traces, pos-
sibly due to an increased need for social support on
the part of the victim. More importantly, P (deleted |
victim) is higher than P (deleted | bully), a statis-
tically significant difference in a two-proportion z-
test. Possibly, victims are more sensitive to their au-
dience?s reaction than bullies.
3.5 Teasing
Many bullying traces are written jokingly. We built a
text classifier to identify teasing bullying traces (Xu
et al, 2012) and applied it to the cleaned corpus.
Table 2 shows that P (deletion | Teasing) is much
lower than P (deletion | Not Teasing) and the differ-
ence is statistically significant in a two-proportion z-
699
NH(h) DH(h) RH(h)
NS(s) DS(s) RS(s)
Figure 2: Counts and deletion rates of geo-tagged bullying traces.
Deleted Total P (deleted | Role)
Accuser 2541 50088 5.07%
Bully 1792 30123 5.95%
Reporter 11370 147164 7.73%
Victim 6497 83412 7.79%
Other 41 450 9.11%
Table 1: Counts and deletion rate for different roles.
Deleted Total P (deleted | Teasing?)
Yes 858 22876 3.75%
Not 21383 288361 7.42%
Table 2: Counts and deletion rate for teasing or not.
test. It seems plausible that authors are less likely to
regret teasing posts because they are less controver-
sial and have less potential to generate negative au-
dience reactions. This also corroborates our findings
in word usage that joking words are less frequent in
deleted tweets.
4 Predicting Regrettable Tweets
Once a bullying tweet is published and seen by oth-
ers, the ensuing effects are often impossible to undo.
Since ill-thought-out posts may cause unexpectedly
negative consequences to an author?s reputation, re-
lationship, and career (Wang et al, 2011), it would
be helpful if a system could warn users before a po-
tentially regrettable tweet is posted. One straightfor-
ward approach is to formulate the task as a binary
text categorization problem.
We use the cleaned dataset, in which each tweet
is known to be surviving or deleted after 20480 min-
utes (about two weeks). Since this dataset contains
22,241 deleted tweets, we randomly sub-sampled
the surviving tweets down to 22,241 to force our
deleted and surviving datasets to be of equal size.
Consequentially, the baseline accuracy of the clas-
sifier is 0.5. While this does make the problem ar-
tificially easier, our initial goal was to test for the
presence of a signal in the data.
We then followed the preprocessing procedure
in (Xu et al, 2012), performing case-folding,
anonymization, and tokenization, treating URLs,
emoticons and hashtags specially. We also chose
the unigrams+bigrams feature representation, only
keeping tokens appearing at least 15 times in the cor-
pus.
We chose to employ a linear SVM implemented
in LIBLINEAR (Fan et al, 2008) due to its effi-
ciency on this large sparse text categorization task
and a 10-fold cross validation was conducted to eval-
700
uate its performance. Within the first fold, we use
an inner 5-fold cross validation on the training por-
tion to tune the regularization parameter on the grid
{2?10, 2?9, . . . , 1}; the selected parameter is then
fixed for all the remaining folds.
The resulting cross validation accuracy was 0.607
with a standard deviation of 0.012. While it is statis-
tically significantly better than the random-guessing
baseline accuracy of 0.5 with a p-value of 5.15 ?
10?10, this accuracy is nevertheless too low to be
useful in a practical system. One possibility is that
the tweet text contains very limited information for
predicting inaccessibility; a user?s decision to delete
a tweet potentially depends on many other factors,
such as the conversation context and the characteris-
tics of the author and audience.
In the spirit of exploring additional informative
features for deletion prediction, we also used the
teasing and author role classifiers in (Xu et al,
2012), and appended the predicted teasing, and au-
thor role labels to our feature vector. This aug-
mented feature representation achieved a cross val-
idation accuracy of 0.606, with standard deviation
0.007; not statistically significantly different from
the text-only feature representation. While it seems
that a signal does exist, leveraging it usefully in real
world scenarios may prove challenging due to the
highly-skewed nature of the data.
5 Discussion
There have been several recent works examin-
ing causes of deletion in social media. Wang
et al (2011) qualitatively investigated regret associ-
ated with users? posts on social networking sites and
identified several possible causes of regret. Bamman
et al (2012) focused on censorship-related deletion
of social media posts, identifying a set of sensitive
terms related to message deletion through a statisti-
cal analysis and spatial variation of deletion rate.
Assuming that deletion in social media is indica-
tive of regret, we studied regret in a bullying con-
text by analyzing deletion trends in bullying re-
lated tweets. Through our analysis, we were able
to isolate several factors related to deletion, includ-
ing word usage, surviving time, and author role. We
used these factors to build a regret predictor which
achieved statistically significant results on this very
noisy data. In the future, we plan to explore more
factors to better understand deletion behavior and re-
gret, including users? recent posts, historical behav-
ior, and other statistics related to their specific social
network.
Acknowledgments
We thank Kwang-Sung Jun, Angie Calvin, and
Charles Dyer for helpful discussions. This work
is supported by National Science Foundation grants
IIS-1216758 and IIS-1148012.
References
Bamman, David, Brendan OConnor, and Noah Smith.
2012. Censorship and deletion practices in chinese so-
cial media. First Monday, 17(3-5).
Child, Jeffrey T., Paul M. Haridakis, and Sandra Petro-
nio. 2012. Blogging privacy rule orientations, privacy
management, and content deletion practices: The vari-
ability of online privacy management activity at differ-
ent stages of social media use. Computers in Human
Behavior, 28(5):1859 ? 1872.
Child, Jeffrey T, Sandra Petronio, Esther A Agyeman-
Budu, and David A Westermann. 2011. Blog scrub-
bing: Exploring triggers that change privacy rules.
Computers in Human Behavior, 27(5):2017?2027.
Christofides, Emily, Amy Muise, and Serge Desmarais.
2009. Information disclosure and control on facebook:
are they two sides of the same coin or two different
processes? CyberPsychology & Behavior, 12(3):341?
345.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Jimerson, Shane R., Susan M. Swearer, and Dorothy L.
Espelage. 2010. Handbook of Bullying in Schools: An
international perspective. Routledge/Taylor & Francis
Group, New York, NY.
Mishna, Faye and Ramona Alaggia. 2005. Weighing the
risks: A child?s decision to disclose peer victimization.
Children & Schools, 27(4):217?226.
Shelley, Danielle and Wendy M Craig. 2010. Attri-
butions and coping styles in reducing victimization.
Canadian Journal of School Psychology, 25(1):84?
100.
Twitter. 2012. The twitter rules. http:
//support.twitter.com/articles/
18311-the-twitter-rules.
701
Valkenburg, Patti M and Jochen Peter. 2009. Social
consequences of the internet for adolescents a decade
of research. Current Directions in Psychological Sci-
ence, 18(1):1?5.
Walther, Joseph B. 1996. Computer-mediated commu-
nication impersonal, interpersonal, and hyperpersonal
interaction. Communication research, 23(1):3?43.
Wang, Yang, Gregory Norcie, Saranga Komanduri,
Alessandro Acquisti, Pedro Giovanni Leon, and Lor-
rie Faith Cranor. 2011. ?I regretted the minute I
pressed share?: a qualitative study of regrets on face-
book. In Proceedings of the Seventh Symposium on
Usable Privacy and Security, SOUPS ?11, pages 10:1?
10:16. ACM.
Xu, Jun-Ming, Kwang-Sung Jun, Xiaojin Zhu, and Amy
Bellmore. 2012. Learning from bullying traces in so-
cial media. In Proceedings of the 2012 Conference
of the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 656?666, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
702
