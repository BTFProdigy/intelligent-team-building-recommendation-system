A Hierarchical Parsing Approach with Punctuation Processing for 
Long Chinese Sentences 
Xing Li  
Institute of Automation,  
Chinese Academy of 
Sciences, Beijing 100080 
xli@nlpr.ia.ac.cn 
Chengqing Zong  
Institute of Automation,  
Chinese Academy of 
Sciences, Beijing 100080 
 cqzong@nlpr.ia.ac.cn
Rile Hu 
Institute of Automation,  
CAS, Beijing 100080 
Nokia (China) Research 
Center, Hepingli Dongjie 
11,Beijing 100013 
rlhu@nlpr.ia.ac.cn 
 
Abstract 
In this paper, the usage and function of 
Chinese punctuations are studied in 
syntactic parsing and a new 
hierarchical approach is proposed for 
parsing long Chinese sentences. It 
differentiates from most of the previous 
approaches mainly in two aspects. 
Firstly, Chinese punctuations are 
classified as ?divide? punctuations and 
?ordinary? ones. Long sentences which 
include ?divide? punctuations are 
broken into suitable units, so the 
parsing will be carried out in two stages. 
This ?divide-and-rule? strategy greatly 
reduces the difficulty of acquiring the 
boundaries of sub-sentences and 
syntactic structures of sub-sentences or 
phrases simultaneously in once-level 
parsing strategy of previous approaches. 
Secondly, a grammar rules system 
including all punctuations and 
probability distribution is built to be 
used in parsing and disambiguating. 
Experiments show that our approach 
can significantly reduce the time 
consumption and numbers of 
ambiguous edges of traditional 
methods, and also improve the 
accuracy and recall when parsing long 
Chinese sentences. 
1 Introduction 
Until recently, although punctuations are clearly 
important parts of the written Chinese, many 
Chinese parsing systems developed to date have 
simply ignored them. Some researches have 
been done on English punctuations in parsing [1, 
2, 3, 4, 5], their researches have used plenty of 
theoretical and experimental facts to prove that 
it is effective to incorporate punctuation 
information into parsing of long complex 
sentences. But as far as we know, little work has 
been done in Chinese syntactic parsing. 
Because the derivation of Chinese punctuations 
was referring to western language [3], they have 
many similarities in usage. Researches on 
Chinese punctuations in parsing will be valuable. 
However, our study shows, there are still 
differences between them, special research on 
Chinese punctuations is necessary. 
In this paper, differences in English and Chinese 
punctuations are compared and the special 
difficulty and corresponding cause in parsing 
Chinese long sentences are analyzed. Then a 
new hierarchical parsing (HP) approach is 
proposed instead of traditional parsing (TP) 
method. This ?divide-and-rule? strategy greatly 
reduces the time consumption. Open test shows, 
parsing accuracy and recall of HP method are 
both about 7% higher than those of TP. 
The remainder of this paper is organized as 
follows: Section 2 is related work. Section 3 
mainly discusses the special difficulties and 
solution in parsing long Chinese sentences. Then 
HP method is discussed in detail in Section 4. 
Section 5 gives the final experiment results and 
corresponding analyses. Finally, the further 
work is expected. 
2 Related Work  
Nunberg?s The Linguistics of Punctuation [2] is 
the foundation for most of the latter researches 
7
in syntactic account of punctuation. In his 
important study, he advocates two separate 
grammars, operating at different levels. A 
lexical grammar accounts for the text-categories 
(text-clauses, text-phrases) occurring between 
the punctuation marks, and a text grammar deals 
with the structure of punctuation, and the 
relation of those marks to the lexical expressions 
they separate.  
Based on above theory, Jones proposes his 
method which uses an integrated grammar. He 
divided punctuations into conjoining and 
adjoining punctuation. Conjoining punctuations 
can be used to indicate coordinate relationship 
between components. Adjoining punctuations, 
otherwise, only can be attached to their adjacent 
sentence components. In Jones? theory, in a 
sense, conjoining punctuation could also be 
treated under the adjunctive principle [3]. So, 
punctuations in his theory are still attached to 
adjacent lexical expressions. An integrated 
syntactic punctuation grammar is given. 
Jones? method shows good modularity. 
However, the grammars he designed can only 
cover a subset of all punctuation phenomena. 
His experiment shows that when parsing a set of 
ten previously unseen punctuationally complex 
sentences, seven of the ten are unparsable! 
In Chinese, Zhou Qiang[6] has used 
punctuations to do automatic acquisition of 
coordinate phrases. In machine translation, 
Chengqing Zong[7] and Huang He-yan[8] have 
used punctuations associating with relative 
pronouns to segment complex sentences into 
several independent simple sentences. Above all, 
none of previous work has carried out a 
thorough study on punctuations from the 
syntactic point of view. 
3 Motivations 
3.1 Differences between Chinese and 
English Punctuations 
In Chinese, there are some punctuations which 
don?t exist in English. The first one is a pair of 
Chinese book-name mark ??? and ???, which 
are obvious marks that the content between them 
must be name of a book. The second one is 
pause mark ???, which replaces comma as the 
separating mark between coordinate components. 
For instance, sentence ?I like to walk, skip, and 
run.? can be translated into Chinese one as ??
??????????. Chinese pause mark is 
the evident mark with the exclusive usage is to 
separate coordinate words or simple phrases, so 
it is easier to get coordinate words or simple 
phrases in Chinese sentences. 
3.2 Special Difficulty in Parsing Long 
Chinese Sentences  
In essence, English is a kind of hypotaxis 
language, so an intact syntax structure denotes a 
sentence. When several simple sentences are 
connected to form a compound sentence, there 
should be obvious conjunctions between them. 
Differently, Chinese is a kind of parataxis 
language, and the language unit which expresses 
a complete thought is an intact Chinese sentence. 
Therefore, several sentences with associative 
meanings can be connected by come 
punctuations to form a compound one without 
any conjunctions. This type of sentence is called 
?run-on sentence?, and which is prevalent in 
Chinese. For example, we randomly selected 
4431 sentences whose lengths are over 30 
characters from a Chinese corpus named TCT 
973.1 There are 1830 run-on sentences, covering 
41.3%. Chinese sentence ?????????
?????????????? is this kind of 
sentence. The corresponding English meaning is 
?Now, I am not young and I still have to take 
bus to work everyday,  which make me very 
tired?. So, in above Chinese sentences, commas 
are used not only as separating marks of sub-
sentences but also as separating marks of 
components in one sub-sentence. However, lack 
of connections makes methods [7, 8] of 
segmenting complex sentences invalid. In this 
situation, acquisition of the boundaries of sub-
sentences and syntactic structure of sub-
sentences or phrases should be done 
simultaneously in once-level parsing strategy, 
which will undoubtedly increase the difficulty of 
parsing long sentences. 
3.3 Corresponding Solution   
In order to solve this problem, a hierarchical 
parsing (HP) approach is proposed by us. 
Nunberg?s theory of two categories grammars 
provides us the theoretical base of HP approach. 
                                                        
1 Please refer to http://www.chineseldc.org/ 
8
According to his definition of two categories of 
grammars described in section 2, the two 
grammars can operate at different levels 
independently. Punctuations which can occur as 
elements of text grammar are defined by us as 
?divide? punctuations. Then punctuations which 
can occur as elements of lexical grammar are 
?ordinary? ones. The ?divide? punctuations can 
be used to divide the whole sentence into several 
parts. Then the parsing will be carried out in two 
steps. Thus, acquisition of syntactic structure of 
sub-sentences or phrases is done in the first level 
parsing, and acquisition of the boundaries of 
sub-sentences and relationship of sub-sentences 
or phrases can be done in second level parsing. 
This is the main idea of HP approach, which can 
reduce the difficulty of parsing run-on sentences 
and other types of compound sentences. The 
framework of HP approach is shown as 
following Figure 1:  
 Original
sentence
Sub-sen.1
Sub-sen.3
Sub-sen.2
Sub-sen.n
??
Sentence
Division
Sub-tree 1
Sub-tree 3
Sub-tree 2
Sub-tree n?
Sub-tree 1'
Sub-tree 2'
Sub-tree n
Parsing tree
of whole sen
First level
parsing
Second level
parsing
Detection of
Improper Division
and Combination
? ?
? ?
?
 
Figure 1. Framework of HP Approach 
4  Hierarchical Parsing Approach 
4.1 Classification of Chinese Punctuations 
In this paper, the ?divide? punctuations are 
defined as follows: If lexical sentences or 
phrases which are separated by certain 
punctuations must be correlative to each other 
wholly not partly, these punctuations are in level 
of text grammar, which are classified as ?divide? 
punctuations. Punctuations in a and b of Figure 
2 are examples of two categories of punctuations 
( P stands for punctuations). 
In Chinese, the semicolon is used to separate 
coordinate sub-sentences. The colon is used as 
separation mark of interpretative phrases or sub-
sentences from former sub-sentences. So, 
according to above definition, they can be 
classified as ?divide? punctuations. The comma, 
specially, can occur as a mark of coordinate 
phrases element. So, using of it as ?divide? 
punctuation may cause improper division 
problems and a compensatory solution is 
introduced in, which will be discussed in detail 
in Section 4.3.3.  
 
 
Figure  2. ?Divide? punctuations (first) and 
?ordinary? punctuations (second) 
4.2 Grammar Rules 
The automatic extraction of grammar rules 
which include punctuations depends on large 
scales of parsed Chinese corpus which has 
ample syntactic phenomena and standard usage 
of punctuations. Fortunately, Chinese tree-bank 
named TCT 973 is such a corpus. It includes 
1,000, 000 words and covers all kinds of text 
after 1990th. The average length of each sentence 
is 23.3 words. Long sentences of over 20 words 
length account for half of it. 
Firstly, original grammar rules are extracted. 
Then generalizations are done about the use of 
the various punctuation marks from the rules set. 
For example, as mentioned before, Chinese 
book-name mark ?? ? and ?? ?are obvious 
marks that the content between ? ? ? and 
???must be name of a book by any syntactic 
category. Therefore, a generalized rule can be 
deduced as below: 
        :{ , , ,  ......}?NP X X NP VP S PP? ?  (1) 
In above generalized rule, X can be any POS of 
phrases or single word, so possible rules that 
have not been deduced from tree-bank are added 
into the grammar rules set with probabilities 1. 
Except for above special situations, 
corresponding probabilities of all grammar rules 
are computed by Maximum Likelihood Estimate 
9
(MLE) method.At last, all rules are combined to 
form an intact grammar system. 
4.3  Parsing Strategy 
4.3.1  Sentence Division 
Depending on above classification, commas, 
semicolons and colons are used to divide 
sentences into a series of sub-sentences. Notice 
that quotation marks and parenthesis are treated 
as transparent and syntactically non-functional. 
4.3.2  First Level Parsing  
All sub-sentences and phrases gotten from the 
division processing are inputs of the first level 
parsing. A chart parsing algorithm is used here. 
The grammar rules and corresponding 
probabilities are used to do parsing and 
disambiguating. Then for all sub-sentences and 
phrases, their parsing trees are the highest 
probabilities ones of all possible trees.  
4.3.3 Detection of Improper Division and 
Combination  
Because of the specialty of comma, using of it 
as the division mark may cause improper 
divisions. The main causation is improper 
division between coordinate phrases which have 
been same component of the sentence. For 
example, Chinese sentence ?????????
???????????????????
????????????????? is a 
typical coordinate structure similar to ?I like to 
do ..., to do ..., to do..., but I like better to...? in 
English. So, the first three ????are coordinate 
predicates of the sentences.  Then the improper 
division will break up this relationship. In this 
section, a detection and combination method is 
proposed by us to solve this problem in parsing 
Chinese sentences.  
Because the lexical expressions surrounding 
punctuations are parsed in first level parsing, it 
is easy to get their internal syntactic structures 
information we need. Just a simple analysis 
procedure is needed to judge if there exists such 
a coordinate relationship between lexical 
expressions surrounding commas. 
A description of the analysis strategy is given 
according to this example. 
Just like Figure 3 shows, the components after 
the first comma are parsed as verb phrase (VP) 
marked as B. Obviously B is composed of a 
preposition phrase (PP) and a verb phrase. If 
there exists a minimal length of phrase 
immediately before the first comma and this 
phrase has totally the same structure to phrase B, 
then they are coordinate phrases. In Figure 3, A2 
is such a phrase. The components after other 
commas are analyzed similarly. Finally, A2, B 
and C are coordinate phrases. Since the verb 
phrase D immediately after the second comma 
has obviously different structure from A2, B and 
C, so they aren?t coordinate components. The 
part-of-speech tags throughout this paper follow 
the standard of TCT973.  
 
Figure 3. Syntactic structure of example 
sentence  
Through the above analysis, we can see that the 
first and second commas are actually in level of 
lexical grammar, using them as ?divide? 
punctuations will cause the improper division as 
shown in Figure 2 of b. Therefore, we present a 
method to use sub-tree adjoining operation, 
firstly combine the sub-tree A2 with tree B and C, 
then use the new tree A2? to replace original A2  
without changing original structure of A . Figure 
4 shows the adjoining procedure.  
 
Figure 4. Sub-tree adjoining operation 
Then the execution conditions and results of 
such adjoining operation are summarized as 
following rules:  
 
[ ] [ ... ] [ [[ ] ] ... ]+ +?S X Y Y S X X X Y YX? ? ? ?
 
X = {NP, VP, AP, DP} ,  S stands for sentence,  
Y = * ( any legal POS)  
(2) 
[ ... ][ ] [ ... [ [ ] ]]+ +?S Y Y X S Y Y X X XX? ? ? ?
 
X = {NP, VP, AP, DP} ,  S stands for sentence,  
Y = * ( any legal POS)  
(3) 
10
 The execution conditions of both Rule (2) and (3) 
are defined as follows:  all X should be 
coordinate phrases with the same syntactic 
categories.  
4.3.4  Second Level Parsing 
The parsing algorithm of this module is totally 
the same to the first level parsing; with the 
difference is the input string. At the first parsing 
stage, inputs are POS sequence of words, but at 
the second parsing stage, inputs are POS 
sequence of all sub-tree root nodes. After this 
stage of parsing, the best parsing trees of whole 
sentences will be constructed.  
5 Performance Evaluation 
5.1 Test Sentences 
The primary aim of the HP strategy is to take 
use of the punctuation information to help to 
conquer the difficulty of parsing long sentences. 
Chinese sentences with over 20 words are 
generally regarded as long sentences. Therefore, 
we conduct experiments on the sentences with 
the length over 20 words. 
Firstly, 8,059 sentences were chosen randomly 
from TCT 973 as train set. The 3,795 PCFG 
rules used in our system are extracted from the 
train set after generalizing. Then, for other 847 
sentences, whose lengths are less than 20 words 
are filtered and 420 sentences are finally 
conserved as our open test data set. Distribution 
of these sentences is shown in Table 1 below:  
Text Type Num
ber of 
Sen 
Length of 
Sen 
(Words) 
Average 
Length of 
Sen (Words) 
Literature 116 21?123 36.06 
News 123 22?100 37.73 
Science 114 21?131 39.47 
Practical writing 67 20?98 38.36 
Total 420 20?131 37.84 
Table 1. Distribution of test sentences 
 
5.2  Efficiency Evaluation  
In order to compare our HP approach with TP 
method of once-parsing algorithm, we do 
compared experiments using same data set in 
Table 1 and same grammar rules set.  
5.2.1  Time Consumption Evaluation  
Running two systems on a PC (Pentium 4, 
1.20GHz, 256M of RAM), their time 
consumptions are shown in Figure 5. 
Time Consumed Comparison
0
20
40
60
80
100
120
140
160
20 40 60 80 100 120 140
Length of Sentence(Words)
T
i
m
e
 
C
o
n
s
u
m
e
d
(
s
e
c
)
HP method
TP method
 
Figure 5. Time consumption 
In our experiment system, we set the upper limit 
execution time as 120 seconds per sentence, 
judging at the end of every algorithm cycle. 
When parsing time of the sentence is overtime, 
the system will exit without getting final result. 
Experiment results shown in Fig.5 prove that 
time efficiency of HP method is greatly superior 
to TP, especially when the sentence has more 
than 40 words. With the increasing of sentence 
length, it is more difficult for TP method to 
parse successfully. 
5.2.2  Accuracy and Recall Evaluation 
Firstly, Table 2 shows numbers of sentences 
failed to be parsed in two methods with the time 
limitation of 120 seconds. 
 
 
Methods 
Numbers of 
Test  Sen 
Numbers of 
Failed Sen Ratio 
TP 420 97 23.1% 
HP 420 16 3.8% 
Table 2. Ratio of failed sentences 
It is evident that HP method can largely reduce 
failed sentences in given time limitation. 
Then, except for failed sentences, only 
considering the successfully parsed sentences, 
the parsing accuracy and recall of the two 
methods should be compared. The standard 
PARSEVAL measures [9] are used to evaluate 
two methods. Results are shown in Table 3.  
From Table 3, we can see that the total parsing 
accuracy and recall of HP method are both 
almost 7% higher than those of TP method. 
Amounts of average crossing brackets are also 
reduced greatly. 
11
Analyzing data in Table 3, to different text types, 
the accuracy and improvement effect of TP 
method are slightly different. Sentences of 
literature text have the highest parsing accuracy 
and recall. Studied show that there are 97 ?run-
on sentences? in the 116 literature text sentences, 
covering 84%. The comparatively higher 
accuracy and recall of these sentences prove that 
our HP approach is effective. 
Text 
type 
Meth
od 
LP% LR% CBs 0CB
% 
?2CB
s% 
TP 67.31 66.76 6.97 19.77 48.84 Literatu
re HP 73.57 73.77 3.24 40.74 62.09 
TP 61.05 61.69 5.80 10.47 34.88 News HP 70.66 70.58 3.52 28.33 61.83 
TP 61.20 60.89 5.63 12.66 37.97 Science HP 68.74 68.98 4.14 23.37 59.10 
TP 64.10 64.61 6.17 6.25 27.08 Practical 
writing HP 66.55 67.81 4.68 21.54 50.77 
TP 63.38 63.41 6.14 13.04 38.46 Total HP 70.06 70.03 3.80 30.24 61.01 
Table 3. Results using standard PARSEVAL 
measures 
Sentences of application have lowest parsing 
accuracy and smallest improvement. Because 
comparing to other three types, sentences of this 
type have more long nested noun phrases or 
coordinate components, such as long 
organization names and commodity names, 
which will cause noun phrase combination 
disambiguation. 
6  Conclusion and Future Work 
This paper studies the usage and function of 
Chinese punctuations in syntactic parsing. A 
new hierarchical parsing approach is proposed. 
Besides, a grammar rules system including all 
punctuations and probability distribution is built 
to be used in parsing and disambiguation. 
Compared experiments prove that HP method is 
effective in long Chinese sentences parsing. 
In future work, theories of punctuations should 
be studied further to get a more formal point of 
view. 
Acknowledges 
The research work is supported by the national 
natural science foundation of China under grant 
No.60375018 and 60121302, and also supported 
by the outstanding oversea scholar foundation of 
CAS. 
References 
1. Benard Jones, Towards a Syntactic Account of 
Punctuation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING-96), Copenhagen, Denmark, 
August . (1996b) 
2. Geoffrey Nunberg. The Linguistics of Punctuation. 
CSLI Lecture Notes, No. 18, Stanford CA, (1990) 
3. Benard Jones,What?s the Point? A (Computational ) 
Theory of Punctuations. PhD thesis, Centre for 
Cognitive Science, Universito of Edinburgh, 
Edinburgh, UK, (1997) 
4. Edward Briscoe. The Syntax and Semantics of 
Punctuation and its Use in Interpretation. In 
Proceedings of the ACL/SIGPARSE International 
Meeting on Punctuation in Computational 
Linguistics, Santa Cruz, California. (1996) 1?7. 
5. Charles Meyer. A Linguistic Study of American 
Punctuation. Peter Lang: New York. 1987. 
6. Zhou Qiang. The Chunk Parsing Algorithm for 
Chinese Language. In Proceedings of JSCL'99, 
(1999) 242-247 
7. Chengqing Zong, Yujie Zhang, Kazuhide 
Yamamoto, Masashi Sakamoto,etc. Chinese 
Utterance Paraphrasing for Spoken Language 
Translation, In Journal of Chinese Language 
Computing, Singapore, 2002,12 (1): 63-77.  
8. Huang He-yan, Chen Zhao-xiong, The Hybrid 
Strategy Processing Approach of Complex Long 
Sentence, In Journal of Chinese Information 
processing, 2002,16(3):1-7.  
9. E.Charniak, ?Statistical parsing with a context-free 
grammar and word statistics?. In Proc of AAAI?97, 
1997. 
 
12
NOKIA Research Center Beijing Chinese Word Segmentation System 
for the SIGHAN Bakeoff 2007 
 
Jiang LI1, 2, Rile HU1, Guohua ZHANG1, Yuezhong TANG1,  
Zhanjiang SONG1 and Xia WANG1 
NOKIA Research Center, Beijing1 
Beijing University of Posts and Telecommunications2 
{ext-jiang.1.li, ext-rile.hu,ext-guohua.zhang,yuezhong.tang, 
zhanjiang.song,xia.s.wang}@nokia.com 
 
 
Abstract 
This paper presents the Chinese word 
segmentation system developed by NOKIA 
Research Center (NRC), which was 
evaluated in the Fourth International 
Chinese Language Processing Bakeoff and 
the First CIPS Chinese Language 
Processing Evaluation organized by 
SIGHAN. In our system, a preprocessing 
module was used to discover the out-of-
vocabulary words which occur repeatedly 
in the text, then an improved n-gram model 
was used for segmentation and some post 
processing strategies are adopted in system 
to recognize the organization names and 
new words. We took part in three tracks, 
which are called the open and closed track 
on corpora State Language Commission of 
P.R.C. (NCC), and closed track on corpora 
Shanxi University (SXU). Our system 
achieved good performance, especially in 
the open track on NCC, our system ranks 
1st among 11 systems. 
 
1 Introduction 
Chinese word segmentation is an essential and core 
technology in Chinese language processing, and 
generally it is the first stage for later processing, 
such as machine translation, text summarization, 
information retrieval and etc. The topic of Chinese 
word segmentation has been researched for many 
years. Many approaches have been developed to 
solve the problems under this topic. Among these 
approaches, statistical approaches are most widely 
used. 
Our system based on a pragmatic approach, 
integrating a lot of features and information, the 
framework is similar to (Jianfeng Gao, 2005). In 
our system, the model is simplified to n-gram 
architecture. First, all the possible paths of 
segmentation will be considered and each of the 
candidate word will be categorized into a certain 
type. Second, each word will be given a value; 
each type has different computational strategy and 
is processed in different ways. At last, all the 
possible paths of segmentation are calculated and 
the best path is selected as the final result. 
 N-gram language model is a generative model, 
and it could express the correlation of the context 
word very well. But it is powerless to detect the 
out-of-vocabulary word (OOV). In the post-
processing module, we detect the OOV through 
some Chinese character information instead of the 
word information. In addition, to deal with the long 
organization names in NCC corpus, a module for 
combining organization name is adopted.       
The remainder of this paper is organized as 
follow: section 2 describes our system in detail; 
section 3 presents the experiment results and 
analysis; in last section we give our conclusions 
and future research directions. 
 
2 System Description 
The basic architecture of our system is shown in 
figure 1, and the detailed description of each 
module is provided in the following subsections. 
86
Sixth SIGHAN Workshop on Chinese Language Processing
2.1 Framework 
The input of the system is text to be segmented. 
First, the system scans the text and finds out the 
character strings appear many times but not 
lexicon words. These strings are called recurring-
OOV. Second, all the candidate words are 
categorized into different types and the optimal 
path is calculated by Viterbi algorithm. Finally, 
some post-processing strategies are used to modify 
the results: NW detection is used to merge two 
single characters as a new word, and organization 
combination is provided to combine some words as 
an organization name.    
 
 
 Figure 1: Framework of system 
 
2.2 Recurring-OOV Detection 
We found that there are some words which appear 
many times in different position of the context. For 
example, a verb ??? ? appears 2 times in a 
sentence, ???? ????????????
????????????, and a person name 
????? appears in different sentences, ????
????????????? ????????
????????. These words are defined as 
Recurring-OOV.   
    Therefore, without any prior knowledge, the 
Chinese text is scanned; the sentence of the text is 
compared with itself and compared with others 
which were close to it for finding out the repeated 
strings. All these repeated character strings were 
saved in list. Not all of them are considered as the 
candidates of OOV word.  Only 2-character or 3-
character repeated strings are considered as the 
candidates of OOV words. And some simple rules 
are used to avoid some wrong classification. For 
example, if there is a repeated string contains 
character ?? ?, which is a high frequent one-
character word, this repeated string is not 
considered as a recurring OOV. 
A value (probability) will be given to each 
Recurring-OOV. Two factors will be considered in 
the value evaluation? 
1? The repeating times of the Recurring-
OOV in the testing corpus. The more it 
repeats, the bigger the value will be. 
2? Character-based statistical information 
and some other information are also 
considered to calculate the probability of 
this string to be a word. The computing 
method is described in Section 2.5, NW 
Detection. 
 
2.3 Word Categorization 
In our system, Chinese words are categorized into 
a set of types as follows: 
1.  Lexicon Words (LW). The words in this type 
can be found in the library we get from the training 
corpus. 
2. Factoids (FT). This type includes the English 
letter, Arabic numerals and etc.  
3. Named Entity (NE). This type includes person 
name and location name. Being different from the 
Gao?s system, the organization detection is a post 
processing in our system. 
4. Recurring-OOV. This type is described in the 
section 2.2. 
5.  Others. 
 
2.4 N-gram Model 
Each candidate word Wi is relegated to a type Tj 
and assigned a value P(Wi |Tj) . The transfer 
probability between word-types is also assigned a 
value P(Tj-1|Tj). We assume that W = W1W2?Wn 
is a word sequence, the segmentation module?s 
task is to find out the most likely word sequence 
among all possible paths: 
 W* = arg max ? P(Wi|Tj)P(Tj-1|Tj)        (1) 
87
Sixth SIGHAN Workshop on Chinese Language Processing
Viterbi algorithm is used to search the optimized 
path described in Equation (1). 
 
2.5 NW Detection 
This module is used to detect the New Words, 
which ?refer to OOV words that are neither 
recognized as named entities or factoids? (Jianfeng 
Gao, 2005). And in particular, we only consider 
the 2-character words for the reason that 2-
character words are the most common in Chinese 
language.  
We identify the new words through some 
features of Chinese character information: 
1. The probability of a character occurring in the 
left/right position.   
    In fact, most Chinese characters have their 
favorite position. For example, ??? almost occurs 
in the left, ??? almost occurs in the right and ??? 
always compose a single word by itself. So the 
string ???? is much more possible to be a new 
word than ?????and string ???? is not likely to 
be a word. 
2. The similarity of different characters.  
    If two characters often occur in the same 
position with the same character to form a word, it 
is considered that the two characters are similar, or 
there is a short distance between them. For 
example, the character ??? is very similar to ??? 
in respect that they are almost in the left position 
with some same characters, such as ???, ???, to 
construct the word ????,????,????,????. 
So if we know the ???? is a word, we can 
speculate the string ???? is also a word. 
    The strict mathematical formula which used to 
describe the similarity of characters is reported in 
(Rile Hu, 2006).  
 
2.6 Organization combination 
The organization name is recognized as a long 
word in the NCC corpus, but during the n-gram 
processing, these long words will be segmented 
into several shorter words. In our system, the 
organization names are combined in this module. 
First, a list of suffix-words of organization name, 
such as ??? ? ??? ?, is selected from the 
training set. Second, the string that has been 
segmented in previous module is searched to find 
out the suffix-word, which is considered as a 
candidate of organization name.  At last, we 
estimate the possibility of the candidate string and 
judge it is an organization name or not. 
 
3 Evaluation Results 
3.1 Results 
We took part in three segmentation tasks in 
Bakeoff-2007, which are named as the open and 
closed track on corpora State Language 
Commission of P.R.C. (NCC), and closed track on 
corpora Shanxi University (SXU). 
Precision (P), Recall (R) and F-measure (F) are 
adopted to measure the performance of word 
segmentation system. In addition, OOV Recall 
(ROOV), OOV Precision (POOV) and OOV F-
measure (FOOV) are very important indicators 
which reflect the system?s ability to deal with the 
OOV words.   
The results of our system in three tasks are 
shown in Table 1. 
            Table 1: Test set results on NCC, SXU 
Corpus NCC-O NCC-C SXU-C 
R 0.9735 0.9417 0.9558 
P 0.9779 0.9272 0.9442 
F 0.9757 0.9344 0.95 
ROOV 0.8893 0.4001 0.5176 
POOV 0.8867 0.6454 0.6966 
FOOV 0.888 0.494 0.5939 
RIV 0.9777 0.9687 0.9794 
PIV 0.9824 0.9356 0.9539 
FIV 0.98 0.9518 0.9665 
3.2 NCC Open Track 
For the open track of NCC, an external corpus is 
used for training and the size of training set is 
about 54M. In addition, there are some special 
dictionary were added to identify some special 
words. For example, an idiom dictionary is used to 
find the idioms and a personal-name dictionary is 
used to identify the common Chinese names.   
3.3 Error Analysis 
Apart from ranking 1st in NCC open test, our 
system got not so good results in NCC close test 
and SXU close test.   
The comparison between our system results and 
best results in bakeoff-2007 are shown in table 2. 
88
Sixth SIGHAN Workshop on Chinese Language Processing
 
Table 2: The comparison between our system 
results and best results 
F-Measures Type 
Bakeoff-2007 Our system 
NCC-O 0.9757 0.9757 
NCC-C 0.9405 0.9344 
SXU-C 0.9623 0.95 
 
Table 3: The comparison between our system 
results and best Top 3 results in OOV 
identification  
 ROOV POOV FOOV 
Our  0.4001 0.6454 0.494 
1st  0.6179 0.5984 0.608 
2nd  0.4502 0.6196 0.5215 
NCC-C 
3rd  0.6158 0.5542 0.5834 
Our  0.5176 0.6966 0.5939 
1st  0.7429 0.7159 0.7292 
2nd  0.6454 0.7022 0.6726 
SXU-C 
3rd  0.6626 0.6639 0.6632 
 
    In table 3, 1st, 2nd and 3rd are the best Top 3 
systems in the test. It shows that in the close track 
in NCC, the OOV Precision of our system is the 
best, but the OOV Recall is the worst in all the four 
system. Similarly, in the close track in SXU, the 
OOV Precision is very close to the best one, and 
the OOV Recall is the worst. It means that our 
system is too cautious in identifying the OOV 
words.   
Our system was carefully tuned on NCC 
training set. The NCC training set contains articles 
from many domains; the OOV words can not be 
easily detected. Therefore, in parameter tuning, we 
raise the threshold of OOV. This strategy increases 
the precision of the OOV detection, but decreases 
the recall of this. And we also use some simple 
rules to filter the OOV candidates. These rules can 
easily pick out the wrongly detected OOVs, but at 
the same time, they remove some correct 
candidates by mistake. 
The performance of our system is good in NCC 
close test but not so good in SXU close test. This 
means that our strategies for OOV detection is too 
cautious for SXU close test. 
4 Conclusion and Future Work  
In this paper, a detailed description on a Chinese 
word segmentation system is presented. N-gram 
model is adopted as the language model, and some 
preprocessing and post processing methods are 
integrated as a unified framework. The evaluation 
results show the efficiency of our approaches. 
    In future research, we will continue to enhance 
our system with other new techniques, especially 
we will focus on improving the recall of OOV 
words. 
References 
Jianfeng Gao, Mu Li, Andi Wu and Chang-Ning Huang. 
2005. Chinese Word Segmentation and Named Entity 
Recognition: A Pragmatic Approach. Computational 
Linguistics, Vol.31(4): 531-574. 
 
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An 
Improved Chinese Word Segmentation System with 
Conditional Random Field. Proceedings of the fifth 
SIGHAN Workshop on Chinese Language 
Processing, 162-165. Sydney, Australia. 
 
Adwait Ratnaparkni. 1996. A Maximum Entropy Part-
of-speech Tagger. In Proceedings of the Empirical 
Method in Natural Language Processing Conference, 
133-142. University of Pennsylvania. 
 
JK Low, HT Ng, W Guo. 2005. A Maximum Entropy 
Approach to Chinese Word Segmentation. 
Proceedings of the fourth SIGHAN Workshop on 
Chinese Language Processing. Jeju Island, Korea. 
 
Manning, Christopher D. and Hinrich Schutze. 1999. 
Foundations of Statistical Natural Language 
Processing. The MIT Press, Cambridge, 
Massachusetts, London, England.  
 
Nianwen Xue. 2003. Chinese word segmentation as 
character tagging. International Journal of 
Computational Linguistics and Chinese Language 
Processing, 8(1).  
 
Rile Hu, Chengqing Zong, and Bo Xu. An Approach to 
Automatic Acquisition of Translation Templates 
Based on Phrase Structure Extraction and Alignment. 
IEEE Transaction on Speech and Audio Processing. 
Vol. 14, No.5, September 2006. 
 
89
Sixth SIGHAN Workshop on Chinese Language Processing
