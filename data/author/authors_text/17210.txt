Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 66?73, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
NTNU-CORE: Combining strong features for semantic similarity
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bjo?rn Gamba?ck, Andre? Lynum
Norwegian University of Science and Technology
Department of Computer and Information and Science
Sem S?lands vei 7-9
NO-7491 Trondheim, Norway
{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.no
Abstract
The paper outlines the work carried out at
NTNU as part of the *SEM?13 shared task
on Semantic Textual Similarity, using an ap-
proach which combines shallow textual, dis-
tributional and knowledge-based features by
a support vector regression model. Feature
sets include (1) aggregated similarity based
on named entity recognition with WordNet
and Levenshtein distance through the calcula-
tion of maximum weighted bipartite graphs;
(2) higher order word co-occurrence simi-
larity using a novel method called ?Multi-
sense Random Indexing?; (3) deeper seman-
tic relations based on the RelEx semantic
dependency relationship extraction system;
(4) graph edit-distance on dependency trees;
(5) reused features of the TakeLab and DKPro
systems from the STS?12 shared task. The
NTNU systems obtained 9th place overall (5th
best team) and 1st place on the SMT data set.
1 Introduction
Intuitively, two texts are semantically similar if they
roughly mean the same thing. The task of formally
establishing semantic textual similarity clearly is
more complex. For a start, it implies that we have
a way to formally represent the intended meaning of
all texts in all possible contexts, and furthermore a
way to measure the degree of equivalence between
two such representations. This goes far beyond the
state-of-the-art for arbitrary sentence pairs, and sev-
eral restrictions must be imposed. The Semantic
Textual Similarity (STS) task (Agirre et al, 2012,
2013) limits the comparison to isolated sentences
only (rather than complete texts), and defines sim-
ilarity of a pair of sentences as the one assigned by
human judges on a 0?5 scale (with 0 implying no
relation and 5 complete semantic equivalence). It is
unclear, however, to what extent two judges would
agree on the level of similarity between sentences;
Agirre et al (2012) report figures on the agreement
between the authors themselves of about 87?89%.
As in most language processing tasks, there are
two overall ways to measure sentence similarity, ei-
ther by data-driven (distributional) methods or by
knowledge-driven methods; in the STS?12 task the
two approaches were used nearly equally much.
Distributional models normally measure similarity
in terms of word or word co-occurrence statistics, or
through concept relations extracted from a corpus.
The basic strategy taken by NTNU in the STS?13
task was to use something of a ?feature carpet bomb-
ing approach? in the way of first automatically ex-
tracting as many potentially useful features as possi-
ble, using both knowledge and data-driven methods,
and then evaluating feature combinations on the data
sets provided by the organisers of the shared task.
To this end, four different types of features were
extracted. The first (Section 2) aggregates similar-
ity based on named entity recognition with WordNet
and Levenshtein distance by calculating maximum
weighted bipartite graphs. The second set of features
(Section 3) models higher order co-occurrence sim-
ilarity relations using Random Indexing (Kanerva
et al, 2000), both in the form of a (standard) sliding
window approach and through a novel method called
?Multi-sense Random Indexing? which aims to sep-
arate the representation of different senses of a term
66
from each other. The third feature set (Section 4)
aims to capture deeper semantic relations using ei-
ther the output of the RelEx semantic dependency
relationship extraction system (Fundel et al, 2007)
or an in-house graph edit-distance matching system.
The final set (Section 5) is a straight-forward gath-
ering of features from the systems that fared best in
STS?12: TakeLab from University of Zagreb (S?aric?
et al, 2012) and DKPro from Darmstadt?s Ubiqui-
tous Knowledge Processing Lab (Ba?r et al, 2012).
As described in Section 6, Support Vector Regres-
sion (Vapnik et al, 1997) was used for solving the
multi-dimensional regression problem of combining
all the extracted feature values. Three different sys-
tems were created based on feature performance on
the supplied development data. Section 7 discusses
scores on the STS?12 and STS?13 test data.
2 Compositional Word Matching
Compositional word matching similarity is based
on a one-to-one alignment of words from the two
sentences. The alignment is obtained by maximal
weighted bipartite matching using several word sim-
ilarity measures. In addition, we utilise named entity
recognition and matching tools. In general, the ap-
proach is similar to the one described by Karnick
et al (2012), with a different set of tools used. Our
implementation relies on the ANNIE components in
GATE (Cunningham et al, 2002) and will thus be
referred to as GateWordMatch.
The processing pipeline for GateWordMatch
is: (1) tokenization by ANNIE English Tokeniser,
(2) part-of-speech tagging by ANNIE POS Tagger,
(3) lemmatization by GATE Morphological Anal-
yser, (4) stopword removal, (5) named entity recog-
nition based on lists by ANNIE Gazetteer, (6) named
entity recognition based on the JAPE grammar by
the ANNIE NE Transducer, (7) matching of named
entities by ANNIE Ortho Matcher, (8) computing
WordNet and Levenstein similarity between words,
(9) calculation of a maximum weighted bipartite
graph matching based on similarities from 7 and 8.
Steps 1?4 are standard preprocessing routines.
In step 5, named entities are recognised based on
lists that contain locations, organisations, compa-
nies, newspapers, and person names, as well as date,
time and currency units. In step 6, JAPE grammar
rules are applied to recognise entities such as ad-
dresses, emails, dates, job titles, and person names
based on basic syntactic and morphological features.
Matching of named entities in step 7 is based on
matching rules that check the type of named entity,
and lists with aliases to identify entities as ?US?,
?United State?, and ?USA? as the same entity.
In step 8, similarity is computed for each pair
of words from the two sentences. Words that are
matched as entities in step 7 get a similarity value
of 1.0. For the rest of the entities and non-entity
words we use LCH (Leacock and Chodorow, 1998)
similarity, which is based on a shortest path between
the corresponding senses in WordNet. Since word
sense disambiguation is not used, we take the simi-
larity between the nearest senses of two words. For
cases when the WordNet-based similarity cannot be
obtained, a similarity based on the Levenshtein dis-
tance (Levenshtein, 1966) is used instead. It is nor-
malised by the length of the longest word in the pair.
For the STS?13 test data set, named entity matching
contributed to 4% of all matched word pairs; LCH
similarity to 61%, and Levenshtein distance to 35%.
In step 9, maximum weighted bipartite matching
is computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in the bipartite graph represent words
from the sentences, and edges have weights that cor-
respond to similarities between tokens obtained in
step 8. Weighted bipartite matching finds the one-to-
one alignment that maximizes the sum of similarities
between aligned tokens. Total similarity normalised
by the number of words in both sentences is used as
the final sentence similarity measure.
3 Distributional Similarity
Our distributional similarity features use Random
Indexing (RI; Kanerva et al, 2000; Sahlgren, 2005),
also employed in STS?12 by Tovar et al (2012);
Sokolov (2012); Semeraro et al (2012). It is an
efficient method for modelling higher order co-
occurrence similarities among terms, comparable to
Latent Semantic Analysis (LSA; Deerwester et al,
1990). It incrementally builds a term co-occurrence
matrix of reduced dimensionality through the use of
a sliding window and fixed size index vectors used
for training context vectors, one per unique term.
A novel variant, which we have called ?Multi-
67
sense Random Indexing? (MSRI), inspired by
Reisinger and Mooney (2010), attempts to capture
one or more ?senses? per unique term in an unsu-
pervised manner, each sense represented as an indi-
vidual vector in the model. The method is similar to
classical sliding window RI, but each term can have
multiple context vectors (referred to as ?sense vec-
tors? here) which are updated individually. When
updating a term vector, instead of directly adding the
index vectors of the neighbouring terms in the win-
dow to its context vector, the system first computes a
separate window vector consisting of the sum of the
index vectors. Then cosine similarity is calculated
between the window vector and each of the term?s
sense vectors. Each similarity score is in turn com-
pared to a set similarity threshold: if no score ex-
ceeds the threshold, the sentence vector is added as
a new separate sense vector for the term; if exactly
one score is above the threshold, the window vector
is added to that sense vector; and if multiple scores
are above the threshold, all the involved senses are
merged into one sense vector, together with the win-
dow vector. This accomplishes an incremental clus-
tering of senses in an unsupervised manner while re-
taining the efficiency of classical RI.
As data for training the models we used the
CLEF 2004?2008 English corpus (approx. 130M
words). Our implementation of RI and MSRI is
based on JavaSDM (Hassel, 2004). For classical
RI, we used stopword removal (using a customised
versions of the English stoplist from the Lucene
project), window size of 4+4, dimensionality set to
1800, 4 non-zeros, and unweighted index vector in
the sliding window. For MSRI, we used a simi-
larity threshold of 0.2, a vector dimensionality of
800, a non-zero count of 4, and window size of
5+5. The index vectors in the sliding window were
shifted to create direction vectors (Sahlgren et al,
2008), and weighted by distance to the target term.
Rare senses with a frequency below 10 were ex-
cluded. Other sliding-window schemes, including
unweighted non-shifted vectors and Random Permu-
tation (Sahlgren et al, 2008), were tested, but none
outperformed the sliding-window schemes used.
Similarity between sentence pairs was calcu-
lated as the normalised maximal bipartite similar-
ity between term pairs in each sentence, resulting
in the following features: (1) MSRI-Centroid:
each term is represented as the sum of its sense
vectors; (2) MSRI-MaxSense: for each term
pair, the sense-pair with max similarity is used;
(3) MSRI-Context: for each term, its neigh-
bouring terms within a window of 2+2 is used as
context for picking a single, max similar, sense
from the target term to be used as its represen-
tation; (4) MSRI-HASenses: similarity between
two terms is computed by applying the Hungarian
Algorithm to all their possible sense pair mappings;
(5) RI-Avg: classical RI, each term is represented
as a single context vector; (6) RI-Hungarian:
similarity between two sentences is calculated us-
ing the Hungarian Algorithm. Alternatively, sen-
tence level similarity was computed as the cosine
similarity between sentence vectors composed of
their terms? vectors. The corresponding features
are (1) RI-SentVectors-Norm: sentence vec-
tors are created by summing their constituent terms
(i.e., context vectors), which have first been normal-
ized; (2) RI-SentVectors-TFIDF: same as be-
fore, but TF*IDF weights are added.
4 Deeper Semantic Relations
Two deep strategies were employed to accompany
the shallow-processed feature sets. Two existing
systems were used to provide the basis for these fea-
tures, namely the RelEx system (Fundel et al, 2007)
from the OpenCog initiative (Hart and Goertzel,
2008), and an in-house graph-edit distance system
developed for plagiarism detection (R?kenes, 2013).
RelEx outputs syntactic trees, dependency graphs,
and semantic frames as this one for the sentence
?Indian air force to buy 126 Rafale fighter jets?:
Commerce buy:Goods(buy,jet)
Entity:Entity(jet,jet)
Entity:Name(jet,Rafale)
Entity:Name(jet,fighter)
Possibilities:Event(hyp,buy)
Request:Addressee(air,you)
Request:Message(air,air)
Transitive action:Beneficiary(buy,jet)
Three features were extracted from this: first, if
there was an exact match of the frame found in s1
with s2; second, if there was a partial match until the
first argument (Commerce buy:Goods(buy);
and third if there was a match of the frame category
68
(Commerce buy:Goods).
In STS?12, Singh et al (2012) matched Universal
Networking Language (UNL) graphs against each
other by counting matches of relations and univer-
sal words, while Bhagwani et al (2012) calculated
WordNet-based word-level similarities and created
a weighted bipartite graph (see Section 2). The
method employed here instead looked at the graph
edit distance between dependency graphs obtained
with the Maltparser dependency parser (Nivre et al,
2006). Edit distance is the defined as the minimum
of the sum of the costs of the edit operations (in-
sertion, deletion and substitution of nodes) required
to transform one graph into the other. It is approx-
imated with a fast but suboptimal algorithm based
on bipartite graph matching through the Hungarian
algorithm (Riesen and Bunke, 2009).
5 Reused Features
The TakeLab ?simple? system (S?aric? et al, 2012) ob-
tained 3rd place in overall Pearson correlation and
1st for normalized Pearson in STS?12. The source
code1 was used to generate all its features, that is,
n-gram overlap, WordNet-augmented word overlap,
vector space sentence similarity, normalized differ-
ence, shallow NE similarity, numbers overlap, and
stock index features.2 This required the full LSA
vector space models, which were kindly provided
by the TakeLab team. The word counts required for
computing Information Content were obtained from
Google Books Ngrams.3
The DKPro system (Ba?r et al, 2012) obtained first
place in STS?12 with the second run. We used the
source code4 to generate features for the STS?12
and STS?13 data. Of the string-similarity features,
we reused the Longest Common Substring, Longest
Common Subsequence (with and without normaliza-
tion), and Greedy String Tiling measures. From the
character/word n-grams features, we used Charac-
ter n-grams (n = 2, 3, 4), Word n-grams by Con-
tainment w/o Stopwords (n = 1, 2), Word n-grams
1http://takelab.fer.hr/sts/
2We did not use content n-gram overlap or skip n-grams.
3http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html, version 20120701,
with 468,491,999,592 words
4http://code.google.com/p/
dkpro-similarity-asl/
by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-
card w/o Stopwords (n = 2, 4). Semantic similarity
measures include WordNet Similarity based on the
Resnik measure (two variants) and Explicit Seman-
tic Similarity based on WordNet, Wikipedia or Wik-
tionary. This means that we reused all features from
DKPro run 1 except for Distributional Thesaurus.
6 Systems
Our systems follow previous submissions to the STS
task (e.g., S?aric? et al, 2012; Banea et al, 2012) in
that feature values are extracted for each sentence
pair and combined with a gold standard score in or-
der to train a Support Vector Regressor on the result-
ing regression task. A postprocessing step guaran-
tees that all scores are in the [0, 5] range and equal 5
if the two sentences are identical. SVR has been
shown to be a powerful technique for predictive data
analysis when the primary goal is to approximate a
function, since the learning algorithm is applicable
to continuous classes. Hence support vector regres-
sion differs from support vector machine classifica-
tion where the goal rather is to take a binary deci-
sion. The key idea in SVR is to use a cost function
for building the model which tries to ignore noise in
training data (i.e., data which is too close to the pre-
diction), so that the produced model in essence only
depends on a more robust subset of the extracted fea-
tures.
Three systems were created using the supplied
annotated data based on Microsoft Research Para-
phrase and Video description corpora (MSRpar and
MSvid), statistical machine translation system out-
put (SMTeuroparl and SMTnews), and sense map-
pings between OntoNotes and WordNet (OnWN).
The first system (NTNU1) includes all TakeLab and
DKPro features plus the GateWordMatch feature
with the SVR in its default setting.5 The training
material consisted of all annotated data available,
except for the SMT test set, where it was limited to
SMTeuroparl and SMTnews. The NTNU2 system is
similar to NTNU1, except that the training material
for OnWN and FNWN excluded MSRvid and that
the SVR parameter C was set to 200. NTNU3 is
similar to NTNU1 except that all features available
are included.
5RBF kernel,  = 0.1, C = #samples, ? = 1#features
69
Data NTNU1 NTNU2 NTNU3
MSRpar 0.7262 0.7507 0.7221
MSRvid 0.8660 0.8882 0.8662
SMTeuroparl 0.5843 0.3386 0.5503
SMTnews 0.5840 0.5592 0.5306
OnWN 0.7503 0.6365 0.7200
mean 0.7022 0.6346 0.6779
Table 1: Correlation score on 2012 test data
7 Results
System performance is evaluated using the Pearson
product-moment correlation coefficient (r) between
the system scores and the human scores. Results on
the 2012 test data (i.e., 2013 development data) are
listed in Table 1. This basically shows that except
for the GateWordMatch, adding our other fea-
tures tends to give slightly lower scores (cf. NTNU1
vs NTNU3). In addition, the table illustrates that op-
timizing the SVR according to cross-validated grid
search on 2012 training data (here C = 200), rarely
pays off when testing on unseen data (cf. NTNU1
vs NTNU2).
Table 2 shows the official results on the test data.
These are generally in agreement with the scores on
the development data, although substantially lower.
Our systems did particularly well on SMT, holding
first and second position, reasonably good on head-
lines, but not so well on the ontology alignment data,
resulting in overall 9th (NTNU1) and 12th (NTNU3)
system positions (5th best team). Table 3 lists the
correlation score and rank of the ten best individual
features per STS?13 test data set, and those among
the top-20 overall, resulting from linear regression
on a single feature. Features in boldface are gen-
uinely new (i.e., described in Sections 2?4).
Overall the character n-gram features are the most
informative, particularly for HeadLine and SMT.
The reason may be that these not only capture word
overlap (Ahn, 2011), but also inflectional forms and
spelling variants.
The (weighted) distributional similarity features
based on NYT are important for HeadLine and SMT,
which obviously contain sentence pairs from the
news genre, whereas the Wikipedia based feature is
more important for OnWN and FNWN. WordNet-
based measures are highly relevant too, with variants
NTNU1 NTNU2 NTNU3
Data r n r n r n
Head 0.7279 11 0.5909 59 0.7274 12
OnWN 0.5952 31 0.1634 86 0.5882 32
FNWN 0.3215 45 0.3650 27 0.3115 49
SMT 0.4015 2 0.3786 9 0.4035 1
mean 0.5519 9 0.3946 68 0.5498 12
Table 2: Correlation score and rank on 2013 test data
relying on path length outperforming those based on
Resnik similarity, except for SMT.
As is to be expected, basic word and lemma uni-
gram overlap prove to be informative, with overall
unweighted variants resulting in higher correlation.
Somewhat surprisingly, higher order n-gram over-
laps (n > 1) seem to be less relevant. Longest com-
mon subsequence and substring appear to work par-
ticularly well for OnWN and FNWN, respectively.
GateWordMatch is highly relevant too, in
agreement with earlier results on the development
data. Although treated as a single feature, it is ac-
tually a combination of similarity features where an
appropriate feature is selected for each word pair.
This ?vertical? way of combining features can po-
tentially provide a more fine-grained feature selec-
tion, resulting in less noise. Indeed, if two words are
matching as named entities or as close synonyms,
less precise types of features such as character-based
and data-driven similarity should not dominate the
overall similarity score.
It is interesting to find that MSRI outper-
forms both classical RI and ESA (Gabrilovich and
Markovitch, 2007) on this task. Still, the more ad-
vanced features, such as MSRI-Context, gave in-
ferior results compared to MSRI-Centroid. This
suggests that more research on MSRI is needed
to understand how both training and retrieval can
be optimised. Also, LSA-based features (see
tl.weight-dist-sim-wiki) achieve better
results than both MSRI, RI and ESA. Then again,
larger corpora were used for training the LSA mod-
els. RI has been shown to be comparable to LSA
(Karlgren and Sahlgren, 2001), and since a relatively
small corpus was used for training the RI/MSRI
models, there are reasons to believe that better
scores can be achieved by both RI- and MSRI-based
features by using more training data.
70
HeadLine OnWN FNWN SMT Mean
Features r n r n r n r n r n
CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1
CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2
CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3
tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4
tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5
GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6
tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7
tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8
tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9
tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10
MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11
TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12
tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13
MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14
tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15
MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16
GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17
ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18
WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19
WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20
RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24
RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25
LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26
ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29
LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30
MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33
MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34
RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39
RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40
RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41
GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42
Table 3: Correlation score and rank of the best features
8 Conclusion and Future Work
The NTNU system can be regarded as continuation
of the most successful systems from the STS?12
shared task, combining shallow textual, distribu-
tional and knowledge-based features into a support
vector regression model. It reuses features from the
TakeLab and DKPro systems, resulting in a very
strong baseline.
Adding new features to further improve
performance turned out to be hard: only
GateWordMatch yielded improved perfor-
mance. Similarity features based on both classical
and innovative variants of Random Indexing were
shown to correlate with semantic textual similarity,
but did not complement the existing distributional
features. Likewise, features designed to reveal
deeper syntactic (graph edit distance) and semantic
relations (RelEx) did not add to the score.
As future work, we would aim to explore a
vertical feature composition approach similar to
GateWordMatch and contrast it with the ?flat?
composition currently used in our systems.
Acknowledgements
Thanks to TakeLab for source code of their ?simple?
system and the full-scale LSA models. Thanks to the
team from Ubiquitous Knowledge Processing Lab
for source code of their DKPro Similarity system.
71
References
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). SemEval-2012 Task 6: A pilot on se-
mantic textual similarity. In *SEM (2012), pages
385?393.
Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,
and Guo, W. (2013). *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational
Semantics. Association for Computational Lin-
guistics.
Ahn, C. S. (2011). Automatically detecting authors?
native language. PhD thesis, Monterey, Califor-
nia. Naval Postgraduate School.
Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.
(2012). UNT: a supervised synergistic approach
to semantic text similarity. In *SEM (2012),
pages 635?642.
Ba?r, D., Biemann, C., Gurevych, I., and Zesch, T.
(2012). UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity
measures. In *SEM (2012), pages 435?440.
Bhagwani, S., Satapathy, S., and Karnick, H. (2012).
sranjans : Semantic textual similarity using maxi-
mal weighted bipartite graph matching. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics ? Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (Sem-
Eval 2012), pages 579?585, Montre?al, Canada.
Association for Computational Linguistics.
Cunningham, H., Maynard, D., Bontcheva, K., and
Tablan, V. (2002). GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Philadel-
phia, Pennsylvania. ACL.
Deerwester, S., Dumais, S., Furnas, G., Landauer,
T., and Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Soci-
ety for Information Science, 41(6):391?407.
Fundel, K., Ku?ffner, R., and Zimmer, R. (2007).
RelEx - Relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of The
Twentieth International Joint Conference for Ar-
tificial Intelligence., pages 1606?1611.
Hart, D. and Goertzel, B. (2008). Opencog: A soft-
ware framework for integrative artificial general
intelligence. In Proceedings of the 2008 confer-
ence on Artificial General Intelligence 2008: Pro-
ceedings of the First AGI Conference, pages 468?
472, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
Hassel, M. (2004). JavaSDM package.
Kanerva, P., Kristoferson, J., and Holst, A. (2000).
Random indexing of text samples for latent se-
mantic analysis. In Gleitman, L. and Josh, A.,
editors, Proceedings of the 22nd Annual Confer-
ence of the Cognitive Science Society, page 1036.
Erlbaum.
Karlgren, J. and Sahlgren, M. (2001). From Words
to Understanding. In Uesaka, Y., Kanerva, P., and
Asoh, H., editors, Foundations of real-world in-
telligence, chapter 26, pages 294?311. Stanford:
CSLI Publications.
Karnick, H., Satapathy, S., and Bhagwani, S. (2012).
sranjans: Semantic textual similarity using max-
imal bipartite graph matching. In *SEM (2012),
pages 579?585.
Kuhn, H. (1955). The Hungarian method for the as-
signment problem. Naval research logistics quar-
terly, 2:83?97.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal . . . .
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707?710.
Nivre, J., Hall, J., and Nilsson, J. (2006). Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In In Proc. of LREC-2006, pages
2216?2219.
72
Reisinger, J. and Mooney, R. (2010). Multi-
prototype vector-space models of word meaning.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
number June, pages 109?117.
Riesen, K. and Bunke, H. (2009). Approximate
graph edit distance computation by means of bi-
partite graph matching. Image and Vision Com-
puting, 27(7):950?959.
R?kenes, H. (2013). Graph-Edit Distance Applied to
the Task of Detecting Plagiarism. Master?s the-
sis, Norwegian University of Science and Tech-
nology.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE, volume 5.
Sahlgren, M., Holst, A., and Kanerva, P. (2008). Per-
mutations as a Means to Encode Order in Word
Space. Proceedings of the 30th Conference of the
Cognitive Science Society.
S?aric?, F., Glavas?, G., Karan, M., S?najder, J., and
Bas?ic?, B. D. (2012). TakeLab: systems for mea-
suring semantic text similarity. In *SEM (2012),
pages 441?448.
*SEM (2012). Proceedings of the First Joint Con-
ference on Lexical and Computational Seman-
tics (*SEM), volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
Montreal, Canada. Association for Computational
Linguistics.
Semeraro, G., Aldo, B., and Orabona, V. E. (2012).
UNIBA: Distributional semantics for textual sim-
ilarity. In *SEM (2012), pages 591?596.
Singh, J., Bhattacharya, A., and Bhattacharyya, P.
(2012). janardhan: Semantic textual similarity us-
ing universal networking language graph match-
ing. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics ? Vol-
ume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalu-
ation (SemEval 2012), pages 662?666, Montre?al,
Canada. Association for Computational Linguis-
tics.
Sokolov, A. (2012). LIMSI: learning semantic simi-
larity by selecting random word subsets. In *SEM
(2012), pages 543?546.
Tovar, M., Reyes, J., and Montes, A. (2012). BUAP:
a first approximation to relational similarity mea-
suring. In *SEM (2012), pages 502?505.
Vapnik, V., Golowich, S. E., and Smola, A. (1997).
Support vector method for function approxima-
tion, regression estimation, and signal process-
ing. In Mozer, M. C., Jordan, M. I., and Petsche,
T., editors, Advances in Neural Information Pro-
cessing Systems, volume 9, pages 281?287. MIT
Press, Cambridge, Massachusetts.
73
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 430?437, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
NTNU: Domain Semi-Independent Short Message Sentiment Classification
?yvind Selmer Mikael Brevik Bjo?rn Gamba?ck Lars Bungum
Department of Computer and Information Science
Norwegian University of Science and Technology (NTNU)
Sem S?lands vei 7?9, NO?7491 Trondheim, Norway
{oyvinsel,mikaelbr}@stud.ntnu.no {gamback,larsbun}@idi.ntnu.no
Abstract
The paper describes experiments using grid
searches over various combinations of ma-
chine learning algorithms, features and pre-
processing strategies in order to produce the
optimal systems for sentiment classification of
microblog messages. The approach is fairly
domain independent, as demonstrated by the
systems achieving quite competitive results
when applied to short text message data, i.e.,
input they were not originally trained on.
1 Introduction
The informal texts in microblogs such as Twitter
and on other social media represent challenges for
traditional language processing systems. The posts
(?tweets?) are limited to 140 characters and often
contain misspellings, slang and abbreviations. On
the other hand, the posts are often opinionated in
nature as a very result of their informal character,
which has led Twitter to being a gold mine for sen-
timent analysis (SA). SA for longer texts, such as
movie reviews, has been explored since the 1990s;1
however, the limited amount of attributes in tweets
makes the feature vectors shorter than in documents
and the task of analysing them closely related to
phrase- and sentence-level SA (Wilson et al, 2005;
Yu and Hatzivassiloglou, 2003). Hence there are
no guarantees that algorithms that perform well on
document-level SA will do as well on tweets. On
the other hand, it is possible to exploit some of the
special features of the web language, e.g., emoticons
1See Pang and Lee (2008); Feldman (2013) for overviews.
and emotionally loaded abbreviations. Thus the data
will normally go through some preprocessing before
any classification is attempted, e.g., by filtering out
Twitter specific symbols and functions, in particular
retweets (reposting another user?s tweet), mentions
(?@?, tags used to mention another user), hashtags
(?#?, used to tag a tweet to a certain topic), emoti-
cons, and URLs (linking to an external resource,
e.g., a news article or a photo). The first system to re-
ally use Twitter as a corpus was created as a student
course project at Stanford (Go et al, 2009). Pak and
Paroubek (2010) experimented with sentiment clas-
sification of tweets using Support Vector Machines
and Conditional Random Fields, benchmarked with
a Na??ve Bayes Classifier baseline, but were unable
to beat the baseline. Later, and as Twitter has grown
in popularity, many other systems for Twitter Senti-
ment Analysis (TSA) have been developed (see, e.g.,
Maynard and Funk, 2011; Mukherjee et al, 2012;
Saif et al, 2012; Chamlertwat et al, 2012).
Clearly, it is possible to classify the sentiment of
tweets in a single step; however, the approach to
TSA most used so far is a two-step strategy where
the first step is subjectivity classification and the
second step is polarity classification. The goal of
subjectivity classification is to separate subjective
and objective statements. Pak and Paroubek (2010)
counted word frequencies in a subjective vs an ob-
jective set of tweets; the results showed that in-
terjections and personal pronouns are the strongest
indicators of subjectivity. In general, these word
classes, adverbs and (in particular) adjectives (Hatzi-
vassiloglou and Wiebe, 2000) have shown to be
good subjectivity indicators, which has made part-
430
of-speech (POS) tagging a reasonable technique for
filtering out objective tweets. Early research on
TSA showed that the challenging vocabulary made
it harder to accurately tag tweets; however, Gimpel
et al (2011) report on using a POS tagger for mark-
ing tweets, performing with almost 90% accuracy.
Polarity classification is the task of separating the
subjective statements into positives and negatives.
Kouloumpis et al (2011) tried different solutions for
tweet polarity classification, and found that the best
performance came from using n-grams together with
lexicon and microblog features. Interestingly, per-
formance dropped when a POS tagger was included.
They speculate that this can be due to the accuracy
of the POS tagger itself, or that POS tagging just is
less effective for analysing tweet polarity.
In this paper we will explore the application of
a set of machine learning algorithms to the task of
Twitter sentiment classification, comparing one-step
and two-step approaches, and investigate a range of
different preprocessing methods. What we explic-
itly will not do, is to utilise a sentiment lexicon, even
though many methods in TSA rely on lexica with a
sentiment score for each word. Nielsen (2011) man-
ually built a sentiment lexicon specialized for Twit-
ter, while others have tried to induce such lexica
automatically with good results (Velikovich et al,
2010; Mohammad et al, 2013). However, sentiment
lexica ? and in particular specialized Twitter senti-
ment lexica ? make the classification more domain
dependent. Here we will instead aim to exploit do-
main independent approaches as far as possible, and
thus abstain from using sentiment lexica. The rest of
the paper is laid out as follows: Section 2 introduces
the twitter data sets used in the study. Then Section 3
describes the system built for carrying out the twitter
sentiment classification experiments, which in turn
are reported and discussed in Sections 4 and 5.
2 Data
Manually collecting information from Twitter would
be a tedious task, but Twitter offers a well doc-
umented Representational State Transfer Applica-
tion Programming Interface (REST API) which al-
lows users to collect a corpus from the micro-
blogosphere. Most of the data used in TSA re-
search is collected through the Twitter API, either by
Training Dev 1 Dev 2 NTNU
Class Num % Num % Num % Num %
Negative 1288 15 176 21 340 26 86 19
Neutral 4151 48 144 45 739 21 232 50
Positive 3270 37 368 35 575 54 142 31
Total 8709 688 1654 461
Table 1: The data sets used in the experiments
searching for a certain topic/keyword or by stream-
ing realtime data. Four different data sets were used
in the experiments described below. three were sup-
plied by the organisers of the SemEval?13 shared
task on Twitter sentiment analysis (Wilson et al,
2013), in the form of a training set, a smaller initial
development set, and a larger development set. All
sets consist of manually annotated tweets on a range
of topics, including different products and events.
Tweet-level classification (Task 2B) was split into
two subtasks in SemEval?13, one allowing training
only on the data sets supplied by the organisers (con-
strained) and one allowing training also on external
data (unconstrained). To this end, a web applica-
tion2 for manual annotation of tweets was built and
used to annotate a small fourth data set (?NTNU?).
Each of the 461 tweets in the ?NTNU? data set was
annotated by one person only.
The distribution of target classes in the data sets is
shown in Table 1. The data was neither preprocessed
nor filtered, and thus contain hashtags, URLs, emoti-
cons, etc. However, all the data sets provided by
SemEval?13 had more than three target classes (e.g.,
?objective?, ?objective-OR-neutral?), so tweets that
were not annotated as ?positive? or ?negative? were
merged into the ?neutral? target class.
Due to Twitter?s privacy policy, the given data sets
do not contain the tweet text, but only the tweet ID
which in turn can be used to download the text. The
Twitter API has a limit on the number of downloads
per hour, so SemEval?13 provided a Python script
to scrape texts from https://twitter.com. This
script was slow and did not download the texts for all
tweet IDs in the data sets, so a faster and more pre-
cise download script3 for node.js was implemented
and submitted to the shared task organisers.
2http://tinyurl.com/tweetannotator
3http://tinyurl.com/twitscraper
431
3 Experimental Setup
In order to run sentiment classification experiments,
a general system was built. It has a Sentiment Anal-
ysis API Layer which works as a thin extension of
the Twitter API, sending all tweets received in par-
allel to a Sentiment Analysis Classifier server. After
classification, the SA API returns the same JSON
structure as the Twitter API sends out, only with an
additional attribute denoting the tweet?s sentiment.
The Sentiment Analysis Classifier system consists
of preprocessing and classification, described below.
3.1 Preprocessing
As mentioned in the introduction, most approaches
to Twitter sentiment analysis start with a pre-
processing step, filtering out some Twitter specific
symbols and functions. Go et al (2009) used ?:)?
and ?:(? as labels for the polarity, so did not remove
these emoticons, but replaced URLs and user names
with placeholders. Kouloumpis et al (2011) used
both an emoticon set and a hashtagged set. The lat-
ter is a subset of the Edinburgh Twitter corpus which
consists of 97 million tweets (Petrovic? et al, 2010).
Some approaches have also experimented with nor-
malizing the tweets, and removing redundant letters,
e.g., ?loooove? and ?crazyyy?, that are used to ex-
press a stronger sentiment in tweets. Redundant let-
ters are therefore often not deleted, but words rather
trimmed down to one additional redundant letter, so
that the stronger sentiment can be taken into consid-
eration by a score/weight adjustment for that feature.
To find the best features to use, a set of eight dif-
ferent combinations of preprocessing methods was
designed, as detailed in Table 2. These include no
preprocessing (P0, not shown in the table), where
all characters are included as features; full remove
(P4), where all special Twitter features like user
names, URLs, hashtags, retweet (RT ) tags, and
emoticons are stripped; and replacing Twitter fea-
tures with placeholder texts to reduce vocabulary.
The ?hashtag as word? method transforms a hashtag
to a regular word and uses the hashtag as a feature.
?Reduce letter duplicate? removes redundant char-
acters more than three (?happyyyyyyyy!!!!!!? ?
?happyyy!!!?). Some methods, like P1, P2, P4, P5
and P7 remove user names from the text, as they
most likely are just noise for the sentiment. Still,
Method P1 P2 P3 P4 P5 P6 P7
Remove Usernames X X X X X
Username placeholder X
Remove URLs X X X X
URL placeholder X
Remove hashtags X X
Hashtag as word X
Hashtag placeholder X
Remove RT -tags X X X
Remove emoticons X X
Reduce letter duplicate X X X X
Negation attachment X X X
Table 2: Overview of the preprocessing methods
the fact that there are references to URLs and user
names might be relevant for the sentiment. To make
these features more informative for the machine
learning algorithms, a preprocessing method (P3)
was implemented for replacing them with place-
holders. In addition, a very rudimentary treatment
of negation was added, in which the negation is at-
tached to the preceding and following words, so that
they will also reflect the change in sentence polarity.
Even though this preprocessing obviously is
Twitter-specific, the results after it will still be do-
main semi-independent, in as far as the strings pro-
duced after the removal of URLs, user names, etc.,
will be general, and can be used for system training.
3.2 Classification
The classification step currently supports three
machine learning algorithms from the Python
scikit-learn4 package: Na??ve Bayes (NB),
Maximum Entropy (MaxEnt), and Support Vector
Machines (SVM). These are all among the super-
vised learners that previously have been shown to
perform well on TSA, e.g., by Bermingham and
Smeaton (2010) who compared SVM and NB for
microblogs. Interestingly, while the SVM technique
normally beats NB and MaxEnt on longer texts, that
comparison indicated that it has some trouble with
outperforming NB when feature vectors are shorter.
Three different models were implemented:
1. One-step model: a single algorithm classifies
tweets as negative, neutral or positive.
2. Two-step model: the tweets are first classified
as either subjective or neutral. Those that are
4http://scikit-learn.org
432
Figure 1: Performance across all models (red=precision, blue=recall, green=F1-score, brown=accuracy)
classified as subjective are then sent to polarity
classification (i.e., negative or positive).
3. Boosting (Freund and Schapire, 1997): a way
to combine classifiers by generating a set of
sub-models, each of which predicts a sentiment
on its own and then sends it to a voting process
that selects the sentiment with highest score.
In all cases, the final classification is returned to the
API Layer sentiment provider.
4 Experimental Results
Experiments were carried out using the platform in-
troduced in the previous section, with models built
on the training set of Table 1. The testing system
generates and trains different models based on a set
of parameters, such as classification algorithm, pre-
processing methods, whether or not to use inverse
document frequency (IDF) or stop words. A grid
search option can be activated, so that a model is
generated with the best possible parameter set for
the given algorithm, using 10-fold cross validation.
4.1 Selection of Learners and Features
An extensive grid search was conducted. This search
cycled through different algorithms, parameters and
preprocessing techniques. The following param-
eters were included in the search. Three binary
(Yes/No) parameters: Use IDF, Use Smooth
IDF, and Use Sublinear IDF, together with
ngram (unigram/bigram/trigram). SVM
and MaxEnt models in addition included C and
NB models alpha parameters, all with the value
ranges [0.1/0.3/0.5/0.7/0.8/1.0]. SVM
and MaxEnt models also had penalty (L1/L2).
Figure 1 displays the precision, recall, F1-score,
and accuracy for each of the thirteen classifiers with
the Dev 2 data set (see Table 1) used for evaluation.
Note that most classifiers involving the NB algo-
rithm perform badly, both in terms of accuracy and
F-score. This was observed for the other data sets as
well. Further, we can see that one-step classifiers did
better than two-step models, with MaxEnt obtaining
the best accuracy, but SVM a slightly better F-score.
433
Data set Dev 2 Dev 1
Learner SVM MaxEnt SVM MaxEnt
Precision 0.627 0.647 0.700 0.561
Recall 0.592 0.578 0.726 0.589
F1-score 0.598 0.583 0.707 0.556
Accuracy 0.638 0.645 0.728 0.581
Table 3: Best classifier performance (bold=best score;
all classifiers were trained on the training set of Table 1)
A second grid search with the two best classifiers
from the first search was performed instead using the
smaller Dev 1 data set for evaluation. The results
for both the SVM and MaxEnt classifiers are shown
in Table 3. With the Dev 1 data set, SVM performs
much better than MaxEnt. The larger Dev 2 develop-
ment set contains more neutral tweets than the Dev 1
set, which gives us reasons to believe that evaluating
on the Dev 2 set favours the MaxEnt classifier.
A detailed error analysis was conducted by in-
specting the confusion matrices of all classifiers. In
general, classifiers involving SVM tend to give bet-
ter confusion matrices than the others. Using SVM
only in a one-step model works well for positive and
neutral tweets, but a bit poorer for negative. Two-
step models with SVM-based subjectivity classifica-
tion exhibit the same basic behaviour. The one-step
MaxEnt model classifies more tweets as neutral than
the other classifiers. Using MaxEnt for subjectivity
classification and either MaxEnt or SVM for polarity
classification performs well, but is too heavy on the
positive class. Boosting does not improve and be-
haves in a fashion similar to two-step MaxEnt mod-
els. All combinations involving NB tend to heavily
favour positive predictions; only the two-step mod-
els involving another algorithm for polarity classifi-
cation gave some improvement for negative tweets.
The confusion matrices of the two best learners
are shown in Figures 2a-2d, where a learner is shown
to perform better if it has redish colours on the main
diagonal and blueish in the other fields, as is the case
for SVM on the Dev 1 data set (Figure 2c).
As a part of the grid search, all preprocessing
methods were tested for each classifier. Figure 3
shows that P2 (removing user names, URLs, hash-
tags prefixes, retweet tokens, and redundant letters)
is the preprocessing method which performs best
(a) SVM Dev 2 (b) MaxEnt Dev 2
(c) SVM Dev 1 (d) MaxEnt Dev 1
Figure 2: SVM and MaxEnt confusion matrices (out-
put is shown from left-to-right: negative-neutral-positive;
the correct classes are in the same order, top-to-bottom.
?Hotter? colours (red) indicate that more instances were
assigned; ?colder? colours (blue) mean fewer instances.)
P2 P7 P6 P1 P3
0
2
4
6
8
10
10
4
3 3
1
Figure 3: Statistics of preprocessing usage
(gives the best accuracy) and thus used most of-
ten (10 times). Figure 3 also indicates that URLs
are noisy and do not contain much sentiment, while
hashtags and emoticons tend to be more valuable
features (P2 and P7 ? removing URLs ? perform
best, while P4 and P5 ? removing hashtags and
emoticons in addition to URLs ? perform badly).
434
Data set Twitter SMS
System NTNUC NTNUU NTNUC NTNUU
Precision 0.652 0.633 0.659 0.623
Recall 0.579 0.564 0.646 0.623
F1-score 0.590 0.572 0.652 0.623
F1 + /? 0.532 0.507 0.580 0.546
Table 4: The NTNU systems in SemEval?13
4.2 SemEval?13 NTNU Systems and Results
Based on the information from the grid search, two
systems were built for SemEval?13. Since one-step
SVM-based classification showed the best perfor-
mance on the training data, it was chosen for the
system participating in the constrained subtask, NT-
NUC. The preprocessing also was the one with the
best performance on the provided data, P2 which
involves lower-casing all letters; reducing letter du-
plicates; using hashtags as words (removing #); and
removing all URLs, user names and RT -tags.
Given the small size of the in-house (?NTNU?)
data set, no major improvement was expected from
adding it in the unconstrained task. Instead, a rad-
ically different set-up was chosen to create a new
system, and train it on both the in-house and pro-
vided data. NTNUU utilizes a two-step approach,
with SVM for subjectivity and MaxEnt for polarity
classification, a combination intended to capture the
strengths of both algorithms. No preprocessing was
used for the subjectivity step, but user names were
removed before attempting polarity classification.
As further described by Wilson et al (2013), the
SemEval?13 shared task involved testing on a set of
3813 tweets (1572 positive, 601 negative, and 1640
neutral). In order to evaluate classification perfor-
mance on data of roughly the same length and type,
but from a different domain, the evaluation data also
included 2094 Short Message Service texts (SMS;
492 positive, 394 negative, and 1208 neutral).
Table 4 shows the results obtained by the NTNU
systems on the SemEval?13 evaluation data, in terms
of average precision, recall and F-score for all three
classes, as well as average F-score for positive and
negative tweets only (F1+/?; i.e., the measure used
to rank the systems participating in the shared task).
5 Discussion and Conclusion
As can be seen in Table 4, the extra data available
to train the NTNUU system did not really help it:
it gets outperformed by NTNUC on all measures.
Notably, both systems perform well on the out-
of-domain data represented by the SMS messages,
which is encouraging and indicates that the approach
taken really is domain semi-independent. This was
also reflected in the rankings of the two systems in
the shared task: both were on the lower half among
the participating systems on Twitter data (24th/36
resp. 10th/15), but near the top on SMS data, with
NTNUC being ranked 5th of 28 constrained systems
and NTNUU 6th of 15 unconstrained systems.
Comparing the results to those shown in Table 3
and Figure 1, NTNUC?s (SVM) performance is in
line with that on Dev 2, but substantially worse
than on Dev 1; NTNUU (SVM?MaxEnt) performs
slightly worse too. Looking at system output with
and without the ?NTNU? data, both one-step SVM
and MaxEnt models and SVM?MaxEnt classified
more tweets as negative when trained on the ex-
tra data; however, while NTNUC benefited slightly
from this, NTNUU even performed better without it.
An obvious extension to the present work would
be to try other classification algorithms (e.g., Condi-
tional Random Fields or more elaborate ensembles)
or other features (e.g., character n-grams). Rather
than the very simple treatment of negation used
here, an approach to automatic induction of scope
through a negation detector (Councill et al, 2010)
could be used. It would also be possible to relax
the domain-independence further, in particular to
utilize sentiment lexica (including twitter specific),
e.g., by automatic phrase-polarity lexicon extraction
(Velikovich et al, 2010). Since many users tweet
from their smartphones, and a large number of them
use iPhones, several tweets contain iPhone-specific
smilies (?Emoji?). Emoji are implemented as their
own character set (rather than consisting of charac-
ters such as ?:)? and ?:(?, etc.), so a potentially major
improvement could be to convert them to character-
based smilies or to emotion-specific placeholders.
Acknowledgements
Thanks to Amitava Das for initial discussions and to
the human annotators of the ?NTNU? data set.
435
References
Bermingham, A. and Smeaton, A. F. (2010). Clas-
sifying sentiment in microblogs: Is brevity an
advantage? In Proceedings of the 19th Inter-
national Conference on Information and Knowl-
edge Management, pages 1833?1836, Toronto,
Canada. ACM.
Chamlertwat, W., Bhattarakosol, P., Rungkasiri, T.,
and Haruechaiyasak, C. (2012). Discovering con-
sumer insight from Twitter via sentiment anal-
ysis. Journal of Universal Computer Science,
18(8):973?992.
Councill, I. G., McDonald, R., and Velikovich, L.
(2010). What?s great and what?s not: learning
to classify the scope of negation for improved
sentiment analysis. In Proceedings of the 48th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 51?59, Uppsala, Swe-
den. ACL. Workshop on Negation and Specula-
tion in Natural Language Processing.
Feldman, R. (2013). Techniques and applications for
sentiment analysis. Communications of the ACM,
56(4):82?89.
Freund, Y. and Schapire, R. E. (1997). A decision-
theoretic generalization of on-line learning and
application to boosting. Journal of Computer and
System Sciences, 55(1):119?139.
Gimpel, K., Schneider, N., O?Connor, B., Das, D.,
Mills, D., Eisenstein, J., Heilman, M., Yogatama,
D., Flanigan, J., and Smith, N. A. (2011). Part-
of-speech tagging for Twitter: Annotation, fea-
tures, and experiments. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technolo-
gies, volume 2: short papers, pages 42?47, Port-
land, Oregon. ACL.
Go, A., Huang, L., and Bhayani, R. (2009). Twit-
ter sentiment analysis. Technical Report CS224N
Project Report, Department of Computer Science,
Stanford University, Stanford, California.
Hatzivassiloglou, V. and Wiebe, J. M. (2000). Ef-
fects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of the 18th
International Conference on Computational Lin-
guistics, pages 299?305, Saarbru?cken, Germany.
ACL.
HLT10 (2010). Proceedings of the 2010 Human
Language Technology Conference of the North
American Chapter of the Association for Com-
putational Linguistics, Los Angeles, California.
ACL.
Kouloumpis, E., Wilson, T., and Moore, J. (2011).
Twitter sentiment analysis: The good the bad and
the OMG! In Proceedings of the 5th International
Conference on Weblogs and Social Media, pages
538?541, Barcelona, Spain. AAAI.
Maynard, D. and Funk, A. (2011). Automatic detec-
tion of political opinions in tweets. In #MSM2011
(2011), pages 81?92.
Mohammad, S., Kiritchenko, S., and Zhu, X.
(2013). NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In SemEval?13
(2013).
#MSM2011 (2011). Proceedings of the 1st
Workshop on Making Sense of Microposts
(#MSM2011), Heraklion, Greece.
Mukherjee, S., Malu, A., Balamurali, A., and Bhat-
tacharyya, P. (2012). TwiSent: A multistage sys-
tem for analyzing sentiment in Twitter. In Pro-
ceedings of the 21st International Conference on
Information and Knowledge Management, pages
2531?2534, Maui, Hawaii. ACM.
Nielsen, F. A?. (2011). A new ANEW: Evaluation of
a word list for sentiment analysis in microblogs.
In #MSM2011 (2011), pages 93?98.
Pak, A. and Paroubek, P. (2010). Twitter as a cor-
pus for sentiment analysis and opinion mining. In
Proceedings of the 7th International Conference
on Language Resources and Evaluation, Valetta,
Malta. ELRA.
Pang, B. and Lee, L. (2008). Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Petrovic?, S., Osborne, M., and Lavrenko, V. (2010).
The Edinburgh Twitter corpus. In HLT10 (2010),
pages 25?26. Workshop on Computational Lin-
guistics in a World of Social Media.
Saif, H., He, Y., and Alani, H. (2012). Semantic
sentiment analysis of Twitter. In Proceedings of
the 11th International Semantic Web Conference,
pages 508?524, Boston, Massachusetts. Springer.
436
SemEval?13 (2013). Proceedings of the Interna-
tional Workshop on Semantic Evaluation, Sem-
Eval ?13, Atlanta, Georgia. ACL.
Velikovich, L., Blair-Goldensohn, S., Hannan, K.,
and McDonald, R. (2010). The viability of web-
derived polarity lexicons. In HLT10 (2010), pages
777?785.
Wilson, T., Kozareva, Z., Nakov, P., Ritter, A.,
Rosenthal, S., and Stoyanov, V. (2013). SemEval-
2013 Task 2: Sentiment analysis in Twitter. In
SemEval?13 (2013).
Wilson, T., Wiebe, J., and Hoffmann, P. (2005). Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In Proceedings of the 2005 Hu-
man Language Technology Conference and Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 347?354, Vancouver,
British Columbia, Canada. ACL.
Yu, H. and Hatzivassiloglou, V. (2003). Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion
sentences. In Collins, M. and Steedman, M., edi-
tors, Proceedings of the 2003 Conference on Em-
pirical Methods in Natural Language Processing,
pages 129?136, Sapporo, Japan. ACL.
437
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 21?30,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Improving Word Translation Disambiguation by
Capturing Multiword Expressions with Dictionaries
Lars Bungum, Bjo?rn Gamba?ck, Andre? Lynum, Erwin Marsi
Norwegian University of Science and Technology
Sem S?lands vei 7?9; NO?7491 Trondheim, Norway
{bungum,gamback,andrely,emarsi}@idi.ntnu.no
Abstract
The paper describes a method for identifying
and translating multiword expressions using a
bi-directional dictionary. While a dictionary-
based approach suffers from limited recall,
precision is high; hence it is best employed
alongside an approach with complementing
properties, such as an n-gram language model.
We evaluate the method on data from the
English-German translation part of the cross-
lingual word sense disambiguation task in the
2010 semantic evaluation exercise (SemEval).
The output of a baseline disambiguation sys-
tem based on n-grams was substantially im-
proved by matching the target words and their
immediate contexts against compound and
collocational words in a dictionary.
1 Introduction
Multiword expressions (MWEs) cause particular
lexical choice problems in machine translation
(MT), but can also be seen as an opportunity to both
generalize outside the bilingual corpora often used
as training data in statistical machine translation ap-
proaches and as a method to adapt to specific do-
mains. The identification of MWEs is in general
important for many language processing tasks (Sag
et al, 2002), but can be crucial in MT: since the se-
mantics of many MWEs are non-compositional, a
suitable translation cannot be constructed by trans-
lating the words in isolation. Identifying MWEs
can help to identify idiomatic or otherwise fixed lan-
guage usage, leading to more fluent translations, and
potentially reduce the amount of lexical choice an
MT system faces during target language generation.
In any translation effort, automatic or otherwise,
the selection of target language lexical items to in-
clude in the translation is a crucial part of the fi-
nal translation quality. In rule-based systems lex-
ical choice is derived from the semantics of the
source words, a process which often involves com-
plex semantic composition. Data-driven systems
on the other hand commonly base their translations
nearly exclusively on cooccurrences of bare words
or phrases in bilingual corpora, leaving the respon-
sibility of selecting lexical items in the translation
entirely to the local context found in phrase trans-
lation tables and language models with no explicit
notion of the source or target language semantics.
Still, systems of this type have been shown to pro-
duce reasonable translation quality without explic-
itly considering word translation disambiguation.
Bilingual corpora are scarce, however, and un-
available for most language pairs and target do-
mains. An alternative approach is to build systems
based on large monolingual knowledge sources and
bilingual lexica, as in the hybrid MT system PRE-
SEMT (Sofianopoulos et al, 2012). Since such
a system explicitly uses a translation dictionary, it
must at some point in the translation process decide
which lexical entries to use; thus a separate word
translation disambiguation module needs to be in-
corporated. To research available methods in such a
module we have identified a task where we can use
public datasets for measuring how well a method is
able to select the optimal of many translation choices
from a source language sentence.
21
In phrase-based statistical MT systems, the trans-
lation of multiword expressions can be a notable
source of errors, despite the fact that those systems
explicitly recognize and use alignments of sequen-
tial chunks of words. Several researchers have ap-
proached this problem by adding MWE translation
tables to the systems, either through expanding the
phrase tables (Ren et al, 2009) or by injecting the
MWE translations into the decoder (Bai et al, 2009).
Furthermore, there has been some interest in auto-
matic mining of MWE pairs from bilingual corpora
as a task in itself: Caseli et al (2010) used a dic-
tionary for evaluation of an automatic MWE extrac-
tion procedure using bilingual corpora. They also
argued for the filtering of stopwords, similarly to the
procedure described in the present paper. Sharoff
et al (2006) showed how MWE pairs can be ex-
tracted from comparable monolingual corpora in-
stead of from a parallel bilingual corpus.
The methodology introduced in this paper em-
ploys bilingual dictionaries as a source of multi-
word expressions. Relationships are induced be-
tween the source sentence and candidate transla-
tion lexical items based on their correspondence in
the dictionary. Specifically, we use a determinis-
tic multiword expression disambiguation procedure
based on translation dictionaries in both directions
(from source to target language and vice versa),
and a baseline system that ranks target lexical items
based on their immediate context and an n-gram
language model. The n-gram model represents a
high-coverage, low-precision companion to the dic-
tionary approach (i.e., it has complementary proper-
ties). Results show that the MWE dictionary infor-
mation substantially improves the baseline system.
The 2010 Semantic Evaluation exercise (Sem-
Eval?10) featured a shared task on Cross-Lingual
Word Sense Disambiguation (CL-WSD), where the
focus was on disambiguating the translation of a sin-
gle noun in a sentence. The participating systems
were given an English word in its context and asked
to produce appropriate substitutes in another lan-
guage (Lefever and Hoste, 2010b). The CL-WSD
data covers Dutch, French, Spanish, Italian and Ger-
man; however, since the purpose of the experiments
in this paper just was to assess our method?s abil-
ity to choose the right translation of a word given its
context, we used the English-to-German part only.
The next section details the employed disam-
biguation methodology and describes the data sets
used in the experiments. Section 3 then reports on
the results of experiments applying the methodology
to the SemEval datasets, particularly addressing the
impact of the dictionary MWE correspondences. Fi-
nally, Section 4 sums up the discussion and points to
issues that can be investigated further.
2 Methodology
The core of the disambiguation model introduced
in this paper is dictionary-based multiword extrac-
tion. Multiword extraction is done in both a direct
and indirect manner: Direct extraction uses adjacent
words in the source language in combination with
the word to be translated, if the combination has an
entry in the source-to-target language (SL?TL) dic-
tionary. Indirect extraction works in the reverse di-
rection, by searching the target-to-source (TL?SL)
dictionary and looking up translation candidates for
the combined words. Using a dictionary to identify
multiword expressions after translation has a low re-
call of target language MWEs, since often there ei-
ther are no multiword expressions to be discovered,
or the dictionary method is unable to find a trans-
lation for an MWE. Nevertheless, when an MWE
really is identified by means of the dictionary-based
method, the precision is high.
Due to the low recall, relying on multiword ex-
pressions from dictionaries would, however, not be
sufficient. Hence this method is combined with an
n-gram language model (LM) based on a large tar-
get language corpus. The LM is used to rank trans-
lation candidates according to the probability of the
n-gram best matching the context around the transla-
tion candidate. This is a more robust but less precise
approach, which servers as the foundation for the
high-precision but low-recall dictionary approach.
In the actual implementation, the n-gram method
thus first provides a list of its best suggestions
(currently top-5), and the dictionary method then
prepends its candidates to the top of this list. Con-
sequently, n-gram matching is described before
dictionary-based multiword extraction in the follow-
ing section. First, however, we introduce the data
sets used in the experiments.
22
(a) AGREEMENT in the form of an exchange of letters between
the European Economic Community and the Bank for Interna-
tional Settlements concerning the mobilization of claims held by
the Member States under the medium-term financial assistance
arrangements
{bank 4; bankengesellschaft 1; kreditinstitut 1; zentralbank 1; fi-
nanzinstitut 1}
(b) The Office shall maintain an electronic data bank with the par-
ticulars of applications for registration of trade marks and entries
in the Register. The Office may also make available the contents
of this data bank on CD-ROM or in any other machine-readable
form.
{datenbank 4; bank 3; datenbanksystem 1; daten 1}
(c) established as a band of 1 km in width from the banks of a
river or the shores of a lake or coast for a length of at least 3 km.
{ufer 4; flussufer 3}
Table 1: Examples of contexts for the English word bank
with possible German translations
2.1 The CL-WSD Datasets
The data sets used for the SemEval?10 Cross-
Lingual Word Sense Disambiguation task were con-
structed by making a ?sense inventory? of all pos-
sible target language translations of a given source
language word based on word-alignments in Eu-
roparl (Koehn, 2005), with alignments involving the
relevant source words being manually checked. The
retrieved target words were manually lemmatised
and clustered into translations with a similar sense;
see Lefever and Hoste (2010a) for details.
Trial and test instances were extracted from two
other corpora, JRC-Acquis (Steinberger et al, 2006)
and BNC (Burnard, 2007). The trial data for each
language consists of five nouns (with 20 sentence
contexts per noun), and the test data of twenty nouns
(50 contexts each, so 1000 in total per language,
with the CL-WSD data covering Dutch, French,
Spanish, Italian and German). Table 1 provides ex-
amples from the trial data of contexts for the English
word bank and its possible translations in German.
Gold standard translations were created by hav-
ing four human translators picking the contextually
appropriate sense for each source word, choosing 0?
3 preferred target language translations for it. The
translations are thus restricted to those appearing in
Europarl, probably introducing a slight domain bias.
Each translation has an associated count indicating
how many annotators considered it to be among their
top-3 preferred translations in the given context.
bank, bankanleihe, bankanstalt, bankdarlehen, bankenge-
sellschaft, bankensektor, bankfeiertag, bankgesellschaft, bankin-
stitut, bankkonto, bankkredit, banknote, blutbank, daten, daten-
bank, datenbanksystem, euro-banknote, feiertag, finanzinstitut,
flussufer, geheimkonto, geldschein, gescha?ftsbank, handelsbank,
konto, kredit, kreditinstitut, nationalbank, notenbank, sparkasse,
sparkassenverband, ufer, weltbank, weltbankgeber, west-bank,
westbank, westjordanien, westjordanland, westjordanufer, west-
ufer, zentralbank
Table 2: All German translation candidates for bank as
extracted from the gold standard
In this way, for the English lemma bank, for ex-
ample, the CL-WSD trial gold standard for German
contains the word Bank itself, together with 40 other
translation candidates, as shown in Table 2. Eight
of those are related to river banks (Ufer, but also,
e.g., Westbank and Westjordanland), three concern
databases (Datenbank), and one is for blood banks.
The rest are connected to different types of finan-
cial institutions (such as Handelsbank and Finanz-
institut, but also by association Konto, Weldbank-
geber, Banknote, Geldschein, Kredit, etc.).
2.2 N-Gram Context Matching
N-gram matching is used to produce a ranked list
of translation candidates and their contexts, both in
order to provide robustness and to give a baseline
performance. The n-gram models were built using
the IRSTLM toolkit (Federico et al, 2008; Bungum
and Gamba?ck, 2012) on the DeWaC corpus (Baroni
and Kilgarriff, 2006), using the stopword list from
NLTK (Loper and Bird, 2002). The n-gram match-
ing procedure consists of two steps:
1. An nth order source context is extracted and the
translations for each SL word in this context
are retrieved from the dictionary. This includes
stopword filtering of the context.
2. All relevant n-grams are inspected in order
from left to right and from more specific (5-
grams) to least specific (single words).
For each part of the context with matching n-grams
in the target language model, the appropriate target
translation candidates are extracted and ranked ac-
cording to their language model probability. This
results in an n-best list of translation candidates.
23
Since dictionary entries are lemma-based, lemma-
tization was necessary to use this approach in com-
bination with the dictionary enhancements. The
source context is formed by the lemmata in the sen-
tence surrounding the focus word (the word to be
disambiguated) by a window of up to four words
in each direction, limited by a 5-gram maximum
length. In order to extract the semantically most rel-
evant content, stopwords are removed before con-
structing this source word window. For each of the
1?5 lemmata in the window, the relevant translation
candidates are retrieved from the bilingual dictio-
nary. The candidates form the ordered translation
context for the source word window.
The following example illustrates how the trans-
lation context is created for the focus word ?bank?.
First the relevant part of the source language sen-
tence with the focus word in bold face:
(1) The BIS could conclude stand-by credit
agreements with the creditor countries? cen-
tral bank if they should so request.
For example, using a context of two words in front
and two words after the focus word, the following
source language context is obtained after a prepro-
cessing involving lemmatization, stopword removal,
and insertion of sentence start (<s>) and end mark-
ers (</s>):
(2) country central bank request </s>
From this the possible n-grams in the target side con-
text are generated by assembling all ordered com-
binations of the translations of the source language
words for each context length: the widest contexts
(5-grams) are looked up first before moving on to
narrower contexts, and ending up with looking up
only the translation candidate in isolation.
Each of the n-grams is looked up in the language
model and for each context part the n-grams are or-
dered according to their language model probability.
Table 3 shows a few examples of such generated n-
grams with their corresponding scores from the n-
gram language model.1 The target candidates (ital-
ics) are then extracted from the ordered list of target
language n-grams. This gives an n-best list of trans-
1There are no scores for 4- and 5-grams; as expected when
using direct translation to generate target language n-grams.
n n-gram LM score
5 land mittig bank nachsuchen </s> Not found
4 mittig bank nachsuchen </s> Not found
3 mittig bank nachsuchen Not found
3 kredit anfragen </s> -0.266291
2 mittig bank -3.382560
2 zentral blutbank -5.144870
1 bank -3.673000
Table 3: Target language n-gram examples from look-
ups of stopword-filtered lemmata country central bank
request reported in log scores. The first 3 n-grams were
not found in the language model.
lation candidates from which the top-1 or top-5 can
be taken. Since multiple senses in the dictionary can
render the same literal output, duplicate translation
candidates are filtered out from the n-best list.
2.3 Dictionary-Based Context Matching
After creating the n-gram based list of translation
candidates, additional candidates are produced by
looking at multiword entries in a bilingual dictio-
nary. The existence of multiword entries in the dic-
tionary corresponding to adjacent lemmata in the
source context or translation candidates in the target
context is taken as a clear indicator for the suitability
of a particular translation candidate. Such entries are
added to the top of the n-best list, which represents
a strong preference in the disambiguation system.
Dictionaries are used in all experiments to look up
translation candidates and target language transla-
tions of the words in the context, but this approach is
mining the dictionaries by using lookups of greater
length. Thus is, for example, the dictionary entry
Community Bank translated to the translation candi-
date Commerzbank; this translation candidate would
be put on top of the list of prioritized answers.
Two separate procedures are used to find such in-
dicators, a direct procedure based on the source con-
text and an indirect procedure based on the weaker
target language context. These are detailed in pseu-
docode in Algorithms 1 and 2, and work as follows:
Source Language (SL) Method (Algorithm 1)
If there is a dictionary entry for the source word
and one of its adjacent words, search the set
of translations for any of the translation candi-
dates for the word alone. Specifically, transla-
24
Algorithm 1 SL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: comblemmas? list(previouslemma(b) + b, b + nextlemma(b)) . Find adjacent lemmata
3: for lem ? comblemmas do
4: c? sl-dictionary-lookup(lem) . Look up lemma in SL?TL dict.
5: if c ? tcands then rlist? list(c + rlist) . Push lookup result c onto rlist if in tcands
6: end if
7: end for
8: return rlist . Return new list with lemmata whose translations were in tcands on top
9: end procedure
Algorithm 2 TL algorithm to rank translation candidates (tcands) for SL lemma b given list of tcands
[The ready-made TL tcands from the dataset are looked up in TL-SL direction. It is necessary to keep a list of the
reverse-translation of the individual tcand as well as the original tcand itself, in order to monitor which tcand it was.
If the SL context is found in either of these reverse lookups the matching tcand is ranked high.]
1: procedure FINDCAND(list rlist,SL-lemma b, const tcands) . rlist is original ranking
2: for cand ? tcands do . Assemble list of TL translations
3: translist? list(cand, tl-dictionary-lookup(cand)) + translist
4: . Append TL?SL lookup results of tcands with cand as id
5: end for
6: for cand, trans ? translist do
7: if previouslemma(b)?nextlemma(b) ? trans then . If trans contains either SL lemma
8: rlist? list(cand) + rlist . append this cand onto rlist
9: end if
10: end for
11: return rlist
12: . Return tcands list; top-ranking tcands whose SL-neighbours were found in TL?SL lookup
13: end procedure
tions of the combination of the source word and
an adjacent word in the context are matched
against translation candidates for the word.
Target Language (TL) Method (Algorithm 2)
If a translation candidate looked up in the re-
verse direction matches the source word along
with one or more adjacent words, it is a good
translation candidate. TL candidates are looked
up in a TL?SL dictionary and multiword results
are matched against SL combinations of disam-
biguation words and their immediate contexts.
For both methods the dictionary entry for the tar-
get word or translation candidate is matched against
the immediate context. Thus both methods result
in two different lookups for each focus word, com-
bining it with the previous and next terms, respec-
tively. This is done exhaustively for all combina-
tions of translations of the words in the context win-
dow. Only one adjacent word was used, since very
few of the candidates were able to match the context
even with one word. Hence, virtually none would
be found with more context, making it very unlikely
that larger contexts would contribute to the disam-
biguation procedure, as wider matches would also
match the one-word contexts.
Also for both methods, translation candidates are
only added once, in case the same translation candi-
date generates hits with either (or both) of the meth-
ods. Looking at the running example, stopword fil-
tered and with lemmatized context:
(3) country central bank request
This example generates two source language multi-
word expressions, central bank and bank request. In
the source language method, these word combina-
25
tions are looked up in the dictionary where the zen-
tralbank entry is found for central bank, which is
also found as a translation candidate for bank.
The target language method works in the reverse
order, looking up the translation candidates in the
TL?SL direction and checking if the combined lem-
mata are among the candidates? translations into the
source language. In the example, the entry zentral-
bank:central bank is found in the dictionary, match-
ing the source language context, so zentralbank is
assumed to be a correct translation.
2.4 Dictionaries
Two English-German dictionaries were used in the
experiments, both with close to 1 million entries
(translations). One is a free on-line resource, while
the other was obtained by reversing an existing pro-
prietary German-English dictionary made available
to the authors by its owners:
? The GFAI dictionary (called ?D1? in Section 3
below) is a proprietary and substantially ex-
tended version of the Chemnitz dictionary, with
549k EN entries including 433k MWEs, and
552k DE entries (79k MWEs). The Chem-
nitz electronic German-English dictionary2 it-
self contains over 470,000 word translations
and is available under a GPL license.
? The freely available CC dictionary3 (?D2? be-
low) is an internet-based German-English and
English-German dictionary built through user
generated word definitions. It has 565k/440k
(total/MWE) EN and 548k/210k DE entries.
Note that the actual dictionaries are irrelevant to the
discussion at hand, and that we do not aim to point
out strengths or weaknesses of either dictionary, nor
to indicate a bias towards a specific resource.
3 Results
Experiments were carried out both on the trial and
test data described in Section 2.1 (5 trial and 20 test
words; with 20 resp. 50 instances for each word; in
total 1100 instances in need of disambiguation). The
results show that the dictionaries yield answers with
2http://dict.tu-chemnitz.de/
3http://www.dict.cc/
high precision, although they are robust enough to
solve the SemEval WSD challenge on their own.
For measuring the success rate of the developed
models, we adopt the ?Out-Of-Five? (OOF) score
(Lefever and Hoste, 2010b) from the SemEval?10
Cross-Lingual Word Sense Disambiguation task.
The Out-Of-Five criterion measures how well the
top five candidates from the system match the top
five translations in the gold standard:
OOF (i) =
?
a?Ai
freq i(a)
|Hi|
where Hi denotes the multiset of translations pro-
posed by humans for the focus word in each source
sentence si (1 ? i ? N , N being the number
of test items). Ai is the set of translations produced
by the system for source term i. Since each transla-
tion has an associated count of how many annotators
chose it, there is for each si a function freq i return-
ing this count for each term in Hi (0 for all other
terms), and max freq i gives the maximal count for
any term in Hi. For the first example in Table 1:
?
??????????
??????????
H1 = {bank, bank, bank, bank, zentralbank,
bankengesellschaft, kreditinstitut, finanzinstitut}
freq1(bank) = 4
. . .
freq1(finanzinstitut) = 1
maxfreq1 = 4
and the cardinality of the multiset is: |H1| = 8. This
equates to the sum of all top-3 preferences given to
the translation candidates by all annotators.
For the Out-Of-Five evaluation, the CL-WSD sys-
tems were allowed to submit up to five candidates
of equal rank. OOF is a recall-oriented measure
with no additional penalty for precision errors, so
there is no benefit in outputting less than five can-
didates. With respect to the previous example from
Table 1, the maximum score is obtained by system
output A1 = {bank, bankengesellschaft, kreditinstitut,
zentralbank, finanzinstitut}, which gives OOF (1) =
(4 + 1 + 1 + 1 + 1)/8 = 1, whereas A2 = {bank,
bankengesellschaft, nationalbank, notenbank, sparkasse}
would give OOF (1) = (4 + 1)/8 = 0.625.4
4Note that the maximum OOF score is not always 1 (i.e., it
is not normalized), since the gold standard sometimes contains
more than five translation alternatives.
26
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
Top 8.89 6.99 8.89 22.71 24.43 25.34 24.67
Low 0.00 0.00 0.00 0.00 0.00 0.00 0.00
Mean 2.71 0.99 3.04 8.35 7.10 9.24 10.13
Table 4: F1-score results for individual dictionaries
Source language Target language All
Dictionary D1 D2 comb D1 D2 comb comb
coach 1.00 0.00 1.00 0.21 0.00 0.21 0.21
education 0.83 0.67 0.83 0.47 0.62 0.54 0.53
execution 0.00 0.00 0.00 0.17 0.22 0.17 0.17
figure 1.00 0.00 1.00 0.51 0.57 0.55 0.55
job 0.88 0.80 0.94 0.45 0.78 0.46 0.44
letter 1.00 0.00 1.00 0.66 0.75 0.62 0.66
match 1.00 1.00 1.00 0.80 0.50 0.80 0.80
mission 0.71 0.33 0.71 0.46 0.37 0.36 0.36
mood 0.00 0.00 0.00 0.00 0.00 0.00 0.00
paper 0.68 0.17 0.68 0.53 0.35 0.55 0.55
post 1.00 1.00 1.00 0.39 0.48 0.45 0.48
pot 0.00 0.00 0.00 1.00 1.00 1.00 1.00
range 1.00 1.00 1.00 0.28 0.37 0.30 0.30
rest 1.00 0.67 1.00 0.60 0.56 0.56 0.58
ring 0.09 0.00 0.09 0.37 0.93 0.38 0.38
scene 1.00 0.00 1.00 0.50 0.42 0.44 0.50
side 1.00 0.00 1.00 0.21 0.16 0.23 0.27
soil 1.00 0.00 1.00 0.72 0.58 0.66 0.69
strain 0.00 0.00 0.00 0.51 0.88 0.55 0.55
test 1.00 1.00 1.00 0.62 0.52 0.57 0.61
Mean 0.84 0.74 0.84 0.50 0.56 0.49 0.51
Table 5: Precision scores for all terms filtering out those
instances for which no candidates were suggested
For assessing overall system performance in
the experiments, we take the best (?Top?), worst
(?Low?), and average (?Mean?) of the OOF scores
for all the SL focus words, with F1-score reported
as the harmonic mean of the precision and recall of
the OOF scores. Table 4 shows results for each dic-
tionary approach on the test set, with ?D1? being
the GFAI dictionary, ?D2? the CC dictionary, and
?comb? the combination of both. Target language
look-up contributes more to providing good transla-
tion candidates than the source language methodol-
ogy, and also outperforms a strategy combining all
dictionaries in both directions (?All comb?).
Filtering out the instances for which no candi-
date translation was produced, and taking the aver-
age precision scores only over these, gives the re-
sults shown in Table 5. Markedly different preci-
sion scores can be noticed, but the source language
Source language Target language
Dictionary D1 D2 D1 D2
Mean 3.25 1.5 12.65 11.45
Total 223 256 1,164 880
Table 6: Number of instances with a translation candidate
(?Mean?) and the total number of suggested candidates
Most Most Freq 5-gram 5-gram All Dict VSM
Freq Aligned + Dict Comb Model
Top 51.77 68.71 52.02 52.74 24.67 55.92
Low 1.76 9.93 14.09 15.40 0.00 10.73
Mean 21.18 34.61 30.36 36.38 10.13 30.30
Table 7: Overview of results (F1-scores) on SemEval data
method again has higher precision on the sugges-
tions it makes than the target language counterpart.
As shown in Table 6, this higher precision is offset
by lower coverage, with far fewer instances actually
producing a translation candidate with the dictionary
lookup methods. There is a notable difference in the
precision of the SL and TL approaches, coinciding
with more candidates produced by the latter. Several
words in Table 5 give 100% precision scores for at
least one dictionary, while a few give 0% precision
for some dictionaries. The word ?mood? even has
0% precision for both dictionaries in both directions.
Table 7 gives an overview of different approaches
to word translation disambiguation on the dataset.
For each method, the three lines again give both
the best and worst scoring terms, and the mean
value for all test words. The maximum attainable
score for each of those would be 99.28, 90.48 and
95.47, respectively, but those are perfect scores not
reachable for all items, as described above (OOF-
scoring). Instead the columns Most Freq and Most
Freq aligned give the baseline scores for the Sem-
Eval dataset: the translation most frequently seen
in the corpus and the translation most frequently
aligned in a word-aligned parallel corpus (Europarl),
respectively. Then follows the results when using
only a stopword-filtered 5-gram model built with the
IRSTLM language modeling kit (Federico and Cet-
tolo, 2007), and when combining the 5-gram model
with the dictionary approach (5-gram + Dict).
The next column (All Dict Comb) shows how the
dictionary methods fared on their own. The com-
27
bined dictionary approach has low recall (see Ta-
ble 6) and does not alone provide a good solution to
the overall problem. Due to high precision, however,
the approach is able to enhance the n-gram method
that already produces acceptable results. Finally, the
column VSM Model as comparison gives the results
obtained when using a Vector Space Model for word
translation disambiguation (Marsi et al, 2011).
Comparing the dictionary approach to state-of-
the-art monolingual solutions to the WTD problem
on this dataset shows that the approach performs bet-
ter for the Lowest and Mean scores of the terms, but
not for the Top scores (Lynum et al, 2012). As can
be seen in Table 7, the vector space model produced
the overall best score for a single term. However, the
method combining a 5-gram language model with
the dictionary approach was best both at avoiding
really low scores for any single term and when com-
paring the mean scores for all the terms.
4 Discussion and Conclusion
The paper has presented a method for using dictio-
nary lookups based on the adjacent words in both
the source language text and target language candi-
date translation texts to disambiguate word transla-
tion candidates. By composing lookup words by us-
ing both neighbouring words, improved disambigua-
tion performance was obtained on the data from the
SemEval?10 English-German Cross-Lingual Word
Sense Disambiguation task. The extended use of
dictionaries proves a valuable source of informa-
tion for disambiguation, and can introduce low-cost
phrase-level translation to quantitative Word Sense
Disambiguation approaches such as N-gram or Vec-
tor Space Model methods, often lacking the phrases-
based dimension.
The results show clear differences between the
source and target language methods of using dictio-
nary lookups, where the former has very high preci-
sion (0.84) but low coverage, while the TL method
compensates lower precision (0.51) with markedly
better coverage. The SL dictionary method pro-
vided answers to only between 1.5 and 3.25 of 50
instances per word on average, depending on the dic-
tionary. This owes largely to the differences in algo-
rithms, where the TL method matches any adjacent
lemma to the focus word with the translation of the
pre-defined translation candidates, whereas the SL
method matches dictionaries of the combined lem-
mata of the focus word and its adjacent words to the
same list of translation candidates. False positives
are expected with lower constraints such as these.
On the SemEval data, the contribution of the dictio-
nary methods to the n-grams is mostly in improving
the average score.
The idea of acquiring lexical information from
corpora is of course not new in itself. So did, e.g.,
Rapp (1999) use vector-space models for the pur-
pose of extracting ranked lists of translation can-
didates for extending a dictionary for word trans-
lation disambiguation. Chiao and Zweigenbaum
(2002) tried to identify translational equivalences
by investigating the relations between target and
source language word distributions in a restricted
domain, and also applied reverse-translation filtering
for improved performance, while Sadat et al (2003)
utilised non-aligned, comparable corpora to induce
a bilingual lexicon, using a bidirectional method
(SL?TL, TL?SL, and a combination of both).
Extending the method to use an arbitrary size win-
dow around all words in the context of each focus
word (not just the word itself) could identify more
multiword expressions and generate a more accurate
bag-of-words for a data-driven approach. Differ-
ences between dictionaries could also be explored,
giving more weight to translations found in two or
more dictionaries. Furthermore, the differences be-
tween the SL and TL methods could explored fur-
ther, investigating in detail the consequences of us-
ing a symmetrical dictionary, in order to study the
effect that increased coverage has on results. Test-
ing the idea on more languages will help verify the
validity of these findings.
Acknowledgements
This research has received funding from NTNU and from
the European Community?s 7th Framework Programme
under contract nr 248307 (PRESEMT). Thanks to the
other project participants and the anonymous reviewers
for several very useful comments.
28
References
Bai, M.-H., You, J.-M., Chen, K.-J., and Chang,
J. S. (2009). Acquiring translation equivalences of
multiword expressions by normalized correlation
frequencies. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 478?486, Singapore. ACL.
Baroni, M. and Kilgarriff, A. (2006). Large
linguistically-processed web corpora for multiple
languages. In Proceedings of the 11th Conference
of the European Chapter of the Association for
Computational Linguistics, pages 87?90, Trento,
Italy. ACL.
Bungum, L. and Gamba?ck, B. (2012). Efficient n-
gram language modeling for billion word web-
corpora. In Proceedings of the 8th International
Conference on Language Resources and Evalua-
tion, pages 6?12, Istanbul, Turkey. ELRA. Work-
shop on Challenges in the Management of Large
Corpora.
Burnard, L., editor (2007). Reference Guide for the
British National Corpus (XML Edition). BNC
Consortium, Oxford, England. http://www.
natcorp.ox.ac.uk/XMLedition/URG.
Caseli, H. d. M., Ramisch, C., das Grac?as
Volpe Nunes, M., and Villavicencio, A. (2010).
Alignment-based extraction of multiword expres-
sions. Language Resources and Evaluation, 44(1-
2):59?77. Special Issue on Multiword expression:
hard going or plain sailing.
Chiao, Y.-C. and Zweigenbaum, P. (2002). Look-
ing for candidate translational equivalents in spe-
cialized comparable corpora. In Proceedings of
the 40th Annual Meeting of the Association for
Computational Linguistics, volume 2, pages 1?5,
Philadelphia, Pennsylvania. ACL. Also published
in AMIA Annual Symposium 2002, pp. 150?154.
Federico, M., Bertoldi, N., and Cettolo, M. (2008).
Irstlm: an open source toolkit for handling large
scale language models. In INTERSPEECH, pages
1618?1621. ISCA.
Federico, M. and Cettolo, M. (2007). Efficient han-
dling of n-gram language models for statistical
machine translation. In Proceedings of the 45th
Annual Meeting of the Association for Compu-
tational Linguistics, pages 88?95, Prague, Czech
Republic. ACL. 2nd Workshop on Statistical Ma-
chine Translation.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the 10th Machine Translation Summit, pages 79?
86, Phuket, Thailand.
Lefever, E. and Hoste, V. (2010a). Construction
of a benchmark data set for cross-lingual word
sense disambiguation. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation, pages 1584?1590, Valetta, Malta.
ELRA.
Lefever, E. and Hoste, V. (2010b). SemEval-2010
Task 3: Cross-lingual word sense disambiguation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
15?20, Uppsala, Sweden. ACL. 5th International
Workshop on Semantic Evaluation.
Loper, E. and Bird, S. (2002). NLTK: the natu-
ral language toolkit. In Proceedings of the ACL-
02 Workshop on Effective tools and methodolo-
gies for teaching natural language processing and
computational linguistics - Volume 1, ETMTNLP
?02, pages 63?70, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
LREC06 (2006). Proceedings of the 5th Interna-
tional Conference on Language Resources and
Evaluation, Genova, Italy. ELRA.
Lynum, A., Marsi, E., Bungum, L., and Gamba?ck,
B. (2012). Disambiguating word translations with
target language models. In Proceedings of the
15th International Conference on Text, Speech
and Dialogue, pages 378?385, Brno, Czech Re-
public. Springer.
Marsi, E., Lynum, A., Bungum, L., and Gamba?ck,
B. (2011). Word translation disambiguation with-
out parallel texts. In Proceedings of the Inter-
national Workshop on Using Linguistic Informa-
tion for Hybrid Machine Translation, pages 66?
74, Barcelona, Spain.
Rapp, R. (1999). Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, Madrid, Spain. ACL.
29
Ren, Z., Lu?, Y., Cao, J., Liu, Q., and Huang, Y.
(2009). Improving statistical machine translation
using domain bilingual multiword expressions. In
Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics, pages
47?54, Singapore. ACL. Workshop on Multiword
Expressions: Identification, Interpretation, Dis-
ambiguation and Applications.
Sadat, F., Yoshikawa, M., and Uemura, S. (2003).
Learning bilingual translations from comparable
corpora to cross-language information retrieval:
Hybrid statistics-based and linguistics-based ap-
proach. In Proceedings of the 41th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 57?64, Sapporo, Japan. ACL. 6th
International Workshop on Information Retrieval
with Asian languages; a shorter version published
in ACL Annual Meeting 2003, pp. 141?144.
Sag, I., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. (2002). Multiword expressions:
A pain in the neck for NLP. In Gelbukh, A.,
editor, Computational Linguistics and Intelligent
Text Processing: Proceedings of the 3rd Interna-
tional Conference, number 2276 in Lecture Notes
in Computer Science, pages 189?206, Mexico
City, Mexico. Springer-Verlag.
Sharoff, S., Babych, B., and Hartley, A. (2006). Us-
ing collocations from comparable corpora to find
translation equivalents. In LREC06 (2006), pages
465?470.
Sofianopoulos, S., Vassiliou, M., and Tambouratzis,
G. (2012). Implementing a language-independent
MT methodology. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 1?10, Jeju, Korea. ACL.
First Workshop on Multilingual Modeling.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat,
C., Erjavec, T., Tufis?, D., and Varga, D. (2006).
The JRC-Acquis: A multilingual aligned parallel
corpus with 20+ languages. In LREC06 (2006),
pages 2142?2147.
30
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 49?54,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
Agent-based modeling of language evolution
Torvald Lekvam Bj
?
orn Gamb
?
ack Lars Bungum
Department of Computer and Information Science, Sem S?lands vei 7?9
Norwegian University of Science and Technology, 7491 Trondheim, Norway
torvald@lekvam.no {gamback,larsbun}@idi.ntnu.no
Abstract
Agent-based models of language evolution
have received a lot of attention in the last
two decades. Researchers wish to under-
stand the origin of language, and aim to
compensate for the lacking empirical evi-
dence by utilizing methods from computer
science and artificial life. The paper looks
at the main theories of language evolution:
biological evolution, learning, and cultural
evolution. In particular, the Baldwin effect
in a naming game model is elaborated on
by describing a set of experimental simu-
lations. This is on-going work and ideas
for further investigating the social aspects
of language evolution are also discussed.
1 Introduction
What is language? It is interesting how we can
take a train of thought and transfer this into an-
other person?s mind by pushing the air around us.
Human language, this complex medium that dis-
tinctly separates humans from animals, has baf-
fled scientists for centuries. While animals also
use language, even with a degree of syntax (Kako,
1999), spoken human language exhibits a vastly
more complex structure and spacious variation.
To understand how language works ? how it
is used, its origin and fundamentals ? our best
information sources are the languages alive (and
some extinct but documented ones). Depending on
definition, there are 6,000?8,000 languages world-
wide today, showing extensive diversity of syntax,
semantics, phonetics and morphology (Evans and
Levinson, 2009). Still, these represent perhaps
only 2% of all languages that have ever existed
(Pagel, 2000). As this is a rather small window, we
want to look back in time. But there is a problem
in linguistic history: our reconstruction techniques
can only take us back some 6,000 to 7,000 years.
Beyond this point, researchers can only speculate
on when and how human language evolved: either
as a slowly proceeding process starting millions
of years (Ma) ago, e.g., 7 Ma ago with the first
appearance of cognitive capacity or 2.5 Ma ago
with the first manufacture of stone implements; or
through some radical change taking place about
100 ka ago with the appearance of the modern hu-
mans or 50?60 ka ago when they started leaving
Africa (Klein, 2008; Tattersall, 2010).
The rest of this introduction covers some key
aspects of language evolution. Section 2 then fo-
cuses on computational models within the field,
while Section 3 describes a specific naming game
model. Finally, Section 4 discusses the results and
some ideas for future work.
1.1 Theories of origin: the biological aspect
There are two main ideas in biological evolution
as to why humans developed the capacity to com-
municate through speech. The first states that lan-
guage (or more precisely the ability to bear the full
structure of language) came as an epiphenomenon,
a by-product (spandrel) of an unrelated mutation.
This theory assumes that a mental language fac-
ulty could not by itself evolve by natural selection;
there would simply be too many costly adaptations
for it to be possible. Thus there should exist an in-
nate capacity in the form of a universal grammar
(Chomsky, 1986), which can hold a finite number
of rules enabling us to carry any kind of language.
According to the second idea, language
emerged in a strictly adaptational process (Pinker
and Bloom, 1990). That is, that language evolu-
tion can be explained by natural selection, in the
same way as the evolution of other complex traits
like echolocation in bats or stereopsis in monkeys.
Both ideas ? innate capacity vs natural selection
? have supporters, as well as standpoints that
hold both aspects as important, but at different lev-
els (Deacon, 2010; Christiansen and Kirby, 2003).
49
1.2 Theories of origin: the cultural aspect
Biology aside, the forces behind the emergence of
human language are not strictly genetic (and do
not operate only on a phylogenetic time scale).
Kirby (2002) argues that, in addition to biological
evolution, there are two more complex adaptive
(dynamical) systems influencing natural language;
namely cultural evolution (on the glossogenetic
time scale) and learning (which operates on a in-
dividual level, on the ontogenetic time scale).
In addition, there is the interesting Darwinian
idea that cultural learning can guide biological
evolution, a process known as the Baldwin effect
(Baldwin, 1896; Simpson, 1953). This theory ar-
gues that culturally learned traits (e.g., a univer-
sal understanding of grammar or a defense mech-
anism against a predator) can assimilate into the
genetic makeup of a species. Teaching each mem-
ber in a population the same thing over and over
again comes with great cost (time, faulty learn-
ing, genetic complexity), and the overall popula-
tion saves a lot of energy if a learned trait would
become innate. On the other hand, there is a cost
of genetic assimilation as it can prohibit plastic-
ity in future generations and make individuals less
adaptive to unstable environments.
There has been much debate recently whether
language is a result of the Baldwin effect or not
(Evans and Levinson, 2009; Chater et al., 2009;
Baronchelli et al., 2012, e.g.), but questions, hypo-
theses, and simulations fly in both directions.
2 Language evolution and computation
Since the 90s, there has been much work on sim-
ulation of language evolution in bottom-up sys-
tems with populations of autonomous agents. The
field is highly influenced by the work of Steels and
Kirby, respectively, and has been summarized and
reviewed both by themselves and others (Steels,
2011; Kirby, 2002; Gong and Shuai, 2013, e.g.).
Computational research in this field is limited
to modeling very simplified features of human
language in isolation, such as strategies for nam-
ing colors (Bleys and Steels, 2011; Puglisi et al.,
2008), different aspects of morphology (Dale and
Lupyan, 2012), and similar. This simplicity is im-
portant to keep in mind, since it is conceivable that
certain features of language can be highly influ-
enced by other features in real life.
A language game simulation (Steels, 1995) is
a model where artificial agents interact with each
other in turn in order to reach a cooperative goal;
to make up a shared language of some sort, all
while minimizing their cognitive effort. All agents
are to some degree given the cognitive ability to
bear language, but not given any prior knowledge
of how language should look like or how consen-
sus should unfold. No centralized anchors are in-
volved: a simulation is all self-organized.
Agents are chosen (mostly at random) as hearer
and speaker, and made to exchange an utterance
about a certain arbitrary concept or meaning in
their environment. If the agents use the same lan-
guage (i.e., the utterance is understood by both
parties), the conversation is a success. If the
speaker utters something unfamiliar to the hearer,
the conversation is termed a failure. If an agent
wants to express some concept without having any
utterances for it, the agent is assumed to have the
ability to make one up and add this to its memory.
While interpretation in real life is a complex af-
fair, it is mostly assumed that there is a fairly direct
connection between utterance and actual meaning
in language game models (emotions and social sit-
uations do not bias how language is interpreted).
A simple language game normally is charac-
terized by many synonyms spawning among the
agents. As agents commence spreading their own
utterances around, high-weighted words start to be
preferred. Consensus is reached when all agents
know the highest weighted word for each concept.
Commonly, the agents aim to reach a single co-
herent language, but the emergence of multilin-
gualism has also been simulated (Lipowska, 2011;
Roberts, 2012). Cultural evolution can be captured
by horizontal communication between individuals
in the same generation or vertical communication
from adults to children. The latter typically lets the
agents breed, age and die, with the iterated learn-
ing model (Smith et al., 2003) being popular.
A variety of language games exist, from sim-
ple naming games, where the agents? only topic
concerns one specific object (Lipowska, 2011), to
more cognitive grounding games (Steels and Loet-
zsch, 2012). There have also been studies on some
more complex types of interaction, such as spa-
tial games (Spranger, 2013), factual description
games (van Trijp, 2012) and action games (Steels
and Spranger, 2009), where the agent communi-
cation is about objects in a physical environment,
about real-world events, and about motoric behav-
iors, respectively.
50
3 The Baldwin effect in a naming game
Several researchers have created simulations to in-
vestigate the Baldwin effect, starting with Hinton
and Nowlan (1987). Cangelosi and Parisi (2002)
simulate agents who evolve a simple grammatical
language in order to survive in a world filled with
edible and poisonous mushrooms. Munroe and
Cangelosi (2002) used this model to pursue the
Baldwin effect, with partially blind agents initially
having to learn features of edible mushrooms, but
with the learned abilities getting more and more
assimilated into the genome over the generations.
Chater et al. (2009) argue that only stable parts of
language may assimilate into the genetic makeup,
while variation within the linguistic environment
is too unstable to be a target of natural selection.
Watanabe et al. (2008) use a similar model, but in
contrast state that genetic assimilation not neces-
sarily requires a stable linguistic environment.
Lipowska (2011) has pursued the Baldwin ef-
fect in a simple naming game model with the in-
tention of mixing up a language game in a simu-
lation that incorporates both learning, cultural and
biological evolution. The model places a set of
agents in a square lattice of a linear size L, where
every agent is allowed ? by a given probability p
? to communicate with a random neighbor.
At each time step, a random agent is chosen and
p initially decides whether the agent is allowed to
communicate or will face a ?population update?.
Every agent has an internal lexicon of N words
with associated weights (w
j
: 1 ? j ? N ). When-
ever a chosen speaker is to utter a word, the agent
selects a word i from its lexicon with the probabil-
ity w
i
/
?
N
j=1
w
j
. If the lexicon is empty (N = 0),
a word is made up. A random neighbor in the lat-
tice is then chosen as the hearer. If both agents
know the uttered word, the dialog is deemed a
success, and if not, a failure. Upon success, both
agents increase the uttered word?s weight in their
lexica by a learning ability variable. Each agent k
is equipped with such a variable l (0 < l
k
< 1).
This learning ability is meant to, in its simplicity,
reflect the genetic assimilation.
Instead of engaging in communication, the cho-
sen agent is occasionally updated, by a probability
1? p. Agents die or survive with a probability p
s
which is given by an equation that takes into ac-
count age, knowledge (lexicon weights in respect
to the population?s average weights), and simula-
tion arguments. If the agent has a high-weighted
lexicon and is young of age, and therefore survives
at a given time step, the agent is allowed to breed
if there are empty spaces in its neighborhood.
All in all, each time step can terminate with
eight different scenarios: in addition to the two
communication scenarios (success or failure), the
scenario where the agent dies, as well as the one
where the agents lives but only has non-empty
neighbors (so that no change is possible), there are
four possibilities for breeding. If the agent breeds,
the off-spring either inherit the parent?s learning
ability or gain a new learning ability, with a proba-
bility p
m
. With the same mutation probability, the
off-spring also either gains a new word or inherits
the parent?s highest-weight word.
This model was implemented with the aim to
reproduce Lipowska?s results. She argues that her
model is fairly robust to both population size and
her given arguments; however, our experiments
do not support this: as the Baldwin effect unfold,
it does not follow the same abrupt course as in
Lipowska?s model. This could be due to some as-
sumptions that had to be made, since Lipowska
(2011), for instance, presents no details on how
age is calculated. We thus assume that every time
an agent is allowed to communicate, its age gets
incremented. Another possibility could be to in-
crement every agent?s age at every time step, so
that agents get older even if they do not commu-
nicate. Furthermore, the initial values for learn-
ability are not clearly stated. Lipowska uses sev-
eral different values in her analysis. We have used
0.5, which makes a decrease in learnability a part
of the evolutionary search space as well.
Simulations with parameters similar to those
used by Lipowska (2011) [iterations = 200, 000,
mutation
chance
= 0.01, L = 25, p = 0.4, l = 0.5],
produce results as in Figure 1, showing the highest
weighted word per agent after 50k and 150k time
steps, with each agent being a dot in a ?heat map?;
black dots indicate dead agents (empty space).
The number of groups are reduced over time, and
their sizes grow, as more agents agree on a lex-
icon and as favorable mutations spread through
the population, (as indicated by agent learnability;
Figure 2). Even after 200k iterations, consensus is
not reached (which it was in Lipowska?s simula-
tion), but the agent population agrees on one word
if the simulation is allowed to run further. It is nat-
ural to assume that the difference lays in the details
of how age is calculated, as noted above.
51
Figure 1: Ca 16 different words dominate the pop-
ulation at iteration 50k and nine at iteration 150k.
Figure 2: Mutations favoring learnability at itera-
tion 50k spread substantially by iteration 150k.
Diverting from Lipowska?s parameters and
skewing towards faster turnover (higher mutation
rate, higher possibility of survival with richer lex-
icon/higher age, etc.), gives behavior similar to
hers, with faster and more abrupt genetic assim-
ilation, as shown Figure 3. The upper line in the
figure represents the fraction of agents alive in the
lattice. It is initially fully populated, but the popu-
lation decreases with time and balances at a point
where death and birth are equally tensioned.
Agents with higher learnability tend to live
longer, and the lower graph in Figure 3 shows the
average learnability in the population. It is roughly
sigmoid (S-shaped; cf. Lipowska?s experiment) as
a result of slow mutation rate in the first phase,
followed by a phase with rapid mutation rate (ca
100k?170k) as the learnability also gets inherited,
and decreasing rate towards the end when mu-
tations are more likely to ruin agent learnability
(when the learning ability l is at its upper limit).
As can be seen in Figure 4, the agents rapidly get
to a stable weighted lexicon before the Baldwin
effect shows itself around time step 100k.
As mentioned, Lipowska?s model did not reflect
the robustness argued in her paper: for other val-
ues of p, the number of empty spots in the popu-
lation lattice starts to diverge substantially, and for
some values all agents simply die. As population
sizes vary, the number of iterations must also be
adjusted to get similar results. If not, the agents
will not reach the same population turn-over as
Figure 3: Fraction of agents alive in the lattice and
average learnability in the population (s-shaped).
Figure 4: Average sum of weights in agent lexica.
for smaller population sizes since only one agent
may be updated per iteration. Lipowska (2011)
compensated with higher mutation rate on simu-
lations with different population sizes; however,
these could be two variables somewhat more inde-
pendent of each other. The model would have been
much more stable if it contained aspects of a typi-
cal genetic algorithm, where agents are allowed to
interact freely within generations. This way, the
model could be acting more upon natural selec-
tion (and in search of the Baldwin effect), instead
of relying on well-chosen parameters to work.
4 Discussion and future work
Language is a complex adaptive system with nu-
merous variables to consider. Thus we must make
a number of assumptions when studying language
and its evolution, and can only investigate certain
aspects at a time through simplifications and ab-
stractions. As this paper has concentrated on the
agent-based models of the field, many studies re-
flecting such other aspects had to be left out.
In addition, there has lately been a lot of work
studying small adjustments to the agent-based
models, in order to make them more realistic by,
for example, having multiple hearers in a lan-
guage game conversations (Li et al., 2013), dif-
ferent topologies (Lei et al., 2010; Lipowska and
Lipowski, 2012), and more heterogeneous popula-
tions (Gong et al., 2006).
52
In general, though, simulations on language
evolution tend to have relatively small and fixed
sizes (Baronchelli et al., 2006; Vogt, 2007) ? and
few studies seem to take social dynamics (Gong et
al., 2008; Kalampokis et al., 2007) or geography
into account (Patriarca and Heinsalu, 2009).
Further work is still needed to make existing
models more realistic and to analyze relations be-
tween different models (e.g., by combining them).
Biological evolution could be studied with more
flexible (or plastic) neural networks. Cultural evo-
lution could be investigated under more realistic
geographical and demographical influence, while
learning could be analyzed even further in light of
social dynamics, as different linguistic phenom-
ena unfold. Quillinan (2006) presented a model
concerning how a network of social relationships
could evolve with language traits. This model
could be taken further in combination with exist-
ing language games or it could be used to show
how language responds to an exposure of continu-
ous change in a complex social network.
Notably, many present models have a rather
na??ve way of selecting cultural parents, and a
genetic algorithm for giving fitness to agents in
terms of having (assimilated) the best strategies
for learning (e.g., memory efficiency), social con-
ventions (e.g., emotions, popularity), and/or sim-
ple or more advanced grammar could be explored.
A particular path we aim to pursue is to study a
language game with a simple grammar under so-
cial influence (e.g., with populations in different
fixed and non-fixed graphs, with multiple hearers),
contained within a genetic algorithm. In such a
setting, the agents must come up with strategies
for spreading and learning new languages, and
need to develop fault-tolerant models for speaking
with close and distant neighbors. This could be a
robust model where a typical language game could
be examined, in respect to both biological and cul-
tural evolution, with a more realistic perspective.
Acknowledgments
We would like thank the three anonymous review-
ers for several very useful comments. Thanks also
to Keith Downing for providing feedback on work
underlying this article.
The third author is supported by a grant from the
Norwegian University of Science and Technology.
Part of this work was funded by the PRESEMT
project (EC grant number FP7-ICT-4-248307).
References
James Mark Baldwin. 1896. A new factor in evolution.
The American Naturalist, 30(354):441?451.
Andrea Baronchelli, Maddalena Felici, Vittorio Loreto,
Emanuele Caglioti, and Luc Steels. 2006. Sharp
transition towards shared vocabularies in multi-
agent systems. Journal of Statistical Mechanics:
Theory and Experiment, 2006(06):P06014.
Andrea Baronchelli, Nick Chater, Romualdo Pastor-
Satorras, and Morten H. Christiansen. 2012. The
biological origin of linguistic diversity. PLoS ONE,
7(10):e48029.
Joris Bleys and Luc Steels. 2011. Linguistic selec-
tion of language strategies. In G. Kampis, I. Kar-
sai, and E. Szathm?ary, editors, Advances in Artificial
Life. Darwin Meets von Neumann, volume 2, pages
150?157. Springer.
Angelo Cangelosi and Domenico Parisi. 2002. Com-
puter simulation: A new scientific approach to the
study of language evolution. In Angelo Cangelosi
and Domenico Parisi, editors, Simulating the Evolu-
tion of Language, chapter 1, pages 3?28. Springer,
London.
Nick Chater, Florencia Reali, and Morten H Chris-
tiansen. 2009. Restrictions on biological adaptation
in language evolution. Proceedings of the National
Academy of Sciences, 106(4):1015?1020.
Noam Chomsky. 1986. Knowledge of language: Its
nature, origins, and use. Greenwood.
Morten H. Christiansen and Simon Kirby. 2003.
Language evolution: consensus and controversies.
TRENDS in Cognitive Sciences, 7(7):300?307.
Rick Dale and Gary Lupyan. 2012. Understanding
the origins of morphological diversity: the linguis-
tic niche hypothesis. Advances in Complex Systems,
15(03n04):1150017.
Terrence W. Deacon. 2010. A role for relaxed se-
lection in the evolution of the language capacity.
Proceedings of the National Academy of Sciences,
107(Supplement 2):9000?9006.
Nicholas Evans and Stephen C. Levinson. 2009. The
myth of language universals: Language diversity
and its importance for cognitive science. Behavioral
and Brain Sciences, 32(05):429?448.
Tao Gong and Lan Shuai. 2013. Computer simulation
as a scientific approach in evolutionary linguistics.
Language Sciences, 40:12?23.
Tao Gong, James W. Minett, and William S-Y Wang.
2006. Language origin and the effects of individ-
uals popularity. In Proceedings of the 2006 IEEE
Congress on Evolutionary Computation, pages 999?
1006, Vancouver, British Columbia, Jul. IEEE.
53
Tao Gong, James W. Minett, and William S-Y Wang.
2008. Exploring social structure effect on language
evolution based on a computational model. Connec-
tion Science, 20(2-3):135?153.
Geoffrey E Hinton and Steven J Nowlan. 1987. How
learning can guide evolution. Complex systems,
1(3):495?502.
Edward Kako. 1999. Elements of syntax in the sys-
tems of three language-trained animals. Animal
Learning & Behavior, 27(1):1?14.
Alkiviadis Kalampokis, Kosmas Kosmidis, and Panos
Argyrakis. 2007. Evolution of vocabulary on scale-
free and random networks. Physica A: Statistical
Mechanics and its Applications, 379(2):665 ? 671.
Simon Kirby. 2002. Natural language from artificial
life. Artificial Life, 8(2):185?215.
Richard G. Klein. 2008. Out of Africa and the evo-
lution of human behavior. Evolutionary Anthropol-
ogy: Issues, News, and Reviews, 17(6):267?281.
Chuang Lei, Jianyuan Jia, Te Wu, and Long Wang.
2010. Coevolution with weights of names in struc-
tured language games. Physica A: Statistical Me-
chanics and its Applications, 389(24):5628?5634.
Bing Li, Guanrong Chen, and Tommy W.S. Chow.
2013. Naming game with multiple hearers. Com-
munications in Nonlinear Science and Numerical
Simulation, 18(5):1214?1228.
Dorota Lipowska and Adam Lipowski. 2012. Naming
game on adaptive weighted networks. Artificial Life,
18(3):311?323.
Dorota Lipowska. 2011. Naming game and compu-
tational modelling of language evolution. Compu-
tational Methods in Science and Technology, 17(1?
2):41?51.
Steve Munroe and Angelo Cangelosi. 2002. Learning
and the evolution of language: the role of cultural
variation and learning costs in the Baldwin effect.
Artificial Life, 8(4):311?339.
Mark Pagel. 2000. The history, rate and pattern of
world linguistic evolution. In Ch. Knight, J.R. Hur-
ford, and M. Studdert-Kennedy, editors, The Evo-
lutionary Emergence of Language: Social Func-
tion and the Origins of Linguistic Form, chapter 22,
pages 391?416. Cambridge University Press.
Marco Patriarca and Els Heinsalu. 2009. Influence
of geography on language competition. Physica A:
Statistical Mechanics and its Applications, 388(2?
3):174?186.
Steven Pinker and Paul Bloom. 1990. Natural lan-
guage and natural selection. Behavioral and Brain
Sciences, 13:707?784.
Andrea Puglisi, Andrea Baronchelli, and Vittorio
Loreto. 2008. Cultural route to the emergence of
linguistic categories. Proceedings of the National
Academy of Sciences, 105(23):7936?7940.
Justin Quillinan. 2006. Social networks and cultural
transmission. Master of Science Thesis, School
of Philosophy, Psychology and Language Sciences,
University of Edinburgh, Edinburgh, Scotland, Aug.
Sean Geraint Roberts. 2012. An evolutionary ap-
proach to bilingualism. Ph.D. thesis, School of Phi-
losophy, Psychology and Language Sciences, Uni-
versity of Edinburgh, Edinburgh, Scotland, Oct.
George Gaylord Simpson. 1953. The Baldwin effect.
Evolution, 7(2):110?117.
Kenny Smith, Simon Kirby, and Henry Brighton.
2003. Iterated learning: A framework for the emer-
gence of language. Artificial Life, 9(4):371?386.
Michael Spranger. 2013. Evolving grounded spa-
tial language strategies. KI-K?unstliche Intelligenz,
27(2):1?10.
Luc Steels and Martin Loetzsch. 2012. The grounded
naming game. In L. Steels, editor, Experiments in
Cultural Language Evolution, pages 41?59. John
Benjamins.
Luc Steels and Michael Spranger. 2009. How ex-
perience of the body shapes language about space.
In Proceedings of the 21st International Joint Con-
ference on Artificial Intelligence, pages 14?19,
Pasadena, California, Jul. IJCAI.
Luc Steels. 1995. A self-organizing spatial vocabulary.
Artificial Life, 2(3):319?332.
Luc Steels. 2011. Modeling the cultural evolution of
language. Physics of Life Reviews, 8(4):339?356.
Ian Tattersall. 2010. Human evolution and cognition.
Theory in Biosciences, 129(2?3):193?201.
Remi van Trijp. 2012. The evolution of case systems
for marking event structure. In L. Steels, editor,
Experiments in Cultural Language Evolution, pages
169?205. John Benjamins.
Paul Vogt. 2007. Group size effects on the emer-
gence of compositional structures in language. In
F. Almeida e Costa, L.M. Rocha, E. Costa, I Har-
vey, and A. Coutinho, editors, Advances in Artifi-
cial Life: Proceedings of the 9th European Confer-
ence (ECAL 2007), pages 405?414, Lisbon, Portu-
gal, Sep. Springer.
Yusuke Watanabe, Reiji Suzuki, and Takaya Arita.
2008. Language evolution and the Baldwin effect.
Artificial Life and Robotics, 12(1-2):65?69.
54
