Pre-processing Closed Captions for Machine Translation 
Dav ide  Turcato  Fred Popowich  Paul  McFet r idge  
Dev lan  N icho lson  Jan ine  Too le  
Natural Language Laboratory, School of Computing Science, Simon Fraser University 
8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada 
and 
gavagai Technology Inc. 
P.O. 374, 3495 Cambie Street, Vancouver, British Columbia, V5Z 4R3, Canada 
{turk, popowich, mcfet, devl an, toole}?cs, sfu. ca 
Abst rac t  
We describe an approach to Machine Transla- 
tion of transcribed speech, as found in closed 
captions. We discuss how the colloquial nature 
and input format peculiarities of closed captions 
are dealt with in a pre-processing pipeline that 
prepares the input for effective processing by 
a core MT system. In particular, we describe 
components for proper name recognition and 
input segmentation. We evaluate the contribu- 
tion of such modules to the system performance. 
The described methods have been implemented 
on an MT system for translating English closed 
captions to Spanish and Portuguese. 
1 In t roduct ion  
Machine Translation (MT) technology can be 
embedded in a device to perform real time 
translation of closed captions included in TV 
signals. While speed is one factor associated 
with the construction of such a device, another 
factor is the language type and format. The 
challenges posed by closed captions to MT can 
be attributed to three distinct characteristics: 
Firstly, closed captions are transcribed 
speech. Although closed captions are not a com- 
pletely faithful transcription of TV programs, 
they render spoken language and therefore the 
language used is typically colloquial (Nyberg 
and Mitamura, 1997). They contain many of 
the phenomena which characterize spoken lan- 
guage: interjections, repetitions, tuttering, el- 
lipsis, interruptions, hesitations. Linguistically 
and stylistically they differ from written lan- 
guage: sentences are shorter and poorly struc- 
tured, and contain idiomatic expressions, un- 
grammaticality, etc. The associated ifficulties 
stem from the inherently colloquial nature of 
closed captions, and, to different degrees, of 
all forms of transcribed speech (Hindle, 1983). 
Such difficulties require a different approach 
than is taken for written documents. 
Secondly, closed captions come in a specific 
format, which poses problems for their optimal 
processing. Closed-captioners may often split 
a single utterance between two screens, if the 
character limit for a screen has been exceeded. 
The split is based on consideration about string 
length, rather than linguistic considerations, 
hence it can happen at non-constituent bound- 
aries (see Table 1), thus making the real time 
processing of the separate segments problem- 
atic. Another problem is that captions have no 
upper/lower case distinction. This poses chal- 
lenges for proper name recognition since names 
cannot be identified by an initial capital. Addi- 
tionally, we cannot rely on the initial uppercase 
letter to identify a sentence initial word. This 
problematic aspect sets the domain of closed 
captions apart from most text-to-text MT do- 
mains, making it more akin, in this respect, to 
speech translation systems. Although, from a 
technical point of view, such input format char- 
acteristics could be amended, most likely they 
are not under a developer's control, hence they 
have to be presumed. 
Thirdly, closed captions are used under oper- 
ational constraints. Users have no control over 
the speed of the image or caption flow so (s)he 
must comprehend the caption in the limited 
time that the caption appears on the screen. 
Accordingly, the translation of closed captions 
is a "time-constrained" application, where the 
user has limited time to comprehend the system 
output. Hence, an MT system should produce 
translations comprehensible within the limited 
time available to the viewer. 
In this paper we focus on the first two fac- 
tors, as the third has been discussed in (Toole 
et al, 1998). We discuss how such domain- 
38 
good evening, i'm jim lehrer. 
on the "newshour" tonight, four members of congress debate the 
u.n. deal with iraq; paul solman tells the troubled story of 
indonesia's currency; mark 
shields and paul gigot analyze the political week; 
and elizabeth farnsworth explains how the universe is getting 
larger. 
Table 1: Closed caption script fragment. 
dependent, problematic factors are dealt with 
in a pre-processing pipeline that prepares the 
input for processing by a core MT system. The 
described methods have been implemented for 
an MT system that translates English closed 
captions to Spanish and Portuguese. All the 
examples here refer to the Spanish module. 
2 P re -process ing  des ign  
Input pre-processing is essential in an embedded 
real time system, in order to simplify the core 
processing and make it both time- and memory- 
effective. In addition to this, we followed the 
guideline of separating domain-dependent pro- 
cesses and resources from general purpose ones. 
On the one hand, grammars and lexicons are 
costly resources. It would be desirable for them 
to be domain-independent a d portable across 
different domains, as well as declarative and 
bidirectional. On the other hand, a domain with 
distinctive characteristics requires ome specific 
treatment, if a system aims at robustness. We 
decided to have a domain independent core MT 
system, locating the domain dependent process- 
ing in a pipeline of low-level components, easy  
to implement, aiming at fast and robust pro- 
cessing and using limited linguistic knowledge. 
We use declarative and bidirectional gram- 
mars and lexicons. The lexicMist approach is 
indeed suitable to the closed caption domain, 
e.g. in terms of its capability of handling loosely 
structured or incomplete sentences. Also, the 
linguistic resources are geared towards this do- 
main in terms of grammatical nd lexical cover- 
age. However, our system architecture and for- 
malism make them equally usable in any other 
domain and translation direction, as the linguis- 
tic knowledge therein contained is valid in any 
domain. For the architecture we refer the reader 
to (Popowich et al, 1997). In the rest of this 
paper we focus on the pre-processing module 
39 
and how it deals with the issues discussed in 
the introduction. 
The task of the pre-processing pipeline is to 
make the input amenable to a linguistically- 
principled, domain independent reatment. 
This task is accomplished in two ways: 
1. By normalizing the input, i.e. removing 
noise, reducing the input to standard typo- 
graphical conventions, and also restructur- 
ing and simplifying it, whenever this can be 
done in a reliable, meaning-preserving way. 
2. By annotating the input with linguistic in- 
formation, whenever this can be reliably 
done with a shallow linguistic analysis, to 
reduce input ambiguity and make a full lin- 
guistic analysis more manageable. 
Figure (1) shows the system architecture, 
with a particular emphasis on the pre- 
processing pipeline. The next section describes 
the pipeline up to tagging. Proper name 
recognition and segmentation, which deal more 
specifically with the problems described in the 
introduction, are discussed in further sections. 
3 Normal i za t ion  and  tagg ing  
The label normalization groups three compo- 
nents, which clean up and tokenize the input. 
The text-level normalization module performs 
operations at the string level, such as remov- 
ing extraneous text and punctuation (e.g. curly 
brackets , used to mark off sound effects), or re- 
moving periods from abbreviations. E.g.: 
(I) "I went to high school in the u.s." 
"I went to high school in the usa." 
The tokenizer breaks a line into words. The 
token-level normalization recognizes and an- 
notates tokens belonging to special categories 
Pre-processing 
Normalization 
\[Text-level normalization \]
\[ Tokenization ) 
\[Token-level normalization I 
+ 
\[ Proper name recognition \] 
\[ Segmentation "') 
Core MT 
system 
Anal' ,sis \] 
+ 
i I Oo.o a, on I 
,1 
\ ]Pos t -process ing  ) 
Figure 1: System architecture. 
(times, numbers, etc.), expands contractions, 
recognizes, normalizes and annotates tutters 
(e.g. b-b-b-bright), identifies compound words 
and converts number words into digits. E.g.: 
(2) "I" "went" "to" "high" "school" 
"in" "the" "usa" " " 
"I" "went" "to" "high school" "in" 
"the" "usa" " " 
(3) "W-wh-wha~'s" "that" "?"0  
"what"/stutter "is" "that" "?" 
Note that annotations associated with tokens 
are carried along the entire translation process, 
so as to be used in producing the output (e.g. 
stutters are re-inserted in the output). 
The tagger assigns parts of speech to tokens. 
Part of speech information is used by the subse- 
quent pre-processing modules, and also in pars- 
ing, to prioritize the most likely lexical assign- 
ments of ambiguous items. 
4 P roper  name recogn i t ion  
Proper names are ubiquitous in closed captions 
(see Table 1). Their recognition is important 
for effective comprehension of closed captions, 
particularly in consideration of two facts: (i) 
users have little time to mentally rectify a mis- 
translation; (ii) a name can occur repeatedly 
in a program (e.g. a movie), with an annoy- 
ing effect if it is systematically mistranslated 
(e.g. a golf tournament where the golfer named 
Tiger Woods is systematically referred to as los 
bosques del tigre, lit. 'the woods of the tiger'). 
Name recognition is made harder in the closed 
caption domain by the fact that no capitaliza- 
tion information is given, thus making unusable 
all methods that rely on capitalization as the 
main way to identify candidates (Wolinski et al, 
1995) (Wacholder et al, 1997). For instance, an 
expression like 'mark sh ie lds ' ,  as occurs in Ta- 
ble (1), is problematic in the absence of capital- 
ization, as both 'mark' and ' sh ie lds '  are three- 
way ambiguous (proper name, common noun 
and verb). Note that this identical problem may 
be encountered if an MT system is embedded 
in a speech-to-speech translation as well. This 
situation forced us to explore different ways of 
identifying proper names. 
The goal of our recognizer is to identify 
proper names in a tagged line and annotate 
them accordingly, in order to override any 
other possiblelexical assignment in the follow- 
ing modules. The recognizer also overrides pre- 
vious tokenization, by possibly compounding 
two or more tokens into a single one, which 
will be treated as such thereafter. Besides part 
of speech, the only other information used by 
the recognizer is the lexical status of words, i.e. 
their ambiguity class (i.e. the range of possible 
syntactic ategories it can be assigned) or their 
status as an unknown word (i.e. a word that 
is not in the lexicon). The recognizer scans an 
input line from left to right, and tries to match 
40 
each item against a sequences of patterns. Each 
pattern expresses constraints (in terms of word, 
part of speech tag and lexical status) on the 
item under inspection and its left and right con- 
texts. Any number of items can be inspected to 
the left and right of the current item. Such pat- 
terns also make use of regular expression bpera- 
tors (conjunction, disjunction, negation, Kleene 
star). For instance (a simplified version of) a 
pattern might look like the following: 
(4) /the/DEW (NOUNIADJ)*\] X' \['NOUN\] 
where we adopt the convention of representing 
words by lowercase strings, part of speech tags 
by uppercase strings and variables by primed 
Xs. The left and right context are enclosed 
in square brackets, respectively to the left and 
right of the current item. They can also con- 
tain special markers for the beginning and end 
of a line, and for the left or right boundary of 
the proper name being identified. This way to- 
kenization can be overridden and separate to- 
kens joined into a single name. Constraints on 
the lexical status of items are expressed as pred- 
icates associated with pattern elements, e.g.: 
(5) proper_and_common (X') 
A pattern like the one above (4-5) would 
match a lexically ambiguous proper/common 
noun preceded by a determiner (with any num- 
ber of nouns or adjectives in between), and not 
followed by a noun (e.g. ' the b i l l  i s . . . ' ) .  Be- 
sides identifying proper names, some patterns 
may establish that a given item is not a name 
(as in the case above). A return value is as- 
sociated with each pattern, specifying whether 
the current match is or is not a proper name. 
Once a successful match occurs, no further pat- 
terns are tried. Patterns are ordered from more 
to less specific. At the bottom of the pattern 
sequence are the simplest patterns, e.g.: 
(6) ( \[\] X' \[\] ), proper_and_common(X') 
yes 
which is the default assignment for words like 
'b i l l '  if no other pattern matched. However 
(6) is overridden by more specific patterns like: 
(7) ( \[x''\] x' \[\] ) ,  
proper_and_common (X'), common(X") 
no 
41 
(s) ( \[x' \]  x' \[\] ) ,  
proper_and_common(X'), proper(X")  
yes 
The former pattern covers cases like 
' te lecommunicat ions  b i l l ' ,  preventing 
'b i l l '  from being interpreted as a proper 
name, the latter covers cases like 'damian 
b i l l ' ,  where 'b i l l '  is more likely to be a name. 
In general, the recognizer tries to disam- 
biguate lexically ambiguous nouns or to as- 
sign a category to unknown words on the ba- 
sis of the available context. However, in prin- 
ciple any word could be turned into a proper 
name. For instance, verbs or adjectives can 
be turned into proper names, when the con- 
text contains strong cues, like a title. Increas- 
ingly larger contexts provide evidence for more 
informed guesses, which override guesses based 
on narrower contexts. Consider the following 
examples that show how a word or expression 
is treated ifferently depending on the available 
context. Recognized names are in italics. 
(9) biZ~ ~ 
(i0) the bill is ... 
(11) the b i l l  clinton is . . .  
(12) the  b i l l  c l in ton  admin is t ra t ion  is  
The lexically ambiguous bill, interpreted as 
a proper name in isolation, becomes a common 
noun if preceded by a determiner. However, 
the interpretation reverts to proper name if an- 
other noun follows. Likewise the unknown word 
clinton is (incorrectly) interpreted as a com- 
mon noun in (11), as it is the last item of a 
noun phrase introduced by a determiner, but it 
becomes a proper name if another noun follows. 
We also use a name memory ,  which patterns 
have access to. As proper names are found in an 
input stream, they are added to the name mem- 
ory. A previous occurrence of a proper name is 
used as evidence in making decisions about fur- 
ther occurrences. The idea is to cache names 
occurred in an 'easy' context (e.g. a name pre- 
ceded by a title, which provides trong evidence 
for its status as a proper name), to use them 
later to make decisions in 'difficult' contexts, 
where the internal evidence would not be suffi- 
cient to support a proper name interpretation. 
Hence, what typically happens is that the same 
name in the same context is interpreted iffer- 
ently at different imes, if previously the name 
has occurred in an 'easy' context and has been 
memorized. E.g.: 
(13) the individual title went to tiger 
woods. 
mr. tiger woods struggled today 
with a final round 80. 
name-memory 
the short well publicized 
professional life of t iger  woods 
has been an open book. 
The name memory was designed to suit the 
peculiarity of closed captions. Typically, in this 
domain proper names have a low dispersion. 
They are concentrated in sections of an input 
stream (e.g. the name of the main characters 
in a movie), then disappear for long sections 
(e.g. after the movie is over). Therefore, a 
name memory needs to be reset to reflect such 
changes. However, it is problematic to decide 
when to reset the name memory. Even if it was 
possible to detect when a new program starts, 
one should take into account the possible sce- 
nario of an MT system embedded in a consumer 
product, in which case the user might unpre- 
dictably change channel at any time. In or- 
der to keep a name memory aligned with the 
current program, without any detection of pro- 
gram changes, we structured the name memory 
as a relatively short queue (first in, first out). 
Every time a new item is added to the end of 
the queue, the first item is removed and all the 
other items are shifted. Moreover, we do not 
check whether a name is already in the mem- 
ory. Every time a suitable item is found, we 
add it to the memory, regardless of whether it 
is already there. Hence, the same item could 
be present wice or more in the memory at any 
given time. The result of this arrangement is 
that a name only remains in the memory :for a 
relatively short time. It can only remain :\[or a 
longer time if it keeps reappearing frequently in 
the input stream (as typically happens), other- 
wise it is removed shortly after it stopped ap- 
pearing. In this way, the name memory is kept 
42 
# of items 
Proper names correctly identified 
False positives 
False negatives 
152 
8 
57 
Table 2: Name recognition evaluation results. 
aligned with the current program, with only a 
short transition period, during which names no 
longer pertinent are still present in the memory, 
before getting replaced by pertinent ones. 
The recognizer currently contains 63 pat- 
terns. We tested the recognizer on a sample of 
1000 lines (5 randomly chosen continuous frag- 
ments of 200 lines each). The results, shown in 
table (2), illustrate a recall of 72.7% and a pre- 
cision of 95.0%. These results reflect our cau- 
tious approach to name recognition. Since the 
core MT system has its own means of identify- 
ing some proper names (either in the lexicon or 
via default assignments o unknown words) we 
aimed at recognizing names in pre-processing 
only when this could be done reliably. Note 
also that 6 out of the 8 false positives were iso- 
lated interjections that would be better left un- 
translated (e.g. p f foo ,  e l  smacko), or closed 
captioner's typos (e.g. yo4swear).  
5 Segmentation 
Segmentation breaks a line into one or more 
segments, which are passed separately to sub- 
sequent modules (Ejerhed, 1996) (Beeferman et 
al., 1997). In translation, segmentation is ap- 
plied to split a line into a sequence of transla- 
tionally self-contained units (Lavie et al, 1996). 
In our system, the translation units we iden- 
tify are syntactic units, motivated by cross- 
linguistic considerations. Each unit is a con- 
stituent that dan be translated independently. 
Its translation is insensitive to the context in 
which the unit occurs, and the order of the units 
is preserved by translation. 
One motivation for segmenting is that pro- 
cessing is faster: syntactic ambiguity is reduced, 
and backtracking from a module to a previ- 
ous one does not involve re-processing an en- 
tire line, but only the segment hat failed. A 
second motivation is robustness: a failure in 
one segment does not involve a failure in the 
entire line, and error-recovery can be limited 
only to a segment. Further motivations are pro- 
vided by the colloquial nature of closed cap- 
tions. A line often contains fragments with a 
loose syntactic relation to each other and to the 
main clause: vocatives, false starts, tag ques- 
tions, etc. These are most easily translated as 
individual segments. Parenthetical expressions 
are often also found in the middle of a main 
clause, thus making complete parses problem- 
atic. However, the solution involves a heavier 
intervention than just segmenting. Dealing with 
parentheticals requires restructuring a line, and 
reducing it to a 'normal' form which ideally al- 
ways has parenthetical expressions at one end of 
a sentence (under the empirical assumption that 
the overall meaning is not affected). We will 
see how this kind of problem is handled in seg- 
mentation. A third motivation is given by the 
format of closed captions, with input lines split 
across non-constituent boundaries. One solu- 
tion would be delaying translation until a sen- 
tence boundary is found, and restructuring the 
stored lines in a linguistically principled way. 
However, the requirements of real time transla- 
tion (either because of real time captioning at 
the source, or because the MT system is embed- 
ded in a consumer product), together with the 
requirement that translations be aligned with 
the source text and, above all, with the images, 
makes this solution problematic. The solution 
we are left with, if we want lines to be bro- 
ken along constituent boundaries, is to further 
segment a sentence, even at the cost of some- 
times separating elements that should go to- 
gether for an optimal translation. We also ar- 
gued elsewhere (Toole et al, 1998) that in a 
time-constrained application the output gram- 
maticality is of paramount importance, even at 
the cost of a complete meaning equivalence with 
the source. For this reason, we also simplify 
likely problematic input, when a simplification 
is possible without affecting the core meaning. 
To sum up, the task at hand is broader than 
just segmentation: re-ordering of constituents 
and removal of words are also required, to syn- 
tactically 'normalize' the input. As with name 
recognition, we aim at using efficient and easy 
to implement techniques, relying on limited lin- 
guistic information. The segmenter works by 
matching input lines against a set of templates 
represented by pushdown transducers. Each 
transducer is specified in a fairly standard way 
(Gazdar and Mellish, 1989, 82), by defining an 
initial state, a final state, and a set of transitions 
of the following form: 
(14) (State I, State2, Label, Transducer> 
Such a transition specifies that Transducer 
can move from Statel to State2 when the in- 
put specified by Label is found. Label can be 
either a pair (InputSymbol, OutputSymbol) or 
the name of another transducer, which needs 
to be entirely traversed for the transition from 
State l  to State2 to take place. An input sym- 
bol is a <Word, Tag> pair. An output symbol 
is an integer anging from 0 to 3, specifying to 
which of two output segments an input sym- 
bol is assigned (0 = neither segment, 3 = both 
segments, 1 and 2 to be interpreted in the ob- 
vious way). The output codes are then used to 
perform the actual split of a line. A successful 
match splits a line into two segments at most. 
However, on a successful split, the resulting seg- 
ments are recursively fed to the segmenter, until 
no match is found. Therefore, there is no limit 
to the number of segments obtained from an 
input line. The segmenter currently contains 
37 top-level transducers, i.e. segmenting pat- 
terns. Not all of them are used at the same time. 
The implementation of patterns is straightfor- 
ward and the segmenter can be easily adapted 
to different domains, by implementing specific 
patterns and excluding others. For instance, a 
very simple patterns plit a line at every comma, 
a slightly more sophisticated one, splits a line at 
every comma, unless tagged as a coordination; 
other patterns plit a final adverb, interjection, 
prepositional phrase, etc. 
Note that a segment can be a discontinuous 
part of a line, as the same output code can be 
assigned to non-contiguous elements. This fea- 
ture is used, e.g., in restructuring a sentence, as 
when a parenthetical expression is encountered. 
Thefollowing example shows an input sentence, 
an assignment, and a resulting segmentation. 
(15) this, however, is a political 
science course. 
(16) this/2 ,/0 however/l ,/i is/2 a/2 
political/2 science/2 course/2. 
(17) I. however , 
43 
2. this is a po l i t ica l  sc ience 
course 
We sometimes use the segmenter's ability to 
simplify the input, e.g. with adverbs like just, 
which are polysemous and difficult to translate, 
but seldom contribute to the core meaning of a 
sentence. 
6 Per fo rmance  
We ran a test to evaluate how the recognizer 
and segmenter affected the quality of transla- 
tions. We selected asample of 200 lines of closed 
captioning, comprising four continuous sections 
of 50 lines each. The sample was run through 
the MT system twice, once with the recognizer 
and segmenter activated and once without. The 
results were evaluated by two native Spanish 
speakers. We adopted a very simple evalua- 
tion measure, asking the subjects to tell whether 
one translation was better than the other. The 
translations differed for 32 input lines out of 200 
(16%). Table (3) shows the evaluation results, 
with input lines as the unit of measurement. 
The third column shows the intersection of the 
two evaluations, i.e. the evaluations on which 
the two subjects agreed. The three rows show 
how often the translation was better (i) with 
pre-processing, (ii) without pre-processing, or 
(iii) no difference could be appreciated. 
The results show a discrepancy in the evalu- 
ations. One evaluator also pointed out that it 
is hard to make sense of transcribed closed cap- 
tions, without the audio-visual context. These 
two facts seem to point out that an appropri- 
ate evaluation should be done in the operational 
context in which closed captions are normally 
used. Still, the intersection of the subjects' eval- 
uations shows that pre-processing improves the 
output quality. In three of the four cases where 
the two evaluators agreed that pre-processing 
yielded a worse result, the worse performance 
was due to an incorrect name recognition oi" seg- 
mentation. However, in two of the three cases, 
the original problem was an incorrect agging. 
Note that even when the name recognizer 
and segmenter are off, the system can identify 
some names, and recover from translation fail- 
ures by piecing together translations of frag- 
ments. Therefore, what was being tested was 
not so much name recognition and segmenting 
44 
per se, but the idea of having separate modules 
for such tasks in the system front end. 
Finally, the test did not take into account 
speed, as we set higher time thresholds than 
an embedded application would require. Since 
segmentation reduces processing time, it is also 
expected to reduce the impact of tighter time 
thresholds, all other things being equal. 
We are planning to conduct an operational 
evaluation of the system. The goal is to evalu- 
ate the system output in its proper visual con- 
text, and compare the results with parallel re- 
sults for human translated closed captions. Dif- 
ferent groups of participants will watch a video 
With either human- or machine-translated sub- 
titles, and complete a questionnaire based on 
the subtitles in the video. The questionnaire 
will contain a set of questions to elicit the sub- 
ject's assessment on the translation quality, and 
a set of questions to assess the subject's level of 
comprehension f the program. 
7 Conc lus ion  
It is apparent hat the peculiarity of closed 
captions, both in terms of transcribed speech 
characteristic and constraints due to the input 
format, require an ad hoc treatment, consider- 
ably different from the approaches suitable for 
written documents. Yet the knowledge about 
a language (or the bilingual knowledge about 
a language-pair) is largely invariant across dif- 
ferent applications domains and should there- 
fore be portable from one application domain 
to another. The architecture we have proposed 
strives to combine the need for domain indepen- 
dent linguistic resources and linguistically prin- 
cipled methods with the need for robust MT 
systems tuned to real world, noisy and idiosyn- 
cratic input, as encountered when embedding 
MT in real woi:ld devices. 
In terms of adequacy, a standard evaluation 
and a comparison among different MT systems 
frtom different domains is hard, as the ade- 
quacy of a system depends on its application 
(Church and Hovy, 1993). This is even truer 
with-closed captions, where the use of transla- 
tion output is heavily influenced by operational 
constraints (time constraints, the presence of 
images, sound, etc.). In some cases such con- 
straints may place a heavier burden on a system 
(e.g. the time constraint), in some other cases 
Judge 1 Judge 2 Both agreed 
Better with pre~processing 
Better without pre-processing 
No difference 
21 16 15 
4 12 4 
7 4 3 
Table 3: Evaluation results. 
they can make an imperfect ranslation accept- 
able (e.g. the presence of images and sounds). 
We did not attempt an assessment in absolute 
terms, which we believe should take into ac- 
count the operational environment and involve 
real-world users. More modestly, we aimed at 
showing that our pre-processing techniques pro- 
vide an improvement in performance. 
Our work on closed captions also shows that 
the challenges coming from this domain, even 
in terms on low-level issues of input format, can 
lead to interesting developments of new linguis- 
tic techniques. We believe that our solutions to 
specific problems (namely, proper name recog- 
nition and segmentation) in the closed caption 
domain bear relevance to a wider context, and 
offer techniques that can be usefully employed 
in a wider range of applications. 
Re ferences  
Doug Beeferman, Adam Berger, and John Laf- 
ferty. 1997. Text segmentation using expo- 
nential models. In Proceedings of the Second 
Conference on Empirical Methods in Natu- 
ral Language Processing (EMNLP-2), Prov- 
idence, USA. 
Kenneth W. Church and Eduard H. Hovy. 
1993. Good applications for crummy machine 
translation. Machine Translation, 8:239-258. 
Eva Ejerhed. 1996. Finite state segmentation 
of discourse into clauses. In A. Kornai, ed- 
itor, Proceedings of the ECAI-96 Workshop 
Extended Finite State Models of Language, 
Budapest,Hungary. 
Gerald Gazdar and Christopher S. Mellish. 
1989. Natural Language Processing in PRO- 
LOG: an Introduction to Computational Lin- 
guistics. Addison-Wesley Publishing Com- 
pany, Wokingham, England. 
Donald Hindle. 1983. Deterministic parsing of 
syntactic non-fluencies. In Proceedings ofthe 
21st Annual Meeting of the Association for 
Computational Linguistics (ACL-83), pages 
123-128, Cambridge, Massachusetts, USA. 
Alon Lavie, Donna Gates, Noah Coccaro, and 
Lori Levin. 1996. Input segmentation of 
spontaneous peech in janus: a speech- 
to-speech translation system. In Proceed- 
ings of ECAI-96 Workshop on Dialogue Pro- 
cessing in Spoken Language Systems, Bu- 
dapest,Hungary. 
Eric Nyberg and Teruko Mitamura. 1997. A 
real-time MT system for translating broad- 
cast captions. In Proceedings ofthe Sixth Ma- 
chine Translation Summit, pages 51-57, San 
Diego, California, USA. 
Fred Popowich, Davide Turcato, Olivier Lau- 
rens, Paul McFetridge, J. Devlan Nicholson, 
Patrick McGivern, Maricela Corzo-Pena, Lisa 
Pidruchney, and Scott MacDonald. 1997. A 
lexicalist approach to the translation of collo- 
quial text. In Proceedings ofthe 7th Interna- 
tional Conference on Theoretical nd Method- 
ological Issues in Machine Translation, pages 
76-86, Santa Fe, New Mexico, USA. 
Janine Toole, Davide Turcato, Fred Popowich, 
Dan Fass, and Paul McFetridge. 1998. Time- 
constrained Machine Translation. In Proceed- 
ings of the Third Conference of the Associa- 
tion for Machine Translation in the Ameri- 
cas (AMTA-98), pages 103-112, Langhorne, 
Pennsylvania, USA. 
Nina Wacholder, Yael Ravin, and Misook Choi. 
? 1997. Disambiguation of proper names in 
texts. In Proceedings of the Fifth Confer- 
ence on Applied Natural Language Processing 
(ANLP-97), pages 202-208, Washington, DC, 
USA. Association for Computational Linguis- 
tics. 
Francis Wolinski, Frantz Vichot, and Bruno Dil- 
let. 1995. Automatic processing of proper 
names in texts. In Proceedings of the 7th 
Conference of the European Chapter of the 
Asscociation for Computational Linguistics 
(EACL-95), pages 23-30, Dublin, Ireland. 
45 
Adapt ing a synonym database to specif ic domains 
Dav ide  Turcato  Fred Popowich  Jan ine  Toole  
Dan Pass Dev lan  N icho lson  Gordon T i sher  
gavagai Technology Inc. 
P.O. 374, 3495 Ca~abie Street, Vancouver, British Columbia, V5Z 4R3, Canada 
and 
Natural Language Laboratory, School of Computing Science, Simon Fraser University 
8888 University Drive, Burnaby, British Columbia, V5A 1S6, Canada 
{turk, popowich ,toole, lass, devl an, gt i sher}@{gavagai, net, as, sfu. ca} 
Abst ract  
This paper describes a method for 
adapting ageneral purpose synonym 
database, like WordNet, to a spe- 
cific domain, where only a sub- 
set of the synonymy relations de- 
fined in the general database hold. 
The method adopts an eliminative 
approach, based on incrementally 
pruning the original database. The 
method is based on a preliminary 
manual pruning phase and an algo- 
rithm for automatically pruning the 
database. This method has been im- 
plemented and used for an Informa- 
tion Retrieval system in the aviation 
domain. 
1 In t roduct ion  
Synonyms can be an important resource for 
Information Retrieval (IR) applications, and 
attempts have been made at using them to 
expand query terms (Voorhees, 1998). In 
expanding query terms, overgeneration is as 
much of a problem as incompleteness or lack 
of synonym resources. Precision can dramat- 
ically drop because of false hits due to in- 
correct synonymy relations. This problem is 
particularly felt when IR is applied to docu- 
ments in specific technical domains. In such 
cases, the synonymy relations that hold in the 
specific domain are only a restricted portion 
of the synonymy relations holding for a given 
language at large. For instance, a set of syn- 
onyms like 
(1) {cocaine, cocain, coke, snow, C} 
valid for English, would be detrimental in a 
specific domain like weather eports, where 
both snow and C (for Celsius) occur very fre- 
quently, but never as synonyms of each other. 
We describe a method for creating a do- 
main specific synonym database from a gen- 
eral purpose one. We use WordNet (Fell- 
baum, 1998) as our initial database, and we 
draw evidence from a domain specific corpus 
about what synonymy relations hold in the 
domain. 
Our task has obvious relations to word 
sense disambiguation (Sanderson, 1997) (Lea- 
cock et al, 1998), since both tasks are based 
on identifying senses of ambiguous words in 
a text. However, the two tasks are quite dis- 
tinct. In word sense disambiguation, a set of 
candidate senses for a given word is checked 
against each occurrence of the relevant word 
in a text, and a single candidate sense is se- 
lected for each occurrence ofthe word. In our 
synonym specialization task a set of candidate 
senses for a given word is checked against an 
entire corpus, and a subset of candidate senses 
is selected. Although the latter task could be 
reduced to the former (by disambiguating all
occurrences of a word in a test and taking 
the union of the selected senses), alternative 
approaches could also be used. In a specific 
domain, where words can be expected to be 
monosemous to a large extent, synonym prun- 
ing can be an effective alternative (or a com- 
plement) to word sense disambiguation. 
From a different perspective, our 
task is also related to the task of as- 
signing Subject Field Codes (SFC) to 
a terminological resource, as done by 
Magnini and Cavagli~ (2000) for WordNet. 
Assuming that a specific domain corresponds 
to a single SFC (or a restricted set of SFCs, 
at most), the difference between SFC as- 
signment and our task is that the former 
assigns one of many possible values to a given 
synset (one of all possible SFCs), while the 
latter assigns one of two possible values (the 
words belongs or does not belong to the SFC 
representing the domain). In other words, 
SFC assignment is a classification task, while 
ours can be seen as either a filtering or 
ranking task. 
Adopting a filtering/ranking perspective 
makes apparent hat the synonym pruning 
task can also be seen as an eliminative pro- 
cess, and as such it can be performed incre- 
mentally. In the following section we will 
show how such characteristics have been ex- 
ploited in performing the task. 
In section 2 we describe the pruning 
methodology, while section 3 provides a prac- 
tical example from a specific domain. Con- 
clusions are offered in section 4. 
2 Methodo logy  
2.1 Out l ine  
The synonym pruning task aims at improv- 
ing both the accuracy and the speed of a syn- 
onym database. In order to set the terms of 
the problem, we find it useful to partition the 
set of synonymy relations defined in WordNet 
into three classes: 
. Relations irrelevant to the specific do- 
main (e.g. relations involving words that 
seldom or never appear in the specific do- 
main) 
. Relations that are relevant but incorrect 
in the specific domain (e.g. the syn- 
onymy of two words that do appear in the 
specific domain, but are only synonyms 
in a sense irrelevant o the specific do- 
main); 
3. Relations that are relevant and correct in 
the specific domain. 
The creation of a domain specific database 
aims at removing relations in the first two 
classes (to improve speed and accuracy, re- 
spectively) and including only relations in the 
third class. 
The overall goal of the described method 
is to inspect all synonymy relations in Word- 
Net and classify each of them into one of the 
three aforementioned classes. We define a 
synonymy relation as a binary relation be- 
tween two synonym terms (with respect to 
? a particular sense). Therefore, a WordNet 
synset containing n terms defines ~11 k syn- 
onym relations. The assignment of a syn- 
onymy relation to a class is based on evidence 
drawn from a domain specific corpus. We use 
a tagged and lemmatized corpus for this pur- 
pose. Accordingly, all frequencies used in the 
rest of the paper are to be intended as fre- 
quencies of ( lemma, tag) pairs. 
The pruning process is carried out in three 
steps: (i) manual pruning; (ii) automatic 
pruning; (iii) optimization. The first two 
steps focus on incrementally eliminating in- 
correct synonyms, while the third step focuses 
on removing irrelevant synonyms. The three 
steps are described in the following sections. 
2.2 Manua l  p run ing  
Different synonymy relations have a different 
impact on the behavior of the application in 
which they are used, depending on how fre- 
quently each synonymy relation is used. Rela- 
tions involving words frequently appearing in 
either queries or corpora have a much higher 
impact (either positive or negative) than re- 
lations involving rarely occurring words. E.g. 
the synonymy between snow and C has a 
higher impact on the weather eport domain 
(or the aviation domain, discussed in this pa- 
per) than the synonymy relation between co- 
caine and coke. Consequently, the precision of 
a synonym database obviously depends much 
more on frequently used relations than on 
rarely used ones. Another important consid- 
eration is that judging the  correctness of a 
given synonymy relation in a given domain is 
often an elusive issue: besides clearcut cases, 
there is a large gray area where judgments 
may not be trivial even for humans evalua- 
tots. E.g. given the following three senses of 
the noun approach 
(2) a. {approach, approach path, glide 
path, glide slope} 
(the final path followed by an air- 
craft as it is landing) 
b. {approach, approach shot} 
(a relatively short golf shot in- 
tended to put the ball onto the 
putting green) 
c. {access, approach} 
(a way of entering or leaving) 
it would be easy to judge the first and second 
senses respectively relevant and irrelevant o 
the aviation domain, but the evaluation of the 
third sense would be fuzzier. 
The combination of the two remarks above 
induced us to consider a manual pruning 
phase for the terms of highest 'weight' as a 
good investment of human effort, in terms of 
rate between the achieved increase in preci- 
sion and the amount of work involved. A 
second reason for performing an initial man- 
ual pruning is that its outcome can be used 
as a reliable test set against which automatic 
pruning algorithms can be tested. 
Based on such considerations, we included a 
manual phase in the pruning process, consist- 
ing of two steps: (i) the ranking of synonymy 
relations in terms of their weight in the spe- 
cific domain; (ii) the actual evaluation of the 
correctness of the top ranking synonymy re- 
lation, by human evaluators. 
2.2.1 Rank ing  of  synonymy re lat ions  
The goal of ranking synonymy relations is 
to associate them with a score that estimates 
how often a synonymy relation is likely to 
be used in the specific domain. The input 
database is sorted by the assigned scores, and 
the top ranking words are checked for manual 
pruning. Only terms appearing in the domain 
specific corpus are considered at this stage. 
In this way the benefit of manual pruning is 
maximized. Ranking is based on three sorting 
criteria, listed below in order of priority. 
Cr i te r ion  1. Since a term that does ap- 
pear in the domain corpus must have at least 
one valid sense in the specific domain, words 
with only one sense are not good candidates 
for pruning (under the assumption of com- 
pleteness of the synonym database). There- 
fore .polysemous terms are prioritized over 
monosemous terms. 
Cr i te r ion  2. The second and third sort- 
ing criteria axe similar, the only difference be- 
ing that the second criterion assumes the ex- 
istence of some inventory of relevant queries 
(a term list, a collection of previous queries, 
etc.), ff such an inventory is not available, the 
second sorting criterion can be omitted. If the 
inventory is available, it is used to check which 
synonymy relations are actually to be used in 
queries to the domain corpus. Given a pair 
(ti,tj) of synonym terms, a score (which we 
name scoreCQ) is assigned to their synonymy 
relation, according to the following formula: 
(3) scoreCQij = 
(fcorpusi * fqueryj) + 
(fcorpusj ? fqueryi) 
where fcorpusn and fqueryn are, respec- 
tively, the frequencies of a term in the domain 
corpus and in the inventory of query terms. 
The above formula aims at estimating how 
often a given synonymy relation is likely to 
be actually used. In particular, each half of 
the formula estimates how often a given term 
in the corpus is likely to be matched as a syn- 
onym of a given term in a query. Consider, 
e.g., the following situation (taken form the 
aviation domain discussed in section 3.1): 
(4) fcorpuSsnow = 3042 
f querysnow = 2 
fcorpusc = 9168 
f queryc = 0 
It is estimated that C would be matched 
18336 times as a synonym for snow (i.e 9168 
* 2), while snow would never be matched as 
a synonym for C, because C never occurs as 
a query term. Therefore scoreCQs,~ow,c is 
18336 (i.e. 18336 + 0). 
Then, for each polysemous term i and 
synset s such that i E s, the following score is 
computed: 
Table 1: Frequencies of sample synset erms. 
j fcorpusj fqueryj 
cocaine 1 0 
cocain 0 0 
coke 8 0 
C 9168 0 
(5) scorePolyCQ i,~ = 
E{scoreCQi,~lj ~ s A i ? j} 
E.g., i f  ,5' is the synset in (1), then 
scorePolyCQs~ow,s is "the sum of 
scoreCQsnow,coc~ine, scoreCQsnow,eocain, 
scoreCQsnow,eoke and scoreCQ,no~o,c. Given 
the data in Table 1 (taken again from our 
aviation domain) the following scoreCQ 
would result: 
(6) scoreCQsnow,cocaine -~2 
scoreCQsnow,cocain = 0 
scoreCQs~ow,cok~ = 16 
scoreCQsno~o,c = 18336 
Therefore, scorePolyCQsnow,s would equal 
18354. 
The final score assigned to each polysemous 
term tl is the highest scorePolyCQi,s. For 
snow, which has the following three senses 
(7) a. {cocaine, cocaine, coke, C, snow} 
(a narcotic (alkaloid) extracted 
from coca leaves) 
b. {snow} 
(a layer of snowflakes (white crys- 
tals of frozen water) covering the 
ground) 
c. {snow, snowfall} 
(precipitation falling from clouds 
in the form of ice crystals) 
the highest score would be the one computed 
above. 
Cr i ter ion 3. The third criterion assigns 
a score in terms of domain corpus frequency 
alone. It is used to further rank terms that 
do not occur in the query term inventory (or 
when no query term inventory is available). It 
is computed in the same way as the previous 
score, with the only difference that a value of 
1 is conventionally assumed for fquery (the 
frequency of a term in the inventory of query 
terms). 
2.2.2 Correctness  evaluat ion 
All the synsets containing the top rank- 
ing terms, according to the hierarchy of crite- 
ria described above, are manuMly checked for 
pruning. For each term, all the synsets con- 
taining the term are clustered together and 
presented to a human operator, who exam- 
ines each (term, synset) pair and answers the 
question: does the term belong to the synset 
in the specific domain? Evidence about the 
answer is drawn from relevant examples auto- 
matically extracted from the domain specific 
corpus. E.g., following up on our example in 
the previous section, the operator would be 
presented with the word snow associated with 
each of the synsets in (7) and would have to 
provide a yes/no answer for each of them. In 
the specific case, the answer would be likely 
to be 'no' for (7a) and 'yes' for (75) and (7c). 
The evaluator is presented with all the 
synsets involving a relevant term (even 
those that did not rank high in terms of 
scorePoIyCQ) in order to apply a contrastive 
approach. It might well be the case that the 
correct sense for a given term is one for which 
the term has no synonyms at all (e.g. 7b in 
the example), therefore all synsets for a given 
term need to be presented to the evaiuator 
in order to make an informed choice. The 
evaluator provides a yes/no answer for all the 
(term, synset) he/she is presented with (with 
some exceptions, as explained in section 3.1). 
2.3 Automat ic  p run ing  
The automatic pruning task is analogous to 
manual pruning in two respects: (i) its in- 
put is the set of synonymy relations involving 
WordNet polysemous words appearing in the 
domain specific orpus; (ii) it is performed by 
examining all (term, synset) input pairs and 
answering the question: does the term belong 
to the synset in the specific domain? How- 
ever, while the manual pruning task was re- 
garded as a filtering task, where a human eval- 
4 
uator assigns a boolean value to each pruning 
candidate, the automatic pruning task can 
be more conveniently regarded as a ranking 
task, where all the pruning candidates are as- 
signed a score, measuring how appropriate a 
given sense is for a given word, in the do- 
main at hand. The actual pruning is left as 
a subsequent step. Different pruning thresh- 
olds can be applied to the ranked list, based 
on different considerations (e.g. depending on 
whether astronger emphasis i  put on the pre- 
cision or the recall of the resulting database). 
The score is based on the frequencies of both 
words in the synset (except the word under 
consideration) and words in the sense gloss. 
We also remove from the gloss all words be- 
longing to a stoplist (a stoplist provided with 
WordNet was used for this purpose). The fol- 
lowing scoring formula is used: 
(8) (average_synset_frequeney/ 
synset_cardinality k) .4- 
(average_gloss_frequency~ 
gloss_cardinality :) 
Note that the synset cardinality does not 
include the word under consideration, reflect- 
ing the fact the word's frequency is not used 
in calculating the score. Therefore a synset 
only containing the word under consideration 
and no synonyms is assigned cardinality 0. 
The goal is to identify (term, sense) pairs 
not pertaining to the domain. For this rea- 
son we tend to assign high scores to candi- 
dates for which we do not have enough evi- 
dence about their inappropriateness. This is 
why average frequencies are divided by some 
factor which is function of the number of av- 
eraged frequencies, in order to increase the 
Scores based on little evidence (i.e. fewer av- 
eraged numbers). In the sample application 
described in section 3 the value of k was set 
to 2. For analogous reasons, we convention- 
ally assign a very high score to candidates for 
which we have no evidence (i.e. no words in 
both the synset and the gloss). If either the 
synset or the gloss is empty, we conventionally 
double the score for the gloss or the synset, 
respectively. We note at this point that our 
final ranking list are sorted in reverse order 
with respect o the assigned scores, since we 
are focusing on removing incorrect items. At 
the top of the list are the items that receive 
the lowest score, i.e. that are more likely to 
be incorrect (term, sense) associations for our 
domain (thus being the best candidates to be 
pruned out). 
Table 2 shows the ranking of the senses 
for the word C in the aviation domain. In 
the table, each term is followed by its corpus 
frequency, separated by a slash. From each 
synset the word C itself has been removed, 
as well as the gloss words found in the stop 
list. Therefore, the table only contains the 
words that contribute to the calculation of the 
sense's core. E.g. the score for the first sense 
in the list is obtained from the following ex- 
pression: 
(9) ((0 + 57)/2/22) + 
( (8+0+0+ 198+9559+0+1298)/7/72 ) 
The third sense in the list exemplifies the 
case of an empty synset (i.e. a synset orig- 
inally containing only the word under con- 
sideration). In this case the score obtained 
from the gloss is doubled. Note that the ob- 
viously incorrect sense of C as a narcotic is 
in the middle of the list. This is due to a tag- 
ging problem, as the word leaves in the gloss 
was tagged as verb instead of noun. Therefore 
it was assigned a very high frequency, as the 
verb leave, unlike the noun leaf, is very com- 
mon in the aviation domain. The last sense 
in the list also requires a brief explanation. 
The original word in the gloss was 10S. How- 
ever, the pre-processor that was used before 
tagging the glosses recognized S as an abbre- 
viation for South and expanded the term ac- 
cordingly. It so happens that both words 10 
and South are very frequent in the aviation 
corpus we used, therefore the sense was as- 
signed a high score. 
2.4 Optimization 
The aim of this phase is to improve the access 
speed to the synonym database, by removing 
all information that is not likely to be used. 
The main idea is to minimize the size of the 
Score 
Table 2: Ranking of synsets containing the word C 
Frequencies 
39.37 
62.75 
224.28 
synset: 
gloss: 
synset: 
gloss: 
synset: 
gloss: 
241.69 synset: 
gloss: 
585.17 synset: 
gloss: 
743.28 synset: 
gloss: 
1053.43 synset: 
gloss: 
ATOMIC_NUMBEK_6/O, CAKBON/57 
ABUNDANT/8, NONMETALLIC/O, TETRAVALENT/O, ELEMENT/198 
0CCUR/9559, ALLOTROPIC/O, FOKM/1298 
AMPEre-SECOND/O, COULOMB/O 
UNIT/3378, ELECTRICAL/2373, CHARGE/523, EQUAL/153 
AMOUNT/1634, CHARGE/523, TKANSFEK/480, CUKKENT/242, 1/37106 
AMPEre/4, 1/37106 
0 
GENEKAL-PUKPOSE/O, PROGRAMING/O, LANGUAGE/445, CLOSELY/841 
ASSOCIATE/543, UNIX/O, OPEKATE/5726, SYSTEM/49863 
COCAIN/O, COCAINE/i, COKE/8, SNOW/3042 
NARCOTIC/i, ALKALOID/O, EXTKACT/31, COCA/I, LEAVE/24220 
LIGHT_SPEED/I, SPEED_OF_LIGHT/O 
SPEED/14665, LIGHT/22481, TRAVEL/f05, VACUUM/192 
DEGREE_CELSIUS/24, DEGREEiENTIGRADE/28 
DEGKEE/43617, CENTIGRADE/34, SCALE/540, TEMPERATURE/2963 
I00/0, CENTRED/O, CENTUKY/31, HUNDRED/O, ONE_C/O 
TEN/Z3, 10/16150, SOUTH/12213 
database in such a way that the database be- 
havior remains unchanged. Two operations 
are performed at the stage: (i) a simple rel- 
evance  tes t  to remove irrelevant erms (i.e. 
terms not pertaining to the domain at hand); 
(ii) a redundancy check, to remove informa- 
tion that, although perhaps relevant, does not 
affect the database behavior. 
2.4.1 Re levance  tes t  
Terms not appearing in the domain cor- 
pus are considered not relevant o the spe- 
cific domain and removed from the synonym 
database. The rationale underlying this step 
is to remove from the synonym database syn- 
onymy relations that are never going to be 
used in the specific domain. In this way the ef- 
ficiency of the module can be increased, by re- 
ducing the size of the database and the num- 
ber of searches performed (synonyms that are 
known to never appear are not searched for), 
without affecting the system's matching at- 
curacy. E.g., the synset in (10a) would be 
reduced to the synset in (10b). 
(10) a. AMPERE-SECOND/O, COULOMB/O, 
C/9168 
b. C/9168 
2.4.2 Redundancy  check 
The final step is the removal of redundant 
synsets, possibly as a consequence of the pre- 
vious pruning steps. Specifically, the follow- 
ing synsets are removed: 
? Synsets containing a single term (al- 
though the associated sense might be a 
valid one for that term, in the specific 
domain). 
? Duplicate synsets, i.e. identical (in terms 
of synset elements) to some other synset 
not being removed (the choice of the only 
synset o be preserved is arbitrary). 
E.g., the synset in (10b) would be finMly 
removed at this stage. 
3 Sample  app l i ca t ion  
The described methodology was applied to 
the aviation domain. We used the Aviation 
Safety Information System (ASRS) corpus 
(h 'e tp : / /as rs .  a rc .nasa .gov / )  as our avia- 
tion specific corpus. The resulting domain- 
specific database is being used in an IR ap- 
plication that retrieves documents relevant 
to user defined queries, expressed as phrase 
patterns, and identifies portions of text that 
are instances of the relevant phrase patterns. 
The application makes use of Natural Lan- 
guage Processing (NLP) techniques (tagging 
and partial parsing) to annotate documents. 
User defined queries are matched against such 
annotated corpora. Synonyms are used to 
expand occurrences of specific words in such 
queries. In the following two sections we de- 
scribe how the pruning process was performed 
and provide some results. 
3.1 Adapt ing  Wordnet  to the  
av iat ion  domain  
A vocabulary of relevant query terms was 
made available by a user of our IR applica- 
tion and was used in our ranking of synonymy 
relations. Manual pruning was performed on 
the 1000 top ranking terms, with which 6565 
synsets were associated overall. The manual 
pruning task was split between two human 
evaluators. The evaluators were programmers 
members of our staff. They were English na- 
tive speakers who had acquaintance with our 
IR application and with the goals of the man- 
ual pruning process, but no specific training 
or background on lexicographic or WordNet- 
related tasks. For each of the 1000 terms, 
the evaluators were provided with a sample 
of 100 (at most) sentences where the rele- 
vant word occurred in the ASRS corpus. 100 
of the 1000 manually checked clusters (i.e. 
groups of synsets referring to the same head 
term) were submitted to both evaluators (576 
synsets overall), in order to check the rate 
of agreement of their evaluations. The eval- 
uators were allowed to leave synsets unan- 
swered, when the synsets only contained the 
head term (and at least one other synset in 
the cluster had been deemed correct). Leav- 
ing out the cases when one or both evalua- 
tors skipped the answer, there remained 418 
synsets for which both answered. There was 
agreement in 315 cases (75%) and disagree- 
ment in 103 cases (25%). A sample of senses 
on which the evaluators disagreed is shown in 
(11). In each case, the term being evaluated 
is the first in the synset. 
(11) a. {about, around} 
(in the area or vicinity) 
b. {accept, admit, take, take on} 
(admit into a group or commu- 
nity) 
c. {accept, consent, go for} 
(give an affirmative reply to) 
d. {accept, swallow} 
(tolerate or accommodate oneself 
to) 
e. {accept, take} 
(be designed to hold or take) 
f. {accomplished, effected, estab- 
lished} 
(settled securely and uncondi- 
tionally) 
g. {acknowledge, know, recognize} 
(discern) 
h. {act, cognitive operation, cogni- 
tive process, operation, process} 
(the performance of some com- 
posite cognitive activity) 
i. {act, act as, play} 
(pretend to have certain qualities 
or state of mind) 
j. {action, activeness, activity} 
(the state of being active) 
k. {action, activity, natural action, 
natural process} 
(a process existing in or produced 
by nature (rather than by the in- 
tent of human beings)) 
It should be noted that the 'yes' and 'no' 
answers were not evenly distributed between 
the evaluators. In 80% of the cases of dis- 
agreement, i  was evaluator A answering 'yes' 
and evaluator B answering 'no'. This seems 
to suggest han one of the reasons for dis- 
agreement was a different degree of strictness 
in evaluating. Since the evaluators matched 
a sense against an entire corpus (represented 
by a sample of occurrences), one common sit- 
uation may have been that a sense did oc- 
cur, but very rarely. Therefore, the evaluators 
may have applied different criteria in judging 
how many occurrences were needed to deem 
a sense correct. This discrepancy, of course, 
may compound with the fact that the differ- 
ences among WordNet senses can sometimes 
be very subtle. 
Automatic pruning was performed on 
the entire WordNet database, regardless of 
whether candidates had already been manu- 
ally checked or not. This was done for test- 
ing purposes, in order to check the results of 
automatic pruning against the test set ob- 
tained from manual pruning. Besides asso- 
ciating ASRS frequencies with all words in 
synsets and glosses, we also computed fre- 
quencies for collocations (i.e. multi-word 
terms) appearing in synsets. The input to 
automatic pruning was constituted by 10352 
polysemous terms appearing at least once in 
ASRS the corpus. Such terms correspond to 
37494 (term, synset) pairs. Therefore, the 
latter was the actual number of pruning can- 
didates that  were ranked. 
The check of WordNet senses against ASRS 
senses was only done unidirectionally, i.e. 
we only checked whether WordNet senses 
were attested in ASRS. Although it would 
be interesting to see how often the appropri- 
ate, domain-specific senses were absent from 
WordNet, no check of this kind was done. We 
took the simplifying assumption that Word- 
Net be complete, thus aiming at assigning at 
least one WordNet sense to each term that 
appeared in both WordNet and ASRS. 
3.2 Resu l ts  
In order to test the automatic pruning per- 
formance, we ran the ranking procedure on 
a test set taken from the manually checked 
files. This file had been set apart and had 
not been used in the preliminary tests on the 
automatic pruning algorithm. The test set 
included 350 clusters, comprising 2300 candi- 
dates. 1643 candidates were actually assigned 
an evaluation during manual pruning. These 
were used for the test. We extracted the 1643 
relevant items from our ranking list, then we 
incrementally computed precision and recall 
in terms of the items that had been manually 
checked by our human evaluators. The re- 
sults are shown in figure 1. As an example of 
how this figure can be interpreted, taking into 
consideration the top 20% of the ranking list 
(along the X axis), an 80% precision (Y axis) 
means that 80% of the items encountered so 
far had been removed in manual pruning; a 
27% recall (Y axis) means that 27% of the 
overall manually removed items have been en- 
countered so far. 
The automatic pruning task was intention- 
ally framed as a ranking problem, in order to 
leave open the issue of what pruning threshold 
would be optimal. This same approach was 
taken in the IR application in which the prun- 
ing procedure was embedded. Users are given 
the option to set their own pruning threshold 
(depending on whether they focus more on 
precision or recall), by setting a value spec- 
ifying what precision they require. Pruning 
is performed on the top section of the rank- 
ing list that guarantees the required precision, 
according to the correlation between precision 
and amount of pruning shown in figure 1. 
A second test was designed to check 
whether there is a correlation between the 
levels of confidence of automatic and man- 
ual pruning. For this purpose we used the 
file that had been manually checked by both 
human evaiuators. We took into account he 
candidates that had been removed by at least 
one evaluator: the candidates that were re- 
moved by both evaluators were deemed to 
have a high level of confidence, while those 
removed by only one evaluator were deemed 
to have a lower level of confidence. Then we 
checked whether the two classes were equally 
distributed in the automatic pruning ranking 
list, or whether higher confidence candidates 
tended to be ranked higher than lower con- 
fidence ones. The results are shown in fig- 
ure 2, where the automatic pruning recall for 
each class is shown. For any given portion 
of the ranking list higher confidence candi- 
dates (solid lines) have a significantly higher 
recall than lower confidence candidates (dot- 
Table 3: WordNet optimization results. 
DB Synsets Word-senses 
Full WN 99,642 174,008 
Reduced WN 9,441 23,368 
ted line). 
Finally, table 3 shows the result of applying 
the described optimization techniques alone, 
i.e. without any prior pruning, with respect 
to the ASRS corpus. The table shows how 
many synsets and how many word-senses are 
contained in the full Wordnet database and in 
its optimized version. Note that such reduc- 
tion does not involve any loss of accuracy. 
4 Conc lus ions  
There is a need for automatically or semi- 
automatically adapting NLP components o 
specific domain, if such components are to be 
effectively used in IR applications without in- 
volving labor-intensive manual adaptation. A 
key part of adapting NLP components ospe- 
cific domains is the adaptation of their lexical 
and terminological resources. It may often be 
the case that a consistent section of a general 
purpose terminological resource is irrelevant 
to a specific domain, thus involving an unnec- 
essary amount of ambiguity that affects both 
the accuracy and efficiency of the overall NLP 
component. In this paper we have proposed 
a method for adapting a general purpose syn- 
onym database to a specific domain. 
Evaluating the performance of the pro- 
posed pruning method is not a straightfor- 
ward task, since there are no other results 
available on a similar task, to the best of our 
knowledge. However, a comparison between 
the results of manual and automatic pruning 
provides ome useful hints. In particular: 
? The discrepancy between the evaluation 
of human operators hows that the task 
is elusive even for humans (the value of 
the agreement evaluation statistic n for 
our human evaluators was 0.5); 
? however, the correlation between the 
level of confidence of human evaluations 
and scores assigned by the automatic 
pruning procedure shows that the auto- 
matic pruning algorithm captures ome 
significant aspect of the problem. 
Although there is probably room for im- 
proving the automatic pruning performance, 
the preliminary results how that the current 
approach is pointing in the right direction. 
Re ferences  
Christiane Fellbaum, editor. 1998. Wordnet: An 
Electronic Lexical Database. MIT Press Books. 
Claudia Leacock, Martin Chodorow, and 
George A. Miller. 1998. Using corpus tatistics 
and WordNet relations for sense identification. 
Computational Linguistics, 24(1):147-165. 
Bernardo Magnini and Gabriela Cavaglih. 2000. 
Integrating Subject Field Codes into WordNet. 
In Maria Gavrilidou, George Carayannis, Stella 
Markantonatou, Stelios Piperidis, and Gregory 
Stainhaouer, editors, Proceedings of the Sec- 
ond International Conference on Language Re- 
sources and Evaluation (LREC-PO00), pages 
1413-1418, Athens, Greece. 
Mark Sanderson. 1997. Word Sense Disambigua- 
tion and Information Retrieval. Ph.D. thesis, 
Department ofComputing Science at the Uni- 
versity of Glasgow, Glasgow G12. Technical 
Report (TR-1997-7). 
Ellen M. Voorhees. 1998. Using WordNet for text 
retrieval. In Fellbaum (Fellbaum, 1998), chap- 
ter 12, pages 285-303. 
9 
~2 
~9 
100 i I I I 
95 
90 
85 
+? i 
75 
70 
65 
60 
55 
0 
100 
80 
60 
40 
20 
0 I I I I 
0 20 40 60 80 100 
Top % of ranking list 
F igure 1: Precision and recall of automat ic  pruning 
10 
~9 
cg 
100 
80 
60 
40 
20 
I I I I t - - ' - /  
-- _ t / f  j" _ 
-- r l  j --  
/ J _  ; _ 
- - J "  I I I I 
0 20 40 60 80 100 
Top % of ranking list 
Figure 2: A recall comparison for different confidence rates 
11 
